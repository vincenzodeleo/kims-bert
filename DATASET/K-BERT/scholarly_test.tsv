label	text_a
0	Automated  classification  of  collaborative  problem  solving  interactions  in  simulated  science  tasks.  We  present  a  novel  situational  task  that  integrates  collaborative  problem  solving  behavior  with  testing  in  a  science  domain.  Participants  engage  in  discourse,  which  is  used  to  evaluate  their  collaborative  skills.  We  present  initial  experiments  for  automatic  classification  of  such  discourse,  using  a  novel  classification  schema.  Considerable  accuracy  is  achieved  with  just  lexical  features.  A  speech-act  classifier,  trained  on  out-of-domain  data,  can  also  be  helpful.
0	Cluster  based  approach  to  improve  affect  recognition  from  passively  sensed  data.  Negative  affect  is  a  proxy  for  mental  health  in  adults.  By  being  able  to  predict  participants'  negative  affect  states  unobtrusively,  researchers  and  clinicians  will  be  bet­ter  positioned  to  deliver  targeted,  just-in-time  mental  health  interventions  via  mobile  applications.  This  work  attempts  to  personalize  the  passive  recognition  of  negative  affect  states  via  group-based  modeling  of  user  behavior  patterns  captured  from  mobility,  communication,  and  activity  patterns.  Results  show  that  group  models  outperform  generalized  models  in  a  dataset  based  on  two  weeks  of  users'  daily  lives.
0	A  signal  quality  index  for  ballistocardiogram  recordings  based  on  electrocardiogram  rr  intervals  and  matched  filtering.  Detection  of  noisy  ballistocardiogram  (BCG)  mea­surements  with  a  matched  filter  is  investigated.  The  BCG  signal  is  segmented  by  the  RR  intervals  of  the  concurrently  recorded  electrocardiogram  (ECG).  These  segments  are  called  BCG  heartbeats  and  are  averaged  to  estimate  a  parent  template  and  sub-templates  of  BCG  heartbeats  leveraging  the  quasi-periodic  nature  of  the  heart.  All  BCG  heartbeats  in  the  measurement  are  used  to  estimate  the  parent  template  BCG  heartbeat.  By  contrast,  sub-templates  are  estimated  using  subsets  of  BCG  heartbeats  that  extend  over  several  breaths  to  capture  the  vari­ations  in  the  phase  of  the  BCG  heartbeat  due  to  the  breathing  cycles.  The  parent  template  BCG  heartbeat  in  a  recording  is  regarded  as  the  subject-specific  matched  filter.  Finally,  the  average  correlation  coefficient  between  sub-templates  and  the  parent  template  BCG  heartbeat  in  a  recording  is  proposed  as  the  signal  quality  index  (SQI).  A  seven-step  experiment  was  performed  among  thirteen  subjects.  For  reliable  parent  template  estimation,  at  least  40  BCG  heartbeats  should  be  recorded.  For  detection  of  noisy  BCG  measurements,  the  area  under  the  receiver  operating  characteristics  curve  was  presented  for  conditions  of  motion  artifacts,  speaking,  and  deep  breathing.  Motion  artifacts  could  be  detected  with  lower  false  positive  rates  for  the  same  constraint  on  the  detection  rate,  when  compared  to  speaking  and  deep  breathing  cases.  This  work  may  facilitate  online  detection  of  noisy  BCG  recordings.
0	Gender  bias  in  coreference  resolution  evaluation  and  debiasing  methods.  In  this  paper,  we  introduce  a  new  benchmark  for  co-reference  resolution  focused  on  gender  bias,  WinoBias.  Our  corpus  contains  Winograd-schema  style  sentences  with  entities  corresponding  to  people  referred  by  their  occupation  (e.g.  the  nurse,  the  doctor,  the  carpenter).  We  demonstrate  that  a  rule-based,  a  feature-rich,  and  a  neural  coreference  system  all  link  gendered  pronouns  to  pro-stereotypical  entities  with  higher  accuracy  than  anti-stereotypical  entities,  by  an  average  difference  of  21.1  in  F1  score.  Finally,  we  demonstrate  a  data-augmentation  approach  that,  in  combination  with  existing  word-embedding  debiasing  techniques,  removes  the  bias  demonstrated  by  these  systems  in  WinoBias  without  significantly  affecting  their  performance  on  existing  datasets.
0	Where  have  i  heard  this  story  before  identifying  narrative  similarity  in  movie  remakes.  People  can  identify  correspondences  between  narratives  in  everyday  life.  For  example,  an  analogy  with  the  Cinderella  story  may  be  made  in  describing  the  unexpected  success  of  an  underdog  in  seemingly  different  stories.  We  present  a  new  task  and  dataset  for  story  understanding:  identifying  instances  of  similar  narratives  from  a  collection  of  narrative  texts.  We  present  an  initial  approach  for  this  problem,  which  finds  correspondences  between  narratives  in  terms  of  plot  events,  and  resemblances  between  characters  and  their  social  relationships.  Our  approach  yields  an  8%  absolute  improvement  in  performance  over  a  competitive  information-retrieval  baseline  on  a  novel  dataset  of  plot  summaries  of  577  movie  remakes  from  Wikipedia.
0	Andrejjan  at  semeval  2019  task  7  a  fusion  approach  for  exploring  the  key  factors  pertaining  to  rumour  analysis.  The  viral  spread  of  false,  unverified  and  misleading  information  on  the  Internet  has  attracted  a  heightened  attention  of  an  interdisciplinary  research  community  on  the  phenomenon.  This  paper  contributes  to  the  research  efforts  of  automatically  determining  the  veracity  of  rumourous  tweets  and  classifying  their  replies  according  to  stance.  Our  research  objective  was  to  investigate  the  interplay  between  a  number  of  phenomenological  and  contextual  features  of  rumours,  in  particular,  we  explore  the  extent  to  which  network  structural  characteristics,  metadata  and  user  profiles  could  complement  the  linguistic  analysis  of  the  written  content  for  the  task  at  hand.  The  current  findings  strongly  demonstrate  that  supplementary  sources  of  information  play  significant  role  in  classifying  the  veracity  and  the  stance  of  Twitter  interactions  deemed  to  be  rumourous.
0	Evaluating  rewards  for  question  generation  models.  Recent  approaches  to  question  generation  have  used  modifications  to  a  Seq2Seq  architecture  inspired  by  advances  in  machine  translation.  Models  are  trained  using  teacher  forcing  to  optimise  only  the  one-step-ahead  prediction.  However,  at  test  time,  the  model  is  asked  to  generate  a  whole  sequence,  causing  errors  to  propagate  through  the  generation  process  (exposure  bias).  A  number  of  authors  have  suggested  that  optimising  for  rewards  less  tightly  coupled  to  the  training  data  might  counter  this  mismatch.  We  therefore  optimise  directly  for  various  objectives  beyond  simply  replicating  the  ground  truth  questions,  including  a  novel  approach  using  an  adversarial  discriminator  that  seeks  to  generate  questions  that  are  indistinguishable  from  real  examples.  We  confirm  that  training  with  policy  gradient  methods  leads  to  increases  in  the  metrics  used  as  rewards.  We  perform  a  human  evaluation,  and  show  that  although  these  metrics  have  previously  been  assumed  to  be  good  proxies  for  question  quality,  they  are  poorly  aligned  with  human  judgement  and  the  model  simply  learns  to  exploit  the  weaknesses  of  the  reward  source.
0	Toward  abstractive  summarization  using  semantic  representations.  We  present  a  novel  abstractive  summarization  framework  that  draws  on  the  recent  development  of  a  treebank  for  the  Abstract  Meaning  Representation  (AMR).  In  this  framework,  the  source  text  is  parsed  to  a  set  of  AMR  graphs,  the  graphs  are  transformed  into  a  summary  graph,  and  then  text  is  generated  from  the  summary  graph.  We  focus  on  the  graph-tograph  transformation  that  reduces  the  source  semantic  graph  into  a  summary  graph,  making  use  of  an  existing  AMR  parser  and  assuming  the  eventual  availability  of  an  AMR-totext  generator.  The  framework  is  data-driven,  trainable,  and  not  specifically  designed  for  a  particular  domain.  Experiments  on  goldstandard  AMR  annotations  and  system  parses  show  promising  results.  Code  is  available  at:  https://github.com/summarization
0	Umdsub  at  semeval  2018  task  2  multilingual  emoji  prediction  multi  channel  convolutional  neural  network  on  subword  embedding.  This  paper  describes  the  UMDSub  system  that  participated  in  Task  2  of  SemEval-2018.  We  developed  a  system  that  predicts  an  emoji  given  the  raw  text  in  a  English  tweet.  The  system  is  a  Multi-channel  Convolutional  Neural  Network  based  on  subword  embeddings  for  the  representation  of  tweets.  This  model  improves  on  character  or  word  based  methods  by  about  2%.  Our  system  placed  21st  of  48  participating  systems  in  the  official  evaluation.
0	Noisy  labeled  ner  with  confidence  estimation.  Recent  studies  in  deep  learning  have  shown  significant  progress  in  named  entity  recognition  (NER).  However,  most  existing  works  assume  clean  data  annotation,  while  real-world  scenarios  typically  involve  a  large  amount  of  noises  from  a  variety  of  sources  (e.g.,  pseudo,  weak,  or  distant  annotations).  This  work  studies  NER  under  a  noisy  labeled  setting  with  calibrated  confidence  estimation.  Based  on  empirical  observations  of  different  training  dynamics  of  noisy  and  clean  labels,  we  propose  strategies  for  estimating  confidence  scores  based  on  local  and  global  independence  assumptions.  We  partially  marginalize  out  labels  of  low  confidence  with  a  CRF  model.  We  further  propose  a  calibration  method  for  confidence  scores  based  on  the  structure  of  entity  labels.  We  integrate  our  approach  into  a  self-training  framework  for  boosting  performance.  Experiments  in  general  noisy  settings  with  four  languages  and  distantly  labeled  settings  demonstrate  the  effectiveness  of  our  method.
0	Massively  multilingual  neural  machine  translation.  Multilingual  Neural  Machine  Translation  enables  training  a  single  model  that  supports  translation  from  multiple  source  languages  into  multiple  target  languages.  We  perform  extensive  experiments  in  training  massively  multilingual  NMT  models,  involving  up  to  103  distinct  languages  and  204  translation  directions  simultaneously.  We  explore  different  setups  for  training  such  models  and  analyze  the  trade-offs  between  translation  quality  and  various  modeling  decisions.  We  report  results  on  the  publicly  available  TED  talks  multilingual  corpus  where  we  show  that  massively  multilingual  many-to-many  models  are  effective  in  low  resource  settings,  outperforming  the  previous  state-of-the-art  while  supporting  up  to  59  languages  in  116  translation  directions  in  a  single  model.  Our  experiments  on  a  large-scale  dataset  with  103  languages,  204  trained  directions  and  up  to  one  million  examples  per  direction  also  show  promising  results,  surpassing  strong  bilingual  baselines  and  encouraging  future  work  on  massively  multilingual  NMT.
0	Biom  transformers  building  large  biomedical  language  models  with  bert  albert  and  electra.  The  impact  of  design  choices  on  the  performance  of  biomedical  language  models  recently  has  been  a  subject  for  investigation.  In  this  paper,  we  empirically  study  biomedical  domain  adaptation  with  large  transformer  models  using  different  design  choices.  We  evaluate  the  performance  of  our  pretrained  models  against  other  existing  biomedical  language  models  in  the  literature.  Our  results  show  that  we  achieve  state-of-the-art  results  on  several  biomedical  domain  tasks  despite  using  similar  or  less  computational  cost  compared  to  other  models  in  the  literature.  Our  findings  highlight  the  significant  effect  of  design  choices  on  improving  the  performance  of  biomedical  language  models.
0	Berxit  early  exiting  for  bert  with  better  fine  tuning  and  extension  to  regression.  The  slow  speed  of  BERT  has  motivated  much  research  on  accelerating  its  inference,  and  the  early  exiting  idea  has  been  proposed  to  make  trade-offs  between  model  quality  and  efficiency.  This  paper  aims  to  address  two  weaknesses  of  previous  work:  (1)  existing  fine-tuning  strategies  for  early  exiting  models  fail  to  take  full  advantage  of  BERT;  (2)  methods  to  make  exiting  decisions  are  limited  to  classification  tasks.  We  propose  a  more  advanced  fine-tuning  strategy  and  a  learning-to-exit  module  that  extends  early  exiting  to  tasks  other  than  classification.  Experiments  demonstrate  improved  early  exiting  for  BERT,  with  better  trade-offs  obtained  by  the  proposed  fine-tuning  strategy,  successful  application  to  regression  tasks,  and  the  possibility  to  combine  it  with  other  acceleration  methods.  Source  code  can  be  found  at  https://github.com/castorini/berxit.
0	A  neural  few  shot  text  classification  reality  check.  Modern  classification  models  tend  to  struggle  when  the  amount  of  annotated  data  is  scarce.  To  overcome  this  issue,  several  neural  few-shot  classification  models  have  emerged,  yielding  significant  progress  over  time,  both  in  Computer  Vision  and  Natural  Language  Processing.  In  the  latter,  such  models  used  to  rely  on  fixed  word  embeddings,  before  the  advent  of  transformers.  Additionally,  some  models  used  in  Computer  Vision  are  yet  to  be  tested  in  NLP  applications.  In  this  paper,  we  compare  all  these  models,  first  adapting  those  made  in  the  field  of  image  processing  to  NLP,  and  second  providing  them  access  to  transformers.  We  then  test  these  models  equipped  with  the  same  transformer-based  encoder  on  the  intent  detection  task,  known  for  having  a  large  amount  of  classes.  Our  results  reveal  that  while  methods  perform  almost  equally  on  the  ARSC  dataset,  this  is  not  the  case  for  the  Intent  Detection  task,  where  most  recent  and  supposedly  best  competitors  perform  worse  than  older  and  simpler  ones  (while  all  are  are  given  access  to  transformers).  We  also  show  that  a  simple  baseline  is  surprisingly  strong.  All  the  new  developed  models  as  well  as  the  evaluation  framework  are  made  publicly  available.
0	Representation  learning  through  cross  modality  supervision.  Learning  robust  representations  for  applications  with  multiple  modalities  of  input  can  have  a  significant  impact  on  its  performance.  Traditional  representation  learning  methods  rely  on  projecting  the  input  modalities  to  a  common  subspace  to  maximize  agreement  amongst  the  modalities  for  a  particular  task.  We  propose  a  novel  approach  to  representation  learning  that  uses  a  latent  representation  decoder  to  reconstruct  the  target  modality  and  thereby  employs  the  target  modality  purely  as  a  supervision  signal  for  discovering  correlations  between  the  modalities.  Through  cross  modality  supervision,  we  demonstrate  that  the  learnt  representation  is  able  to  improve  the  performance  of  the  task  of  facial  action  unit  (AU)  recognition  when  compared  with  the  modality  specific  representations  and  even  their  fused  counterparts.  Our  experiments  on  three  AU  recognition  datasets  -  MMSE,  BP4D  and  DISFA,  show  strong  performance  gains  producing  state-of-the-art  results  in  spite  of  the  absence  of  a  modality.
0	Cost  sensitive  decision  forest  and  voting  for  software  defect  prediction.  While  traditional  classification  algorithms  optimize  for  accuracy,  cost-sensitive  classification  methods  attempt  to  make  predictions  that  produce  the  lowest  classification  cost.  In  this  paper  we  propose  a  cost-sensitive  classification  technique  called  CSForest  which  is  an  ensemble  of  decision  trees.  We  also  propose  a  cost-sensitive  voting  technique  called  CSVoting.  The  proposed  techniques  are  empirically  evaluated  by  comparing  them  with  five  (5)  classifier  algorithms  on  six  (6)  publicly  available  clean  datasets  that  are  commonly  used  in  the  research  on  software  defect  prediction.  Our  initial  experimental  results  indicate  a  clear  superiority  of  the  proposed  techniques  over  the  existing  ones.
0	Beyond  cls  through  ranking  by  generation.  Generative  models  for  Information  Retrieval,  where  ranking  of  documents  is  viewed  as  the  task  of  generating  a  query  from  a  document’s  language  model,  were  very  successful  in  various  IR  tasks  in  the  past.  However,  with  the  advent  of  modern  deep  neural  networks,  attention  has  shifted  to  discriminative  ranking  functions  that  model  the  semantic  similarity  of  documents  and  queries  instead.  Recently,  deep  generative  models  such  as  GPT2  and  BART  have  been  shown  to  be  excellent  text  generators,  but  their  effectiveness  as  rankers  have  not  been  demonstrated  yet.  In  this  work,  we  revisit  the  generative  framework  for  information  retrieval  and  show  that  our  generative  approaches  are  as  effective  as  state-of-the-art  semantic  similarity-based  discriminative  models  for  the  answer  selection  task.  Additionally,  we  demonstrate  the  effectiveness  of  unlikelihood  losses  for  IR.
0	Tencent  ai  lab  machine  translation  systems  for  the  wmt20  biomedical  translation  task.  This  paper  describes  the  Tencent  AI  Lab  submission  of  the  WMT2020  shared  task  on  biomedical  translation  in  four  language  directions:  German  English,  English  German,  Chinese  English  and  English  Chinese.  We  implement  our  system  with  model  ensemble  technique  on  different  transformer  architectures  (Deep,  Hybrid,  Big,  Large  Transformers).  To  enlarge  the  in-domain  bilingual  corpus,  we  use  back-translation  of  monolingual  in-domain  data  in  the  target  language  as  additional  in-domain  training  data.  Our  systems  in  German->English  and  English->German  are  ranked  1st  and  3rd  respectively  according  to  the  official  evaluation  results  in  terms  of  BLEU  scores.
0	Fast  valuation  of  large  portfolios  of  variable  annuities  via  transfer  learning.  Variable  annuities  are  important  financial  products  that  result  in  100  billion  sales  in  2018.  These  products  contain  complex  guarantees  that  are  computationally  expensive  to  value,  and  insurance  companies  are  turning  to  machine  learning  for  the  valuation  of  large  portfolios  of  variable  annuity  policies.  Although  earlier  studies,  exemplified  by  the  regression  modelling  approach,  have  shown  promising  results,  the  valuation  accuracy  is  unsatisfying.  In  this  paper,  we  show  that  one  main  cause  for  the  poor  valuation  accuracy  is  the  inefficient  selection  of  representative  policies.  To  overcome  this  problem,  we  propose  a  novel  transfer-learning  based  portfolio  valuation  framework.  The  framework  first  builds  a  backbone  deep  neural  network  using  historical  Monte  Carlo  simulation  results.  The  backbone  network  provides  a  valuation-driven  representation  for  selecting  the  policies  that  best  represent  a  large  portfolio.  Furthermore,  the  transferred  network  provides  a  way  to  adaptively  extrapolate  from  these  representative  policies  to  the  remaining  policies  in  the  portfolio.  By  overcoming  a  major  difficulty  faced  by  the  popular  Kriging  model,  the  need  of  matrix  inversion,  the  transferred  network  can  handle  a  large  number  of  representative  policies  to  sufficiently  cover  a  diverse  portfolio.
0	Improving  alignment  of  system  combination  by  using  multi  objective  optimization.  This  paper  proposes  a  multi-objective  optimization  framework  which  supports  heterogeneous  information  sources  to  improve  alignment  in  machine  translation  system  combination  techniques.  In  this  area,  most  of  techniques  usually  utilize  confusion  networks  (CN)  as  their  central  data  structure  to  compact  an  exponential  number  of  an  potential  hypotheses,  and  because  better  hypothesis  alignment  may  benefit  constructing  better  quality  confusion  networks,  it  is  natural  to  add  more  useful  information  to  improve  alignment  results.  However,  these  information  may  be  heterogeneous,  so  the  widely-used  Viterbi  algorithm  for  searching  the  best  alignment  may  not  apply  here.  In  the  multi-objective  optimization  framework,  each  information  source  is  viewed  as  an  independent  objective,  and  a  new  goal  of  improving  all  objectives  can  be  searched  by  mature  algorithms.  The  solutions  from  this  framework,  termed  Pareto  optimal  solutions,  are  then  combined  to  construct  confusion  networks.  Experiments  on  two  Chinese-to-English  translation  datasets  show  significant  improvements,  0.97  and  1.06  BLEU  points  over  a  strong  Indirected  Hidden  Markov  Model-based  (IHMM)  system,  and  4.75  and  3.53  points  over  the  best  single  machine  translation  systems.
0	Formality  style  transfer  for  noisy  user  generated  conversations  extracting  labeled  parallel  data  from  unlabeled  corpora.  Typical  datasets  used  for  style  transfer  in  NLP  contain  aligned  pairs  of  two  opposite  extremes  of  a  style.  As  each  existing  dataset  is  sourced  from  a  specific  domain  and  context,  most  use  cases  will  have  a  sizable  mismatch  from  the  vocabulary  and  sentence  structures  of  any  dataset  available.  This  reduces  the  performance  of  the  style  transfer,  and  is  particularly  significant  for  noisy,  user-generated  text.  To  solve  this  problem,  we  show  a  technique  to  derive  a  dataset  of  aligned  pairs  (style-agnostic  vs  stylistic  sentences)  from  an  unlabeled  corpus  by  using  an  auxiliary  dataset,  allowing  for  in-domain  training.  We  test  the  technique  with  the  Yahoo  Formality  Dataset  and  6  novel  datasets  we  produced,  which  consist  of  scripts  from  5  popular  TV-shows  (Friends,  Futurama,  Seinfeld,  Southpark,  Stargate  SG-1)  and  the  Slate  Star  Codex  online  forum.  We  gather  1080  human  evaluations,  which  show  that  our  method  produces  a  sizable  change  in  formality  while  maintaining  fluency  and  context;  and  that  it  considerably  outperforms  OpenNMT’s  Seq2Seq  model  directly  trained  on  the  Yahoo  Formality  Dataset.  Additionally,  we  publish  the  full  pipeline  code  and  our  novel  datasets.
0	Convolutions  are  all  you  need  for  classifying  character  sequences.  While  recurrent  neural  networks  (RNNs)  are  widely  used  for  text  classification,  they  demonstrate  poor  performance  and  slow  convergence  when  trained  on  long  sequences.  When  text  is  modeled  as  characters  instead  of  words,  the  longer  sequences  make  RNNs  a  poor  choice.  Convolutional  neural  networks  (CNNs),  although  somewhat  less  ubiquitous  than  RNNs,  have  an  internal  structure  more  appropriate  for  long-distance  character  dependencies.  To  better  understand  how  CNNs  and  RNNs  differ  in  handling  long  sequences,  we  use  them  for  text  classification  tasks  in  several  character-level  social  media  datasets.  The  CNN  models  vastly  outperform  the  RNN  models  in  our  experiments,  suggesting  that  CNNs  are  superior  to  RNNs  at  learning  to  classify  character-level  data.
0	Tsdg  content  aware  neural  response  generation  with  two  stage  decoding  process.  Neural  response  generative  models  have  achieved  remarkable  progress  in  recent  years  but  tend  to  yield  irrelevant  and  uninformative  responses.  One  of  the  reasons  is  that  encoder-decoder  based  models  always  use  a  single  decoder  to  generate  a  complete  response  at  a  stroke.  This  tends  to  generate  high-frequency  function  words  with  less  semantic  information  rather  than  low-frequency  content  words  with  more  semantic  information.  To  address  this  issue,  we  propose  a  content-aware  model  with  two-stage  decoding  process  named  Two-stage  Dialogue  Generation  (TSDG).  We  separate  the  decoding  process  of  content  words  and  function  words  so  that  content  words  can  be  generated  independently  without  the  interference  of  function  words.  Experimental  results  on  two  datasets  indicate  that  our  model  significantly  outperforms  several  competitive  generative  models  in  terms  of  automatic  and  human  evaluation.
0	Out  of  sample  representation  learning  for  knowledge  graphs.  Many  important  problems  can  be  formulated  as  reasoning  in  knowledge  graphs.  Representation  learning  has  proved  extremely  effective  for  transductive  reasoning,  in  which  one  needs  to  make  new  predictions  for  already  observed  entities.  This  is  true  for  both  attributed  graphs(where  each  entity  has  an  initial  feature  vector)  and  non-attributed  graphs  (where  the  only  initial  information  derives  from  known  relations  with  other  entities).  For  out-of-sample  reasoning,  where  one  needs  to  make  predictions  for  entities  that  were  unseen  at  training  time,  much  prior  work  considers  attributed  graph.  However,  this  problem  is  surprisingly  under-explored  for  non-attributed  graphs.  In  this  paper,  we  study  the  out-of-sample  representation  learning  problem  for  non-attributed  knowledge  graphs,  create  benchmark  datasets  for  this  task,  develop  several  models  and  baselines,  and  provide  empirical  analyses  and  comparisons  of  the  proposed  models  and  baselines.
0	Look  at  the  first  sentence  position  bias  in  question  answering.  Many  extractive  question  answering  models  are  trained  to  predict  start  and  end  positions  of  answers.  The  choice  of  predicting  answers  as  positions  is  mainly  due  to  its  simplicity  and  effectiveness.  In  this  study,  we  hypothesize  that  when  the  distribution  of  the  answer  positions  is  highly  skewed  in  the  training  set  (e.g.,  answers  lie  only  in  the  k-th  sentence  of  each  passage),  QA  models  predicting  answers  as  positions  can  learn  spurious  positional  cues  and  fail  to  give  answers  in  different  positions.  We  first  illustrate  this  position  bias  in  popular  extractive  QA  models  such  as  BiDAF  and  BERT  and  thoroughly  examine  how  position  bias  propagates  through  each  layer  of  BERT.  To  safely  deliver  position  information  without  position  bias,  we  train  models  with  various  de-biasing  methods  including  entropy  regularization  and  bias  ensembling.  Among  them,  we  found  that  using  the  prior  distribution  of  answer  positions  as  a  bias  model  is  very  effective  at  reducing  position  bias,  recovering  the  performance  of  BERT  from  37.48%  to  81.64%  when  trained  on  a  biased  SQuAD  dataset.
0	Mining  knowledge  for  natural  language  inference  from  wikipedia  categories.  Accurate  lexical  entailment  (LE)  and  natural  language  inference  (NLI)  often  require  large  quantities  of  costly  annotations.  To  alleviate  the  need  for  labeled  data,  we  introduce  WikiNLI:  a  resource  for  improving  model  performance  on  NLI  and  LE  tasks.  It  contains  428,899  pairs  of  phrases  constructed  from  naturally  annotated  category  hierarchies  in  Wikipedia.  We  show  that  we  can  improve  strong  baselines  such  as  BERT  and  RoBERTa  by  pretraining  them  on  WikiNLI  and  transferring  the  models  on  downstream  tasks.  We  conduct  systematic  comparisons  with  phrases  extracted  from  other  knowledge  bases  such  as  WordNet  and  Wikidata  to  find  that  pretraining  on  WikiNLI  gives  the  best  performance.  In  addition,  we  construct  WikiNLI  in  other  languages,  and  show  that  pretraining  on  them  improves  performance  on  NLI  tasks  of  corresponding  languages.
0	Dependency  parsing  for  spoken  dialog  systems.  Dependency  parsing  of  conversational  input  can  play  an  important  role  in  language  understanding  for  dialog  systems  by  identifying  the  relationships  between  entities  extracted  from  user  utterances.  Additionally,  effective  dependency  parsing  can  elucidate  differences  in  language  structure  and  usage  for  discourse  analysis  of  human-human  versus  human-machine  dialogs.  However,  models  trained  on  datasets  based  on  news  articles  and  web  data  do  not  perform  well  on  spoken  human-machine  dialog,  and  currently  available  annotation  schemes  do  not  adapt  well  to  dialog  data.  Therefore,  we  propose  the  Spoken  Conversation  Universal  Dependencies  (SCUD)  annotation  scheme  that  extends  the  Universal  Dependencies  (UD)  (Nivre  et  al.,  2016)  guidelines  to  spoken  human-machine  dialogs.  We  also  provide  ConvBank,  a  conversation  dataset  between  humans  and  an  open-domain  conversational  dialog  system  with  SCUD  annotation.  Finally,  to  demonstrate  the  utility  of  the  dataset,  we  train  a  dependency  parser  on  the  ConvBank  dataset.  We  demonstrate  that  by  pre-training  a  dependency  parser  on  a  set  of  larger  public  datasets  and  fine-tuning  on  ConvBank  data,  we  achieved  the  best  result,  85.05%  unlabeled  and  77.82%  labeled  attachment  accuracy.
0	Neural  news  recommendation  with  multi  head  self  attention.  News  recommendation  can  help  users  find  interested  news  and  alleviate  information  overload.  Precisely  modeling  news  and  users  is  critical  for  news  recommendation,  and  capturing  the  contexts  of  words  and  news  is  important  to  learn  news  and  user  representations.  In  this  paper,  we  propose  a  neural  news  recommendation  approach  with  multi-head  self-attention  (NRMS).  The  core  of  our  approach  is  a  news  encoder  and  a  user  encoder.  In  the  news  encoder,  we  use  multi-head  self-attentions  to  learn  news  representations  from  news  titles  by  modeling  the  interactions  between  words.  In  the  user  encoder,  we  learn  representations  of  users  from  their  browsed  news  and  use  multi-head  self-attention  to  capture  the  relatedness  between  the  news.  Besides,  we  apply  additive  attention  to  learn  more  informative  news  and  user  representations  by  selecting  important  words  and  news.  Experiments  on  a  real-world  dataset  validate  the  effectiveness  and  efficiency  of  our  approach.
0	Neural  ranking  models  for  temporal  dependency  structure  parsing.  We  design  and  build  the  first  neural  temporal  dependency  parser.  It  utilizes  a  neural  ranking  model  with  minimal  feature  engineering,  and  parses  time  expressions  and  events  in  a  text  into  a  temporal  dependency  tree  structure.  We  evaluate  our  parser  on  two  domains:  news  reports  and  narrative  stories.  In  a  parsing-only  evaluation  setup  where  gold  time  expressions  and  events  are  provided,  our  parser  reaches  0.81  and  0.70  f-score  on  unlabeled  and  labeled  parsing  respectively,  a  result  that  is  very  competitive  against  alternative  approaches.  In  an  end-to-end  evaluation  setup  where  time  expressions  and  events  are  automatically  recognized,  our  parser  beats  two  strong  baselines  on  both  data  domains.  Our  experimental  results  and  discussions  shed  light  on  the  nature  of  temporal  dependency  structures  in  different  domains  and  provide  insights  that  we  believe  will  be  valuable  to  future  research  in  this  area.
0	Automatic  essay  scoring  incorporating  rating  schema  via  reinforcement  learning.  Automatic  essay  scoring  (AES)  is  the  task  of  assigning  grades  to  essays  without  human  interference.  Existing  systems  for  AES  are  typically  trained  to  predict  the  score  of  each  single  essay  at  a  time  without  considering  the  rating  schema.  In  order  to  address  this  issue,  we  propose  a  reinforcement  learning  framework  for  essay  scoring  that  incorporates  quadratic  weighted  kappa  as  guidance  to  optimize  the  scoring  system.  Experiment  results  on  benchmark  datasets  show  the  effectiveness  of  our  framework.
0	Decipherment  of  substitution  ciphers  with  neural  language  models.  Decipherment  of  homophonic  substitution  ciphers  using  language  models  is  a  well-studied  task  in  NLP.  Previous  work  in  this  topic  scores  short  local  spans  of  possible  plaintext  decipherments  using  n-gram  language  models.  The  most  widely  used  technique  is  the  use  of  beam  search  with  n-gram  language  models  proposed  by  Nuhn  et  al.(2013).  We  propose  a  beam  search  algorithm  that  scores  the  entire  candidate  plaintext  at  each  step  of  the  decipherment  using  a  neural  language  model.  We  augment  beam  search  with  a  novel  rest  cost  estimation  that  exploits  the  prediction  power  of  a  neural  language  model.  We  compare  against  the  state  of  the  art  n-gram  based  methods  on  many  different  decipherment  tasks.  On  challenging  ciphers  such  as  the  Beale  cipher  we  provide  significantly  better  error  rates  with  much  smaller  beam  sizes.
0	Prophetnet  predicting  future  n  gram  for  sequence  to  sequence  pre  training.  This  paper  presents  a  new  sequence-to-sequence  pre-training  model  called  ProphetNet,  which  introduces  a  novel  self-supervised  objective  named  future  n-gram  prediction  and  the  proposed  n-stream  self-attention  mechanism.  Instead  of  optimizing  one-step-ahead  prediction  in  the  traditional  sequence-to-sequence  model,  the  ProphetNet  is  optimized  by  n-step  ahead  prediction  that  predicts  the  next  n  tokens  simultaneously  based  on  previous  context  tokens  at  each  time  step.  The  future  n-gram  prediction  explicitly  encourages  the  model  to  plan  for  the  future  tokens  and  prevent  overfitting  on  strong  local  correlations.  We  pre-train  ProphetNet  using  a  base  scale  dataset  (16GB)  and  a  large-scale  dataset  (160GB),  respectively.  Then  we  conduct  experiments  on  CNN/DailyMail,  Gigaword,  and  SQuAD  1.1  benchmarks  for  abstractive  summarization  and  question  generation  tasks.  Experimental  results  show  that  ProphetNet  achieves  new  state-of-the-art  results  on  all  these  datasets  compared  to  the  models  using  the  same  scale  pre-training  corpus.
0	A  novel  dependency  to  string  model  for  statistical  machine  translation.  Dependency  structure,  as  a  first  step  towards  semantics,  is  believed  to  be  helpful  to  improve  translation  quality.  However,  previous  works  on  dependency  structure  based  models  typically  resort  to  insertion  operations  to  complete  translations,  which  make  it  difficult  to  specify  ordering  information  in  translation  rules.  In  our  model  of  this  paper,  we  handle  this  problem  by  directly  specifying  the  ordering  information  in  head-dependents  rules  which  represent  the  source  side  as  head-dependents  relations  and  the  target  side  as  strings.  The  head-dependents  rules  require  only  substitution  operation,  thus  our  model  requires  no  heuristics  or  separate  ordering  models  of  the  previous  works  to  control  the  word  order  of  translations.  Large-scale  experiments  show  that  our  model  performs  well  on  long  distance  reordering,  and  outperforms  the  state-of-the-art  constituency-to-string  model  (+1.47  BLEU  on  average)  and  hierarchical  phrase-based  model  (+0.46  BLEU  on  average)  on  two  Chinese-English  NIST  test  sets  without  resort  to  phrases  or  parse  forest.  For  the  first  time,  a  source  dependency  structure  based  model  catches  up  with  and  surpasses  the  state-of-the-art  translation  models.
0	Do  models  of  mental  health  based  on  social  media  data  generalize.  Proxy-based  methods  for  annotating  mental  health  status  in  social  media  have  grown  popular  in  computational  research  due  to  their  ability  to  gather  large  training  samples.  However,  an  emerging  body  of  literature  has  raised  new  concerns  regarding  the  validity  of  these  types  of  methods  for  use  in  clinical  applications.  To  further  understand  the  robustness  of  distantly  supervised  mental  health  models,  we  explore  the  generalization  ability  of  machine  learning  classifiers  trained  to  detect  depression  in  individuals  across  multiple  social  media  platforms.  Our  experiments  not  only  reveal  that  substantial  loss  occurs  when  transferring  between  platforms,  but  also  that  there  exist  several  unreliable  confounding  factors  that  may  enable  researchers  to  overestimate  classification  performance.  Based  on  these  results,  we  enumerate  recommendations  for  future  mental  health  dataset  construction.
0	Bsc  participation  in  the  wmt  translation  of  biomedical  abstracts.  This  paper  describes  the  machine  translation  systems  developed  by  the  Barcelona  Supercomputing  (BSC)  team  for  the  biomedical  translation  shared  task  of  WMT19.  Our  system  is  based  on  Neural  Machine  Translation  unsing  the  OpenNMT-py  toolkit  and  Transformer  architecture.  We  participated  in  four  translation  directions  for  the  English/Spanish  and  English/Portuguese  language  pairs.  To  create  our  training  data,  we  concatenated  several  parallel  corpora,  both  from  in-domain  and  out-of-domain  sources,  as  well  as  terminological  resources  from  UMLS.
0	That  s  so  annoying  a  lexical  and  frame  semantic  embedding  based  data  augmentation  approach  to  automatic  categorization  of  annoying  behaviors  using  petpeeve  tweets.  We  propose  a  novel  data  augmentation  approach  to  enhance  computational  behavioral  analysis  using  social  media  text.  In  particular,  we  collect  a  Twitter  corpus  of  the  descriptions  of  annoying  behaviors  using  the  #petpeeve  hashtags.  In  the  qualitative  analysis,  we  study  the  language  use  in  these  tweets,  with  a  special  focus  on  the  fine-grained  categories  and  the  geographic  variation  of  the  language.  In  quantitative  analysis,  we  show  that  lexical  and  syntactic  features  are  useful  for  automatic  categorization  of  annoying  behaviors,  and  frame-semantic  features  further  boost  the  performance;  that  leveraging  large  lexical  embeddings  to  create  additional  training  instances  significantly  improves  the  lexical  model;  and  incorporating  frame-semantic  embedding  achieves  the  best  overall  performance.
0	On  the  language  neutrality  of  pre  trained  multilingual  representations.  Multilingual  contextual  embeddings,  such  as  multilingual  BERT  and  XLM-RoBERTa,  have  proved  useful  for  many  multi-lingual  tasks.  Previous  work  probed  the  cross-linguality  of  the  representations  indirectly  using  zero-shot  transfer  learning  on  morphological  and  syntactic  tasks.  We  instead  investigate  the  language-neutrality  of  multilingual  contextual  embeddings  directly  and  with  respect  to  lexical  semantics.  Our  results  show  that  contextual  embeddings  are  more  language-neutral  and,  in  general,  more  informative  than  aligned  static  word-type  embeddings,  which  are  explicitly  trained  for  language  neutrality.  Contextual  embeddings  are  still  only  moderately  language-neutral  by  default,  so  we  propose  two  simple  methods  for  achieving  stronger  language  neutrality:  first,  by  unsupervised  centering  of  the  representation  for  each  language  and  second,  by  fitting  an  explicit  projection  on  small  parallel  data.  Besides,  we  show  how  to  reach  state-of-the-art  accuracy  on  language  identification  and  match  the  performance  of  statistical  methods  for  word  alignment  of  parallel  sentences  without  using  parallel  data.
0	Dagobert  generating  derivational  morphology  with  a  pretrained  language  model.  Can  pretrained  language  models  (PLMs)  generate  derivationally  complex  words?  We  present  the  first  study  investigating  this  question,  taking  BERT  as  the  example  PLM.  We  examine  BERT’s  derivational  capabilities  in  different  settings,  ranging  from  using  the  unmodified  pretrained  model  to  full  finetuning.  Our  best  model,  DagoBERT  (Derivationally  and  generatively  optimized  BERT),  clearly  outperforms  the  previous  state  of  the  art  in  derivation  generation  (DG).  Furthermore,  our  experiments  show  that  the  input  segmentation  crucially  impacts  BERT’s  derivational  knowledge,  suggesting  that  the  performance  of  PLMs  could  be  further  improved  if  a  morphologically  informed  vocabulary  of  units  were  used.
0	Record  linkage  using  graph  consistency.  This  paper  provides  a  method  for  automated  record  linkage  in  the  historical  domain  based  on  collective  entity  resolution.  Multiple  records  are  considered  for  linkage  simultaneously,  using  plausible  record  sequences  as  a  substitute  for  pair-wise  record  similarity  measures  such  as  string  edit  distance.  The  method  is  applied  to  the  problem  of  family  reconstruction  from  historical  archives.  A  benchmark  evaluation  shows  that  the  approach  provides  a  computationally  efficient  way  to  produce  family  reconstructions  which  are  useful  in  practise.  Further  improvements  in  linkage  accuracy  are  expected  by  addressing  data  issues  and  linkage  assumption  violations.
0	Web  based  validation  for  contextual  targeted  paraphrasing.  In  this  work,  we  present  a  scenario  where  contextual  targeted  paraphrasing  of  sub-sentential  phrases  is  performed  automatically  to  support  the  task  of  text  revision.  Candidate  paraphrases  are  obtained  from  a  preexisting  repertoire  and  validated  in  the  context  of  the  original  sentence  using  information  derived  from  the  Web.  We  report  on  experiments  on  French,  where  the  original  sentences  to  be  rewritten  are  taken  from  a  rewriting  memory  automatically  extracted  from  the  edit  history  of  Wikipedia.
0	Grounded  language  learning  from  video  described  with  sentences.  We  present  a  method  that  learns  representations  for  word  meanings  from  short  video  clips  paired  with  sentences.  Unlike  prior  work  on  learning  language  from  symbolic  input,  our  input  consists  of  video  of  people  interacting  with  multiple  complex  objects  in  outdoor  environments.  Unlike  prior  computer-vision  approaches  that  learn  from  videos  with  verb  labels  or  images  with  noun  labels,  our  labels  are  sentences  containing  nouns,  verbs,  prepositions,  adjectives,  and  adverbs.  The  correspondence  between  words  and  concepts  in  the  video  is  learned  in  an  unsupervised  fashion,  even  when  the  video  depicts  simultaneous  events  described  by  multiple  sentences  or  when  different  aspects  of  a  single  event  are  described  with  multiple  sentences.  The  learned  word  meanings  can  be  subsequently  used  to  automatically  generate  description  of  new  video.
0	Automatic  interpretation  of  the  english  possessive.  The  English  ’s  possessive  construction  occurs  frequently  in  text  and  can  encode  several  different  semantic  relations;  however,  it  has  received  limited  attention  from  the  computational  linguistics  community.  This  paper  describes  the  creation  of  a  semantic  relation  inventory  covering  the  use  of  ’s,  an  inter-annotator  agreement  study  to  calculate  how  well  humans  can  agree  on  the  relations,  a  large  collection  of  possessives  annotated  according  to  the  relations,  and  an  accurate  automatic  annotation  system  for  labeling  new  examples.  Our  21,938  example  dataset  is  by  far  the  largest  annotated  possessives  dataset  we  are  aware  of,  and  both  our  automatic  classification  system,  which  achieves  87.4%  accuracy  in  our  classification  experiment,  and  our  annotation  data  are  publicly  available.
0	Imitation  learning  for  non  autoregressive  neural  machine  translation.  Non-autoregressive  translation  models  (NAT)  have  achieved  impressive  inference  speedup.  A  potential  issue  of  the  existing  NAT  algorithms,  however,  is  that  the  decoding  is  conducted  in  parallel,  without  directly  considering  previous  context.  In  this  paper,  we  propose  an  imitation  learning  framework  for  non-autoregressive  machine  translation,  which  still  enjoys  the  fast  translation  speed  but  gives  comparable  translation  performance  compared  to  its  auto-regressive  counterpart.  We  conduct  experiments  on  the  IWSLT16,  WMT14  and  WMT16  datasets.  Our  proposed  model  achieves  a  significant  speedup  over  the  autoregressive  models,  while  keeping  the  translation  quality  comparable  to  the  autoregressive  models.  By  sampling  sentence  length  in  parallel  at  inference  time,  we  achieve  the  performance  of  31.85  BLEU  on  WMT16  Ro→En  and  30.68  BLEU  on  IWSLT16  En→De.
0	Cross  target  stance  classification  with  self  attention  networks.  In  stance  classification,  the  target  on  which  the  stance  is  made  defines  the  boundary  of  the  task,  and  a  classifier  is  usually  trained  for  prediction  on  the  same  target.  In  this  work,  we  explore  the  potential  for  generalizing  classifiers  between  different  targets,  and  propose  a  neural  model  that  can  apply  what  has  been  learned  from  a  source  target  to  a  destination  target.  We  show  that  our  model  can  find  useful  information  shared  between  relevant  targets  which  improves  generalization  in  certain  scenarios.
0	Task  oriented  dialogue  system  for  automatic  diagnosis.  In  this  paper,  we  make  a  move  to  build  a  dialogue  system  for  automatic  diagnosis.  We  first  build  a  dataset  collected  from  an  online  medical  forum  by  extracting  symptoms  from  both  patients’  self-reports  and  conversational  data  between  patients  and  doctors.  Then  we  propose  a  task-oriented  dialogue  system  framework  to  make  diagnosis  for  patients  automatically,  which  can  converse  with  patients  to  collect  additional  symptoms  beyond  their  self-reports.  Experimental  results  on  our  dataset  show  that  additional  symptoms  extracted  from  conversation  can  greatly  improve  the  accuracy  for  disease  identification  and  our  dialogue  system  is  able  to  collect  these  symptoms  automatically  and  make  a  better  diagnosis.
0	What  to  do  with  an  airport  mining  arguments  in  the  german  online  participation  project  tempelhofer  feld.  This  paper  focuses  on  the  automated  extraction  of  argument  components  from  user  content  in  the  German  online  participation  project  Tempelhofer  Feld.  We  adapt  existing  argumentation  models  into  a  new  model  for  decision-oriented  online  participation.  Our  model  consists  of  three  categories:  major  positions,  claims,  and  premises.  We  create  a  new  German  corpus  for  argument  mining  by  annotating  our  dataset  with  our  model.  Afterwards,  we  focus  on  the  two  classification  tasks  of  identifying  argumentative  sentences  and  predicting  argument  components  in  sentences.  We  achieve  macro-averaged  F1  measures  of  69.77%  and  68.5%,  respectively.
0	An  unsupervised  model  for  joint  phrase  alignment  and  extraction.  We  present  an  unsupervised  model  for  joint  phrase  alignment  and  extraction  using  non-parametric  Bayesian  methods  and  inversion  transduction  grammars  (ITGs).  The  key  contribution  is  that  phrases  of  many  granularities  are  included  directly  in  the  model  through  the  use  of  a  novel  formulation  that  memorizes  phrases  generated  not  only  by  terminal,  but  also  non-terminal  symbols.  This  allows  for  a  completely  probabilistic  model  that  is  able  to  create  a  phrase  table  that  achieves  competitive  accuracy  on  phrase-based  machine  translation  tasks  directly  from  unaligned  sentence  pairs.  Experiments  on  several  language  pairs  demonstrate  that  the  proposed  model  matches  the  accuracy  of  traditional  two-step  word  alignment/phrase  extraction  approach  while  reducing  the  phrase  table  to  a  fraction  of  the  original  size.
0	Bigpatent  a  large  scale  dataset  for  abstractive  and  coherent  summarization.  Most  existing  text  summarization  datasets  are  compiled  from  the  news  domain,  where  summaries  have  a  flattened  discourse  structure.  In  such  datasets,  summary-worthy  content  often  appears  in  the  beginning  of  input  articles.  Moreover,  large  segments  from  input  articles  are  present  verbatim  in  their  respective  summaries.  These  issues  impede  the  learning  and  evaluation  of  systems  that  can  understand  an  article’s  global  content  structure  as  well  as  produce  abstractive  summaries  with  high  compression  ratio.  In  this  work,  we  present  a  novel  dataset,  BIGPATENT,  consisting  of  1.3  million  records  of  U.S.  patent  documents  along  with  human  written  abstractive  summaries.  Compared  to  existing  summarization  datasets,  BIGPATENT  has  the  following  properties:  i)  summaries  contain  a  richer  discourse  structure  with  more  recurring  entities,  ii)  salient  content  is  evenly  distributed  in  the  input,  and  iii)  lesser  and  shorter  extractive  fragments  are  present  in  the  summaries.  Finally,  we  train  and  evaluate  baselines  and  popular  learning  models  on  BIGPATENT  to  shed  light  on  new  challenges  and  motivate  future  directions  for  summarization  research.
0	Detecting  simultaneously  chinese  grammar  errors  based  on  a  bilstm  crf  model.  In  the  process  of  learning  and  using  Chinese,  many  learners  of  Chinese  as  foreign  language(CFL)  may  have  grammar  errors  due  to  negative  migration  of  their  native  languages.  This  paper  introduces  our  system  that  can  simultaneously  diagnose  four  types  of  grammatical  errors  including  redundant  (R),  missing  (M),  selection  (S),  disorder  (W)  in  NLPTEA-5  shared  task.  We  proposed  a  Bidirectional  LSTM  CRF  neural  network  (BiLSTM-CRF)  that  combines  BiLSTM  and  CRF  without  hand-craft  features  for  Chinese  Grammatical  Error  Diagnosis  (CGED).  Evaluation  includes  three  levels,  which  are  detection  level,  identification  level  and  position  level.  At  the  detection  level  and  identification  level,  our  system  got  the  third  recall  scores,  and  achieved  good  F1  values.
0	Transducing  sentences  to  syntactic  feature  vectors  an  alternative  way  to  parse.  Classification  and  learning  algorithms  use  syntactic  structures  as  proxies  between  source  sentences  and  feature  vectors.  In  this  paper,  we  explore  an  alternative  path  to  use  syntax  in  feature  spaces:  the  Distributed  Representation  “Parsers”(DRP).  The  core  of  the  idea  is  straightforward:  DRPs  directly  obtain  syntactic  feature  vectors  from  sentences  without  explicitly  producing  symbolic  syntactic  interpretations.  Results  show  that  DRPs  produce  feature  spaces  significantly  better  than  those  obtained  by  existing  methods  in  the  same  conditions  and  competitive  with  those  obtained  by  existing  methods  with  lexical  information.
0	Neural  models  for  detecting  binary  semantic  textual  similarity  for  algerian  and  msa.  We  explore  the  extent  to  which  neural  networks  can  learn  to  identify  semantically  equivalent  sentences  from  a  small  variable  dataset  using  an  end-to-end  training.  We  collect  a  new  noisy  non-standardised  user-generated  Algerian  (ALG)  dataset  and  also  translate  it  to  Modern  Standard  Arabic  (MSA)  which  serves  as  its  regularised  counterpart.  We  compare  the  performance  of  various  models  on  both  datasets  and  report  the  best  performing  configurations.  The  results  show  that  relatively  simple  models  composed  of  2  LSTM  layers  outperform  by  far  other  more  sophisticated  attention-based  architectures,  for  both  ALG  and  MSA  datasets.
0	Not  far  away  not  so  close  sample  efficient  nearest  neighbour  data  augmentation  via  minimax.  In  Natural  Language  Processing  (NLP),  finding  data  augmentation  techniques  that  can  produce  high-quality  human-interpretable  examples  has  always  been  challenging.  Recently,  leveraging  kNN  such  that  augmented  examples  are  retrieved  from  large  repositories  of  unlabelled  sentences  has  made  a  step  toward  interpretable  augmentation.  Inspired  by  this  paradigm,  we  introduce  Minimax-kNN,  a  sample  efficient  data  augmentation  strategy  tailored  for  Knowledge  Distillation  (KD).  We  exploit  a  semi-supervised  approach  based  on  KD  to  train  a  model  on  augmented  data.  In  contrast  to  existing  kNN  augmentation  techniques  that  blindly  incorporate  all  samples,  our  method  dynamically  selects  a  subset  of  augmented  samples  that  maximizes  KL-divergence  between  the  teacher  and  student  models.  This  step  aims  to  extract  the  most  efficient  samples  to  ensure  our  augmented  data  covers  regions  in  the  input  space  with  maximum  loss  value.  We  evaluated  our  technique  on  several  text  classification  tasks  and  demonstrated  that  Minimax-kNN  consistently  outperforms  strong  baselines.  Our  results  show  that  Minimax-kNN  requires  fewer  augmented  examples  and  less  computation  to  achieve  superior  performance  over  the  state-of-the-art  kNN-based  augmentation  techniques.
0	Benchmarking  multimodal  regex  synthesis  with  complex  structures.  Existing  datasets  for  regular  expression  (regex)  generation  from  natural  language  are  limited  in  complexity;  compared  to  regex  tasks  that  users  post  on  StackOverflow,  the  regexes  in  these  datasets  are  simple,  and  the  language  used  to  describe  them  is  not  diverse.  We  introduce  StructuredRegex,  a  new  regex  synthesis  dataset  differing  from  prior  ones  in  three  aspects.  First,  to  obtain  structurally  complex  and  realistic  regexes,  we  generate  the  regexes  using  a  probabilistic  grammar  with  pre-defined  macros  observed  from  real-world  StackOverflow  posts.  Second,  to  obtain  linguistically  diverse  natural  language  descriptions,  we  show  crowdworkers  abstract  depictions  of  the  underlying  regex  and  ask  them  to  describe  the  pattern  they  see,  rather  than  having  them  paraphrase  synthetic  language.  Third,  we  augment  each  regex  example  with  a  collection  of  strings  that  are  and  are  not  matched  by  the  ground  truth  regex,  similar  to  how  real  users  give  examples.  Our  quantitative  and  qualitative  analysis  demonstrates  the  advantages  of  StructuredRegex  over  prior  datasets.  Further  experimental  results  using  various  multimodal  synthesis  techniques  highlight  the  challenge  presented  by  our  dataset,  including  non-local  constraints  and  multi-modal  inputs.
0	You  sound  just  like  your  father  commercial  machine  translation  systems  include  stylistic  biases.  The  main  goal  of  machine  translation  has  been  to  convey  the  correct  content.  Stylistic  considerations  have  been  at  best  secondary.  We  show  that  as  a  consequence,  the  output  of  three  commercial  machine  translation  systems  (Bing,  DeepL,  Google)  make  demographically  diverse  samples  from  five  languages  ``sound''  older  and  more  male  than  the  original.  Our  findings  suggest  that  translation  models  reflect  demographic  bias  in  the  training  data.  This  opens  up  interesting  new  research  avenues  in  machine  translation  to  take  stylistic  considerations  into  account.
0	Multi  input  attention  for  unsupervised  ocr  correction.  We  propose  a  novel  approach  to  OCR  post-correction  that  exploits  repeated  texts  in  large  corpora  both  as  a  source  of  noisy  target  outputs  for  unsupervised  training  and  as  a  source  of  evidence  when  decoding.  A  sequence-to-sequence  model  with  attention  is  applied  for  single-input  correction,  and  a  new  decoder  with  multi-input  attention  averaging  is  developed  to  search  for  consensus  among  multiple  sequences.  We  design  two  ways  of  training  the  correction  model  without  human  annotation,  either  training  to  match  noisily  observed  textual  variants  or  bootstrapping  from  a  uniform  error  model.  On  two  corpora  of  historical  newspapers  and  books,  we  show  that  these  unsupervised  techniques  cut  the  character  and  word  error  rates  nearly  in  half  on  single  inputs  and,  with  the  addition  of  multi-input  decoding,  can  rival  supervised  methods.
0	The  curse  of  dense  low  dimensional  information  retrieval  for  large  index  sizes.  Information  Retrieval  using  dense  low-dimensional  representations  recently  became  popular  and  showed  out-performance  to  traditional  sparse-representations  like  BM25.  However,  no  previous  work  investigated  how  dense  representations  perform  with  large  index  sizes.  We  show  theoretically  and  empirically  that  the  performance  for  dense  representations  decreases  quicker  than  sparse  representations  for  increasing  index  sizes.  In  extreme  cases,  this  can  even  lead  to  a  tipping  point  where  at  a  certain  index  size  sparse  representations  outperform  dense  representations.  We  show  that  this  behavior  is  tightly  connected  to  the  number  of  dimensions  of  the  representations:  The  lower  the  dimension,  the  higher  the  chance  for  false  positives,  i.e.  returning  irrelevant  documents
0	Automatic  generation  of  citation  texts  in  scholarly  papers  a  pilot  study.  In  this  paper,  we  study  the  challenging  problem  of  automatic  generation  of  citation  texts  in  scholarly  papers.  Given  the  context  of  a  citing  paper  A  and  a  cited  paper  B,  the  task  aims  to  generate  a  short  text  to  describe  B  in  the  given  context  of  A.  One  big  challenge  for  addressing  this  task  is  the  lack  of  training  data.  Usually,  explicit  citation  texts  are  easy  to  extract,  but  it  is  not  easy  to  extract  implicit  citation  texts  from  scholarly  papers.  We  thus  first  train  an  implicit  citation  extraction  model  based  on  BERT  and  leverage  the  model  to  construct  a  large  training  dataset  for  the  citation  text  generation  task.  Then  we  propose  and  train  a  multi-source  pointer-generator  network  with  cross  attention  mechanism  for  citation  text  generation.  Empirical  evaluation  results  on  a  manually  labeled  test  dataset  verify  the  efficacy  of  our  model.  This  pilot  study  confirms  the  feasibility  of  automatically  generating  citation  texts  in  scholarly  papers  and  the  technique  has  the  great  potential  to  help  researchers  prepare  their  scientific  papers.
0	The  approach  of  using  fractal  dimension  and  linguistic  descriptors  in  cbir.  In  this  paper  we  describe  a  system  which  uses  linguistic  expression  and  fractal  dimensions  to  retrieve  images  in  database.  Brodaz  texture  images  are  used  for  our  experiment.  In  addition,  Taruma  features  are  used  to  extract  images  of  the  database  and  several  linguistic  expressions  are  used  for  classified  the  images.  These  linguistic  expressions,  with  the  help  of  fractal  dimension  are  more  efficient  in  searching  images.  By  visual  inspection,  we  are  satisfied  with  our  experiment  results.
0	Traffic  signals  timing  cycle  length  learning  using  taxi  gps  trajectories.  Traffic  signals  timing  information  at  intersections,  including  cycle  length,  lengths  of  green  interval  and  red  interval  within  a  cycle,  has  great  values  in  auto-driving,  traffic  control  intelligent  and  various  other  intelligent  transportation  applications.  A  cycle  length  analysis  approach  based  on  GPS  trajectory  data  was  proposed  in  this  paper,  which  consists  of  three  stages:  vehicle  travelling  behavior  analysis  based  on  GPS  trajectories,  green  start  events  extraction,  and  cycle  length  estimation  based  on  sparse  observational  data.  The  proposed  approach  was  validated  by  a  set  of  taxi  GPS  trajectories  from  Nanjing.  The  experimental  results  shows  that  the  proposed  method  could  effectively  estimate  the  timing  cycle  length  of  traffic  signals  of  given  intersections.
0	Author  profiling  with  doc2vec  neural  network  based  document  embeddings.  To  determine  author  demographics  of  texts  in  social  media  such  as  Twitter,  blogs,  and  reviews,  we  use  doc2vec  document  embeddings  to  train  a  logistic  regression  classifier.  We  experimented  with  age  and  gender  identification  on  the  PAN  author  profiling  2014–2016  corpora  under  both  single-  and  cross-genre  conditions.  We  show  that  under  certain  settings  the  neural  network-based  features  outperform  the  traditional  features  when  using  the  same  classifier.  Our  method  outperforms  existing  state  of  the  art  under  some  settings,  though  the  current  state-of-the-art  results  on  those  tasks  have  been  quite  weak.
0	Protecting  elections  with  minimal  resource  consumption.  In  democratic  elections,  malicious  agents  may  attempt  to  control  elections  to  achieve  their  own  goals.  To  guarantee  impartiality,  it  is  necessary  to  protect  the  election  outcomes  from  control.  In  this  paper,  we  consider  how  to  protect  election  outcome  from  control  using  minimal  resources.  We  assume  malicious  agents  attempt  to  prevent  a  specific  candidate  from  winning  a  democratic  election  with  plurality  rule  through  denial-of-service  (deletion)  attacks  on  voter  groups  (e.g.,  polling  places).  First,  we  show  that  the  problem  is  NP-hard.  Second,  we  propose  a  (|C|-1)-approximation  algorithm  for  the  problem,  where  |C|  is  the  number  of  candidates.  Finally,  we  validate  the  efficiency  of  our  approximation  algorithm  based  on  simulation  experiments.
0	Team  formation  with  learning  agents  that  improve  coordination.  Learning  agents  increase  their  team's  performance  by  learning  to  coordinate  better  with  their  teammates,  and  we  are  interested  in  forming  teams  that  contain  such  learning  agents.  In  particular,  we  consider  finite  training  instances  for  learning  agents  to  improve  their  coordination  before  the  final  team  is  formed.  We  formally  define  the  learning  agents  team  formation  problem,  and  focus  on  learning  agent  pairs  that  improve  their  coordination.  Learning  agent  pairs  have  heterogeneous  rates  of  improving  coordination,  and  hence  the  allocation  of  training  instances  has  a  large  impact  on  the  performance  of  the  final  team.
0	Fairness  in  multiagent  resource  allocation  with  dynamic  and  partial  observations.  We  investigate  fairness  issues  in  distributed  resource  allocation  of  indivisible  goods.  More  specifically,  we  study  envy-freeness  in  a  setting  where  the  observations  of  agents  only  result  from  encounters  with  other  agents.  Agents  thus  have  a  partial  and  uncertain  view  of  the  entire  allocation,  that  they  maintain  throughout  the  process,  and  which  allows  them  to  have  different  estimates  of  their  envy.  We  provide  a  fully  distributed  protocol  allowing  to  guarantee  termination  despite  the  limited  knowledge  of  agents.
0	Finding  and  recognizing  popular  coalition  structures.  An  important  aspect  of  multi-agent  systems  concerns  the  formation  of  coalitions  that  are  stable  or  optimal  in  some  well-defined  way.  The  notion  of  popularity  has  recently  received  a  lot  of  attention  in  this  context.  A  partition  is  popular  if  there  is  no  other  partition  in  which  more  agents  are  better  off  than  worse  off.  In  2019,  a  long-standing  open  problem  concerning  popularity  was  solved  by  proving  that  computing  popular  partitions  in  roommate  games  is  NP-hard,  even  when  preferences  are  strict.  We  show  that  this  result  breaks  down  when  allowing  for  randomization:  mixed  popular  partitions  can  be  found  efficiently  via  linear  programming  and  a  separation  oracle.  Mixed  popular  partitions  are  particularly  attractive  because  they  are  guaranteed  to  exist  in  any  coalition  formation  game.  Our  result  implies  that  one  can  efficiently  verify  whether  a  given  partition  in  a  roommate  game  is  popular  and  that  strongly  popular  partitions  can  be  found  in  polynomial  time  (resolving  an  open  problem).  By  contrast,  we  prove  that  both  problems  become  computationally  intractable  when  moving  from  coalitions  of  size~2  to  coalitions  of  size~3,  even  when  preferences  are  strict  and  globally  ranked.  Moreover,  we  give  elaborate  proofs  showing  the  NP-hardness  of  finding  popular,  strongly  popular,  and  mixed  popular  partitions  in  additively  separable  hedonic  games  and  finding  popular  partitions  in  fractional  hedonic  games.
0	On  improving  mus  extraction  algorithms.  Minimally  Unsatisfiable  Subformulas  (MUS)  find  a  wide  range  of  practical  applications,  including  product  configuration,  knowledge-based  validation,  and  hardware  and  software  design  and  verification.  MUSes  also  find  application  in  recentMaximum  Satisfiability  algorithms  and  in  CNF  formula  redundancy  removal.  Besides  direct  applications  in  Propositional  Logic,  algorithms  for  MUS  extraction  have  been  applied  to  more  expressive  logics.  This  paper  proposes  two  algorithms  forMUS  extraction.  The  first  algorithm  is  optimal  in  its  class,  meaning  that  it  requires  the  smallest  number  of  calls  to  a  SAT  solver.  The  second  algorithm  extends  earlier  work,  but  implements  a  number  of  new  techniques.  The  resulting  algorithms  achieve  significant  performance  gains  with  respect  to  state  of  the  art  MUS  extraction  algorithms.
0	Coordinating  vessel  traffic  to  improve  safety  and  efficiency.  Global  increase  in  trade  leads  to  congestion  of  maritime  traffic  at  the  ports.  This  often  leads  to  increased  maritime  incidents  or  near-miss  situations.  To  improve  maritime  safety  while  maintaining  efficiency,  movement  of  vessels  needs  to  be  better  coordinated.  Our  work  formulates  this  problem  of  coordinating  the  paths  of  vessels  as  a  multi-agent  path-finding  (MAPF)  problem.  To  address  this  problem,  we  introduce  an  innovative  application  of  MAPF  in  the  maritime  domain  known  as  Vessel  Coordination  Module  (VCM).  Based  on  the  local  search  paradigm,  VCM  plans  on  a  joint  state  space  updated  using  the  Electronic  Navigation  Charts  (ENC)  and  the  paths  of  vessels.  We  introduce  the  notion  of  path  quality  that  measures  the  number  of  positions  on  a  vessel  path  that  is  too  close  to  some  other  vessels  spatially  and  temporally.  VCM  aims  to  improve  the  overall  path  quality  of  vessels  by  improving  path  quality  of  selected  vessels.  Experiments  are  conducted  on  the  Singapore  Straits  to  evaluate  and  compare  performance  of  our  proposed  approach  in  heterogeneous  maritime  scenario.  Our  experiment  results  show  that  VCM  can  improve  the  overall  path  quality  of  the  vessels.
0	An  evolutionary  approach  to  find  optimal  policies  with  an  agent  based  simulation.  In  this  paper,  we  introduce  a  new  agent-based  method  to  build  a  decision-aid  tool  aimed  to  improve  policy  design.  In  our  approach,  a  policy  is  defined  as  a  set  of  levers,  modelling  the  set  of  actions,  the  means  to  impact  a  complex  system.  Our  method  is  generic,  as  it  could  be  applied  to  any  domain,  and  be  coupled  with  any  agent-based  simulator.  We  could  deal  not  only  with  simple  levers  (a  single  variable  whose  value  is  modified)  but  also  complex  ones  (multiple  variable  modifications,  qualitative  effects,  ...),  unlike  most  optimization  methods.  It  is  based  on  the  evolutionary  algorithm  CMA-ES,  coupled  with  a  normalized  and  aggregated  fitness  function.  The  fitness  is  normalized  using  estimated  Ideal  (best  policy)  and  Nadir  (worst  policy)  values,  these  values  being  dynamically  computed  during  the  execution  of  CMA-ES  through  a  Pareto  Front  estimated  with  the  ABM  simulation.  Moreover,  to  deal  with  complex  levers,  we  introduce  the  FSM-branching  algorithm,  where  a  Finite  State  Machine  (FSM)  determines  whether  a  complex  policy  can  potentially  be  improved  or  has  to  be  aborted.  We  tested  our  method  with  Economic  Policies  on  the  French  Labor  Market  (FLM),  allowing  the  modification  of  multiple  elements  of  the  FLM,  and  we  compared  the  results  to  the  reference,  the  FLM  without  any  policy  applied.  The  policies  studied  here  comprise  simple  and  complex  levers.  This  experience  shows  the  viability  of  our  approach,  the  efficiency  of  our  algorithms  and  illustrates  how  this  combination  of  evolutionary  optimization,  multi-criteria  aggregation  and  agent-based  simulation  could  help  any  policy-maker  to  design  better  policies.
0	Coordinating  multi  agent  reinforcement  learning  with  limited  communication.  Coordinated  multi-agent  reinforcement  learning  (MARL)  provides  a  promising  approach  to  scaling  learning  in  large  cooperative  multi-agent  systems.  Distributed  constraint  optimization  (DCOP)  techniques  have  been  used  to  coordinate  action  selection  among  agents  during  both  the  learning  phase  and  the  policy  execution  phase  (if  learning  is  off-line)  to  ensure  good  overall  system  performance.  However,  running  DCOP  algorithms  for  each  action  selection  through  the  whole  system  results  in  significant  communication  among  agents,  which  is  not  practical  for  most  applications  with  limited  communication  bandwidth.  In  this  paper,  we  develop  a  learning  approach  that  generalizes  previous  coordinated  MARL  approaches  that  use  DCOP  algorithms  and  enables  MARL  to  be  conducted  over  a  spectrum  from  independent  learning  (without  communication)  to  fully  coordinated  learning  depending  on  agents'  communication  bandwidth.  Our  approach  defines  an  interaction  measure  that  allows  agents  to  dynamically  identify  their  beneficial  coordination  set  (i.e.,  whom  to  coordinate  with)  in  different  situations  and  to  trade  off  its  performance  and  communication  cost.  By  limiting  their  coordination  set,  agents  dynamically  decompose  the  coordination  network  in  a  distributed  way,  resulting  in  dramatically  reduced  communication  for  DCOP  algorithms  without  significantly  affecting  overall  learning  performance.  Essentially,  our  learning  approach  conducts  co-adaptation  of  agents'  policy  learning  and  coordination  set  identification,  which  outperforms  approaches  that  sequence  them.
0	Pac  bayesian  theory  for  transductive  learning.  We  propose  a  PAC-Bayesian  analysis  of  the  transductive  learning  setting,  introduced  by  Vapnik  [1998],  by  proposing  a  family  of  new  bounds  on  the  generalization  error.  Some  of  them  are  derived  from  their  counterpart  in  the  inductive  setting,  and  others  are  new.  We  also  compare  their  behavior.
0	Nonlinear  ica  using  auxiliary  variables  and  generalized  contrastive  learning.  Nonlinear  ICA  is  a  fundamental  problem  for  unsupervised  representation  learning,  emphasizing  the  capacity  to  recover  the  underlying  latent  variables  generating  the  data  (i.e.,  identifiability).  Recently,  the  very  first  identifiability  proofs  for  nonlinear  ICA  have  been  proposed,  leveraging  the  temporal  structure  of  the  independent  components.  Here,  we  propose  a  general  framework  for  nonlinear  ICA,  which,  as  a  special  case,  can  make  use  of  temporal  structure.  It  is  based  on  augmenting  the  data  by  an  auxiliary  variable,  such  as  the  time  index,  the  history  of  the  time  series,  or  any  other  available  information.  We  propose  to  learn  nonlinear  ICA  by  discriminating  between  true  augmented  data,  or  data  in  which  the  auxiliary  variable  has  been  randomized.  This  enables  the  framework  to  be  implemented  algorithmically  through  logistic  regression,  possibly  in  a  neural  network.  We  provide  a  comprehensive  proof  of  the  identifiability  of  the  model  as  well  as  the  consistency  of  our  estimation  method.  The  approach  not  only  provides  a  general  theoretical  framework  combining  and  generalizing  previously  proposed  nonlinear  ICA  models  and  algorithms,  but  also  brings  practical  advantages.
0	Bayesian  learning  of  neural  network  architectures.  In  this  paper  we  propose  a  Bayesian  method  for  estimating  architectural  parameters  of  neural  networks,  namely  layer  size  and  network  depth.  We  do  this  by  learning  concrete  distributions  over  these  parameters.  Our  results  show  that  regular  networks  with  a  learned  structure  can  generalise  better  on  small  datasets,  while  fully  stochastic  networks  can  be  more  robust  to  parameter  initialisation.  The  proposed  method  relies  on  standard  neural  variational  learning  and,  unlike  randomised  architecture  search,  does  not  require  a  retraining  of  the  model,  thus  keeping  the  computational  overhead  at  minimum.
0	Xbart  accelerated  bayesian  additive  regression  trees.  Bayesian  additive  regression  trees  (BART)  (Chipman  et.  al.,  2010)  is  a  powerful  predictive  model  that  often  outperforms  alternative  models  at  out-of-sample  prediction.  BART  is  especially  well-suited  to  settings  with  unstructured  predictor  variables  and  substantial  sources  of  unmeasured  variation  as  is  typical  in  the  social,  behavioral  and  health  sciences.  This  paper  develops  a  modified  version  of  BART  that  is  amenable  to  fast  posterior  estimation.  We  present  a  stochastic  hill  climbing  algorithm  that  matches  the  remarkable  predictive  accuracy  of  previous  BART  implementations,  but  is  many  times  faster  and  less  memory  intensive.  Simulation  studies  show  that  the  new  method  is  comparable  in  computation  time  and  more  accurate  at  function  estimation  than  both  random  forests  and  gradient  boosting.
0	Movement  segmentation  and  recognition  for  imitation  learning.  In  human  movement  learning,  it  is  most  common  to  teach  constituent  elements  of  complex  movements  in  isolation,  before  chaining  them  into  complex  movements.  Segmentation  and  recognition  of  observed  movement  could  thus  proceed  out  of  this  existing  knowledge,  which  is  directly  compatible  with  movement  generation.  In  this  paper,  we  address  exactly  this  scenario.  We  assume  that  a  library  of  movement  primitives  has  already  been  taught,  and  we  wish  to  identify  elements  of  the  library  in  a  complex  motor  act,  where  the  individual  elements  have  been  smoothed  together,  and,  occasionally,  there  might  be  a  movement  segment  that  is  not  in  our  library  yet.  We  employ  a  flexible  machine  learning  representation  of  movement  primitives  based  on  learnable  nonlinear  attractor  system.  For  the  purpose  of  movement  segmentation  and  recognition,  it  is  possible  to  reformulate  this  representation  as  a  controlled  linear  dynamical  system.  An  Expectation-Maximization  algorithm  can  be  developed  to  estimate  the  open  parameters  of  a  movement  primitive  from  the  library,  using  as  input  an  observed  trajectory  piece.  If  no  matching  primitive  from  the  library  can  be  found,  a  new  primitive  is  created.  This  process  allows  a  straightforward  sequential  segmentation  of  observed  movement  into  known  and  new  primitives,  which  are  suitable  for  robot  imitation  learning.  We  illustrate  our  approach  with  synthetic  examples  and  data  collected  from  human  movement.
0	Corruption  tolerant  gaussian  process  bandit  optimization.  We  consider  the  problem  of  optimizing  an  unknown  (typically  non-convex)  function  with  a  bounded  norm  in  some  Reproducing  Kernel  Hilbert  Space  (RKHS),  based  on  noisy  bandit  feedback.  We  consider  a  novel  variant  of  this  problem  in  which  the  point  evaluations  are  not  only  corrupted  by  random  noise,  but  also  adversarial  corruptions.  We  introduce  an  algorithm  Fast-Slow  GP-UCB  based  on  Gaussian  process  methods,  randomized  selection  between  two  instances  labeled  "fast"  (but  non-robust)  and  "slow"  (but  robust),  enlarged  confidence  bounds,  and  the  principle  of  optimism  under  uncertainty.  We  present  a  novel  theoretical  analysis  upper  bounding  the  cumulative  regret  in  terms  of  the  corruption  level,  the  time  horizon,  and  the  underlying  kernel,  and  we  argue  that  certain  dependencies  cannot  be  improved.  We  observe  that  distinct  algorithmic  ideas  are  required  depending  on  whether  one  is  required  to  perform  well  in  both  the  corrupted  and  non-corrupted  settings,  and  whether  the  corruption  level  is  known  or  not.
0	Sampling  adaptively  using  the  massart  inequality  for  scalable  learning.  With  the  advent  of  the  "big  data"  era,  the  data  mining  community  is  facing  an  increasingly  critical  problem  of  developing  scalable  algorithms  capable  of  mining  knowledge  from  massive  amount  of  data.  This  paper  develops  a  sampling-based  method  to  address  the  issue  of  scalability.  We  show  how  to  utilize  the  new,  adaptive  sampling  method  in  [4]  to  develop  a  scalable  learning  algorithm  by  boosting,  an  ensemble  learning  method.  We  present  experimental  results  using  bench-mark  data  sets  from  the  UC-Irvine  ML  data  repository  that  confirm  the  much  improved  efficiency  and  thus  scalability,  and  competitive  prediction  accuracy  of  the  new  adaptive  boosting  method,  in  comparison  with  existing  approaches.
0	Hmm  based  intrusion  detection  system  for  software  defined  networking.  Software  Defined  Networking  (SDN)  is  a  networking  model  that  allows  for  greater  dynamic  control  of  a  networking  environment.  With  today's  increasingly  complex  networking  environment,  SDN  networks  allow  for  a  greater  degree  of  control  and  flexibility  of  a  network.  This  is  accomplished  through  the  separation  of  the  control  and  data  planes,  as  well  as  the  implementation  of  a  global  programmable  controller.  A  Network  Intrusion  Detection  Systems  (NIDS)  can  work  very  well  with  SDN  networks  as  it  can  help  monitor  the  overall  security  of  a  network  by  analyzing  the  network  as  a  whole  and  making  choices  to  defend  the  network  based  on  data  from  the  entire  network.  Using  a  Hidden  Markov  Model  (HMM),  a  NIDS  could  monitor  a  network  and  learn  from  the  evolving  network  activity  of  the  present  and  react  accordingly.  This  machine-learning  NIDS  could  improve  the  efficiency  of  security  applications  and  increases  the  range  of  activities  that  they  are  able  to  accomplish.  In  this  paper  we  plan  to  demonstrate  the  possibility  of  using  Hidden  Markov  models  to  develop  an  adaptive  NIDS  for  use  in  the  new  emerging  technology  of  SDN.
0	A  machine  learning  pipeline  for  three  way  classification  of  alzheimer  patients  from  structural  magnetic  resonance  images  of  the  brain.  Magnetic  resonance  imaging  (MRI)  has  emerged  as  an  important  tool  to  identify  intermediate  biomarkers  of  Alzheimer's  disease  (AD)  due  to  its  ability  to  measure  regional  changes  in  the  brain  that  are  thought  to  reflect  disease  severity  and  progression.  In  this  paper,  we  set  out  a  novel  pipeline  that  uses  volumetric  MRI  data  collected  from  different  subjects  as  input  and  classifies  them  into  one  of  three  classes:  AD,  mild  cognitive  impairment  (MCI)  and  cognitively  normal  (CN).  Our  pipeline  consists  of  three  stages  --  (1)  a  segmentation  layer  where  brain  MRI  data  is  divided  into  clinically  relevant  regions,  (2)  a  classification  layer  that  uses  relational  learning  algorithms  to  make  pair  wise  predictions  between  the  three  classes,  and  (3)a  combination  layer  that  combines  the  results  of  the  different  classes  to  obtain  the  final  classification.  One  of  the  key  features  of  our  proposed  approach  is  that  it  allows  for  domain  expert's  knowledge  to  guide  the  learning  in  all  the  layers.  We  evaluate  our  pipeline  on  397  patients  acquired  from  the  Alzheimer's  Disease  Neuroimaging  Initiative  and  demonstrate  that  it  obtains  state-of  the-art  performance  with  minimal  feature  engineering.
0	Gradient  based  algorithms  for  the  automatic  construction  of  fuzzy  cognitive  maps.  Fuzzy  Cognitive  Map  (FCM)  is  a  tool  for  modeling  and  representing  discrete  dynamical  systems.  Several  approaches  were  proposed  for  the  automatic  learning  of  FCM  on  the  basis  of  historical  data.  The  learning  techniques  can  be  grouped  into  three  types:  Hebbian-based,  population-based,  and  hybrid,  which  combines  both  types.  Despite  the  good  overall  results  achieved  by  population-based  approaches  relative  to  the  other  learning  paradigms,  it  is  possible  to  improve  their  performance  by  combining  them  with  local  search  procedures.  In  this  paper,  we  investigate  the  performance  of  a  multi-start  gradient-based  method  and  two  evolutionary  methods  hybridized  with  a  gradient-based  local  search  procedure  for  the  learning  of  FCMs.  We  tested  the  proposed  approaches  for  synthetic  and  real  world  FCM  models.  The  results  show  that  it  was  possible  to  improve  the  performance  of  the  evolutionary  methods  with  a  relatively  small  increase  in  the  resultant  computational  time.
0	Incremental  open  set  intrusion  recognition  using  extreme  value  machine.  Typically,  most  network  intrusion  detection  systems  use  supervised  learning  techniques  to  identify  network  anomalies.  A  problem  exists  when  identifying  the  unknowns  and  automatically  updating  a  classifier  with  new  query  classes.  This  is  defined  as  an  open  set  incremental  learning  problem  and  we  propose  to  extend  a  recently  introduced  method,  the  Extreme  Value  Machine  (EVM)  to  address  the  issue  of  identifying  new  classes  during  query  time.  The  EVM  is  derived  from  the  statistical  extreme  value  theory  and  is  the  first  classifier  that  can  perform  kernel-free,  nonlinear,  variable  bandwidth  outlier  detection  combined  with  incremental  learning.  In  this  paper,  we  utilize  the  EVM  for  intrusion  detection  and  measure  the  open  set  recognition  performance  of  identifying  known  and  unknown  classes.  Additionally,  we  evaluate  the  performance  on  the  KDDCUP’99  dataset  and  compare  the  results  with  the  state-ofthe-  art  Weibull-SVM  (W-SVM).  Our  findings  demonstrate  that  the  EVM  mirrors  the  performance  of  the  W-SVM  classifier,  while  it  supports  incremental  learning.
0	Hierarchical  classification  approach  to  emotion  recognition  in  twitter.  Twitter  is  a  micro  logging  service  where  worldwide  users  publish  and  share  their  feelings.  However,  sentiment  analysis  for  Twitter  messages  ('tweets')  is  regarded  as  a  challenging  problem  because  tweets  are  short  and  informal.  In  this  paper,  we  apply  a  novel  approach  for  automatically  classifying  the  sentiment  and  emotions  of  Twitter  messages.  These  messages  are  hierarchically  categorized  on  basis  of  neutrality,  polarity  (positive  or  negative)  and  presence  of  various  emotions.  The  hierarchical  classification  approach  (HC)  is  a  specialization  of  the  well-known  flat  classification  task.  The  main  difference  between  them  is  that  when  using  HC,  examples  must  be  assigned  to  classes  organized  in  a  previously  defined  class  hierarchy,  while  traditional  flat  classification  does  not  take  into  account  the  hierarchical  information.  We  applied  our  model  to  posts  collected  from  Twitter  regarding  the  2011  season  of  the  Brazilian  Soccer  League.  Our  results  show  that  the  proposed  method  outperforms  the  corresponding  flat  approach  in  emotion  classification.
0	Understanding  fairness  of  gender  classification  algorithms  across  gender  race  groups.  Automated  gender  classification  has  important  applications  in  many  domains,  such  as  demographic  research,  law  enforcement,  online  advertising,  as  well  as  human-computer  interaction.  Recent  research  has  questioned  the  fairness  of  this  technology  across  gender  and  race.  Specifically,  the  majority  of  the  studies  raised  the  concern  of  higher  error  rates  of  the  face-based  gender  classification  system  for  darker-skinned  people  like  African-American  and  for  women.  However,  to  date,  the  majority  of  existing  studies  were  limited  to  African-American  and  Caucasian  only.  The  aim  of  this  paper  is  to  investigate  the  differential  performance  of  the  gender  classification  algorithms  across  gender-race  groups.  To  this  aim,  we  investigate  the  impact  of  (a)  architectural  differences  in  the  deep  learning  algorithms  and  (b)  training  set  imbalance,  as  a  potential  source  of  bias  causing  differential  performance  across  gender  and  race.  Experimental  investigations  are  conducted  on  two  latest  large-scale  publicly  available  facial  attribute  datasets,  namely,  UTKFace  and  FairFace.  The  experimental  results  suggested  that  the  algorithms  with  architectural  differences  varied  in  performance  with  consistency  towards  specific  gender-race  groups.  For  instance,  for  all  the  algorithms  used,  Black  females  (Black  race  in  general)  always  obtained  the  least  accuracy  rates.  Middle  Eastern  males  and  Latino  females  obtained  higher  accuracy  rates  most  of  the  time.  Training  set  imbalance  further  widens  the  gap  in  the  unequal  accuracy  rates  across  all  gender-race  groups.  Further  investigations  using  facial  landmarks  suggested  that  facial  morphological  differences  due  to  the  bone  structure  influenced  by  genetic  and  environmental  factors  could  be  the  cause  of  the  least  performance  of  Black  females  and  Black  race,  in  general.
0	Combined  convex  technique  on  delay  dependent  stability  for  delayed  neural  networks.  In  this  brief,  by  employing  an  improved  Lyapunov-Krasovskii  functional  (LKF)  and  combining  the  reciprocal  convex  technique  with  the  convex  one,  a  new  sufficient  condition  is  derived  to  guarantee  a  class  of  delayed  neural  networks  (DNNs)  to  be  globally  asymptotically  stable.  Since  some  previously  ignored  terms  can  be  considered  during  the  estimation  of  the  derivative  of  LKF,  a  less  conservative  stability  criterion  is  derived  in  the  forms  of  linear  matrix  inequalities,  whose  solvability  heavily  depends  on  the  information  of  addressed  DNNs.  Finally,  we  demonstrate  by  two  numerical  examples  that  our  results  reduce  the  conservatism  more  efficiently  than  some  currently  used  methods.
0	Retargeted  least  squares  regression  algorithm.  This  brief  presents  a  framework  of  retargeted  least  squares  regression  (ReLSR)  for  multicategory  classification.  The  core  idea  is  to  directly  learn  the  regression  targets  from  data  other  than  using  the  traditional  zero–one  matrix  as  regression  targets.  The  learned  target  matrix  can  guarantee  a  large  margin  constraint  for  the  requirement  of  correct  classification  for  each  data  point.  Compared  with  the  traditional  least  squares  regression  (LSR)  and  a  recently  proposed  discriminative  LSR  models,  ReLSR  is  much  more  accurate  in  measuring  the  classification  error  of  the  regression  model.  Furthermore,  ReLSR  is  a  single  and  compact  model,  hence  there  is  no  need  to  train  two-class  (binary)  machines  that  are  independent  of  each  other.  The  convex  optimization  problem  of  ReLSR  is  solved  elegantly  and  efficiently  with  an  alternating  procedure  including  regression  and  retargeting  as  substeps.  The  experimental  evaluation  over  a  range  of  databases  identifies  the  validity  of  our  method.
0	Neural  approximation  based  adaptive  control  for  a  class  of  nonlinear  nonstrict  feedback  discrete  time  systems.  In  this  paper,  an  adaptive  control  approach-based  neural  approximation  is  developed  for  a  class  of  uncertain  nonlinear  discrete-time  (DT)  systems.  The  main  characteristic  of  the  considered  systems  is  that  they  can  be  viewed  as  a  class  of  multi-input  multioutput  systems  in  the  nonstrict  feedback  structure.  The  similar  control  problem  of  this  class  of  systems  has  been  addressed  in  the  past,  but  it  focused  on  the  continuous-time  systems.  Due  to  the  complicacies  of  the  system  structure,  it  will  become  more  difficult  for  the  controller  design  and  the  stability  analysis.  To  stabilize  this  class  of  systems,  a  new  recursive  procedure  is  developed,  and  the  effect  caused  by  the  noncausal  problem  in  the  nonstrict  feedback  DT  structure  can  be  solved  using  a  semirecurrent  neural  approximation.  Based  on  the  Lyapunov  difference  approach,  it  is  proved  that  all  the  signals  of  the  closed-loop  system  are  semiglobal,  ultimately  uniformly  bounded,  and  a  good  tracking  performance  can  be  guaranteed.  The  feasibility  of  the  proposed  controllers  can  be  validated  by  setting  a  simulation  example.
0	Stabilization  of  second  order  memristive  neural  networks  with  mixed  time  delays  via  nonreduced  order.  In  this  brief,  we  investigate  a  class  of  second-order  memristive  neural  networks  (SMNNs)  with  mixed  time-varying  delays.  Based  on  nonsmooth  analysis,  the  Lyapunov  stability  theory,  and  adaptive  control  theory,  several  new  results  ensuring  global  stabilization  of  the  SMNNs  are  obtained.  In  addition,  compared  with  the  reduced-order  method  used  in  the  existing  research  studies,  we  consider  the  global  stabilization  directly  from  the  SMNNs  themselves  without  the  reduced-order  method.  Finally,  we  give  some  numerical  simulations  to  show  the  effectiveness  of  the  results.
0	Uttime  temporal  relation  classification  using  deep  syntactic  features.  In  this  paper,  we  present  a  system,  UTTime,  which  we  submitted  to  TempEval-3  for  Task  C:  Annotating  temporal  relations.  The  system  uses  logistic  regression  classifiers  and  exploits  features  extracted  from  a  deep  syntactic  parser,  including  paths  between  event  words  in  phrase  structure  trees  and  their  path  lengths,  and  paths  between  event  words  in  predicateargument  structures  and  their  subgraphs.  UTTime  achieved  an  F1  score  of  34.9  based  on  the  graphed-based  evaluation  for  Task  C  (ranked  2  nd  )  and  56.45  for  Task  C-relationonly  (ranked  1  st  )  in  the  TempEval-3  evaluation.
0	Nonparametric  coupled  bayesian  dictionary  and  classifier  learning  for  hyperspectral  classification.  We  present  a  principled  approach  to  learn  a  discriminative  dictionary  along  a  linear  classifier  for  hyperspectral  classification.  Our  approach  places  Gaussian  Process  priors  over  the  dictionary  to  account  for  the  relative  smoothness  of  the  natural  spectra,  whereas  the  classifier  parameters  are  sampled  from  multivariate  Gaussians.  We  employ  two  Beta-Bernoulli  processes  to  jointly  infer  the  dictionary  and  the  classifier.  These  processes  are  coupled  under  the  same  sets  of  Bernoulli  distributions.  In  our  approach,  these  distributions  signify  the  frequency  of  the  dictionary  atom  usage  in  representing  class-specific  training  spectra,  which  also  makes  the  dictionary  discriminative.  Due  to  the  coupling  between  the  dictionary  and  the  classifier,  the  popularity  of  the  atoms  for  representing  different  classes  gets  encoded  into  the  classifier.  This  helps  in  predicting  the  class  labels  of  test  spectra  that  are  first  represented  over  the  dictionary  by  solving  a  simultaneous  sparse  optimization  problem.  The  labels  of  the  spectra  are  predicted  by  feeding  the  resulting  representations  to  the  classifier.  Our  approach  exploits  the  nonparametric  Bayesian  framework  to  automatically  infer  the  dictionary  size—the  key  parameter  in  discriminative  dictionary  learning.  Moreover,  it  also  has  the  desirable  property  of  adaptively  learning  the  association  between  the  dictionary  atoms  and  the  class  labels  by  itself.  We  use  Gibbs  sampling  to  infer  the  posterior  probability  distributions  over  the  dictionary  and  the  classifier  under  the  proposed  model,  for  which,  we  derive  analytical  expressions.  To  establish  the  effectiveness  of  our  approach,  we  test  it  on  benchmark  hyperspectral  images.  The  classification  performance  is  compared  with  the  state-of-the-art  dictionary  learning-based  classification  methods.
0	A  fast  algorithm  for  nonnegative  matrix  factorization  and  its  convergence.  Nonnegative  matrix  factorization  (NMF)  has  recently  become  a  very  popular  unsupervised  learning  method  because  of  its  representational  properties  of  factors  and  simple  multiplicative  update  algorithms  for  solving  the  NMF.  However,  for  the  common  NMF  approach  of  minimizing  the  Euclidean  distance  between  approximate  and  true  values,  the  convergence  of  multiplicative  update  algorithms  has  not  been  well  resolved.  This  paper  first  discusses  the  convergence  of  existing  multiplicative  update  algorithms.  We  then  propose  a  new  multiplicative  update  algorithm  for  minimizing  the  Euclidean  distance  between  approximate  and  true  values.  Based  on  the  optimization  principle  and  the  auxiliary  function  method,  we  prove  that  our  new  algorithm  not  only  converges  to  a  stationary  point,  but  also  does  faster  than  existing  ones.  To  verify  our  theoretical  results,  the  experiments  on  three  data  sets  have  been  conducted  by  comparing  our  proposed  algorithm  with  other  existing  methods.
0	A  maximally  split  and  relaxed  admm  for  regularized  extreme  learning  machines.  One  of  the  salient  features  of  the  extreme  learning  machine  (ELM)  is  its  fast  learning  speed.  However,  in  a  big  data  environment,  the  ELM  still  suffers  from  an  overly  heavy  computational  load  due  to  the  high  dimensionality  and  the  large  amount  of  data.  Using  the  alternating  direction  method  of  multipliers  (ADMM),  a  convex  model  fitting  problem  can  be  split  into  a  set  of  concurrently  executable  subproblems,  each  with  just  a  subset  of  model  coefficients.  By  maximally  splitting  across  the  coefficients  and  incorporating  a  novel  relaxation  technique,  a  maximally  split  and  relaxed  ADMM  (MS-RADMM),  along  with  a  scalarwise  implementation,  is  developed  for  the  regularized  ELM  (RELM).  The  convergence  conditions  and  the  convergence  rate  of  the  MS-RADMM  are  established,  which  exhibits  linear  convergence  with  a  smaller  convergence  ratio  than  the  unrelaxed  maximally  split  ADMM.  The  optimal  parameter  values  of  the  MS-RADMM  are  obtained  and  a  fast  parameter  selection  scheme  is  provided.  Experiments  on  ten  benchmark  classification  data  sets  are  conducted,  the  results  of  which  demonstrate  the  fast  convergence  and  parallelism  of  the  MS-RADMM.  Complexity  comparisons  with  the  matrix-inversion-based  method  in  terms  of  the  numbers  of  multiplication  and  addition  operations,  the  computation  time  and  the  number  of  memory  cells  are  provided  for  performance  evaluation  of  the  MS-RADMM.
0	Reachable  set  estimation  for  neural  network  control  systems  a  simulation  guided  approach.  The  vulnerability  of  artificial  intelligence  (AI)  and  machine  learning  (ML)  against  adversarial  disturbances  and  attacks  significantly  restricts  their  applicability  in  safety-critical  systems  including  cyber-physical  systems  (CPS)  equipped  with  neural  network  components  at  various  stages  of  sensing  and  control.  This  article  addresses  the  reachable  set  estimation  and  safety  verification  problems  for  dynamical  systems  embedded  with  neural  network  components  serving  as  feedback  controllers.  The  closed-loop  system  can  be  abstracted  in  the  form  of  a  continuous-time  sampled-data  system  under  the  control  of  a  neural  network  controller.  First,  a  novel  reachable  set  computation  method  in  adaptation  to  simulations  generated  out  of  neural  networks  is  developed.  The  reachability  analysis  of  a  class  of  feedforward  neural  networks  called  multilayer  perceptrons  (MLPs)  with  general  activation  functions  is  performed  in  the  framework  of  interval  arithmetic.  Then,  in  combination  with  reachability  methods  developed  for  various  dynamical  system  classes  modeled  by  ordinary  differential  equations,  a  recursive  algorithm  is  developed  for  over-approximating  the  reachable  set  of  the  closed-loop  system.  The  safety  verification  for  neural  network  control  systems  can  be  performed  by  examining  the  emptiness  of  the  intersection  between  the  over-approximation  of  reachable  sets  and  unsafe  sets.  The  effectiveness  of  the  proposed  approach  has  been  validated  with  evaluations  on  a  robotic  arm  model  and  an  adaptive  cruise  control  system.
0	Underestimation  assisted  global  local  cooperative  differential  evolution  and  the  application  to  protein  structure  prediction.  Various  mutation  strategies  show  distinct  advantages  in  differential  evolution  (DE).  The  cooperation  of  multiple  strategies  in  the  evolutionary  process  may  be  effective.  This  article  presents  an  underestimation-assisted  global  and  local  cooperative  DE  to  simultaneously  enhance  the  effectiveness  and  efficiency.  In  the  proposed  algorithm,  two  phases,  namely,  the  global  exploration  and  the  local  exploitation,  are  performed  in  each  generation.  In  the  global  phase,  a  set  of  trial  vectors  is  produced  for  each  target  individual  by  employing  multiple  strategies  with  strong  exploration  capability.  Afterward,  an  adaptive  underestimation  model  with  an  self-adapted  slope  control  parameter  is  proposed  to  evaluate  these  trial  vectors,  the  best  of  which  is  selected  as  the  candidate.  In  the  local  phase,  the  better-based  strategies  guided  by  individuals  that  are  better  than  the  target  individual  are  designed.  For  each  individual  accepted  in  the  global  phase,  multiple  trial  vectors  are  generated  by  using  these  strategies  and  filtered  by  the  underestimation  value.  The  cooperation  between  the  global  and  local  phases  includes  two  aspects.  First,  both  of  them  concentrate  on  generating  better  individuals  for  the  next  generation.  Second,  the  global  phase  aims  to  locate  promising  regions  quickly  while  the  local  phase  serves  as  a  local  search  for  enhancing  convergence.  Moreover,  a  simple  mechanism  is  designed  to  determine  the  parameter  of  DE  adaptively  in  the  searching  process.  Finally,  the  proposed  approach  is  applied  to  predict  the  protein  3-D  structure.  The  experimental  studies  on  classical  benchmark  functions,  CEC  test  sets,  and  protein  structure  prediction  problem  show  that  the  proposed  approach  is  superior  to  the  competitors.
0	Synchronization  control  for  nonlinear  stochastic  dynamical  networks  pinning  impulsive  strategy.  In  this  paper,  a  new  control  strategy  is  proposed  for  the  synchronization  of  stochastic  dynamical  networks  with  nonlinear  coupling.  Pinning  state  feedback  controllers  have  been  proved  to  be  effective  for  synchronization  control  of  state-coupled  dynamical  networks.  We  will  show  that  pinning  impulsive  controllers  are  also  effective  for  synchronization  control  of  the  above  mentioned  dynamical  networks.  Some  generic  mean  square  stability  criteria  are  derived  in  terms  of  algebraic  conditions,  which  guarantee  that  the  whole  state-coupled  dynamical  network  can  be  forced  to  some  desired  trajectory  by  placing  impulsive  controllers  on  a  small  fraction  of  nodes.  An  effective  method  is  given  to  select  the  nodes  which  should  be  controlled  at  each  impulsive  constants.  The  proportion  of  the  controlled  nodes  guaranteeing  the  stability  is  explicitly  obtained,  and  the  synchronization  region  is  also  derived  and  clearly  plotted.  Numerical  simulations  are  exploited  to  demonstrate  the  effectiveness  of  the  pinning  impulsive  strategy  proposed  in  this  paper.
0	Neural  adaptive  control  of  single  master  multiple  slaves  teleoperation  for  coordinated  multiple  mobile  manipulators  with  time  varying  communication  delays  and  input  uncertainties.  In  this  paper,  adaptive  neural  network  control  is  investigated  for  single-master-multiple-slaves  teleoperation  in  consideration  of  time  delays  and  input  dead-zone  uncertainties  for  multiple  mobile  manipulators  carrying  a  common  object  in  a  cooperative  manner.  Firstly,  concise  dynamics  of  teleoperation  systems  consisting  of  a  single  master  robot,  multiple  coordinated  slave  robots,  and  the  object  are  developed  in  the  task  space.  To  handle  asymmetric  time-varying  delays  in  communication  channels  and  unknown  asymmetric  input  dead  zones,  the  nonlinear  dynamics  of  the  teleoperation  system  are  transformed  into  two  subsystems  through  feedback  linearization:  local  master  or  slave  dynamics  including  the  unknown  input  dead  zones  and  delayed  dynamics  for  the  purpose  of  synchronization.  Then,  a  model  reference  neural  network  control  strategy  based  on  linear  matrix  inequalities  (LMI)  and  adaptive  techniques  is  proposed.  The  developed  control  approach  ensures  that  the  defined  tracking  errors  converge  to  zero  whereas  the  coordination  internal  force  errors  remain  bounded  and  can  be  made  arbitrarily  small.  Throughout  this  paper,  stability  analysis  is  performed  via  explicit  Lyapunov  techniques  under  specific  LMI  conditions.  The  proposed  adaptive  neural  network  control  scheme  is  robust  against  motion  disturbances,  parametric  uncertainties,  time-varying  delays,  and  input  dead  zones,  which  is  validated  by  simulation  studies.
0	Learning  a  no  reference  quality  assessment  model  of  enhanced  images  with  big  data.  In  this  paper,  we  investigate  into  the  problem  of  image  quality  assessment  (IQA)  and  enhancement  via  machine  learning.  This  issue  has  long  attracted  a  wide  range  of  attention  in  computational  intelligence  and  image  processing  communities,  since,  for  many  practical  applications,  e.g.,  object  detection  and  recognition,  raw  images  are  usually  needed  to  be  appropriately  enhanced  to  raise  the  visual  quality  (e.g.,  visibility  and  contrast).  In  fact,  proper  enhancement  can  noticeably  improve  the  quality  of  input  images,  even  better  than  originally  captured  images,  which  are  generally  thought  to  be  of  the  best  quality.  In  this  paper,  we  present  two  most  important  contributions.  The  first  contribution  is  to  develop  a  new  no-reference  (NR)  IQA  model.  Given  an  image,  our  quality  measure  first  extracts  17  features  through  analysis  of  contrast,  sharpness,  brightness  and  more,  and  then  yields  a  measure  of  visual  quality  using  a  regression  module,  which  is  learned  with  big-data  training  samples  that  are  much  bigger  than  the  size  of  relevant  image  data  sets.  The  results  of  experiments  on  nine  data  sets  validate  the  superiority  and  efficiency  of  our  blind  metric  compared  with  typical  state-of-the-art  full-reference,  reduced-reference  and  NA  IQA  methods.  The  second  contribution  is  that  a  robust  image  enhancement  framework  is  established  based  on  quality  optimization.  For  an  input  image,  by  the  guidance  of  the  proposed  NR-IQA  measure,  we  conduct  histogram  modification  to  successively  rectify  image  brightness  and  contrast  to  a  proper  level.  Thorough  tests  demonstrate  that  our  framework  can  well  enhance  natural  images,  low-contrast  images,  low-light  images,  and  dehazed  images.  The  source  code  will  be  released  at  https://sites.google.com/site/guke198701/publications  .
0	Semisupervised  multitask  learning  with  gaussian  processes.  We  present  a  probabilistic  framework  for  transferring  learning  across  tasks  and  between  labeled  and  unlabeled  data.  The  approach  is  based  on  Gaussian  process  (GP)  prediction  and  incorporates  both  the  geometry  of  the  data  and  the  similarity  between  tasks  within  a  GP  covariance,  allowing  Bayesian  prediction  in  a  natural  way.  We  discuss  the  transfer  of  learning  in  a  multitask  scenario  in  the  two  cases  where  the  underlying  geometry  is  assumed  to  be  the  same  across  tasks  and  where  different  tasks  are  assumed  to  have  independent  geometric  structures.  We  demonstrate  the  method  on  a  number  of  real  datasets,  indicating  that  the  semisupervised  multitask  approach  can  result  in  very  significant  improvements  in  performance  when  very  few  labeled  training  examples  are  available.
0	Towards  dynamically  crafted  affective  experiences  through  emotional  response  player  modelling.  Designing  adaptive  games  for  individual  emotional  experiences  is  an  elaborate  task  due  to  the  necessary  dedicated  hardware,  complex  signal  processing  and  unique  emotional  bonds  each  player  may  forge  with  the  game.  The  research  work  I  propose  focuses  on  developing  a  novel  affective  interaction  scheme  where  players’  emotional  reactions  are  learned  by  a  computational  model  that  leverages  this  information  to  influence  their  future  emotional  states.  To  this  end  I  aim  at  interpreting  the  causality  relations  between  physiologically  measured  emotional  response  variations  and  their  eliciting  events.  I  then  plan  using  these  reactions  to  build  each  player’s  individual  emotional  reaction  model,  which  will  guide  the  game  engine’s  logic  on  how  to  plan  future  game  events,  in  order  to  elicit  a  target  affective  experience.  Ultimately,  I  expect  this  technique  to  allow  game  designers  to  focus  on  defining  high-level  rules  for  generating  their  envisioned  affective  gameplay  experience,  rather  than  in  manually  tuning  it  in  a  repetitive  and  time-consuming  fashion.
0	Towards  large  scale  data  annotation  of  audio  from  wearables  validating  zooniverse  annotations  of  infant  vocalization  types.  Recent  developments  allow  the  collection  of  audio  data  from  lightweight  wearable  devices,  potentially  enabling  us  to  study  language  use  from  everyday  life  samples.  However,  extracting  useful  information  from  these  data  is  currently  impossible  with  automatized  routines,  and  overly  expensive  with  trained  human  annotators.  We  explore  a  strategy  fit  to  the  21st  century,  relying  on  the  collaboration  of  citizen  scientists.  A  large  dataset  of  infant  speech  was  uploaded  on  a  citizen  science  platform.  The  same  data  were  annotated  in  the  laboratory  by  highly  trained  annotators.  We  investigate  whether  crowd-sourced  annotations  are  qualitatively  and  quantitatively  com-parable  to  those  produced  by  expert  annotators  in  a  dataset  of  children  at  high-  and  low-risk  for  language  disorders.  Our  results  reveal  that  classification  of  individual  vocalizations  on  Zooniverse  was  overall  moderately  accurate  compared  to  the  laboratory  gold  standard.  The  analysis  of  descriptors  defined  at  the  level  of  individual  children  found  strong  correlations  between  descriptors  derived  from  Zooniverse  versus  laboratory  annotations.
0	Comparison  of  parsing  algorithms  for  recognizing  online  handwritten  mathematical  expressions.  Parsing  process  is  the  most  important  process  in  recognition  of  online  handwritten  mathematical  expressions.  There  are  two  basic  approaches:  stroke  order  dependent  (SOD)  and  stroke  order  free  (SOF)  approaches.  The  SOD  approach  depends  on  stroke  order  while  the  SOF  approach  is  free  from  stroke  order.  Although  both  approaches  have  shown  high  recognition  rates  in  recently  competitions,  there  are  a  few  of  works  that  analyze  the  complexities  of  parsing  algorithms  in  the  same  experimental  conditions.  In  this  work,  we  have  tested  and  analyzed  recognition  rate,  recognition  speed  and  memory  space  required  by  parsing  algorithms  on  CROHME  2014.  SOF  is  slightly  superior  to  SOD  in  recognition  rate,  but  SOD  is  faster  in  processing  time  and  lower  in  memory  space  than  SOF.
0	Online  anomaly  detection  with  constant  false  alarm  rate.  We  propose  a  computationally  highly  scalable  online  anomaly  detection  algorithm  for  time  series,  which  achieves  with  no  parameter  tuning  a  specified  false  alarm  rate  while  minimizing  the  miss  rate.  The  proposed  algorithm  sequentially  operates  on  a  fast  streaming  temporal  data,  extracts  the  nominal  attributes  under  possibly  varying  Markov  statistics  and  then  declares  an  anomaly  when  the  observations  are  statistically  sufficiently  deviant.  Regardless  of  whether  the  source  is  stationary  or  non-stationary,  our  algorithm  is  guaranteed  to  closely  achieve  the  desired  false  alarm  rates  at  negligible  computational  costs.  In  this  regard,  the  proposed  algorithm  is  highly  novel  and  appropriate  especially  for  big  data  applications.  Through  the  presented  simulations,  we  demonstrate  that  our  algorithm  outperforms  its  competitor,  i.e.,  the  Neyman-Pearson  test  that  relies  on  the  Monte  Carlo  trials,  even  in  the  case  of  strong  non-stationarity.
0	Dictionary  of  gray  level  3d  patches  for  action  recognition.  Sparse  dictionary-based  methods  have  received  a  lot  of  focus  in  the  recent  years.  In  this  paper,  we  are  interested  in  action  recognition  in  videos  using  a  3D  representation  based  on  the  use  of  a  dictionary.  A  3D  volume  can  be  seen  as  a  set  of  gray-level  3D  patches  which  are  2D  patches  taken  in  successive  frames  in  order  to  capture  a  motion  pattern.  The  goal  is  to  recognize  human  actions  within  these  3D  volumes,  whose  3D  patches  are  described  with  the  dictionary  atoms.  A  motion  signature  is  computed  by  building  a  histogram  based  on  the  use  of  the  atoms  of  the  dictionary.  Paired  with  a  SVM,  we  show  that  these  signatures  can  be  exploited  in  the  context  of  action  recognition.  This  method  was  tested  on  the  KTH  database  with  good  results.
0	Constructing  energy  efficient  mixed  precision  neural  networks  through  principal  component  analysis  for  edge  intelligence.  The  ‘Internet  of  Things’  has  brought  increased  demand  for  artificial  intelligence-based  edge  computing  in  applications  ranging  from  healthcare  monitoring  systems  to  autonomous  vehicles.  Quantization  is  a  powerful  tool  to  address  the  growing  computational  cost  of  such  applications  and  yields  significant  compression  over  full-precision  networks.  However,  quantization  can  result  in  substantial  loss  of  performance  for  complex  image  classification  tasks.  To  address  this,  we  propose  a  principal  component  analysis  (PCA)-driven  methodology  to  identify  the  important  layers  of  a  binary  network,  and  design  mixed-precision  networks.  The  proposed  Hybrid-Net  achieves  a  more  than  10%  improvement  in  classification  accuracy  over  binary  networks  such  as  XNOR-Net  for  ResNet  and  VGG  architectures  on  CIFAR-100  and  ImageNet  datasets,  while  still  achieving  up  to  94%  of  the  energy  efficiency  of  XNOR-Nets.  This  work  advances  the  feasibility  of  using  highly  compressed  neural  networks  for  energy-efficient  neural  computing  in  edge  devices.  Neural  networks  are  often  implemented  with  reduced  precision  in  order  to  meet  the  tight  energy  and  memory  budget  required  by  edge  computing  devices.  Chakraborty  et  al.  develop  a  technique  for  assessing  which  layers  can  be  quantized,  and  by  how  much,  without  sacrificing  too  much  on  performance.
0	Propa  l  a  semantic  filtering  service  from  a  lexical  network  created  using  games  with  a  purpose.  This  article  presents  Propa-L,  a  freely  accessible  Web  service  that  allows  to  semantically  filter  a  lexical  network.  The  language  resources  behind  the  service  are  dynamic  and  created  through  Games  With  A  Purpose.  We  show  an  example  of  application  of  this  service:  the  generation  of  a  list  of  keywords  for  parental  filtering  on  the  Web,  but  many  others  can  be  envisaged.  Moreover,  the  propagation  algorithm  we  present  here  can  be  applied  to  any  lexical  network,  in  any  language.
0	A  multi  party  multi  modal  dataset  for  focus  of  visual  attention  in  human  human  and  human  robot  interaction.  A  multi-party  multi-modal  dataset  for  focus  of  visual  attention  in  human-human  and  human-robot  interaction
0	Using  resource  rich  languages  to  improve  morphological  analysis  of  under  resourced  languages.  The  world-wide  proliferation  of  digital  communications  has  created  the  need  for  language  and  speech  processing  systems  for  underresourced  languages.  Developing  such  systems  is  challenging  if  only  small  data  sets  are  available,  and  the  problem  is  exacerbated  for  languages  with  highly  productive  morphology.  However,  many  under-resourced  languages  are  spoken  in  multi-lingual  environments  together  with  at  least  one  resource-rich  language  and  thus  have  numerous  borrowings  from  resource-rich  languages.  Based  on  this  insight,  we  argue  that  readily  available  resources  from  resource-rich  languages  can  be  used  to  bootstrap  the  morphological  analyses  of  under-resourced  language  with  complex  and  productive  morphological  systems.  In  a  case  study  of  two  such  languages,  Tagalog  and  Zulu,  we  show  that  an  easily  obtainable  English  wordlist  can  be  deployed  to  seed  a  morphological  analysis  algorithm  from  a  small  training  set  of  conversational  transcripts.  Our  method  achieves  a  precision  of  100%  and  identifies  28  and  66  of  the  most  productive  affixes  in  Tagalog  and  Zulu,  respectively.
0	On  paraphrase  identification  corpora.  We  analyze  in  this  paper  a  number  of  data  sets  proposed  over  the  last  decade  or  so  for  the  task  of  paraphrase  identification.  The  goal  of  the  analysis  is  to  identify  the  advantages  as  well  as  shortcomings  of  the  previously  proposed  data  sets.  Based  on  the  analysis,  we  then  make  recommendations  about  how  to  improve  the  process  of  creating  and  using  such  data  sets  for  evaluating  in  the  future  approaches  to  the  task  of  paraphrase  identification  or  the  more  general  task  of  semantic  similarity.  The  recommendations  are  meant  to  improve  our  understanding  of  what  a  paraphrase  is,  offer  a  more  fair  ground  for  comparing  approaches,  increase  the  diversity  of  actual  linguistic  phenomena  that  future  data  sets  will  cover,  and  offer  ways  to  improve  our  understanding  of  the  contributions  of  various  modules  or  approaches  proposed  for  solving  the  task  of  paraphrase  identification  or  similar  tasks.  We  also  developed  a  data  collection  tool,  called  Data  Collector,  that  proactively  targets  the  collection  of  paraphrase  instances  covering  linguistic  phenomena  important  to  paraphrasing.
0	The  making  of  ancient  greek  wordnet.  This  paper  describes  the  process  of  creation  and  review  of  a  new  lexico-semantic  resource  for  the  classical  studies:  AncientGreekWordNet.  The  candidate  sets  of  synonyms  (synsets)  are  extracted  from  Greek-English  dictionaries,  on  the  assumption  that  Greek  words  translated  by  the  same  English  word  or  phrase  have  a  high  probability  of  being  synonyms  or  at  least  semantically  closely  related.  The  process  of  validation  and  the  web  interface  developed  to  edit  and  query  the  resource  are  described  in  detail.  The  lexical  coverage  of  Ancient  Greek  WordNet  is  illustrated  and  the  accuracy  is  evaluated.  Finally,  scenarios  for  exploiting  the  resource  are  discussed.
0	Automatic  lexical  semantic  classification  of  nouns.  The  work  we  present  here  addresses  cue-based  noun  classification  in  English  and  Spanish.  Its  main  objective  is  to  automatically  acquire  lexical  semantic  information  by  classifying  nouns  into  previously  known  noun  lexical  classes.  This  is  achieved  by  using  particular  aspects  of  linguistic  contexts  as  cues  that  identify  a  specific  lexical  class.  Here  we  concentrate  on  the  task  of  identifying  such  cues  and  the  theoretical  background  that  allows  for  an  assessment  of  the  complexity  of  the  task.  The  results  show  that,  despite  of  the  a-priori  complexity  of  the  task,  cue-based  classification  is  a  useful  tool  in  the  automatic  acquisition  of  lexical  semantic  classes.
0	Measuring  the  compositionality  of  nv  expressions  in  basque  by  means  of  distributional  similarity  techniques.  We  present  several  experiments  aiming  at  measuring  the  semantic  compositionality  of  NV  expressions  in  Basque.  Our  approach  is  based  on  the  hypothesis  that  compositionality  can  be  related  to  distributional  similarity.  The  contexts  of  each  NV  expression  are  compared  with  the  contexts  of  its  corresponding  components,  by  means  of  different  techniques,  as  similarity  measures  usually  used  with  the  Vector  Space  Model  (VSM),  Latent  Semantic  Analysis  (LSA)  and  some  measures  implemented  in  the  Lemur  Toolkit,  as  Indri  index,  tf-idf,  Okapi  index  and  Kullback-Leibler  divergence.  Using  our  previous  work  with  cooccurrence  techniques  as  a  baseline,  the  results  point  to  improvements  using  the  Indri  index  or  Kullback-Leibler  divergence,  and  a  slight  further  improvement  when  used  in  combination  with  cooccurrence  measures  such  as  t-score,  via  rank-aggregation.  This  work  is  part  of  a  project  for  MWE  extraction  and  characterization  using  different  techniques  aiming  at  measuring  the  properties  related  to  idiomaticity,  as  institutionalization,  non-compositionality  and  lexico-syntactic  fixedness.
0	Eprobe  an  efficient  subspace  probing  framework.  In  multi-dimensional  data,  Promotional  Subspace  Mining  (PSM)  aims  to  find  out  the  most  promotive  subspaces  for  a  given  object.  One  major  research  issue  is  to  produce  top  subspaces  efficiently  given  a  predefined  subspace  ranking  measure.  In  this  paper,  we  propose  EProbe,  an  Efficient  Subspace  Probing  framework.  As  opposed  to  commonly  adopted  strategies  that  produce  an  exact  solution,  this  novel  framework  aims  at  providing  a  scalable,  cost-effective,  and  flexible  solution  where  its  accuracy  can  be  traded  with  the  efficiency  using  adjustable  parameters.  As  a  first  effort  to  implement  this  framework,  we  propose  two  novel  algorithms  SRatio  and  Sliding  Cluster.  The  former  applies  score  ratio  (SR)  based  subspace  sorting  to  obtain  a  sorted  subspace  set.  The  latter  further  includes  the  design  of  subspace  sampling  from  sliding  subspace  clusters.  The  ultimate  objective  of  both  algorithms  is  to  achieve  an  âearly  stopâ  of  the  subspace  search,  when  a  certain  number  of  top  subspaces  have  been  probed  and  evaluated.  We  propose  two  evaluation  metrics:  AVG  Trace  Index  and  Coverage,  which  favor  the  situation  where  important  subspaces  are  captured  within  the  subspace  evaluation  limit.  By  comparing  SRatio  and  Sliding  Cluster  with  a  baseline  algorithm  DFP  (Depth  First  Parent  Subspace  Pruning),  we  show  a  remarkable  superiority  of  algorithm  SRatio  (Sliding  Cluster  with  w  =  1)  over  DFP,  and  a  consistent  and  significant  improvement  of  algorithm  Sliding  Cluster  over  SRatio,  when  the  computation  resources  are  insufficient  and  only  a  limited  number  of  candidate  subspaces  can  be  evaluated.
0	A  multi  objective  genetic  algorithm  for  pruning  support  vector  machines.  Support  vector  machines  (SVMs)  often  contain  a  large  number  of  support  vectors  which  reduce  the  run-time  speeds  of  decision  functions.  In  addition,  this  might  cause  an  over  fitting  effect  where  the  resulting  SVM  adapts  itself  to  the  noise  in  the  training  set  rather  than  the  true  underlying  data  distribution  and  will  probably  fail  to  correctly  classify  unseen  examples.  To  obtain  more  fast  and  accurate  SVMs,  many  methods  have  been  proposed  to  prune  SVs  in  trained  SVMs.  In  this  paper,  we  propose  a  multi-objective  genetic  algorithm  to  reduce  the  complexity  of  support  vector  machines  as  well  as  to  improve  generalization  accuracy  by  the  reduction  of  over  fitting.  Experiments  on  four  benchmark  datasets  show  that  the  proposed  evolutionary  approach  can  effectively  reduce  the  number  of  support  vectors  included  in  the  decision  functions  of  SVMs  without  sacrificing  their  classification  accuracy.
0	Iterative  sat  solving  for  minimum  satisfiability.  Minimum  Satisfiability  (MinSAT)  denotes  one  of  the  optimization  versions  of  the  Boolean  Satisfiability  (SAT)  problem.  In  some  settings  MinSAT  is  preferred  to  using  Maximum  Satis-fiability  (MaxSAT).  Several  encodings  and  dedicated  branch  and  bound  algorithms  for  MinSAT  have  been  recently  proposed,  and  evaluated  on  small  challenging  randomly  generated  instances.  Motivated  by  the  observation  that  current  best  performing  MaxSAT  algorithms  for  structured  and  industrial  instances  are  based  on  computing  unsatisfiable  cores  with  a  SAT  solver,  this  paper  proposes  novel  approaches  for  MinSAT,  that  also  target  these  instances.  First,  the  paper  proposes  an  algorithm  based  on  iteratively  calling  a  SAT  solver  which  uses  the  computed  models  to  relax  clauses.  Second,  the  paper  proposes  group-based  MinSAT  solving,  which  is  essentially  a  novel  reduction  of  the  MinSAT  problem  into  the  Group  MaxSAT  problem.  For  a  given  MinSAT  instance,  the  resulting  Group  MaxSAT  formula  is  then  translated  into  a  standard  MaxSAT  formula  which  specifically  targets  unsatisfiability-based  MaxSAT  algorithms.  Experimental  results  indicate  that,  similarly  to  MaxSAT,  the  proposed  approaches  outperform  branch  and  bound  algorithms  on  problem  instances  obtained  from  practical  applications.
0	Irrelevant  features  class  separability  and  complexity  of  classification  problems.  In  this  paper,  analysis  of  class  separability  measures  is  performed  in  attempt  to  relate  their  descriptive  abilities  to  geometrical  properties  of  classification  problems  in  presence  of  irrelevant  features.  The  study  is  performed  on  synthetic  and  benchmark  data  with  known  irrelevant  features  and  other  characteristics  of  interest,  such  as  class  boundaries,  shapes,  margins  between  classes,  and  density.  The  results  have  shown  that  some  measures  are  individually  informative,  while  others  are  less  reliable  and  only  can  provide  complimentary  information.  Classification  problem  complexity  measurements  on  selected  data  sets  are  made  to  gain  additional  insights  on  the  obtained  results.
0	Cardiorespiratory  sleep  stage  detection  using  conditional  random  fields.  This  paper  explores  the  probabilistic  properties  of  sleep  stage  sequences  and  transitions  to  improve  the  performance  of  sleep  stage  detection  using  cardiorespiratory  features.  A  new  classifier,  based  on  conditional  random  fields,  is  used  in  different  sleep  stage  detection  tasks  (N3,  NREM,  REM,  and  wake)  in  night-time  recordings  of  electrocardiogram  and  respiratory  inductance  plethysmography  of  healthy  subjects.  Using  a  dataset  of  342  polysomnographic  recordings  of  healthy  subjects,  among  which  135  with  regular  sleep  architecture,  it  outperforms  hidden  Markov  models  and  Bayesian  linear  discriminants  in  all  tasks,  achieving  an  average  accuracy  of  87.38%  and  kappa  of  0.41  (87.27%  and  0.49  for  regular  subjects)  for  N3  detection,  78.71%  and  0.55  (80.34%  and  0.56  for  regular  subjects)  for  NREM  detection,  88.49%  and  0.51  (87.35%  and  0.57  for  regular  subjects)  for  REM,  and  85.69%  and  0.51  (90.42%  and  0.52  for  regular  subjects)  for  wake.  In  comparison  with  the  state  of  the  art,  and  having  been  tested  on  a  much  larger  dataset,  the  classifier  was  found  to  outperform  most  of  the  work  reported  in  the  literature  for  some  of  the  tasks,  in  particular  for  subjects  with  regular  sleep  architecture.  It  achieves  a  comparable  accuracy  for  N3,  higher  accuracy  and  kappa  for  REM,  and  higher  accuracy  and  comparable  kappa  for  NREM  than  the  best  performing  classifiers  described  in  the  literature.
0	Towards  a  theoretical  understanding  of  negative  transfer  in  collective  matrix  factorization.  Collective  matrix  factorization  (CMF)  is  a  popular  technique  to  improve  the  overall  factorization  quality  of  multiple  matrices  presuming  they  share  the  same  latent  factor.  However,  it  suffers  from  performance  degeneration  when  this  assumption  fails,  an  effect  called  negative  transfer  (n.t.).  Although  the  effect  is  widely  admitted,  its  theoretical  nature  remains  a  mystery  to  date.    This  paper  presents  a  first  theoretical  understanding  of  n.t.  in  theory.  Under  the  statistical  mini-max  framework,  we  derive  lower  bounds  for  the  CMF  estimator  and  gain  two  insights.  First,  the  n.t.  effect  can  be  explained  as  the  rise  of  a  bias  term  in  the  standard  lower  bound,  which  depends  only  on  the  structure  of  factor  space  but  neither  the  estimator  nor  samples.  Second,  the  n.t.  effect  can  be  explained  as  the  rise  of  an  dth-root  function  on  the  learning  rate,  where  d  is  the  dimension  of  a  Grassmannian  containing  the  sub-spaces  spanned  by  latent  factors.  These  discoveries  are  also  supported  in  simulation,  and  suggest  n.t.  may  be  more  effectively  addressed  via  model  construction  other  than  model  selection.
0	Efficient  bayesian  inference  for  a  gaussian  process  density  model.  We  reconsider  a  nonparametric  density  model  based  on  Gaussian  processes.  By  augmenting  the  model  with  latent  Polya--Gamma  random  variables  and  a  latent  marked  Poisson  process  we  obtain  a  new  likelihood  which  is  conjugate  to  the  model's  Gaussian  process  prior.  The  augmented  posterior  allows  for  efficient  inference  by  Gibbs  sampling  and  an  approximate  variational  mean  field  approach.  For  the  latter  we  utilise  sparse  GP  approximations  to  tackle  the  infinite  dimensionality  of  the  problem.  The  performance  of  both  algorithms  and  comparisons  with  other  density  estimators  are  demonstrated  on  artificial  and  real  datasets  with  up  to  several  thousand  data  points.
0	Variational  dual  tree  framework  for  large  scale  transition  matrix  approximation.  In  recent  years,  non-parametric  methods  utilizing  random  walks  on  graphs  have  been  used  to  solve  a  wide  range  of  machine  learning  problems,  but  in  their  simplest  form  they  do  not  scale  well  due  to  the  quadratic  complexity.  In  this  paper,  a  new  dual-tree  based  variational  approach  for  approximating  the  transition  matrix  and  efficiently  performing  the  random  walk  is  proposed.  The  approach  exploits  a  connection  between  kernel  density  estimation,  mixture  modeling,  and  random  walk  on  graphs  in  an  optimization  of  the  transition  matrix  for  the  data  graph  that  ties  together  edge  transitions  probabilities  that  are  similar.  Compared  to  the  de  facto  standard  approximation  method  based  on  k-nearest-neighbors,  we  demonstrate  order  of  magnitudes  speedup  without  sacrificing  accuracy  for  Label  Propagation  tasks  on  benchmark  data  sets  in  semi-supervised  learning.
0	Improving  neural  tagging  with  lexical  information.  Neural  part-of-speech  tagging  has  achieved  competitive  results  with  the  incorporation  of  character-based  and  pre-trained  word  embeddings.  In  this  paper,  we  show  that  a  state-of-the-art  bi-LSTM  tagger  can  benefit  from  using  information  from  morphosyntactic  lexicons  as  additional  input.  The  tagger,  trained  on  several  dozen  languages,  shows  a  consistent,  average  improvement  when  using  lexical  information,  even  when  also  using  character-based  embeddings,  thus  showing  the  complementarity  of  the  different  sources  of  lexical  information.  The  improvements  are  particularly  important  for  the  smaller  datasets.
0	Speaker  verification  on  unbalanced  data  with  genetic  programming.  Automatic  Speaker  Verification  (ASV)  is  a  highly  unbalanced  binary  classification  problem,  in  which  any  given  speaker  must  be  verified  against  everyone  else.  We  apply  Genetic  programming  (GP)  to  this  problem  with  the  aim  of  both  prediction  and  inference.  We  examine  the  generalisation  of  evolved  programs  using  a  variety  of  fitness  functions  and  data  sampling  techniques  found  in  the  literature.  A  significant  difference  between  train  and  test  performance,  which  can  indicate  overfitting,  is  found  in  the  evolutionary  runs  of  all  to-be-verified  speakers.  Nevertheless,  in  all  speakers,  the  best  test  performance  attained  is  always  superior  than  just  merely  predicting  the  majority  class.  We  examine  which  features  are  used  in  good-generalising  individuals.  The  findings  can  inform  future  applications  of  GP  or  other  machine  learning  techniques  to  ASV  about  the  suitability  of  feature-extraction  techniques.
0	Evaluating  reward  definitions  for  parameter  control.  Parameter  controllers  for  Evolutionary  Algorithms  (EAs)  deal  with  adjusting  parameter  values  during  an  evolutionary  run.  Many  ad  hoc  approaches  have  been  presented  for  parameter  control,  but  few  generic  parameter  controllers  exist.  Recently,  successful  parameter  control  methods  based  on  Reinforcement  Learning  (RL)  have  been  suggested  for  one-off  applications,  i.e.  relatively  long  runs  with  controllers  used  out-of-the-box  with  no  tailoring  to  the  problem  at  hand.  However,  the  reward  function  used  was  not  investigated  in  depth,  though  it  is  a  non-trivial  factor  with  an  important  impact  on  the  performance  of  a  RL  mechanism.  In  this  paper,  we  address  this  issue  by  defining  and  comparing  four  alternative  reward  functions  for  such  generic  and  RL-based  EA  parameter  controllers.  We  conducted  experiments  with  different  EAs,  test  problems  and  controllers  and  results  showed  that  the  simplest  reward  function  performs  at  least  as  well  as  the  others,  making  it  an  ideal  choice  for  generic  out-of-the-box  parameter  control.
0	Gesture  class  prediction  by  recurrent  neural  network  and  attention  mechanism.  Our  objective  is  to  develop  a  machine-learning  model  that  allows  a  virtual  agent  to  automatically  perform  appropriate  communicative  gestures.  Our  first  step  is  to  compute  when  a  gesture  should  be  performed.  We  express  this  as  classification  problem.  We  initially  split  the  data  into  NoGesture  class  and  HasGesture  class.  We  develop  a  model  based  on  recurrent  neural  network  with  attention  mechanism  to  compute  the  class  based  on  the  speech  prosody.  We  apply  the  model  on  a  dialog  corpus  segmented  into  different  gesture  classes  and  gesture  phases.  We  treat  the  prosody  as  the  input  sequence  and  the  gesture  classes  as  the  output  sequence.
0	Minimum  bayes  risk  subsequence  combination  for  machine  translation.  System  combination  has  proved  to  be  a  successful  technique  in  the  pattern  recognition  field.  However,  several  difficulties  arise  when  combining  the  outputs  of  tasks,  e.g.  machine  translation,  that  generates  structured  patterns.  So  far,  machine  translation  system  combination  approaches  either  implement  sophisticated  classifiers  to  select  one  of  the  provided  translations,  or  generate  new  sentences  by  combining  the  "best"  subsequences  of  the  provided  translations.  We  present  minimum  Bayes'  risk  system  combination  (MBRSC),  a  system  combination  method  for  machine  translation  that  gathers  together  the  advantages  of  sentence-selection  and  subsequence-combination  methods.  MBRSC  is  able  to  detect  and  utilize  the  "best"  subsequences  of  the  provided  translations  to  generate  the  optimal  consensus  translation  with  respect  to  a  particular  performance  metric.  Experiments  show  that  MBRSC  obtains  significant  improvements  in  translation  quality,  and  a  particularly  competitive  performance  when  applied  to  languages  with  scarce  resources.
0	Self  adjusting  mutation  rates  with  provably  optimal  success  rules.  The  one-fifth  success  rule  is  one  of  the  best-known  and  most  widely  accepted  techniques  to  control  the  parameters  of  evolutionary  algorithms.  While  it  is  often  applied  in  the  literal  sense,  a  common  interpretation  sees  the  one-fifth  success  rule  as  a  family  of  success-based  updated  rules  that  are  determined  by  an  update  strength  F  and  a  success  rate  s.  We  analyze  in  this  work  how  the  performance  of  the  (1+1)  Evolutionary  Algorithm  (EA)  on  LeadingOnes  depends  on  these  two  hyper-parameters.  Our  main  result  shows  that  the  best  performance  is  obtained  for  small  update  strengths  F  =  1+o(1)  and  success  rate  1/e.  We  also  prove  that  the  runtime  obtained  by  this  parameter  setting  is  asymptotically  optimal  among  all  dynamic  choices  of  the  mutation  rate  for  the  (1+1)  EA,  up  to  lower  order  error  terms.  We  show  similar  results  for  the  resampling  variant  of  the  (1+1)  EA,  which  enforces  to  flip  at  least  one  bit  per  iteration.
0	Differential  evolution  based  human  body  pose  estimation  from  point  clouds.  This  paper  describes  a  method  to  estimate  the  body  pose  of  a  human  from  the  point  cloud  obtained  from  a  depth  sensor.  It  uses  Differential  Evolution  to  find  the  best  match  between  a  candidate  pose,  represented  by  an  instance  of  a  42-parameter  articulated  model  of  a  human,  and  the  point  cloud.  The  results,  compared  to  other  four  state-of-the  art  methods  on  a  publicly  available  dataset,  show  that  the  method  has  good  ability  to  estimate  the  pose  of  a  person  and  to  track  him  in  video  sequences.      The  entire  method,  from  Differential  Evolution  to  fitness  computation,  is  run  on  nVIDIA  GPUs.  Thanks  to  its  massively  parallel  implementation  in  CUDA-C,  it  produces  pose  estimates  in  real  time.
0	Generalizing  the  improved  run  time  complexity  algorithm  for  non  dominated  sorting.  This  paper  generalizes  the  "Improved  Run-Time  Complexity  Algorithm  for  Non-Dominated  Sorting"  by  Jensen,  removing  its  limitation  that  no  two  solutions  can  share  identical  values  for  any  of  the  problem's  objectives.  This  constraint  is  especially  limiting  for  discrete  combinatorial  problems,  but  can  also  lead  the  Jensen  algorithm  to  produce  incorrect  results  even  for  problems  that  appear  to  have  a  continuous  nature,  but  for  which  identical  objective  values  are  nevertheless  possible.  Moreover,  even  when  values  are  not  meant  to  be  identical,  the  limited  precision  of  floating  point  numbers  can  sometimes  make  them  equal  anyway.  Thus  a  fast  and  correct  algorithm  is  needed  for  the  general  case.  The  paper  shows  that  generalizing  the  Jensen  algorithm  can  be  achieved  without  affecting  its  time  complexity,  and  experimental  results  are  provided  to  demonstrate  speedups  of  up  to  two  orders  of  magnitude  for  common  problem  sizes,  when  compared  with  the  correct  baseline  algorithm  from  Deb.
0	A  comparative  study  between  two  regression  methods  on  lidar  data  a  case  study.  Airborne  LiDAR  (Light  Detection  and  Ranging)  has  become  an  excellent  tool  for  accurately  assessing  vegetation  characteristics  in  forest  environments.  Previous  studies  showed  empirical  relationships  between  LiDAR  and  field-measured  biophysical  variables.  Multiple  linear  regression  (MLR)  with  stepwise  feature  selection  is  the  most  common  method  for  building  estimation  models.  Although  this  technique  has  provided  very  interesting  results,  many  other  data  mining  techniques  may  be  applied.  The  overall  goal  of  this  study  is  to  compare  different  methodologies  for  assessing  biomass  fractions  at  stand  level  using  airborne  Li-DAR  data  in  forest  settings.  In  order  to  choose  the  best  methodology,  a  comparison  between  two  different  feature  selection  techniques  (stepwise  selection  vs.  genetic-based  selection)  is  presented.  In  addition,  classical  MLR  is  also  compared  with  regression  trees  (M5P).  The  results  when  each  methodology  is  applied  to  estimate  stand  biomass  fractions  from  an  area  of  northern  Spain  show  that  genetically-selected  M5P  obtains  the  best  results.
0	Generating  structured  test  data  with  specific  properties  using  nested  monte  carlo  search.  Software  acting  on  complex  data  structures  can  be  challenging  to  test:  it  is  difficult  to  generate  diverse  test  data  that  satisfies  structural  constraints  while  simultaneously  exhibiting  properties,  such  as  a  particular  size,  that  the  test  engineer  believes  will  be  effective  in  detecting  faults.  In  our  previous  work  we  introduced  GodelTest,  a  framework  for  generating  such  data  structures  using  non-deterministic  programs,  and  combined  it  with  Differential  Evolution  to  optimize  the  generation  process.  Monte-Carlo  Tree  Search  (MCTS)  is  a  search  technique  that  has  shown  great  success  in  playing  games  that  can  be  represented  as  a  sequence  of  decisions.  In  this  paper  we  apply  Nested  Monte-Carlo  Search,  a  single-player  variant  of  MCTS,  to  the  sequence  of  decisions  made  by  the  generating  programs  used  by  GodelTest,  and  show  that  this  combination  can  efficiently  generate  random  data  structures  which  exhibit  the  specific  properties  that  the  test  engineer  requires.  We  compare  the  results  to  Boltzmann  sampling,  an  analytical  approach  to  generating  random  combinatorial  data  structures.
0	Pattern  guided  genetic  programming.  Online  progress  in  search  and  optimization  is  often  hindered  by  neutrality  in  the  fitness  landscape,  when  many  genotypes  map  to  the  same  fitness  value.  We  propose  a  method  for  imposing  a  gradient  on  the  fitness  function  of  a  metaheuristic  (in  this  case,  Genetic  Programming)  via  a  metric  (Minimum  Description  Length)  induced  from  patterns  detected  in  the  trajectory  of  program  execution.  These  patterns  are  induced  via  a  decision  tree  classifier.  We  apply  this  method  to  a  range  of  integer  and  boolean-valued  problems,  significantly  outperforming  the  standard  approach.  The  method  is  conceptually  straightforward  and  applicable  to  virtually  any  metaheuristic  that  can  be  appropriately  instrumented.
0	Genetic  improvement  using  higher  order  mutation.  This  paper  presents  a  brief  outline  of  a  higher-order  mutation-based  framework  for  Genetic  Improvement  (GI).  We  argue  that  search-based  higher-order  mutation  testing  can  be  used  to  implement  a  form  of  genetic  programming  (GP)  to  increase  the  search  granularity  and  testability  of  GI.
0	Improving  the  performance  of  max  min  ant  system  on  the  tsp  using  stubborn  ants.  In  ant  colony  optimization  (ACO)  methods,  including  Ant  System  and  MAX-MIN  Ant  System,  each  ant  stochastically  generates  its  candidate  solution,  in  a  given  iteration,  based  on  the  same  pheromone  and  heuristic  information  as  every  other  ant.  Stubborn  ants  is  an  ACO  variation  in  which  if  an  ant  generates  a  particular  candidate  solution  in  a  given  iteration,  then  the  components  of  that  solution  will  have  a  higher  probability  of  being  selected  in  the  candidate  solution  generated  by  that  ant  in  the  next  iteration.  We  evaluate  this  variation  in  the  context  of  MAX-MIN  Ant  System  using  41  instances  of  the  Traveling  Salesman  Problem,  and  find  that  it  improves  solution  quality  to  a  statistically-significant  extent.
0	Minimizing  regular  objectives  for  blocking  permutation  flow  shop  scheduling  heuristic  approaches.  The  objective  of  this  work  is  to  present  and  evaluate  meta-heuristics  for  the  blocking  permutation  flow  shop  scheduling  problem  subject  to  regular  objectives.  The  blocking  problem  is  known  to  be  NP-hard  with  more  than  two  machines.  We  assess  the  difficulty  level  of  this  problem  by  developing  two  population-based  meta-heuristics:  Genetic  Algorithm  and  Artificial  Bee  Colony  algorithm.  The  final  goal  is  to  measure  the  performance  of  these  proposed  techniques  and  potentially  contribute  in  possible  improvements  in  the  blocking  benchmark  instances.  Furthermore,  computational  tests  carried  out  on  randomly  generated  test  problems  show  that  the  approaches  consistently  yields  good  solutions  in  a  moderate  amount  of  time.  Finally,  an  updated  list  of  best-known  solutions  for  the  Taillard's  and  Ronconi  and  Henriques's  benchmark  is  exposed:  new  best-known  solutions  for  the  blocking  flow  shop  scheduling  problem  with  makespan,  total  flow  time,  and  total  tardiness  criteria  are  found.
0	Towards  real  life  application  of  emg  based  speech  recognition  by  using  unsupervised  adaptation.  This  paper  deals  with  a  Silent  Speech  Interface  based  on  Surface  Electromyography  (EMG),  where  electrodes  capture  the  electric  activity  generated  by  the  articulatory  muscles  from  a  user’s  face  in  order  to  decode  the  underlying  speech,  allowing  speech  to  be  recognized  even  when  no  sound  is  heard  or  created.  So  far,  most  EMG-based  speech  recognizers  described  in  literature  do  not  allow  electrode  reattachment  between  system  training  and  usage,  which  we  consider  unsuitable  for  practical  applications.  In  this  study  we  report  on  our  research  on  unsupervised  session  adaptation:  A  system  is  pre-trained  with  data  from  multiple  recording  sessions  and  then  adapted  towards  the  current  recording  session  using  data  accruable  during  normal  use,  without  requiring  a  time-consuming  specific  enrollment  phase.  We  show  that  considerable  accuracy  improvements  can  be  achieved  with  this  method,  paving  the  way  towards  real-life  applications  of  the  technology.  Index  Terms:  Silent  Speech  Interfaces,  EMG,  EMG-based  Speech  Recognition,  Unsupervised  Adaptation
0	Leveraging  knowledge  graphs  for  web  scale  unsupervised  semantic  parsing.  The  past  decade  has  seen  the  emergence  of  web-scale  structured  and  linked  semantic  knowledge  resources  (e.g.,  Freebase,  DBPedia).  These  semantic  knowledge  graphs  provide  a  scalable  “schema  for  the  web”,  representing  a  significant  opportunity  for  the  spoken  language  understanding  (SLU)  research  community.  This  paper  leverages  these  resources  to  bootstrap  a  web-scale  semantic  parser  with  no  requirement  for  semantic  schema  design,  no  data  collection,  and  no  manual  annotations.  Our  approach  is  based  on  an  iterative  graph  crawl  algorithm.  From  an  initial  seed  node  (entity-type),  the  method  learns  the  related  entity-types  from  the  graph  structure,  and  automatically  annotates  documents  that  can  be  linked  to  the  node  (e.g.,  Wikipedia  articles,  web  search  documents).  Following  the  branches,  the  graph  is  crawled  and  the  procedure  is  repeated.  The  resulting  collection  of  annotated  documents  is  used  to  bootstrap  webscale  conditional  random  field  (CRF)  semantic  parsers.  Finally,  we  use  a  maximum-a-posteriori  (MAP)  unsupervised  adaptation  technique  on  sample  data  from  a  specific  domain  to  refine  the  parsers.  The  scale  of  the  unsupervised  parsers  is  on  the  order  of  thousands  of  domains  and  entity-types,  millions  of  entities,  and  hundreds  of  millions  of  relations.  The  precision-recall  of  the  semantic  parsers  trained  with  our  unsupervised  method  approaches  those  trained  with  supervised  annotations.
0	Reconstructing  intelligible  audio  speech  from  visual  speech  features.  This  work  describes  an  investigation  into  the  feasibility  of  producing  intelligible  audio  speech  from  only  visual  speech  fea-  tures.  The  proposed  method  aims  to  estimate  a  spectral  enve-  lope  from  visual  features  which  is  then  combined  with  an  arti-  ficial  excitation  signal  and  used  within  a  model  of  speech  pro-  duction  to  reconstruct  an  audio  signal.  Different  combinations  of  audio  and  visual  features  are  considered,  along  with  both  a  statistical  method  of  estimation  and  a  deep  neural  network.  The  intelligibility  of  the  reconstructed  audio  speech  is  measured  by  human  listeners,  and  then  compared  to  the  intelligibility  of  the  video  signal  only  and  when  combined  with  the  reconstructed  audio.
0	Deep  learning  based  dereverberation  of  temporal  envelopes  for  robust  speech  recognition.  Automatic  speech  recognition  in  reverberant  conditions  is  a  challenging  task  as  the  long-term  envelopes  of  the  reverberant  speech  are  temporally  smeared.  In  this  paper,  we  propose  a  neural  model  for  enhancement  of  sub-band  temporal  envelopes  for  dereverberation  of  speech.  The  temporal  envelopes  are  derived  using  the  autoregressive  modeling  framework  of  frequency  domain  linear  prediction  (FDLP).  The  neural  enhancement  model  proposed  in  this  paper  performs  an  envelop  gain  based  enhancement  of  temporal  envelopes  and  it  consists  of  a  series  of  convolutional  and  recurrent  neural  network  layers.  The  enhanced  sub-band  envelopes  are  used  to  generate  features  for  automatic  speech  recognition  (ASR).  The  ASR  experiments  are  performed  on  the  REVERB  challenge  dataset  as  well  as  the  CHiME-3  dataset.  In  these  experiments,  the  proposed  neural  enhancement  approach  provides  significant  improvements  over  a  baseline  ASR  system  with  beamformed  audio  (average  relative  improvements  of  21%  on  the  development  set  and  about  11%  on  the  evaluation  set  in  word  error  rates  for  REVERB  challenge  dataset).
0	Adaptive  speech  recognition  and  dialogue  management  for  users  with  speech  disorders.  Spoken  control  interfaces  are  very  attractive  to  people  with  severe  physical  disabilities  who  often  also  have  a  type  of  speech  disorder  known  as  dysarthria.  This  condition  is  known  to  decrease  the  accuracy  of  automatic  speech  recognisers  (ASRs)  especially  for  users  with  moderate  to  severe  dysathria.  In  this  paper  we  investigate  how  applying  probabilistic  dialogue  management  (DM)  techniques  can  improve  interaction  performance  of  an  environmental  control  system  for  such  users.  The  effect  of  having  access  to  different  amounts  of  adaptation  data,  as  well  as  using  different  vocabulary  size  for  speakers  of  different  intelligibilities  is  investigated.  We  explore  the  effect  of  adapting  the  DM  models  as  the  ASR  performance  increases,  such  as  is  the  case  in  systems  where  more  adaptation  data  is  collected  through  system  use.  Improvements  compared  to  a  non-probabilistic  DM  baseline  are  seen  both  in  terms  of  dialogue  length  and  success  rate,  9%  and  25%  mean  relative  improvement  respectively.  Looking  at  just  the  more  severe  dysarthric  speakers  these  numbers  rise  25%  and  75%  mean  relative  improvement.  These  improvements  are  higher  when  the  ASR  data  adaptation  amount  is  small.  Further  results  show  that  a  DM  trained  on  data  from  multiple  speakers  outperform  a  DM  trained  on  data  from  a  single  speaker.
0	Intermediate  state  hmms  to  capture  continuously  changing  signal  features.  Traditional  discrete-state  HMMs  are  not  well  suited  for  describing  steadily  evolving,  path-following  natural  processes  like  motion  capture  data  or  speech.  HMMs  cannot  represent  incremental  progress  between  behaviors,  and  sequences  sampled  from  the  models  have  unnatural  segment  durations,  unsmooth  transitions,  and  excessive  rapid  variation.  We  propose  to  address  these  problems  by  permitting  the  state  variable  to  occupy  positions  between  the  discrete  states,  and  present  a  concrete  left-right  model  incorporating  this  idea.  We  call  this  intermediate-state  HMMs.  The  state  evolution  remains  Markovian.  We  describe  training  using  the  generalized  EM-algorithm  and  present  associated  update  formulas.  An  experiment  shows  that  the  intermediate-state  model  is  capable  of  gradual  transitions,  with  more  natural  durations  and  less  noise  in  sampled  sequences  compared  to  a  conventional  HMM.
0	Best  of  both  worlds  robust  accented  speech  recognition  with  adversarial  transfer  learning.  Training  deep  neural  networks  for  automatic  speech  recognition  (ASR)  requires  large  amounts  of  transcribed  speech.  This  becomes  a  bottleneck  for  training  robust  models  for  accented  speech  which  typically  contains  high  variability  in  pronunciation  and  other  semantics,  since  obtaining  large  amounts  of  annotated  accented  data  is  both  tedious  and  costly.  Often,  we  only  have  access  to  large  amounts  of  unannotated  speech  from  different  accents.  In  this  work,  we  leverage  this  unannotated  data  to  provide  semantic  regularization  to  an  ASR  model  that  has  been  trained  only  on  one  accent,  to  improve  its  performance  for  multiple  accents.  We  propose  Accent  Pre-Training  (Acc-PT),  a  semi-supervised  training  strategy  that  combines  transfer  learning  and  adversarial  training.  Our  approach  improves  the  performance  of  a  state-of-the-art  ASR  model  by  33%  on  average  over  the  baseline  across  multiple  accents,  training  only  on  annotated  samples  from  one  standard  accent,  and  as  little  as  105  minutes  of  unannotated  speech  from  a  target  accent.
0	Unsupervised  sentence  simplification  using  deep  semantics.  We  present  a  novel  approach  to  sentence  simplification  which  departs  from  previous  work  in  two  main  ways.  First,  it  requires  neither  hand  written  rules  nor  a  training  corpus  of  aligned  standard  and  simplified  sentences.  Second,  sentence  splitting  operates  on  deep  semantic  structure.  We  show  (i)  that  the  unsupervised  framework  we  propose  is  competitive  with  four  state-of-the-art  supervised  systems  and  (ii)  that  our  semantic  based  approach  allows  for  a  principled  and  effective  handling  of  sentence  splitting.
0	Automatic  analysis  of  typical  and  atypical  encoding  of  spontaneous  emotion  in  the  voice  of  children.  Children  with  Autism  Spectrum  Disorders  (ASD)  present  significant  difficulties  to  understand  and  express  emotions.  Systems  have  thus  been  proposed  to  provide  objective  measurements  of  acoustic  features  used  by  children  suffering  from  ASD  to  encode  emotion  in  speech.  However,  only  a  few  studies  have  exploited  such  systems  to  compare  different  groups  of  children  in  their  ability  to  express  emotions,  and  even  less  have  focused  on  the  analysis  of  spontaneous  emotion.  In  this  contribution,  we  provide  insights  by  extensive  evaluations  carried  out  on  a  new  database  of  spontaneous  speech  inducing  three  emotion  categories  of  valence  (positive,  neutral,  and  negative).  We  evaluate  the  potential  of  using  an  automatic  recognition  system  to  differentiate  groups  of  children,  i.e.,  pervasive  developmental  disorders,  pervasive  developmental  disorders  not-otherwise  specified,  specific  language  impairments,  and  typically  developing,  in  their  abilities  to  express  spontaneous  emotion  in  a  common  unconstrained  task.  Results  show  that  all  groups  of  children  can  be  differentiated  directly  (diagnosis  recognition)  and  indirectly  (emotion  recognition)  by  the  proposed  system.
0	Estimating  the  voice  source  in  noise.  Estimation  of  the  glottal  source  has  applications  in  many  areas  of  speech  processing.  Therefore,  a  noise-robust  automatic  source  estimation  algorithm  is  proposed  in  this  paper.  The  source  signal  is  estimated  using  a  codebook  search  approach.  The  glottal  area  waveforms  extracted  from  high-speed  recordings  of  the  glottis  is  converted  to  the  glottal  flow  signals  in  order  to  evaluate  the  performance  of  the  proposed  source  estimation  algorithm.  Results  in  clean  and  noisy  conditions,  on  average,  show  that  the  proposed  algorithm  provides  more  accurate  estimation  than  the  software  toolkit  Aparat  [1]  as  well  as  an  earlier  approach  [2].
0	Large  scale  sequence  discriminative  joint  adaptive  training  for  masking  based  robust  asr.  Recently,  it  was  shown  that  the  performance  of  supervised  timefrequency  masking  based  robust  automatic  speech  recognition  techniques  can  be  improved  by  training  them  jointly  with  the  acoustic  model  [1].  The  system  in  [1],  termed  deep  neural  network  based  joint  adaptive  training,  used  fully-connected  feedforward  deep  neural  networks  for  estimating  time-frequency  masks  and  for  acoustic  modeling;  stacked  log  mel  spectra  was  used  as  features  and  training  minimized  cross  entropy  loss.  In  this  work,  we  extend  such  jointly  trained  systems  in  several  ways.  First,  we  use  recurrent  neural  networks  based  on  long  short-term  memory  (LSTM)  units  –  this  allows  the  use  of  unstacked  features,  simplifying  joint  optimization.  Next,  we  use  a  sequence  discriminative  training  criterion  for  optimizing  parameters.  Finally,  we  conduct  experiments  on  large  scale  data  and  show  that  joint  adaptive  training  can  provide  gains  over  a  strong  baseline.  Systematic  evaluations  on  noisy  voice-search  data  show  relative  improvements  ranging  from  2%  at  15  dB  to  5.4%  at  -5  dB  over  a  sequence  discriminative,  multi-condition  trained  LSTM  acoustic  model.
0	Improving  automatic  speech  recognition  in  spatially  aware  hearing  aids.  In  the  context  of  ambient  assisted  living,  automatic  speech  recognition  (ASR)  has  the  potential  to  provide  textual  support  for  hearing  aid  users  in  challenging  acoustic  conditions.  In  this  paper  we  therefore  investigate  possibilities  to  improve  ASR  based  on  binaural  hearing  aid  signals  in  complex  acoustic  scenes.  Particularly,  information  about  the  spatial  configuration  of  sound  sources  is  exploited  and  estimated  using  a  recently  developed  method  that  employs  probabilistic  information  about  the  location  of  a  target  speaker  (and  a  simultaneous  localized  masker)  for  robust  real-time  localization.  Two  different  strategies  are  investigated:  straightforward  better-ear  listening  and  a  multi-channel  beamforming  system  aiming  at  enhancement  of  a  target  speech  source  with  additional  suppression  of  localized  masking  sound.  The  latter  method  is  also  complemented  by  better-ear  listening.  Both  approaches  are  evaluated  in  different  acoustic  scenarios  containing  moving  target  and  interfering  speakers  or  noise  sources.  Compared  to  using  nonpreprocessed  signals,  we  obtain  average  relative  reductions  in  word  error  rate  of  28.4%  in  the  presence  of  a  localized  interfering  noise,  19.2%  in  the  case  of  a  concurrent  talker  and  23.7%  in  presence  of  a  concurrent  talker  in  spatially  diffuse  noise.  A  post-analysis  assesses  the  relation  of  localization  performance  and  beamforming  for  improved  speech  recognition  in  complex  acoustic  scenes.
0	Leveraging  word  embeddings  for  spoken  document  summarization.  Owing  to  the  rapidly  growing  multimedia  content  available  on  the  Internet,  extractive  spoken  document  summarization,  with  the  purpose  of  automatically  selecting  a  set  of  representative  sentences  from  a  spoken  document  to  concisely  express  the  most  important  theme  of  the  document,  has  been  an  active  area  of  research  and  experimentation.  On  the  other  hand,  word  embedding  has  emerged  as  a  newly  favorite  research  subject  because  of  its  excellent  performance  in  many  natural  language  processing  (NLP)-related  tasks.  However,  as  far  as  we  are  aware,  there  are  relatively  few  studies  investigating  its  use  in  extractive  text  or  speech  summarization.  A  common  thread  of  leveraging  word  embeddings  in  the  summarization  process  is  to  represent  the  document  (or  sentence)  by  averaging  the  word  embeddings  of  the  words  occurring  in  the  document  (or  sentence).  Then,  intuitively,  the  cosine  similarity  measure  can  be  employed  to  determine  the  relevance  degree  between  a  pair  of  representations.  Beyond  the  continued  efforts  made  to  improve  the  representation  of  words,  this  paper  focuses  on  building  novel  and  efficient  ranking  models  based  on  the  general  word  embedding  methods  for  extractive  speech  summarization.  Experimental  results  demonstrate  the  effectiveness  of  our  proposed  methods,  compared  to  existing  state-of-the-art  methods.
0	On  the  effectiveness  of  using  syntactic  and  shallow  semantic  tree  kernels  for  automatic  assessment  of  essays.  This  paper  is  concerned  with  the  problem  of  automatic  essay  grading,  where  the  task  is  to  grade  student  written  essays  given  course  materials  and  a  set  of  humangraded  essays  as  training  data.  Latent  Semantic  Analysis  (LSA)  has  been  used  extensively  over  the  years  to  accomplish  this  task.  However,  the  major  limitation  of  LSA  is  that  it  only  retains  the  frequency  of  words  by  disregarding  the  word  sequence,  and  the  syntactic  and  semantic  structure  of  texts.  As  a  remedy,  we  propose  the  use  of  syntactic  and  shallow  semantic  tree  kernels  for  grading  essays.  Experiments  suggest  that  syntactic  and  semantic  structural  information  can  significantly  improve  the  performance  of  the  state-of-the-art  LSAbased  models  for  automatic  essay  grading.
0	Disambiguating  explicit  discourse  connectives  without  oracles.  Deciding  whether  a  word  serves  a  discourse  function  in  context  is  a  prerequisite  for  discourse  processing,  and  the  performance  of  this  subtask  bounds  performance  on  subsequent  tasks.  Pitler  and  Nenkova  (2009)  report  96.29%  accuracy  (F1  94.19%)  relying  on  features  extracted  from  gold-standard  parse  trees.  This  figure  is  an  average  over  several  connectives,  some  of  which  are  extremely  hard  to  classify.  More  importantly,  performance  drops  considerably  in  the  absence  of  an  oracle  providing  gold-standard  features.  We  show  that  a  very  simple  model  using  only  lexical  and  predicted  part-of-speech  features  actually  performs  slightly  better  than  Pitler  and  Nenkova  (2009)  and  not  significantly  different  from  a  state-of-the-art  model,  which  combines  lexical,  part-ofspeech,  and  parse  features.
0	Improving  named  entity  recognition  in  tweets  via  detecting  non  standard  words.  Most  previous  work  of  text  normalization  on  informal  text  made  a  strong  assumption  that  the  system  has  already  known  which  tokens  are  non-standard  words  (NSW)  and  thus  need  normalization.  However,  this  is  not  realistic.  In  this  paper,  we  propose  a  method  for  NSW  detection.  In  addition  to  the  information  based  on  the  dictionary,  e.g.,  whether  a  word  is  out-ofvocabulary  (OOV),  we  leverage  novel  information  derived  from  the  normalization  results  for  OOV  words  to  help  make  decisions.  Second,  this  paper  investigates  two  methods  using  NSW  detection  results  for  named  entity  recognition  (NER)  in  social  media  data.  One  adopts  a  pipeline  strategy,  and  the  other  uses  a  joint  decoding  fashion.  We  also  create  a  new  data  set  with  newly  added  normalization  annotation  beyond  the  existing  named  entity  labels.  This  is  the  first  data  set  with  such  annotation  and  we  release  it  for  research  purpose.  Our  experiment  results  demonstrate  the  effectiveness  of  our  NSW  detection  method  and  the  benefit  of  NSW  detection  for  NER.  Our  proposed  methods  perform  better  than  the  state-of-the-art  NER  system.
0	Sparsity  and  step  size  adaptive  regularized  matching  pursuit  algorithm  for  compressed  sensing.  A  novel  greedy  matching  pursuit  reconstruction  algorithm  for  compressed  sensing  (CS)  was  proposed  in  this  paper,  called  Sparsity  and  Step-size  Adaptive  Regularized  Matching  Pursuit  (SSARMP).  Compared  with  other  traditional  matching  pursuit  algorithms,  e.g.  Orthogonal  Matching  Pursuit  (OMP),  SSARMP  can  recover  the  sparse  signal  without  prior  information  of  the  sparsity,  and  compared  with  Sparsity  Adaptive  Matching  Pursuit  (SAMP)  algorithm,  the  presented  algorithm  can  get  a  compressibility  estimation  by  estimating  the  signal's  compressibility  firstly  and  then  set  this  estimation  value  as  the  finalist  in  the  first  stage.  The  regularized  idea  and  the  variable  step-size  were  added  in  selecting  elements  of  the  candidate  set  and  changing  finalist  stage  respectively.  A  reliable  numerical  sparsity  estimation  can  reduce  the  number  of  iterations  of  the  algorithm  and  the  regularized  and  variable  step-size  can  improve  the  recovery  accuracy  obviously.  So,  SSARMP  can  finally  reach  better  complexity  and  better  reconstruction  accuracy  at  the  same  time.  Simulation  results  show  that  SSARMP  outperforms  almost  all  existing  iterative  algorithms  without  prior  information  of  the  sparsity,  especially  for  compressible  Gaussian  signal.
0	Towards  artificial  evolution  of  complex  behaviors  observed  in  insect  colonies.  Studies  on  social  insects  have  demonstrated  that  complex,  adaptive  and  self-organized  behavior  can  arise  at  the  macroscopic  level  from  relatively  simple  rules  at  the  microscopic  level.  Several  past  studies  in  robotics  and  artificial  life  have  focused  on  the  evolution  and  understanding  of  the  rules  that  give  rise  to  a  specific  macroscopic  behavior  such  as  task  allocation,  communication  or  synchronization.  In  this  study,  we  demonstrate  how  colonies  of  embodied  agents  can  be  evolved  to  display  multiple  complex  macroscopic  behaviors  at  the  same  time.  In  our  evolutionary  model,  we  incorporate  key  features  present  in  many  natural  systems,  namely  energy  consumption,  birth,  death  and  a  long  evaluation  time.  We  use  a  generic  foraging  scenario  in  which  agents  spend  energy  while  they  move  and  they  must  periodically  recharge  in  the  nest  to  avoid  death.  New  robots  are  added  (born)  when  the  colony  has  foraged  a  sufficient  number  of  preys.  We  perform  an  analysis  of  the  evolved  behaviors  and  demonstrate  that  several  colonies  display  multiple  complex  and  scalable  macroscopic  behaviors.
0	Multiobjective  wind  driven  optimization  approach  applied  to  transformer  design.  Metaheuristics  of  the  natural  computing  field  have  been  proposed  as  an  alternative  to  mathematical  optimization  approaches  to  address  non  convex  problems  involving  large  search  spaces.  In  recent  years  a  new  optimization  metaheuristic  algorithm  was  proposed  called  Wind  Driven  Optimization  (WDO).  WDO  is  a  stochastic  nature-inspired  paradigm  based  on  atmospheric  motion.  In  this  paper,  a  modified  version  of  WDO  is  proposed  and  evaluated,  based  on  Levy  flights  (or  Levy  motions)  to  tune  its  control  parameters,  called  Levy  WDO  (LWDO).  Levy  flight  or  anomalous  diffusion  process  is  a  random  walk  characterized  by  Markov  chain  in  which  the  step-lengths  have  a  probability  distribution  that  is  heavy-tailed.  To  evaluate  the  multiobjective  optimization  performance  of  the  WDO  and  the  proposed  LWDO,  a  benchmark  for  optimizing  of  a  safety  isolating  transformer  is  adopted.  In  this  paper,  the  transformer  design  optimization  is  treated  as  a  multiobjective  problem,  with  the  aim  to  minimize  both  the  total  mass  (iron  and  copper  materials)  and  losses  taking  into  consideration  design  constraints.  Simulation  results  testify  that  the  multiobjective  LWDO  is  a  promising  approach  for  multiobjective  optimization  as  it  outperforms  the  WDO  in  multiobjective  version  and  the  classical  NSGA-II  (Non-dominated  Sorting  Genetic  Algorithm,  version  II).
0	A  random  based  dynamic  grouping  strategy  for  large  scale  multi  objective  optimization.  This  paper  presents  a  random-based  dynamic  grouping  strategy  (RDG)  for  cooperative  coevolution  to  deal  with  large  scale  multi-objective  optimization  problems  (MOPs)  by  decomposing  the  whole  dimension  into  several  groups  of  variables  with  an  equal  size.  First,  a  decomposer  pool  containing  different  group  sizes  is  designed.  Then,  a  group  size  is  dynamically  selected  with  probability  in  the  evolution  process.  The  probability  of  each  group  size  in  the  pool  is  computed  based  on  the  historical  performance  measured  by  C-metric,  a  common  metric  in  multi-objective  optimization.  Under  the  selected  group  size,  random  grouping  is  executed  to  decompose  the  whole  dimension  into  groups.  Through  this,  both  the  group  size  and  the  group  components  are  dynamic.  Finally,  combining  RDG  with  a  traditional  and  famous  multi-objective  evolutionary  algorithm  (MOEA)  named  MOEA/D,  we  develop  MOEA/D-RDG  to  cope  with  large  scale  MOPs.  The  efficacy  of  the  proposed  MOEA/D-RDG  is  verified  on  two  sets  of  MOPs  (UF1-UF10  and  WFG1-WFG9)  through  comparing  with  two  MOEA/D  variants.
0	Training  spiking  neural  models  using  cuckoo  search  algorithm.  Several  meta-heuristic  algorithms  have  been  proposed  in  the  last  years  for  solving  a  wide  range  of  optimization  problems.  Cuckoo  Search  Algorithm  (CS)  is  a  novel  meta-heuristic  based  on  the  obligate  brood  parasitic  behaviour  of  some  cuckoo  species  in  combination  with  the  Levy  flight  behavior  of  some  birds  and  fruit  flies.  This  algorithm  has  been  applied  in  a  wide  range  of  optimization  problems;  nonetheless,  their  promising  results  suggest  its  application  in  the  field  of  artificial  neural  networks,  specially  during  the  adjustment  of  the  synaptic  weights.  On  the  other  hand,  spiking  neurons  are  neural  models  that  try  to  simulate  the  behavior  of  biological  neurons  when  they  are  excited  with  an  input  current  (input  pattern)  during  a  certain  period  time.  Instead  of  generating  a  response  in  its  output  every  iteration,  as  classical  neurons  do,  this  model  generates  a  response  (spikes  or  spike  train)  only  when  the  model  reaches  a  specific  threshold.  This  response  could  be  coded  into  a  firing  rate  and  perform  a  pattern  classification  task  according  to  the  firing  rate  generated  with  the  input  current.  To  perform  a  classification  task  the  model  ought  to  exhibit  the  next  behavior:  patterns  from  the  same  class  must  generate  similar  firing  rates  and  patterns  from  other  classes  have  to  generate  firing  rates  sufficiently  dissimilar  to  differentiate  among  the  classes.  The  model  needs  of  a  training  phase  aimed  to  adjust  their  synaptic  weights  and  exhibit  the  desired  behavior.  In  this  paper,  we  describe  how  the  CS  algorithm  can  be  useful  to  train  a  spiking  neuron  to  be  applied  in  a  pattern  classification  task.  The  accuracy  of  the  methodology  is  tested  using  several  pattern  recognition  problems.
0	Two  stage  tabu  particle  swarm  algorithms  for  the  facility  layout  problem  with  size  constraints.  The  Facility  Layout  Problem  (FLP)  in  this  paper  is  an  extension  of  the  traditional  Quadratic  Assignment  Problems  (QAP).  While  the  objective  is  still  to  minimize  the  summed  cost  of  the  (flow  ∗  distance),  the  facilities  in  the  FLP  have  different  given  sizes  and  their  locations  must  be  determined  on  a  continual  planar  site.  Based  on  the  visual  facility  layout  design  system  proposed  by  Chiang  [13],  this  paper  presents  a  study  on  using  Tabu  Search  (TS),  Particle  Swarm  Optimization  (PSO)  and  their  combinations  (TS+PSO  and  PSO+TS)  to  tackle  the  FLP.  The  computation  results  show  that  the  two-stage  algorithms  are  able  to  achieve  better  results  in  most  cases  than  TS  and  PSO  individually  on  the  FLP.  The  proposed  two-stage  algorithms  and  visual  layout  design  system  provide  an  effective  tool  to  solve  the  practical  FLP.
0	A  square  lattice  probability  model  for  optimising  the  graph  partitioning  problem.  Estimation  of  Distribution  Algorithms  have  proved  to  be  very  competitive  for  solving  combinatorial  and  continuous  optimisation  problems.  However,  there  are  problems  for  which  they  have  not  been  extensively  developed:  we  refer  to  constrained  optimisation  problems.  Existing  proposals  approach  these  problems  by  (i)  modifying  the  sampling  strategy  of  the  probabilistic  model  to  allow  feasible  solutions  or  (ii)  adopting  general  approaches  used  in  the  context  of  heuristic  optimisation  such  as  penalisation.  Nonetheless,  from  a  theoretical  point  of  view,  little  progress  have  been  given  in  the  context  of  EDAs  when  developing  algorithms  designed  specifically  to  solve  constrained  problems.  In  this  paper,  we  propose  developing  EDAs  by  introducing  probability  models  defined  exclusively  on  the  space  of  feasible  solutions.  In  this  sense,  we  give  a  first  approach  by  taking  the  Graph  Partitioning  Problem  (GPP)  as  a  case  of  study,  and  present  a  probabilistic  model  defined  exclusively  on  the  feasible  region  of  solutions:  a  square  lattice  probability  model.  The  experiments  conducted  on  a  benchmark  of  22  artificial  instances  confirm  the  effectiveness  of  the  proposal  in  terms  of  quality  of  solutions  and  execution  time.
0	Can  deterministic  chaos  improve  differential  evolution  for  the  linear  ordering  problem.  Linear  ordering  problem  is  a  popular  NP-hard  combinatorial  optimization  problem  attractive  for  its  complexity,  rich  library  of  test  data,  and  variety  of  real  world  applications.  It  has  been  solved  by  a  number  of  heuristic  as  well  as  metaheuristic  methods  in  the  past.  The  implementation  of  nature-inspired  metaheuristic  optimization  and  search  methods  usually  depends  on  streams  of  integer  and  floating  point  numbers  generated  in  course  of  their  execution.  The  pseudo-random  numbers  are  utilized  for  an  in-silico  emulation  of  probability-driven  natural  processes  such  as  arbitrary  modification  of  genetic  information  (mutation,  crossover),  partner  selection,  and  survival  of  the  fittest  (selection,  migration)  and  environmental  effects  (small  random  changes  in  particle  motion  direction  and  velocity).  Deterministic  chaos  is  a  well  known  mathematical  concept  that  can  be  used  to  generate  sequences  of  seemingly  random  real  numbers  within  selected  interval  in  a  predictable  and  well  controllable  way.  In  the  past,  it  has  been  used  as  a  basis  for  various  pseudo-random  number  generators  with  interesting  properties.  Recently,  it  has  been  shown  that  it  can  be  successfully  used  as  a  source  of  stochasticity  for  nature-inspired  algorithms  solving  a  continuous  optimization  problem.  In  this  work  we  compare  effectiveness  of  the  differential  evolution  with  different  pseudo-random  number  generators  and  chaotic  systems  as  sources  of  stochasticity  when  solving  the  linear  ordering  problem.
0	Musical  genre  classification  by  means  of  fuzzy  rule  based  systems  a  preliminary  approach.  Musical  Genre  is  part  of  the  basic  information  required  for  classifying  musical  audio,  and  fundamental  for  music  information  retrieval  systems.  The  problem  of  automatic  musical  genre  detection  has  attracted  large  attention  in  the  last  decade,  due  to  the  emergence  of  digital  music  databases  and  Internet.  Although  a  number  of  techniques  has  been  applied  to  the  problem,  no  general  solution  still  exists,  due  to  the  imprecise  features  that  properly  define  musical  genre.  This  paper  presents  a  preliminary  attempt  to  apply  Fuzzy  Rule-Based  System  (FRBS)  in  cooperation  with  Evolutionary  Algorithms  to  musical  genre  classification.  The  novelty  of  the  approach  —  which  allows  us  to  use  fuzzy  information  extracted  from  audio  files  —  is  aligned  with  the  fuzzy  nature  of  the  problem  at  hand,  where  no  clear-cut  rules  are  available  for  the  classification.  Preliminary  results  presented  allows  to  foresee  the  potential  of  the  technique.
0	Aggressive  and  effective  feature  selection  using  genetic  programming.  One  of  the  major  challenges  in  automatic  classification  is  to  deal  with  highly  dimensional  data.  Several  dimensionality  reduction  strategies,  including  popular  feature  selection  metrics  such  as  Information  Gain  and  χ2,  have  already  been  proposed  to  deal  with  this  situation.  However,  these  strategies  are  not  well  suited  when  the  data  is  very  skewed,  a  common  situation  in  real-world  data  sets.  This  occurs  when  the  number  of  samples  in  one  class  is  much  larger  than  the  others,  causing  common  feature  selection  metrics  to  be  biased  towards  the  features  observed  in  the  largest  class.  In  this  paper,  we  propose  the  use  of  Genetic  Programming  (GP)  to  implement  an  aggressive,  yet  very  effective,  selection  of  attributes.  Our  GP-based  strategy  is  able  to  largely  reduce  dimensionality,  while  dealing  effectively  with  skewed  data.  To  this  end,  we  exploit  some  of  the  most  common  feature  selection  metrics  and,  with  GP,  combine  their  results  into  new  sets  of  features,  obtaining  a  better  unbiased  estimate  for  the  discriminative  power  of  each  feature.  Our  proposal  was  evaluated  against  each  individual  feature  selection  metric  used  in  our  GP-based  solution  (namely,  Information  Gain,  χ2,  Odds-Ratio,  Correlation  Coefficient)  using  a  k8  cancer-rescue  mutants  data  set,  a  very  unbalanced  collection  referring  to  examples  of  p53  protein.  For  this  data  set,  our  solution  not  only  increases  the  efficiency  of  the  learning  algorithms,  with  an  aggressive  reduction  of  the  input  space,  but  also  significantly  increases  its  accuracy.
0	Particle  swam  optimization  based  reliability  redundancy  allocation  in  a  type  2  fuzzy  environment.  In  this  paper,  we  have  addressed  the  reliability-redundancy  allocation  problem  with  a  particle  swam  optimization  based  technique.  The  parameters  of  the  system  components  are  actually  imprecise  or  uncertain  quantity  since  those  are  generally  guessed  by  the  designers  during  the  design-time.  Thus,  important  features  of  the  designed  system,  viz.  reliability,  costs,  weight  etc  very  suitably  qualifies  to  be  considered  as  fuzzy  quantity.  Our  problem  formulation  considers  these  parameters  as  type-2  fuzzy  quantity.  There  are  few  reports  where  the  problem  has  been  studied  under  type-1  fuzzy  uncertainty.  As  far  as  we  know,  no  research  has  been  reported  where  the  problem  has  been  addressed  with  a  particle  swam  optimization  based  approach  in  a  type-2  fuzzy  environment.  Suitable  examples  are  included  to  demonstrate  our  approach.  Results  are  compared  showing  that  the  type-2  fuzzy  uncertainty  based  approach  outperforms  other  recently  reported  results.
0	Eaten  entity  aware  attention  for  single  shot  visual  text  extraction.  Extracting  Text  of  Interest  (ToI)  from  images  is  a  crucial  part  of  many  OCR  applications,  such  as  entity  recognition  of  cards,  invoices,  and  receipts.  Most  of  the  existing  works  employ  complicated  engineering  pipeline,  which  contains  OCR  and  structure  information  extraction,  to  fulfill  this  task.  This  paper  proposes  an  Entity-aware  Attention  Text  Extraction  Network  called  EATEN,  which  is  an  end-to-end  trainable  system  to  extract  the  ToIs  without  any  post-processing.  In  the  proposed  framework,  each  entity  is  parsed  by  its  corresponding  entity-aware  decoder,  respectively.  Moreover,  we  innovatively  introduce  a  state  transition  mechanism  which  further  improves  the  robustness  of  visual  ToI  extraction.  In  consideration  of  the  absence  of  public  benchmarks,  we  construct  a  dataset  of  almost  0.6  million  images  in  three  real-world  scenarios  (train  ticket,  passport  and  business  card),  which  is  publicly  available  at  https://github.com/beacandler/EATEN.  To  the  best  of  our  knowledge,  EATEN  is  the  first  single  shot  method  to  extract  entities  from  images.  Extensive  experiments  on  these  benchmarks  demonstrate  the  state-of-the-art  performance  of  EATEN.
0	Evaluating  sequence  to  sequence  models  for  handwritten  text  recognition.  Encoder-decoder  models  have  become  an  effective  approach  for  sequence  learning  tasks  like  machine  translation,  image  captioning  and  speech  recognition,  but  have  yet  to  show  competitive  results  for  handwritten  text  recognition.  To  this  end,  we  propose  an  attention-based  sequence-to-sequence  model.  It  combines  a  convolutional  neural  network  as  a  generic  feature  extractor  with  a  recurrent  neural  network  to  encode  both  the  visual  information,  as  well  as  the  temporal  context  between  characters  in  the  input  image,  and  uses  a  separate  recurrent  neural  network  to  decode  the  actual  character  sequence.  We  make  experimental  comparisons  between  various  attention  mechanisms  and  positional  encodings,  in  order  to  find  an  appropriate  alignment  between  the  input  and  output  sequence.  The  model  can  be  trained  end-to-end  and  the  optional  integration  of  a  hybrid  loss  allows  the  encoder  to  retain  an  interpretable  and  usable  output,  if  desired.  We  achieve  competitive  results  on  the  IAM  and  ICFHR2016  READ  data  sets  compared  to  the  state-of-the-art  without  the  use  of  a  language  model,  and  we  significantly  improve  over  any  recent  sequence-to-sequence  approaches.
0	Cognitive  state  measurement  on  learning  materials  by  utilizing  eye  tracker  and  thermal  camera.  We  demonstrate  how  information  derived  from  pervasive  sensors  can  quantify  cognitive  states  of  learners  while  they  are  reading  a  textbook.  Eye  tracking  is  one  of  the  most  effective  approaches  to  measuring  reading  behavior.  For  example,  high  fixation  duration  represents  a  reader's  attention  on  a  document.  However,  it  is  still  a  challenging  task  to  predict  the  reason  for  the  attention  (i.e.,  is  it  because  of  his/her  interest  or  trouble  of  understanding?).  In  this  paper,  we  utilize  additional  sensing  modality  to  solve  the  problem.  On  the  dataset  of  12  high  school  students'  reading  behaviors,  we  have  found  that  the  changing  of  pupil  diameter  and  nose  temperature  are  highly  correlated  with  their  cognitive  states  including  their  interests  and  efforts  for  reading/solving  tasks  on  learning  materials  in  Physics.
0	Efficient  numerical  schemes  for  gradient  vector  flow.  Since  its  publication  more  than  10  years  ago,  the  gradient  vector  flow  (GVF)  technique  has  been  used  and  adapted  to  various  models  and  problems.  Its  effectiveness  has  greatly  contributed  to  its  popularity.  The  main  drawback  of  GVF  and  its  generalisation,  however,  is  their  expensive  computation  load  and  its  consequence  on  the  capture  range.  In  this  work,  we  propose  and  compare  different  efficient  numerical  schemes  to  solve  the  GVF  and  its  generalisations.
0	Compact  learning  for  multi  label  classification.  Abstract  Multi-label  classification  (MLC)  studies  the  problem  where  each  instance  is  associated  with  multiple  relevant  labels,  which  leads  to  the  exponential  growth  of  output  space.  It  confronts  with  the  great  challenge  for  the  exploration  of  the  latent  label  relationship  and  the  intrinsic  correlation  between  feature  and  label  spaces.  MLC  gave  rise  to  a  framework  named  label  compression  (LC)  to  obtain  a  compact  space  for  efficient  learning.  Nevertheless,  most  existing  LC  methods  failed  to  consider  the  influence  of  the  feature  space  or  misguided  by  original  problematic  features,  which  may  result  in  performance  degradation  instead.  In  this  paper,  we  present  a  compact  learning  (CL)  framework  to  embed  the  features  and  labels  simultaneously  and  with  mutual  guidance.  The  proposal  is  a  versatile  concept  that  does  not  rigidly  adhere  to  some  specific  embedding  methods,  and  is  independent  of  the  subsequent  learning  process.  Following  its  spirit,  a  simple  yet  effective  implementation  called  compact  multi-label  learning  (CMLL)  is  proposed  to  learn  a  compact  low-dimensional  representation  for  both  spaces.  CMLL  maximizes  the  dependence  between  the  embedded  spaces  of  the  labels  and  features,  and  minimizes  the  loss  of  label  space  recovery  concurrently.  Theoretically,  we  provide  a  general  analysis  for  different  embedding  methods.  Practically,  we  conduct  extensive  experiments  to  validate  the  effectiveness  of  the  proposed  method.
0	Visual  slam  for  robot  navigation  in  healthcare  facility.  The  COVID-19  pandemic  has  affected  many  countries,  posing  a  threat  to  human  health  and  safety,  and  putting  tremendous  pressure  on  the  medical  system.  This  paper  proposes  a  novel  SLAM  technology  using  RGB  and  depth  images  to  improve  hospital  operation  efficiency,  reduce  the  risk  of  doctor-patient  cross-infection,  and  curb  the  spread  of  the  COVID-19.  Most  current  visual  SLAM  researches  assume  that  the  environment  is  stationary,  which  makes  handling  real-world  scenarios  such  as  hospitals  a  challenge.  This  paper  proposes  a  method  that  effectively  deals  with  SLAM  problems  for  scenarios  with  dynamic  objects,  e.g.,  people  and  movable  objects,  based  on  the  semantic  descriptor  extracted  from  images  with  help  of  a  knowledge  graph.  Specifically,  our  method  leverages  a  knowledge  graph  to  construct  a  priori  movement  relationship  between  entities  and  establishes  high-level  semantic  information.  Built  upon  this  knowledge  graph,  a  semantic  descriptor  is  constructed  to  describe  the  semantic  information  around  key  points,  which  is  rotation-invariant  and  robust  to  illumination.  The  seamless  integration  of  the  knowledge  graph  and  semantic  descriptor  helps  eliminate  the  dynamic  objects  and  improves  the  accuracy  of  tracking  and  positioning  of  robots  in  dynamic  environments.  Experiments  are  conducted  using  data  acquired  from  healthcare  facilities,  and  semantic  maps  are  established  to  meet  the  needs  of  robots  for  delivering  medical  services.  In  addition,  to  compare  with  the  state-of-the-art  methods,  a  publicly  available  dataset  is  used  in  our  evaluation.  Compared  with  the  state-of-the-art  methods,  our  proposed  method  demonstrated  great  improvement  with  respect  to  both  accuracy  and  robustness  in  dynamic  environments.  The  computational  efficiency  is  also  competitive.
0	Tackling  mode  collapse  in  multi  generator  gans  with  orthogonal  vectors.  Abstract  Generative  Adversarial  Networks  (GANs)  have  been  widely  used  to  generate  realistic-looking  instances.  However,  training  robust  GAN  is  a  non-trivial  task  due  to  the  problem  of  mode  collapse.  Although  many  GAN  variants  are  proposed  to  overcome  this  problem,  they  have  limitations.  Those  existing  studies  either  generate  identical  instances  or  result  in  negative  gradients  during  training.  In  this  paper,  we  propose  a  new  approach  to  training  GAN  to  overcome  mode  collapse  by  employing  a  set  of  generators,  an  encoder  and  a  discriminator.  A  new  minimax  formula  is  proposed  to  simultaneously  train  all  components  in  a  similar  spirit  to  vanilla  GAN.  The  orthogonal  vector  strategy  is  employed  to  guide  multiple  generators  to  learn  different  information  in  a  complementary  manner.  In  this  way,  we  term  our  approach  Multi-Generator  Orthogonal  GAN  (MGO-GAN).  Specifically,  the  synthetic  data  produced  by  those  generators  are  fed  into  the  encoder  to  obtain  feature  vectors.  The  orthogonal  value  is  calculated  between  any  two  feature  vectors,  which  loyally  reflects  the  correlation  between  vectors.  Such  a  correlation  indicates  how  different  information  has  been  learnt  by  generators.  The  lower  the  orthogonal  value  is,  the  more  different  information  the  generators  learn.  We  minimize  the  orthogonal  value  along  with  minimizing  the  generator  loss  through  back-propagation  in  the  training  of  GAN.  The  orthogonal  value  is  integrated  with  the  original  generator  loss  to  jointly  update  the  corresponding  generator’s  parameters.  We  conduct  extensive  experiments  utilizing  MNIST,  CIFAR10  and  CelebA  datasets  to  demonstrate  the  significant  performance  improvement  of  MGO-GAN  in  terms  of  generated  data  quality  and  diversity  at  different  resolutions.
0	Amplitude  only  log  radon  transform  for  geometric  invariant  shape  descriptor.  A  shape  descriptor  combining  the  Radon  transform,  the  amplitude  extraction,  and  the  log-mapping  is  proposed  in  this  paper.  It  is  invariant  to  shape  rotation,  scaling,  and  translation.  Invariance  to  translation  is  achieved  by  amplitude  extraction  on  the  radial  coordinate.  Rotation  and  scaling  are  log-mapped  into  two-dimensional  translations  and  recovered  with  the  phase-only  correlation  function.  In  addition,  all  transformation  parameters  (rotating  angle,  scaling  factor,  and  position  shift)  can  be  determined  also.  The  efficiency  of  the  proposed  descriptor  compared  to  existing  methods  is  shown  experimentally  on  different  kind  of  datasets.  HighlightsWe  propose  a  new  shape  descriptor  based  on  Radon  transformThis  descriptor  needs  no  normalizations.It  is  invariant  to  geometric  transformations  (rotation  scale  and  translation)  and  robust  to  noise.The  rotation  angle,  the  scaling  factor  and  the  position  shift  can  be  easily  determined.
0	Rotated  cascade  r  cnn  a  shape  robust  detector  with  coordinate  regression.  Abstract  General  object  detection  task  mainly  takes  axis-aligned  bounding-boxes  as  the  detection  outputs.  To  address  more  challenging  scenarios,  such  as  curved  text  detection  and  multi-oriented  object  detection  in  aerial  images,  we  propose  a  novel  two-stage  approach  for  shape  robust  object  detection.  In  the  first  stage,  a  locally  sliding  line-based  point  regression  (LocSLPR)  approach  is  presented  to  estimate  the  outline  of  the  object,  which  is  denoted  as  the  intersections  of  the  sliding  lines  and  the  bounding-box  of  the  object.  To  make  full  use  of  information,  we  only  regress  partial  coordinates  and  calculate  the  remaining  coordinates  by  the  sliding  rule.  We  find  that  regression  can  achieve  higher  precision  with  fewer  parameters  than  the  segmentation  method.  In  the  second  stage,  a  rotated  cascade  region-based  convolutional  neural  network  (RCR-CNN)  is  used  to  gradually  regress  the  target  object,  which  can  further  improve  the  performance  of  our  system.  Experiments  demonstrate  that  our  method  achieves  state-of-the-art  performance  in  several  quadrangular  object  detection  tasks.  For  example,  our  method  yielded  a  score  of  0.796  in  the  ICPR  2018  Contest  on  Robust  Reading  for  Multi-Type  Web  Images,  where  we  won  first  place  for  text  detection  tasks.  The  method  also  achieved  69.2%  mAP  on  Task  1  of  the  ICPR  2018  Contest  on  Object  Detection  in  Aerial  Images,  which  was  our  best  single  model,  where  we  also  won  first  place.  In  addition,  the  method  outperforms  the  previously  published  best  record  on  the  curved  text  dataset  (CTW1500).
0	Semi  supervised  person  re  identification  using  multi  view  clustering.  Abstract  Person  Re-Identification  (Re-Id)  is  a  challenging  task  focusing  on  identifying  the  same  person  among  disjoint  camera  views.  A  number  of  deep  learning  algorithms  have  been  reported  for  this  task  in  fully-supervised  fashion  which  requires  a  large  amount  of  labeled  training  data,  while  obtaining  high  quality  labels  for  Re-Id  is  extremely  time  consuming.  To  address  this  problem,  we  propose  a  semi-supervised  Re-Id  framework  by  using  only  a  small  portion  of  labeled  data  and  some  additional  unlabeled  samples.  This  paper  approaches  the  problem  by  constructing  a  set  of  heterogeneous  Convolutional  Neural  Networks  (CNNs)  fine-tuned  using  the  labeled  portion,  and  then  propagating  the  labels  to  the  unlabeled  portion  for  further  fine-tuning  the  overall  system.  In  this  work,  label  estimation  is  a  key  component  during  the  propagation  process.  We  propose  a  novel  multi-view  clustering  method,  which  integrates  features  of  multiple  heterogeneous  CNNs  to  cluster  and  generate  pseudo  labels  for  unlabeled  samples.  Then  we  fine-tune  each  of  the  multiple  heterogeneous  CNNs  by  minimizing  an  identification  loss  and  a  verification  loss  simultaneously,  using  training  data  with  both  true  labels  and  pseudo  labels.  The  procedure  is  iterated  until  the  estimation  of  pseudo  labels  no  longer  changes.  Extensive  experiments  on  three  large-scale  person  Re-Id  datasets  demonstrate  the  effectiveness  of  the  proposed  method.
0	Efficient  multi  modal  fusion  on  supergraph  for  scalable  image  annotation.  Different  types  of  visual  features  provide  multi-modal  representation  for  images  in  the  annotation  task.  Conventional  graph-based  image  annotation  methods  integrate  various  features  into  a  single  descriptor  and  consider  one  node  for  each  descriptor  on  the  learning  graph.  However,  this  graph  does  not  capture  the  information  of  individual  features,  making  it  unsuitable  for  propagating  the  labels  of  annotated  images.  In  this  paper,  we  address  this  issue  by  proposing  an  approach  for  fusing  the  visual  features  such  that  a  specific  subgraph  is  constructed  for  each  visual  modality  and  then  subgraphs  are  connected  to  form  a  supergraph.  As  the  size  of  supergraph  grows  linearly  with  the  number  of  visual  features,  it  is  essential  to  handle  large  computational  complexity  of  label  propagation  on  the  supergraph.  To  this  end,  we  extract  some  prototypes  from  the  feature  vectors  of  images  and  incorporate  them  into  the  supergraph  construction.  The  learning  process  is  then  conducted  on  the  prototypes,  instead  of  a  large  number  of  feature  vectors,  making  the  label  inference  scalable.  The  experiments  on  a  wide  range  of  standard  datasets  reveal  that  the  proposed  approach  achieves  scalable  image  annotation  while  having  an  acceptable  level  of  performance.  HighlightsWe  construct  a  supergraph  to  structurally  combine  various  types  of  visual  features.The  main  challenge  of  learning  on  supergraph  is  its  large  computational  time.To  reach  scalability,  we  conduct  learning  on  a  small  prototype  graph  in  supergraph.Prototype  graph  is  a  good  replacement  for  sample  graph  during  label  propagation.We  achieve  good  performance  by  reconstructing  labels  of  images  from  prototypes.
0	A  simple  feature  combination  method  based  on  dominant  sets.  Abstract      Feature  combination  is  a  popular  method  for  improving  object  classification  performances.  In  this  paper  we  present  a  simple  and  effective  weighting  scheme  for  feature  combination  based  on  the  dominant-set  notion  of  a  cluster.  Specifically,  we  use  dominant  sets  clustering  to  evaluate  how  accurate  a  kernel  matrix  is  expected  to  be  for  a  SVM  classifier.  This  expected  kernel  accuracy  reflects  the  discriminative  power  of  the  kernel  matrix  and  thus  used  in  weighting  the  kernel  matrix  in  feature  combination.  Our  method  is  simple,  intuitive,  memory  and  computation  efficient,  and  performs  comparably  to  the  popular  and  sophisticated  optimization  based  methods.  We  conduct  experiments  with  several  datasets  of  diverse  object  types  and  validate  the  effectiveness  of  the  proposed  method.  In  fact,  in  five  out  of  the  six  datasets  used  in  our  experiments,  we  obtained  the  best  results  until  now  in  our  knowledge.
0	Stratified  sampling  for  feature  subspace  selection  in  random  forests  for  high  dimensional  data.  For  high  dimensional  data  a  large  portion  of  features  are  often  not  informative  of  the  class  of  the  objects.  Random  forest  algorithms  tend  to  use  a  simple  random  sampling  of  features  in  building  their  decision  trees  and  consequently  select  many  subspaces  that  contain  few,  if  any,  informative  features.  In  this  paper  we  propose  a  stratified  sampling  method  to  select  the  feature  subspaces  for  random  forests  with  high  dimensional  data.  The  key  idea  is  to  stratify  features  into  two  groups.  One  group  will  contain  strong  informative  features  and  the  other  weak  informative  features.  Then,  for  feature  subspace  selection,  we  randomly  select  features  from  each  group  proportionally.  The  advantage  of  stratified  sampling  is  that  we  can  ensure  that  each  subspace  contains  enough  informative  features  for  classification  in  high  dimensional  data.  Testing  on  both  synthetic  data  and  various  real  data  sets  in  gene  classification,  image  categorization  and  face  recognition  data  sets  consistently  demonstrates  the  effectiveness  of  this  new  method.  The  performance  is  shown  to  better  that  of  state-of-the-art  algorithms  including  SVM,  the  four  variants  of  random  forests  (RF,  ERT,  enrich-RF,  and  oblique-RF),  and  nearest  neighbor  (NN)  algorithms.
0	Iris  recognition  the  need  to  recognise  the  iris  as  a  dynamic  biological  system.  Highlights?  A  high  quality  database  can  detect  more  iris  changes  than  previous  databases.  ?  The  iris  changes  with  time  are  detectable  at  the  binary  code  level.  ?  Texture  grading  can  have  an  effect  on  recognition  rate.
0	Globally  consistent  alignment  for  planar  mosaicking  via  topology  analysis.  In  this  paper,  we  propose  a  generic  framework  for  globally  consistent  alignment  of  images  captured  from  approximately  planar  scenes  via  topology  analysis,  capable  of  resisting  the  perspective  distortion  meanwhile  preserving  the  local  alignment  accuracy.  Firstly,  to  estimate  the  topological  relations  of  images  efficiently,  we  search  for  a  main  chain  connecting  all  images  over  a  fast  built  similarity  table  of  image  pairs  (mainly  for  the  unordered  image  sequence),  along  which  the  potential  overlapping  pairs  are  incrementally  detected  according  to  the  gradually  recovered  geometric  positions  and  orientations.  Secondly,  all  the  sequential  images  are  organized  as  a  spanning  tree  through  applying  a  graph  algorithm  on  the  topological  graph,  so  as  to  find  the  optimal  reference  image  which  minimizes  the  total  number  of  error  propagation.  Thirdly,  the  global  alignment  under  topology  analysis  is  performed  in  the  strategy  that  images  are  initially  aligned  by  groups  via  the  affine  model,  followed  by  the  homography  refinement  under  the  anti-perspective  constraint,  which  manages  to  keep  the  optimal  balance  between  aligning  precision  and  global  consistency.  Finally,  experimental  results  on  two  challenging  aerial  image  sets  illustrate  the  superiority  of  the  proposed  approach.  HighlightsA  generic  framework  for  mosaicking  (ordered  and  unordered)  images  captured  from  an  approximately  planar  scene  is  proposed.Method  finding  a  valid  main  chain  of  unordered  images  efficiently  by  the  Minimum  Spanning  Tree  (MST)  algorithm  is  proposed.A  global  alignment  strategy  able  to  keep  an  optimal  balance  between  global  consistency  and  aligning  accuracy  is  proposed.
0	Attenuation  of  capsaicin  induced  ongoing  pain  and  secondary  hyperalgesia  during  exposure  to  an  immersive  virtual  reality  environment.  Introduction  There  is  growing  evidence  that  virtual  reality  (VR)  can  be  used  in  the  treatment  of  chronic  pain  conditions.  However,  further  research  is  required  to  better  understand  the  analgesic  mechanisms  during  sensitised  pain  states.  Objectives  We  examined  the  effects  of  an  immersive  polar  VR  environment  on  capsaicin-induced  ongoing  pain  and  secondary  hyperalgesia.  We  also  investigated  whether  the  degree  of  analgesia  was  related  to  baseline  conditioned  pain  modulation  (CPM)  responses.  Methods  Nineteen  subjects  had  baseline  CPM  and  electrical  pain  perception  (EPP)  thresholds  measured  before  the  topical  application  of  capsaicin  cream.  Visual  analogue  scale  ratings  were  measured  to  track  the  development  of  an  ongoing  pain  state,  and  EPP  thresholds  were  used  to  measure  secondary  hyperalgesia.  The  effects  of  a  passive  polar  VR  environment  on  ongoing  pain  and  secondary  hyperalgesia  were  compared  with  sham  VR  (ie,  2D  monitor  screen)  in  responders  to  capsaicin  (n  =  15).  Results  Virtual  reality  was  associated  with  a  transient  reduction  in  ongoing  pain  and  an  increase  in  EPP  thresholds  in  an  area  of  secondary  hyperalgesia.  Baseline  CPM  measurements  showed  a  significant  correlation  with  VR-induced  changes  in  secondary  hyperalgesia,  but  not  with  VR-induced  changes  in  ongoing  pain  perception.  There  was  no  correlation  between  VR-induced  changes  in  pain  perception  and  VR-induced  changes  in  secondary  hyperalgesia.  Conclusion  Virtual  reality  can  reduce  the  perception  of  capsaicin-induced  ongoing  pain  and  secondary  hyperalgesia.  We  also  show  that  CPM  may  provide  a  means  by  which  to  identify  individuals  likely  to  respond  to  VR  therapy.
0	Metric  learning  based  kernel  transformer  with  triplets  and  label  constraints  for  feature  fusion.  Abstract  Feature  fusion  is  an  important  skill  to  improve  the  performance  in  computer  vision,  the  difficult  problem  of  feature  fusion  is  how  to  learn  the  complementary  properties  of  different  features.  We  recognize  that  feature  fusion  can  benefit  from  kernel  metric  learning.  Thus,  a  metric  learning-based  kernel  transformer  method  for  feature  fusion  is  proposed  in  this  paper.  First,  we  propose  a  kernel  transformer  to  convert  data  from  data  space  to  kernel  space,  which  makes  feature  fusion  and  metric  learning  can  be  performed  in  the  transformed  kernel  space.  Second,  in  order  to  realize  supervised  learning,  both  triplets  and  label  constraints  are  embedded  into  our  model.  Third,  in  order  to  solve  the  unknown  kernel  matrices,  LogDet  divergence  is  also  introduced  into  our  model.  Finally,  a  complete  optimization  objective  function  is  formed.  Based  on  an  alternating  direction  method  of  multipliers  (ADMM)  solver  and  the  Karush-Kuhn-Tucker  (KKT)  theorem,  the  proposed  optimization  problem  is  solved  with  the  rigorous  theoretical  analysis.  Experimental  results  on  image  retrieval  demonstrate  the  effectiveness  of  the  proposed  methods.
0	Granular  multi  label  feature  selection  based  on  mutual  information.  We  granulate  the  label  space  into  information  granules  to  exploit  label  dependency.We  present  a  multi-label  maximal  correlation  minimal  redundancy  criterion.The  proposed  method  can  select  compact  and  specific  feature  subsets.The  proposed  method  can  significantly  improve  the  algorithm  performance.  Like  the  traditional  machine  learning,  the  multi-label  learning  is  faced  with  the  curse  of  dimensionality.  Some  feature  selection  algorithms  have  been  proposed  for  multi-label  learning,  which  either  convert  the  multi-label  feature  selection  problem  into  numerous  single-label  feature  selection  problems,  or  directly  select  features  from  the  multi-label  data  set.  However,  the  former  omit  the  label  dependency,  or  produce  too  many  new  labels  leading  to  learning  with  significant  difficulties;  the  latter,  taking  the  global  label  dependency  into  consideration,  usually  select  a  few  redundant  or  irrelevant  features,  because  actually  not  all  labels  depend  on  each  other,  which  may  confuse  the  algorithm  and  degrade  its  classification  performance.  To  select  a  more  relevant  and  compact  feature  subset  as  well  as  explore  the  label  dependency,  a  granular  feature  selection  method  for  multi-label  learning  is  proposed  with  a  maximal  correlation  minimal  redundancy  criterion  based  on  mutual  information.  The  maximal  correlation  minimal  redundancy  criterion  makes  sure  that  the  selected  feature  subset  contains  the  most  class-discriminative  information,  while  in  the  meantime  exhibits  the  least  intra-redundancy.  Granulation  can  help  explore  the  label  dependency.  We  study  the  relation  of  the  label  granularity  and  the  performance  on  four  data  sets,  and  compare  the  proposed  method  with  other  three  multi-label  feature  selection  methods.  The  experimental  results  demonstrate  that  the  proposed  method  can  select  compact  and  specific  feature  subsets,  improve  the  classification  performance  and  performs  better  than  other  three  methods  on  the  widely-used  multi-label  learning  evaluation  criteria.
0	Modeling  self  efficacy  across  age  groups  with  automatically  tracked  facial  expression.  Affect  plays  a  central  role  in  learning.  Students'  facial  expressions  are  key  indicators  of  affective  states  and  recent  work  has  increasingly  used  automated  facial  expression  tracking  technologies  as  a  method  of  affect  detection.  However,  there  has  not  been  an  investigation  of  facial  expressions  compared  across  age  groups.  The  present  study  collected  facial  expressions  of  college  and  middle  school  students  in  the  CRYSTAL  ISLAND  game-based  learning  environment.  Facial  expressions  were  tracked  using  the  Computer  Expression  Recognition  Toolbox  and  models  of  self-efficacy  for  each  age  group  highlighted  differences  in  facial  expressions.  Age-specific  findings  such  as  these  will  inform  the  development  of  enriched  affect  models  for  broadening  populations  of  learners  using  affect-sensitive  learning  environments.
0	Assessing  free  student  answers  in  tutorial  dialogues  using  lstm  models.  In  this  paper,  we  present  an  LSTM  approach  to  assess  free  short  answers  in  tutorial  dialogue  contexts.  A  major  advantage  of  the  proposed  method  is  that  it  does  not  require  any  sort  of  feature  engineering.  The  method  performs  on  par  and  even  slightly  better  than  existing  state-of-the-art  methods  that  rely  on  expert-engineered  features.
0	Hyper  parameter  optimization  in  classification  to  do  or  not  to  do.  Abstract  Hyper-parameter  optimization  is  a  process  to  find  suitable  hyper-parameters  for  predictive  models.  It  typically  incurs  highly  demanding  computational  costs  due  to  the  need  of  the  time-consuming  model  training  process  to  determine  the  effectiveness  of  each  set  of  candidate  hyper-parameter  values.  A  priori,  there  is  no  guarantee  that  hyper-parameter  optimization  leads  to  improved  performance.  In  this  work,  we  propose  a  framework  to  address  the  problem  of  whether  one  should  apply  hyper-parameter  optimization  or  use  the  default  hyper-parameter  settings  for  traditional  classification  algorithms.  We  implemented  a  prototype  of  the  framework,  which  we  use  a  basis  for  a  three-fold  evaluation  with  486  datasets  and  4  algorithms.  The  results  indicate  that  our  framework  is  effective  at  supporting  modeling  tasks  in  avoiding  adverse  effects  of  using  ineffective  optimizations.  The  results  also  demonstrate  that  incrementally  adding  training  datasets  improves  the  predictive  performance  of  framework  instantiations  and  hence  enables  “life-long  learning.”
0	Human  trajectory  prediction  and  generation  using  lstm  models  and  gans.  Abstract  Human  trajectory  prediction  is  an  important  topic  in  several  application  domains,  ranging  from  self-driving  cars  to  environment  design  and  planning,  from  socially-aware  robots  to  intelligent  tracking  systems.  This  complex  subject  comes  with  different  challenges,  such  as  human-space  interaction,  human-human  interaction,  multimodality,  and  generalizability.  Currently,  these  challenges,  especially  generalizability,  have  not  been  completely  explored  by  state-of-the-art  works.  This  work  attempts  to  fill  this  gap  by  proposing  and  defining  new  methods  and  metrics  to  help  understand  trajectories.  In  particular,  new  deep  learning  models  based  on  Long  Short-Term  Memory  and  Generative  Adversarial  Network  architectures  are  used  in  both  unimodal  and  multimodal  contexts.  These  approaches  are  evaluated  with  new  error  metrics,  which  normalize  some  biases  in  standard  metrics.  Tests  have  been  assessed  using  newly  collected  datasets  characterized  by  a  higher  diversity  and  lower  linearity  than  those  used  in  state-of-the-art  works.  The  results  prove  that  the  proposed  models  and  datasets  are  comparable  to  and  yield  better  generalizability  than  state-of-the-art  works.  Moreover,  we  also  prove  that  our  datasets  better  represent  multimodal  scenarios  (allowing  for  multiple  possible  behaviors)  and  that  human  trajectories  are  moderately  influenced  by  their  spatial  region  and  slightly  influenced  by  their  date  and  time.
0	How  do  learners  behave  in  help  seeking  when  given  a  choice.  We  describe  the  results  of  a  study  that  investigated  learners’  help-seeking  behavior  using  two  feedback  options  implemented  in  an  ITS  for  Java  programming.  The  25  students  had  the  choice  between  asking  for  feedback  on  errors  in  their  programs  and  feedback  on  possible  next  steps  in  the  solution  process.  We  hypothesized  that  learners’  choices  would  depend  on  correctness  of  their  programs  and  their  progress  in  problem-solving.  Surprisingly,  this  hypothesis  was  not  confirmed.
0	A  categorical  semantics  for  causal  structure.  We  present  a  categorical  construction  for  modelling  both  definite  and  indefinite  causal  structures  within  a  general  class  of  process  theories  that  include  classical  probability  theory  and  quantum  theory.  Unlike  prior  constructions  within  categorical  quantum  mechanics,  the  objects  of  this  theory  encode  fine-grained  causal  relationships  between  subsystems  and  give  a  new  method  for  expressing  and  deriving  consequences  for  a  broad  class  of  causal  structures.  To  illustrate  this  point,  we  show  that  this  framework  admits  processes  with  definite  causal  structures,  namely  one-way  signalling  processes,  non-signalling  processes,  and  quantum  n-combs,  as  well  as  processes  with  indefinite  causal  structure,  such  as  the  quantum  switch  and  the  process  matrices  of  Oreshkov,  Costa,  and  Brukner.  We  furthermore  give  derivations  of  their  operational  behaviour  using  simple,  diagrammatic  axioms.
0	Better  algorithms  for  benign  bandits.  The  online  multi-armed  bandit  problem  and  its  generalizations  are  repeated  decision  making  problems,  where  the  goal  is  to  select  one  of  several  possible  decisions  in  every  round,  and  incur  a  cost  associated  with  the  decision,  in  such  a  way  that  the  total  cost  incurred  over  all  iterations  is  close  to  the  cost  of  the  best  fixed  decision  in  hindsight.  The  difference  in  these  costs  is  known  as  the  regret  of  the  algorithm.  The  term  bandit  refers  to  the  setting  where  one  only  obtains  the  cost  of  the  decision  used  in  a  given  iteration  and  no  other  information.  A  very  general  form  of  this  problem  is  the  non-stochastic  bandit  linear  optimization  problem,  where  the  set  of  decisions  is  a  convex  set  in  some  Euclidean  space,  and  the  cost  functions  are  linear.  Only  recently  an  efficient  algorithm  attaining  O(√T)  regret  was  discovered  in  this  setting.  In  this  paper  we  propose  a  new  algorithm  for  the  bandit  linear  optimization  problem  which  obtains  a  tighter  regret  bound  of  O(√Q),  where  Q  is  the  total  variation  in  the  cost  functions.  This  regret  bound,  previously  conjectured  to  hold  in  the  full  information  case,  shows  that  it  is  possible  to  incur  much  less  regret  in  a  slowly  changing  environment  even  in  the  bandit  setting.  Our  algorithm  is  efficient  and  applies  several  new  ideas  to  bandit  optimization  such  as  reservoir  sampling.
0	Local  network  community  detection  with  continuous  optimization  of  conductance  and  weighted  kernel  k  means.  Local  network  community  detection  is  the  task  of  finding  a  single  community  of  nodes  concentrated  around  few  given  seed  nodes  in  a  localized  way.  Conductance  is  a  popular  objective  function  used  in  many  algorithms  for  local  community  detection.  This  paper  studies  a  continuous  relaxation  of  conductance.  We  show  that  continuous  optimization  of  this  objective  still  leads  to  discrete  communities.  We  investigate  the  relation  of  conductance  with  weighted  kernel  k-means  for  a  single  community,  which  leads  to  the  introduction  of  a  new  objective  function,  σ-conductance.  Conductance  is  obtained  by  setting  σ  to  0.  Two  algorithms,  EMc  and  PGDc,  are  proposed  to  locally  optimize  σ-conductance  and  automatically  tune  the  parameter  σ.  They  are  based  on  expectation  maximization  and  projected  gradient  descent,  respectively.  We  prove  locality  and  give  performance  guarantees  for  EMc  and  PGDc  for  a  class  of  dense  and  well  separated  communities  centered  around  the  seeds.  Experiments  are  conducted  on  networks  with  ground-truth  communities,  comparing  to  state-of-the-art  graph  diffusion  algorithms  for  conductance  optimization.  On  large  graphs,  results  indicate  that  EMc  and  PGDc  stay  localized  and  produce  communities  most  similar  to  the  ground,  while  graph  diffusion  algorithms  generate  large  communities  of  lower  quality.
0	New  learning  methods  for  supervised  and  unsupervised  preference  aggregation.  In  this  paper  we  present  a  general  treatment  of  the  preference  aggregation  problem,  in  which  multiple  preferences  over  objects  must  be  combined  into  a  single  consensus  ranking.  We  consider  two  instances  of  this  problem:  unsupervised  aggregation  where  no  information  about  a  target  ranking  is  available,  and  supervised  aggregation  where  ground  truth  preferences  are  provided.  For  each  problem  class  we  develop  novel  learning  methods  that  are  applicable  to  a  wide  range  of  preference  types.  Specifically,  for  unsupervised  aggregation  we  introduce  the  Multinomial  Preference  model  (MPM)  which  uses  a  multinomial  generative  process  to  model  the  observed  preferences.  For  the  supervised  problem  we  develop  a  supervised  extension  for  MPM  and  then  propose  two  fully  supervised  models.  The  first  model  employs  SVD  factorization  to  derive  effective  item  features,  transforming  the  aggregation  problems  into  a  learning-to-rank  one.  The  second  model  aims  to  eliminate  the  costly  SVD  factorization  and  instantiates  a  probabilistic  CRF  framework,  deriving  unary  and  pairwise  potentials  directly  from  the  observed  preferences.  Using  a  probabilistic  framework  allows  us  to  directly  optimize  the  expectation  of  any  target  metric,  such  as  NDCG  or  ERR.  All  the  proposed  models  operate  on  pairwise  preferences  and  can  thus  be  applied  to  a  wide  range  of  preference  types.  We  empirically  validate  the  models  on  rank  aggregation  and  collaborative  filtering  data  sets  and  demonstrate  superior  empirical  accuracy.
0	A  comparison  of  the  lasso  and  marginal  regression.  The  lasso  is  an  important  method  for  sparse,  high-dimensional  regression  problems,  with  efficient  algorithms  available,  a  long  history  of  practical  success,  and  a  large  body  of  theoretical  results  supporting  and  explaining  its  performance.  But  even  with  the  best  available  algorithms,  finding  the  lasso  solutions  remains  a  computationally  challenging  task  in  cases  where  the  number  of  covariates  vastly  exceeds  the  number  of  data  points.    Marginal  regression,  where  each  dependent  variable  is  regressed  separately  on  each  covariate,  offers  a  promising  alternative  in  this  case  because  the  estimates  can  be  computed  roughly  two  orders  faster  than  the  lasso  solutions.  The  question  that  remains  is  how  the  statistical  performance  of  the  method  compares  to  that  of  the  lasso  in  these  cases.    In  this  paper,  we  study  the  relative  statistical  performance  of  the  lasso  and  marginal  regression  for  sparse,  high-dimensional  regression  problems.  We  consider  the  problem  of  learning  which  coefficients  are  non-zero.  Our  main  results  are  as  follows:  (i)  we  compare  the  conditions  under  which  the  lasso  and  marginal  regression  guarantee  exact  recovery  in  the  fixed  design,  noise  free  case;  (ii)  we  establish  conditions  under  which  marginal  regression  provides  exact  recovery  with  high  probability  in  the  fixed  design,  noise  free,  random  coefficients  case;  and  (iii)  we  derive  rates  of  convergence  for  both  procedures,  where  performance  is  measured  by  the  number  of  coefficients  with  incorrect  sign,  and  characterize  the  regions  in  the  parameter  space  recovery  is  and  is  not  possible  under  this  metric.    In  light  of  the  computational  advantages  of  marginal  regression  in  very  high  dimensional  problems,  our  theoretical  and  simulations  results  suggest  that  the  procedure  merits  further  study.
0	Design  and  analysis  of  the  nips  2016  review  process.  Neural  Information  Processing  Systems  (NIPS)  is  a  top-tier  annual  conference  in  machine  learning.  The  2016  edition  of  the  conference  comprised  more  than  2,400  paper  submissions,  3,000  reviewers,  and  8,000  attendees.  This  represents  a  growth  of  nearly  40%  in  terms  of  submissions,  96%  in  terms  of  reviewers,  and  over  100%  in  terms  of  attendees  as  compared  to  the  previous  year.  The  massive  scale  as  well  as  rapid  growth  of  the  conference  calls  for  a  thorough  quality  assessment  of  the  peer-review  process  and  novel  means  of  improvement.  In  this  paper,  we  analyze  several  aspects  of  the  data  collected  during  the  review  process,  including  an  experiment  investigating  the  efficacy  of  collecting  ordinal  rankings  from  reviewers.  We  make  a  number  of  key  observations,  provide  suggestions  that  may  be  useful  for  subsequent  conferences,  and  discuss  open  problems  towards  the  goal  of  improving  peer  review.
0	Tslearn  a  machine  learning  toolkit  for  time  series  data.  tslearn  is  a  general-purpose  Python  machine  learning  library  for  time  series  that  offers  tools  for  pre-processing  and  feature  extraction  as  well  as  dedicated  models  for  clustering,  classification  and  regression.  It  follows  scikit-learn's  Application  Programming  Interface  for  transformers  and  estimators,  allowing  the  use  of  standard  pipelines  and  model  selection  tools  on  top  of  tslearn  objects.  It  is  distributed  under  the  BSD-2-Clause  license,  and  its  source  code  is  available  at  https://github.com/tslearn-team/tslearn.
0	A  mobile  framework  for  personalized  diabetes  telecare.  The  latest  estimates  for  worldwide  diabetic  population  is  about  366  million,  with  the  expected  diabetic  population  in  2030  at  552  million.  The  steadily  increasing  diabetic  population  and  strained  health  resources  call  for  a  more  efficient  approach  to  diabetes  care.  In  the  past  few  years,  smart  phones  have  become  the  mainstream  communications  device,  especially  for  the  younger  generation.  Coupled  with  the  increased  ease  in  developing  and  deploying  mobile  applications,  there  has  been  a  surge  in  medical  and  health  related  mobile  programs.  These  modern  smart  phones  provide  a  readily  available  platform  for  deploying  healthcare  related  services.  By  employing  their  computing  resources  and  connectivity,  health  institutions  can  easily  provide  telehealth  services  to  chronically  ill  patients.  In  this  work,  we  put  forward  a  mobile  framework  for  providing  personalized  diabetes  telehealth.  Through  leveraging  the  power  of  smart  phones  and  wireless  connectivity,  better  care  and  improved  quality  of  life  can  be  provided  for  diabetic  patients  by  this  framework.
0	A  deep  cnn  approach  with  transfer  learning  for  image  recognition.  This  paper  presents  a  model  of  Deep  Convolutional  Neural  Networks  (CNN)  based  on  transfer  learning  for  image  recognition.  This  means  to  use  a  Deep  CNN  system  pretrained  on  the  large  ImageNet  dataset  of  14  million  images  and  1000  classes  in  order  to  learn  feature  selection.  The  results  of  the  pretraining  phase  are  transferred  to  the  problem  of  classification  for  the  images  belonging  to  the  UC  Merced  Land  Use  dataset  with  21  classes.  As  benchmark,  we  have  considered  a  Deep  CNN  trained  with  a  fraction  of  the  same  UC  Merced  Land  Use  dataset  containing  the  test  images  for  classification.  The  experimental  results  have  pointed  out  the  obvious  advantage  of  the  Deep  CNN  with  transfer  learning  (accuracy  of  0.87  using  pretraining  over  0.46  for  fully  training  on  the  same  dataset).
0	Subjectivity  and  sentiment  analysis  an  overview  of  the  current  state  of  the  area  and  envisaged  developments.  In  this  introduction,  we  present  an  overview  of  the  current  state  of  research  in  the  Natural  Language  Processing  tasks  of  subjectivity  and  sentiment  analysis,  as  well  as  their  application  domains  and  closely-related  research  field  of  emotion  detection.  Although  many  definitions  exist  for  these  tasks  and  the  research  done  within  their  frame  spans  over  approaches  with  different  objectives,  we  consider  subjectivity  analysis  to  deal  with  the  detection  of  ''private  states''  (opinions,  emotions,  sentiments,  beliefs,  speculations)  and  sentiment  analysis  as  the  task  of  detecting,  extracting  and  classifying  opinions  and  sentiments  concerning  different  topics,  as  expressed  in  textual  input.  After  describing  the  key  concepts  and  research  directions  in  these  tasks,  we  present  the  main  achievements  obtained  so  far  and  the  issues  that  remain  to  be  tackled.  Subsequently,  we  introduce  each  of  the  papers  in  this  volume  and  present  their  contribution  to  the  research  areas  of  subjectivity  and  sentiment  analysis.  Finally,  we  conclude  on  the  present  state  of  work  in  these  fields  and  reflect  on  the  possible  future  developments.
0	Tracking  geographical  locations  using  a  geo  aware  topic  model  for  analyzing  social  media  data.  Abstract      Tracking  how  discussion  topics  evolve  in  social  media  and  where  these  topics  are  discussed  geographically  over  time  has  the  potential  to  provide  useful  information  for  many  different  purposes.  In  crisis  management,  knowing  a  specific  topic's  current  geographical  location  could  provide  vital  information  to  where,  or  even  which,  resources  should  be  allocated.  This  paper  describes  an  attempt  to  track  online  discussions  geographically  over  time.  A  distributed  geo-aware  streaming  latent  Dirichlet  allocation  model  was  developed  for  the  purpose  of  recognizing  topics'  locations  in  unstructured  text.  To  evaluate  the  model  it  has  been  implemented  and  used  for  automatic  discovery  and  geographical  tracking  of  election  topics  during  parts  of  the  2016  American  presidential  primary  elections.  It  was  shown  that  the  locations  correlated  with  the  actual  election  locations,  and  that  the  model  provides  a  better  geolocation  classification  compared  to  using  a  keyword-based  approach.
0	Analyzing  slavic  textual  sentiment  using  deep  convolutional  neural  networks.  In  this  paper  we  deploy  a  deep  architecture  for  convolutional  neural  networks  for  understanding  of  Croatian.  We  follow  the  same  approach  as  in  [34],  and  we  use  deep  learning  on  character  inputs  on  a  sentiment  analysis  dataset  in  Croatian.  Although  we  have  archived  considerable  results  (without  any  complex  parsing  or  background  knowledge),  the  result  was  inferior  to  that  reported  in  the  abovementioned  paper.  As  Croatian  is  one  of  the  low-resource  languages,  there  are  considerable  links  between  using  such  an  approach  (that  maximizes  the  role  of  data)  and  sustainability.  The  main  objective  of  this  chapter  is  to  give  a  clear  understanding  of  the  position  of  low-resource  languages  and  propose  a  direction  for  sustainable  development  of  language  technologies  illustrated  using  convolutional  neural  networks  for  textual  sentiment  analysis  .  Impact  of  this  research  to  scientific  but  also  business  community  is  significant  due  to  fact  that  every  method  with  acceptable  ratio  of  simplicity  and  effectiveness  can  be  included  inside  more  complex  logic  environment  focusing  on  specific  language  or  appliance  in  specific  area.  Since  language  structure  is  generally  not  easy  to  manage,  there  is  constant  need  for  improvement  of  tools  used  to  score  text  data  especially  while  majority  of  unstructured  data  analysis  tools  often  transfer  various  data  to  text  and  create  further  analysis  paths  form  there.
0	Mindfully  going  omni  channel  an  economic  decision  model  for  evaluating  omni  channel  strategies.  Abstract  In  the  digital  age,  customers  want  to  define  on  their  own  how  to  interact  with  organizations  during  their  customer  journeys.  Thus,  many  organizations  struggle  to  implement  an  omni-channel  strategy  (OCS)  that  meets  their  customers'  channel  preferences  and  can  be  operated  efficiently.  Despite  this  high  practical  need,  research  on  omni-channel  management  predominantly  takes  a  descriptive  perspective.  What  is  missing  is  prescriptive  knowledge  that  guides  organizations  in  the  valuation  and  selection  of  an  appropriate  OCS.  Most  existing  studies  investigate  single  facets  of  omni-channel  management  in  detail  while  neglecting  the  big  picture.  They  also  require  customer  journeys  to  follow  sequential  and  organization-defined  purchase  decision  processes.  To  address  this  research  gap,  we  propose  an  economic  decision  model  that  considers  online  and  offline  channels,  the  opening  and  closing  of  channels,  non-sequential  customer  journeys,  and  customers'  channel  preferences.  Drawing  from  the  principles  of  value-based  management,  the  decision  model  recommends  choosing  the  OCS  with  the  highest  contribution  to  an  organization's  long-term  firm  value.  We  applied  and  validated  the  decision  model  based  on  real-world  data  from  a  German  bank.
0	Internet  based  knowledge  acquisition.  Internet  tools  used  as  knowledge  retrieval  mechanisms  can  be  beneficial  for  knowledge  acquisition  (KA).  This  study  applies  the  concepts  of  decisional  guidance  and  restrictiveness  to  three  commonly  used  tools  to  predict  perceived  information  overload,  task  quality,  and  task  speed  for  tasks  that  differ  in  complexity.  In  an  experimental  setting  we  find  that  less  restrictive  pull  systems  (i.e.,  web  directories  and  Google  search)  increased  quality  in  high-complexity  tasks  over  more  restrictive  push  systems  (i.e.,  listservs)  and  lowered  perceived  information  overload  in  both  low-  and  high-complexity  tasks.  In  low-complexity  tasks,  subjects  using  predefined  guidance  (i.e.,  listservs  and  web  directories)  performed  better  and  faster  than  those  using  participative  guidance  (i.e.,  Google  search).  In  high-complexity  tasks,  participative  guidance  provided  lower  perceived  information  overload  and  higher  task  quality.  Implications  for  research  and  practice  are  discussed.  Highlights?  In  high-complexity  tasks,  less  restrictive  pull  systems  (i.e.,  web  directories  and  Google  search)  increased  quality  and  lowered  perceived  information  overload  over  more  restrictive  push  systems  (i.e.,  listservs).  ?  In  low-complexity  tasks  less  restrictive  pull  systems  lowered  perceived  information  overload.  ?  In  low-complexity  tasks,  predefined  guidance  (i.e.,  listservs  and  web  directories)  performed  better  and  faster  than  those  using  participative  guidance  (i.e.,  Google  search).  ?  In  high-complexity  tasks,  participative  guidance  provided  lower  perceived  information  overload  and  higher  task  quality  than  predefined  guidance.
0	A  novel  gsp  auction  mechanism  for  ranking  bitcoin  transactions  in  blockchain  mining.  Abstract  Bitcoin  is  gaining  ground  in  recent  years.  In  the  Bitcoin  system,  miners  provide  computing  power  to  confirm  transactions  and  mine  blocks  in  pursuit  of  transaction  fees,  while  users  compete  by  bidding  transaction  fees  for  faster  confirmation.  This  process  is  in  essence  analogous  to  online  ad  auctions,  where  advertisers  bid  for  more  prominent  ad  slots.  Therefore,  inspired  by  the  Generalized  Second  Price  (GSP)  mechanism  dominantly  used  in  online  ad  auctions,  we  propose  to  adopt  the  GSP  auction  model  in  the  Bitcoin  transaction  confirmation  game.  Also,  we  use  weighted  fees  as  the  new  ranking  basis,  which  can  be  calculated  by  user-submitted  fees,  transaction  size,  quality  scores  and  virtual  fees  accumulated  from  the  waiting  time.  We  show  that  the  formulated  static  GSP  transaction  fee  auction  with  complete  information  has  a  unique  Pure  Strategy  Nash  Equilibrium.  Then,  we  discuss  the  impacts  of  quality  scores  and  virtual  fees  on  users'  equilibrium  fee  decisions  and  payoffs.  Finally,  computational  experiments  are  designed  to  validate  our  theoretical  models  and  analysis.  Our  research  findings  indicate  that  this  novel  GSP  mechanism  is  superior  to  the  currently  adopted  GFP  mechanism,  and  can  help  users  save  fees.  Besides,  quality  scores  and  virtual  fees  are  also  proven  to  be  effective  on  reducing  users'  paid  fees.  Moreover,  the  design  of  virtual  fees  allows  all  transactions  to  be  processed  more  efficiently  in  a  uniform  pipeline,  and  the  interests  of  transactions  with  and  without  associated  fees  are  taken  into  consideration.
0	Arabic  part  of  speech  tagging.  The  study  described  in  this  paper  belongs  to  the  area  of  computational  linguistics.  Computational  linguistics  is  a  field  of  artificial  intelligence  dealing  with  the  logical  modeling  of  natural  language  from  a  computational  perspective.  It  unites  two  areas  that  are  quite  different  in  appearance,  computer  science  and  natural  languages.  Computational  linguistics  might  be  considered  as  a  synonym  of  automatic  processing  of  natural  language,  since  the  main  task  of  computational  linguistics  is  just  the  construction  of  computer  programs  to  process  words  and  texts  in  natural  language.  There  are  many  areas  that  may  be  considered  as  properly  included  within  the  discipline  of  computational  linguistics.  One  of  these  areas  is  part-of-speech  tagging  (POS-tagging).  POS-tagging  is  considered  as  a  process  for  automatically  assigning  the  proper  grammatical  tag  to  each  word  of  a  written  text  according  to  its  appearance  on  the  text.  Thus,  the  task  of  POS-tagging  is  attaching  appropriate  grammatical  or  morpho-syntactical  category  labels  to  each  word,  token,  symbol,  abbreviation  and  even  punctuation  mark  in  a  corpus.  POS-tagging  is  usually  the  first  step  in  linguistic  analysis.  Also,  it  is  very  important  intermediate  step  to  build  many  natural  language  processing  applications.  It  could  be  used  in  spell  checking  and  correcting  systems,  speech  recognition  systems,  information  retrieval  systems  and  text-to-speech  synthesis  systems.
0	Computational  framework  based  on  task  and  resource  scheduling  for  micro  grid  design.  Within  micro  grid  scenarios,  optimal  energy  management  represents  an  important  paradigm  to  improve  the  grid  efficiency  while  lowering  its  burden.  While  usually  real  time  energy  management  is  considered,  an  offline  approach  can  be  also  adopted  to  maximize  the  grid  efficiency  in  certain  contexts.  Indeed,  by  evaluating  the  energy  management  performance  according  to  the  user  needs,  it  is  possible  to  asses  which  technologies  allow  the  overall  system  to  operate  at  its  best,  given  the  expected  load  level.  From  this  perspective,  a  computational  framework  based  on  the  "Mixed-Integer  Linear  Programming"  paradigm  has  been  proposed  in  this  paper  as  a  tool  to  simulate  the  micro  grid  behaviour  in  terms  of  energy  consumption  and  in  dependence  on  the  technology  of  choice.  By  modelling  the  energy  production  and  storage  means,  the  pool  of  electricity  tasks,  and  the  thermal  behaviour  of  the  building,  suitable  energy  management  policies  for  the  micro  grid  scenario  under  study  can  be  developed  and  tested  in  different  operating  conditions  and  time  horizons.  Moreover,  the  forecasting  paradigm  has  been  integrated  into  the  framework  to  deal  with  data  uncertainty,  and  a  Neural  Network  approach  has  been  employed  on  purpose.  Performed  computer  simulations,  related  to  a  six-apartments  building  scenario,  have  proven  that  the  suggested  framework  can  fruitfully  be  adopted  to  assess  the  effectiveness  of  different  technical  solutions  in  terms  of  overall  energy  cost,  thus  supporting  the  decisional  process  occurring  during  the  micro  grid  design.
0	Ensemble  neural  network  rule  extraction  using  re  rx  algorithm.  In  this  paper,  we  propose  a  feed-forward  ensemble  neural  network  for  data  sets  having  both  discrete  and  continuous  attributes.  The  ensemble  provides  results  that  are  more  accurate  than  those  of  conventional  neural  networks  and  expresses  more  comprehensible  rules.  Through  the  separation  of  data  in  compliance  with  primary  rules,  it  enables  the  generation  of  secondary  rules  that  apply  solely  to  instances  of  non-compliance  with  the  primary  rules  and  maintain  higher  accuracy  than  is  conventionally  attainable.  We  demonstrate  the  high  performance  of  the  ensemble  neural  network  with  rules  extracted  by  Re-RX,  and  verify  that  it  can  reduce  the  complexity  of  handling  multiple  neural  networks.
0	Learning  probability  distributions  over  permutations  by  means  of  fourier  coefficients.  An  increasing  number  of  data  mining  domains  consider  data  that  can  be  represented  as  permutations.  Therefore,  it  is  important  to  devise  new  methods  to  learn  predictive  models  over  datasets  of  permutations.  However,  maintaining  probability  distributions  over  the  space  of  permutations  is  a  hard  task  since  there  are  n!  permutations  of  n  elements.  The  Fourier  transform  has  been  successfully  generalized  to  functions  over  permutations.  One  of  its  main  advantages  in  the  context  of  probability  distributions  is  that  it  compactly  summarizes  approximations  to  functions  by  discarding  high  order  marginals  information.  In  this  paper,  we  present  a  method  to  learn  a  probability  distribution  that  approximates  the  generating  distribution  of  a  given  sample  of  permutations.  In  particular,  this  method  learns  the  Fourier  domain  information  representing  this  probability  distribution.
0	A  neural  path  integration  mechanism  for  adaptive  vector  navigation  in  autonomous  agents.  Animals  show  remarkable  capabilities  in  navigating  their  habitat  in  a  fully  autonomous  and  energy-efficient  way.  In  many  species,  these  capabilities  rely  on  a  process  called  path  integration,  which  enables  them  to  estimate  their  current  location  and  to  find  their  way  back  home  after  long-distance  journeys.  Path  integration  is  achieved  by  integrating  compass  and  odometric  cues.  Here  we  introduce  a  neural  path  integration  mechanism  that  interacts  with  a  neural  locomotion  control  to  simulate  homing  behavior  and  path  integration-related  behaviors  observed  in  animals.  The  mechanism  is  applied  to  a  simulated  six-legged  artificial  agent.  Input  signals  from  an  allothetic  compass  and  odometry  are  sustained  through  leaky  neural  integrator  circuits,  which  are  then  used  to  compute  the  home  vector  by  local  excitation-global  inhibition  interactions.  The  home  vector  is  computed  and  represented  in  circular  arrays  of  neurons,  where  compass  directions  are  population-coded  and  linear  displacements  are  rate-coded.  The  mechanism  allows  for  robust  homing  behavior  in  the  presence  of  external  sensory  noise.  The  emergent  behavior  of  the  controlled  agent  does  not  only  show  a  robust  solution  for  the  problem  of  autonomous  agent  navigation,  but  it  also  reproduces  various  aspects  of  animal  navigation.  Finally,  we  discuss  how  the  proposed  path  integration  mechanism  may  be  used  as  a  scaffold  for  spatial  learning  in  terms  of  vector  navigation.
0	Point  cloud  data  filtering  and  downsampling  using  growing  neural  gas.  3D  sensors  provide  valuable  information  for  mobile  robotic  tasks  like  scene  classification  or  object  recognition,  but  these  sensors  often  produce  noisy  data  that  makes  impossible  applying  classical  keypoint  detection  and  feature  extraction  techniques.  Therefore,  noise  removal  and  downsampling  have  become  essential  steps  in  3D  data  processing.  In  this  work,  we  propose  the  use  of  a  3D  filtering  and  downsampling  technique  based  on  a  Growing  Neural  Gas  (GNG)  network.  GNG  method  is  able  to  deal  with  outliers  presents  in  the  input  data.  These  features  allows  to  represent  3D  spaces,  obtaining  an  induced  Delaunay  Triangulation  of  the  input  space.  Experiments  show  how  GNG  method  yields  better  input  space  adaptation  to  noisy  data  than  other  filtering  and  downsampling  methods  like  Voxel  Grid.  It  is  also  demonstrated  how  the  state-of-the-art  keypoint  detectors  improve  their  performance  using  filtered  data  with  GNG  network.  Descriptors  extracted  on  improved  keypoints  perform  better  matching  in  robotics  applications  as  3D  scene  registration.
0	A  computational  model  for  motor  learning  in  insects.  The  aim  of  this  paper  is  to  propose  a  computational  model,  inspired  by  Drosophila  melanogaster,  able  to  handle  problems  related  to  motor  learning.  The  role  of  the  Mushroom  Bodies  and  the  Central  Complex  in  solving  this  problem  is  analyzed  and  plausible  biologically  inspired  models  are  proposed.  The  designed  computational  models  have  been  evaluated  in  simulation  using  a  dynamic  structure  inspired  by  the  fruit  fly.  The  obtained  results  open  the  way  to  new  neurobiological  experiments  focused  to  better  understand  the  underlined  mechanisms  involved,  to  verify  the  feasibility  of  the  hypotheses  formulated  and  the  significance  of  the  obtained  results.
0	Unsupervised  learning  of  event  based  image  recordings  using  spike  timing  dependent  plasticity.  The  remarkable  versatility  and  efficiency  of  the  brain  makes  it  important  to  understand  its  principles  in  order  to  push  the  boundaries  of  modern  computing.  Many  models  in  neuroscience  effectively  model  the  detailed  biological  properties  of  the  brain  —  however,  they  do  not  exhibit  good  performance  on  any  benchmark  task.  Other  systems  have  good  performance,  but  do  not  work  in  the  manner  that  the  brain  does,  thus  they  do  not  give  any  insight  into  the  mechanisms  of  the  brain.  A  recent  model  by  Diehl  and  Cook  [1]  is  a  neuromorphic  system  modelled  by  spiking  neural  networks  (SNN)  and  spike-timing-dependant  plasticity  (STDP)  that  is  able  to  achieve  very  good  results  on  the  MNIST  dataset,  a  benchmark  dataset  of  handwritten  numbers  for  classification  algorithms.  However,  in  this  system  the  manner  in  which  MNIST  dataset  is  converted  into  spikes  does  not  mimic  the  manner  in  which  neuromorphic  sensors  record  information.  Therefore,  in  this  paper,  we  test  the  method  proposed  by  Diehl  and  Cook  on  the  N-MNIST  (neuromorphic  MNIST)  where  the  MNIST  images  were  recorded  using  a  silicon  retina,  a  DVS  sensor  that  was  moved  similar  to  the  saccade-like  movements  of  the  retina.  The  output  of  the  camera  consists  of  events  at  pixels  similar  to  the  retinal  spikes.  In  this  paper  we  examined  how  the  original  system  had  to  be  changed  to  accommodate  the  new  dataset  and  noted  that  with  modified  parameters  that  respond  to  characteristics  of  the  new  dataset  the  system  had  better  accuracy.  The  400  neuron  system  has  an  accuracy  of  93.68%  for  a  smaller  3-class  training  dataset,  and  entire  10-class  N-MNIST  dataset  on  an  800  neuron  network  has  80.63%  accuracy.  We  noted  that  output  neurons  form  clusters  that  respond  to  a  particular  class,  making  it  suitable  to  stack  the  system  in  a  hierarchical  manner.
0	Extending  mlp  ann  hyper  parameters  optimization  by  using  genetic  algorithm.  Optimizing  the  hyper-parameters  of  a  multi-layer  perceptron  (MLP)  artificial  neural  network  (ANN)  is  not  a  trivial  task,  and  even  today  the  trial-and-error  approach  is  widely  used.  Many  works  have  already  presented  using  the  genetic  algorithm  (GA)  to  help  in  this  optimization  search  including  MLP  topology,  weights,  and  bias  optimization.  This  work  proposes  adding  hyperparameters  for  weights  initialization  and  regularization  to  be  optimized  simultaneously  with  the  usually  MLP  topology  and  learning  hyper-parameters.  It  also  analyses  which  hyperparameters  are  more  correlated  with  classification  performance,  allowing  a  reduction  in  the  search  space,  which  decreases  the  time  and  computation  needed  to  reach  a  good  set  of  hyper-parameters.  Results  achieved  with  public  datasets  reveal  an  increase  in  performance  when  compared  with  similar  works.  Also,  the  hyperparameters  related  to  weights  initialization  and  regularization  are  among  the  top  5  most  relevant  hyper-parameters  to  explain  the  accuracy  performance  in  all  datasets,  showing  the  importance  of  including  them  in  the  optimization  process.
0	Stock  price  prediction  based  on  stock  specific  and  sub  industry  specific  news  articles.  Accurate  forecasting  of  upcoming  trends  in  the  capital  markets  is  extremely  important  for  algorithmic  trading  and  investment  management.  Before  making  a  trading  decision,  investors  estimate  the  probability  that  a  certain  news  item  will  influence  the  market  based  on  the  available  information.  Speculation  among  traders  is  often  caused  by  the  release  of  a  breaking  news  article  and  results  in  price  movements.  Publications  of  news  articles  influence  the  market  state  that  makes  them  a  powerful  source  of  data  in  financial  forecasting.  Recently,  researchers  have  developed  trend  and  price  prediction  models  based  on  information  extracted  from  news  articles.  However,  to  date  no  previous  research  that  investigates  the  advantages  of  using  news  articles  with  different  levels  of  relevance  to  the  target  stock  has  been  conducted.  This  research  study  uses  the  multiple  kernel  learning  technique  to  effectively  combine  information  extracted  from  stock-specific  and  sub-industry-specific  news  articles  for  prediction  of  an  upcoming  price  movement.  News  articles  are  divided  into  these  two  categories  based  on  their  relevance  to  a  targeted  stock  and  analyzed  by  separate  kernels.  The  experimental  results  show  that  utilizing  two  categories  of  news  improves  the  prediction  accuracy  in  comparison  with  methods  based  on  a  single  news  category.
0	Cross  lingual  word  sense  disambiguation  for  languages  with  scarce  resources.  Word  Sense  Disambiguation  has  long  been  a  central  problem  in  computational  linguistics.  Word  Sense  Disambiguation  is  the  ability  to  identify  the  meaning  of  words  in  context  in  a  computational  manner.  Statistical  and  supervised  approaches  require  a  large  amount  of  labeled  resources  as  training  datasets.  In  contradistinction  to  English,  the  Persian  language  has  neither  any  semantically  tagged  corpus  to  aid  machine  learning  approaches  for  Persian  texts,  nor  any  suitable  parallel  corpora.  Yet  due  to  the  ever-increasing  development  of  Persian  pages  in  Wikipedia,  this  resource  can  act  as  a  comparable  corpus  for  English-Persian  texts.    In  this  paper,  we  propose  a  cross-lingual  approach  to  tagging  the  word  senses  in  Persian  texts.  The  new  approach  makes  use  of  English  sense  disambiguators,  the  Wikipedia  articles  in  both  English  and  Persian,  and  a  newly  developed  lexical  ontology,  FarsNet.  It  overcomes  the  lack  of  knowledge  resources  and  NLP  tools  for  the  Persian  language.  We  demonstrate  the  effectiveness  of  the  proposed  approach  by  comparing  it  to  a  direct  sense  disambiguation  approach  for  Persian.  The  evaluation  results  indicate  a  comparable  performance  to  the  utilized  English  sense  tagger.
0	Time  series  based  link  prediction.  Link  prediction  is  a  task  in  Social  Network  Analysis  that  consists  of  predicting  connections  that  are  most  likely  to  appear  considering  previous  observed  links  in  a  social  network.  The  majority  of  works  in  this  area  only  performs  the  task  by  exploring  the  state  of  the  network  at  a  specific  moment  to  make  the  prediction  of  new  links,  without  considering  the  behavior  of  links  as  time  goes  by.  In  this  light,  we  investigate  if  temporal  information  can  bring  any  performance  gain  to  the  link  prediction  task.  A  traditional  approach  for  link  prediction  uses  a  chosen  topological  similarity  metric  on  non-connected  pairs  of  nodes  of  the  network  at  present  time  to  obtain  a  score  that  is  going  to  be  used  by  an  unsupervised  or  a  supervised  method  for  link  prediction.  Our  approach  initially  consists  of  building  time  series  for  each  pair  of  non-connected  nodes  by  computing  their  similarity  scores  at  different  past  times.  Then,  we  deploy  a  forecasting  model  on  these  time  series  and  use  their  forecasts  as  the  final  scores  of  the  pairs.  Our  preliminary  results  using  two  link  prediction  methods  (unsupervised  and  supervised)  on  co-authorship  networks  revealed  satisfactory  results  when  temporal  information  was  considered.
0	On  the  information  plane  of  autoencoders.  The  training  dynamics  of  hidden  layers  in  deep  learning  are  poorly  understood  in  theory.  Recently,  the  Information  Plane  (IP)  was  proposed  to  analyze  them,  which  is  based  on  the  information-theoretic  concept  of  mutual  information  (MI).  The  Information  Bottleneck  (IB)  theory  predicts  that  layers  maximize  relevant  information  and  compress  irrelevant  information.  Due  to  the  limitations  in  MI  estimation  from  samples,  there  is  an  ongoing  debate  about  the  properties  of  the  IP  for  the  supervised  learning  case.  In  this  work,  we  derive  a  theoretical  convergence  for  the  IP  of  autoencoders.  The  theory  predicts  that  ideal  autoencoders  with  a  large  bottleneck  layer  size  do  not  compress  input  information,  whereas  a  small  size  causes  compression  only  in  the  encoder  layers.  For  the  experiments,  we  use  a  Gram-matrix  based  MI  estimator  recently  proposed  in  the  literature.  We  propose  a  new  rule  to  adjust  its  parameters  that  compensates  scale  and  dimensionality  effects.  Using  our  proposed  rule,  we  obtain  experimental  IPs  closer  to  the  theory.  Our  theoretical  IP  for  autoencoders  could  be  used  as  a  benchmark  to  validate  new  methods  to  estimate  MI  in  neural  networks.  In  this  way,  experimental  limitations  could  be  recognized  and  corrected,  helping  with  the  ongoing  debate  on  the  supervised  learning  case.
0	Setexpan  corpus  based  set  expansion  via  context  feature  selection  and  rank  ensemble.  Corpus-based  set  expansion  (i.e.,  finding  the  “complete”  set  of  entities  belonging  to  the  same  semantic  class,  based  on  a  given  corpus  and  a  tiny  set  of  seeds)  is  a  critical  task  in  knowledge  discovery.  It  may  facilitate  numerous  downstream  applications,  such  as  information  extraction,  taxonomy  induction,  question  answering,  and  web  search.
0	Maximum  entropy  linear  manifold  for  learning  discriminative  low  dimensional  representation.  Representation  learning  is  currently  a  very  hot  topic  in  modern  machine  learning,  mostly  due  to  the  great  success  of  the  deep  learning  methods.  In  particular  low-dimensional  representation  which  discriminates  classes  can  not  only  enhance  the  classification  procedure,  but  also  make  it  faster,  while  contrary  to  the  high-dimensional  embeddings  can  be  efficiently  used  for  visual  based  exploratory  data  analysis.    In  this  paper  we  propose  Maximum  Entropy  Linear  Manifold  (MELM),  a  multidimensional  generalization  of  Multithreshold  Entropy  Linear  Classifier  model  which  is  able  to  find  a  low-dimensional  linear  data  projection  maximizing  discriminativeness  of  projected  classes.  As  a  result  we  obtain  a  linear  embedding  which  can  be  used  for  classification,  class  aware  dimensionality  reduction  and  data  visualization.  MELM  provides  highly  discriminative  2D  projections  of  the  data  which  can  be  used  as  a  method  for  constructing  robust  classifiers.    We  provide  both  empirical  evaluation  as  well  as  some  interesting  theoretical  properties  of  our  objective  function  such  us  scale  and  affine  transformation  invariance,  connections  with  PCA  and  bounding  of  the  expected  balanced  accuracy  error.
0	Parallel  structural  graph  clustering.  We  address  the  problem  of  clustering  large  graph  databases  according  to  scaffolds  (i.e.,  large  structural  overlaps)  that  are  shared  between  cluster  members.  In  previous  work,  an  online  algorithm  was  proposed  for  this  task  that  produces  overlapping  (non-disjoint)  and  nonexhaustive  clusterings.  In  this  paper,  we  parallelize  this  algorithm  to  take  advantage  of  high-performance  parallel  hardware  and  further  improve  the  algorithm  in  three  ways:  a  refined  cluster  membership  test  based  on  a  set  abstraction  of  graphs,  sorting  graphs  according  to  size,  to  avoid  cluster  membership  tests  in  the  first  place,  and  the  definition  of  a  cluster  representative  once  the  cluster  scaffold  is  unique,  to  avoid  cluster  comparisons  with  all  cluster  members.  In  experiments  on  a  large  database  of  chemical  structures,  we  show  that  running  times  can  be  reduced  by  a  large  factor  for  one  parameter  setting  used  in  previous  work.  For  harder  parameter  settings,  it  was  possible  to  obtain  results  within  reasonable  time  for  300,000  structures,  compared  to  10,000  structures  in  previous  work.  This  shows  that  structural,  scaffold-based  clustering  of  smaller  libraries  for  virtual  screening  is  already  feasible.
0	On  the  limitation  of  local  intrinsic  dimensionality  for  characterizing  the  subspaces  of  adversarial  examples.  Understanding  and  characterizing  the  subspaces  of  adversarial  examples  aid  in  studying  the  robustness  of  deep  neural  networks  (DNNs)  to  adversarial  perturbations.  Very  recently,  Ma  et  al.  (ICLR  2018)  proposed  to  use  local  intrinsic  dimensionality  (LID)  in  layer-wise  hidden  representations  of  DNNs  to  study  adversarial  subspaces.  It  was  demonstrated  that  LID  can  be  used  to  characterize  the  adversarial  subspaces  associated  with  different  attack  methods,  e.g.,  the  Carlini  and  Wagner's  (C&W)  attack  and  the  fast  gradient  sign  attack.    In  this  paper,  we  use  MNIST  and  CIFAR-10  to  conduct  two  new  sets  of  experiments  that  are  absent  in  existing  LID  analysis  and  report  the  limitation  of  LID  in  characterizing  the  corresponding  adversarial  subspaces,  which  are  (i)  oblivious  attacks  and  LID  analysis  using  adversarial  examples  with  different  confidence  levels;  and  (ii)  black-box  transfer  attacks.  For  (i),  we  find  that  the  performance  of  LID  is  very  sensitive  to  the  confidence  parameter  deployed  by  an  attack,  and  the  LID  learned  from  ensembles  of  adversarial  examples  with  varying  confidence  levels  surprisingly  gives  poor  performance.  For  (ii),  we  find  that  when  adversarial  examples  are  crafted  from  another  DNN  model,  LID  is  ineffective  in  characterizing  their  adversarial  subspaces.  These  two  findings  together  suggest  the  limited  capability  of  LID  in  characterizing  the  subspaces  of  adversarial  examples.
0	A  closer  look  at  deep  learning  heuristics  learning  rate  restarts  warmup  and  distillation.  The  convergence  rate  and  final  performance  of  common  deep  learning  models  have  significantly  benefited  from  heuristics  such  as  learning  rate  schedules,  knowledge  distillation,  skip  connections,  and  normalization  layers.  In  the  absence  of  theoretical  underpinnings,  controlled  experiments  aimed  at  explaining  these  strategies  can  aid  our  understanding  of  deep  learning  landscapes  and  the  training  dynamics.  Existing  approaches  for  empirical  analysis  rely  on  tools  of  linear  interpolation  and  visualizations  with  dimensionality  reduction,  each  with  their  limitations.  Instead,  we  revisit  such  analysis  of  heuristics  through  the  lens  of  recently  proposed  methods  for  loss  surface  and  representation  analysis,  viz.,  mode  connectivity  and  canonical  correlation  analysis  (CCA),  and  hypothesize  reasons  for  the  success  of  the  heuristics.  In  particular,  we  explore  knowledge  distillation  and  learning  rate  heuristics  of  (cosine)  restarts  and  warmup  using  mode  connectivity  and  CCA.  Our  empirical  analysis  suggests  that:  (a)  the  reasons  often  quoted  for  the  success  of  cosine  annealing  are  not  evidenced  in  practice;  (b)  that  the  effect  of  learning  rate  warmup  is  to  prevent  the  deeper  layers  from  creating  training  instability;  and  (c)  that  the  latent  knowledge  shared  by  the  teacher  is  primarily  disbursed  to  the  deeper  layers.
0	A  survey  of  human  motion  analysis  using  depth  imagery.  Analysis  of  human  behaviour  through  visual  information  has  been  a  highly  active  research  topic  in  the  computer  vision  community.  This  was  previously  achieved  via  images  from  a  conventional  camera,  however  recently  depth  sensors  have  made  a  new  type  of  data  available.  This  survey  starts  by  explaining  the  advantages  of  depth  imagery,  then  describes  the  new  sensors  that  are  available  to  obtain  it.  In  particular,  the  Microsoft  Kinect  has  made  high-resolution  real-time  depth  cheaply  available.  The  main  published  research  on  the  use  of  depth  imagery  for  analysing  human  activity  is  reviewed.  Much  of  the  existing  work  focuses  on  body  part  detection  and  pose  estimation.  A  growing  research  area  addresses  the  recognition  of  human  actions.  The  publicly  available  datasets  that  include  depth  imagery  are  listed,  as  are  the  software  libraries  that  can  acquire  it  from  a  sensor.  This  survey  concludes  by  summarising  the  current  state  of  work  on  this  topic,  and  pointing  out  promising  future  research  directions.  For  both  researchers  and  practitioners  who  are  familiar  with  this  topic  and  those  who  are  new  to  this  field,  the  review  will  aid  in  the  selection,  and  development,  of  algorithms  using  depth  data.
0	Clustering  data  in  an  uncertain  environment  using  an  artificial  immune  system.  Clustering  of  data  in  an  uncertain  environment  can  result  into  different  partitions  of  the  data  at  different  points  in  time.  Therefore,  the  initial  formed  clusters  of  non-stationary  data  can  adapt  over  time  which  means  that  feature  vectors  associated  with  different  clusters  can  follow  different  migration  types  to  and  from  other  clusters.  This  paper  investigates  different  data  migration  types  and  proposes  a  technique  to  generate  artificial  non-stationary  data  which  follows  different  migration  types.  Furthermore,  the  paper  proposes  clustering  performance  measures  which  are  more  applicable  to  measure  the  clustering  quality  in  a  non-stationary  environment  compared  to  the  clustering  performance  measures  for  stationary  environments.  The  proposed  clustering  performance  measures  in  this  paper  are  then  used  to  compare  the  clustering  results  of  three  network  based  artificial  immune  models,  since  the  adaptability  and  self-organising  behaviour  of  the  natural  immune  system  inspired  the  modelling  of  network  based  artificial  immune  models  for  clustering  of  non-stationary  data.
0	Unsupervised  generation  of  polygonal  approximations  based  on  the  convex  hull.  Abstract  The  present  paper  proposes  a  new  non-optimal  but  unsupervised  algorithm,  called  ICT-RDP,  for  generation  of  polygonal  approximations  based  on  the  convex  hull.  Firstly,  the  new  algorithm  takes  into  account  the  convex  hull  of  the  2D  closed  curves  or  contours  to  select  a  set  of  initial  points;  secondly,  the  significance  levels  of  the  contour  points  are  computed  using  a  symmetric  version  of  the  well-known  Ramer,  Douglas-Peucker  algorithm;  and,  finally,  a  thresholding  process  is  applied  to  obtain  the  vertices  or  dominant  points  of  the  polygonal  approximation.  Since  the  convex  hull  can  select  many  initial  points  in  rounded  parts  of  the  contour,  an  additional  deletion  process  is  required  to  remove  quasi-collinear  dominant  points.  Furthermore,  an  additional  improvement  process  is  applied  to  shift  the  dominant  points  in  order  to  increase  the  quality  of  the  polygonal  approximation.  Experiments  performed  on  a  public  available  dataset  show  that  the  new  proposal  outperforms  other  unsupervised  algorithms  for  generation  of  polygonal  approximations.
0	Biologically  plausible  learning  algorithms  can  scale  to  large  datasets.  The  backpropagation  (BP)  algorithm  is  often  thought  to  be  biologically  implausible  in  the  brain.  One  of  the  main  reasons  is  that  BP  requires  symmetric  weight  matrices  in  the  feedforward  and  feedback  pathways.  To  address  this  "weight  transport  problem"  (Grossberg,  1987),  two  more  biologically  plausible  algorithms,  proposed  by  Liao  et  al.  (2016)  and  Lillicrap  et  al.  (2016),  relax  BP's  weight  symmetry  requirements  and  demonstrate  comparable  learning  capabilities  to  that  of  BP  on  small  datasets.  However,  a  recent  study  by  Bartunov  et  al.  (2018)  evaluate  variants  of  target-propagation  (TP)  and  feedback  alignment  (FA)  on  MINIST,  CIFAR,  and  ImageNet  datasets,  and  find  that  although  many  of  the  proposed  algorithms  perform  well  on  MNIST  and  CIFAR,  they  perform  significantly  worse  than  BP  on  ImageNet.  Here,  we  additionally  evaluate  the  sign-symmetry  algorithm  (Liao  et  al.,  2016),  which  differs  from  both  BP  and  FA  in  that  the  feedback  and  feedforward  weights  share  signs  but  not  magnitudes.  We  examine  the  performance  of  sign-symmetry  and  feedback  alignment  on  ImageNet  and  MS  COCO  datasets  using  different  network  architectures  (ResNet-18  and  AlexNet  for  ImageNet,  RetinaNet  for  MS  COCO).  Surprisingly,  networks  trained  with  sign-symmetry  can  attain  classification  performance  approaching  that  of  BP-trained  networks.  These  results  complement  the  study  by  Bartunov  et  al.  (2018),  and  establish  a  new  benchmark  for  future  biologically  plausible  learning  algorithms  on  more  difficult  datasets  and  more  complex  architectures.
0	Viewmaker  networks  learning  views  for  unsupervised  representation  learning.  Many  recent  methods  for  unsupervised  representation  learning  involve  training  models  to  be  invariant  to  different  "views,"  or  augmented  versions  of  an  input.  However,  designing  these  views  requires  considerable  human  expertise  and  experimentation,  hindering  widespread  adoption  of  unsupervised  representation  learning  methods  across  domains  and  modalities.  To  address  this,  we  propose  viewmaker  networks:  generative  models  which  learn  to  produce  input-dependent  views  for  contrastive  learning.  We  train  these  networks  jointly  with  the  main  network  to  produce  adversarial  lp  perturbations  for  an  input,  which  yields  challenging  yet  faithful  views  without  extensive  human  tuning.  Our  learned  views  enable  comparable  transfer  accuracy  to  the  the  well-studied  SimCLR  augmentations  when  applied  on  CIFAR-10,  while  significantly  outperforming  baseline  augmentations  in  speech  (+9%  absolute)  and  IMU  sensor  (+17%  absolute)  domains.  We  also  show  how  viewmaker  views  can  be  combined  with  SimCLR  views  to  improve  robustness  to  common  image  corruptions.  Our  method  provides  a  roadmap  for  reducing  the  amount  of  expertise  and  effort  needed  for  unsupervised  learning,  potentially  extending  its  benefits  to  a  much  wider  set  of  domains.
0	Self  learning  to  filter  noisy  labels  with  self  ensembling.  Deep  neural  networks  (DNNs)  have  been  shown  to  over-fit  a  dataset  when  being  trained  with  noisy  labels  for  a  long  enough  time.  To  overcome  this  problem,  we  present  a  simple  and  effective  method  self-ensemble  label  filtering  (SELF)  to  progressively  filter  out  the  wrong  labels  during  training.  Our  method  improves  the  task  performance  by  gradually  allowing  supervision  only  from  the  potentially  non-noisy  (clean)  labels  and  stops  learning  on  the  filtered  noisy  labels.  For  the  filtering,  we  form  running  averages  of  predictions  over  the  entire  training  dataset  using  the  network  output  at  different  training  epochs.  We  show  that  these  ensemble  estimates  yield  more  accurate  identification  of  inconsistent  predictions  throughout  training  than  the  single  estimates  of  the  network  at  the  most  recent  training  epoch.  While  filtered  samples  are  removed  entirely  from  the  supervised  training  loss,  we  dynamically  leverage  them  via  semi-supervised  learning  in  the  unsupervised  loss.  We  demonstrate  the  positive  effect  of  such  an  approach  on  various  image  classification  tasks  under  both  symmetric  and  asymmetric  label  noise  and  at  different  noise  ratios.  It  substantially  outperforms  all  previous  works  on  noise-aware  learning  across  different  datasets  and  can  be  applied  to  a  broad  set  of  network  architectures.
0	Automated  diagnosis  of  multi  class  brain  abnormalities  using  mri  images  a  deep  convolutional  neural  network  based  method.  Abstract  Automated  detection  of  multi-class  brain  abnormalities  through  magnetic  resonance  imaging  (MRI)  has  received  much  attention  due  to  its  clinical  significance  and  therefore  has  become  an  active  area  of  research  over  the  years.  The  earlier  automated  schemes  often  followed  traditional  machine  learning  paradigms,  in  which  the  proper  choice  of  features  and  classifiers  has  remained  a  major  concern.  Therefore,  deep  learning  algorithms  have  been  profoundly  applied  in  various  medical  imaging  applications.  In  this  paper,  a  deep  convolutional  neural  network  (CNN)  based  automated  approach  is  designed  for  the  diagnosis  of  multi-class  brain  abnormalities.  The  proposed  CNN  model  comprises  five  layers  with  learnable  parameters:  four  convolutional  layers  and  one  fully-connected  layer.  The  objective  of  designing  such  a  custom  deep  network  is  to  achieve  greater  classification  performance  with  reduced  number  of  parameters.  The  proposed  model  is  evaluated  on  two  benchmark  multi-class  brain  MRI  datasets  namely,  MD-1  and  MD-2.  The  model  achieved  a  classification  accuracy  of  100.00%  and  97.50%  on  MD-1  and  MD-2  datasets  respectively.  Moreover,  four  pre-trained  CNN  models  based  on  the  transfer  learning  approach  have  been  tested  over  the  same  datasets.  The  comparative  analysis  with  existing  schemes  indicates  the  superiority  of  the  proposed  method.
0	Clothing  and  carrying  condition  invariant  gait  recognition  based  on  rotation  forest.  Introduces  averaged  gait  key-phase  image  (AGKI)  for  gait  recognition.Recognition  is  robust  to  unpredictable  variation  in  clothing  and  carrying  conditions.AGKIs  are  analysed  using  high-pass  and  low-pass  Gaussian  filters  at  different  cut-off  frequencies.Optimal  cut-off  frequencies  are  chosen  based  on  focus  value  analysis.The  method  uses  rotation  forest  ensemble  method  for  classification.  This  paper  proposes  a  gait  recognition  method  which  is  invariant  to  maximum  number  of  challenging  factors  of  gait  recognition  mainly  unpredictable  variation  in  clothing  and  carrying  conditions.  The  method  introduces  an  averaged  gait  key-phase  image  (AGKI)  which  is  computed  by  averaging  each  of  the  five  key-phases  of  the  gait  periods  of  a  gait  sequence.  It  analyses  the  AGKIs  using  high-pass  and  low-pass  Gaussian  filters,  each  at  three  cut-off  frequencies  to  achieve  robustness  against  unpredictable  variation  in  clothing  and  carrying  conditions  in  addition  to  other  covariate  factors,  e.g.,  walking  speed,  segmentation  noise,  shadows  under  feet  and  change  in  hair  style  and  ground  surface.  The  optimal  cut-off  frequencies  of  the  Gaussian  filters  are  determined  based  on  an  analysis  of  the  focus  values  of  filtered  human  subject's  silhouettes.  The  method  applies  rotation  forest  ensemble  learning  recognition  to  enhance  both  individual  accuracy  and  diversity  within  the  ensemble  for  improved  identification  rate.  Extensive  experiments  on  public  datasets  demonstrate  the  efficacy  of  the  proposed  method.
0	On  the  use  of  kinect  depth  data  for  identity  gender  and  ethnicity  classification  from  facial  images.  Investigate  the  usefulness  of  the  depth  images  provided  by  RGB-D  sensors  in  face  analysis  tasks.Provide  a  comprehensive  review  of  the  prior  work  and  present  publicly  available  RGB-D  face  databases.Compare  the  performance  of  the  depth  images  against  RGB  counterpart  in  three  face  analysis  tasks.  Display  Omitted  This  article  explores  the  usefulness  of  the  depth  images  provided  by  the  current  Microsoft  Kinect  sensors  in  different  face  analysis  tasks  including  identity,  gender  and  ethnicity.  Four  local  feature  extraction  methods  (LBP,  LPQ,  HOG  and  BSIF)  are  investigated  for  both  face  texture  and  shape  description.  Extensive  experiments  on  three  publicly  available  Kinect  face  databases  are  reported.  The  experimental  analysis  yields  into  interesting  findings.  Furthermore,  a  comprehensive  review  of  the  literature  on  the  use  of  Kinect  depth  data  in  face  analysis  is  provided  along  with  the  description  of  the  available  databases.
0	Episodic  exploration  for  deep  deterministic  policies  for  starcraft  micromanagement.  We  consider  scenarios  from  the  real-time  strategy  game  StarCraft  as  benchmarks  for  reinforcement  learning  algorithms.  We  focus  on  micromanagement,  that  is,  the  short-term,  low-level  control  of  team  members  during  a  battle.  We  propose  several  scenarios  that  are  challenging  for  reinforcement  learning  algorithms  because  the  state-  action  space  is  very  large,  and  there  is  no  obvious  feature  representation  for  the  value  functions.  We  describe  our  approach  to  tackle  the  micromanagement  scenarios  with  deep  neural  network  controllers  from  raw  state  features  given  by  the  game  engine.  We  also  present  a  heuristic  reinforcement  learning  algorithm  which  combines  direct  exploration  in  the  policy  space  and  backpropagation.  This  algorithm  collects  traces  for  learning  using  deterministic  policies,  which  appears  much  more  efficient  than,  e.g.,  e-greedy  exploration.  Experiments  show  that  this  algorithm  allows  to  successfully  learn  non-trivial  strategies  for  scenarios  with  armies  of  up  to  15  agents,  where  both  Q-learning  and  REINFORCE  struggle.
0	Dream  to  control  learning  behaviors  by  latent  imagination.  To  select  effective  actions  in  complex  environments,  intelligent  agents  need  to  generalize  from  past  experience.  World  models  can  represent  knowledge  about  the  environment  to  facilitate  such  generalization.  While  learning  world  models  from  high-dimensional  sensory  inputs  is  becoming  feasible  through  deep  learning,  there  are  many  potential  ways  for  deriving  behaviors  from  them.  We  present  Dreamer,  a  reinforcement  learning  agent  that  solves  long-horizon  tasks  purely  by  latent  imagination.  We  efficiently  learn  behaviors  by  backpropagating  analytic  gradients  of  learned  state  values  through  trajectories  imagined  in  the  compact  state  space  of  a  learned  world  model.  On  20  challenging  visual  control  tasks,  Dreamer  exceeds  existing  approaches  in  data-efficiency,  computation  time,  and  final  performance.
0	Dynamical  distance  learning  for  semi  supervised  and  unsupervised  skill  discovery.  Reinforcement  learning  requires  manual  specification  of  a  reward  function  to  learn  a  task.  While  in  principle  this  reward  function  only  needs  to  specify  the  task  goal,  in  practice  reinforcement  learning  can  be  very  time-consuming  or  even  infeasible  unless  the  reward  function  is  shaped  so  as  to  provide  a  smooth  gradient  towards  a  successful  outcome.  This  shaping  is  difficult  to  specify  by  hand,  particularly  when  the  task  is  learned  from  raw  observations,  such  as  images.  In  this  paper,  we  study  how  we  can  automatically  learn  dynamical  distances:  a  measure  of  the  expected  number  of  time  steps  to  reach  a  given  goal  state  from  any  other  state.  These  dynamical  distances  can  be  used  to  provide  well-shaped  reward  functions  for  reaching  new  goals,  making  it  possible  to  learn  complex  tasks  efficiently.  We  show  that  dynamical  distances  can  be  used  in  a  semi-supervised  regime,  where  unsupervised  interaction  with  the  environment  is  used  to  learn  the  dynamical  distances,  while  a  small  amount  of  preference  supervision  is  used  to  determine  the  task  goal,  without  any  manually  engineered  reward  function  or  goal  examples.  We  evaluate  our  method  both  on  a  real-world  robot  and  in  simulation.  We  show  that  our  method  can  learn  to  turn  a  valve  with  a  real-world  9-DoF  hand,  using  raw  image  observations  and  just  ten  preference  labels,  without  any  other  supervision.  Videos  of  the  learned  skills  can  be  found  on  the  project  website:  https://sites.google.com/view/skills-via-distance-learning.
0	From  hard  to  soft  understanding  deep  network  nonlinearities  via  vector  quantization  and  statistical  inference.  Nonlinearity  is  crucial  to  the  performance  of  a  deep  (neural)  network  (DN).  To  date  there  has  been  little  progress  understanding  the  menagerie  of  available  nonlinearities,  but  recently  progress  has  been  made  on  understanding  the  r\^ole  played  by  piecewise  affine  and  convex  nonlinearities  like  the  ReLU  and  absolute  value  activation  functions  and  max-pooling.  In  particular,  DN  layers  constructed  from  these  operations  can  be  interpreted  as  {\em  max-affine  spline  operators}  (MASOs)  that  have  an  elegant  link  to  vector  quantization  (VQ)  and  $K$-means.  While  this  is  good  theoretical  progress,  the  entire  MASO  approach  is  predicated  on  the  requirement  that  the  nonlinearities  be  piecewise  affine  and  convex,  which  precludes  important  activation  functions  like  the  sigmoid,  hyperbolic  tangent,  and  softmax.  {\em  This  paper  extends  the  MASO  framework  to  these  and  an  infinitely  large  class  of  new  nonlinearities  by  linking  deterministic  MASOs  with  probabilistic  Gaussian  Mixture  Models  (GMMs).}  We  show  that,  under  a  GMM,  piecewise  affine,  convex  nonlinearities  like  ReLU,  absolute  value,  and  max-pooling  can  be  interpreted  as  solutions  to  certain  natural  "hard"  VQ  inference  problems,  while  sigmoid,  hyperbolic  tangent,  and  softmax  can  be  interpreted  as  solutions  to  corresponding  "soft"  VQ  inference  problems.  We  further  extend  the  framework  by  hybridizing  the  hard  and  soft  VQ  optimizations  to  create  a  $\beta$-VQ  inference  that  interpolates  between  hard,  soft,  and  linear  VQ  inference.  A  prime  example  of  a  $\beta$-VQ  DN  nonlinearity  is  the  {\em  swish}  nonlinearity,  which  offers  state-of-the-art  performance  in  a  range  of  computer  vision  tasks  but  was  developed  ad  hoc  by  experimentation.  Finally,  we  validate  with  experiments  an  important  assertion  of  our  theory,  namely  that  DN  performance  can  be  significantly  improved  by  enforcing  orthogonality  in  its  linear  filters.
0	Generating  high  fidelity  images  with  subscale  pixel  networks  and  multidimensional  upscaling.  The  unconditional  generation  of  high  fidelity  images  is  a  longstanding  benchmark  for  testing  the  performance  of  image  decoders.  Autoregressive  image  models  have  been  able  to  generate  small  images  unconditionally,  but  the  extension  of  these  methods  to  large  images  where  fidelity  can  be  more  readily  assessed  has  remained  an  open  problem.  Among  the  major  challenges  are  the  capacity  to  encode  the  vast  previous  context  and  the  sheer  difficulty  of  learning  a  distribution  that  preserves  both  global  semantic  coherence  and  exactness  of  detail.  To  address  the  former  challenge,  we  propose  the  Subscale  Pixel  Network  (SPN),  a  conditional  decoder  architecture  that  generates  an  image  as  a  sequence  of  sub-images  of  equal  size.  The  SPN  compactly  captures  image-wide  spatial  dependencies  and  requires  a  fraction  of  the  memory  and  the  computation  required  by  other  fully  autoregressive  models.  To  address  the  latter  challenge,  we  propose  to  use  Multidimensional  Upscaling  to  grow  an  image  in  both  size  and  depth  via  intermediate  stages  utilising  distinct  SPNs.  We  evaluate  SPNs  on  the  unconditional  generation  of  CelebAHQ  of  size  256  and  of  ImageNet  from  size  32  to  256.  We  achieve  state-of-the-art  likelihood  results  in  multiple  settings,  set  up  new  benchmark  results  in  previously  unexplored  settings  and  are  able  to  generate  very  high  fidelity  large  scale  samples  on  the  basis  of  both  datasets.
0	Efficient  riemannian  optimization  on  the  stiefel  manifold  via  the  cayley  transform.  Strictly  enforcing  orthonormality  constraints  on  parameter  matrices  has  been  shown  advantageous  in  deep  learning.  This  amounts  to  Riemannian  optimization  on  the  Stiefel  manifold,  which,  however,  is  computationally  expensive.  To  address  this  challenge,  we  present  two  main  contributions:  (1)  A  new  efficient  retraction  map  based  on  an  iterative  Cayley  transform  for  optimization  updates,  and  (2)  An  implicit  vector  transport  mechanism  based  on  the  combination  of  a  projection  of  the  momentum  and  the  Cayley  transform  on  the  Stiefel  manifold.  We  specify  two  new  optimization  algorithms:  Cayley  SGD  with  momentum,  and  Cayley  ADAM  on  the  Stiefel  manifold.  Convergence  of  the  Cayley  SGD  is  theoretically  analyzed.  Our  experiments  for  CNN  training  demonstrate  that  both  algorithms:  (a)  Use  less  running  time  per  iteration  relative  to  existing  approaches  which  also  enforce  orthonormality  of  CNN  parameters;  and  (b)  Achieve  faster  convergence  rates  than  the  baseline  SGD  and  ADAM  algorithms  without  compromising  the  CNN's  performance.  The  Cayley  SGD  and  Cayley  ADAM  are  also  shown  to  reduce  the  training  time  for  optimizing  the  unitary  transition  matrices  in  RNNs.
0	A  fair  comparison  of  graph  neural  networks  for  graph  classification.  The  graph  representation  learning  field  has  recently  attracted  the  attention  of  a  wide  research  community.  Several  Graph  Neural  Network  models  are  being  developed  to  tackle  effective  graph  classification.  However,  experimental  procedures  often  lack  rigorousness  and  are  hardly  reproducible.  Motivated  by  this,  we  provide  an  overview  of  common  practices  that  should  be  avoided  to  fairly  compare  with  the  state  of  the  art.  To  counter  this  troubling  trend,  we  ran  more  than  47000  experiments  in  a  controlled  and  uniform  framework  to  re-evaluate  five  popular  models  across  nine  common  benchmarks.  Moreover,  by  comparing  GNNs  with  structure-agnostic  baselines  we  provide  convincing  evidence  that,  on  some  datasets,  structural  information  has  not  been  exploited  yet.  We  believe  that  this  work  can  contribute  to  the  development  of  the  graph  learning  field,  by  providing  a  much  needed  grounding  for  rigorous  evaluations  of  graph  classification  models.
0	People  counting  by  learning  their  appearance  in  a  multi  view  camera  environment.  We  present  a  people  counting  system  that,  based  on  the  information  gathered  by  multiple  cameras,  is  able  to  tackle  occlusions  and  lack  of  visibility  that  are  typical  in  crowded  and  cluttered  scenes.  In  our  method,  evidence  of  the  foreground  likelihood  in  each  available  view  is  obtained  through  a  bio-inspired  mechanism  of  self-organizing  background  subtraction,  that  is  robust  against  well  known  foreground  detection  challenges  and  is  able  to  detect  both  moving  and  stationary  foreground  objects.  This  information  is  gathered  into  a  synergistic  framework,  that  exploits  the  homography  associated  to  each  scene  view  and  the  scene  ground  plane,  thus  allowing  to  reconstruct  people  feet  positions  in  a  single  ''feet  map''  image.  Finally,  people  counting  is  obtained  by  a  k-NN  classification,  based  on  learning  the  count  estimates  from  the  feet  maps,  supported  by  a  tracking  mechanism  that  keeps  track  of  people  movements  and  of  their  identities  along  time,  also  enabling  tolerance  to  occasional  misdetections.  Experimental  results  with  detailed  qualitative  and  quantitative  analysis  and  comparisons  with  state-of-the-art  methods  are  provided  on  publicly  available  benchmark  datasets  with  different  crowd  densities  and  environmental  conditions.
0	An  unsupervised  2d  point  set  registration  algorithm  for  unlabeled  feature  points  application  to  fingerprint  matching.  Abstract      An  unsupervised,  iterative  2D  point-set  registration  algorithm  for  unlabeled  data  and  based  on  linear  least  squares  is  proposed,  and  subsequently  utilized  for  minutia-based  fingerprint  matching.  The  matcher  considers  all  possible  minutia  pairings  and  iteratively  aligns  the  two  sets  until  the  number  of  minutia  pairs  does  not  exceed  the  maximum  number  of  allowable  one-to-one  pairings.  The  first  alignment  establishes  a  region  of  overlap  between  the  two  minutia  sets,  which  is  then  (iteratively)  refined  by  each  successive  alignment.  After  each  alignment,  minutia  pairs  that  exhibit  weak  correspondence  are  discarded.  The  process  is  repeated  until  the  number  of  remaining  pairs  no  longer  exceeds  the  maximum  number  of  allowable  one-to-one  pairings.  The  proposed  algorithm  is  tested  on  both  the  FVC2000  and  FVC2002  databases,  and  the  results  indicate  that  the  proposed  matcher  is  both  effective  and  efficient  for  fingerprint  authentication;  it  is  fast  and  consciously  utilizes  as  few  computationally  expensive  mathematical  functions  (e.g.  trigonometric,  exponential)  as  possible.  In  addition  to  the  proposed  matcher,  another  contribution  of  the  paper  is  the  analytical  derivation  of  the  least  squares  solution  for  the  optimal  alignment  parameters  for  two  point-sets  lacking  one-to-one  correspondence.
0	Robust  gender  recognition  by  exploiting  facial  attributes  dependencies.  Estimating  human  face  gender  from  images  is  a  problem  that  has  been  extensively  studied  because  of  its  relevant  applications.  Recent  works  report  significant  drops  in  performance  for  state-of-the-art  gender  classifiers  when  evaluated  ''in  the  wild,''  i.e.,  with  uncontrolled  demography  and  environmental  conditions.  We  hypothesize  that  this  is  caused  by  the  existence  of  dependencies  among  facial  demographic  attributes  that  have  not  been  considered  when  building  the  classifier.  In  the  paper  we  study  the  dependencies  among  gender,  age  and  pose  facial  attributes.  By  considering  the  relation  between  gender  and  pose  attributes  we  also  avoid  the  use  of  computationally  expensive  and  fragile  face  alignment  procedures.  In  the  experiments  we  confirm  the  existence  of  dependencies  among  gender,  age  and  pose  facial  attributes  and  prove  that  we  can  improve  the  performance  and  robustness  of  gender  classifiers  by  exploiting  these  dependencies.
0	Optimizing  the  class  information  divergence  for  transductive  classification  of  texts  using  propagation  in  bipartite  graphs.  Scalable  algorithm  based  on  bipartite  graphs  to  perform  transduction  learning.Label  propagation  procedure  that  uses  class  information  associated  with  vertices  and  edges.Better  performance  than  state-of-the-art  algorithms  based  on  vector  space  or  graphs.Comprehensive  evaluation  showing  the  proposal  performance  with  few  labeled  instances.Optimization  process  using  KL-Divergence.  Transductive  classification  is  an  useful  way  to  classify  a  collection  of  unlabeled  textual  documents  when  only  a  small  fraction  of  this  collection  can  be  manually  labeled.  Graph-based  algorithms  have  aroused  considerable  interests  in  recent  years  to  perform  transductive  classification  since  the  graph-based  representation  facilitates  label  propagation  through  the  graph  edges.  In  a  bipartite  graph  representation,  nodes  represent  objects  of  two  types,  here  documents  and  terms,  and  the  edges  between  documents  and  terms  represent  the  occurrences  of  the  terms  in  the  documents.  In  this  context,  the  label  propagation  is  performed  from  documents  to  terms  and  then  from  terms  to  documents  iteratively.  In  this  paper  we  propose  a  new  graph-based  transductive  algorithm  that  use  the  bipartite  graph  structure  to  associate  the  available  class  information  of  labeled  documents  and  then  propagate  these  class  information  to  assign  labels  for  unlabeled  documents.  By  associating  the  class  information  to  edges  linking  documents  to  terms  we  guarantee  that  a  single  term  can  propagate  different  class  information  to  its  distinct  neighbors.  We  also  demonstrated  that  the  proposed  method  surpasses  the  algorithms  for  transductive  classification  based  on  vector  space  model  or  graphs  when  only  a  small  number  of  labeled  documents  is  available.
0	Robust  re  identification  using  randomness  and  statistical  learning  quo  vadis.  The  re-identification  problem  is  to  match  objects  across  multiple  but  possibly  disjoint  fields  of  view  for  the  purpose  of  sequential  authentication  over  space  and  time.  Detection  and  seeding  for  initialization  do  not  presume  known  identity  and  allow  for  re-identification  of  objects  and/or  faces  whose  identity  might  remain  unknown.  Specific  functionalities  involved  in  re-identification  include  clustering  and  selection,  recognition-by-parts,  anomaly  and  change  detection,  sampling  and  tracking,  fast  indexing  and  search,  sensitivity  analysis,  and  their  integration  for  the  purpose  of  identity  management.  As  re-identification  processes  data  streams  and  involves  change  detection  and  on-line  adaptation  three  complementary  statistical  learning  frameworks,  driven  by  randomness  for  the  purpose  of  robust  prediction,  are  advanced  here  to  support  the  functionalities  listed  earlier  and  their  combination  thereof.  The  intertwined  learning  frameworks  employed  are  those  of  (a)  semi-supervised  learning  (SSL);  (b)  transduction;  and  (c)  conformal  prediction.  The  overall  architecture  proposed  is  data-driven  and  modular,  on  one  side,  and  discriminative  and  progressive,  on  the  other  side.  The  architecture  is  built  around  autonomic  computing  and  W5+.  Autonomic  computing  or  self-management  provides  for  closed-loop  control.  W5+  answers  questions  related  to  What  data  to  consider  for  sampling  and  collection,  When  to  capture  the  data  and  from  Where,  and  How  to  best  process  the  data.  The  Who  (is)  query  is  about  identity  for  biometrics,  and  the  Why  question  for  explanation  purposes.  The  challenge  addressed  throughout  is  that  of  evidence-based  management  to  progressively  collect  and  add  value  to  data  in  order  to  generate  knowledge  that  leads  to  purposeful  and  gainful  action  including  active  learning  for  the  overall  purpose  of  re-identification.  A  venue  for  future  research  includes  adversarial  learning  when  re-identification  is  possibly  ''distracted''  using  deliberate  corrupt  information.
0	On  convergence  of  differential  evolution  over  a  class  of  continuous  functions  with  unique  global  optimum.  Differential  evolution  (DE)  is  arguably  one  of  the  most  powerful  stochastic  real-parameter  optimization  algorithms  of  current  interest.  Since  its  inception  in  the  mid  1990s,  DE  has  been  finding  many  successful  applications  in  real-world  optimization  problems  from  diverse  domains  of  science  and  engineering.  This  paper  takes  a  first  significant  step  toward  the  convergence  analysis  of  a  canonical  DE  (DE/rand/1/bin)  algorithm.  It  first  deduces  a  time-recursive  relationship  for  the  probability  density  function  (PDF)  of  the  trial  solutions,  taking  into  consideration  the  DE-type  mutation,  crossover,  and  selection  mechanisms.  Then,  by  applying  the  concepts  of  Lyapunov  stability  theorems,  it  shows  that  as  time  approaches  infinity,  the  PDF  of  the  trial  solutions  concentrates  narrowly  around  the  global  optimum  of  the  objective  function,  assuming  the  shape  of  a  Dirac  delta  distribution.  Asymptotic  convergence  behavior  of  the  population  PDF  is  established  by  constructing  a  Lyapunov  functional  based  on  the  PDF  and  showing  that  it  monotonically  decreases  with  time.  The  analysis  is  applicable  to  a  class  of  continuous  and  real-valued  objective  functions  that  possesses  a  unique  global  optimum  (but  may  have  multiple  local  optima).  Theoretical  results  have  been  substantiated  with  relevant  computer  simulations.
0	Adaptive  sliding  mode  control  for  interval  type  2  fuzzy  systems.  This  paper  is  concerned  with  the  adaptive  sliding  mode  control  problem  of  uncertain  nonlinear  systems.  Interval  type-2  Takagi–Sugeno  (T–S)  fuzzy  model  is  employed  to  represent  uncertain  nonlinear  systems.  The  input  matrices  of  the  nonlinear  systems  are  allowed  to  be  different  for  the  sliding  mode  controller  design.  The  uncertain  parameters  are  described  by  the  lower  and  upper  membership  functions.  An  integral  sliding  mode  surface  is  designed  for  analysis  of  sliding  motion.  Based  on  the  sliding  mode  surface,  a  novel  sliding  mode  controller  is  designed  to  guarantee  that  the  closed-loop  system  is  uniformly  ultimately  bounded.  Some  simulation  results  are  given  to  illustrate  the  effectiveness  of  the  presented  control  scheme.
0	Performance  evaluation  of  self  configured  two  tier  heterogeneous  cellular  networks.  Two-tier  macro/femto  heterogeneous  cellular  networks  (HCNs)  have  received  considerable  attention  due  to  substantial  improvements  in  high  quality  in-building  coverage  and  system  capacity.  Distributed  self-configured  femtocells  can  be  realized  to  mitigate  inter-tier  interference  between  macro  cells  and  femtocells  without  heavy  operating  costs  by  incorporating  broadcasting  mechanism  of  macro  cell.  With  the  aid  of  the  macro  cell,  who  provides  critical  global  information,  femtocells  can  configure  related  parameters  to  achieve  interference  mitigation.  A  tractable  stochastic  geometry-based  analytical  model  is  proposed  to  evaluate  of  proposed  self-configured  scheme  in  terms  of  coverage  probability.  We  also  conduct  simulation  experiments  according  to  data  from  OpenCellID  to  prove  the  effectiveness  of  the  proposed  self-configured  scheme  in  the  realistic  two-tier  HCNs.
0	Identification  of  multimodal  control  behavior  in  pursuit  tracking  tasks.  In  manual  control,  a  pursuit  display  may  support  the  use  of  a  multimodal  “pursuit”  control  strategy  by  the  human  operator.  This  paper  evaluates  two  methods  that  may  be  used  to  directly  estimate  describing  functions  for  such  multimodal  human  operator  control  dynamics  in  pursuit  tracking.  The  first  is  a  previously  developed  frequency-domain  method  based  on  Fourier  coefficients.  The  second  method  makes  use  of  linear-time  invariant  ARMAX  models.  An  experiment  is  described  in  which  participants  performed  tracking  tasks  with  quasi-random  multisine  target  and  disturbance  forcing  functions,  for  single  and  double  integrator  controlled  elements  and  with  compensatory  and  pursuit  displays.  The  experiment  data  confirms  the  findings  from  previous  experiments,  where  it  was  found  that  multimodal  pursuit  control  dynamics  are  adopted  in  control  of  systems  with  double  integrator  dynamics,  but  not  for  single  integrator  control  tasks.  Furthermore,  the  direct  multimodal  identification  methods  were  found  to  give  improved  insight  into  the  internal  organization  and  dynamics  of  the  human  operator  in  pursuit  tracking.
0	A  new  similarity  measure  between  intuitionistic  fuzzy  sets  for  pattern  recognition  based  on  the  centroid  points  of  transformed  fuzzy  numbers.  This  paper  proposes  a  new  similarity  measure  between  intuitionistic  fuzzy  sets  (IFSs)  for  pattern  recognition  based  on  the  centroid  points  of  transformed  right-angled  triangular  fuzzy  numbers  to  deal  with  pattern  recognition  problems.  The  proposed  similarity  measure  between  IFSs  outperforms  the  existing  similarity  measures  for  dealing  with  the  pattern  recognition  problems.
0	Power  series  representation  model  of  text  knowledge  based  on  human  concept  learning.  How  to  build  a  text  knowledge  representation  model,  which  carries  rich  knowledge  and  has  a  flexible  reasoning  ability  as  well  as  can  be  automatically  constructed  with  a  low  computational  complexity,  is  a  fundamental  challenge  for  reasoning-based  knowledge  services,  especially  with  the  rapid  growth  of  web  resources.  However,  current  text  knowledge  representation  models  either  lose  much  knowledge  [e.g.,  vector  space  model  (VSM)]  or  have  a  high  complex  computation  [e.g.,  latent  Dirichlet  allocation  (LDA)];  even  some  of  them  cannot  be  constructed  automatically  [e.g.,  web  ontology  language,  (OWL)].  In  this  paper,  a  novel  text  knowledge  representation  model,  power  series  representation  (PSR)  model,  which  has  a  low  complex  computation  in  text  knowledge  constructing  process,  is  proposed  to  leverage  the  contradiction  between  carrying  rich  knowledge  and  automatic  construction.  First,  concept  algebra  of  human  concept  learning  is  developed  to  represent  text  knowledge  as  the  form  of  power  series.  Then,  degree-2  power  series  hypothesis  is  introduced  to  simplify  the  proposed  PSR  model,  which  can  be  automatically  constructed  with  a  lower  complex  computation  and  has  more  knowledge  than  the  VSM  and  LDA.  After  that,  degree-2  power  series  hypothesis-based  reasoning  operations  are  developed,  which  provide  a  more  flexible  reasoning  ability  than  OWL  and  LDA.  Furthermore,  experiments  and  comparisons  with  current  knowledge  representation  models  show  that  our  model  has  better  characteristics  than  others  when  representing  text  knowledge.  Finally,  a  demo  is  given  to  indicate  that  PSR  model  has  a  good  prospect  over  the  area  of  web  semantic  search.
0	Fuzzy  pid  controllers  for  dual  sensor  pacing  systems  in  patients  with  bradycardias  at  rest.  This  work  describes  the  design  of  a  fuzzy  proportional-integral  derivative  (FPID)  controller  for  dual-sensors  cardiac  pacemaker  system  in  patients  with  bradycardias  at  rest.  It  can  automatically  control  the  heart  rate  to  accurately  track  a  desired  preset  profile.  The  combination  of  PID  control  and  conventional  fuzzy  control  approaches  is  adopted  for  the  controller  design  based  on  dual-sensors.  This  controller  offers  good  adaptation  of  the  heart  rate  to  the  physiological  needs  of  the  patient  at  rest.  Simulation  results  confirm  that  this  proposed  controller  is  more  effective  for  heartbeat  recovery  and  maintenance,  comparing  with  the  existing  fuzzy  control  algorithm.
0	Improving  hierarchical  planning  performance  by  the  use  of  landmarks.  In  hierarchical  planning,  landmarks  are  tasks  that  occur  on  every  search  path  leading  from  the  initial  plan  to  a  solution.  In  this  work,  we  present  novel  domain-independent  planning  strategies  based  on  such  hierarchical  landmarks.  Our  empirical  evaluation  on  four  benchmark  domains  shows  that  these  landmark-aware  strategies  outperform  established  search  strategies  in  many  cases.
0	Intrinsic  and  extrinsic  evaluations  of  word  embeddings.  In  this  paper,  we  first  analyze  the  semantic  composition  of  word  embeddings  by  cross-referencing  their  clusters  with  the  manual  lexical  database,  WordNet.  We  then  evaluate  a  variety  of  word  embedding  approaches  by  comparing  their  contributions  to  two  NLP  tasks.  Our  experiments  show  that  the  word  embedding  clusters  give  high  correlations  to  the  synonym  and  hyponym  sets  in  WordNet,  and  give  0.88%  and  0.17%  absolute  improvements  in  accuracy  to  named  entity  recognition  and  part-of-speech  tagging,  respectively.
0	Affinity  preserving  quantization  for  hashing  a  vector  quantization  approach  to  learning  compact  binary  codes.  Hashing  techniques  are  powerful  for  approximate  nearest  neighbour  (ANN)  search.  Existing  quantization  methods  in  hashing  are  all  focused  on  scalar  quantization  (SQ)  which  is  inferior  in  utilizing  the  inherent  data  distribution.  In  this  paper,  we  propose  a  novel  vector  quantization  (VQ)  method  named  affinity  preserving  quantization  (APQ)  to  improve  the  quantization  quality  of  projection  values,  which  has  significantly  boosted  the  performance  of  state-of-the-art  hashing  techniques.  In  particular,  our  method  incorporates  the  neighbourhood  structure  in  the  pre-  and  post-projection  data  space  into  vector  quantization.  APQ  minimizes  the  quantization  errors  of  projection  values  as  well  as  the  loss  of  affinity  property  of  original  space.  An  effective  algorithm  has  been  proposed  to  solve  the  joint  optimization  problem  in  APQ,  and  the  extension  to  larger  binary  codes  has  been  resolved  by  applying  product  quantization  to  APQ.  Extensive  experiments  have  shown  that  APQ  consistently  outperforms  the  state-of-the-art  quantization  methods,  and  has  significantly  improved  the  performance  of  various  hashing  techniques.
0	Bounding  the  support  size  in  extensive  form  games  with  imperfect  information.  It  is  a  well  known  fact  that  in  extensive  form  games  with  perfect  information,  there  is  a  Nash  equilibrium  with  support  of  size  one.  This  doesn't  hold  for  games  with  imperfect  information,  where  the  size  of  minimal  support  can  be  larger.  We  present  a  dependency  between  the  level  of  uncertainty  and  the  minimum  support  size.  For  many  games,  there  is  a  big  disproportion  between  the  game  uncertainty  and  the  number  of  actions  available.  In  Bayesian  extensive  games  with  perfect  information,  the  only  uncertainty  is  about  the  type  of  players.  In  card  games,  the  uncertainty  comes  from  dealing  the  deck.  In  these  games,  we  can  significantly  reduce  the  support  size.  Our  result  applies  to  general-sum  extensive  form  games  with  any  finite  number  of  players.
0	Beyond  rnns  positional  self  attention  with  co  attention  for  video  question  answering.  Most  of  the  recent  progresses  on  visual  question  answering  are  based  on  recurrent  neural  networks  (RNNs)  with  attention.  Despite  the  success,  these  models  are  often  timeconsuming  and  having  difficulties  in  modeling  long  range  dependencies  due  to  the  sequential  nature  of  RNNs.  We  propose  a  new  architecture,  Positional  Self-Attention  with  Coattention  (PSAC),  which  does  not  require  RNNs  for  video  question  answering.  Specifically,  inspired  by  the  success  of  self-attention  in  machine  translation  task,  we  propose  a  Positional  Self-Attention  to  calculate  the  response  at  each  position  by  attending  to  all  positions  within  the  same  sequence,  and  then  add  representations  of  absolute  positions.  Therefore,  PSAC  can  exploit  the  global  dependencies  of  question  and  temporal  information  in  the  video,  and  make  the  process  of  question  and  video  encoding  executed  in  parallel.  Furthermore,  in  addition  to  attending  to  the  video  features  relevant  to  the  given  questions  (i.e.,  video  attention),  we  utilize  the  co-attention  mechanism  by  simultaneously  modeling  “what  words  to  listen  to”  (question  attention).  To  the  best  of  our  knowledge,  this  is  the  first  work  of  replacing  RNNs  with  selfattention  for  the  task  of  visual  question  answering.  Experimental  results  of  four  tasks  on  the  benchmark  dataset  show  that  our  model  significantly  outperforms  the  state-of-the-art  on  three  tasks  and  attains  comparable  result  on  the  Count  task.  Our  model  requires  less  computation  time  and  achieves  better  performance  compared  with  the  RNNs-based  methods.  Additional  ablation  study  demonstrates  the  effect  of  each  component  of  our  proposed  model.
0	Greedy  maximization  of  functions  with  bounded  curvature  under  partition  matroid  constraints.  We  investigate  the  performance  of  a  deterministic  GREEDY  algorithm  for  the  problem  of  maximizing  functions  under  a  partition  matroid  constraint.  We  consider  non-monotone  submodular  functions  and  monotone  subadditive  functions.  Even  though  constrained  maximization  problems  of  monotone  submodular  functions  have  been  extensively  studied,  little  is  known  about  greedy  maximization  of  non-monotone  submodular  functions  or  monotone  subadditive  functions.  We  give  approximation  guarantees  for  GREEDY  on  these  problems,  in  terms  of  the  curvature.  We  find  that  this  simple  heuristic  yields  a  strong  approximation  guarantee  on  a  broad  class  of  functions.  We  discuss  the  applicability  of  our  results  to  three  real-world  problems:  Maximizing  the  determinant  function  of  a  positive  semidefinite  matrix,  and  related  problems  such  as  the  maximum  entropy  sampling  problem,  the  constrained  maximum  cut  problem  on  directed  graphs,  and  combinatorial  auction  games.  We  conclude  that  GREEDY  is  well-suited  to  approach  these  problems.  Overall,  we  present  evidence  to  support  the  idea  that,  when  dealing  with  constrained  maximization  problems  with  bounded  curvature,  one  needs  not  search  for  (approximate)  monotonicity  to  get  good  approximate  solutions.
0	Off  policy  evaluation  in  partially  observable  environments.  This  work  studies  the  problem  of  batch  off-policy  evaluation  for  Reinforcement  Learning  in  partially  observable  environments.  Off-policy  evaluation  under  partial  observability  is  inherently  prone  to  bias,  with  risk  of  arbitrarily  large  errors.  We  define  the  problem  of  off-policy  evaluation  for  Partially  Observable  Markov  Decision  Processes  (POMDPs)  and  establish  what  we  believe  is  the  first  off-policy  evaluation  result  for  POMDPs.  In  addition,  we  formulate  a  model  in  which  observed  and  unobserved  variables  are  decoupled  into  two  dynamic  processes,  called  a  Decoupled  POMDP.  We  show  how  off-policy  evaluation  can  be  performed  under  this  new  model,  mitigating  estimation  errors  inherent  to  general  POMDPs.  We  demonstrate  the  pitfalls  of  off-policy  evaluation  in  POMDPs  using  a  well-known  off-policy  method,  Importance  Sampling,  and  compare  it  with  our  result  on  synthetic  medical  data.
0	Tweet  timeline  generation  with  determinantal  point  processes.  The  task  of  tweet  timeline  generation  (TTG)  aims  at  selecting  a  small  set  of  representative  tweets  to  generate  a  meaningful  timeline  and  providing  enough  coverage  for  a  given  topical  query.  This  paper  presents  an  approach  based  on  determinantal  point  processes  (DPPs)  by  jointly  modeling  the  topical  relevance  of  each  selected  tweet  and  overall  selectional  diversity.  Aiming  at  better  treatment  for  balancing  relevance  and  diversity,  we  introduce  two  novel  strategies,  namely  spectral  rescaling  and  topical  prior.  Extensive  experiments  on  the  public  TREC  2014  dataset  demonstrate  that  our  proposed  DPP  model  along  with  the  two  strategies  can  achieve  fairly  competitive  results  against  the  state-of-the-art  TTG  systems.
0	Atp  directed  graph  embedding  with  asymmetric  transitivity  preservation.  Directed  graphs  have  been  widely  used  in  Community  Question  Answering  services  (CQAs)  to  model  asymmetric  relationships  among  different  types  of  nodes  in  CQA  graphs,  e.g.,  question,  answer,  user.  Asymmetric  transitivity  is  an  essential  property  of  directed  graphs,  since  it  can  play  an  important  role  in  downstream  graph  inference  and  analysis.  Question  difficulty  and  user  expertise  follow  the  characteristic  of  asymmetric  transitivity.  Maintaining  such  properties,  while  reducing  the  graph  to  a  lower  dimensional  vector  embedding  space,  has  been  the  focus  of  much  recent  research.  In  this  paper,  we  tackle  the  challenge  of  directed  graph  embedding  with  asymmetric  transitivity  preservation  and  then  leverage  the  proposed  embedding  method  to  solve  a  fundamental  task  in  CQAs:  how  to  appropriately  route  and  assign  newly  posted  questions  to  users  with  the  suitable  expertise  and  interest  in  CQAs.  The  technique  incorporates  graph  hierarchy  and  reachability  information  naturally  by  relying  on  a  nonlinear  transformation  that  operates  on  the  core  reachability  and  implicit  hierarchy  within  such  graphs.  Subsequently,  the  methodology  levers  a  factorization-based  approach  to  generate  two  embedding  vectors  for  each  node  within  the  graph,  to  capture  the  asymmetric  transitivity.  Extensive  experiments  show  that  our  framework  consistently  and  significantly  outperforms  the  state-of-the-art  baselines  on  three  diverse  realworld  tasks:  link  prediction,  and  question  difficulty  estimation  and  expert  finding  in  online  forums  like  Stack  Exchange.  Particularly,  our  framework  can  support  inductive  embedding  learning  for  newly  posted  questions  (unseen  nodes  during  training),  and  therefore  can  properly  route  and  assign  these  kinds  of  questions  to  experts  in  CQAs.
0	Fast  relational  probabilistic  inference  and  learning  approximate  counting  via  hypergraphs.  Counting  the  number  of  true  instances  of  a  clause  is  arguably  a  major  bottleneck  in  relational  probabilistic  inference  and  learning.  We  approximate  counts  in  two  steps:  (1)  transform  the  fully  grounded  relational  model  to  a  large  hypergraph,  and  partially-instantiated  clauses  to  hypergraph  motifs;  (2)  since  the  expected  counts  of  the  motifs  are  provably  the  clause  counts,  approximate  them  using  summary  statistics  (in/outdegrees,  edge  counts,  etc).  Our  experimental  results  demonstrate  the  efficiency  of  these  approximations,  which  can  be  applied  to  many  complex  statistical  relational  models,  and  can  be  significantly  faster  than  state-of-the-art,  both  for  inference  and  learning,  without  sacrificing  effectiveness.
0	Towards  a  unified  framework  for  syntactic  inconsistency  measures.  A  number  of  proposals  have  been  made  to  define  inconsistency  measures.  Each  has  its  rationale.  But  to  date,  it  is  not  clear  how  to  delineate  the  space  of  options  for  measures,  nor  is  it  clear  how  we  can  classify  measures  systematically.  In  this  paper,  we  introduce  a  general  framework  for  comparing  syntactic  inconsistency  measures.  It  uses  the  construction  of  an  inconsistency  graph  for  each  knowledgebase.  We  then  introduce  abstractions  of  the  inconsistency  graph  and  use  the  hierarchy  of  the  abstractions  to  classify  a  range  of  inconsistency  measures.
0	Inverse  reinforcement  learning  from  like  minded  teachers.  We  study  the  problem  of  learning  a  policy  in  a  Markov  decision  process  (MDP)  based  on  observations  of  the  actions  taken  by  multiple  teachers.  We  assume  that  the  teachers  are  like-minded  in  that  their  reward  functions  --  while  different  from  each  other  --  are  random  perturbations  of  an  underlying  reward  function.  Under  this  assumption,  we  demonstrate  that  inverse  reinforcement  learning  algorithms  that  satisfy  a  certain  property  --  that  of  matching  feature  expectations  --  yield  policies  that  are  approximately  optimal  with  respect  to  the  underlying  reward  function,  and  that  no  algorithm  can  do  better  in  the  worst  case.  We  also  show  how  to  efficiently  recover  the  optimal  policy  when  the  MDP  has  one  state  --  a  setting  that  is  akin  to  multi-armed  bandits.
0	Fastened  crown  tightened  neural  network  robustness  certificates.  The  rapid  growth  of  deep  learning  applications  in  real  life  is  accompanied  by  severe  safety  concerns.  To  mitigate  this  uneasy  phenomenon,  much  research  has  been  done  providing  reliable  evaluations  of  the  fragility  level  in  different  deep  neural  networks.  Apart  from  devising  adversarial  attacks,  quantifiers  that  certify  safeguarded  regions  have  also  been  designed  in  the  past  five  years.  The  summarizing  work  in  (Salman  et  al.  2019)  unifies  a  family  of  existing  verifiers  under  a  convex  relaxation  framework.  We  draw  inspiration  from  such  work  and  further  demonstrate  the  optimality  of  deterministic  CROWN  (Zhang  et  al.  2018)  solutions  in  a  given  linear  programming  problem  under  mild  constraints.  Given  this  theoretical  result,  the  computationally  expensive  linear  programming  based  method  is  shown  to  be  unnecessary.  We  then  propose  an  optimization-based  approach  FROWN  (Fastened  CROWN):  a  general  algorithm  to  tighten  robustness  certificates  for  neural  networks.  Extensive  experiments  on  various  networks  trained  individually  verify  the  effectiveness  of  FROWN  in  safeguarding  larger  robust  regions.
0	Iterative  learning  cascade  control  of  continuous  noninvasive  blood  pressure  measurement.  A  noninvasive  continuous  blood  pressure  measurement  technique  that  has  been  developed  lately  requires  precise  control  of  the  blood  flow  through  a  superficial  artery.  The  flow  is  measured  using  ultrasound  and  influenced  via  manipulating  the  pressure  inside  an  inflatable  air  balloon  which  is  placed  over  the  artery.  This  contribution  is  concerned  with  the  design  and  evaluation  of  a  learning  cascaded  control  structure  for  such  measurement  devices.  Two  feedback  control  loops  are  designed  in  discrete  time  via  pole  placement  and  then  combined  with  an  iterative  learning  control.  The  latter  exploits  the  repetitive  nature  of  the  disturbance  that  is  induced  by  the  oscillating  arterial  pressure.  Experimental  results  indicate  that  the  proposed  controller  structure  yields  considerably  smaller  set  point  deviations  than  previous  approaches.
0	Preparing  drivers  for  dangerous  situations  a  critical  reflection  on  continuous  shared  control.  Shared  control  (also  known  as  continuous  haptic  guidance  or  haptically  active  controls)  has  recently  been  introduced  in  car  driving.  With  shared  control,  the  driver  receives  continuous  force  feedback  on  the  gas  pedal  or  steering  wheel,  so  that  human  and  machine  conduct  the  driving  task  simultaneously.  Experiments  in  driving  simulators  have  shown  that  shared  control  reduces  control  variability  and  mental  workload,  and  improves  accuracy  in  path  tracking  and  car  following.  Crucial  to  road  safety,  however,  is  not  whether  shared  control  improves  performance  in  routine  driving  tasks,  but  what  happens  in  dangerous  situations  when  a  conflict  of  authority  occurs,  or  when  the  force  feedback  cannot  be  relied  upon  or  is  suddenly  disengaged.  Drawing  on  research  into  transfer  of  training,  it  is  shown  that  shared  control  may  induce  aftereffects  and  may  hamper  retention  of  robust  driving  skills.  Supplementary  information  should  not  be  provided  continuously,  but  on  an  as-needed  basis,  warning  or  assisting  drivers  only  when  deviations  from  acceptable  tolerance  limits  arise.
0	Modeling  service  execution  on  data  centers  for  energy  efficiency  and  quality  of  service  monitoring.  This  work  provides  a  system  modeling  approach  to  describe  a  virtualized  data  center  environment  running  a  business  process.  This  model  allows  the  collection  of  simulation  data  at  different  workload  rates  that  can  be  used  as  a  dataset  for  reasoning  about  quality  of  service  and  energy  efficiency  issues  related  to  the  process.  The  model  overcomes  the  issue  of  obtaining  real  data  from  data  center  administrators  and  collecting  relevant  information  needed  for  studying  the  process  behavior.  The  model  is  flexible  and  can  be  used  to  model  data  centers  of  different  dimensions  and  characteristics.  It  can  also  be  used  for  "what-if"  analysis  about  the  system  configuration,  predicting  the  outcome  of  a  modification  over  energy  efficiency  and  quality  of  service.
0	Evolving  a  multiagent  controller  for  micro  aerial  vehicles.  Micro  aerial  vehicles  (MAVs)  are  notoriously  difficult  to  control  as  they  are  light,  susceptible  to  minor  fluctuations  in  the  environment,  and  obey  highly  nonlinear  dynamics.  Indeed,  traditional  control  methods,  particularly  those  relying  on  difficult  to  obtain  models  of  the  interaction  between  an  MAV  and  its  environment,  have  been  unable  to  provide  adequate  control  beyond  simple  maneuvers.  In  this  paper,  we  address  the  problem  of  controlling  an  MAV  (which  has  segmented  control  surfaces)  by  evolving  a  neurocontroller  and  fine  tuning  it  using  multiagent  coordination  techniques.  This  approach  is  based  on  a  control  strategy  that  learns  to  map  MAV  states  (position  and  velocity)  to  MAV  actions  (e.g.,  actuator  position)  to  achieve  good  performance  (e.g.,  flight  time)  by  maximizing  an  objective  function.  The  main  difficulty  with  this  approach  is  defining  the  objective  functions  at  the  MAV  level  that  allow  good  performance.  In  addition,  to  provide  added  robustness,  we  investigate  a  multiagent  approach  to  control  where  each  control  surface  aims  to  optimize  a  local  objective.  Our  results  show  that  this  approach  not  only  provides  good  MAV  control,  but  provides  robustness  to:  1)  wind  gusts  by  a  factor  of  6;  2)  turbulence  by  a  factor  of  4;  and  3)  hardware  failures  by  a  factor  of  8  over  a  traditional  control  method.
0	Adaptive  fuzzy  control  for  nonlinear  networked  control  systems.  This  paper  studies  the  problem  of  adaptive  fuzzy  control  for  a  category  of  single-input  single-output  nonlinear  networked  control  systems  with  network-induced  delay  and  data  loss  based  on  adaptive  backstepping  control  approach.  Fuzzy  logic  systems  are  used  to  approximate  the  unknown  nonlinear  characteristics  existing  in  the  system,  while  Pade  approximation  is  introduced  to  handle  network-induced  delay.  Data  loss  occurs  intermittently  and  stochastically  in  the  data  transmitting  process,  which  is  regarded  as  the  delay  in  the  controller  design.  In  the  framework  of  adaptive  fuzzy  backstepping  technique,  a  novel  state-feedback  adaptive  controller  is  constructed  to  ensure  all  signals  in  the  resulting  closed-loop  system  to  be  bounded  and  the  state  variables  can  be  regulated  to  the  origin.  Finally,  two  examples  are  given  to  show  the  validity  of  the  proposed  results.
0	A  low  cost  semi  autonomous  wheelchair  controlled  by  motor  imagery  and  jaw  muscle  activation.  Wheelchairs  controlled  using  electroencephalography  (EEG)  have  been  proposed  to  facilitate  independent  mobility  for  people  with  motor  disabilities.  To  date,  the  majority  of  these  systems  have  relied  on  distracting  external  stimuli  such  as  flashing  lights  and  expensive,  medical-grade  EEG  amplifiers.  We  propose  a  wheelchair  prototype  that  uses  hand  motor  imagery  (MI)  and  jaw  clench  data  collected  with  a  consumer-grade  EEG  system  to  generate  left,  right,  forward,  and  stop  commands.  The  signal  is  classified  with  logistic  regression,  and  using  only  two  scalp  electrodes  and  a  two-second  window  size,  we  obtained  a  mean  subject  accuracy  of  60  ±  5%  and  a  peak  subject  accuracy  of  82±  3%.  We  introduce  a  novel  control-flow  paradigm  relying  on  an  intermediate  control  state,  engaged  by  jaw  clenching,  to  reduce  the  complexity  of  our  classification  problem,  as  well  as  real-time  spectrograms  for  neurofeedback  training.  Additionally,  we  supplement  our  system  with  automated  driving  features,  a  location  tracker,  and  a  heart-rate  monitor  to  increase  usability  and  safety.
0	Quantum  annealing  algorithm  for  vehicle  scheduling.  In  this  paper  we  propose  a  new  strategy  for  solving  the  Capacitated  Vehicle  Routing  Problem  using  a  quantum  annealing  algorithm.  The  Capacitated  Vehicle  Routing  Problem  is  a  variant  of  the  Vehicle  Routing  Problem  being  characterized  by  capacitated  vehicles  which  contain  goods  up  to  a  certain  maximum  capacity.  Quantum  annealing  is  a  metaheuristic  that  uses  quantum  tunneling  in  the  annealing  process.  We  discuss  devising  a  spin  encoding  scheme  for  solving  the  Capacitated  Vehicle  Routing  Problem  with  a  quantum  annealing  algorithm  and  an  empirical  approach  for  tuning  parameters.  We  study  the  effectiveness  of  quantum  annealing  in  comparison  with  best  known  solutions  for  a  range  of  benchmark  instances.
0	Naturally  embedded  ssvep  phase  tagging  in  a  p300  based  bci  lsc  4q  speller.  This  paper  proposes  a  P300-based  BCI  speller  called  LSC-4Q  that  takes  advantage  of  steady  state  visual  evoked  potentials  (SSVEP)  that  appear  naturally  in  the  brain  as  a  side  effect  of  the  inter-stimulus  interval  of  P300  visual  paradigms.  The  LSC-4Q  speller  has  a  circular  layout  divided  into  quadrants,  and  symbols  flash  individually  with  a  given  pseudo-random  strategy.  Controlling  the  sequence  of  the  events  such  that  consecutive  flashes  alternate  between  sides  or  quadrants  of  the  speller,  we  research  the  possibility  of  detecting  the  SSVEP  phase  associated  with  the  side  or  quadrant  for  which  the  user  is  focusing  on  the  target  symbol,  without  explicitly  incorporating  a  SSVEP  flickering  stimulator.  The  SSVEP  phase  is  extracted  using  a  statistical  spatio-spectral  Fisher  criterion  beamformer  (SSFCB)  implemented  in  the  frequency  domain.  Results  show  that  SSFCB  efficiently  extracts  phase  tags  from  the  SSVEPs  embedded  on  the  visual  evoked  potentials  of  the  oddball  paradigm.  Preliminary  results  with  4  participants  suggest  that  it  is  possible  to  detect  with  high  accuracy  the  side  of  the  screen  to  which  the  user  is  looking,  and  with  less  precision  the  detection  of  the  quadrant.  Main  issues  are  related  to  phase  variability  across  sessions.  Online  results  show  that  some  participants  can  benefit  from  the  combined  P300-Lateral  approach,  improving  the  overall  classification  when  P300  misclassifications  occur.
0	Furuta  pendulum  a  tensor  product  model  based  design  approach  case  study.  The  main  purpose  of  this  paper  to  demonstrate  how  to  apply  basic  convex  optimization  on  robot  controller  design  specifically  on  the  example  of  the  inverted  Furuta  pendulum.  The  work  shows  how  convex  optimization  can  be  implemented  and  that  the  Tensor  Product-based  approach  could  be  easily  applied  for  robot  stabilization.  The  design  stages  are  shown  from  the  quasi  Linear  Parameter  Varying  form  of  the  Lagrange-Euler  equations  through  giving  a  deeper  insight  into  Tensor  Product  model  transformation  until  the  optimal  control  problem  formulation  via  Linear  Matrix  Inequalities.  The  design  method  is  well  supported  by  TPtool  and  CVX  Mat  Lab  toolboxes  that  are  both  convenient  to  use.  Numerical  simulations  have  also  been  performed  to  show  the  power  of  the  computational  solution  of  robot  stabilization.
0	Precision  recall  versus  accuracy  and  the  role  of  large  data  sets.  Practitioners  of  data  mining  and  machine  learning  have  long  observed  that  the  imbalance  of  classes  in  a  data  set  negatively  impacts  the  quality  of  classifiers  trained  on  that  data.  Numerous  techniques  for  coping  with  such  imbalances  have  been  proposed,  but  nearly  all  lack  any  theoretical  grounding.  By  contrast,  the  standard  theoretical  analysis  of  machine  learning  admits  no  dependence  on  the  imbalance  of  classes  at  all.  The  basic  theorems  of  statistical  learning  establish  the  number  of  examples  needed  to  estimate  the  accuracy  of  a  classifier  as  a  function  of  its  complexity  (VC-dimension)  and  the  confidence  desired;  the  class  imbalance  does  not  enter  these  formulas  anywhere.  In  this  work,  we  consider  the  measures  of  classifier  performance  in  terms  of  precision  and  recall,  a  measure  that  is  widely  suggested  as  more  appropriate  to  the  classification  of  imbalanced  data.  We  observe  that  whenever  the  precision  is  moderately  large,  the  worse  of  the  precision  and  recall  is  within  a  small  constant  factor  of  the  accuracy  weighted  by  the  class  imbalance.  A  corollary  of  this  observation  is  that  a  larger  number  of  examples  is  necessary  and  sufficient  to  address  class  imbalance,  a  finding  we  also  illustrate  empirically.
0	Hierarchical  classification  based  on  label  distribution  learning.  Hierarchical  classification  is  a  challenging  problem  where  the  class  labels  are  organized  in  a  predefined  hierarchy.  One  primary  challenge  in  hierarchical  classification  is  the  small  training  set  issue  of  the  local  module.  The  local  classifiers  in  the  previous  hierarchical  classification  approaches  are  prone  to  over-fitting,  which  becomes  a  major  bottleneck  of  hierarchical  classification.  Fortunately,  the  labels  in  the  local  module  are  correlated,  and  the  siblings  of  the  true  label  can  provide  additional  supervision  information  for  the  instance.  This  paper  proposes  a  novel  method  to  deal  with  the  small  training  set  issue.  The  key  idea  of  the  method  is  to  represent  the  correlation  among  the  labels  by  the  label  distribution.  It  generates  a  label  distribution  that  contains  the  supervision  information  of  each  label  for  the  given  instance,  and  then  learns  a  mapping  from  the  instance  to  the  label  distribution.  Experimental  results  on  several  hierarchical  classification  datasets  show  that  our  method  significantly  outperforms  other  state-of-theart  hierarchical  classification  approaches.
0	Sample  bounded  distributed  reinforcement  learning  for  decentralized  pomdps.  Decentralized  partially  observable  Markov  decision  processes  (Dec-POMDPs)  offer  a  powerful  modeling  technique  for  realistic  multi-agent  coordination  problems  under  uncertainty.  Prevalent  solution  techniques  are  centralized  and  assume  prior  knowledge  of  the  model.  We  propose  a  distributed  reinforcement  learning  approach,  where  agents  take  turns  to  learn  best  responses  to  each  other's  policies.  This  promotes  decentralization  of  the  policy  computation  problem,  and  relaxes  reliance  on  the  full  knowledge  of  the  problem  parameters.  We  derive  the  relation  between  the  sample  complexity  of  best  response  learning  and  error  tolerance.  Our  key  contribution  is  to  show  that  sample  complexity  could  grow  exponentially  with  the  problem  horizon.  We  show  empirically  that  even  if  the  sample  requirement  is  set  lower  than  what  theory  demands,  our  learning  approach  can  produce  (near)  optimal  policies  in  some  benchmark  Dec-POMDP  problems.
0	An  adaptive  framework  for  conversational  question  answering.  In  Conversational  Question  Answering  (CoQA),  humans  propose  a  series  of  questions  to  satisfy  their  information  needs.  Based  on  our  preliminary  analysis,  there  are  two  major  types  of  questions,  namely  verification  questions  and  knowledgeseeking  questions.  The  first  one  is  to  verify  some  existing  facts,  while  the  latter  is  to  obtain  new  knowledge  about  some  specific  object.  These  two  types  of  questions  differ  significantly  in  their  answering  ways.  However,  existing  methods  usually  treat  them  uniformly,  which  may  easily  be  biased  by  the  dominant  type  of  questions  and  obtain  inferior  overall  performance.  In  this  work,  we  propose  an  adaptive  framework  to  handle  these  two  types  of  questions  in  different  ways  based  on  their  own  characteristics.  We  conduct  experiments  on  the  recently  released  CoQA  benchmark  dataset,  and  the  results  demonstrate  that  our  method  outperforms  the  state-of-the-art  baseline  methods.
0	Opportunities  and  challenges  for  artificial  intelligence  in  india.  In  the  future  of  India  lies  the  future  of  a  sixth  of  the  world's  population.  As  the  Artificial  Intelligence  (AI)  revolution  sweeps  through  societies  and  enters  daily  life,  its  role  in  shaping  India's  development  and  growth  is  bound  to  be  substantial.  For  India,  AI  holds  promise  as  a  catalyst  to  accelerate  progress,  while  providing  mechanisms  to  leapfrog  traditional  hurdles  such  as  poor  infrastructure  and  bureaucracy.  At  the  same  time,  an  investment  in  AI  is  accompanied  by  risk  factors  with  long-term  implications  on  society:  it  is  imperative  that  risks  be  vetted  at  this  early  stage.  In  this  paper,  we  describe  opportunities  and  challenges  for  AI  in  India.  We  detail  opportunities  that  are  cross-cutting  (bridging  India's  linguistic  divisions,  mining  public  data),  and  also  specific  to  one  particular  sector  (healthcare).  We  list  challenges  that  originate  from  existing  social  conditions  (such  as  equations  of  caste  and  gender).  Thereafter  we  distill  out  concrete  steps  and  safeguards,  which  we  believe  are  necessary  for  robust  and  inclusive  development  as  India  enters  the  AI  era.
0	Fouriersat  a  fourier  expansion  based  algebraic  framework  for  solving  hybrid  boolean  constraints.  The  Boolean  SATisfiability  problem  (SAT)  is  of  central  importance  in  computer  science.  Although  SAT  is  known  to  be  NP-complete,  progress  on  the  engineering  side—especially  that  of  Conflict-Driven  Clause  Learning  (CDCL)  and  Local  Search  SAT  solvers—has  been  remarkable.  Yet,  while  SAT  solvers,  aimed  at  solving  industrial-scale  benchmarks  in  Conjunctive  Normal  Form  (CNF),  have  become  quite  mature,  SAT  solvers  that  are  effective  on  other  types  of  constraints  (e.g.,  cardinality  constraints  and  XORs)  are  less  well-studied;  a  general  approach  to  handling  non-CNF  constraints  is  still  lacking.  In  addition,  previous  work  indicated  that  for  specific  classes  of  benchmarks,  the  running  time  of  extant  SAT  solvers  depends  heavily  on  properties  of  the  formula  and  details  of  encoding,  instead  of  the  scale  of  the  benchmarks,  which  adds  uncertainty  to  expectations  of  running  time.To  address  the  issues  above,  we  design  FourierSAT,  an  incomplete  SAT  solver  based  on  Fourier  analysis  of  Boolean  functions,  a  technique  to  represent  Boolean  functions  by  multilinear  polynomials.  By  such  a  reduction  to  continuous  optimization,  we  propose  an  algebraic  framework  for  solving  systems  consisting  of  different  types  of  constraints.  The  idea  is  to  leverage  gradient  information  to  guide  the  search  process  in  the  direction  of  local  improvements.  Empirical  results  demonstrate  that  FourierSAT  is  more  robust  than  other  solvers  on  certain  classes  of  benchmarks.
0	Learning  cp  net  preferences  online  from  user  queries.  We  present  an  online,  heuristic  algorithm  for  learning  Conditional  Preference  networks  CP-nets  from  user  queries.  This  is  the  first  efficient  and  resolute  CP-net  learning  algorithm:  if  a  preference  order  can  be  represented  as  a  CP-net,  our  algorithm  learns  a  CP-net  in  time  n  p  ,  where  p  is  a  bound  on  the  number  of  parents  a  node  may  have.  The  learned  CP-net  is  guaranteed  to  be  consistent  with  the  original  CP-net  on  all  queries  from  the  learning  process.  We  tested  the  algorithm  on  randomly  generated  CP-nets;  the  learned  CP-nets  agree  with  the  originals  on  a  high  percent  of  non-training  preference  comparisons.
0	Learning  to  generate  posters  of  scientific  papers.  Researchers  often  summarize  their  work  in  the  form  of  posters.  Posters  provide  a  coherent  and  efficient  way  to  convey  core  ideas  from  scientific  papers.  Generating  a  good  scientific  poster,  however,  is  a  complex  and  time  consuming  cognitive  task,  since  such  posters  need  to  be  readable,  informative,  and  visually  aesthetic.  In  this  paper,  for  the  first  time,  we  study  the  challenging  problem  of  learning  to  generate  posters  from  scientific  papers.  To  this  end,  a  data-driven  framework,  that  utilizes  graphical  models,  is  proposed.  Specifically,  given  content  to  display,  the  key  elements  of  a  good  poster,  including  panel  layout  and  attributes  of  each  panel,  are  learned  and  inferred  from  data.  Then,  given  inferred  layout  and  attributes,  composition  of  graphical  elements  within  each  panel  is  synthesized.  To  learn  and  validate  our  model,  we  collect  and  make  public  a  Poster-Paper  dataset,  which  consists  of  scientific  papers  and  corresponding  posters  with  exhaustively  labelled  panels  and  attributes.  Qualitative  and  quantitative  results  indicate  the  effectiveness  of  our  approach.
0	Automatic  abstraction  in  reinforcement  learning  using  ant  system  algorithm.  Nowadays  developing  autonomous  systems,  which  can  act  in  various  environments  and  interactively  perform  their  assigned  tasks,  are  intensively  desirable.  These  systems  would  be  ready  to  be  applied  in  different  fields  such  as  medicine,  controller  robots  and  social  life.  Reinforcement  learning  is  an  attractive  area  of  machine  learning  which  addresses  these  concerns.  In  large  scales,  learning  performance  of  an  agent  can  be  improved  by  using  hierarchical  Reinforcement  Learning  techniques  and  temporary  extended  actions.  The  higher  level  of  abstraction  helps  the  learning  agent  approach  lifelong  learning  goals.  In  this  paper  a  new  method  is  presented  for  discovering  subgoal  states  and  constructing  useful  skills.  The  method  utilizes  Ant  System  optimization  algorithm  to  identify  bottleneck  edges,  which  act  like  bridges  between  different  connected  areas  of  the  problem  space.  Using  discovered  subgoals,  the  agent  creates  temporal  abstractions,  which  enable  it  to  explore  more  effectively.  Experimental  Results  show  that  the  proposed  method  can  significantly  improve  the  learning  performance  of  the  agent.
0	The  news  we  like  are  not  the  news  we  visit  news  categories  popularity  in  usage  data.  Most  of  our  knowledge  about  online  news  consumption  comes  from  survey-based  news  market  reports,  partial  usage  data  from  a  single  editor,  or  what  people  publicly  share  on  social  networks.  This  paper  complements  these  sources  by  presenting  the  first  holistic  study  of  visits  across  online  news  outlets  that  a  population  uses  to  read  news.  We  monitor  the  entire  network  traffic  generated  by  Internet  users  in  four  locations  in  Italy.  Together  these  users  generated  80  million  visits  to  5.4  million  news  articles  in  about  one  year  and  a  half.  This  unique  view  allows  us  to  evaluate  how  usage  data  complements  existing  data  sources.  We  find  for  instance  that  only  16%  of  news  visits  in  our  datasets  came  from  online  social  networks.  In  addition,  the  popularity  of  news  categories  when  considering  all  visits  is  quite  different  from  the  one  when  considering  only  news  discovered  on  social  media,  or  visits  to  a  single  major  news  outlet.  Interestingly,  a  substantial  mismatch  emerges  between  self-reported  news-category  preferences  (as  measured  by  Reuters  Institute  in  the  same  year  and  same  country)  and  their  actual  popularity  in  terms  of  visits  in  our  datasets.  In  particular,  unlike  self-reported  preferences  expressed  by  users  in  surveys  that  put  “Politics”,  “Science”  and  “International”  as  the  most  appreciated  categories,  “Tragedies  and  Weird  news”’  and  “Sport”  are  by  far  the  most  visited.  We  discuss  two  possible  causes  of  this  mismatch  and  conjecture  that  the  most  plausible  reason  is  the  disassociation  that  may  occur  between  individuals’  cognitive  values  and  their  cue-triggered  attraction.
0	Metastyle  three  way  trade  off  among  speed  flexibility  and  quality  in  neural  style  transfer.  An  unprecedented  booming  has  been  witnessed  in  the  research  area  of  artistic  style  transfer  ever  since  Gatys  et  al.  introduced  the  neural  method.  One  of  the  remaining  challenges  is  to  balance  a  trade-off  among  three  critical  aspects—speed,  flexibility,  and  quality:  (i)  the  vanilla  optimization-based  algorithm  produces  impressive  results  for  arbitrary  styles,  but  is  unsatisfyingly  slow  due  to  its  iterative  nature,  (ii)  the  fast  approximation  methods  based  on  feed-forward  neural  networks  generate  satisfactory  artistic  effects  but  bound  to  only  a  limited  number  of  styles,  and  (iii)  feature-matching  methods  like  AdaIN  achieve  arbitrary  style  transfer  in  a  real-time  manner  but  at  a  cost  of  the  compromised  quality.  We  find  it  considerably  difficult  to  balance  the  trade-off  well  merely  using  a  single  feed-forward  step  and  ask,  instead,  whether  there  exists  an  algorithm  that  could  adapt  quickly  to  any  style,  while  the  adapted  model  maintains  high  efficiency  and  good  image  quality.  Motivated  by  this  idea,  we  propose  a  novel  method,  coined  MetaStyle,  which  formulates  the  neural  style  transfer  as  a  bilevel  optimization  problem  and  combines  learning  with  only  a  few  post-processing  update  steps  to  adapt  to  a  fast  approximation  model  with  satisfying  artistic  effects,  comparable  to  the  optimization-based  methods  for  an  arbitrary  style.  The  qualitative  and  quantitative  analysis  in  the  experiments  demonstrates  that  the  proposed  approach  achieves  high-quality  arbitrary  artistic  style  transfer  effectively,  with  a  good  trade-off  among  speed,  flexibility,  and  quality.
0	Towards  formal  definitions  of  blameworthiness  intention  and  moral  responsibility.  We  provide  formal  definitions  of  degree  of  blameworthiness  and  intention  relative  to  an  epistemic  state  (a  probability  over  causal  models  and  a  utility  function  on  outcomes).  These,  together  with  a  definition  of  actual  causality,  provide  the  key  ingredients  for  moral  responsibility  judgments.  We  show  that  these  definitions  give  insight  into  commonsense  intuitions  in  a  variety  of  puzzling  cases  from  the  literature.
0	A  few  queries  go  a  long  way  information  distortion  tradeoffs  in  matching.  We  consider  the  one-sided  matching  problem,  where  n  agents  have  preferences  over  n  items,  and  these  preferences  are  induced  by  underlying  cardinal  valuation  functions.  The  goal  is  to  match  every  agent  to  a  single  item  so  as  to  maximize  the  social  welfare.  Most  of  the  related  literature,  however,  assumes  that  the  values  of  the  agents  are  not  a  priori  known,  and  only  access  to  the  ordinal  preferences  of  the  agents  over  the  items  is  provided.  Consequently,  this  incomplete  information  leads  to  loss  of  efficiency,  which  is  measured  by  the  notion  of  distortion.  In  this  paper,  we  further  assume  that  the  agents  can  answer  a  small  number  of  queries,  allowing  us  partial  access  to  their  values.  We  study  the  interplay  between  elicited  cardinal  information  (measured  by  the  number  of  queries  per  agent)  and  distortion  for  one-sided  matching,  as  well  as  a  wide  range  of  well-studied  related  problems.  Qualitatively,  our  results  show  that  with  a  limited  number  of  queries,  it  is  possible  to  obtain  significant  improvements  over  the  classic  setting,  where  only  access  to  ordinal  information  is  given.
0	Gromov  wasserstein  factorization  models  for  graph  clustering.  We  propose  a  new  nonlinear  factorization  model  for  graphs  that  are  with  topological  structures,  and  optionally,  node  attributes.  This  model  is  based  on  a  pseudometric  called  Gromov-Wasserstein  (GW)  discrepancy,  which  compares  graphs  in  a  relational  way.  It  estimates  observed  graphs  as  GW  barycenters  constructed  by  a  set  of  atoms  with  different  weights.  By  minimizing  the  GW  discrepancy  between  each  observed  graph  and  its  GW  barycenter-based  estimation,  we  learn  the  atoms  and  their  weights  associated  with  the  observed  graphs.  The  model  achieves  a  novel  and  flexible  factorization  mechanism  under  GW  discrepancy,  in  which  both  the  observed  graphs  and  the  learnable  atoms  can  be  unaligned  and  with  different  sizes.  We  design  an  effective  approximate  algorithm  for  learning  this  Gromov-Wasserstein  factorization  (GWF)  model,  unrolling  loopy  computations  as  stacked  modules  and  computing  gradients  with  backpropagation.  The  stacked  modules  can  be  with  two  different  architectures,  which  correspond  to  the  proximal  point  algorithm  (PPA)  and  Bregman  alternating  direction  method  of  multipliers  (BADMM),  respectively.  Experiments  show  that  our  model  obtains  encouraging  results  on  clustering  graphs.
0	Large  scale  analogical  reasoning.  Cognitive  simulation  of  analogical  processing  can  be  used  to  answer  comparison  questions  such  as:  What  are  the  similarities  and/or  differences  between  A  and  B,  for  concepts  A  and  B  in  a  knowledge  base  (KB).  Previous  attempts  to  use  a  general-purpose  analogical  reasoner  to  answer  such  questions  revealed  three  major  problems:  (a)  the  system  presented  too  much  information  in  the  answer,  and  the  salient  similarity  or  difference  was  not  highlighted;  (b)  analogical  inference  found  some  incorrect  differences;  and  (c)  some  expected  similarities  were  not  found.  The  cause  of  these  problems  was  primarily  a  lack  of  a  well-curated  KB  and,  and  secondarily,  algorithmic  deficiencies.  In  this  paper,  relying  on  a  well-curated  biology  KB,  we  present  a  specific  implementation  of  comparison  questions  inspired  by  a  general  model  of  analogical  reasoning.  We  present  numerous  examples  of  answers  produced  by  the  system  and  empirical  data  on  answer  quality  to  illustrate  that  we  have  addressed  many  of  the  problems  of  the  previous  system.
0	Finding  most  compatible  phylogenetic  trees  over  multi  state  characters.  The  reconstruction  of  the  evolutionary  tree  of  a  set  of  species  based  on  qualitative  attributes  is  a  central  problem  in  phylogenetics.  In  the  NP-hard  perfect  phylogeny  problem  the  input  is  a  set  of  taxa  (species)  and  characters  (attributes)  on  them,  and  the  task  is  to  find  an  evolutionary  tree  that  describes  the  evolution  of  the  taxa  so  that  each  character  state  evolves  only  once.  However,  in  practical  situations  a  perfect  phylogeny  rarely  exists,  motivating  the  maximum  compatibility  problem  of  finding  the  largest  subset  of  characters  admitting  a  perfect  phylogeny.  Various  declarative  approaches,  based  on  applying  integer  programming  (IP),  answer  set  programming  (ASP)  and  pseudo-Boolean  optimization  (PBO)  solvers,  have  been  proposed  for  maximum  compatibility.  In  this  work  we  develop  a  new  hybrid  approach  to  solving  maximum  compatibility  for  multi-state  characters,  making  use  of  both  declarative  optimization  techniques  (specifically  maximum  satisfiability,  MaxSAT)  and  an  adaptation  of  the  Bouchitt'e-Todinca  approach  to  triangulation-based  graph  optimization  problems.  Empirically  our  approach  outperforms  in  scalability  the  earlier  proposed  approaches  w.r.t.  various  parameters  underlying  the  problem.
0	Temporal  interlacing  network.  For  a  long  time,  the  vision  community  tries  to  learn  the  spatio-temporal  representation  by  combining  convolutional  neural  network  together  with  various  temporal  models,  such  as  the  families  of  Markov  chain,  optical  flow,  RNN  and  temporal  convolution.  However,  these  pipelines  consume  enormous  computing  resources  due  to  the  alternately  learning  process  for  spatial  and  temporal  information.  One  natural  question  is  whether  we  can  embed  the  temporal  information  into  the  spatial  one  so  the  information  in  the  two  domains  can  be  jointly  learned  once-only.  In  this  work,  we  answer  this  question  by  presenting  a  simple  yet  powerful  operator  –  temporal  interlacing  network  (TIN).  Instead  of  learning  the  temporal  features,  TIN  fuses  the  two  kinds  of  information  by  interlacing  spatial  representations  from  the  past  to  the  future,  and  vice  versa.  A  differentiable  interlacing  target  can  be  learned  to  control  the  interlacing  process.  In  this  way,  a  heavy  temporal  model  is  replaced  by  a  simple  interlacing  operator.  We  theoretically  prove  that  with  a  learnable  interlacing  target,  TIN  performs  equivalently  to  the  regularized  temporal  convolution  network  (r-TCN),  but  gains  4%  more  accuracy  with  6x  less  latency  on  6  challenging  benchmarks.  These  results  push  the  state-of-the-art  performances  of  video  understanding  by  a  considerable  margin.  Not  surprising,  the  ensemble  model  of  the  proposed  TIN  won  the  1st  place  in  the  ICCV19  -  Multi  Moments  in  Time  challenge.  Code  is  made  available  to  facilitate  further  research.1
0	Self  calibrating  marker  tracking  in  3d  with  event  based  vision  sensors.  Following  an  object's  position  relative  to  oneself  is  a  fundamental  functionality  required  in  intelligent  real-world  interacting  robotic  systems.  This  paper  presents  a  computationally  efficient  vision  based  3D  tracking  system,  which  can  ultimately  operate  in  real-time  on  autonomous  mobile  robots  in  cluttered  environments.  At  the  core  of  the  system,  two  neural  inspired  event-based  dynamic  vision  sensors  (eDVS)  independently  track  a  high  frequency  flickering  LED  in  their  respective  2D  angular  coordinate  frame.  A  self-adjusted  feed-forward  neural  network  maps  those  independent  2D  angular  coordinates  into  a  Cartesian  3D  position  in  world  coordinates.  During  an  initial  calibration  phase,  an  object  composed  of  multiple  independent  markers  with  known  geometry  provides  relative  position  information  between  those  markers  for  network  training  (without  ever  using  absolute  world  coordinates  for  training).  In  a  subsequent  application  phase  tracking  a  single  marker  yields  position  estimates  relative  to  sensor  origin,  while  tracking  multiple  markers  provides  additional  orientation.  The  neural  inspired  vision-based  tracking  system  runs  in  real-time  on  ARM7  microcontrollers,  without  the  need  for  an  external  PC.
0	A  gaussian  process  reinforcement  learning  algorithm  with  adaptability  and  minimal  tuning  requirements.  We  present  a  novel  Bayesian  reinforcement  learning  algorithm  that  addresses  model  bias  and  exploration  overhead  issues.  The  algorithm  combines  different  aspects  of  several  state-of-the-art  reinforcement  learning  methods  that  use  Gaussian  Processes  model-based  approaches  to  increase  the  use  of  the  online  data  samples.  The  algorithm  uses  a  smooth  reward  function  requiring  the  reward  value  to  be  derived  from  the  environment  state.  It  works  with  continuous  states  and  actions  in  a  coherent  way  with  a  minimized  need  for  expert  knowledge  in  parameter  tuning.  We  analyse  and  discuss  the  practical  benefits  of  the  selected  approach  in  comparison  to  more  traditional  methodological  choices,  and  illustrate  the  use  of  the  algorithm  in  a  motor  control  problem  involving  a  two-link  simulated  arm.
0	Exploring  knowledge  engineering  strategies  in  designing  and  modelling  a  road  traffic  accident  management  domain.  Formulating  knowledge  for  use  in  AI  Planning  engines  is  currently  something  of  an  ad-hoc  process,  where  the  skills  of  knowledge  engineers  and  the  tools  they  use  may  significantly  influence  the  quality  of  the  resulting  planning  application.  There  is  little  in  the  way  of  guidelines  or  standard  procedures,  however,  for  knowledge  engineers  to  use  when  formulating  knowledge  into  planning  domain  languages  such  as  PDDL.  This  paper  seeks  to  investigate  this  process  using  as  a  case  study  a  road  traffic  accident  management  domain.    Managing  road  accidents  requires  systematic,  sound  planning  and  coordination  of  resources  to  improve  outcomes  for  accident  victims.  We  have  derived  a  set  of  requirements  in  consultation  with  stakeholders  for  the  resource  coordination  part  of  managing  accidents.  We  evaluate  two  separate  knowledge  engineering  strategies  for  encoding  the  resulting  planning  domain  from  the  set  of  requirements:  (a)  the  traditional  method  of  PDDL  experts  and  text  editor,  and  (b)  a  leading  planning  GUI  with  built  in  UML  modelling  tools.    These  strategies  are  evaluated  using  process  and  product  metrics,  where  the  domain  model  (the  product)  was  tested  extensively  with  a  range  of  planning  engines.  The  results  give  insights  into  the  strengths  and  weaknesses  of  the  approaches,  highlight  lessons  learned  regarding  knowledge  encoding,  and  point  to  important  lines  of  research  for  knowledge  engineering  for  planning.
0	Sequences  of  mechanisms  for  causal  reasoning  in  artificial  intelligence.  We  present  a  new  approach  to  token-level  causal  reasoning  that  we  call  Sequences  Of  Mechanisms  (SoMs),  which  models  causality  as  a  dynamic  sequence  of  active  mechanisms  that  chain  together  to  propagate  causal  influence  through  time.  We  motivate  this  approach  by  using  examples  from  AI  and  robotics  and  show  why  existing  approaches  are  inadequate.  We  present  an  algorithm  for  causal  reasoning  based  on  SoMs,  which  takes  as  input  a  knowledge  base  of  first-order  mechanisms  and  a  set  of  observations,  and  it  hypothesizes  which  mechanisms  are  active  at  what  time.  We  show  empirically  that  our  algorithm  produces  plausible  causal  explanations  of  simulated  observations  generated  from  a  causal  model.    We  argue  that  the  SoMs  approach  is  qualitatively  closer  to  the  human  causal  reasoning  process,  for  example,  it  will  only  include  relevant  variables  in  explanations.  We  present  new  insights  about  causal  reasoning  that  become  apparent  with  this  view.  One  such  insight  is  that  observation  and  manipulation  do  not  commute  in  causal  models,  a  fact  which  we  show  to  be  a  generalization  of  the  Equilibration-Manipulation  Commutability  of  [Dash(2005)].
0	Solving  most  systems  of  random  quadratic  equations.  This  paper  deals  with  finding  an  $n$-dimensional  solution  $\bm{x}$  to  a  system  of  quadratic  equations  $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$,  $1\le  i  \le  m$,  which  in  general  is  known  to  be  NP-hard.  We  put  forth  a  novel  procedure,  that  starts  with  a  \emph{weighted  maximal  correlation  initialization}  obtainable  with  a  few  power  iterations,  followed  by  successive  refinements  based  on  \emph{iteratively  reweighted  gradient-type  iterations}.  The  novel  techniques  distinguish  themselves  from  prior  works  by  the  inclusion  of  a  fresh  (re)weighting  regularization.  For  certain  random  measurement  models,  the  proposed  procedure  returns  the  true  solution  $\bm{x}$  with  high  probability  in  time  proportional  to  reading  the  data  $\{(\bm{a}_i;y_i)\}_{1\le  i  \le  m}$,  provided  that  the  number  $m$  of  equations  is  some  constant  $c>0$  times  the  number  $n$  of  unknowns,  that  is,  $m\ge  cn$.  Empirically,  the  upshots  of  this  contribution  are:  i)  perfect  signal  recovery  in  the  high-dimensional  regime  given  only  an  \emph{information-theoretic  limit  number}  of  equations;  and,  ii)  (near-)optimal  statistical  accuracy  in  the  presence  of  additive  noise.  Extensive  numerical  tests  using  both  synthetic  data  and  real  images  corroborate  its  improved  signal  recovery  performance  and  computational  efficiency  relative  to  state-of-the-art  approaches.
0	Greedy  subspace  clustering.  We  consider  the  problem  of  subspace  clustering:  given  points  that  lie  on  or  near  the  union  of  many  low-dimensional  linear  subspaces,  recover  the  subspaces.  To  this  end,  one  first  identifies  sets  of  points  close  to  the  same  subspace  and  uses  the  sets  to  estimate  the  subspaces.  As  the  geometric  structure  of  the  clusters  (linear  subspaces)  forbids  proper  performance  of  general  distance  based  approaches  such  as  K-means,  many  model-specific  methods  have  been  proposed.  In  this  paper,  we  provide  new  simple  and  efficient  algorithms  for  this  problem.  Our  statistical  analysis  shows  that  the  algorithms  are  guaranteed  exact  (perfect)  clustering  performance  under  certain  conditions  on  the  number  of  points  and  the  affinity  between  subspaces.  These  conditions  are  weaker  than  those  considered  in  the  standard  statistical  literature.  Experimental  results  on  synthetic  data  generated  from  the  standard  unions  of  subspaces  model  demonstrate  our  theory.  We  also  show  that  our  algorithm  performs  competitively  against  state-of-the-art  algorithms  on  real-world  applications  such  as  motion  segmentation  and  face  clustering,  with  much  simpler  implementation  and  lower  computational  cost.
0	Multilabel  structured  output  learning  with  random  spanning  trees  of  max  margin  markov  networks.  We  show  that  the  usual  score  function  for  conditional  Markov  networks  can  be  written  as  the  expectation  over  the  scores  of  their  spanning  trees.  We  also  show  that  a  small  random  sample  of  these  output  trees  can  attain  a  significant  fraction  of  the  margin  obtained  by  the  complete  graph  and  we  provide  conditions  under  which  we  can  perform  tractable  inference.  The  experimental  results  confirm  that  practical  learning  is  scalable  to  realistic  datasets  using  this  approach.
0	Learning  to  prune  deep  neural  networks  via  layer  wise  optimal  brain  surgeon.  How  to  develop  slim  and  accurate  deep  neural  networks  has  become  crucial  for  real-  world  applications,  especially  for  those  employed  in  embedded  systems.  Though  previous  work  along  this  research  line  has  shown  some  promising  results,  most  existing  methods  either  fail  to  significantly  compress  a  well-trained  deep  network  or  require  a  heavy  retraining  process  for  the  pruned  deep  network  to  re-boost  its  prediction  performance.  In  this  paper,  we  propose  a  new  layer-wise  pruning  method  for  deep  neural  networks.  In  our  proposed  method,  parameters  of  each  individual  layer  are  pruned  independently  based  on  second  order  derivatives  of  a  layer-wise  error  function  with  respect  to  the  corresponding  parameters.  We  prove  that  the  final  prediction  performance  drop  after  pruning  is  bounded  by  a  linear  combination  of  the  reconstructed  errors  caused  at  each  layer.  By  controlling  layer-wise  errors  properly,  one  only  needs  to  perform  a  light  retraining  process  on  the  pruned  network  to  resume  its  original  prediction  performance.  We  conduct  extensive  experiments  on  benchmark  datasets  to  demonstrate  the  effectiveness  of  our  pruning  method  compared  with  several  state-of-the-art  baseline  methods.  Codes  of  our  work  are  released  at:  https://github.com/csyhhu/L-OBS.
0	A  bayes  sard  cubature  method.  This  paper  focusses  on  the  formulation  of  numerical  integration  as  an  inferential  task.  To  date,  research  effort  has  largely  focussed  on  the  development  of  Bayesian  cubature,  whose  distributional  output  provides  uncertainty  quantification  for  the  integral.  However,  the  point  estimators  associated  to  Bayesian  cubature  can  be  inaccurate  and  acutely  sensitive  to  the  prior  when  the  domain  is  high-dimensional.  To  address  these  drawbacks  we  introduce  Bayes-Sard  cubature,  a  probabilistic  framework  that  combines  the  flexibility  of  Bayesian  cubature  with  the  robustness  of  classical  cubatures  which  are  well-established.  This  is  achieved  by  considering  a  Gaussian  process  model  for  the  integrand  whose  mean  is  a  parametric  regression  model,  with  an  improper  prior  on  each  regression  coefficient.  The  features  in  the  regression  model  consist  of  test  functions  which  are  guaranteed  to  be  exactly  integrated,  with  remaining  degrees  of  freedom  afforded  to  the  non-parametric  part.  The  asymptotic  convergence  of  the  Bayes-Sard  cubature  method  is  established  and  the  theoretical  results  are  numerically  verified.  In  particular,  we  report  two  orders  of  magnitude  reduction  in  error  compared  to  Bayesian  cubature  in  the  context  of  a  high-dimensional  financial  integral.
0	Optimal  algorithms  for  stochastic  multi  armed  bandits  with  heavy  tailed  rewards.  In  this  paper,  we  consider  stochastic  multi-armed  bandits  (MABs)  with  heavy-tailed  rewards,  whose  $p$-th  moment  is  bounded  by  a  constant  $\nu_{p}$  for  $1<p\leq2$.  First,  we  propose  a  novel  robust  estimator  which  does  not  require  $\nu_{p}$  as  prior  information,  while  other  existing  robust  estimators  demand  prior  knowledge  about  $\nu_{p}$.  We  show  that  an  error  probability  of  the  proposed  estimator  decays  exponentially  fast.  Using  this  estimator,  we  propose  a  perturbation-based  exploration  strategy  and  develop  a  generalized  regret  analysis  scheme  that  provides  upper  and  lower  regret  bounds  by  revealing  the  relationship  between  the  regret  and  the  cumulative  density  function  of  the  perturbation.  From  the  proposed  analysis  scheme,  we  obtain  gap-dependent  and  gap-independent  upper  and  lower  regret  bounds  of  various  perturbations.  We  also  find  the  optimal  hyperparameters  for  each  perturbation,  which  can  achieve  the  minimax  optimal  regret  bound  with  respect  to  total  rounds.  In  simulation,  the  proposed  estimator  shows  favorable  performance  compared  to  existing  robust  estimators  for  various  $p$  values  and,  for  MAB  problems,  the  proposed  perturbation  strategy  outperforms  existing  exploration  methods.
0	Nested  mini  batch  k  means.  A  new  algorithm  is  proposed  which  accelerates  the  mini-batch  k-means  algorithm  of  Sculley  (2010)  by  using  the  distance  bounding  approach  of  Elkan  (2003).  We  argue  that,  when  incorporating  distance  bounds  into  a  mini-batch  algorithm,  already  used  data  should  preferentially  be  reused.  To  this  end  we  propose  using  nested  mini-batches,  whereby  data  in  a  mini-batch  at  iteration  t  is  automatically  reused  at  iteration  t+1.  Using  nested  mini-batches  presents  two  difficulties.  The  first  is  that  unbalanced  use  of  data  can  bias  estimates,  which  we  resolve  by  ensuring  that  each  data  sample  contributes  exactly  once  to  centroids.  The  second  is  in  choosing  mini-batch  sizes,  which  we  address  by  balancing  premature  fine-tuning  of  centroids  with  redundancy  induced  slow-down.  Experiments  show  that  the  resulting  nmbatch  algorithm  is  very  effective,  often  arriving  within  1\%  of  the  empirical  minimum  100  times  earlier  than  the  standard  mini-batch  algorithm.
0	A  market  framework  for  eliciting  private  data.  We  propose  a  mechanism  for  purchasing  information  from  a  sequence  of  participants.  The  participants  may  simply  hold  data  points  they  wish  to  sell,  or  may  have  more  sophisticated  information;  either  way,  they  are  incentivized  to  participate  as  long  as  they  believe  their  data  points  are  representative  or  their  information  will  improve  the  mechanism's  future  prediction  on  a  test  set.  The  mechanism,  which  draws  on  the  principles  of  prediction  markets,  has  a  bounded  budget  and  minimizes  generalization  error  for  Bregman  divergence  loss  functions.  We  then  show  how  to  modify  this  mechanism  to  preserve  the  privacy  of  participants'  information:  At  any  given  time,  the  current  prices  and  predictions  of  the  mechanism  reveal  almost  no  information  about  any  one  participant,  yet  in  total  over  all  participants,  information  is  accurately  aggregated.
0	Spike  and  slab  variational  bayes  for  high  dimensional  logistic  regression.  Variational  Bayes  (VB)  is  a  popular  scalable  alternative  to  Markov  chain  Monte  Carlo  for  Bayesian  inference.  We  study  a  mean-field  spike  and  slab  VB  approximation  of  widely  used  Bayesian  model  selection  priors  in  sparse  high-dimensional  logistic  regression.  We  provide  non-asymptotic  theoretical  guarantees  for  the  VB  posterior  in  both  $\ell_2$  and  prediction  loss  for  a  sparse  truth,  giving  optimal  (minimax)  convergence  rates.  Since  the  VB  algorithm  does  not  depend  on  the  unknown  truth  to  achieve  optimality,  our  results  shed  light  on  effective  prior  choices.  We  confirm  the  improved  performance  of  our  VB  algorithm  over  common  sparse  VB  approaches  in  a  numerical  study.
0	Triple  descent  and  the  two  kinds  of  overfitting  where  why  do  they  appear.  A  recent  line  of  research  has  highlighted  the  existence  of  a  "double  descent"  phenomenon  in  deep  learning,  whereby  increasing  the  number  of  training  examples  $N$  causes  the  generalization  error  of  neural  networks  to  peak  when  $N$  is  of  the  same  order  as  the  number  of  parameters  $P$.  In  earlier  works,  a  similar  phenomenon  was  shown  to  exist  in  simpler  models  such  as  linear  regression,  where  the  peak  instead  occurs  when  $N$  is  equal  to  the  input  dimension  $D$.  Since  both  peaks  coincide  with  the  interpolation  threshold,  they  are  often  conflated  in  the  litterature.  In  this  paper,  we  show  that  despite  their  apparent  similarity,  these  two  scenarios  are  inherently  different.  In  fact,  both  peaks  can  co-exist  when  neural  networks  are  applied  to  noisy  regression  tasks.  The  relative  size  of  the  peaks  is  then  governed  by  the  degree  of  nonlinearity  of  the  activation  function.  Building  on  recent  developments  in  the  analysis  of  random  feature  models,  we  provide  a  theoretical  ground  for  this  sample-wise  triple  descent.  As  shown  previously,  the  nonlinear  peak  at  $N\!=\!P$  is  a  true  divergence  caused  by  the  extreme  sensitivity  of  the  output  function  to  both  the  noise  corrupting  the  labels  and  the  initialization  of  the  random  features  (or  the  weights  in  neural  networks).  This  peak  survives  in  the  absence  of  noise,  but  can  be  suppressed  by  regularization.  In  contrast,  the  linear  peak  at  $N\!=\!D$  is  solely  due  to  overfitting  the  noise  in  the  labels,  and  forms  earlier  during  training.  We  show  that  this  peak  is  implicitly  regularized  by  the  nonlinearity,  which  is  why  it  only  becomes  salient  at  high  noise  and  is  weakly  affected  by  explicit  regularization.  Throughout  the  paper,  we  compare  analytical  results  obtained  in  the  random  feature  model  with  the  outcomes  of  numerical  experiments  involving  deep  neural  networks.
0	Scalable  estimation  of  dirichlet  process  mixture  models  on  distributed  data.  We  consider  the  estimation  of  Dirichlet  Process  Mixture  Models  (DPMMs)  in  distributed  environments,  where  data  are  distributed  across  multiple  computing  nodes.  A  key  advantage  of  Bayesian  nonparametric  models  such  as  DPMMs  is  that  they  allow  new  components  to  be  introduced  on  the  fly  as  needed.  This,  however,  posts  an  important  challenge  to  distributed  estimation  --  how  to  handle  new  components  efficiently  and  consistently.  To  tackle  this  problem,  we  propose  a  new  estimation  method,  which  allows  new  components  to  be  created  locally  in  individual  computing  nodes.  Components  corresponding  to  the  same  cluster  will  be  identified  and  merged  via  a  probabilistic  consolidation  scheme.  In  this  way,  we  can  maintain  the  consistency  of  estimation  with  very  low  communication  cost.  Experiments  on  large  real-world  data  sets  show  that  the  proposed  method  can  achieve  high  scalability  in  distributed  and  asynchronous  environments  without  compromising  the  mixing  performance.
0	Neural  collective  entity  linking  based  on  recurrent  random  walk  network  learning.  Benefiting  from  the  excellent  ability  of  neural  networks  on  learning  semantic  representations,  existing  studies  for  entity  linking  (EL)  have  resorted  to  neural  networks  to  exploit  both  the  local  mention-to-entity  compatibility  and  the  global  interdependence  between  different  EL  decisions  for  target  entity  disambiguation.  However,  most  neural  collective  EL  methods  depend  entirely  upon  neural  networks  to  automatically  model  the  semantic  dependencies  between  different  EL  decisions,  which  lack  of  the  guidance  from  external  knowledge.  In  this  paper,  we  propose  a  novel  end-to-end  neural  network  with  recurrent  random-walk  layers  for  collective  EL,  which  introduces  external  knowledge  to  model  the  semantic  interdependence  between  different  EL  decisions.  Specifically,  we  first  establish  a  model  based  on  local  context  features,  and  then  stack  random-walk  layers  to  reinforce  the  evidence  for  related  EL  decisions  into  high-probability  decisions,  where  the  semantic  interdependence  between  candidate  entities  is  mainly  induced  from  an  external  knowledge  base.  Finally,  a  semantic  regularizer  that  preserves  the  collective  EL  decisions  consistency  is  incorporated  into  the  conventional  objective  function,  so  that  the  external  knowledge  base  can  be  fully  exploited  in  collective  EL  decisions.  Experimental  results  and  in-depth  analysis  on  various  datasets  show  that  our  model  achieves  better  performance  than  other  state-of-the-art  models.  Our  code  and  data  are  released  at  \url{this  https  URL}.
0	Interactive  learning  with  convolutional  neural  networks  for  image  labeling.  Recently,  deep  learning  models,  such  as  Convolutional  Neural  Networks,  have  shown  to  give  good  performance  for  various  computer  vision  tasks.  A  pre-requisite  for  such  models  is  to  have  access  to  lo  ...
0	Fast  and  robust  least  squares  estimation  in  corrupted  linear  models.  Subsampling  methods  have  been  recently  proposed  to  speed  up  least  squares  estimation  in  large  scale  settings.  However,  these  algorithms  are  typically  not  robust  to  outliers  or  corruptions  in  the  observed  covariates.    The  concept  of  influence  that  was  developed  for  regression  diagnostics  can  be  used  to  detect  such  corrupted  observations  as  shown  in  this  paper.  This  property  of  influence  -  for  which  we  also  develop  a  randomized  approximation  -  motivates  our  proposed  subsampling  algorithm  for  large  scale  corrupted  linear  regression  which  limits  the  influence  of  data  points  since  highly  influential  points  contribute  most  to  the  residual  error.  Under  a  general  model  of  corrupted  observations,  we  show  theoretically  and  empirically  on  a  variety  of  simulated  and  real  datasets  that  our  algorithm  improves  over  the  current  state-of-the-art  approximation  schemes  for  ordinary  least  squares.
0	Subspace  embeddings  for  the  polynomial  kernel.  Sketching  is  a  powerful  dimensionality  reduction  tool  for  accelerating  statistical  learning  algorithms.  However,  its  applicability  has  been  limited  to  a  certain  extent  since  the  crucial  ingredient,  the  so-called  oblivious  subspace  embedding,  can  only  be  applied  to  data  spaces  with  an  explicit  representation  as  the  column  span  or  row  span  of  a  matrix,  while  in  many  settings  learning  is  done  in  a  high-dimensional  space  implicitly  defined  by  the  data  matrix  via  a  kernel  transformation.  We  propose  the  first  fast  oblivious  subspace  embeddings  that  are  able  to  embed  a  space  induced  by  a  non-linear  kernel  without  explicitly  mapping  the  data  to  the  high-dimensional  space.  In  particular,  we  propose  an  embedding  for  mappings  induced  by  the  polynomial  kernel.  Using  the  subspace  embeddings,  we  obtain  the  fastest  known  algorithms  for  computing  an  implicit  low  rank  approximation  of  the  higher-dimension  mapping  of  the  data  matrix,  and  for  computing  an  approximate  kernel  PCA  of  the  data,  as  well  as  doing  approximate  kernel  principal  component  regression.
0	Relational  knowledge  extraction  from  neural  networks.  The  effective  integration  of  learning  and  reasoning  is  a  well-known  and  challenging  area  of  research  within  artificial  intelligence.  Neural-symbolic  systems  seek  to  integrate  learning  and  reasoning  by  combining  neural  networks  and  symbolic  knowledge  representation.  In  this  paper,  a  novel  methodology  is  proposed  for  the  extraction  of  relational  knowledge  from  neural  networks  which  are  trainable  by  the  efficient  application  of  the  backpropagation  learning  algorithm.  First-order  logic  rules  are  extracted  from  the  neural  networks,  offering  interpretable  symbolic  relational  models  on  which  logical  reasoning  can  be  performed.  The  wellknown  knowledge  extraction  algorithm  TREPAN  was  adapted  and  incorporated  into  the  first-order  version  of  the  neural-symbolic  system  CILP++.  Empirical  results  obtained  in  comparison  with  a  probabilistic  model  for  relational  learning,  Markov  Logic  Networks,  and  a  state-of-the-art  Inductive  Logic  Programming  system,  Aleph,  indicate  that  the  proposed  methodology  achieves  competitive  accuracy  results  consistently  in  all  datasets  investigated,  while  either  Markov  Logic  Networks  or  Aleph  show  considerably  worse  results  in  at  least  one  dataset.  It  is  expected  that  effective  knowledge  extraction  from  neural  networks  can  contribute  to  the  integration  of  heterogeneous  knowledge  representations.
0	Semantic  guided  multi  attention  localization  for  zero  shot  learning.  Zero-shot  learning  extends  the  conventional  object  classification  to  the  unseen  class  recognition  by  introducing  semantic  representations  of  classes.  Existing  approaches  predominantly  focus  on  learning  the  proper  mapping  function  for  visual-semantic  embedding,  while  neglecting  the  effect  of  learning  discriminative  visual  features.  In  this  paper,  we  study  the  significance  of  the  discriminative  region  localization.  We  propose  a  semantic-guided  multi-attention  localization  model,  which  automatically  discovers  the  most  discriminative  parts  of  objects  for  zero-shot  learning  without  any  human  annotations.  Our  model  jointly  learns  cooperative  global  and  local  features  from  the  whole  object  as  well  as  the  detected  parts  to  categorize  objects  based  on  semantic  descriptions.  Moreover,  with  the  joint  supervision  of  embedding  softmax  loss  and  class-center  triplet  loss,  the  model  is  encouraged  to  learn  features  with  high  inter-class  dispersion  and  intra-class  compactness.  Through  comprehensive  experiments  on  three  widely  used  zero-shot  learning  benchmarks,  we  show  the  efficacy  of  the  multi-attention  localization  and  our  proposed  approach  improves  the  state-of-the-art  results  by  a  considerable  margin.
0	Convergence  of  meta  learning  with  task  specific  adaptation  over  partial  parameters.  Although  model-agnostic  meta-learning  (MAML)  is  a  very  successful  algorithm  in  meta-learning  practice,  it  can  have  high  computational  cost  because  it  updates  all  model  parameters  over  both  the  inner  loop  of  task-specific  adaptation  and  the  outer-loop  of  meta  initialization  training.  A  more  efficient  algorithm  ANIL  (which  refers  to  almost  no  inner  loop)  was  proposed  recently  by  Raghu  et  al.  2019,  which  adapts  only  a  small  subset  of  parameters  in  the  inner  loop  and  thus  has  substantially  less  computational  cost  than  MAML  as  demonstrated  by  extensive  experiments.  However,  the  theoretical  convergence  of  ANIL  has  not  been  studied  yet.  In  this  paper,  we  characterize  the  convergence  rate  and  the  computational  complexity  for  ANIL  under  two  representative  inner-loop  loss  geometries,  i.e.,  strongly-convexity  and  nonconvexity.  Our  results  show  that  such  a  geometric  property  can  significantly  affect  the  overall  convergence  performance  of  ANIL.  For  example,  ANIL  achieves  a  faster  convergence  rate  for  a  strongly-convex  inner-loop  loss  as  the  number  $N$  of  inner-loop  gradient  descent  steps  increases,  but  a  slower  convergence  rate  for  a  nonconvex  inner-loop  loss  as  $N$  increases.  Moreover,  our  complexity  analysis  provides  a  theoretical  quantification  on  the  improved  efficiency  of  ANIL  over  MAML.  The  experiments  on  standard  few-shot  meta-learning  benchmarks  validate  our  theoretical  findings.
0	Organizational  culture  impact  on  knowledge  exchange  saudi  telecom  context.  Purpose  –  The  purpose  of  this  study  is  to  investigate  the  impact  of  some  organizational  culture  attributes,  including:  openness  to  change,  innovation,  trust,  teamwork,  morale,  information  flow,  employees'  involvement,  supervision,  customer  service  and  reward  orientation  on  the  knowledge  exchange  (KE)  process  within  the  context  of  the  Saudi  Telecom  Company  (STC)  as  a  representation  of  the  Saudi  context.Design/methodology/approach  –  A  descriptive  correlation  design  was  used.  A  web  survey  was  used  to  collect  data  from  378  employees  working  on  STC  using  Random  Number  Generator  0.2  software.  The  sample  was  selected  using  an  e‐mailing  list.Findings  –  The  findings  revealed  that  the  cultural  attributes  of  trust,  innovation,  information  flow,  supervision,  and  reward  have  an  impact  on  KE  within  the  context  of  the  STC.  Additionally,  the  study  revealed  a  statistically  significant  correlation  between  organizational  culture  and  KE  as  a  whole  (0.75),  which  emphasizes  the  effective  role  of  organizational  culture  factors  ...
0	Epistemic  boolean  games  based  on  a  logic  of  visibility  and  control.  We  analyse  epistemic  boolean  games  in  a  computationally  grounded  dynamic  epistemic  logic.  The  agents'  knowledge  is  determined  by  what  they  see,  including  higher-order  visibility:  agents  may  observe  whether  another  agent  observes  an  atom  or  not.  The  agents'  actions  consist  in  modifying  the  truth  values  of  atoms.  We  provide  an  axiomatisation  of  the  logic,  establish  that  the  model  checking  problem  is  in  PSPACE,  and  show  how  one  can  reason  about  equilibria  in  epistemic  boolean  games.
0	Buried  utility  pipeline  mapping  based  on  multiple  spatial  data  sources  a  bayesian  data  fusion  approach.  Statutory  records  of  underground  utility  apparatus  (such  as  pipes  and  cables)  are  notoriously  inaccurate,  so  street  surveys  are  usually  undertaken  before  road  excavation  takes  place  to  minimize  the  extent  and  duration  of  excavation  and  for  health  and  safety  reasons.  This  involves  the  use  of  sensors  such  as  Ground  Penetrating  Radar  (GPR).  The  GPR  scans  are  then  manually  interpreted  and  combined  with  the  expectations  from  the  utility  records  and  other  data  such  as  surveyed  manholes.  The  task  is  complex  owing  to  the  difficulty  in  interpreting  the  sensor  data,  and  the  spatial  complexity  and  extent  of  under  street  assets.  We  explore  the  application  of  AI  techniques,  in  particular  Bayesian  data  fusion  (BDF),  to  automatically  generate  maps  of  buried  apparatus.  Hypotheses  about  the  spatial  location  and  direction  of  buried  assets  are  extracted  by  identifying  hyperbolae  in  the  GPR  scans.  The  spatial  location  of  surveyed  manholes  provides  further  input  to  the  algorithm,  as  well  as  the  prior  expectations  from  the  statutory  records.  These  three  data  sources  are  used  to  produce  the  most  probable  map  of  the  buried  assets.  Experimental  results  on  real  and  simulated  data  sets  are  presented.
0	Constrained  preference  embedding  for  item  recommendation.  To  learn  users'  preference,  their  feedback  information  is  commonly  modeled  as  scalars  and  integrated  into  matrix  factorization  (MF)  based  algorithms.  Based  on  MF  techniques,  the  preference  degree  is  computed  by  the  product  of  user  and  item  vectors,  which  is  also  represented  by  scalars.  On  the  contrary,  in  this  paper,  we  express  users'  feedback  as  constrained  vectors,  and  call  the  idea  constrained  preference  embedding  (CPE);  it  means  that  we  regard  users,  items  and  all  users'  behavior  as  vectors.  We  find  that  this  viewpoint  is  more  flexible  and  powerful  than  traditional  MF  for  item  recommendation.  For  example,  by  the  proposed  assumption,  users'  heterogeneous  actions  can  be  coherently  mined  because  all  entities  and  actions  can  be  transferred  to  a  space  of  the  same  dimension.  In  addition,  CPE  is  able  to  model  the  feedback  of  uncertain  preference  degree.  To  test  our  assumption,  we  propose  two  models  called  CPE-s  and  CPE-ps  based  on  CPE  for  item  recommendation,  and  show  that  the  popular  pair-wise  ranking  model  BPR-MF  can  be  deduced  by  some  restrictions  and  variations  on  CPE-s.  In  the  experiments,  we  will  test  CPE  and  the  proposed  algorithms,  and  prove  their  effectiveness.
0	Attributed  graph  clustering  a  deep  attentional  embedding  approach.  Graph  clustering  is  a  fundamental  task  which  discovers  communities  or  groups  in  networks.  Recent  studies  have  mostly  focused  on  developing  deep  learning  approaches  to  learn  a  compact  graph  embedding,  upon  which  classic  clustering  methods  like  k-means  or  spectral  clustering  algorithms  are  applied.  These  two-step  frameworks  are  difficult  to  manipulate  and  usually  lead  to  suboptimal  performance,  mainly  because  the  graph  embedding  is  not  goal-directed,  i.e.,  designed  for  the  specific  clustering  task.  In  this  paper,  we  propose  a  goal-directed  deep  learning  approach,  Deep  Attentional  Embedded  Graph  Clustering  (DAEGC  for  short).  Our  method  focuses  on  attributed  graphs  to  sufficiently  explore  the  two  sides  of  information  in  graphs.  By  employing  an  attention  network  to  capture  the  importance  of  the  neighboring  nodes  to  a  target  node,  our  DAEGC  algorithm  encodes  the  topological  structure  and  node  content  in  a  graph  to  a  compact  representation,  on  which  an  inner  product  decoder  is  trained  to  reconstruct  the  graph  structure.  Furthermore,  soft  labels  from  the  graph  embedding  itself  are  generated  to  supervise  a  self-training  graph  clustering  process,  which  iteratively  refines  the  clustering  results.  The  self-training  process  is  jointly  learned  and  optimized  with  the  graph  embedding  in  a  unified  framework,  to  mutually  benefit  both  components.  Experimental  results  compared  with  state-of-the-art  algorithms  demonstrate  the  superiority  of  our  method.
0	An  architecture  for  component  based  design  of  representative  based  clustering  algorithms.  We  propose  an  architecture  for  the  design  of  representative-based  clustering  algorithms  based  on  reusable  components.  These  components  were  derived  from  K-means-like  algorithms  and  their  extensions.  With  the  suggested  clustering  design  architecture,  it  is  possible  to  reconstruct  popular  algorithms,  but  also  to  build  new  algorithms  by  exchanging  components  from  original  algorithms  and  their  improvements.  In  this  way,  the  design  of  a  myriad  of  representative-based  clustering  algorithms  and  their  fair  comparison  and  evaluation  are  possible.  In  addition  to  the  architecture,  we  show  the  usefulness  of  the  proposed  approach  by  providing  experimental  evaluation.
0	Disentangling  value  enhancing  and  cost  increasing  effects  of  knowledge  management.  Purpose  –  The  main  purpose  of  this  paper  is  to  provide  large‐scale  empirical  evidence  on  the  value‐enhancing  and  cost‐increasing  effects  of  knowledge  management  (KM)  techniques.Design/methodology/approach  –  The  authors  conduct  structural  equation  analyses,  using  data  from  the  Community  Innovation  Survey  2007  and  from  annual  accounts  of  705  innovative  Belgian  firms.Findings  –  Results  confirm  that  the  use  of  KM  techniques  has  an  indirect  positive  impact  on  financial  performance  via  increased  innovation  performance.  In  addition,  a  direct  cost‐increasing  effect  of  KM  practices  on  financial  performance  is  observed.  In  the  short  term,  this  direct  cost‐increasing  effect  exceeds  the  indirect  value‐generating  effect  of  KM  techniques.Research  limitations/implications  –  This  study  investigates  the  short‐term  effects  of  KM  techniques.  Future  research  should  study  the  long‐term  costs  and  benefits.  Data  were  collected  in  Belgium  and  may  not  reflect  the  impact  of  KM  practices  in  other  geographic,  economic  or  cultural  se...
0	Equilibrated  adaptive  learning  rates  for  non  convex  optimization.  Parameter-specific  adaptive  learning  rate  methods  are  computationally  efficient  ways  to  reduce  the  ill-conditioning  problems  encountered  when  training  large  deep  networks.  Following  recent  work  that  strongly  suggests  that  most  of  the  critical  points  encountered  when  training  such  networks  are  saddle  points,  we  find  how  considering  the  presence  of  negative  eigenvalues  of  the  Hessian  could  help  us  design  better  suited  adaptive  learning  rate  schemes.  We  show  that  the  popular  Jacobi  preconditioner  has  undesirable  behavior  in  the  presence  of  both  positive  and  negative  curvature,  and  present  theoretical  and  empirical  evidence  that  the  so-called  equilibration  preconditioner  is  comparatively  better  suited  to  non-convex  problems.  We  introduce  a  novel  adaptive  learning  rate  scheme,  called  ESGD,  based  on  the  equilibration  preconditioner.  Our  experiments  show  that  ESGD  performs  as  well  or  better  than  RMSProp  in  terms  of  convergence  speed,  always  clearly  improving  over  plain  stochastic  gradient  descent.
0	Nonlinear  random  matrix  theory  for  deep  learning.  Neural  network  configurations  with  random  weights  play  an  important  role  in  the  analysis  of  deep  learning.  They  define  the  initial  loss  landscape  and  are  closely  related  to  kernel  and  random  feature  methods.  Despite  the  fact  that  these  networks  are  built  out  of  random  matrices,  the  vast  and  powerful  machinery  of  random  matrix  theory  has  so  far  found  limited  success  in  studying  them.  A  main  obstacle  in  this  direction  is  that  neural  networks  are  nonlinear,  which  prevents  the  straightforward  utilization  of  many  of  the  existing  mathematical  results.  In  this  work,  we  open  the  door  for  direct  applications  of  random  matrix  theory  to  deep  learning  by  demonstrating  that  the  pointwise  nonlinearities  typically  applied  in  neural  networks  can  be  incorporated  into  a  standard  method  of  proof  in  random  matrix  theory  known  as  the  moments  method.  The  test  case  for  our  study  is  the  Gram  matrix  $Y^TY$,  $Y=f(WX)$,  where  $W$  is  a  random  weight  matrix,  $X$  is  a  random  data  matrix,  and  $f$  is  a  pointwise  nonlinear  activation  function.  We  derive  an  explicit  representation  for  the  trace  of  the  resolvent  of  this  matrix,  which  defines  its  limiting  spectral  distribution.  We  apply  these  results  to  the  computation  of  the  asymptotic  performance  of  single-layer  random  feature  methods  on  a  memorization  task  and  to  the  analysis  of  the  eigenvalues  of  the  data  covariance  matrix  as  it  propagates  through  a  neural  network.  As  a  byproduct  of  our  analysis,  we  identify  an  intriguing  new  class  of  activation  functions  with  favorable  properties.
0	Memory  efficient  adaptive  optimization.  Adaptive  gradient-based  optimizers  such  as  Adagrad  and  Adam  are  crucial  for  achieving  state-of-the-art  performance  in  machine  translation  and  language  modeling.  However,  these  methods  maintain  second-order  statistics  for  each  parameter,  thus  introducing  significant  memory  overheads  that  restrict  the  size  of  the  model  being  used  as  well  as  the  number  of  examples  in  a  mini-batch.  We  describe  an  effective  and  flexible  adaptive  optimization  method  with  greatly  reduced  memory  overhead.  Our  method  retains  the  benefits  of  per-parameter  adaptivity  while  allowing  significantly  larger  models  and  batch  sizes.  We  give  convergence  guarantees  for  our  method,  and  demonstrate  its  effectiveness  in  training  very  large  translation  and  language  models  with  up  to  2-fold  speedups  compared  to  the  state-of-the-art.
0	Middle  out  decoding.  Despite  being  virtually  ubiquitous,  sequence-to-sequence  models  are  challenged  by  their  lack  of  diversity  and  inability  to  be  externally  controlled.  In  this  paper,  we  speculate  that  a  fundamental  shortcoming  of  sequence  generation  models  is  that  the  decoding  is  done  strictly  from  left-to-right,  meaning  that  outputs  values  generated  earlier  have  a  profound  effect  on  those  generated  later.  To  address  this  issue,  we  propose  a  novel  middle-out  decoder  architecture  that  begins  from  an  initial  middle-word  and  simultaneously  expands  the  sequence  in  both  directions.  To  facilitate  information  flow  and  maintain  consistent  decoding,  we  introduce  a  dual  self-attention  mechanism  that  allows  us  to  model  complex  dependencies  between  the  outputs.  We  illustrate  the  performance  of  our  model  on  the  task  of  video  captioning,  as  well  as  a  synthetic  sequence  de-noising  task.  Our  middle-out  decoder  achieves  significant  improvements  on  de-noising  and  competitive  performance  in  the  task  of  video  captioning,  while  quantifiably  improving  the  caption  diversity.  Furthermore,  we  perform  a  qualitative  analysis  that  demonstrates  our  ability  to  effectively  control  the  generation  process  of  our  decoder.
0	E  snli  natural  language  inference  with  natural  language  explanations.  In  order  for  machine  learning  to  garner  widespread  public  adoption,  models  must  be  able  to  provide  interpretable  and  robust  explanations  for  their  decisions,  as  well  as  learn  from  human-provided  explanations  at  train  time.  In  this  work,  we  extend  the  Stanford  Natural  Language  Inference  dataset  with  an  additional  layer  of  human-annotated  natural  language  explanations  of  the  entailment  relations.  We  further  implement  models  that  incorporate  these  explanations  into  their  training  process  and  output  them  at  test  time.  We  show  how  our  corpus  of  explanations,  which  we  call  e-SNLI,  can  be  used  for  various  goals,  such  as  obtaining  full  sentence  justifications  of  a  model’s  decisions,  improving  universal  sentence  representations  and  transferring  to  out-of-domain  NLI  datasets.  Our  dataset  thus  opens  up  a  range  of  research  directions  for  using  natural  language  explanations,  both  for  improving  models  and  for  asserting  their  trust
0	Faster  wasserstein  distance  estimation  with  the  sinkhorn  divergence.  The  squared  Wasserstein  distance  is  a  natural  quantity  to  compare  probability  distributions  in  a  non-parametric  setting.  This  quantity  is  usually  estimated  with  the  plug-in  estimator,  defined  via  a  discrete  optimal  transport  problem  which  can  be  solved  to  $\epsilon$-accuracy  by  adding  an  entropic  regularization  of  order  $\epsilon$  and  using  for  instance  Sinkhorn's  algorithm.  In  this  work,  we  propose  instead  to  estimate  it  with  the  Sinkhorn  divergence,  which  is  also  built  on  entropic  regularization  but  includes  debiasing  terms.  We  show  that,  for  smooth  densities,  this  estimator  has  a  comparable  sample  complexity  but  allows  higher  regularization  levels,  of  order  $\epsilon^{1/2}$,  which  leads  to  improved  computational  complexity  bounds  and  a  strong  speedup  in  practice.  Our  theoretical  analysis  covers  the  case  of  both  randomly  sampled  densities  and  deterministic  discretizations  on  uniform  grids.  We  also  propose  and  analyze  an  estimator  based  on  Richardson  extrapolation  of  the  Sinkhorn  divergence  which  enjoys  improved  statistical  and  computational  efficiency  guarantees,  under  a  condition  on  the  regularity  of  the  approximation  error,  which  is  in  particular  satisfied  for  Gaussian  densities.  We  finally  demonstrate  the  efficiency  of  the  proposed  estimators  with  numerical  experiments.
0	Morel  model  based  offline  reinforcement  learning.  In  offline  reinforcement  learning  (RL),  the  goal  is  to  learn  a  highly  rewarding  policy  based  solely  on  a  dataset  of  historical  interactions  with  the  environment.  The  ability  to  train  RL  policies  offline  would  greatly  expand  where  RL  can  be  applied,  its  data  efficiency,  and  its  experimental  velocity.  Prior  work  in  offline  RL  has  been  confined  almost  exclusively  to  model-free  RL  approaches.  In  this  work,  we  present  MOReL,  an  algorithmic  framework  for  model-based  offline  RL.  This  framework  consists  of  two  steps:  (a)  learning  a  pessimistic  MDP  (P-MDP)  using  the  offline  dataset;  (b)  learning  a  near-optimal  policy  in  this  P-MDP.  The  learned  P-MDP  has  the  property  that  for  any  policy,  the  performance  in  the  real  environment  is  approximately  lower-bounded  by  the  performance  in  the  P-MDP.  This  enables  it  to  serve  as  a  good  surrogate  for  purposes  of  policy  evaluation  and  learning,  and  overcome  common  pitfalls  of  model-based  RL  like  model  exploitation.  Theoretically,  we  show  that  MOReL  is  minimax  optimal  (up  to  log  factors)  for  offline  RL.  Through  experiments,  we  show  that  MOReL  matches  or  exceeds  state-of-the-art  results  in  widely  studied  offline  RL  benchmarks.  Moreover,  the  modular  design  of  MOReL  enables  future  advances  in  its  components  (e.g.,  in  model  learning,  planning  etc.)  to  directly  translate  into  improvements  for  offline  RL.
0	Measuring  robustness  to  natural  distribution  shifts  in  image  classification.  We  study  how  robust  current  ImageNet  models  are  to  distribution  shifts  arising  from  natural  variations  in  datasets.  Most  research  on  robustness  focuses  on  synthetic  image  perturbations  (noise,  simulated  weather  artifacts,  adversarial  examples,  etc.),  which  leaves  open  how  robustness  on  synthetic  distribution  shift  relates  to  distribution  shift  arising  in  real  data.  Informed  by  an  evaluation  of  204  ImageNet  models  in  213  different  test  conditions,  we  find  that  there  is  often  little  to  no  transfer  of  robustness  from  current  synthetic  to  natural  distribution  shift.  Moreover,  most  current  techniques  provide  no  robustness  to  the  natural  distribution  shifts  in  our  testbed.  The  main  exception  is  training  on  larger  and  more  diverse  datasets,  which  in  multiple  cases  increases  robustness,  but  is  still  far  from  closing  the  performance  gaps.  Our  results  indicate  that  distribution  shifts  arising  in  real  data  are  currently  an  open  research  problem.  We  provide  our  testbed  and  data  as  a  resource  for  future  work  at  this  https  URL  .
0	Mental  health  knowledge  management  critical  success  factors  and  strategy  of  implementation.  Purpose          This  paper  aims  to  recommend  implementation  of  the  knowledge  management  (KM)  strategy  for  a  mental  health  organisation,  an  area  that  has,  to  date,  limited  attention  in  literature  based  on  the  factors  that  influence  KM  success.          Design/methodology/approach          A  mixed-methods  research  was  conducted  to  identify  the  organisational  culture,  resources,  enablers  and  the  influential  factors  of  mental  health  knowledge  management  (MHKM).  The  data  were  collected  in  five  referral  mental  hospitals  and  were  analysed  using  quantitative,  qualitative  and  triangulation  methods.          Findings          The  organisational  culture  has  become  a  great  barrier.  Forty-three  influential  factors  were  identified.  Otherwise,  based  on  culture,  resources,  enablers  and  strengthen,  weakness,  opportunities  and  threaten  (SWOT)  analysed  were  adopted  to  propose  ten  of  the  critical  success  factors  and  were  recommended  into  an  implementation  strategy.          Research  limitations/implications          The  paper  has  proven  that  KM  is  a  new  and  emerging  discipline  in  Indonesia,  especially  on  mental  health  care.  This  will  contribute  to  the  governmental  policy  of  KM  implementation  and  enforce  the  quality  of  services.          Practical  implications          This  result  has  the  potential  to  leverage  interdisciplinary  KM  research.  It  supports  a  mental  health  organisation  in  applying  KM.          Originality/value          This  study  is  probably  the  first  to  analyse  factors  that  are  of  influence  in  an  MHKM  initiative  programme.
0	Online  motion  selection  for  semi  optimal  stabilization  using  reverse  time  tree.  This  paper  presents  a  general  method  for  creating  an  approximately  optimal  online  stabilization  system.  An  optimal  stabilization  system  is  an  ideal  online  system  that  can  calculate  each  optimal  motion  leading  to  a  stable  mechanical  goal  state  depending  on  the  current  state.  We  propose  a  system  that  selects  each  semi-optimal  motion  according  to  the  current  state  from  a  reverse-time  tree.  To  create  the  reverse-time  tree,  we  applied  rapid  semi-optimal  motion  planning  method  (RASMO)  to  a  reverse-time  search  from  a  stable  state.  We  also  developed  an  online  motion  selection  technique.  To  validate  the  proposed  method,  we  simulated  the  stabilization  of  a  double  inverted  pendulum.  When  we  used  an  optimization  criteria,  time  optimal,  the  system  quickly  stabilized  the  pendulum's  posture  and  velocity.  When  we  used  higher  resolution  RASMO,  the  time  approached  the  optimal  time.  The  general  framework  proposed  here  is  applicable  to  a  variety  of  machines.
0	Sensor  and  actuator  integrated  low  profile  robotic  origami.  The  robotic  origami  (Robogami)  is  a  low-profile,  sheet-like  robot  with  multi  degrees-of-freedom  (DoF)  that  embeds  different  functional  layers.  Due  to  its  planar  form,  it  can  take  advantage  of  precise  2D  fabrication  methods  usually  reserved  for  micro  and  nano  systems.  Not  only  can  these  methods  reduce  fabrication  time  and  expenses,  by  offering  a  high  precision,  they  enable  us  to  integrate  actuators,  sensors  and  electronic  components  into  a  thin  sheet.  In  this  research,  we  study  sensors,  actuators  and  fabrication  methods  for  Robogami  which  can  reconfigure  into  various  forms.  Our  main  objective  is  to  develop  technologies  that  can  be  easily  applied  to  Robogamis  consisting  of  many  active  folds  and  DoFs.  In  this  paper,  after  studying  the  performance  of  the  proposed  sensors  and  actuators  in  one  fold,  we  use  a  design  for  a  crawler  robot  consisting  of  four  folds  to  assess  the  performance  of  these  technologies.
0	Structured  skip  list  a  compact  data  structure  for  3d  reconstruction.  The  model  produced  by  3D  reconstruction  algorithm  is  usually  represented  by  voxels.  The  management  of  these  voxels  is  usually  divided  into  two  categories:  ordered  and  unordered  methods.  The  ordered  method  holds  too  many  empty  voxels  to  maintain  data  order  which  leads  to  a  low  storage  efficiency.  On  the  contrary,  the  unordered  method  keeps  massive  index  data  to  only  store  nonempty  voxels.  In  this  paper,  we  design  a  new  data  management  method  for  real-time  indoor  3D  reconstruction,  called  Structured  Skip  List  (SSL).  The  SSL  can  be  treated  as  a  semi-ordered  method,  because  the  advantages  of  both  the  ordered  and  unordered  methods  are  taken  into  account:  1)  it  only  holds  nonempty  voxels  similar  to  the  unordered  method;  2)  the  structured  information  is  introduced  to  reduce  the  storage  space  of  index  data.  By  these  designs,  the  SSL  has  a  better  performance  on  storage  efficiency.  To  handle  the  data  collision  in  voxel  allocation,  a  hash  allocation  list  (HAL)  is  proposed.  The  length  of  each  Skip  List  is  kept  balanced  by  fusing  the  IMU  (Inertial  Measurement  Unit)  information  for  a  high  operation  efficiency.  The  storage  efficiency  analysis  of  different  data  management  methods  is  shown  in  this  paper.  What's  more,  exhaustive  investigation  is  carried  out  on  several  datasets  with  these  methods.  The  experimental  result  demonstrates  that  our  design  can  achieve  a  high  storage  efficiency  with  little  time  loss  compared  to  the  state-of-the-art  methods.
0	Fem  based  deformation  control  for  dexterous  manipulation  of  3d  soft  objects.  In  this  paper,  a  method  for  dexterous  manipulation  of  3D  soft  objects  for  real-time  deformation  control  is  presented,  relying  on  Finite  Element  modelling.  The  goal  is  to  generate  proper  forces  on  the  fingertips  of  an  anthropomorphic  device  during  in-hand  manipulation  to  produce  desired  displacements  of  selected  control  points  on  the  object.  The  desired  motions  of  the  fingers  are  computed  in  real-time  as  an  inverse  solution  of  a  Finite  Element  Method  (FEM),  the  forces  applied  by  the  fingertips  at  the  contact  points  being  modelled  by  Lagrange  multipliers.  The  elasticity  parameters  of  the  model  are  preliminarly  estimated  using  a  vision  system  and  a  force  sensor.  Experimental  results  are  shown  with  an  underactuated  anthropomorphic  hand  that  performs  a  manipulation  task  on  a  soft  cylindrical  object.
0	A  multiplicative  model  for  learning  distributed  text  based  attribute  representations.  In  this  paper  we  propose  a  general  framework  for  learning  distributed  representations  of  attributes:  characteristics  of  text  whose  representations  can  be  jointly  learned  with  word  embeddings.  Attributes  can  correspond  to  a  wide  variety  of  concepts,  such  as  document  indicators  (to  learn  sentence  vectors),  language  indicators  (to  learn  distributed  language  representations),  meta-data  and  side  information  (such  as  the  age,  gender  and  industry  of  a  blogger)  or  representations  of  authors.  We  describe  a  third-order  model  where  word  context  and  attribute  vectors  interact  multiplicatively  to  predict  the  next  word  in  a  sequence.  This  leads  to  the  notion  of  conditional  word  similarity:  how  meanings  of  words  change  when  conditioned  on  different  attributes.  We  perform  several  experimental  tasks  including  sentiment  classification,  cross-lingual  document  classification,  and  blog  authorship  attribution.  We  also  qualitatively  evaluate  conditional  word  neighbours  and  attribute-conditioned  text  generation.
0	Compressive  neural  representation  of  sparse  high  dimensional  probabilities.  This  paper  shows  how  sparse,  high-dimensional  probability  distributions  could  be  represented  by  neurons  with  exponential  compression.  The  representation  is  a  novel  application  of  compressive  sensing  to  sparse  probability  distributions  rather  than  to  the  usual  sparse  signals.  The  compressive  measurements  correspond  to  expected  values  of  nonlinear  functions  of  the  probabilistically  distributed  variables.  When  these  expected  values  are  estimated  by  sampling,  the  quality  of  the  compressed  representation  is  limited  only  by  the  quality  of  sampling.  Since  the  compression  preserves  the  geometric  structure  of  the  space  of  sparse  probability  distributions,  probabilistic  computation  can  be  performed  in  the  compressed  domain.  Interestingly,  functions  satisfying  the  requirements  of  compressive  sensing  can  be  implemented  as  simple  perceptrons.  If  we  use  perceptrons  as  a  simple  model  of  feedforward  computation  by  neurons,  these  results  show  that  the  mean  activity  of  a  relatively  small  number  of  neurons  can  accurately  represent  a  high-dimensional  joint  distribution  implicitly,  even  without  accounting  for  any  noise  correlations.  This  comprises  a  novel  hypothesis  for  how  neurons  could  encode  probabilities  in  the  brain.
0	Stochastic  network  design  in  bidirected  trees.  We  investigate  the  problem  of  stochastic  network  design  in  bidirected  trees.  In  this  problem,  an  underlying  phenomenon  (e.g.,  a  behavior,  rumor,  or  disease)  starts  at  multiple  sources  in  a  tree  and  spreads  in  both  directions  along  its  edges.  Actions  can  be  taken  to  increase  the  probability  of  propagation  on  edges,  and  the  goal  is  to  maximize  the  total  amount  of  spread  away  from  all  sources.  Our  main  result  is  a  rounded  dynamic  programming  approach  that  leads  to  a  fully  polynomial-time  approximation  scheme  (FPTAS),  that  is,  an  algorithm  that  can  find  (1  —  e)-optimal  solutions  for  any  problem  instance  in  time  polynomial  in  the  input  size  and  1/e.  Our  algorithm  outperforms  competing  approaches  on  a  motivating  problem  from  computational  sustainability  to  remove  barriers  in  river  networks  to  restore  the  health  of  aquatic  ecosystems.
0	Gradient  dynamics  of  shallow  univariate  relu  networks.  We  present  a  theoretical  and  empirical  study  of  the  gradient  dynamics  of  overparameterized  shallow  ReLU  networks  with  one-dimensional  input,  solving  least-squares  interpolation.  We  show  that  the  gradient  dynamics  of  such  networks  are  determined  by  the  gradient  flow  in  a  non-redundant  parameterization  of  the  network  function.  We  examine  the  principal  qualitative  features  of  this  gradient  flow.  In  particular,  we  determine  conditions  for  two  learning  regimes:  \emph{kernel}  and  \emph{adaptive},  which  depend  both  on  the  relative  magnitude  of  initialization  of  weights  in  different  layers  and  the  asymptotic  behavior  of  initialization  coefficients  in  the  limit  of  large  network  widths.  We  show  that  learning  in  the  kernel  regime  yields  smooth  interpolants,  minimizing  curvature,  and  reduces  to  \emph{cubic  splines}  for  uniform  initializations.  Learning  in  the  adaptive  regime  favors  instead  \emph{linear  splines},  where  knots  cluster  adaptively  at  the  sample  points.
0	An  embedding  framework  for  consistent  polyhedral  surrogates.  We  formalize  and  study  the  natural  approach  of  designing  convex  surrogate  loss  functions  via  embeddings  for  problems  such  as  classification  or  ranking.  In  this  approach,  one  embeds  each  of  the  finitely  many  predictions  (e.g.  classes)  as  a  point  in  \reals^d,  assigns  the  original  loss  values  to  these  points,  and  convexifies  the  loss  in  some  way  to  obtain  a  surrogate.  We  prove  that  this  approach  is  equivalent,  in  a  strong  sense,  to  working  with  polyhedral  (piecewise  linear  convex)  losses.  Moreover,  given  any  polyhedral  loss  L,  we  give  a  construction  of  a  link  function  through  which  L  is  a  consistent  surrogate  for  the  loss  it  embeds.  We  go  on  to  illustrate  the  power  of  this  embedding  framework  with  succinct  proofs  of  consistency  or  inconsistency  of  various  polyhedral  surrogates  in  the  literature.
0	A  neurally  plausible  model  for  online  recognition  and  postdiction  in  a  dynamical  environment.  Humans  and  other  animals  are  frequently  near-optimal  in  their  ability  to  integrate  noisy  and  ambiguous  sensory  data  to  form  robust  percepts---which  are  informed  both  by  sensory  evidence  and  by  prior  expectations  about  the  structure  of  the  environment.  It  is  suggested  that  the  brain  does  so  using  the  statistical  structure  provided  by  an  internal  model  of  how  latent,  causal  factors  produce  the  observed  patterns.  In  dynamic  environments,  such  integration  often  takes  the  form  of  \emph{postdiction},  wherein  later  sensory  evidence  affects  inferences  about  earlier  percepts.  As  the  brain  must  operate  in  current  time,  without  the  luxury  of  acausal  propagation  of  information,  how  does  such  postdictive  inference  come  about?  Here,  we  propose  a  general  framework  for  neural  probabilistic  inference  in  dynamic  models  based  on  the  distributed  distributional  code  (DDC)  representation  of  uncertainty,  naturally  extending  the  underlying  encoding  to  incorporate  implicit  probabilistic  beliefs  about  both  present  and  past.  We  show  that,  as  in  other  uses  of  the  DDC,  an  inferential  model  can  be  learnt  efficiently  using  samples  from  an  internal  model  of  the  world.  Applied  to  stimuli  used  in  the  context  of  psychophysics  experiments,  the  framework  provides  an  online  and  plausible  mechanism  for  inference,  including  postdictive  effects.
0	Xnas  neural  architecture  search  with  expert  advice.  This  paper  introduces  a  novel  optimization  method  for  differential  neural  architecture  search,  based  on  the  theory  of  prediction  with  expert  advice.  Its  optimization  criterion  is  well  fitted  for  an  architecture-selection,  i.e.,  it  minimizes  the  regret  incurred  by  a  sub-optimal  selection  of  operations.  Unlike  previous  search  relaxations,  that  require  hard  pruning  of  architectures,  our  method  is  designed  to  dynamically  wipe  out  inferior  architectures  and  enhance  superior  ones.  It  achieves  an  optimal  worst-case  regret  bound  and  suggests  the  use  of  multiple  learning-rates,  based  on  the  amount  of  information  carried  by  the  backward  gradients.  Experiments  show  that  our  algorithm  achieves  a  strong  performance  over  several  image  classification  datasets.  Specifically,  it  obtains  an  error  rate  of  1.6%  for  CIFAR-10,  23.9%  for  ImageNet  under  mobile  settings,  and  achieves  state-of-the-art  results  on  three  additional  datasets.
0	Learning  disentangled  joint  continuous  and  discrete  representations.  We  present  a  framework  for  learning  disentangled  and  interpretable  jointly  continuous  and  discrete  representations  in  an  unsupervised  manner.  By  augmenting  the  continuous  latent  distribution  of  variational  autoencoders  with  a  relaxed  discrete  distribution  and  controlling  the  amount  of  information  encoded  in  each  latent  unit,  we  show  how  continuous  and  categorical  factors  of  variation  can  be  discovered  automatically  from  data.  Experiments  show  that  the  framework  disentangles  continuous  and  discrete  generative  factors  on  various  datasets  and  outperforms  current  disentangling  methods  when  a  discrete  generative  factor  is  prominent.
0	Empirical  characterization  of  a  high  performance  exterior  rotor  type  brushless  dc  motor  and  drive.  Recently,  brushless  motors  with  especially  high  torque  densities  have  been  developed  for  applications  in  autonomous  aerial  vehicles  (i.e.  drones),  which  usually  employ  exterior  rotortype  geometries  (ER-BLDC  motors).  These  motors  are  promising  for  other  applications,  such  as  humanoids  and  wearable  robots;  however,  the  emerging  companies  that  produce  motors  for  drone  applications  do  not  typically  provide  adequate  technical  specifications  that  would  permit  their  general  use  across  robotics–for  example,  the  specifications  are  often  tested  in  unrealistic  forced  convection  environments,  or  are  drone-specific,  such  as  thrust  efficiency.  Furthermore,  the  high  magnetic  pole  count  in  many  ER-BLDC  motors  restricts  the  brushless  drives  able  to  efficiently  commutate  these  motors  at  speeds  needed  for  lightly-geared  operation.  This  paper  provides  an  empirical  characterization  of  a  popular  ER-BLDC  motor  and  a  new  brushless  drive,  which  includes  efficiencies  of  the  motor  across  different  power  regimes,  identification  of  the  motor  transfer  function  coefficients,  thermal  response  properties,  and  closed  loop  control  performance  in  the  time  and  frequency  domains.  The  intent  of  this  work  is  to  serve  as  a  benchmark  and  reference  for  other  researchers  seeking  to  utilize  these  exciting  and  emerging  motor  geometries.
0	Translational  damping  on  flapping  cicada  wings.  We  measured  the  dynamic  damping  of  a  pair  of  flapping  cicada  wings  mounted  on  a  robotic  insect  thorax  mechanism  capable  of  high  frequency  flapping.  The  damping  coefficients  were  derived  based  on  the  measurements  of  the  wing-thorax  mechanism  translating  along  its  body  principal  axes.  The  robotic  mechanism  has  a  10cm  wingtip-to-wingtip  span,  flaps  up  to  65Hz,  and  weigh  2.86  gram  including  the  motor  and  wings.  To  measure  the  flapping  induced  damping  during  translation,  we  developed  a  pendulum  system  mounted  with  encoder,  and  attached  the  flapper  at  the  end  and  in  different  orientations  such  that  its  motion  is  along  its  principle  axes.  The  damping  of  the  flapper  is  then  calculated  from  the  decaying  rate  of  the  magnitude  of  the  oscillating  pendulum.  The  damping  coefficients  calculated  from  the  experiments  are  very  close  to  those  estimated  based  on  our  mathematic  models  using  Blade-Element  Theory  (BET)  and  quasi-steady  aerodynamic  models.  As  expected,  the  damping  linearly  increases  with  the  flapping  frequency  and  is  most  prominent  along  forward/backward  direction.
0	Uncertainty  based  online  mapping  and  motion  planning  for  marine  robotics  guidance.  In  real-world  robotics,  motion  planning  remains  to  be  an  open  challenge.  Not  only  robotic  systems  are  required  to  move  through  unexplored  environments,  but  also  their  manoeuvrability  is  constrained  by  their  dynamics  and  often  suffer  from  uncertainty.  One  approach  to  overcome  this  problem  is  to  incrementally  map  the  surroundings  while,  simultaneously,  planning  a  safe  and  feasible  path  to  a  desired  goal.  This  is  especially  critical  in  underwater  environments,  where  autonomous  vehicles  must  deal  with  both  motion  and  environment  uncertainties.  In  order  to  cope  with  these  constraints,  this  work  proposes  an  uncertainty-based  framework  for  mapping  and  planning3  feasible  motions  online  with  probabilistic  safety-guarantees.  The  proposed  approach  deals  with  the  motion,  probabilistic  safety,  and  online  computation  constraints  by  (i)  incrementally  representing  the  environment  as  a  collection  of  local  maps,  and  (ii)  iteratively  (re)planning  kinodynamically-feasible  and  probabilistically-safe  paths  to  goal.  The  proposed  framework  is  evaluated  on  the  Sparus  II,  a  nonholonomic  torpedo-shaped  AUV,  by  conducting  simulated  and  real-world  trials,  thus  proving  the  efficacy  of  the  method  and  its  suitability  even  for  systems  with  limited  on-board  computational  power.
0	Coping  with  context  change  in  open  ended  object  recognition  without  explicit  context  information.  To  deploy  a  robot  in  a  human-centric  environment,  it  is  important  that  the  robot  is  able  to  continuously  acquire  and  update  object  categories  while  working  in  the  environment.  Therefore,  autonomous  robots  must  have  the  ability  to  continuously  execute  learning  and  recognition  in  a  concurrent  or  interleaved  fashion.  One  of  the  main  challenges  in  unconstrained  human  environments  is  to  cope  with  the  effects  of  context  change.  This  paper  presents  two  main  contributions:  (i)  an  approach  for  evaluating  open-ended  object  category  learning  and  recognition  methods  in  multi-context  scenarios;  (ii)  evaluation  of  different  object  category  learning  and  recognition  approaches  regarding  their  ability  to  cope  with  the  effects  of  context  change.  Off-line  evaluation  approaches  such  as  cross-validation  do  not  comply  with  the  simultaneous  nature  of  learning  and  recognition.  A  teaching  protocol,  supporting  context  change,  was  therefore  designed  and  used  in  this  work  for  experimental  evaluation.  Seven  learning  and  recognition  approaches  were  evaluated  and  compared  using  the  protocol.  The  best  performance,  in  terms  of  number  of  learned  categories,  was  obtained  with  a  recently  proposed  local  variant  of  Latent  Dirichlet  Allocation  (LDA),  closely  followed  by  a  Bag-of-Words  (BoW)  approach.  In  terms  of  adaptability,  i.e.  coping  with  context  change,  the  best  result  was  obtained  with  BoW,  immediately  followed  by  the  local  LDA  variant.
0	Design  and  closed  loop  control  of  a  tri  layer  polypyrrole  based  telescopic  soft  robot.  A  novel  structure  of  a  2  DoF  telescopic  soft  robot  using  a  tri-layer  Polypyrrole  (PPy)  soft  micro-actuator  with  deployment  is  presented  in  this  paper.  The  kinematic  model  is  introduced  and  the  Position  Based  Visual  Servo  (PBVS)  control  with  path-planning  and  obstacle  avoidance  algorithms  is  developed.  A  prototype  is  presented  and  the  control  schemes  are  validated  experimentally.  A  satisfactory  accuracy  with  a  submillimetric  positioning  error  is  obtained  namely  287.6  microns  for  a  circular  path  and  210  microns  for  obstacle  avoidance.
0	Multi  objective  parameter  cpg  optimization  for  gait  generation  of  a  quadruped  robot  considering  behavioral  diversity.  This  paper  presents  a  gait  multi-objective  optimization  system  that  combines  bio-inspired  Central  Patterns  Generators  (CPGs)  and  a  multi-objective  evolutionary  algorithm.  CPGs  are  modeled  as  autonomous  differential  equations,  that  generate  the  necessary  limb  movement  to  perform  the  required  walking  gait.  In  order  to  optimize  the  walking  gait,  four  conflicting  objectives  are  considered,  simultaneously:  minimize  the  body  vibration,  maximize  the  velocity,  maximize  the  wide  stability  margin  and  maximize  the  behavioral  diversity.  The  results  of  NSGA-II  for  this  multi-objective  problem  are  discussed.  The  effect  of  the  inclusion  of  a  behavioral  diversity  objective  in  the  system  is  also  studied  in  terms  of  the  walking  gait  achieved.  The  experimental  results  show  the  effectiveness  of  this  multi-objective  approach.  The  several  walking  gait  solutions  obtained  correspond  to  different  trade-off  between  the  objectives.
0	Minimizing  communication  latency  in  multirobot  situation  aware  patrolling.  We  consider  the  problem  of  computing  patrolling  strategies  under  communication  constraints  for  a  team  of  autonomous  robots  employed  in  repeated  surveillance  missions  on  a  set  of  predefined  locations.  We  assume  the  presence  of  a  communication  infrastructure  providing  only  some  regions  of  the  environment  with  a  communication  link  to  a  mission  control  center  (MCC).  We  define  the  problem  of  computing  a  joint  patrolling  strategy  that  minimizes  communication  latencies,  defined  as  the  delays  between  inspecting  some  locations  and  reporting  the  outcome  to  the  MCC.  We  provide  and  experimentally  evaluate  a  MILP  formulation  and  a  heuristic  method.
0	Active  learning  of  reward  dynamics  from  hierarchical  queries.  Enabling  robots  to  act  according  to  human  preferences  across  diverse  environments  is  a  crucial  task,  extensively  studied  by  both  roboticists  and  machine  learning  researchers.  To  achieve  it,  human  preferences  are  often  encoded  by  a  reward  function  which  the  robot  optimizes  for.  This  reward  function  is  generally  static  in  the  sense  that  it  does  not  vary  with  time  or  the  interactions.  Unfortunately,  such  static  reward  functions  do  not  always  adequately  capture  human  preferences,  especially,  in  non-stationary  environments:  Human  preferences  change  in  response  to  the  emergent  behaviors  of  the  other  agents  in  the  environment.  In  this  work,  we  propose  learning  reward  dynamics  that  can  adapt  in  non-stationary  environments  with  several  interacting  agents.  We  define  reward  dynamics  as  a  tuple  of  reward  functions,  one  for  each  mode  of  interaction,  and  mode-utility  functions  governing  transitions  between  the  modes.  Reward  dynamics  thereby  encodes  not  only  different  human  preferences  but  also  how  the  preferences  change.  Our  contribution  is  in  the  way  we  adapt  preference-based  learning  into  a  hierarchical  approach  that  aims  at  learning  not  only  reward  functions  but  also  how  they  evolve  based  on  interactions.  We  derive  a  probabilistic  observation  model  of  how  people  will  respond  to  the  hierarchical  queries.  Our  algorithm  leverages  this  model  to  actively  select  hierarchical  queries  that  will  maximize  the  volume  removed  from  a  continuous  hypothesis  space  of  reward  dynamics.  We  empirically  demonstrate  reward  dynamics  can  match  human  preferences  accurately.
0	Improving  innovation  performance  through  knowledge  acquisition  the  moderating  role  of  employee  retention  and  human  resource  management  practices.  Purpose          This  paper  aims  to  study  the  effects  of  knowledge  acquisition  on  innovation  performance  and  the  moderating  effects  of  human  resource  management  (HRM),  in  terms  of  employee  retention  and  HRM  practices,  on  the  above-mentioned  relationship.          Design/methodology/approach          A  sample  of  129  firms  operating  in  a  wide  array  of  sectors  has  been  used  to  gather  data  through  a  standardized  questionnaire  for  testing  the  hypotheses  through  ordinary  least  squares  (OLS)  regression  models.          Findings          The  results  indicate  that  knowledge  acquisition  positively  affects  innovation  performance  and  that  HRM  moderates  the  relationship  between  knowledge  acquisition  and  innovation  performance.          Originality/value          With  the  increasing  proclivity  towards  engaging  in  open  innovation,  firms  are  likely  to  face  some  tensions  and  opportunities  leading  to  a  shift  in  the  management  of  human  resources.  This  starts  from  the  assumption  that  the  knowledge  base  of  the  firm  resides  in  the  people  who  work  for  the  firm  and  that  some  HRM  factors  can  influence  innovation  within  firms.  Despite  this,  there  is  a  lack  of  research  investigating  the  link  between  knowledge  acquisition,  HRM  and  innovation  performance  under  the  open  innovation  lens.  This  paper  intends  to  fill  this  gap  and  nurture  future  research  by  assessing  whether  knowledge  acquisition  influences  innovation  performance  and  whether  HRM  moderates  such  a  relationship.
0	Social  media  as  a  tool  of  knowledge  sharing  in  academia  an  empirical  study  using  valance  instrumentality  and  expectancy  vie  approach.  The  purpose  of  this  paper  is  to  understand  the  factors  that  determine  the  knowledge  exchange  intention  and  behavioural  nature  of  academics  by  the  help  of  social  media  tools  in  the  Indian  higher  education.,This  study  has  used  valance–instrumentality–expectancy  (VIE)  theory  to  determine  the  knowledge  exchange  behaviour  of  academics.  The  study  has  considered  the  effects  of  knowledge  contributor  (KC)  and  knowledge  seeker  (KS)  as  moderators.  The  model  has  been  validated  by  using  a  survey  with  320  usable  respondents.,The  results  highlight  that  if  the  stakeholders  of  higher  education  institutions  feel  the  deficits  of  knowledge  exchange,  they  realize  importance  of  knowledge  sharing  and  use  social  media  to  increase  effect  of  knowledge  exchange.  Besides,  perceived  usefulness  impacts  on  the  use  of  social  media  for  knowledge  exchange  by  the  concerned  stakeholders.  Moreover,  it  is  observed  that  experience  of  the  use  of  social  media  impacts  the  use  of  this  tool  for  knowledge  exchange.,The  use  and  application  of  VIE  theory  have  successfully  been  able  to  interpret  the  factors  affecting  use  of  social  media  for  knowledge  exchange  in  higher  educational  institutes.  The  use  of  VIE  theory  has  also  been  able  to  explain  the  proposed  model  better  as  the  model  could  achieve  a  high  explanative  power  (87%).,This  study  has  provided  meaningful  insights  to  the  practitioners  and  policymakers  to  realize  how  the  stakeholders  of  the  higher  education  institutions  in  India  can  be  motivated  to  feel  the  need  of  sharing  of  knowledge  and  how  they  can  use  social  media  with  ease  for  this  purpose.,Not  much  research  has  been  conducted  with  regard  to  the  usage  of  social  media  as  a  tool  for  knowledge  sharing  in  higher  education  sector  in  India.  In  that  sense,  this  study  is  a  novel  attempt  to  undertake  such  research.
0	Black  box  data  efficient  policy  search  for  robotics.  The  most  data-efficient  algorithms  for  reinforcement  learning  (RL)  in  robotics  are  based  on  uncertain  dynamical  models:  after  each  episode,  they  first  learn  a  dynamical  model  of  the  robot,  then  they  use  an  optimization  algorithm  to  find  a  policy  that  maximizes  the  expected  return  given  the  model  and  its  uncertainties.  It  is  often  believed  that  this  optimization  can  be  tractable  only  if  analytical,  gradient-based  algorithms  are  used;  however,  these  algorithms  require  using  specific  families  of  reward  functions  and  policies,  which  greatly  limits  the  flexibility  of  the  overall  approach.  In  this  paper,  we  introduce  a  novel  model-based  RL  algorithm,  called  Black-DROPS  (Black-box  Data-efficient  RObot  Policy  Search)  that:  (1)  does  not  impose  any  constraint  on  the  reward  function  or  the  policy  (they  are  treated  as  black-boxes),  (2)  is  as  data-efficient  as  the  state-of-the-art  algorithm  for  data-efficient  RL  in  robotics,  and  (3)  is  as  fast  (or  faster)  than  analytical  approaches  when  several  cores  are  available.  The  key  idea  is  to  replace  the  gradient-based  optimization  algorithm  with  a  parallel,  black-box  algorithm  that  takes  into  account  the  model  uncertainties.  We  demonstrate  the  performance  of  our  new  algorithm  on  two  standard  control  benchmark  problems  (in  simulation)  and  a  low-cost  robotic  manipulator  (with  a  real  robot).
0	Laparoscopic  optical  biopsies  in  vivo  robotized  mosaicing  with  probe  based  confocal  endomicroscopy.  Probe-based  confocal  laser  endomicroscopy  is  a  promising  technology  for  performing  minimally-invasive  optical  biopsies.  With  the  help  of  mosaicing  algorithms,  several  studies  reported  successful  results  in  endoluminal  surgery.  In  this  paper,  we  present  a  prototype  for  making  robotized  optical  biopsies  on  a  variety  of  organs  inside  the  abdominal  cavity.  We  chose  a  macro-micro  association,  with  a  macropositioner,  a  micropositioner  and  a  passive  mechanical  compensation  of  physiological  motion.  The  probe  is  actuated  by  three  hydraulic  micro-balloons  and  can  be  moved  on  the  surface  of  an  organ  to  generate  a  mosaic.  This  paper  presents  the  design  and  experimental  results  of  a  first  in  vivo  trial  on  a  porcine  model.
0	An  ankle  foot  orthosis  with  insertion  point  eccentricity  control.  A  prototype  active  Ankle  Foot  Orthosis  has  been  developed  to  control  and  assist  plantar  flexion  and  dorsiflexion  using  an  innovative  series  elastic  actuator  integrated  into  a  standard  passive  AFO.  A  motor  controls  the  moment  arm  (Insertion  Point  Eccentricity  Control  or  IPEC)  of  a  pretensioned  spring,  and  though  the  magnitude  of  the  spring  force  remains  relatively  constant,  the  changing  moment  arm  produces  torque  about  the  ankle.  The  IPEC  AFO  is  able  to  provide  3.51  Nm  about  the  ankle  in  dorsiflexion  and  3.88  Nm  in  plantar  flexion  with  a  spring  modulus  of  3110  N/m  and  an  initial  tension  of  77  N.  The  torque  was  sufficient  to  prevent  toe-drag  during  swing  phase.  This  technology  may  benefit  users  with  drop-foot  or  other  ankle  disorders,  including  those  who  have  suffered  stroke  or  spinal  cord  injury.
0	Making  sense  of  audio  vibration  for  liquid  height  estimation  in  robotic  pouring.  In  this  paper,  we  focus  on  the  challenging  perception  problem  in  robotic  pouring.  Most  of  the  existing  approaches  either  leverage  visual  or  haptic  information.  However,  these  techniques  may  suffer  from  poor  generalization  performances  on  opaque  containers  or  concerning  measuring  precision.  To  tackle  these  drawbacks,  we  propose  to  make  use  of  audio  vibration  sensing  and  design  a  deep  neural  network  PouringNet  to  predict  the  liquid  height  from  the  audio  fragment  during  the  robotic  pouring  task.  PouringNet  is  trained  on  our  collected  real-world  pouring  dataset  with  multimodal  sensing  data,  which  contains  more  than  3000  recordings  of  audio,  force  feedback,  video  and  trajectory  data  of  the  human  hand  that  performs  the  pouring  task.  Each  record  represents  a  complete  pouring  procedure.  We  conduct  several  evaluations  on  PouringNet  with  our  dataset  and  robotic  hardware.  The  results  demonstrate  that  our  PouringNet  generalizes  well  across  different  liquid  containers,  positions  of  the  audio  receiver,  initial  liquid  heights  and  types  of  liquid,  and  facilitates  a  more  robust  and  accurate  audio-based  perception  for  robotic  pouring.
0	On  the  robustness  of  speech  emotion  recognition  for  human  robot  interaction  with  deep  neural  networks.  Speech  emotion  recognition  (SER)  is  an  important  aspect  of  effective  human-robot  collaboration  and  received  a  lot  of  attention  from  the  research  community.  For  example,  many  neural  network-based  architectures  were  proposed  recently  and  pushed  the  performance  to  a  new  level.  However,  the  applicability  of  such  neural  SER  models  trained  only  on  in-domain  data  to  noisy  conditions  is  currently  under-researched.  In  this  work,  we  evaluate  the  robustness  of  state-of-the-art  neural  acoustic  emotion  recognition  models  in  human-robot  interaction  scenarios.  We  hypothesize  that  a  robot's  ego  noise,  room  conditions,  and  various  acoustic  events  that  can  occur  in  a  home  environment  can  significantly  affect  the  performance  of  a  model.  We  conduct  several  experiments  on  the  iCub  robot  platform  and  propose  several  novel  ways  to  reduce  the  gap  between  the  model's  performance  during  training  and  testing  in  real-world  conditions.  Furthermore,  we  observe  large  improvements  in  the  model  performance  on  the  robot  and  demonstrate  the  necessity  of  introducing  several  data  augmentation  techniques  like  overlaying  background  noise  and  loudness  variations  to  improve  the  robustness  of  the  neural  approaches.
0	An  active  stabilizer  for  cable  driven  parallel  robot  vibration  damping.  Cable-Driven  Parallel  Robots  (CDPRs)  can  execute  fast  motions  across  a  large  workspace.  However,  these  performances  are  reached  at  the  cost  of  a  relatively  low  stiffness  which  often  yields  parasitic  vibrations  at  the  CDPR  mobile  platform.  In  this  paper,  vibration  damping  of  CDPRs  is  addressed  by  means  of  an  original  active  stabilizer  consisting  of  actuated  rotating  arms  installed  on-board  the  CDPR  mobile  platform.  A  control  strategy  for  the  whole  system,  which  consists  of  the  CDPR  and  the  stabilizer,  and  with  one  purpose  for  each-position  control  for  the  platform  and  vibration  damping  for  the  stabilizer—is  designed.  The  system  being  controlled  at  two  different  time  scales,  the  singular  perturbation  theory  can  be  used  to  prove  the  stability  of  the  corresponding  closed-loop  system.  The  efficiency  of  the  proposed  device  and  control  strategy  is  tested  in  simulations  in  the  case  of  a  planar  3-DOF  CDPR  equipped  with  a  three-arm  stabilizer.
0	Robocentric  visual  inertial  odometry.  In  this  paper,  we  propose  a  novel  robocentric  formulation  of  visual-inertial  navigation  systems  (VINS)within  a  multi-state  constraint  Kalman  filter  (MSCKF)framework  and  develop  an  efficient,  lightweight,  robocentric  visual-inertial  odometry  (R-VIO)algorithm  for  consistent  localization  in  challenging  environments  using  only  monocular  vision.  The  key  idea  of  the  proposed  approach  is  to  deliberately  reformulate  the  3D  VINS  with  respect  to  a  moving  local  frame  (i.e.,  robocentric),  rather  than  a  fixed  global  frame  of  reference  as  in  the  standard  world-centric  VINS,  and  instead  utilize  high-accuracy  relative  motion  estimates  for  global  pose  update.  As  an  immediate  advantage  of  using  this  robocentric  formulation,  the  proposed  R-VIO  can  start  from  an  arbitrary  pose,  without  the  need  to  align  its  orientation  with  the  global  gravity  vector.  More  importantly,  we  analytically  show  that  the  proposed  robocentric  EKF-based  VINS  does  not  undergo  the  observability  mismatch  issue  as  in  the  standard  world-centric  frameworks  which  was  identified  as  the  main  cause  of  inconsistency  of  estimation.  The  proposed  R-VIO  is  extensively  tested  through  both  Monte  Carlo  simulations  and  real-world  experiments  using  different  sensor  platforms  in  different  environments  and  shown  to  achieve  competitive  performance  with  the  state-of-the-art  VINS  algorithms  in  terms  of  consistency,  accuracy  and  efficiency.
0	An  adaptive  controller  for  autonomous  underwater  vehicles.  This  paper  introduces  an  adaptive  tuning  method  for  the  controllers  of  a  4  degrees-of-freedom  autonomous  underwater  vehicle.  The  proposed  scheme  consists  of  two  control  loops,  one  for  position  control  and  an  inner  one  for  velocity  control.  The  gains  of  the  controller  are  determined  on-line,  according  to  the  position/velocity  errors.  Using  the  proposed  adaptive  architecture,  the  uncertainties  in  the  parameters  of  the  system  are  addressed  and  the  system  is  able  to  operate  when  hydrodynamic  disturbances  are  present.  The  complexity  of  the  fixed  gain  tuning  procedure  is  also  greatly  decreased  for  underwater  vehicles  when  the  algorithm  suggested  here  is  used.  Experimental  results  with  the  Nessie  VII  AUV  show  that  the  adaptive  controller  is  beneficial  for  underwater  vehicles.  Finally  it  is  shown  that  the  current  approach  reduces  the  energy  consumption  of  the  system.
0	Mpc  based  humanoid  pursuit  evasion  in  the  presence  of  obstacles.  We  consider  a  pursuit-evasion  problem  between  humanoids  in  the  presence  of  obstacles.  In  our  scenario,  the  pursuer  enters  the  safety  area  of  the  evader  headed  for  collision,  while  the  latter  executes  a  fast  evasive  motion.  Control  schemes  are  designed  for  both  the  pursuer  and  the  evader.  They  are  structurally  identical,  although  the  objectives  are  different:  the  pursuer  tries  to  align  its  direction  of  motion  with  the  line-of-sight  to  the  evader,  whereas  the  evader  tries  to  move  in  a  direction  orthogonal  to  the  line-of-sight  to  the  pursuer.  At  the  core  of  the  control  architecture  is  a  Model  Predictive  Control  scheme  for  generating  a  stable  gait.  This  allows  for  the  inclusion  of  workspace  obstacles,  which  we  take  into  account  at  two  levels:  during  the  determination  of  the  footsteps  orientation  and  as  an  explicit  MPC  constraint.  We  illustrate  the  results  with  simulations  on  NAO  humanoids.
0	Vision  only  estimation  of  wind  field  strength  and  direction  from  an  aerial  platform.  This  study  describes  a  novel  method  for  estimating  the  strength  and  direction  of  the  local  wind  field  from  a  mobile  airborne  platform.  An  iterative  optimisation  is  derived  that  allows  the  properties  of  the  wind  field  to  be  determined  from  successive  measurements  of  the  heading  direction  and  ground  track  of  the  aircraft  only.  We  have  previously  described  methods  for  estimating  these  parameters  using  a  single  vision  system.  This  approach  therefore  constitutes  a  purely  visual  method  for  estimating  the  properties  of  the  local  wind  field.  We  present  results  from  simulated  and  real-world  flight  tests  that  demonstrate  the  accuracy  and  robustness  of  the  proposed  method  and  its  practicality  in  uncontrolled  environmental  conditions.  These  properties  and  the  simplicity  of  the  implementation  should  make  this  approach  useful  as  an  alternative  means  for  estimating  the  properties  of  the  local  wind  field  from  small-scale,  fixed-wing  UAVs.
0	Haptic  feedback  and  dynamic  active  constraints  for  robot  assisted  endovascular  catheterization.  Robotic  and  computer  assistance  can  bring  significant  benefits  to  endovascular  procedures  in  terms  of  precision  and  stability,  reduced  radiation  doses,  improved  comfort  and  access  to  difficult  and  tortuous  anatomy.  However,  the  design  of  current  commercially  available  platforms  tends  to  alter  the  natural  bedside  manipulation  skills  of  the  operator,  so  that  the  manually  acquired  experience  and  dexterity  are  not  well  utilized.  Furthermore,  most  of  these  systems  lack  of  haptic  feedback,  preventing  their  acceptance  and  limiting  the  clinical  usability.  In  this  paper  a  new  robotic  platform  for  endovascular  catheterization,  the  CathBot,  is  presented.  It  is  an  ergonomic  master-slave  system  with  navigation  system  and  integrated  vision-based  haptic  feedback,  designed  to  maintain  the  natural  bedside  skills  of  the  vascular  surgeon.  Unlike  previous  work  reported  in  literature,  dynamic  motion  tracking  of  both  the  vessel  walls  the  catheter  tip  is  incorporated  to  create  dynamic  active  constraints.  The  system  was  evaluated  through  a  combined  quantitative  and  qualitative  user  study  simulating  catheterization  tasks  on  a  phantom.  Forces  exerted  on  the  phantom  were  measured.  The  results  showed  a  70%  decrease  in  mean  force  and  61%  decrease  in  maximum  force  when  force  feedback  is  provided.  This  research  provides  the  first  integration  of  vision-based  dynamic  active  constraints  within  an  ergonomic  robotic  catheter  manipulator.  The  technological  advances  presented  here,  demonstrates  that  vision-based  haptic  feedback  can  improve  the  effectiveness,  precision,  and  safety  of  robot-assisted  endovascular  procedures.
0	A  hybrid  multi  objective  iterated  local  search  heuristic  for  vehicle  routing  problem  with  time  windows.  The  Vehicle  Routing  Problem  with  Time  Windows  (VRPTW)  is  a  well  known  NP-Hard  combinatorial  optimization  problem  and  it  has  received  a  lot  of  attention  in  the  literature.  In  this  problem,  a  fleet  of  identical  vehicles  must  leave  the  depot,  supply  all  costumers  demands,  and  return  to  the  depot,  at  minimum  cost,  without  violating  the  capacity  of  the  vehicles  as  well  as  the  time  window  specified  by  each  customer.  This  paper  considers  a  VRPTW  which  aims  to  minimize  two  objective  functions  simultaneously:  the  total  traveling  distance  and  the  imbalance  in  the  distances  traveled  by  the  vehicles  used.  To  obtain  near  Pareto-optimal  solutions,  we  propose  a  hybrid  heuristic  based  on  Iterated  Local  Search,  Variable  Neighbourhood  Descent  with  random  neighbourhood  ordering  for  solution  improvement  and  Recombination  of  non-dominated  solutions  as  used  in  genetic  algorithms.  The  results  obtained  when  solving  a  subset  of  Solomon's  benchmark  problems  show  the  good  performance  of  the  hybrid  heuristic.
0	Personalized  ontology  model  a  survey.  Ontology  is  an  explicit  specification  of  knowledge  as  formal  description  of  concepts,  relations  between  the  concepts  and  axioms  about  the  target  domain.  Ontology  is  also  used  to  represent  user  profiles  in  personalized  web  information  gathering.  For  representing  user  profiles  many  models  have  been  developed,  these  models  provide  knowledge  from  either  a  global  or  local  knowledge  base.  The  global  analysis  uses  existing  global  knowledge  bases  and  to  produce  effective  performance.  The  local  analysis  observes  user  behavior  in  user  profiles.  The  user  background  knowledge  can  be  better  discovered  and  represented  if  we  integrate  global  and  local  analysis.  However,  it  is  challenging  to  use  semantic  relations  of  “kind-of”,  “part-of”,  and  “is-a”  and  synthesize  commonsense  and  expert  knowledge  in  a  single  computational  model.  In  this  paper,  a  personalized  ontology  model  based  on  user  profiles  is  used.  This  model  learns  user  profile  from  both  global  knowledge  base  and  local  instance  repositories.  After  comparing  with  benchmark  models  in  web  information  gathering  ontology  model  has  an  edge  over  other  models
0	Semi  supervised  multi  label  topic  models  for  document  classification  and  sentence  labeling.  Extracting  parts  of  a  text  document  relevant  to  a  class  label  is  a  critical  information  retrieval  task.  We  propose  a  semi-supervised  multi-label  topic  model  for  jointly  achieving  document  and  sentence-level  class  inferences.  Under  our  model,  each  sentence  is  associated  with  only  a  subset  of  the  document's  labels  (including  possibly  none  of  them),  with  the  label  set  of  the  document  the  union  of  the  labels  of  all  of  its  sentences.  For  training,  we  use  both  labeled  documents,  and,  typically,  a  larger  set  of  unlabeled  documents.  Our  model,  in  a  semisupervised  fashion,  discovers  the  topics  present,  learns  associations  between  topics  and  class  labels,  predicts  labels  for  new  (or  unlabeled)  documents,  and  determines  label  associations  for  each  sentence  in  every  document.  For  learning,  our  model  does  not  require  any  ground-truth  labels  on  sentences.  We  develop  a  Hamiltonian  Monte  Carlo  based  algorithm  for  efficiently  sampling  from  the  joint  label  distribution  over  all  sentences,  a  very  high-dimensional  discrete  space.  Our  experiments  show  that  our  approach  outperforms  several  benchmark  methods  with  respect  to  both  document  and  sentence-level  classification,  as  well  as  test  set  log-likelihood.  All  code  for  replicating  our  experiments  is  available  from  https://github.com/hsoleimani/MLTM.
0	Timely  crawling  of  high  quality  ephemeral  new  content.  In  this  paper,  we  study  the  problem  of  timely  finding  and  crawling  of  \textit{ephemeral}  new  pages,  i.e.,  for  which  user  traffic  grows  really  quickly  right  after  they  appear,  but  lasts  only  for  several  days  (e.g.,  news,  blog  and  forum  posts).  Traditional  crawling  policies  do  not  give  any  particular  priority  to  such  pages  and  may  thus  crawl  them  not  quickly  enough,  and  even  crawl  already  obsolete  content.  We  thus  propose  a  new  metric,  well  thought  out  for  this  task,  which  takes  into  account  the  decrease  of  user  interest  for  ephemeral  pages  over  time.      We  show  that  most  ephemeral  new  pages  can  be  found  at  a  relatively  small  set  of  content  sources  and  suggest  a  method  for  finding  such  a  set.  Our  idea  is  to  periodically  recrawl  content  sources  and  crawl  newly  created  pages  linked  from  them,  focusing  on  high-quality  (in  terms  of  user  interest)  content.  One  of  the  main  difficulties  here  is  to  divide  resources  between  these  two  activities  in  an  efficient  way.  We  find  the  adaptive  balance  between  crawls  and  recrawls  by  maximizing  the  proposed  metric.  Further,  we  incorporate  search  engine  click  logs  to  give  our  crawler  an  insight  about  the  current  user  demands.  The  effectiveness  of  our  approach  is  finally  demonstrated  experimentally  on  real-world  data.
0	Bayesian  variable  selection  for  linear  regression  in  high  dimensional  microarray  data.  Variable  selection  is  a  fundamental  problem  in  Bayesian  statistics  whose  solution  requires  exploring  a  combinatorial  search  space.  We  study  the  solution  of  variable  selection  with  a  well-known  MCMC  method,  which  requires  thousands  of  iterations.  We  present  several  algorithmic  optimizations  to  accelerate  the  MCMC  method  to  make  it  work  efficiently  inside  a  database  system.  Our  optimizations  include  sufficient  statistics,  variable  preselection,  hash  tables  and  calling  a  linear  algebra  library.  We  present  experiments  with  very  high  dimensional  microarray  data  sets  to  predict  cancer  survival  time.  We  discuss  encouraging  findings,  identifying  specific  genes  likely  to  predict  the  survival  time  for  brain  cancer  patients.  We  also  show  our  DBMS-based  algorithm  is  orders  of  magnitude  faster  than  the  R  statistical  package.  Our  work  shows  a  DBMS  is  a  promising  platform  to  analyze  microarray  data.
0	Suggesting  ghost  edges  for  a  smaller  world.  Small  changes  in  the  network  topology  can  have  dramatic  effects  on  its  capacity  to  disseminate  information.  In  this  paper,  we  consider  the  problem  of  adding  a  small  number  of  ghost  edges  in  the  network  in  order  to  minimize  the  average  shortest-path  distance  between  nodes,  towards  a  smaller-world  network.  We  formalize  the  problem  of  suggesting  ghost  edges  and  we  propose  a  novel  method  for  quickly  evaluating  the  importance  of  ghost  edges  in  sparse  graphs.  Through  experiments  on  real  and  synthetic  data  sets,  we  demonstrate  that  our  approach  performs  very  well,  for  a  varying  range  of  conditions,  and  it  outperforms  sensible  baselines.
0	Exploring  missing  interactions  a  convolutional  generative  adversarial  network  for  collaborative  filtering.  Adversarial  examples  can  be  detrimental  to  a  recommender,leading  to  a  surging  enthusiasm  for  applying  adversarial  learning  to  improve  recommendation  performance,  e.g.  raising  model  robustness,  alleviating  data  sparsity,  generating  initial  profiles  for  cold-start  users  or  items,  etc.  Most  existing  adversarial  example  generation  methods  fall  within  three  categories:  attacking  the  user-item  interactions  or  auxiliary  contents,  adding  perturbations  in  latent  space,  sampling  the  latent  space  according  to  certain  distribution.  In  this  work,  we  focus  on  the  semantic-rich  user-item  interactions  in  a  recommender  system  and  propose  a  novel  generative  adversarial  network  (GAN)  named  Convolutional  Generative  Collaborative  Filtering  (Conv-GCF).  We  develop  an  effective  perturbation  mechanism  (adversarial  noise  layer)  for  convolutional  neural  networks  (CNN),  based  on  which  we  design  a  generator  with  residual  blocks  to  synthesize  user-item  interactions.  We  empirically  demonstrate  that  on  Conv-GCF,  the  adversarial  noise  layer  is  superior  to  the  conventional  noise-adding  approach.  Moreover,  we  propose  two  types  of  discriminators:  one  using  Bayes  Personalized  Ranking  (BPR)  and  the  other  with  binary  classification.  On  four  public  datasets,  we  show  that  our  approach  achieves  the  state-of-the-art  top-n  recommendation  performance  among  competitive  baselines.
0	A  comparison  of  nuggets  and  clusters  for  evaluating  timeline  summaries.  There  is  growing  interest  in  systems  that  generate  timeline  summaries  by  filtering  high-volume  streams  of  documents  to  retain  only  those  that  are  relevant  to  a  particular  event  or  topic.  Continued  advances  in  algorithms  and  techniques  for  this  task  depend  on  standardized  and  reproducible  evaluation  methodologies  for  comparing  systems.  However,  timeline  summary  evaluation  is  still  in  its  infancy,  with  competing  methodologies  currently  being  explored  in  international  evaluation  forums  such  as  TREC.  One  area  of  active  exploration  is  how  to  explicitly  represent  the  units  of  information  that  should  appear  in  a  "good"  summary.  Currently,  there  are  two  main  approaches,  one  based  on  identifying  nuggets  in  an  external  "ground  truth",  and  the  other  based  on  clustering  system  outputs.  In  this  paper,  by  building  test  collections  that  have  both  nugget  and  cluster  annotations,  we  are  able  to  compare  these  two  approaches.  Specifically,  we  address  questions  related  to  evaluation  effort,  differences  in  the  final  evaluation  products,  and  correlations  between  scores  and  rankings  generated  by  both  approaches.  We  summarize  advantages  and  disadvantages  of  nuggets  and  clusters  to  offer  recommendations  for  future  system  evaluations.
0	Emotion  recognition  from  eeg  signals  using  hierarchical  bayesian  network  with  privileged  information.  Current  work  of  emotion  recognition  from  electroencephalogram  (EEG)  signals  mainly  focuses  on  the  generality  among  users,  ignoring  users'  specificity.  However,  users'  emotion  is  a  subjective  phenomenon  with  both  common  and  specific  characteristics.  Therefore,  we  propose  a  novel  emotion  recognition  method  using  hierarchical  Bayesian  network  to  handle  generality  and  specificity  of  emotions  simultaneously.  Specifically,  by  modeling  the  prior  distributions  of  parameters  for  each  subject,  the  classifier  for  a  subject  is  learned  together  with  those  of  others,  with  a  shared  representation.  In  addition,  by  marginalizing  over  the  node  of  subjects,  the  subject  information  is  used  as  privileged  information,  which  is  only  required  during  training  to  build  a  better  classifier.  Experimental  results  on  the  MAHNOB-HCI  and  DEAP  databases  demonstrate  that  our  model  with  the  subject  id  as  privileged  information  can  improve  the  emotion  recognition  performance.
0	Semiconducting  bilinear  deep  learning  for  incomplete  image  recognition.  Image  recognition  with  incomplete  data  is  a  well-known  hard  problem  in  multimedia  content  analysis.  This  paper  proposes  a  novel  deep  learning  technique  called  semiconducting  bilinear  deep  belief  networks  (SBDBN)  by  referencing  human's  visual  cortex  and  intelligent  perception.  Inheriting  from  deep  models,  SBDBN  simulates  the  laminar  structure  of  human's  cerebral  cortex  and  the  neural  loop  in  human's  visual  areas.  To  address  the  special  difficulties  of  image  recognition  with  incomplete  data,  we  design  a  novel  second-order  deep  architecture  with  semiconducting  restricted  boltzmann  machines.  Moreover,  two  peaks  activation  of  human's  perception  is  implemented  by  three  learning  stages  of  semiconducting  bilinear  discriminant  initialization,  greedy  layer-wise  reconstruction,  and  global  fine-tuning.  Owing  to  exploiting  the  embedding  information  according  to  the  reliable  features  rather  than  any  completion  of  missing  features,  the  proposed  SBDBN  has  demonstrated  outstanding  recognition  ability  on  two  standard  datasets  and  one  constructed  dataset,  comparing  with  both  incomplete  image  recognition  techniques  and  existing  deep  learning  models.
0	All  vehicles  are  cars  subclass  preferences  in  container  concepts.  This  paper  investigates  the  natural  bias  humans  display  when  labeling  images  with  a  container  label  like  vehicle  or  carnivore.  Using  three  container  concepts  as  subtree  root  nodes,  and  all  available  concepts  between  these  roots  and  the  images  from  the  ImageNet  Large  Scale  Visual  Recognition  Challenge  (ILSVRC)  dataset,  we  analyze  the  differences  between  the  images  labeled  at  these  varying  levels  of  abstraction  and  the  union  of  their  constituting  leaf  nodes.  We  find  that  for  many  container  concepts,  a  strong  preference  for  one  or  a  few  different  constituting  leaf  nodes  occurs.  These  results  indicate  that  care  is  needed  when  using  hierarchical  knowledge  in  image  classification:  if  the  aim  is  to  classify  vehicles  the  way  humans  do,  then  cars  and  buses  may  be  the  only  correct  results.
0	Visual  sensor  networks  for  infomobility.  The  wide  availability  of  embedded  sensor  platforms  and  low-cost  cameras--together  with  the  developments  in  wireless  communication--make  it  now  possible  the  conception  of  pervasive  intelligent  systems  based  on  vision.  Such  systems  may  be  understood  as  distributed  and  collaborative  sensor  networks,  able  to  produce,  aggregate  and  process  images  in  order  to  understand  the  observed  scene  and  communicate  the  relevant  information  found  about  it.  In  this  paper,  we  investigate  the  peculiarities  of  visual  sensor  networks  with  respect  to  standard  vision  systems  and  we  identify  possible  strategies  to  accomplish  image  processing  and  analysis  tasks  over  them.  Although  the  rather  strong  constraints  in  computational  and  transmission  power  of  embedded  platforms  that  may  prevent  the  use  of  state  of  the  art  computer  vision  and  pattern  recognition  methods,  we  argue  that  multi-node  processing  methods  may  be  envisaged  to  decompose  a  complex  task  into  a  hierarchy  of  computationally  simpler  problems  to  be  solved  over  the  nodes  of  the  network.  These  ideas  are  illustrated  by  describing  an  application  of  visual  sensor  network  to  infomobility.  In  particular,  we  consider  an  experimental  setting  in  which  several  views  of  a  parking  lot  are  acquired  by  the  sensor  nodes  in  the  network.  By  integrating  the  various  views,  the  network  is  capable  to  provide  a  description  of  the  scene  in  terms  of  the  available  spaces  in  the  parking  lot.
0	A  method  of  facial  expression  recognition  based  on  gabor  and  nmf.  The  technology  of  facial  expression  recognition  is  a  challenging  problem  in  the  field  of  intelligent  human-computer  interaction.  An  algorithm  based  on  the  Gabor  wavelet  transformation  and  non-negative  matrix  factorization  (G-NMF)  is  presented.  The  main  process  includes  image  preprocessing,  feature  extraction  and  classification.  At  first,  the  face  region  containing  emotional  information  is  obtained  and  normalized.  Then,  expressional  features  are  extracted  by  Gabor  wavelet  transformation  and  the  high-dimensional  data  are  reduced  by  non-negative  matrix  factorization  (NMF).  Finally,  two-layer  classifier  (TLC)  is  designed  for  expression  recognition.  Experiments  are  done  on  JAFFE  facial  expressions  database.  The  results  show  that  the  method  proposed  has  a  better  performance.
0	Review  on  pedestrian  gait  feature  expression  and  recognition.  Various  methods  for  gait  recognition  are  summarized  by  unique  metods  from  anthropometry,spatial  temporal,kinematics,kinetics,and  video  stream  data  forms.And  video  stream  data  are  analyzed  in  detail.The  expressions,meanings  and  characteristics  including  static,dynamic,and  time-varying  information  among  different  energy  image  species  methods  are  compared.Moreover,the  fusion  of  gait  features,gait  and  other  biometric  and  extended  gait  recognition  are  reviewed.The  assessment  methods  for  gait  recognition  research  are  also  presented.In  addition,the  future  research  directions  of  gait  recognition  are  addressed.
0	Geometrics  exploiting  geometric  structure  for  graph  encoded  objects.  Mesh  models  are  a  promising  approach  for  encoding  the  structure  of  3D  objects.  Current  mesh  reconstruction  systems  predict  uniformly  distributed  vertex  locations  of  a  predetermined  graph  through  a  series  of  graph  convolutions,  leading  to  compromises  with  respect  to  performance  or  resolution.  In  this  paper,  we  argue  that  the  graph  representation  of  geometric  objects  allows  for  additional  structure,  which  should  be  leveraged  for  enhanced  reconstruction.  Thus,  we  propose  a  system  which  properly  benefits  from  the  advantages  of  the  geometric  structure  of  graph  encoded  objects  by  introducing  (1)  a  graph  convolutional  update  preserving  vertex  information;  (2)  an  adaptive  splitting  heuristic  allowing  detail  to  emerge;  and  (3)  a  training  objective  operating  both  on  the  local  surfaces  defined  by  vertices  as  well  as  the  global  structure  defined  by  the  mesh.  Our  proposed  method  is  evaluated  on  the  task  of  3D  object  reconstruction  from  images  with  the  ShapeNet  dataset,  where  we  demonstrate  state  of  the  art  performance,  both  visually  and  numerically,  while  having  far  smaller  space  requirements  by  generating  adaptive  meshes
0	Bayesian  counterfactual  risk  minimization.  We  present  a  Bayesian  view  of  counterfactual  risk  minimization  (CRM)  for  offline  learning  from  logged  bandit  feedback.  Using  PAC-Bayesian  analysis,  we  derive  a  new  generalization  bound  for  the  truncated  inverse  propensity  score  estimator.  We  apply  the  bound  to  a  class  of  Bayesian  policies,  which  motivates  a  novel,  potentially  data-dependent,  regularization  technique  for  CRM.  Experimental  results  indicate  that  this  technique  outperforms  standard  $L_2$  regularization,  and  that  it  is  competitive  with  variance  regularization  while  being  both  simpler  to  implement  and  more  computationally  efficient.
0	Semi  cyclic  stochastic  gradient  descent.  We  consider  convex  SGD  updates  with  a  block-cyclic  structure,  i.e.  where  each  cycle  consists  of  a  small  number  of  blocks,  each  with  many  samples  from  a  possibly  different,  block-specific,  distribution.  This  situation  arises,  e.g.,  in  Federated  Learning  where  the  mobile  devices  available  for  updates  at  different  times  during  the  day  have  different  characteristics.  We  show  that  such  block-cyclic  structure  can  significantly  deteriorate  the  performance  of  SGD,  but  propose  a  simple  approach  that  allows  prediction  with  the  same  performance  guarantees  as  for  i.i.d.,  non-cyclic,  sampling.
0	Being  bayesian  even  just  a  bit  fixes  overconfidence  in  relu  networks.  The  point  estimates  of  ReLU  classification  networks---arguably  the  most  widely  used  neural  network  architecture---have  been  shown  to  yield  arbitrarily  high  confidence  far  away  from  the  training  data.  This  architecture,  in  conjunction  with  a  maximum  a  posteriori  estimation  scheme,  is  thus  not  calibrated  nor  robust.  Approximate  Bayesian  inference  has  been  empirically  demonstrated  to  improve  predictive  uncertainty  in  neural  networks,  although  the  theoretical  analysis  of  such  Bayesian  approximations  is  limited.  We  theoretically  analyze  approximate  Gaussian  distributions  on  the  weights  of  ReLU  networks  and  show  that  they  fix  the  overconfidence  problem.  Furthermore,  we  show  that  even  a  simplistic,  thus  cheap,  Bayesian  approximation,  also  fixes  these  issues.  This  indicates  that  a  sufficient  condition  for  a  calibrated  uncertainty  on  a  ReLU  network  is  "to  be  a  bit  Bayesian".  These  theoretical  results  validate  the  usage  of  last-layer  Bayesian  approximation  and  motivate  a  range  of  a  fidelity-cost  trade-off.  We  further  validate  these  findings  empirically  via  various  standard  experiments  using  common  deep  ReLU  networks  and  Laplace  approximations.
0	Near  tight  margin  based  generalization  bounds  for  support  vector  machines.  Support  Vector  Machines  (SVMs)  are  among  the  most  fundamental  tools  for  binary  classification.  In  its  simplest  formulation,  an  SVM  produces  a  hyperplane  separating  two  classes  of  data  using  the  largest  possible  margin  to  the  data.  The  focus  on  maximizing  the  margin  has  been  well  motivated  through  numerous  generalization  bounds.  In  this  paper,  we  revisit  and  improve  the  classic  generalization  bounds  in  terms  of  margins.  Furthermore,  we  complement  our  new  generalization  bound  by  a  nearly  matching  lower  bound,  thus  almost  settling  the  generalization  performance  of  SVMs  in  terms  of  margins.
0	Stochastic  smoothing  for  nonsmooth  minimizations  accelerating  sgd  by  exploiting  structure.  In  this  work  we  consider  the  stochastic  minimization  of  nonsmooth  convex  loss  functions,  a  central  problem  in  machine  learning.  We  propose  a  novel  algorithm  called  Accelerated  Nonsmooth  Stochastic  Gradient  Descent  (ANSGD),  which  exploits  the  structure  of  common  nonsmooth  loss  functions  to  achieve  optimal  convergence  rates  for  a  class  of  problems  including  SVMs.  It  is  the  first  stochastic  algorithm  that  can  achieve  the  optimal  O(1/t)  rate  for  minimizing  nonsmooth  loss  functions  (with  strong  convexity).  The  fast  rates  are  confirmed  by  empirical  comparisons,  in  which  ANSGD  significantly  outperforms  previous  subgradient  descent  algorithms  including  SGD.
0	On  fast  sampling  of  diffusion  probabilistic  models.  In  this  work,  we  propose  FastDPM,  a  unified  framework  for  fast  sampling  in  diffusion  probabilistic  models.  FastDPM  generalizes  previous  methods  and  gives  rise  to  new  algorithms  with  improved  sample  quality.  We  systematically  investigate  the  fast  sampling  methods  under  this  framework  across  different  domains,  on  different  datasets,  and  with  different  amount  of  conditional  information  provided  for  generation.  We  find  the  performance  of  a  particular  method  depends  on  data  domains  (e.g.,  image  or  audio),  the  trade-off  between  sampling  speed  and  sample  quality,  and  the  amount  of  conditional  information.  We  further  provide  insights  and  recipes  on  the  choice  of  methods  for  practitioners.
0	Learning  to  label  aerial  images  from  noisy  data.  When  training  a  system  to  label  images,  the  amount  of  labeled  training  data  tends  to  be  a  limiting  factor.  We  consider  the  task  of  learning  to  label  aerial  images  from  existing  maps.  These  provide  abundant  labels,  but  the  labels  are  often  incomplete  and  sometimes  poorly  registered.  We  propose  two  robust  loss  functions  for  dealing  with  these  kinds  of  label  noise  and  use  the  loss  functions  to  train  a  deep  neural  network  on  two  challenging  aerial  image  datasets.  The  robust  loss  functions  lead  to  big  improvements  in  performance  and  our  best  system  substantially  outperforms  the  best  published  results  on  the  task  we  consider.
0	Tuning  free  plug  and  play  proximal  algorithm  for  inverse  imaging  problems.  Plug-and-play  (PnP)  is  a  non-convex  framework  that  combines  ADMM  or  other  proximal  algorithms  with  advanced  denoiser  priors.  Recently,  PnP  has  achieved  great  empirical  success,  especially  with  the  integration  of  deep  learning-based  denoisers.  However,  a  key  problem  of  PnP  based  approaches  is  that  they  require  manual  parameter  tweaking.  It  is  necessary  to  obtain  high-quality  results  across  the  high  discrepancy  in  terms  of  imaging  conditions  and  varying  scene  content.  In  this  work,  we  present  a  tuning-free  PnP  proximal  algorithm,  which  can  automatically  determine  the  internal  parameters  including  the  penalty  parameter,  the  denoising  strength  and  the  terminal  time.  A  key  part  of  our  approach  is  to  develop  a  policy  network  for  automatic  search  of  parameters,  which  can  be  effectively  learned  via  mixed  model-free  and  model-based  deep  reinforcement  learning.  We  demonstrate,  through  numerical  and  visual  experiments,  that  the  learned  policy  can  customize  different  parameters  for  different  states,  and  often  more  efficient  and  effective  than  existing  handcrafted  criteria.  Moreover,  we  discuss  the  practical  considerations  of  the  plugged  denoisers,  which  together  with  our  learned  policy  yield  state-of-the-art  results.  This  is  prevalent  on  both  linear  and  nonlinear  exemplary  inverse  imaging  problems,  and  in  particular,  we  show  promising  results  on  Compressed  Sensing  MRI  and  phase  retrieval.
0	Multifocus  image  fusion  using  focus  measure  of  fractional  differential  and  nsct.  In  this  paper,  a  new  multifocus  image  fusion  scheme  based  on  fractional  differential  and  NSCT  is  proposed.  Firstly,  in  virtue  of  the  properties  of  fractional  differential,  a  novel  focus  measure  in  nonsubsampled  contourlet  transform  (NSCT)  domain  is  presented  and  used  to  determine  which  coefficient  is  from  the  focused  region.  Then,  based  on  the  imaging  principle  of  the  multifocus  image  and  the  focus  measure,  a  new  selection  principle  for  the  lowpass  subbands  coefficients  is  developed.  Meanwhile,  focus  measure  maximum  choosing  scheme,  namely  select  the  coefficient  with  maximum  focus  measure  value  as  the  corresponding  coefficient  of  the  fused  image,  is  applied  to  the  high-frequency  subbands.  Finally,  the  inverse  NSCT  is  employed  to  reconstruct  the  fused  image  and  a  pleasing  fused  result  is  generated.  The  experimental  results  show  that  the  proposed  method  outperforms  the  conventional  multifocus  image  fusion  methods  in  both  subjective  and  objective  qualities.
0	Discovering  conditionally  salient  features  with  statistical  guarantees.  The  goal  of  feature  selection  is  to  identify  important  features  that  are  relevant  to  explain  an  outcome  variable.  Most  of  the  work  in  this  domain  has  focused  on  identifying  globally  relevant  features,  which  are  features  that  are  related  to  the  outcome  using  evidence  across  the  entire  dataset.  We  study  a  more  fine-grained  statistical  problem:  conditional  feature  selection,  where  a  feature  may  be  relevant  depending  on  the  values  of  the  other  features.  For  example  in  genetic  association  studies,  variant  $A$  could  be  associated  with  the  phenotype  in  the  entire  dataset,  but  conditioned  on  variant  $B$  being  present  it  might  be  independent  of  the  phenotype.  In  this  sense,  variant  $A$  is  globally  relevant,  but  conditioned  on  $B$  it  is  no  longer  locally  relevant  in  that  region  of  the  feature  space.  We  present  a  generalization  of  the  knockoff  procedure  that  performs  conditional  feature  selection  while  controlling  a  generalization  of  the  false  discovery  rate  (FDR)  to  the  conditional  setting.  By  exploiting  the  feature/response  model-free  framework  of  the  knockoffs,  the  quality  of  the  statistical  FDR  guarantee  is  not  degraded  even  when  we  perform  conditional  feature  selections.  We  implement  this  method  and  present  an  algorithm  that  automatically  partitions  the  feature  space  such  that  it  enhances  the  differences  between  selected  sets  in  different  regions,  and  validate  the  statistical  theoretical  results  with  experiments.
0	Representation  tradeoffs  for  hyperbolic  embeddings.  Hyperbolic  embeddings  offer  excellent  quality  with  few  dimensions  when  embedding  hierarchical  data  structures  like  synonym  or  type  hierarchies.  Given  a  tree,  we  give  a  combinatorial  construction  that  embeds  the  tree  in  hyperbolic  space  with  arbitrarily  low  distortion  without  using  optimization.  On  WordNet,  our  combinatorial  embedding  obtains  a  mean-average-precision  of  0.989  with  only  two  dimensions,  while  Nickel  et  al.'s  recent  construction  obtains  0.87  using  200  dimensions.  We  provide  upper  and  lower  bounds  that  allow  us  to  characterize  the  precision-dimensionality  tradeoff  inherent  in  any  hyperbolic  embedding.  To  embed  general  metric  spaces,  we  propose  a  hyperbolic  generalization  of  multidimensional  scaling  (h-MDS).  We  show  how  to  perform  exact  recovery  of  hyperbolic  points  from  distances,  provide  a  perturbation  analysis,  and  give  a  recovery  result  that  allows  us  to  reduce  dimensionality.  The  h-MDS  approach  offers  consistently  low  distortion  even  with  few  dimensions  across  several  datasets.  Finally,  we  extract  lessons  from  the  algorithms  and  theory  above  to  design  a  PyTorch-based  implementation  that  can  handle  incomplete  information  and  is  scalable.
0	Robust  and  scalable  models  of  microbiome  dynamics.  Microbes  are  everywhere,  including  in  and  on  our  bodies,  and  have  been  shown  to  play  key  roles  in  a  variety  of  prevalent  human  diseases.  Consequently,  there  has  been  intense  interest  in  the  design  of  bacteriotherapies  or  "bugs  as  drugs,"  which  are  communities  of  bacteria  administered  to  patients  for  specific  therapeutic  applications.  Central  to  the  design  of  such  therapeutics  is  an  understanding  of  the  causal  microbial  interaction  network  and  the  population  dynamics  of  the  organisms.  In  this  work  we  present  a  Bayesian  nonparametric  model  and  associated  efficient  inference  algorithm  that  addresses  the  key  conceptual  and  practical  challenges  of  learning  microbial  dynamics  from  time  series  microbe  abundance  data.  These  challenges  include  high-dimensional  (300+  strains  of  bacteria  in  the  gut)  but  temporally  sparse  and  non-uniformly  sampled  data;  high  measurement  noise;  and,  nonlinear  and  physically  non-negative  dynamics.  Our  contributions  include  a  new  type  of  dynamical  systems  model  for  microbial  dynamics  based  on  what  we  term  interaction  modules,  or  learned  clusters  of  latent  variables  with  redundant  interaction  structure  (reducing  the  expected  number  of  interaction  coefficients  from  $O(n^2)$  to  $O((\log  n)^2)$);  a  fully  Bayesian  formulation  of  the  stochastic  dynamical  systems  model  that  propagates  measurement  and  latent  state  uncertainty  throughout  the  model;  and  introduction  of  a  temporally  varying  auxiliary  variable  technique  to  enable  efficient  inference  by  relaxing  the  hard  non-negativity  constraint  on  states.  We  apply  our  method  to  simulated  and  real  data,  and  demonstrate  the  utility  of  our  technique  for  system  identification  from  limited  data  and  gaining  new  biological  insights  into  bacteriotherapy  design.
0	Coherent  probabilistic  forecasts  for  hierarchical  time  series.  Many  applications  require  forecasts  for  a  hierarchy  comprising  a  set  of  time  series  along  with  aggregates  of  subsets  of  these  series.  Although  forecasts  can  be  produced  independently  for  each  series  in  the  hierarchy,  typically  this  does  not  lead  to  coherent  forecasts  --  the  property  that  forecasts  add  up  appropriately  across  the  hierarchy.  State-of-the-art  hierarchical  forecasting  methods  usually  reconcile  the  independently  generated  forecasts  to  satisfy  the  aggregation  constraints.  A  fundamental  limitation  of  prior  research  is  that  it  has  considered  only  the  problem  of  forecasting  the  mean  of  each  time  series.  We  consider  the  situation  where  probabilistic  forecasts  are  needed  for  each  series  in  the  hierarchy.  We  define  forecast  coherency  in  this  setting,  and  propose  an  algorithm  to  compute  predictive  distributions  for  each  series  in  the  hierarchy.  Our  algorithm  has  the  advantage  of  synthesizing  information  from  different  levels  in  the  hierarchy  through  a  sparse  forecast  combination  and  a  probabilistic  hierarchical  aggregation.  We  evaluate  the  accuracy  of  our  forecasting  algorithm  on  both  simulated  data  and  large-scale  electricity  smart  meter  data.  The  results  show  consistent  performance  gains  compared  to  state-of-the  art  methods.
0	Efficientnet  rethinking  model  scaling  for  convolutional  neural  networks.  Convolutional  Neural  Networks  (ConvNets)  are  commonly  developed  at  a  fixed  resource  budget,  and  then  scaled  up  for  better  accuracy  if  more  resources  are  available.  In  this  paper,  we  systematically  study  model  scaling  and  identify  that  carefully  balancing  network  depth,  width,  and  resolution  can  lead  to  better  performance.  Based  on  this  observation,  we  propose  a  new  scaling  method  that  uniformly  scales  all  dimensions  of  depth/width/resolution  using  a  simple  yet  highly  effective  compound  coefficient.  We  demonstrate  the  effectiveness  of  this  method  on  scaling  up  MobileNets  and  ResNet.    To  go  even  further,  we  use  neural  architecture  search  to  design  a  new  baseline  network  and  scale  it  up  to  obtain  a  family  of  models,  called  EfficientNets,  which  achieve  much  better  accuracy  and  efficiency  than  previous  ConvNets.  In  particular,  our  EfficientNet-B7  achieves  state-of-the-art  84.3%  top-1  accuracy  on  ImageNet,  while  being  8.4x  smaller  and  6.1x  faster  on  inference  than  the  best  existing  ConvNet.  Our  EfficientNets  also  transfer  well  and  achieve  state-of-the-art  accuracy  on  CIFAR-100  (91.7%),  Flowers  (98.8%),  and  3  other  transfer  learning  datasets,  with  an  order  of  magnitude  fewer  parameters.  Source  code  is  at  this  https  URL.
0	Tesseract  tensorised  actors  for  multi  agent  reinforcement  learning.  Reinforcement  Learning  in  large  action  spaces  is  a  challenging  problem.  This  is  especially  true  for  cooperative  multi-agent  reinforcement  learning  (MARL),  which  often  requires  tractable  learning  while  respecting  various  constraints  like  communication  budget  and  information  about  other  agents.  In  this  work,  we  focus  on  the  fundamental  hurdle  affecting  both  value-based  and  policy-gradient  approaches:  an  exponential  blowup  of  the  action  space  with  the  number  of  agents.  For  value-based  methods,  it  poses  challenges  in  accurately  representing  the  optimal  value  function  for  value-based  methods,  thus  inducing  suboptimality.  For  policy  gradient  methods,  it  renders  the  critic  ineffective  and  exacerbates  the  problem  of  the  lagging  critic.  We  show  that  from  a  learning  theory  perspective,  both  problems  can  be  addressed  by  accurately  representing  the  associated  action-value  function  with  a  low-complexity  hypothesis  class.  This  requires  accurately  modelling  the  agent  interactions  in  a  sample  efficient  way.  To  this  end,  we  propose  a  novel  tensorised  formulation  of  the  Bellman  equation.  This  gives  rise  to  our  method  Tesseract,  which  utilises  the  view  of  Q-function  seen  as  a  tensor  where  the  modes  correspond  to  action  spaces  of  different  agents.  Algorithms  derived  from  Tesseract  decompose  the  Q-tensor  across  the  agents  and  utilise  low-rank  tensor  approximations  to  model  the  agent  interactions  relevant  to  the  task.  We  provide  PAC  analysis  for  Tesseract  based  algorithms  and  highlight  their  relevance  to  the  class  of  rich  observation  MDPs.  Empirical  results  in  different  domains  confirm  the  gains  in  sample  efficiency  using  Tesseract  as  supported  by  the  theory.
0	Unit  propagation  by  means  of  coordinate  wise  minimization.  We  present  a  novel  theoretical  result  concerning  the  applicability  of  coordinate-wise  minimization  on  the  dual  problem  of  linear  programming  (LP)  relaxation  of  weighted  partial  Max-SAT  that  shows  that  every  fixed  point  of  this  procedure  defines  a  feasible  primal  solution.  In  addition,  this  primal  solution  corresponds  to  the  result  of  a  certain  propagation  rule  applicable  to  weighted  Max-SAT.  Moreover,  we  analyze  the  particular  case  of  LP  relaxation  of  SAT  and  observe  that  coordinate-wise  minimization  on  the  dual  problem  resembles  unit  propagation  and  also  has  the  same  time  complexity  as  a  naive  unit  propagation  algorithm.  We  compare  our  theoretical  results  with  max-sum  diffusion  which  is  a  coordinate-wise  minimization  algorithm  that  is  used  to  optimize  the  dual  of  the  LP  relaxation  of  the  Max-Sum  problem  and  can  in  fact  perform  a  different  kind  of  constraint  propagation,  namely  deciding  whether  a  given  constraint  satisfaction  problem  (CSP)  has  non-empty  arc  consistency  closure.
0	A  free  energy  principle  for  representation  learning.  This  paper  employs  a  formal  connection  of  machine  learning  with  thermodynamics  to  characterize  the  quality  of  learnt  representations  for  transfer  learning.  We  discuss  how  information-theoretic  functional  such  as  rate,  distortion  and  classification  loss  of  a  model  lie  on  a  convex,  so-called  equilibrium  surface.We  prescribe  dynamical  processes  to  traverse  this  surface  under  constraints,  e.g.,  an  iso-classification  process  that  trades  off  rate  and  distortion  to  keep  the  classification  loss  unchanged.  We  demonstrate  how  this  process  can  be  used  for  transferring  representations  from  a  source  dataset  to  a  target  dataset  while  keeping  the  classification  loss  constant.  Experimental  validation  of  the  theoretical  results  is  provided  on  standard  image-classification  datasets.
0	Open  information  extraction  from  conjunctive  sentences.  We  develop  CALM,  a  coordination  analyzer  that  improves  upon  the  conjuncts  identified  from  dependency  parses.  It  uses  a  language  model  based  scoring  and  several  linguistic  constraints  to  search  over  hierarchical  conjunct  boundaries  (for  nested  coordination).  By  splitting  a  conjunctive  sentence  around  these  conjuncts,  CALM  outputs  several  simple  sentences.  We  demonstrate  the  value  of  our  coordination  analyzer  in  the  end  task  of  Open  Information  Extraction  (Open  IE).  State-of-the-art  Open  IE  systems  lose  substantial  yield  due  to  ineffective  processing  of  conjunctive  sentences.  Our  Open  IE  system,  CALMIE,  performs  extraction  over  the  simple  sentences  identified  by  CALM  to  obtain  up  to  1.8x  yield  with  a  moderate  increase  in  precision  compared  to  extractions  from  original  sentences.
0	Lstms  with  attention  for  aggression  detection.  In  this  paper,  we  describe  the  system  submitted  for  the  shared  task  on  Aggression  Identification  in  Facebook  posts  and  comments  by  the  team  Nishnik.  Previous  works  demonstrate  that  LSTMs  have  achieved  remarkable  performance  in  natural  language  processing  tasks.  We  deploy  an  LSTM  model  with  an  attention  unit  over  it.  Our  system  ranks  6th  and  4th  in  the  Hindi  subtask  for  Facebook  comments  and  subtask  for  generalized  social  media  data  respectively.  And  it  ranks  17th  and  10th  in  the  corresponding  English  subtasks.
0	Authorship  identification  for  literary  book  recommendations.  Book  recommender  systems  can  help  promote  the  practice  of  reading  for  pleasure,  which  has  been  declining  in  recent  years.  One  factor  that  influences  reading  preferences  is  writing  style.  We  propose  a  system  that  recommends  books  after  learning  their  authors’  style.  To  our  knowledge,  this  is  the  first  work  that  applies  the  information  learned  by  an  author-identification  model  to  book  recommendations.  We  evaluated  the  system  according  to  a  top-k  recommendation  scenario.  Our  system  gives  better  accuracy  when  compared  with  many  state-of-the-art  methods.  We  also  conducted  a  qualitative  analysis  by  checking  if  similar  books/authors  were  annotated  similarly  by  experts.
0	Transition  based  neural  rst  parsing  with  implicit  syntax  features.  Syntax  has  been  a  useful  source  of  information  for  statistical  RST  discourse  parsing.  Under  the  neural  setting,  a  common  approach  integrates  syntax  by  a  recursive  neural  network  (RNN),  requiring  discrete  output  trees  produced  by  a  supervised  syntax  parser.  In  this  paper,  we  propose  an  implicit  syntax  feature  extraction  approach,  using  hidden-layer  vectors  extracted  from  a  neural  syntax  parser.  In  addition,  we  propose  a  simple  transition-based  model  as  the  baseline,  further  enhancing  it  with  dynamic  oracle.  Experiments  on  the  standard  dataset  show  that  our  baseline  model  with  dynamic  oracle  is  highly  competitive.  When  implicit  syntax  features  are  integrated,  we  are  able  to  obtain  further  improvements,  better  than  using  explicit  Tree-RNN.
0	Generating  plausible  counterfactual  explanations  for  deep  transformers  in  financial  text  classification.  Corporate  mergers  and  acquisitions  (M&A)  account  for  billions  of  dollars  of  investment  globally  every  year  and  offer  an  interesting  and  challenging  domain  for  artificial  intelligence.  However,  in  these  highly  sensitive  domains,  it  is  crucial  to  not  only  have  a  highly  robust/accurate  model,  but  be  able  to  generate  useful  explanations  to  garner  a  user’s  trust  in  the  automated  system.  Regrettably,  the  recent  research  regarding  eXplainable  AI  (XAI)  in  financial  text  classification  has  received  little  to  no  attention,  and  many  current  methods  for  generating  textual-based  explanations  result  in  highly  implausible  explanations,  which  damage  a  user’s  trust  in  the  system.  To  address  these  issues,  this  paper  proposes  a  novel  methodology  for  producing  plausible  counterfactual  explanations,  whilst  exploring  the  regularization  benefits  of  adversarial  training  on  language  models  in  the  domain  of  FinTech.  Exhaustive  quantitative  experiments  demonstrate  that  not  only  does  this  approach  improve  the  model  accuracy  when  compared  to  the  current  state-of-the-art  and  human  performance,  but  it  also  generates  counterfactual  explanations  which  are  significantly  more  plausible  based  on  human  trials.
0	What  good  are  nominalkomposita  for  noun  compounds  multilingual  extraction  and  structure  analysis  of  nominal  compositions  using  linguistic  restrictors.  Finding  a  definition  of  compoundhood  that  is  cross-lingually  valid  is  a  non-trivial  task  as  shown  by  linguistic  literature.  We  present  an  iterative  method  for  defining  and  extracting  English  noun  compounds  in  a  multilingual  setting.  We  show  how  linguistic  criteria  can  be  used  to  extract  compounds  automatically  and  vice  versa  how  the  results  of  this  extraction  can  shed  new  lights  on  linguistic  theories  about  compounding.  The  extracted  compound  nouns  and  their  multilingual  contexts  are  a  rich  source  that  serves  several  purposes.  In  an  additional  case  study  we  show  how  the  database  serves  to  predict  the  internal  structure  of  tripartite  noun  compounds  using  spelling  variations  across  languages,  which  leads  to  a  precision  of  over  91%.
0	Using  neural  transfer  learning  for  morpho  syntactic  tagging  of  south  slavic  languages  tweets.  In  this  paper,  we  describe  a  morpho-syntactic  tagger  of  tweets,  an  important  component  of  the  CEA  List  DeepLIMA  tool  which  is  a  multilingual  text  analysis  platform  based  on  deep  learning.  This  tagger  is  built  for  the  Morpho-syntactic  Tagging  of  Tweets  (MTT)  Shared  task  of  the  2018  VarDial  Evaluation  Campaign.  The  MTT  task  focuses  on  morpho-syntactic  annotation  of  non-canonical  Twitter  varieties  of  three  South-Slavic  languages:  Slovene,  Croatian  and  Serbian.  We  propose  to  use  a  neural  network  model  trained  in  an  end-to-end  manner  for  the  three  languages  without  any  need  for  task  or  domain  specific  features  engineering.  The  proposed  approach  combines  both  character  and  word  level  representations.  Considering  the  lack  of  annotated  data  in  the  social  media  domain  for  South-Slavic  languages,  we  have  also  implemented  a  cross-domain  Transfer  Learning  (TL)  approach  to  exploit  any  available  related  out-of-domain  annotated  data.
0	Towards  a  friendly  online  community  an  unsupervised  style  transfer  framework  for  profanity  redaction.  Offensive  and  abusive  language  is  a  pressing  problem  on  social  media  platforms.  In  this  work,  we  propose  a  method  for  transforming  offensive  comments,  statements  containing  profanity  or  offensive  language,  into  non-offensive  ones.  We  design  a  Retrieve,  Generate  and  Edit  unsupervised  style  transfer  pipeline  to  redact  the  offensive  comments  in  a  word-restricted  manner  while  maintaining  a  high  level  of  fluency  and  preserving  the  content  of  the  original  text.  We  extensively  evaluate  our  method’s  performance  and  compare  it  to  previous  style  transfer  models  using  both  automatic  metrics  and  human  evaluations.  Experimental  results  show  that  our  method  outperforms  other  models  on  human  evaluations  and  is  the  only  approach  that  consistently  performs  well  on  all  automatic  evaluation  metrics.
0	Metric  learning  for  graph  based  domain  adaptation.  In  many  domain  adaption  formulations,  it  is  assumed  to  have  large  amount  of  unlabeled  data  from  the  domain  of  interest  (target  domain),  some  portion  of  it  may  be  labeled,  and  large  amount  of  labeled  data  from  other  domains,  also  known  as  source  domain(s).  Motivated  by  the  fact  that  labeled  data  is  hard  to  obtain  in  any  domain,  we  design  algorithms  for  the  settings  in  which  there  exists  large  amount  of  unlabeled  data  from  all  domains,  small  portion  of  which  may  be  labeled.  We  build  on  recent  advances  in  graph-based  semi-supervised  learning  and  supervised  metric  learning.  Given  all  instances,  labeled  and  unlabeled,  from  all  domains,  we  build  a  large  similarity  graph  between  them,  where  an  edge  exists  between  two  instances  if  they  are  close  according  to  some  metric.  Instead  of  using  predefined  metric,  as  commonly  performed,  we  feed  the  labeled  instances  into  metric-learning  algorithms  and  (re)construct  a  data-dependent  metric,  which  is  used  to  construct  the  graph.  We  employ  different  types  of  edges  depending  on  the  domain-identity  of  the  two  vertices  touching  it,  and  learn  the  weights  of  each  edge.  Experimental  results  show  that  our  approach  leads  to  significant  reduction  in  classification  error  across  domains,  and  performs  better  than  two  state-of-the-art  models  on  the  task  of  sentiment  classification.
0	Uwb  at  semeval  2020  task  1  lexical  semantic  change  detection.  In  this  paper,  we  describe  our  method  for  detection  of  lexical  semantic  change,  i.e.,  word  sense  changes  over  time.  We  examine  semantic  differences  between  specific  words  in  two  corpora,  chosen  from  different  time  periods,  for  English,  German,  Latin,  and  Swedish.  Our  method  was  created  for  the  SemEval  2020  Task  1:  Unsupervised  Lexical  Semantic  Change  Detection.  We  ranked  1st  in  Sub-task  1:  binary  change  detection,  and  4th  in  Sub-task  2:  ranked  change  detection.  We  present  our  method  which  is  completely  unsupervised  and  language  independent.  It  consists  of  preparing  a  semantic  vector  space  for  each  corpus,  earlier  and  later;  computing  a  linear  transformation  between  earlier  and  later  spaces,  using  Canonical  Correlation  Analysis  and  orthogonal  transformation;and  measuring  the  cosines  between  the  transformed  vector  for  the  target  word  from  the  earlier  corpus  and  the  vector  for  the  target  word  in  the  later  corpus.
0	A  comprehensive  evaluation  of  incremental  speech  recognition  and  diarization  for  conversational  ai.  Automatic  Speech  Recognition  (ASR)  systems  are  increasingly  powerful  and  more  accurate,  but  also  more  numerous  with  several  options  existing  currently  as  a  service  (e.g.  Google,  IBM,  and  Microsoft).  Currently  the  most  stringent  standards  for  such  systems  are  set  within  the  context  of  their  use  in,  and  for,  Conversational  AI  technology.  These  systems  are  expected  to  operate  incrementally  in  real-time,  be  responsive,  stable,  and  robust  to  the  pervasive  yet  peculiar  characteristics  of  conversational  speech  such  as  disfluencies  and  overlaps.  In  this  paper  we  evaluate  the  most  popular  of  such  systems  with  metrics  and  experiments  designed  with  these  standards  in  mind.  We  also  evaluate  the  speaker  diarization  (SD)  capabilities  of  the  same  systems  which  will  be  particularly  important  for  dialogue  systems  designed  to  handle  multi-party  interaction.  We  found  that  Microsoft  has  the  leading  incremental  ASR  system  which  preserves  disfluent  materials  and  IBM  has  the  leading  incremental  SD  system  in  addition  to  the  ASR  that  is  most  robust  to  speech  overlaps.  Google  strikes  a  balance  between  the  two  but  none  of  these  systems  are  yet  suitable  to  reliably  handle  natural  spontaneous  conversations  in  real-time.
0	Intent  mining  from  past  conversations  for  conversational  agent.  Conversational  systems  are  of  primary  interest  in  the  AI  community.  Organizations  are  increasingly  using  chatbot  to  provide  round-the-clock  support  and  to  increase  customer  engagement.  Many  commercial  bot  building  frameworks  follow  a  standard  approach  that  requires  one  to  build  and  train  an  intent  model  to  recognize  user  input.  These  frameworks  require  a  collection  of  user  utterances  and  corresponding  intent  to  train  an  intent  model.  Collecting  a  substantial  coverage  of  training  data  is  a  bottleneck  in  the  bot  building  process.  In  cases  where  past  conversation  data  is  available,  the  cost  of  labeling  hundreds  of  utterances  with  intent  labels  is  time-consuming  and  laborious.  In  this  paper,  we  present  an  intent  discovery  framework  that  can  mine  a  vast  amount  of  conversational  logs  and  to  generate  labeled  data  sets  for  training  intent  models.  We  have  introduced  an  extension  to  the  DBSCAN  algorithm  and  presented  a  density-based  clustering  algorithm  ITER-DBSCAN  for  unbalanced  data  clustering.  Empirical  evaluation  on  one  conversation  dataset,  six  different  intent  dataset,  and  one  short  text  clustering  dataset  show  the  effectiveness  of  our  hypothesis.
0	A  summary  of  the  coliee  2019  competition.  We  summarize  the  evaluation  of  the  6th  Competition  on  Legal  Information  Extraction/Entailment  (COLIEE  2019).  The  competition  consists  of  four  tasks:  two  on  case  law  and  two  on  statute  law.  The  case  law  component  includes  an  information  retrieval  task  (Task  1),  and  the  confirmation  of  an  entailment  relation  between  an  existing  case  and  an  unseen  case  (Task  2).  The  statute  law  component  also  includes  an  information  retrieval  task  (Task  3)  and  an  entailment/question  answering  task  (Task  4),  which  attempts  to  confirm  whether  a  particular  statute  applies  to  a  yes/no  question.  Participation  was  open  to  any  group  in  the  world,  based  on  any  approach.  Eleven  different  teams  participated  in  the  case  law  competition  tasks,  some  of  them  in  more  than  one  task.  We  received  results  from  7  teams  for  Task  1  (15  runs)  and  7  teams  for  Task  2  (18  runs).  For  the  statute  law  tasks,  8  different  teams  participated,  some  in  more  than  one  task.  Seven  teams  submitted  a  total  of  13  runs  for  Task  3,  and  7  teams  submitted  a  total  of  15  runs  for  Task  4.  Here  we  summarize  each  team’s  approaches,  our  official  evaluation,  and  some  analysis  of  the  variety  of  methods  that  produced  the  evaluation  results.
0	Deep  learning  in  visual  computing  and  signal  processing.  Deep  learning  is  a  subfield  of  machine  learning,  which  aims  to  learn  a  hierarchy  of  features  from  input  data.  Nowadays,  researchers  have  intensively  investigated  deep  learning  algorithms  for  solving  challenging  problems  in  many  areas  such  as  image  classification,  speech  recognition,  signal  processing,  and  natural  language  processing.  In  this  study,  we  not  only  review  typical  deep  learning  algorithms  in  computer  vision  and  signal  processing  but  also  provide  detailed  information  on  how  to  apply  deep  learning  to  specific  areas  such  as  road  crack  detection,  fault  diagnosis,  and  human  activity  detection.  Besides,  this  study  also  discusses  the  challenges  of  designing  and  training  deep  neural  networks.
0	A  new  efficient  method  using  fibonacci  polynomials  for  solving  of  first  order  fuzzy  fredholm  volterra  integro  differential  equations.  In  this  paper,  a  new  numerical  approach  based  on  Fibonacci  polynomials  is  proposed  for  solving  first-order  fuzzy  Fredholm–Volterra  integro-differential  equations  of  the  second  kind.  The  existence,  uniqueness  and  convergence  of  the  solution  under  generalized  differentiability  are  proved.  Because  of  reducing  the  problem  into  a  system  of  linear  algebraic  equations,  this  simple  method  is  computationally  attractive.  The  reliability  and  accuracy  of  the  presented  scheme  are  demonstrated  by  several  numerical  experiments.
0	A  hybrid  technique  for  simultaneous  network  reconfiguration  and  optimal  placement  of  distributed  generation  resources.  A  new  meta-heuristic  method,  comprehensive  teaching  learning  harmony  search  optimization  algorithm  (CTLHSO),  is  developed  in  this  paper  for  the  simultaneous  reconfiguration  and  optimal  allocation  of  distributed  generation  resources  in  radial  distribution  systems.  The  proposed  method  is  a  hybridization  of  the  teaching–learning-based  optimization  (TLBO)  and  the  harmony  search  (HS)  algorithms.  Primarily,  eleven  mathematical  benchmark  functions  are  used  to  test  the  performance  of  the  CTLHSO  algorithm.  The  results  are  then  compared  with  that  of  global  best  artificial  bee  colony  (G-ABC),  particle  swarm  artificial  bee  colony  (PS-ABC),  TLBO  and  improved  TLBO  (I-TLBO)  with  identical  parameters  and  initial  conditions.  The  results  show  that  the  CTLHSO  performance  is  better  than  the  G-ABC,  PS-ABC,  TLBO  and  I-TLBO.  Subsequently,  CTLHSO  is  implemented  on  the  IEEE  33-bus  and  69-bus  radial  distribution  systems  for  network  reconfiguration  and  optimal  placement  of  distributed  generation  resources  to  minimize  the  power  losses  and  improve  the  voltage  profiles.  Five  case  studies  at  three  different  load  levels  are  carried  out.  The  results  obtained  are  found  to  be  better  than  those  obtained  with  HS  algorithm,  genetic  algorithm,  refined  genetic  algorithm  and  fireworks  algorithm.
0	Uncertain  aggregate  production  planning.  Based  on  uncertainty  theory,  multiproduct  aggregate  production  planning  model  is  presented,  where  the  market  demand,  production  cost,  subcontracting  cost,  etc.,  are  all  characterized  as  uncertain  variables.  The  objective  is  to  maximize  the  belief  degree  of  obtaining  the  profit  more  than  the  predetermined  profit  over  the  whole  planning  horizon.  When  these  uncertain  variables  are  linear,  the  objective  function  and  constraints  can  be  converted  into  crisp  equivalents,  the  model  is  a  nonlinear  programming,  then  can  be  solved  by  traditional  methods.  An  example  is  given  to  illustrate  the  model  and  the  converting  method.
0	A  solution  to  the  double  dummy  contract  bridge  problem  influenced  by  supervised  learning  module  adapted  by  artificial  neural  network.  Contract  Bridge  is  an  intellectual  game  which  motivates  multiple  skills  and  application  of  prior  experience  and  knowledge,  as  no  player  knows  accurately  what  moves  other  players  are  capable  of  making.  The  Bridge  is  a  game  played  in  the  presence  of  imperfect  information,  yet  its  strategies  must  be  well  formulated,  since  the  outcome  at  any  intermediate  stage  is  solely  based  on  the  choices  made  during  the  immediately  preceding  phase.  In  this  paper,  we  train  an  Artificial  Neural  Network  architecture  using  sample  deals  and  use  it  to  estimate  the  number  of  tricks  to  be  taken  by  one  pair  of  bridge  players,  which  is  the  main  challenge  in  the  Double  Dummy  Bridge  Problem.  We  focus  on  Back  Propagation  Neural  Network  Architecture  with  Back  Propagation  Algorithm  with  Sigmoidal  transfer  functions.  We  used  two  approaches  namely,  High  -  Card  Point  Count  System  and  Distribution  Point  Method  during  the  bidding  phase  of  Contract  Bridge.  We  experimented  with  two  sigmoidal  transfer  functions  namely,  Log  Sigmoid  transfer  function  and  the  Hyperbolic  Tangent  Sigmoid  function.  Results  reveal  that  the  later  performs  better  giving  lower  mean  squared  error  on  the  output.
0	Adaptive  clustering  method  in  intelligent  automated  decision  support  systems.  The  method  of  adaptive  clustering  used  in  intelligent  automation  systems  for  the  information  analysis  and  decision-making  is  considered.  Experiment  according  to  function  performance  estimation  in  the  database  management  system  is  made.  It's  allowed  to  form  recommendations  for  improving  the  single  information  space  components  in  the  field  of  electrical  power  equipment  maintenance.
0	Gene  clustering  for  time  series  microarray  with  production  outputs.  The  identification  of  coexpressed  genes  from  microarray  data  is  a  challenging  problem  in  bioinformatics  and  computational  biology.  The  objective  of  this  study  is  to  obtain  knowledge  about  the  most  important  genes  and  clusters  related  to  production  outputs  of  real-world  time-series  microarray  data  in  the  industrial  microbiology  area.  Each  sample  in  the  microarray  data  experiment  is  complemented  with  the  measurement  of  the  corresponding  production  and  growth  values.  A  novel  aspect  of  this  research  refers  to  considering  the  relation  of  coexpression  patterns  with  the  measured  outputs  to  guide  the  biological  interpretation  of  results.  Shape-based  clustering  models  are  developed  using  the  pattern  of  gene  expression  values  over  time  and  further  incorporating  knowledge  about  the  correlation  between  the  change  in  the  gene  expression  level  and  the  output  value.  Experiments  are  performed  for  time-series  microarray  of  bacteria,  and  an  analysis  from  a  biological  perspective  is  carried  out.  The  obtained  results  confirm  the  existence  of  relationships  between  output  variables  and  gene  expressions.  Moreover,  the  shape-based  clustering  methods  show  promising  results,  being  able  to  guide  metabolic  engineering  actions  with  the  identification  of  potential  targets.
0	Modeling  and  implementation  of  z  number.  Computing  with  words  provides  symbolic  and  semantic  methodology  to  deal  with  imprecise  information  associated  with  natural  languages.  It  encapsulates  various  fuzzy  logic  techniques  developed  in  past  decades  and  formalizes  them.  Z-number  is  an  emerging  paradigm  that  has  been  utilized  in  computing  with  words  among  others.  The  concept  of  a  Z-number  is  intended  to  provide  a  basis  for  computation  with  numbers,  specifically  with  reliability  of  information.  Z-numbers  are  in  confluence  between  the  two  most  prominent  approaches  to  uncertainty,  probability  and  possibility,  that  allow  computations  on  complex  statements.  Certain  computations  related  to  Z-numbers  are  ambiguous  and  complicated  leading  to  its  slow  adaptation  into  areas  such  as  computing  with  words.  The  biggest  contributing  factor  to  the  complexity  is  the  usage  of  probability  distributions  in  the  computations.  This  paper  seeks  to  provide  an  applied  model  of  Z-number  based  on  certain  realistic  assumptions  regarding  probability  distributions.  Algorithms  are  presented  to  implement  this  model  and  integrate  it  into  an  expert  system  shell  for  computing  with  words  called  CWShell.  CWShell  is  a  software  tool  that  abstracts  the  underlying  computation  required  for  computing  with  words,  and  provides  a  convenient  way  to  represent  and  compute  with  unstructured  natural  language  using  specialized  language  called  Generalized  constraint  language  (GCL).  This  paper  introduces  new  constructs  for  Z-numbers  to  GCL  and  provides  detailed  inference  mechanism  and  computation  strategy  on  those  constructs.  We  present  two  case  studies  to  demonstrate  the  working  and  feasibility  of  the  approach.
0	Biometric  recognition  using  finger  and  palm  vein  images.  In  recent  times,  biometrics  is  the  best  alternative  for  the  token-based  and  knowledge-based  security  systems.  Out  of  the  existing  biometric  modalities,  the  vascular  biometric  modalities  are  preferred  for  authenticating  the  person,  because  of  its  uniqueness  among  all  individuals.  This  paper  proposes  a  multimodal  biometric  system  using  vascular  patterns  of  the  hand  such  as  finger  vein  and  palm  vein  images.  Initially,  the  input  palm  vein  and  finger  vein  images  are  pre-processed  so  as  to  make  them  suitable  for  further  processing.  Subsequently,  the  features  from  palm  and  finger  vein  images  are  extracted  using  a  modified  two-dimensional  Gabor  filter  and  a  gradient-based  techniques.  These  extracted  features  are  matched  using  the  Euclidean  distance  metric,  and  they  are  fused  at  the  score  level  using  fuzzy  logic  technique.  The  proposed  technique  is  tested  on  the  standard  databases  of  finger  vein  and  palm  vein  images.  This  method  provides  lower  false  acceptance  rate,  false  rejection  rate  and  high  accuracy  of  99.5%  when  compared  with  the  existing  techniques,  indicating  the  effectiveness  of  the  proposed  system.
0	Relationships  between  generalized  bosbach  states  and  l  filters  on  residuated  lattices.  Generalized  Bosbach  states  and  filters  on  residuated  lattices  have  been  extensively  studied  in  the  literature.  In  this  paper,  relationships  between  generalized  Bosbach  states  and  residuated-lattice-valued  filters,  also  called  L-filters,  on  residuated  lattices  are  investigated.  Particularly,  type  I  and  type  II  L-filters  and  their  subclasses  are  defined,  and  some  their  properties  are  obtained.  Then  relationships  between  special  types  of  L-filters  and  the  generalized  Bosbach  states  are  considered  where  generalized  Bosbach  states  are  characterized  by  some  type  I  or  type  II  L-filters  with  additional  conditions.  Associated  with  these  relationships,  new  subclasses  of  generalized  Bosbach  states  such  as  implicative  type  IV,  V,  VI  states,  fantastic  type  IV  states  and  Boolean  type  IV  states  are  introduced,  and  the  relationships  between  various  types  of  generalized  Bosbach  states  are  investigated  in  detail.  In  particular,  the  existence  of  several  generalized  Bosbach  states  is  provided  and,  as  application,  some  typical  subclasses  of  residuated  lattices  such  as  Rl-monoids,  Heyting  algebras  and  Boolean  algebras  are  characterized  by  these  generalized  Bosbach  states.
0	A  bibliometric  analysis  of  aggregation  operators.  Abstract  Aggregation  operators  consist  of  mathematical  functions  that  enable  the  combining  and  processing  of  different  types  of  information.  The  aim  of  this  work  is  to  present  the  main  contributions  in  this  field  by  a  bibliometric  review  approach.  The  paper  employs  an  extensive  range  of  bibliometric  indicators  using  the  Web  of  Science  (WoS)  Core  Collection  and  Scopus  datasets.  The  work  considers  leading  journals,  articles,  authors,  institutions  countries  and  patterns.  This  paper  highlights  that  Xu  is  the  most  productive  author  and  Yager  is  the  most  influential  author  in  the  field.  Likewise,  China  is  leading  the  field  with  many  new  researchers  who  have  entered  the  field  in  recent  years.  This  discipline  has  been  strengthening  to  create  a  unique  theory  and  will  continue  to  expand  with  many  new  theoretical  developments  and  applications.
0	Amniotic  fluid  segmentation  by  pixel  classification  in  b  mode  ultrasound  image  for  computer  assisted  diagnosis.  B-mode  ultrasound  imaging  segmentation  is  facing  a  challenge  in  the  artifacts  such  as  speckle  noise,  blurry  edges,  low  contrast,  and  unexpected  shadow.  This  study  proposed  a  model  segmentation  considering  the  local  information  from  each  pixel  based  upon  its  neighborhood  information.  The  features  used  are  a  statistical  texture  (mean  intensity,  deviation  standard,  skewness,  entropy,  and  property)  taken  based  upon  the  3  ×  3  and  5  ×  5  window.  Random  forest  was  used  to  classify  each  pixel  into  three  regions:  the  amniotic  fluid,  uterus,  and  fetal  body.  An  evaluation  was  carried  out  by  calculating  the  comparison  between  the  ground  truth  area  and  the  segmentation  results  of  the  proposed  model.  The  experimental  results  showed  that  the  proposed  model  has  an  average  accuracy  of  81.45%  in  the  3  ×  3  window  and  85.86%  in  the  5  ×  5  window  on  50  tested  images.
0	Developed  multi  objective  grey  wolf  optimizer  with  fuzzy  logic  decision  making  tool  for  direction  overcurrent  relays  coordination.  This  paper  proposes  a  new  methodology  for  solving  the  coordination  problem  of  DOCRs  based  on  multi-objective  grey  wolf  optimizer  and  fuzzy  logic  decision-making.  In  addition  to  the  conventional  objective  function,  a  new  objective  function  which  aims  to  minimize  the  discrimination  time  between  primary  and  backup  relays  is  proposed.  Moreover,  the  conventional  objective  function  related  to  minimizing  the  total  operating  time  of  primary  and  backup  relays  is  considered.  The  feasibility  and  performance  of  the  proposed  methodology  for  solving  the  coordination  problem  of  DOCRs  are  investigated  using  two  different  systems  (8-bus  system  and  IEEE-30  bus  system).  The  proposed  methodology  is  compared  with  other  reported  methods.  The  results  prove  the  viability  and  effectiveness  of  the  proposed  methodology  to  solve  the  DOCR  coordination  problem  without  any  miscoordination  between  primary  and  backup  relays.
0	Rough  fuzzy  bipolar  soft  sets  and  application  in  decision  making  problems.  The  rough  set  theory  is  a  successful  tool  to  study  the  vagueness  in  data,  while  the  fuzzy  bipolar  soft  sets  have  ability  to  handle  the  uncertainty,  as  well  as  bipolarity  of  the  information  in  many  situations.  We  connect  the  Pawlak’s  rough  sets  with  the  fuzzy  bipolar  soft  sets  and  introduce  the  concept  of  rough  fuzzy  bipolar  soft  sets.  We  also  examine  some  structural  properties  of  rough  fuzzy  bipolar  soft  sets  and  study  the  effects  of  the  equivalence  relation  in  Pawlak  approximation  space  on  the  roughness  of  the  fuzzy  bipolar  soft  sets.  We  also  discuss  some  similarity  relations  among  the  fuzzy  bipolar  soft  sets,  based  on  their  roughness.  At  the  end,  an  application  of  the  rough  fuzzy  bipolar  soft  sets  in  a  decision-making  problem  is  discussed  and  an  algorithm  for  this  application  is  proposed,  supported  by  an  example.
0	Patch  based  fuzzy  clustering  for  image  segmentation.  Fuzzy  C-means  has  been  adopted  for  image  segmentation,  but  it  is  sensitive  to  noise  and  other  image  artifacts  due  to  not  considering  neighbor  information.  In  order  to  solve  this  problem,  many  improved  algorithms  have  been  proposed,  such  as  fuzzy  local  information  C-means  clustering  algorithm  (FLICM).  However,  the  segmentation  results  of  FLICM  are  unsatisfactory  when  performed  on  complex  images.  To  overcome  this,  a  novel  fuzzy  clustering  algorithm  is  proposed  in  this  paper,  and  more  information  is  utilized  to  guide  the  procedure  of  image  segmentation.  In  the  proposed  algorithm,  pixel  relevance  based  on  patch  similarity  will  be  investigated  firstly,  by  which  all  information  over  the  whole  image  can  be  considered,  not  limited  to  local  context.  Compared  with  Zhang  et  al.  (Multimed  Tools  Appl  76(6):7869–7895,  2017a.  https://doi.org/10.1007/s11042-016-3399-x    )  pixel  relevance  is  unnecessary  to  be  normalized,  and  much  more  information  can  play  positive  role  in  image  segmentation.  Experimental  results  show  that  the  proposed  algorithm  outperforms  current  fuzzy  algorithms,  especially  in  enhancing  the  robustness  of  corresponding  fuzzy  clustering  algorithms.
0	Robust  predictive  synchronization  of  uncertain  fractional  order  time  delayed  chaotic  systems.  In  this  paper,  a  novel  robust  predictive  control  strategy  is  proposed  for  the  synchronization  of  fractional-order  time-delay  chaotic  systems.  A  recurrent  non-singleton  type-2  fuzzy  neural  network  (RNT2FNN)  is  used  for  the  estimation  of  the  unknown  functions.  Additionally,  another  RNT2FNN  is  used  for  the  modeling  of  the  tracking  error.  A  nonlinear  model-based  predictive  controller  is  then  designed  based  on  the  proposed  fuzzy  model.  The  asymptotic  stability  of  the  approach  is  derived  based  on  the  Lyapunov  stability  theorem.  A  number  of  simulation  examples  are  presented  to  verify  the  effectiveness  of  the  proposed  control  method  for  the  synchronization  of  two  uncertain  fractional-order  time-delay  identical  and  nonidentical  chaotic  systems.  The  proposed  control  strategy  is  also  employed  for  high-performance  position  control  of  a  hydraulic  actuator.  In  this  example,  the  nonlinear  mechanical  model  of  the  hydraulic  actuator,  instead  of  a  mathematical  model,  is  simulated.  The  example  demonstrates  that  the  proposed  control  strategy  can  be  applied  to  a  wide  class  of  nonlinear  systems.
0	The  fuzzy  economic  order  quantity  problem  with  a  finite  production  rate  and  backorders.  The  track  of  developing  Economic  Order  Quantity  (EOQ)  models  with  uncertainties  described  as  fuzzy  numbers  has  been  very  lucrative.  In  this  paper,  a  fuzzy  Economic  Production  Quantity  (EPQ)  model  is  developed  to  address  a  specific  problem  in  a  theoretical  setting.  Not  only  is  the  production  time  finite,  but  also  backorders  are  allowed.  The  uncertainties,  in  the  industrial  context,  come  from  the  fact  that  the  production  availability  is  uncertain  as  well  as  the  demand.  These  uncertainties  will  be  handled  with  fuzzy  numbers  and  the  analytical  solution  to  the  optimization  problem  will  be  obtained.  A  theoretical  example  from  the  process  industry  is  also  given  to  illustrate  the  new  model.
0	Palm  vein  recognition  based  on  2d  discrete  wavelet  transform  and  linear  discrimination  analysis.  Palm  Vein  Recognition  (PVR)  is  a  promising  new  biometric  that  has  been  applied  successfully  as  a  method  of  access  control  by  many  organizations  that  has  even  further  potential  in  the  field  of  forensics.  The  palm  vein  pattern  has  highly  discriminative  features  that  are  difficult  to  forge  because  of  its  subcutaneous  position  in  the  palm.  Despite  considerable  progress  and  a  few  practical  issues,  providing  accurate  palm  vein  readings  has  remained  an  unsolved  issue  in  the  biometrics.  In  this  paper,  we  propose  an  improved  scheme  of  palm  vein  recognition  method  based  on  the  Two  Dimensional  Discrete  Wavelet  Transform  (DWT)  and  Linear  Discrimination  Analysis  (LDA).  The  palm  vein  image  is  first  subjected  to  2D-  DWT  decomposition.  Then  the  low  frequency  sub-bands  approximate  of  the  image  is  used  as  an  input  for  LDA  algorithm.  LDA  is  a  subspace  projection  method  that  aims  to  maximize  between  class  covariance  and  minimize  the  within  class  covariance.  It  is  used  initially  to  reduce  the  features,  and  later  followed  by  the  matching  procedure  using  cosine  distance  nearest  neighbor.  Based  on  our  experiments,  the  method  has  produced  an  identification  rate  of  99.74%  and  100%  of  verification  rate  and  0.0%  of  Equal  Error  Rate  (EER)  on  380  different  palms  from  the  hyperspectral  PolyU  database.  The  total  images  of  3800  were  captured  with  the  wavelength  of  850nm  and  the  performance  of  the  proposed  method  was  better  compared  to  images  extracted  using  the  Gabor  filter  method.
0	Outranking  approach  for  multi  criteria  decision  making  problems  with  hesitant  interval  valued  fuzzy  sets.  For  decision  makers,  expressing  their  opinions  through  subintervals  of  [0,  1]  is  sometimes  easier  than  using  crisp  numbers.  This  study  defines  some  outranking  relations  derived  by  ELECTRE  III  for  hesitant  interval-valued  fuzzy  sets  (HIVFSs).  The  properties  of  these  outranking  relations  are  discussed  in  detail.  The  concordance  and  discordance  indexes  of  HIVFSs  are  then  proposed.  Ranking  approach  is  developed  based  on  the  outranking  relations  of  HIVFSs  to  handle  multi-criteria  decision-making  problems.  Finally,  we  provide  practical  examples,  as  well  as  sensitivity  analysis  and  comparison  with  other  techniques.
0	Apply  the  quantum  particle  swarm  optimization  for  the  k  traveling  repairman  problem.  This  paper  deals  with  an  optimization  problem  encountered  in  the  field  of  transport  of  goods  and  services,  namely  the  K-traveling  repairman  problem  (K-TRP).  This  problem  is  a  generalization  of  the  metric  traveling  repairman  problem  (TRP)  which  is  also  known  as  the  deliveryman  problem  and  the  minimum  latency  problem.  The  K-TRP  and  the  related  problems  can  be  considered  as  “customer-centric”  routing  problems  because  the  objectif  function  consists  in  minimize  the  sum  of  the  waiting  times  of  customers  rather  than  the  vehicles  travel  time.  These  problems  are  also  considered  as  problems  with  “cumulative  costs.”  In  this  paper,  we  propose  a  quantum  particle  swarm  optimization  (QPSO)  method  to  solve  the  K-TRP.  In  order  to  avoid  the  violations  of  problem  constraints,  the  proposed  approach  also  incorporates  a  heuristic  repair  operator  that  uses  problem-specific  knowledge  instead  of  the  penalty  function  technique  commonly  used  for  constrained  problem.  To  the  best  of  our  knowledge,  this  study  is  the  first  to  report  on  the  application  of  the  QPSO  method  to  the  K-TRP.  Experimental  results  obtained  on  sets  of  the  Capacitated  Vehicle  Routing  Problem  test  instances,  of  up  to  100  customers,  available  in  the  literature  clearly  demonstrate  the  competitiveness  of  the  proposed  method  compared  to  the  commercial  MIP  solver  CPLEX  12.5  of  IBM-ILOG  and  the  state-of-the-art  heuristic  methods.  The  results  also  demonstrate  that  the  proposed  approach  was  able  to  reach  more  optimal  solutions  and  to  improve  5  best  known  solutions  in  a  short  and  reasonable  computation  time.
0	Ranking  interval  type  2  fuzzy  number  based  on  a  novel  value  ambiguity  ranking  index  and  its  application  in  risk  analysis.  Consequence  of  decision  making  problems  under  fuzzy  domain  mostly  necessitate  choosing  the  best  alternative  among  the  sets  of  alternatives.  This  requirement  is  generally  fulfilled  through  the  methodology  called  the  ranking  of  fuzzy  numbers.  In  this  paper,  an  attempt  under  fuzzy  domain  to  develop  a  coherent  methodology  of  ordering  fuzzy  numbers  is  thrived.  The  ill-defined  quantity  ‘value’  and  the  amount  of  uncertainty  in  the  ill-defined  quantity  ‘ambiguity’  of  a  fuzzy  number  constituted  the  important  tools  in  ranking  fuzzy  numbers.  As  such,  an  effort  has  been  made  to  employ  these  quantities  in  ranking  interval  type-2  fuzzy  numbers  constructing  a  novel  value-ambiguity  ranking  index.  A  through  comparative  study  with  the  existing  method  and  the  present  method  is  executed  to  exhibit  the  essence  of  the  proposed  ranking  method.  The  comparative  study  exhibit  the  superiority  of  the  present  method  of  ranking  interval  type-2  fuzzy  numbers  over  the  existing  method.  Further,  the  method  is  not  just  limited  to  interval  type-2  fuzzy  numbers,  it  can  be  broadly  applied  to  arbitrary  fuzzy  numbers.  Finally,  a  risk  analysis  problem  has  been  discussed  and  the  proposed  novel  value-ambiguity  ranking  index  of  ranking  interval  type-2  fuzzy  numbers  has  been  applied  successfully.
0	Densely  connected  convolutional  networks.  Recent  work  has  shown  that  convolutional  networks  can  be  substantially  deeper,  more  accurate,  and  efficient  to  train  if  they  contain  shorter  connections  between  layers  close  to  the  input  and  those  close  to  the  output.  In  this  paper,  we  embrace  this  observation  and  introduce  the  Dense  Convolutional  Network  (DenseNet),  which  connects  each  layer  to  every  other  layer  in  a  feed-forward  fashion.  Whereas  traditional  convolutional  networks  with  L  layers  have  L  connections  -  one  between  each  layer  and  its  subsequent  layer  -  our  network  has  L(L+1)/2  direct  connections.  For  each  layer,  the  feature-maps  of  all  preceding  layers  are  used  as  inputs,  and  its  own  feature-maps  are  used  as  inputs  into  all  subsequent  layers.  DenseNets  have  several  compelling  advantages:  they  alleviate  the  vanishing-gradient  problem,  strengthen  feature  propagation,  encourage  feature  reuse,  and  substantially  reduce  the  number  of  parameters.  We  evaluate  our  proposed  architecture  on  four  highly  competitive  object  recognition  benchmark  tasks  (CIFAR-10,  CIFAR-100,  SVHN,  and  ImageNet).  DenseNets  obtain  significant  improvements  over  the  state-of-the-art  on  most  of  them,  whilst  requiring  less  computation  to  achieve  high  performance.  Code  and  pre-trained  models  are  available  at  this  https  URL  .
0	Weakly  supervised  object  localization  using  size  estimates.  We  present  a  technique  for  weakly  supervised  object  localization  (WSOL),  building  on  the  observation  that  WSOL  algorithms  usually  work  better  on  images  with  bigger  objects.  Instead  of  training  the  object  detector  on  the  entire  training  set  at  the  same  time,  we  propose  a  curriculum  learning  strategy  to  feed  training  images  into  the  WSOL  learning  loop  in  an  order  from  images  containing  bigger  objects  down  to  smaller  ones.  To  automatically  determine  the  order,  we  train  a  regressor  to  estimate  the  size  of  the  object  given  the  whole  image  as  input.  Furthermore,  we  use  these  size  estimates  to  further  improve  the  re-localization  step  of  WSOL  by  assigning  weights  to  object  proposals  according  to  how  close  their  size  matches  the  estimated  object  size.  We  demonstrate  the  effectiveness  of  using  size  order  and  size  weighting  on  the  challenging  PASCAL  VOC  2007  dataset,  where  we  achieve  a  significant  improvement  over  existing  state-of-the-art  WSOL  techniques.
0	Im2pano3d  extrapolating  360  structure  and  semantics  beyond  the  field  of  view.  We  present  Im2Pano3D,  a  convolutional  neural  network  that  generates  a  dense  prediction  of  3D  structure  and  a  probability  distribution  of  semantic  labels  for  a  full  360  panoramic  view  of  an  indoor  scene  when  given  only  a  partial  observation  (<=  50%)  in  the  form  of  an  RGB-D  image.  To  make  this  possible,  Im2Pano3D  leverages  strong  contextual  priors  learned  from  large-scale  synthetic  and  real-world  indoor  scenes.  To  ease  the  prediction  of  3D  structure,  we  propose  to  parameterize  3D  surfaces  with  their  plane  equations  and  train  the  model  to  predict  these  parameters  directly.  To  provide  meaningful  training  supervision,  we  use  multiple  loss  functions  that  consider  both  pixel  level  accuracy  and  global  context  consistency.  Experiments  demon-  strate  that  Im2Pano3D  is  able  to  predict  the  semantics  and  3D  structure  of  the  unobserved  scene  with  more  than  56%  pixel  accuracy  and  less  than  0.52m  average  distance  error,  which  is  significantly  better  than  alternative  approaches.
0	Object  recognition  from  short  videos  for  robotic  perception.  Deep  neural  networks  have  become  the  primary  learning  technique  for  object  recognition.  Videos,  unlike  still  images,  are  temporally  coherent  which  makes  the  application  of  deep  networks  non-trivial.  Here,  we  investigate  how  motion  can  aid  object  recognition  in  short  videos.  Our  approach  is  based  on  Long  Short-Term  Memory  (LSTM)  deep  networks.  Unlike  previous  applications  of  LSTMs,  we  implement  each  gate  as  a  convolution.  We  show  that  convolutional-based  LSTM  models  are  capable  of  learning  motion  dependencies  and  are  able  to  improve  the  recognition  accuracy  when  more  frames  in  a  sequence  are  available.  We  evaluate  our  approach  on  the  Washington  RGBD  Object  dataset  and  on  the  Washington  RGBD  Scenes  dataset.  Our  approach  outperforms  deep  nets  applied  to  still  images  and  sets  a  new  state-of-the-art  in  this  domain.
0	Ad  net  audio  visual  convolutional  neural  network  for  advertisement  detection  in  videos.  Personalized  advertisement  is  a  crucial  task  for  many  of  the  online  businesses  and  video  broadcasters.  Many  of  today's  broadcasters  use  the  same  commercial  for  all  customers,  but  as  one  can  imagine  different  viewers  have  different  interests  and  it  seems  reasonable  to  have  customized  commercial  for  different  group  of  people,  chosen  based  on  their  demographic  features,  and  history.  In  this  project,  we  propose  a  framework,  which  gets  the  broadcast  videos,  analyzes  them,  detects  the  commercial  and  replaces  it  with  a  more  suitable  commercial.  We  propose  a  two-stream  audio-visual  convolutional  neural  network,  that  one  branch  analyzes  the  visual  information  and  the  other  one  analyzes  the  audio  information,  and  then  the  audio  and  visual  embedding  are  fused  together,  and  are  used  for  commercial  detection,  and  content  categorization.  We  show  that  using  both  the  visual  and  audio  content  of  the  videos  significantly  improves  the  model  performance  for  video  analysis.  This  network  is  trained  on  a  dataset  of  more  than  50k  regular  video  and  commercial  shots,  and  achieved  much  better  performance  compared  to  the  models  based  on  hand-crafted  features.
0	Towards  compact  convnets  via  structure  sparsity  regularized  filter  pruning.  The  success  of  convolutional  neural  networks  (CNNs)  in  computer  vision  applications  has  been  accompanied  by  a  significant  increase  of  computation  and  memory  costs,  which  prohibits  its  usage  on  resource-limited  environments  such  as  mobile  or  embedded  devices.  To  this  end,  the  research  of  CNN  compression  has  recently  become  emerging.  In  this  paper,  we  propose  a  novel  filter  pruning  scheme,  termed  structured  sparsity  regularization  (SSR),  to  simultaneously  speedup  the  computation  and  reduce  the  memory  overhead  of  CNNs,  which  can  be  well  supported  by  various  off-the-shelf  deep  learning  libraries.  Concretely,  the  proposed  scheme  incorporates  two  different  regularizers  of  structured  sparsity  into  the  original  objective  function  of  filter  pruning,  which  fully  coordinates  the  global  outputs  and  local  pruning  operations  to  adaptively  prune  filters.  We  further  propose  an  Alternative  Updating  with  Lagrange  Multipliers  (AULM)  scheme  to  efficiently  solve  its  optimization.  AULM  follows  the  principle  of  ADMM  and  alternates  between  promoting  the  structured  sparsity  of  CNNs  and  optimizing  the  recognition  loss,  which  leads  to  a  very  efficient  solver  (2.5x  to  the  most  recent  work  that  directly  solves  the  group  sparsity-based  regularization).  Moreover,  by  imposing  the  structured  sparsity,  the  online  inference  is  extremely  memory-light,  since  the  number  of  filters  and  the  output  feature  maps  are  simultaneously  reduced.  The  proposed  scheme  has  been  deployed  to  a  variety  of  state-of-the-art  CNN  structures  including  LeNet,  AlexNet,  VGG,  ResNet  and  GoogLeNet  over  different  datasets.  Quantitative  results  demonstrate  that  the  proposed  scheme  achieves  superior  performance  over  the  state-of-the-art  methods.  We  further  demonstrate  the  proposed  compression  scheme  for  the  task  of  transfer  learning,  including  domain  adaptation  and  object  detection,  which  also  show  exciting  performance  gains  over  the  state-of-the-arts.
0	Infinite  brain  mr  images  pggan  based  data  augmentation  for  tumor  detection.  Due  to  the  lack  of  available  annotated  medical  images,  accurate  computer-assisted  diagnosis  requires  intensive  Data  Augmentation  (DA)  techniques,  such  as  geometric/intensity  transformations  of  original  images;  however,  those  transformed  images  intrinsically  have  a  similar  distribution  to  the  original  ones,  leading  to  limited  performance  improvement.  To  fill  the  data  lack  in  the  real  image  distribution,  we  synthesize  brain  contrast-enhanced  Magnetic  Resonance  (MR)  images---realistic  but  completely  different  from  the  original  ones---using  Generative  Adversarial  Networks  (GANs).  This  study  exploits  Progressive  Growing  of  GANs  (PGGANs),  a  multi-stage  generative  training  method,  to  generate  original-sized  256  X  256  MR  images  for  Convolutional  Neural  Network-based  brain  tumor  detection,  which  is  challenging  via  conventional  GANs;  difficulties  arise  due  to  unstable  GAN  training  with  high  resolution  and  a  variety  of  tumors  in  size,  location,  shape,  and  contrast.  Our  preliminary  results  show  that  this  novel  PGGAN-based  DA  method  can  achieve  promising  performance  improvement,  when  combined  with  classical  DA,  in  tumor  detection  and  also  in  other  medical  imaging  tasks.
0	Speaker  follower  models  for  vision  and  language  navigation.  Navigation  guided  by  natural  language  instructions  presents  a  challenging  reasoning  problem  for  instruction  followers.  Natural  language  instructions  typically  identify  only  a  few  high-level  decisions  and  landmarks  rather  than  complete  low-level  motor  behaviors;  much  of  the  missing  information  must  be  inferred  based  on  perceptual  context.  In  machine  learning  settings,  this  is  doubly  challenging:  it  is  difficult  to  collect  enough  annotated  data  to  enable  learning  of  this  reasoning  process  from  scratch,  and  also  difficult  to  implement  the  reasoning  process  using  generic  sequence  models.  Here  we  describe  an  approach  to  vision-and-language  navigation  that  addresses  both  these  issues  with  an  embedded  speaker  model.  We  use  this  speaker  model  to  (1)  synthesize  new  instructions  for  data  augmentation  and  to  (2)  implement  pragmatic  reasoning,  which  evaluates  how  well  candidate  action  sequences  explain  an  instruction.  Both  steps  are  supported  by  a  panoramic  action  space  that  reflects  the  granularity  of  human-generated  instructions.  Experiments  show  that  all  three  components  of  this  approach---speaker-driven  data  augmentation,  pragmatic  reasoning  and  panoramic  action  space---dramatically  improve  the  performance  of  a  baseline  instruction  follower,  more  than  doubling  the  success  rate  over  the  best  existing  approach  on  a  standard  benchmark.
0	Cross  domain  detection  via  graph  induced  prototype  alignment.  Applying  the  knowledge  of  an  object  detector  trained  on  a  specific  domain  directly  onto  a  new  domain  is  risky,  as  the  gap  between  two  domains  can  severely  degrade  model's  performance.  Furthermore,  since  different  instances  commonly  embody  distinct  modal  information  in  object  detection  scenario,  the  feature  alignment  of  source  and  target  domain  is  hard  to  be  realized.  To  mitigate  these  problems,  we  propose  a  Graph-induced  Prototype  Alignment  (GPA)  framework  to  seek  for  category-level  domain  alignment  via  elaborate  prototype  representations.  In  the  nutshell,  more  precise  instance-level  features  are  obtained  through  graph-based  information  propagation  among  region  proposals,  and,  on  such  basis,  the  prototype  representation  of  each  class  is  derived  for  category-level  domain  alignment.  In  addition,  in  order  to  alleviate  the  negative  effect  of  class-imbalance  on  domain  adaptation,  we  design  a  Class-reweighted  Contrastive  Loss  to  harmonize  the  adaptation  training  process.  Combining  with  Faster  R-CNN,  the  proposed  framework  conducts  feature  alignment  in  a  two-stage  manner.  Comprehensive  results  on  various  cross-domain  detection  tasks  demonstrate  that  our  approach  outperforms  existing  methods  with  a  remarkable  margin.  Our  code  is  available  at  this  https  URL.
0	Analyzing  the  affect  of  a  group  of  people  using  multi  modal  framework.  Millions  of  images  on  the  web  enable  us  to  explore  images  from  social  events  such  as  a  family  party,  thus  it  is  of  interest  to  understand  and  model  the  affect  exhibited  by  a  group  of  people  in  images.  But  analysis  of  the  affect  expressed  by  multiple  people  is  challenging  due  to  varied  indoor  and  outdoor  settings,  and  interactions  taking  place  between  various  numbers  of  people.  A  few  existing  works  on  Group-level  Emotion  Recognition  (GER)  have  investigated  on  face-level  information.  Due  to  the  challenging  environments,  face  may  not  provide  enough  information  to  GER.  Relatively  few  studies  have  investigated  multi-modal  GER.  Therefore,  we  propose  a  novel  multi-modal  approach  based  on  a  new  feature  description  for  understanding  emotional  state  of  a  group  of  people  in  an  image.  In  this  paper,  we  firstly  exploit  three  kinds  of  rich  information  containing  face,  upperbody  and  scene  in  a  group-level  image.  Furthermore,  in  order  to  integrate  multiple  person's  information  in  a  group-level  image,  we  propose  an  information  aggregation  method  to  generate  three  features  for  face,  upperbody  and  scene,  respectively.  We  fuse  face,  upperbody  and  scene  information  for  robustness  of  GER  against  the  challenging  environments.  Intensive  experiments  are  performed  on  two  challenging  group-level  emotion  databases  to  investigate  the  role  of  face,  upperbody  and  scene  as  well  as  multi-modal  framework.  Experimental  results  demonstrate  that  our  framework  achieves  very  promising  performance  for  GER.
0	Max  margin  object  detection.  Most  object  detection  methods  operate  by  applying  a  binary  classifier  to  sub-windows  of  an  image,  followed  by  a  non-maximum  suppression  step  where  detections  on  overlapping  sub-windows  are  removed.  Since  the  number  of  possible  sub-windows  in  even  moderately  sized  image  datasets  is  extremely  large,  the  classifier  is  typically  learned  from  only  a  subset  of  the  windows.  This  avoids  the  computational  difficulty  of  dealing  with  the  entire  set  of  sub-windows,  however,  as  we  will  show  in  this  paper,  it  leads  to  sub-optimal  detector  performance.    In  particular,  the  main  contribution  of  this  paper  is  the  introduction  of  a  new  method,  Max-Margin  Object  Detection  (MMOD),  for  learning  to  detect  objects  in  images.  This  method  does  not  perform  any  sub-sampling,  but  instead  optimizes  over  all  sub-windows.  MMOD  can  be  used  to  improve  any  object  detection  method  which  is  linear  in  the  learned  parameters,  such  as  HOG  or  bag-of-visual-word  models.  Using  this  approach  we  show  substantial  performance  gains  on  three  publicly  available  datasets.  Strikingly,  we  show  that  a  single  rigid  HOG  filter  can  outperform  a  state-of-the-art  deformable  part  model  on  the  Face  Detection  Data  Set  and  Benchmark  when  the  HOG  filter  is  learned  via  MMOD.
0	Catgan  coupled  adversarial  transfer  for  domain  generation.  This  paper  introduces  a  Coupled  adversarial  transfer  GAN  (CatGAN),  an  efficient  solution  to  domain  alignment.  The  basic  principles  of  CatGAN  focus  on  the  domain  generation  strategy  for  adaptation  which  is  motivated  by  the  generative  adversarial  net  (GAN)  and  the  adversarial  discriminative  domain  adaptation  (ADDA).  CatGAN  is  structured  by  shallow  multilayer  perceptrons  (MLPs)  for  adversarial  domain  adaptation.  The  CatGAN  comprises  of  two  slim  and  symmetric  subnetworks,  which  then  formulates  a  coupled  adversarial  learning  framework.  With  such  symmetry,  the  input  images  from  source/target  domain  can  be  fed  into  the  MLP  network  for  target/source  domain  generation,  supervised  by  the  coupled  discriminators  for  confrontation.  Notablely,  each  generator  contains  GAN  loss  and  domain  loss  to  guarantee  the  simple  network  work  well.  The  content  fidelity  term  aims  at  preserving  the  domain  specific  knowledge  during  generation.  Another  finding  is  that  the  class-wise  CatGAN  is  an  effective  alternative  to  conditional  GAN  without  label  constraint  in  generative  learning.  We  show  experimentally  that  the  proposed  model  achieves  competitive  performance  with  state-of-the  art  approaches.
0	Bit  planes  dense  subpixel  alignment  of  binary  descriptors.  Binary  descriptors  have  been  instrumental  in  the  recent  evolution  of  computationally  efficient  sparse  image  alignment  algorithms.  Increasingly,  however,  the  vision  community  is  interested  in  dense  image  alignment  methods,  which  are  more  suitable  for  estimating  correspondences  from  high  frame  rate  cameras  as  they  do  not  rely  on  exhaustive  search.  However,  classic  dense  alignment  approaches  are  sensitive  to  illumination  change.  In  this  paper,  we  propose  an  easy  to  implement  and  low  complexity  dense  binary  descriptor,  which  we  refer  to  as  bit-planes,  that  can  be  seamlessly  integrated  within  a  multi-channel  Lucas  &  Kanade  framework.  This  novel  approach  combines  the  robustness  of  binary  descriptors  with  the  speed  and  accuracy  of  dense  alignment  methods.  The  approach  is  demonstrated  on  a  template  tracking  problem  achieving  state-of-the-art  robustness  and  faster  than  real-time  performance  on  consumer  laptops  (400+  fps  on  a  single  core  Intel  i7)  and  hand-held  mobile  devices  (100+  fps  on  an  iPad  Air  2).
0	End  to  end  face  detection  and  cast  grouping  in  movies  using  erd  h  o  s  r  e  nyi  clustering.  We  present  an  end-to-end  system  for  detecting  and  clustering  faces  by  identity  in  full-length  movies.  Unlike  works  that  start  with  a  predefined  set  of  detected  faces,  we  consider  the  end-to-end  problem  of  detection  and  clustering  together.  We  make  three  separate  contributions.  First,  we  combine  a  state-of-the-art  face  detector  with  a  generic  tracker  to  extract  high  quality  face  tracklets.  We  then  introduce  a  novel  clustering  method,  motivated  by  the  classic  graph  theory  results  of  Erdős  and  Renyi.  It  is  based  on  the  observations  that  large  clusters  can  be  fully  connected  by  joining  just  a  small  fraction  of  their  point  pairs,  while  just  a  single  connection  between  two  different  people  can  lead  to  poor  clustering  results.  This  suggests  clustering  using  a  verification  system  with  very  few  false  positives  but  perhaps  moderate  recall.  We  introduce  a  novel  verification  method,  rank-1  counts  verification,  that  has  this  property,  and  use  it  in  a  link-based  clustering  scheme.  Finally,  we  define  a  novel  end-to-end  detection  and  clustering  evaluation  metric  allowing  us  to  assess  the  accuracy  of  the  entire  end-to-end  system.  We  present  state-of-the-art  results  on  multiple  video  data  sets  and  also  on  standard  face  databases.
0	Dist  gan  an  improved  gan  using  distance  constraints.  We  introduce  effective  training  algorithms  for  Generative  Adversarial  Networks  (GAN)  to  alleviate  mode  collapse  and  gradient  vanishing.  In  our  system,  we  constrain  the  generator  by  an  Autoencoder  (AE).  We  propose  a  formulation  to  consider  the  reconstructed  samples  from  AE  as  "real"  samples  for  the  discriminator.  This  couples  the  convergence  of  the  AE  with  that  of  the  discriminator,  effectively  slowing  down  the  convergence  of  discriminator  and  reducing  gradient  vanishing.  Importantly,  we  propose  two  novel  distance  constraints  to  improve  the  generator.  First,  we  propose  a  latent-data  distance  constraint  to  enforce  compatibility  between  the  latent  sample  distances  and  the  corresponding  data  sample  distances.  We  use  this  constraint  to  explicitly  prevent  the  generator  from  mode  collapse.  Second,  we  propose  a  discriminator-score  distance  constraint  to  align  the  distribution  of  the  generated  samples  with  that  of  the  real  samples  through  the  discriminator  score.  We  use  this  constraint  to  guide  the  generator  to  synthesize  samples  that  resemble  the  real  ones.  Our  proposed  GAN  using  these  distance  constraints,  namely  Dist-GAN,  can  achieve  better  results  than  state-of-the-art  methods  across  benchmark  datasets:  synthetic,  MNIST,  MNIST-1K,  CelebA,  CIFAR-10  and  STL-10  datasets.  Our  code  is  published  here  (this  https  URL)  for  research.
0	Contextdesc  local  descriptor  augmentation  with  cross  modality  context.  Most  existing  studies  on  learning  local  features  focus  on  the  patch-based  descriptions  of  individual  keypoints,  whereas  neglecting  the  spatial  relations  established  from  their  keypoint  locations.  In  this  paper,  we  go  beyond  the  local  detail  representation  by  introducing  context  awareness  to  augment  off-the-shelf  local  feature  descriptors.  Specifically,  we  propose  a  unified  learning  framework  that  leverages  and  aggregates  the  cross-modality  contextual  information,  including  (i)  visual  context  from  high-level  image  representation,  and  (ii)  geometric  context  from  2D  keypoint  distribution.  Moreover,  we  propose  an  effective  N-pair  loss  that  eschews  the  empirical  hyper-parameter  search  and  improves  the  convergence.  The  proposed  augmentation  scheme  is  lightweight  compared  with  the  raw  local  feature  description,  meanwhile  improves  remarkably  on  several  large-scale  benchmarks  with  diversified  scenes,  which  demonstrates  both  strong  practicality  and  generalization  ability  in  geometric  matching  applications.
0	Zero  shot  action  recognition  in  videos  a  survey.  Zero-Shot  Action  Recognition  has  attracted  attention  in  the  last  years  and  many  approaches  have  been  proposed  for  recognition  of  objects,  events  and  actions  in  images  and  videos.  There  is  a  demand  for  methods  that  can  classify  instances  from  classes  that  are  not  present  in  the  training  of  models,  especially  in  the  complex  problem  of  automatic  video  understanding,  since  collecting,  annotating  and  labeling  videos  are  difficult  and  laborious  tasks.  We  have  identified  that  there  are  many  methods  available  in  the  literature,  however,  it  is  difficult  to  categorize  which  techniques  can  be  considered  state  of  the  art.  Despite  the  existence  of  some  surveys  about  zero-shot  action  recognition  in  still  images  and  experimental  protocol,  there  is  no  work  focused  on  videos.  Therefore,  we  present  a  survey  of  the  methods  that  comprise  techniques  to  perform  visual  feature  extraction  and  semantic  feature  extraction  as  well  to  learn  the  mapping  between  these  features  considering  specifically  zero-shot  action  recognition  in  videos.  We  also  provide  a  complete  description  of  datasets,  experiments  and  protocols,  presenting  open  issues  and  directions  for  future  work,  essential  for  the  development  of  the  computer  vision  research  field.
0	A3gan  an  attribute  aware  attentive  generative  adversarial  network  for  face  aging.  Face  aging,  which  aims  at  aesthetically  rendering  a  given  face  to  predict  its  future  appearance,  has  received  significant  research  attention  in  recent  years.  Although  great  progress  has  been  achieved  with  the  success  of  Generative  Adversarial  Networks  (GANs)  in  synthesizing  realistic  images,  most  existing  GAN-based  face  aging  methods  have  two  main  problems:  1)  unnatural  changes  of  high-level  semantic  information  (e.g.  facial  attributes)  due  to  the  insufficient  utilization  of  prior  knowledge  of  input  faces,  and  2)  distortions  of  low-level  image  content  including  ghosting  artifacts  and  modifications  in  age-irrelevant  regions.  In  this  paper,  we  introduce  A3GAN,  an  Attribute-Aware  Attentive  face  aging  model  to  address  the  above  issues.  Facial  attribute  vectors  are  regarded  as  the  conditional  information  and  embedded  into  both  the  generator  and  discriminator,  encouraging  synthesized  faces  to  be  faithful  to  attributes  of  corresponding  inputs.  To  improve  the  visual  fidelity  of  generation  results,  we  leverage  the  attention  mechanism  to  restrict  modifications  to  age-related  areas  and  preserve  image  details.  Moreover,  the  wavelet  packet  transform  is  employed  to  capture  textural  features  at  multiple  scales  in  the  frequency  space.  Extensive  experimental  results  demonstrate  the  effectiveness  of  our  model  in  synthesizing  photorealistic  aged  face  images  and  achieving  state-of-the-art  performance  on  popular  face  aging  datasets.
0	Rethinking  table  parsing  using  graph  neural  networks.  Document  structure  analysis,  such  as  zone  segmentation  and  table  parsing,  is  a  complex  problem  in  document  processing  and  is  an  active  area  of  research.  The  recent  success  of  deep  learning  in  solving  various  computer  vision  and  machine  learning  problems  has  not  been  reflected  in  document  structure  analysis  since  conventional  neural  networks  are  not  well  suited  to  the  input  structure  of  the  problem.  In  this  paper,  we  propose  an  architecture  based  on  graph  networks  as  a  better  alternative  to  standard  neural  networks  for  table  parsing.  We  argue  that  graph  networks  are  a  more  natural  choice  for  these  problems,  and  explore  two  gradient-based  graph  neural  networks.  Our  proposed  architecture  combines  the  benefits  of  convolutional  neural  networks  for  visual  feature  extraction  and  graph  networks  for  dealing  with  the  problem  structure.  We  empirically  demonstrate  that  our  method  outperforms  the  baseline  by  a  significant  margin.  In  addition,  we  identify  the  lack  of  large  scale  datasets  as  a  major  hindrance  for  deep  learning  research  for  structure  analysis,  and  present  a  new  large  scale  synthetic  dataset  for  the  problem  of  table  parsing.  Finally,  we  open-source  our  implementation  of  dataset  generation  and  the  training  framework  of  our  graph  networks  to  promote  reproducible  research  in  this  direction.
0	Transpose  keypoint  localization  via  transformer.  While  CNN-based  models  have  made  remarkable  progress  on  human  pose  estimation,  what  spatial  dependencies  they  capture  to  localize  keypoints  remains  unclear.  In  this  work,  we  propose  a  model  called  \textbf{TransPose},  which  introduces  Transformer  for  human  pose  estimation.  The  attention  layers  built  in  Transformer  enable  our  model  to  capture  long-range  relationships  efficiently  and  also  can  reveal  what  dependencies  the  predicted  keypoints  rely  on.  To  predict  keypoint  heatmaps,  the  last  attention  layer  acts  as  an  aggregator,  which  collects  contributions  from  image  clues  and  forms  maximum  positions  of  keypoints.  Such  a  heatmap-based  localization  approach  via  Transformer  conforms  to  the  principle  of  Activation  Maximization~\cite{erhan2009visualizing}.  And  the  revealed  dependencies  are  image-specific  and  fine-grained,  which  also  can  provide  evidence  of  how  the  model  handles  special  cases,  e.g.,  occlusion.  The  experiments  show  that  TransPose  achieves  75.8  AP  and  75.0  AP  on  COCO  validation  and  test-dev  sets,  while  being  more  lightweight  and  faster  than  mainstream  CNN  architectures.  The  TransPose  model  also  transfers  very  well  on  MPII  benchmark,  achieving  superior  performance  on  the  test  set  when  fine-tuned  with  small  training  costs.  Code  and  pre-trained  models  are  publicly  available\footnote{\url{this  https  URL}}.
0	Dilated  convolutions  with  lateral  inhibitions  for  semantic  image  segmentation.  Dilated  convolutions  are  widely  used  in  deep  semantic  segmentation  models  as  they  can  enlarge  the  filters'  receptive  field  without  adding  additional  weights  nor  sacrificing  spatial  resolution.  However,  as  dilated  convolutional  filters  do  not  possess  positional  knowledge  about  the  pixels  on  semantically  meaningful  contours,  they  could  lead  to  ambiguous  predictions  on  object  boundaries.  In  addition,  although  dilating  the  filter  can  expand  its  receptive  field,  the  total  number  of  sampled  pixels  remains  unchanged,  which  usually  comprises  a  small  fraction  of  the  receptive  field's  total  area.  Inspired  by  the  Lateral  Inhibition  (LI)  mechanisms  in  human  visual  systems,  we  propose  the  dilated  convolution  with  lateral  inhibitions  (LI-Convs)  to  overcome  these  limitations.  Introducing  LI  mechanisms  improves  the  convolutional  filter's  sensitivity  to  semantic  object  boundaries.  Moreover,  since  LI-Convs  also  implicitly  take  the  pixels  from  the  laterally  inhibited  zones  into  consideration,  they  can  also  extract  features  at  a  denser  scale.  By  integrating  LI-Convs  into  the  Deeplabv3+  architecture,  we  propose  the  Lateral  Inhibited  Atrous  Spatial  Pyramid  Pooling  (LI-ASPP),  the  Lateral  Inhibited  MobileNet-V2  (LI-MNV2)  and  the  Lateral  Inhibited  ResNet  (LI-ResNet).  Experimental  results  on  three  benchmark  datasets  (PASCAL  VOC  2012,  CelebAMask-HQ  and  ADE20K)  show  that  our  LI-based  segmentation  models  outperform  the  baseline  on  all  of  them,  thus  verify  the  effectiveness  and  generality  of  the  proposed  LI-Convs.
0	Towards  learning  food  portion  from  monocular  images  with  cross  domain  feature  adaptation.  We  aim  to  estimate  food  portion  size,  a  property  that  is  strongly  related  to  the  presence  of  food  object  in  3D  space,  from  single  monocular  images  under  real  life  setting.  Specifically,  we  are  interested  in  end-to-end  estimation  of  food  portion  size,  which  has  great  potential  in  the  field  of  personal  health  management.  Unlike  image  segmentation  or  object  recognition  where  annotation  can  be  obtained  through  large  scale  crowd  sourcing,  it  is  much  more  challenging  to  collect  datasets  for  portion  size  estimation  since  human  cannot  accurately  estimate  the  size  of  an  object  in  an  arbitrary  2D  image  without  expert  knowledge.  To  address  such  challenge,  we  introduce  a  real  life  food  image  dataset  collected  from  a  nutrition  study  where  the  groundtruth  food  energy  (calorie)  is  provided  by  registered  dietitians,  and  will  be  made  available  to  the  research  community.  We  propose  a  deep  regression  process  for  portion  size  estimation  by  combining  features  estimated  from  both  RGB  and  learned  energy  distribution  domains.  Our  estimates  of  food  energy  achieved  state-of-the-art  with  a  MAPE  of  11.47%,  significantly  outperforms  non-expert  human  estimates  by  27.56%.
0	Zero  shot  text  to  image  generation.  Text-to-image  generation  has  traditionally  focused  on  finding  better  modeling  assumptions  for  training  on  a  fixed  dataset.  These  assumptions  might  involve  complex  architectures,  auxiliary  losses,  or  side  information  such  as  object  part  labels  or  segmentation  masks  supplied  during  training.  We  describe  a  simple  approach  for  this  task  based  on  a  transformer  that  autoregressively  models  the  text  and  image  tokens  as  a  single  stream  of  data.  With  sufficient  data  and  scale,  our  approach  is  competitive  with  previous  domain-specific  models  when  evaluated  in  a  zero-shot  fashion.
0	Learning  to  dehaze  from  realistic  scene  with  a  fast  physics  based  dehazing  network.  Dehazing  is  a  popular  computer  vision  topic  for  long.  A  real-time  dehazing  method  with  reliable  performance  is  highly  desired  for  many  applications  such  as  autonomous  driving.  While  recent  learning-based  methods  require  datasets  containing  pairs  of  hazy  images  and  clean  ground  truth  references,  it  is  generally  impossible  to  capture  accurate  ground  truth  in  real  scenes.  Many  existing  works  compromise  this  difficulty  to  generate  hazy  images  by  rendering  the  haze  from  depth  on  common  RGBD  datasets  using  the  haze  imaging  model.  However,  there  is  still  a  gap  between  the  synthetic  datasets  and  real  hazy  images  as  large  datasets  with  high-quality  depth  are  mostly  indoor  and  depth  maps  for  outdoor  are  imprecise.  In  this  paper,  we  complement  the  existing  datasets  with  a  new,  large,  and  diverse  dehazing  dataset  containing  real  outdoor  scenes  from  High-Definition  (HD)  3D  movies.  We  select  a  large  number  of  high-quality  frames  of  real  outdoor  scenes  and  render  haze  on  them  using  depth  from  stereo.  Our  dataset  is  more  realistic  than  existing  ones  and  we  demonstrate  that  using  this  dataset  greatly  improves  the  dehazing  performance  on  real  scenes.  In  addition  to  the  dataset,  we  also  propose  a  light  and  reliable  dehazing  network  inspired  by  the  physics  model.  Our  approach  outperforms  other  methods  by  a  large  margin  and  becomes  the  new  state-of-the-art  method.  Moreover,  the  light-weight  design  of  the  network  enables  our  method  to  run  at  a  real-time  speed,  which  is  much  faster  than  other  baseline  methods.
0	Contrastive  learning  of  global  and  local  features  for  medical  image  segmentation  with  limited  annotations.  A  key  requirement  for  the  success  of  supervised  deep  learning  is  a  large  labeled  dataset  -  a  condition  that  is  difficult  to  meet  in  medical  image  analysis.  Self-supervised  learning  (SSL)  can  help  in  this  regard  by  providing  a  strategy  to  pre-train  a  neural  network  with  unlabeled  data,  followed  by  fine-tuning  for  a  downstream  task  with  limited  annotations.  Contrastive  learning,  a  particular  variant  of  SSL,  is  a  powerful  technique  for  learning  image-level  representations.  In  this  work,  we  propose  strategies  for  extending  the  contrastive  learning  framework  for  segmentation  of  volumetric  medical  images  in  the  semi-supervised  setting  with  limited  annotations,  by  leveraging  domain-specific  and  problem-specific  cues.  Specifically,  we  propose  (1)  novel  contrasting  strategies  that  leverage  structural  similarity  across  volumetric  medical  images  (domain-specific  cue)  and  (2)  a  local  version  of  the  contrastive  loss  to  learn  distinctive  representations  of  local  regions  that  are  useful  for  per-pixel  segmentation  (problem-specific  cue).  We  carry  out  an  extensive  evaluation  on  three  Magnetic  Resonance  Imaging  (MRI)  datasets.  In  the  limited  annotation  setting,  the  proposed  method  yields  substantial  improvements  compared  to  other  self-supervision  and  semi-supervised  learning  techniques.  When  combined  with  a  simple  data  augmentation  technique,  the  proposed  method  reaches  within  8%  of  benchmark  performance  using  only  two  labeled  MRI  volumes  for  training,  corresponding  to  only  4%  (for  ACDC)  of  the  training  data  used  to  train  the  benchmark.  The  code  is  made  public  at  this  https  URL.
0	Synthetic  data  for  multi  parameter  camera  based  physiological  sensing.  Synthetic  data  is  a  powerful  tool  in  training  data  hungry  deep  learning  algorithms.  However,  to  date,  camera-based  physiological  sensing  has  not  taken  full  advantage  of  these  techniques.  In  this  work,  we  leverage  a  high-fidelity  synthetics  pipeline  for  generating  videos  of  faces  with  faithful  blood  flow  and  breathing  patterns.  We  present  systematic  experiments  showing  how  physiologically-grounded  synthetic  data  can  be  used  in  training  camera-based  multi-parameter  cardiopulmonary  sensing.  We  provide  empirical  evidence  that  heart  and  breathing  rate  measurement  accuracy  increases  with  the  number  of  synthetic  avatars  in  the  training  set.  Furthermore,  training  with  avatars  with  darker  skin  types  leads  to  better  overall  performance  than  training  with  avatars  with  lighter  skin  types.  Finally,  we  discuss  the  opportunities  that  synthetics  present  in  the  domain  of  camera-based  physiological  sensing  and  limitations  that  need  to  be  overcome.
0	Are  we  missing  confidence  in  pseudo  lidar  methods  for  monocular  3d  object  detection.  Pseudo-LiDAR-based  methods  for  monocular  3D  object  detection  have  received  considerable  attention  in  the  community  due  to  the  performance  gains  exhibited  on  the  KITTI3D  benchmark,  in  particular  on  the  commonly  reported  validation  split.  This  generated  a  distorted  impression  about  the  superiority  of  Pseudo-LiDAR-based  (PL-based)  approaches  over  methods  working  with  RGB  images  only.  Our  first  contribution  consists  in  rectifying  this  view  by  pointing  out  and  showing  experimentally  that  the  validation  results  published  by  PL-based  methods  are  substantially  biased.  The  source  of  the  bias  resides  in  an  overlap  between  the  KITTI3D  object  detection  validation  set  and  the  training/validation  sets  used  to  train  depth  predictors  feeding  PL-based  methods.  Surprisingly,  the  bias  remains  also  after  geographically  removing  the  overlap.  This  leaves  the  test  set  as  the  only  reliable  set  for  comparison,  where  published  PL-based  methods  do  not  excel.  Our  second  contribution  brings  PL-based  methods  back  up  in  the  ranking  with  the  design  of  a  novel  deep  architecture  which  introduces  a  3D  confidence  prediction  module.  We  show  that  3D  confidence  estimation  techniques  derived  from  RGB-only  3D  detection  approaches  can  be  successfully  integrated  into  our  framework  and,  more  importantly,  that  improved  performance  can  be  obtained  with  a  newly  designed  3D  confidence  measure,  leading  to  state-of-the-art  performance  on  the  KITTI3D  benchmark.
0	Performance  comparison  of  evolutionary  algorithms  applied  to  hybrid  rocket  problem.  Optimizer  is  an  essence  in  design-informatics  to  efficiently  obtain  nondominated  solutions.  In  the  present  study,  the  best  optimizer  is  decided  through  the  competition  for  the  mathematical  standard  test  functions  and  is  also  applied  to  a  conceptual  design  problem  of  simple  single-stage  hybrid  rocket  as  the  real-world  problem.  Consequently,  a  hybrid  method  between  differential  evolution  and  genetic  algorithm  has  good  exploration  performance.  Moreover,  the  principal  component  analysis  blended  crossover  and  the  confidence  interval  based  crossover  have  good  capability  on  the  hybrid  method  between  differential  evolution  and  genetic  algorithm.
0	Genetic  algorithm  with  a  structure  based  representation  for  genetic  fuzzy  data  mining.  Mining  association  rules  is  an  important  data  mining  technology  aiming  to  find  the  relationship  among  items  in  the  databases.  Genetic-fuzzy  data  mining  uses  evolutionary  algorithm,  such  as  genetic  algorithm  (GA),  to  optimize  the  membership  functions  for  mining  fuzzy  association  rules,  and  has  received  considerable  success.  The  increase  in  data,  especially  in  big  data  analytics,  poses  serious  challenges  to  GA  in  the  effectiveness  and  efficiency  of  finding  appropriate  membership  functions.  This  study  proposes  a  GA  for  enhancing  genetic-fuzzy  mining  of  association  rules.  First,  we  design  a  novel  chromosome  representation  considering  the  structures  of  membership  functions.  The  representation  facilitates  arrangement  of  membership  functions.  Second,  this  study  presents  two  heuristics  in  the  light  of  overlap  and  coverage  for  removing  inappropriate  arrangement.  A  series  of  experiments  is  conducted  to  examine  the  proposed  GA  on  different  amounts  of  transactions.  The  experimental  results  show  that  GA  benefits  from  the  proposed  representation.  The  two  heuristics  help  to  explore  the  structures  of  membership  functions  and  achieve  significant  improvement  on  GA  in  terms  of  solution  quality  and  convergence  speed.  The  satisfactory  outcomes  validate  the  high  capability  of  the  proposed  GA  in  genetic-fuzzy  mining  of  association  rules.
0	A  two  step  surrogate  modeling  strategy  for  single  objective  and  multi  objective  optimization  of  radiofrequency  circuits.  The  knowledge-intensive  radiofrequency  circuit  design  and  the  scarce  design  automation  support  play  against  the  increasingly  stringent  time-to-market  demands.  Optimization  algorithms  are  starting  to  play  a  crucial  role;  however,  their  effectiveness  is  dramatically  limited  by  the  accuracy  of  the  evaluation  functions  of  objectives  and  constraints.  Accurate  performance  evaluation  of  radiofrequency  passive  elements,  e.g.,  inductors,  is  provided  by  electromagnetic  simulators,  but  their  computational  cost  makes  their  use  within  iterative  optimization  loops  unaffordable.  Surrogate  modeling  strategies,  e.g.,  Kriging,  support  vector  machines,  artificial  neural  networks,  etc.,  arise  as  a  promising  modeling  alternative.  However,  their  limited  accuracy  in  this  kind  of  applications  has  prevented  a  widespread  use.  In  this  paper,  inductor  performance  properties  are  exploited  to  develop  a  two-step  surrogate  modeling  strategy  in  order  to  evaluate  the  behavior  of  inductors  with  high  efficiency  and  accuracy.  An  automated  design  flow  for  radiofrequency  circuits  using  this  surrogate  modeling  of  passive  components  is  presented.  The  methodology  couples  a  circuit  simulator  with  evolutionary  computation  algorithms  such  as  particle  swarm  optimization,  genetic  algorithm  or  non-dominated  sorting  genetic  algorithm  (NSGA-II).  This  methodology  ensures  optimal  performances  within  short  computation  times  by  avoiding  electromagnetic  simulations  of  inductors  during  the  entire  optimization  process  and  using  a  surrogate  model  that  has  less  than  1%  error  in  inductance  and  quality  factor  when  compared  against  electromagnetic  simulations.  Numerous  real-life  experiments  of  single-objective  and  multi-objective  low-noise  amplifier  design  demonstrate  the  accuracy  and  efficiency  of  the  proposed  strategies.
0	An  approach  based  on  knowledge  exploration  for  state  space  management  in  checking  reachability  of  complex  software  systems.  Model  checking  is  one  of  the  most  efficient  techniques  in  software  system  verification.  However,  state  space  explosion  is  a  big  challenge  while  using  this  technique  to  check  different  properties  like  safety  ones.  In  this  situation,  one  can  search  the  state  space  to  find  a  reachable  state  in  which  the  safety  property  is  violated.  Hence,  reachability  checking  can  be  done  instead  of  checking  safety  property.  However,  checking  reachability  in  the  worst  case  may  cause  state  space  explosion  again.  To  handle  this  problem,  our  idea  is  based  on  generating  a  small  model  consistent  with  the  main  model.  Then  by  exploring  the  state  space  entirely,  we  search  it  to  find  the  goal  states.  After  finding  the  goal  states,  we  label  the  paths  which  start  from  the  initial  state  and  leading  to  a  goal  state.  Then  using  the  ensemble  classification  technique,  the  necessary  knowledge  is  extracted  from  these  paths  to  intelligently  explore  the  state  space  of  the  bigger  model.  Ensemble  machine  learning  technique  uses  Boosting  method  along  with  decision  trees.  It  follows  sampling  techniques  by  replacement.  This  method  generates  k  predictive  models  after  sampling  k  times.  Finally,  it  uses  a  voting  mechanism  to  predict  the  labels  of  the  final  path.  Our  proposed  approach  is  implemented  in  GROOVE,  which  is  an  open  source  toolset  for  designing  and  model  checking  graph  transformation  systems.  Our  experiments  show  a  significant  improvement  in  terms  of  both  speed  and  memory  usage.  In  average,  our  approach  consumes  nearly  42%  fewer  memory  than  other  approaches.  Also,  it  generates  witnesses  nearly  20%  shorter  than  others,  in  average.
0	Unsupervised  pre  training  of  image  features  on  non  curated  data.  Pre-training  general-purpose  visual  features  with  convolutional  neural  networks  without  relying  on  annotations  is  a  challenging  and  important  task.  Most  recent  efforts  in  unsupervised  feature  learning  have  focused  on  either  small  or  highly  curated  datasets  like  ImageNet,  whereas  using  uncurated  raw  datasets  was  found  to  decrease  the  feature  quality  when  evaluated  on  a  transfer  task.  Our  goal  is  to  bridge  the  performance  gap  between  unsupervised  methods  trained  on  curated  data,  which  are  costly  to  obtain,  and  massive  raw  datasets  that  are  easily  available.  To  that  effect,  we  propose  a  new  unsupervised  approach  which  leverages  self-supervision  and  clustering  to  capture  complementary  statistics  from  large-scale  data.  We  validate  our  approach  on  96  million  images  from  YFCC100M,  achieving  state-of-the-art  results  among  unsupervised  methods  on  standard  benchmarks,  which  confirms  the  potential  of  unsupervised  learning  when  only  uncurated  data  are  available.  We  also  show  that  pre-training  a  supervised  VGG-16  with  our  method  achieves  74.9%  top-1  classification  accuracy  on  the  validation  set  of  ImageNet,  which  is  an  improvement  of  +0.8%  over  the  same  network  trained  from  scratch.  Our  code  is  available  at  this  https  URL.
0	Utility  based  control  for  computer  vision.  Several  key  issues  arise  in  implementing  computer  vision  recognition  of  world  objects  in  terms  of  Bayesian  networks.  Computational  efficiency  is  a  driving  force.  Perceptual  networks  are  very  deep,  typically  fifteen  levels  of  structure.  Images  are  wide,  e.g.,  an  unspecified-number  of  edges  may  appear  anywhere  in  an  image  512  x  512  pixels  or  larger.  For  efficiency,  we  dynamically  instantiate  hypotheses  of  observed  objects.  The  network  is  not  fixed,  but  is  created  incrementally  at  runtime.  Generation  of  hypotheses  of  world  objects  and  indexing  of  models  for  recognition  are  important,  but  they  are  not  considered  here  [4,11].  This  work  is  aimed  at  near-term  implementation  with  parallel  computation  in  a  radar  surveillance  system,  ADRIES  [5,  15],  and  a  system  for  industrial  part  recognition,  SUCCESSOR  [2].  For  many  applications,  vision  must  be  faster  to  be  practical  and  so  efficiently  controlling  the  machine  vision  process  is  critical.  Perceptual  operators  may  scan  megapixels  and  may  require  minutes  of  computation  time.  It  is  necessary  to  avoid  unnecessary  sensor  actions  and  computation.  Parallel  computation  is  available  at  several  levels  of  processor  capability.  The  potential  for  parallel,  distributed  computation  for  high-level  vision  means  distributing  non-homogeneous  computations.  This  paper  addresses  the  problem  of  task  control  in  machine  vision  systems  based  on  Bayesian  probability  models.  We  separate  control  and  inference  to  extend  the  previous  work  [3]  to  maximize  utility  instead  of  probability.  Maximizing  utility  allows  adopting  perceptual  strategies  for  efficient  information  gathering  with  sensors  and  analysis  of  sensor  data.  Results  of  controlling  machine  vision  via  utility  to  recognize  military  situations  are  presented  in  this  paper.  Future  work  extends  this  to  industrial  part  recognition  for  SUCCESSOR.
0	Wide  slice  residual  networks  for  food  recognition.  Food  diary  applications  represent  a  tantalizing  market.  Such  applications,  based  on  image  food  recognition,  opened  to  new  challenges  for  computer  vision  and  pattern  recognition  algorithms.  Recent  works  in  the  field  are  focusing  either  on  hand-crafted  representations  or  on  learning  these  by  exploiting  deep  neural  networks.  Despite  the  success  of  such  a  last  family  of  works,  these  generally  exploit  off-the  shelf  deep  architectures  to  classify  food  dishes.  Thus,  the  architectures  are  not  cast  to  the  specific  problem.  We  believe  that  better  results  can  be  obtained  if  the  deep  architecture  is  defined  with  respect  to  an  analysis  of  the  food  composition.  Following  such  an  intuition,  this  work  introduces  a  new  deep  scheme  that  is  designed  to  handle  the  food  structure.  Specifically,  inspired  by  the  recent  success  of  residual  deep  network,  we  exploit  such  a  learning  scheme  and  introduce  a  slice  convolution  block  to  capture  the  vertical  food  layers.  Outputs  of  the  deep  residual  blocks  are  combined  with  the  sliced  convolution  to  produce  the  classification  score  for  specific  food  categories.  To  evaluate  our  proposed  architecture  we  have  conducted  experimental  results  on  three  benchmark  datasets.  Results  demonstrate  that  our  solution  shows  better  performance  with  respect  to  existing  approaches  (e.g.,  a  top-1  accuracy  of  90.27%  on  the  Food-101  challenging  dataset).
0	Classification  regions  of  deep  neural  networks.  The  goal  of  this  paper  is  to  analyze  the  geometric  properties  of  deep  neural  network  classifiers  in  the  input  space.  We  specifically  study  the  topology  of  classification  regions  created  by  deep  networks,  as  well  as  their  associated  decision  boundary.  Through  a  systematic  empirical  investigation,  we  show  that  state-of-the-art  deep  nets  learn  connected  classification  regions,  and  that  the  decision  boundary  in  the  vicinity  of  datapoints  is  flat  along  most  directions.  We  further  draw  an  essential  connection  between  two  seemingly  unrelated  properties  of  deep  networks:  their  sensitivity  to  additive  perturbations  in  the  inputs,  and  the  curvature  of  their  decision  boundary.  The  directions  where  the  decision  boundary  is  curved  in  fact  remarkably  characterize  the  directions  to  which  the  classifier  is  the  most  vulnerable.  We  finally  leverage  a  fundamental  asymmetry  in  the  curvature  of  the  decision  boundary  of  deep  nets,  and  propose  a  method  to  discriminate  between  original  images,  and  images  perturbed  with  small  adversarial  examples.  We  show  the  effectiveness  of  this  purely  geometric  approach  for  detecting  small  adversarial  perturbations  in  images,  and  for  recovering  the  labels  of  perturbed  images.
0	Coconet  a  collaborative  convolutional  network.  We  present  an  end-to-end  deep  network  for  fine-grained  visual  categorization  called  Collaborative  Convolutional  Network  (CoCoNet).  The  network  uses  a  collaborative  layer  after  the  convolutional  layers  to  represent  an  image  as  an  optimal  weighted  collaboration  of  features  learned  from  training  samples  as  a  whole  rather  than  one  at  a  time.  This  gives  CoCoNet  more  power  to  encode  the  fine-grained  nature  of  the  data  with  limited  samples.  We  perform  a  detailed  study  of  the  performance  with  1-stage  and  2-stage  transfer  learning.  The  ablation  study  shows  that  the  proposed  method  outperforms  its  constituent  parts  consistently.  CoCoNet  also  outperforms  few  state-of-the-art  competing  methods.  Experiments  have  been  performed  on  the  fine-grained  bird  species  classification  problem  as  a  representative  example,  but  the  method  may  be  applied  to  other  similar  tasks.  We  also  introduce  a  new  public  dataset  for  fine-grained  species  recognition,  that  of  Indian  endemic  birds  and  have  reported  initial  results  on  it.
0	Beyond  forward  shortcuts  fully  convolutional  master  slave  networks  msnets  with  backward  skip  connections  for  semantic  segmentation.  Recent  deep  CNNs  contain  forward  shortcut  connections;  i.e.  skip  connections  from  low  to  high  layers.  Reusing  features  from  lower  layers  that  have  higher  resolution  (location  information)  benefit  higher  layers  to  recover  lost  details  and  mitigate  information  degradation.  However,  during  inference  the  lower  layers  do  not  know  about  high  layer  features,  although  they  contain  contextual  high  semantics  that  benefit  low  layers  to  adaptively  extract  informative  features  for  later  layers.  In  this  paper,  we  study  the  influence  of  backward  skip  connections  which  are  in  the  opposite  direction  to  forward  shortcuts,  i.e.  paths  from  high  layers  to  low  layers.  To  achieve  this  --  which  indeed  runs  counter  to  the  nature  of  feed-forward  networks  --  we  propose  a  new  fully  convolutional  model  that  consists  of  a  pair  of  networks.  A  `Slave'  network  is  dedicated  to  provide  the  backward  connections  from  its  top  layers  to  the  `Master'  network's  bottom  layers.  The  Master  network  is  used  to  produce  the  final  label  predictions.  In  our  experiments  we  validate  the  proposed  FCN  model  on  ADE20K  (ImageNet  scene  parsing),  PASCAL-Context,  and  PASCAL  VOC  2011  datasets.
0	Shape  aware  organ  segmentation  by  predicting  signed  distance  maps.  In  this  work,  we  propose  to  resolve  the  issue  existing  in  current  deep  learning  based  organ  segmentation  systems  that  they  often  produce  results  that  do  not  capture  the  overall  shape  of  the  target  organ  and  often  lack  smoothness.  Since  there  is  a  rigorous  mapping  between  the  Signed  Distance  Map  (SDM)  calculated  from  object  boundary  contours  and  the  binary  segmentation  map,  we  exploit  the  feasibility  of  learning  the  SDM  directly  from  medical  scans.  By  converting  the  segmentation  task  into  predicting  an  SDM,  we  show  that  our  proposed  method  retains  superior  segmentation  performance  and  has  better  smoothness  and  continuity  in  shape.  To  leverage  the  complementary  information  in  traditional  segmentation  training,  we  introduce  an  approximated  Heaviside  function  to  train  the  model  by  predicting  SDMs  and  segmentation  maps  simultaneously.  We  validate  our  proposed  models  by  conducting  extensive  experiments  on  a  hippocampus  segmentation  dataset  and  the  public  MICCAI  2015  Head  and  Neck  Auto  Segmentation  Challenge  dataset  with  multiple  organs.  While  our  carefully  designed  backbone  3D  segmentation  network  improves  the  Dice  coefficient  by  more  than  5%  compared  to  current  state-of-the-arts,  the  proposed  model  with  SDM  learning  produces  smoother  segmentation  results  with  smaller  Hausdorff  distance  and  average  surface  distance,  thus  proving  the  effectiveness  of  our  method.
0	Face  detection  with  a  3d  model.  Abstract      This  chapter  presents  a  part-based  face  detection  approach  where  the  spatial  relationship  between  the  face  parts  is  represented  by  a  hidden  3D  model  with  six  parameters.  The  computational  complexity  of  the  search  in  the  six-dimensional  pose  space  is  addressed  by  proposing  meaningful  3D  pose  candidates  by  image-based  regression  from  detected  face  keypoint  locations.  The  3D  pose  candidates  are  evaluated  using  a  parameter  sensitive  classifier  based  on  difference  features  relative  to  the  3D  pose.  A  compatible  subset  of  candidates  is  then  obtained  by  nonmaximal  suppression.  Experiments  on  the  Face  Detection  Database  (FDDB)  and  Annotated  Faces  in  the  Wild  (AFW)  face  detection  datasets  show  that  the  proposed  3D  model-based  approach  obtains  results  comparable  to  the  Cascade-CNN  [  17  ].
0	Associatively  segmenting  instances  and  semantics  in  point  clouds.  A  3D  point  cloud  describes  the  real  scene  precisely  and  intuitively.To  date  how  to  segment  diversified  elements  in  such  an  informative  3D  scene  is  rarely  discussed.  In  this  paper,  we  first  introduce  a  simple  and  flexible  framework  to  segment  instances  and  semantics  in  point  clouds  simultaneously.  Then,  we  propose  two  approaches  which  make  the  two  tasks  take  advantage  of  each  other,  leading  to  a  win-win  situation.  Specifically,  we  make  instance  segmentation  benefit  from  semantic  segmentation  through  learning  semantic-aware  point-level  instance  embedding.  Meanwhile,  semantic  features  of  the  points  belonging  to  the  same  instance  are  fused  together  to  make  more  accurate  per-point  semantic  predictions.  Our  method  largely  outperforms  the  state-of-the-art  method  in  3D  instance  segmentation  along  with  a  significant  improvement  in  3D  semantic  segmentation.  Code  has  been  made  available  at:  this  https  URL
0	Automatic  adaptation  of  object  detectors  to  new  domains  using  self  training.  This  work  addresses  the  unsupervised  adaptation  of  an  existing  object  detector  to  a  new  target  domain.  We  assume  that  a  large  number  of  unlabeled  videos  from  this  domain  are  readily  available.  We  automatically  obtain  labels  on  the  target  data  by  using  high-confidence  detections  from  the  existing  detector,  augmented  with  hard  (misclassified)  examples  acquired  by  exploiting  temporal  cues  using  a  tracker.  These  automatically-obtained  labels  are  then  used  for  re-training  the  original  model.  A  modified  knowledge  distillation  loss  is  proposed,  and  we  investigate  several  ways  of  assigning  soft-labels  to  the  training  examples  from  the  target  domain.  Our  approach  is  empirically  evaluated  on  challenging  face  and  pedestrian  detection  tasks:  a  face  detector  trained  on  WIDER-Face,  which  consists  of  high-quality  images  crawled  from  the  web,  is  adapted  to  a  large-scale  surveillance  data  set;  a  pedestrian  detector  trained  on  clear,  daytime  images  from  the  BDD-100K  driving  data  set  is  adapted  to  all  other  scenarios  such  as  rainy,  foggy,  night-time.  Our  results  demonstrate  the  usefulness  of  incorporating  hard  examples  obtained  from  tracking,  the  advantage  of  using  soft-labels  via  distillation  loss  versus  hard-labels,  and  show  promising  performance  as  a  simple  method  for  unsupervised  domain  adaptation  of  object  detectors,  with  minimal  dependence  on  hyper-parameters.
0	A  comprehensive  study  of  alzheimer  s  disease  classification  using  convolutional  neural  networks.  A  plethora  of  deep  learning  models  have  been  developed  for  the  task  of  Alzheimer's  disease  classification  from  brain  MRI  scans.  Many  of  these  models  report  high  performance,  achieving  three-class  classification  accuracy  of  up  to  95%.  However,  it  is  common  for  these  studies  to  draw  performance  comparisons  between  models  that  are  trained  on  different  subsets  of  a  dataset  or  use  varying  imaging  preprocessing  techniques,  making  it  difficult  to  objectively  assess  model  performance.  Furthermore,  many  of  these  works  do  not  provide  details  such  as  hyperparameters,  the  specific  MRI  scans  used,  or  their  source  code,  making  it  difficult  to  replicate  their  experiments.  To  address  these  concerns,  we  present  a  comprehensive  study  of  some  of  the  deep  learning  methods  and  architectures  on  the  full  set  of  images  available  from  ADNI.  We  find  that,  (1)  classification  using  3D  models  gives  an  improvement  of  1%  in  our  setup,  at  the  cost  of  significantly  longer  training  time  and  more  computation  power,  (2)  with  our  dataset,  pre-training  yields  minimal  ($<0.5\%$)  improvement  in  model  performance,  (3)  most  popular  convolutional  neural  network  models  yield  similar  performance  when  compared  to  each  other.  Lastly,  we  briefly  compare  the  effects  of  two  image  preprocessing  programs:  FreeSurfer  and  Clinica,  and  find  that  the  spatially  normalized  and  segmented  outputs  from  Clinica  increased  the  accuracy  of  model  prediction  from  63%  to  89%  when  compared  to  FreeSurfer  images.
0	Recognizing  cuneiform  signs  using  graph  based  methods.  The  cuneiform  script  constitutes  one  of  the  earliest  systems  of  writing  and  is  realized  by  wedge-shaped  marks  on  clay  tablets.  A  tremendous  number  of  cuneiform  tablets  have  already  been  discovered  and  are  incrementally  digitalized  and  made  available  to  automated  processing.  As  reading  cuneiform  script  is  still  a  manual  task,  we  address  the  real-world  application  of  recognizing  cuneiform  signs  by  two  graph  based  methods  with  complementary  runtime  characteristics.  We  present  a  graph  model  for  cuneiform  signs  together  with  a  tailored  distance  measure  based  on  the  concept  of  the  graph  edit  distance.  We  propose  efficient  heuristics  for  its  computation  and  demonstrate  its  effectiveness  in  classification  tasks  experimentally.  To  this  end,  the  distance  measure  is  used  to  implement  a  nearest  neighbor  classifier  leading  to  a  high  computational  cost  for  the  prediction  phase  with  increasing  training  set  size.  In  order  to  overcome  this  issue,  we  propose  to  use  CNNs  adapted  to  graphs  as  an  alternative  approach  shifting  the  computational  cost  to  the  training  phase.  We  demonstrate  the  practicability  of  both  approaches  in  an  extensive  experimental  comparison  regarding  runtime  and  prediction  accuracy.  Although  currently  available  annotated  real-world  data  is  still  limited,  we  obtain  a  high  accuracy  using  CNNs,  in  particular,  when  the  training  set  is  enriched  by  augmented  examples.
0	Sean  image  synthesis  with  semantic  region  adaptive  normalization.  We  propose  semantic  region-adaptive  normalization  (SEAN),  a  simple  but  effective  building  block  for  Generative  Adversarial  Networks  conditioned  on  segmentation  masks  that  describe  the  semantic  regions  in  the  desired  output  image.  Using  SEAN  normalization,  we  can  build  a  network  architecture  that  can  control  the  style  of  each  semantic  region  individually,  e.g.,  we  can  specify  one  style  reference  image  per  region.  SEAN  is  better  suited  to  encode,  transfer,  and  synthesize  style  than  the  best  previous  method  in  terms  of  reconstruction  quality,  variability,  and  visual  quality.  We  evaluate  SEAN  on  multiple  datasets  and  report  better  quantitative  metrics  (e.g.  FID,  PSNR)  than  the  current  state  of  the  art.  SEAN  also  pushes  the  frontier  of  interactive  image  editing.  We  can  interactively  edit  images  by  changing  segmentation  masks  or  the  style  for  any  given  region.  We  can  also  interpolate  styles  from  two  reference  images  per  region.
0	Neighbourhood  consensus  networks.  We  address  the  problem  of  finding  reliable  dense  correspondences  between  a  pair  of  images.  This  is  a  challenging  task  due  to  strong  appearance  differences  between  the  corresponding  scene  elements  and  ambiguities  generated  by  repetitive  patterns.  The  contributions  of  this  work  are  threefold.  First,  inspired  by  the  classic  idea  of  disambiguating  feature  matches  using  semi-local  constraints,  we  develop  an  end-to-end  trainable  convolutional  neural  network  architecture  that  identifies  sets  of  spatially  consistent  matches  by  analyzing  neighbourhood  consensus  patterns  in  the  4D  space  of  all  possible  correspondences  between  a  pair  of  images  without  the  need  for  a  global  geometric  model.  Second,  we  demonstrate  that  the  model  can  be  trained  effectively  from  weak  supervision  in  the  form  of  matching  and  non-matching  image  pairs  without  the  need  for  costly  manual  annotation  of  point  to  point  correspondences.  Third,  we  show  the  proposed  neighbourhood  consensus  network  can  be  applied  to  a  range  of  matching  tasks  including  both  category-  and  instance-level  matching,  obtaining  the  state-of-the-art  results  on  the  PF  Pascal  dataset  and  the  InLoc  indoor  visual  localization  benchmark.
0	Attention  is  all  we  need  nailing  down  object  centric  attention  for  egocentric  activity  recognition.  In  this  paper  we  propose  an  end-to-end  trainable  deep  neural  network  model  for  egocentric  activity  recognition.  Our  model  is  built  on  the  observation  that  egocentric  activities  are  highly  characterized  by  the  objects  and  their  locations  in  the  video.  Based  on  this,  we  develop  a  spatial  attention  mechanism  that  enables  the  network  to  attend  to  regions  containing  objects  that  are  correlated  with  the  activity  under  consideration.  We  learn  highly  specialized  attention  maps  for  each  frame  using  class-specific  activations  from  a  CNN  pre-trained  for  generic  image  recognition,  and  use  them  for  spatio-temporal  encoding  of  the  video  with  a  convolutional  LSTM.  Our  model  is  trained  in  a  weakly  supervised  setting  using  raw  video-level  activity-class  labels.  Nonetheless,  on  standard  egocentric  activity  benchmarks  our  model  surpasses  by  up  to  +6%  points  recognition  accuracy  the  currently  best  performing  method  that  leverages  hand  segmentation  and  object  location  strong  supervision  for  training.  We  visually  analyze  attention  maps  generated  by  the  network,  revealing  that  the  network  successfully  identifies  the  relevant  objects  present  in  the  video  frames  which  may  explain  the  strong  recognition  performance.  We  also  discuss  an  extensive  ablation  analysis  regarding  the  design  choices.
0	Mixup  cam  weakly  supervised  semantic  segmentation  via  uncertainty  regularization.  Obtaining  object  response  maps  is  one  important  step  to  achieve  weakly-supervised  semantic  segmentation  using  image-level  labels.  However,  existing  methods  rely  on  the  classification  task,  which  could  result  in  a  response  map  only  attending  on  discriminative  object  regions  as  the  network  does  not  need  to  see  the  entire  object  for  optimizing  the  classification  loss.  To  tackle  this  issue,  we  propose  a  principled  and  end-to-end  train-able  framework  to  allow  the  network  to  pay  attention  to  other  parts  of  the  object,  while  producing  a  more  complete  and  uniform  response  map.  Specifically,  we  introduce  the  mixup  data  augmentation  scheme  into  the  classification  network  and  design  two  uncertainty  regularization  terms  to  better  interact  with  the  mixup  strategy.  In  experiments,  we  conduct  extensive  analysis  to  demonstrate  the  proposed  method  and  show  favorable  performance  against  state-of-the-art  approaches.
0	Investigating  bias  in  deep  face  analysis  the  kanface  dataset  and  empirical  study.  Deep  learning-based  methods  have  pushed  the  limits  of  the  state-of-the-art  in  face  analysis.  However,  despite  their  success,  these  models  have  raised  concerns  regarding  their  bias  towards  certain  demographics.  This  bias  is  inflicted  both  by  limited  diversity  across  demographics  in  the  training  set,  as  well  as  the  design  of  the  algorithms.  In  this  work,  we  investigate  the  demographic  bias  of  deep  learning  models  in  face  recognition,  age  estimation,  gender  recognition  and  kinship  verification.  To  this  end,  we  introduce  the  most  comprehensive,  large-scale  dataset  of  facial  images  and  videos  to  date.  It  consists  of  40K  still  images  and  44K  sequences  (14.5M  video  frames  in  total)  captured  in  unconstrained,  real-world  conditions  from  1,045  subjects.  The  data  are  manually  annotated  in  terms  of  identity,  exact  age,  gender  and  kinship.  The  performance  of  state-of-the-art  models  is  scrutinized  and  demographic  bias  is  exposed  by  conducting  a  series  of  experiments.  Lastly,  a  method  to  debias  network  embeddings  is  introduced  and  tested  on  the  proposed  benchmarks.
0	Co  matching  combating  noisy  labels  by  augmentation  anchoring.  Deep  learning  with  noisy  labels  is  challenging  as  deep  neural  networks  have  the  high  capacity  to  memorize  the  noisy  labels.  In  this  paper,  we  propose  a  learning  algorithm  called  Co-matching,  which  balances  the  consistency  and  divergence  between  two  networks  by  augmentation  anchoring.  Specifically,  we  have  one  network  generate  anchoring  label  from  its  prediction  on  a  weakly-augmented  image.  Meanwhile,  we  force  its  peer  network,  taking  the  strongly-augmented  version  of  the  same  image  as  input,  to  generate  prediction  close  to  the  anchoring  label.  We  then  update  two  networks  simultaneously  by  selecting  small-loss  instances  to  minimize  both  unsupervised  matching  loss  (i.e.,  measure  the  consistency  of  the  two  networks)  and  supervised  classification  loss  (i.e.  measure  the  classification  performance).  Besides,  the  unsupervised  matching  loss  makes  our  method  not  heavily  rely  on  noisy  labels,  which  prevents  memorization  of  noisy  labels.  Experiments  on  three  benchmark  datasets  demonstrate  that  Co-matching  achieves  results  comparable  to  the  state-of-the-art  methods.
0	Mapping  the  world  population  one  building  at  a  time.  High  resolution  datasets  of  population  density  which  accurately  map  sparsely-distributed  human  populations  do  not  exist  at  a  global  scale.  Typically,  population  data  is  obtained  using  censuses  and  statistical  modeling.  More  recently,  methods  using  remotely-sensed  data  have  emerged,  capable  of  effectively  identifying  urbanized  areas.  Obtaining  high  accuracy  in  estimation  of  population  distribution  in  rural  areas  remains  a  very  challenging  task  due  to  the  simultaneous  requirements  of  sufficient  sensitivity  and  resolution  to  detect  very  sparse  populations  through  remote  sensing  as  well  as  reliable  performance  at  a  global  scale.  Here,  we  present  a  computer  vision  method  based  on  machine  learning  to  create  population  maps  from  satellite  imagery  at  a  global  scale,  with  a  spatial  sensitivity  corresponding  to  individual  buildings  and  suitable  for  global  deployment.  By  combining  this  settlement  data  with  census  data,  we  create  population  maps  with  ~30  meter  resolution  for  18  countries.  We  validate  our  method,  and  find  that  the  building  identification  has  an  average  precision  and  recall  of  0.95  and  0.91,  respectively  and  that  the  population  estimates  have  a  standard  error  of  a  factor  ~2  or  less.  Based  on  our  data,  we  analyze  29  percent  of  the  world  population,  and  show  that  99  percent  lives  within  36  km  of  the  nearest  urban  cluster.  The  resulting  high-resolution  population  datasets  have  applications  in  infrastructure  planning,  vaccination  campaign  planning,  disaster  response  efforts  and  risk  analysis  such  as  high  accuracy  flood  risk  analysis.
0	Deep  ten  texture  encoding  network.  We  propose  a  Deep  Texture  Encoding  Network  (Deep-TEN)  with  a  novel  Encoding  Layer  integrated  on  top  of  convolutional  layers,  which  ports  the  entire  dictionary  learning  and  encoding  pipeline  into  a  single  model.  Current  methods  build  from  distinct  components,  using  standard  encoders  with  separate  off-the-shelf  features  such  as  SIFT  descriptors  or  pre-trained  CNN  features  for  material  recognition.  Our  new  approach  provides  an  end-to-end  learning  framework,  where  the  inherent  visual  vocabularies  are  learned  directly  from  the  loss  function.  The  features,  dictionaries  and  the  encoding  representation  for  the  classifier  are  all  learned  simultaneously.  The  representation  is  orderless  and  therefore  is  particularly  useful  for  material  and  texture  recognition.  The  Encoding  Layer  generalizes  robust  residual  encoders  such  as  VLAD  and  Fisher  Vectors,  and  has  the  property  of  discarding  domain  specific  information  which  makes  the  learned  convolutional  features  easier  to  transfer.  Additionally,  joint  training  using  multiple  datasets  of  varied  sizes  and  class  labels  is  supported  resulting  in  increased  recognition  performance.  The  experimental  results  show  superior  performance  as  compared  to  state-of-the-art  methods  using  gold-standard  databases  such  as  MINC-2500,  Flickr  Material  Database,  KTH-TIPS-2b,  and  two  recent  databases  4D-Light-Field-Material  and  GTOS.  The  source  code  for  the  complete  system  are  publicly  available.
0	Lsq  improving  low  bit  quantization  through  learnable  offsets  and  better  initialization.  Unlike  ReLU,  newer  activation  functions  (like  Swish,  H-swish,  Mish)  that  are  frequently  employed  in  popular  efficient  architectures  can  also  result  in  negative  activation  values,  with  skewed  positive  and  negative  ranges.  Typical  learnable  quantization  schemes  [PACT,  LSQ]  assume  unsigned  quantization  for  activations  and  quantize  all  negative  activations  to  zero  which  leads  to  significant  loss  in  performance.  Naively  using  signed  quantization  to  accommodate  these  negative  values  requires  an  extra  sign  bit  which  is  expensive  for  low-bit  (2-,  3-,  4-bit)  quantization.  To  solve  this  problem,  we  propose  LSQ+,  a  natural  extension  of  LSQ,  wherein  we  introduce  a  general  asymmetric  quantization  scheme  with  trainable  scale  and  offset  parameters  that  can  learn  to  accommodate  the  negative  activations.  Gradient-based  learnable  quantization  schemes  also  commonly  suffer  from  high  instability  or  variance  in  the  final  training  performance,  hence  requiring  a  great  deal  of  hyper-parameter  tuning  to  reach  a  satisfactory  performance.  LSQ+  alleviates  this  problem  by  using  an  MSE-based  initialization  scheme  for  the  quantization  parameters.  We  show  that  this  initialization  leads  to  significantly  lower  variance  in  final  performance  across  multiple  training  runs.  Overall,  LSQ+  shows  state-of-the-art  results  for  EfficientNet  and  MixNet  and  also  significantly  outperforms  LSQ  for  low-bit  quantization  of  neural  nets  with  Swish  activations  (e.g.:  1.8%  gain  with  W4A4  quantization  and  upto  5.6%  gain  with  W2A2  quantization  of  EfficientNet-B0  on  ImageNet  dataset).  To  the  best  of  our  knowledge,  ours  is  the  first  work  to  quantize  such  architectures  to  extremely  low  bit-widths.
0	An  empirical  analysis  of  the  impact  of  data  augmentation  on  knowledge  distillation.  Generalization  Performance  of  Deep  Learning  models  trained  using  Empirical  Risk  Minimization  can  be  improved  significantly  by  using  Data  Augmentation  strategies  such  as  simple  transformations,  or  using  Mixed  Samples.  We  attempt  to  empirically  analyze  the  impact  of  such  strategies  on  the  transfer  of  generalization  between  teacher  and  student  models  in  a  distillation  setup.  We  observe  that  if  a  teacher  is  trained  using  any  of  the  mixed  sample  augmentation  strategies,  such  as  MixUp  or  CutMix,  the  student  model  distilled  from  it  is  impaired  in  its  generalization  capabilities.  We  hypothesize  that  such  strategies  limit  a  model's  capability  to  learn  example-specific  features,  leading  to  a  loss  in  quality  of  the  supervision  signal  during  distillation.  We  present  a  novel  Class-Discrimination  metric  to  quantitatively  measure  this  dichotomy  in  performance  and  link  it  to  the  discriminative  capacity  induced  by  the  different  strategies  on  a  network's  latent  space.
0	Nerf  neural  radiance  fields  without  known  camera  parameters.  This  paper  tackles  the  problem  of  novel  view  synthesis  (NVS)  from  2D  images  without  known  camera  poses  and  intrinsics.  Among  various  NVS  techniques,  Neural  Radiance  Field  (NeRF)  has  recently  gained  popularity  due  to  its  remarkable  synthesis  quality.  Existing  NeRF-based  approaches  assume  that  the  camera  parameters  associated  with  each  input  image  are  either  directly  accessible  at  training,  or  can  be  accurately  estimated  with  conventional  techniques  based  on  correspondences,  such  as  Structure-from-Motion.  In  this  work,  we  propose  an  end-to-end  framework,  termed  NeRF--,  for  training  NeRF  models  given  only  RGB  images,  without  pre-computed  camera  parameters.  Specifically,  we  show  that  the  camera  parameters,  including  both  intrinsics  and  extrinsics,  can  be  automatically  discovered  via  joint  optimisation  during  the  training  of  the  NeRF  model.  On  the  standard  LLFF  benchmark,  our  model  achieves  comparable  novel  view  synthesis  results  compared  to  the  baseline  trained  with  COLMAP  pre-computed  camera  parameters.  We  also  conduct  extensive  analyses  to  understand  the  model  behaviour  under  different  camera  trajectories,  and  show  that  in  scenarios  where  COLMAP  fails,  our  model  still  produces  robust  results.
0	C  mil  continuation  multiple  instance  learning  for  weakly  supervised  object  detection.  Weakly  supervised  object  detection  (WSOD)  is  a  challenging  task  when  provided  with  image  category  supervision  but  required  to  simultaneously  learn  object  locations  and  object  detectors.  Many  WSOD  approaches  adopt  multiple  instance  learning  (MIL)  and  have  non-convex  loss  functions  which  are  prone  to  get  stuck  into  local  minima  (falsely  localize  object  parts)  while  missing  full  object  extent  during  training.  In  this  paper,  we  introduce  a  continuation  optimization  method  into  MIL  and  thereby  creating  continuation  multiple  instance  learning  (C-MIL),  with  the  intention  of  alleviating  the  non-convexity  problem  in  a  systematic  way.  We  partition  instances  into  spatially  related  and  class  related  subsets,  and  approximate  the  original  loss  function  with  a  series  of  smoothed  loss  functions  defined  within  the  subsets.  Optimizing  smoothed  loss  functions  prevents  the  training  procedure  falling  prematurely  into  local  minima  and  facilitates  the  discovery  of  Stable  Semantic  Extremal  Regions  (SSERs)  which  indicate  full  object  extent.  On  the  PASCAL  VOC  2007  and  2012  datasets,  C-MIL  improves  the  state-of-the-art  of  weakly  supervised  object  detection  and  weakly  supervised  object  localization  with  large  margins.
0	End  to  end  learning  of  visual  representations  from  uncurated  instructional  videos.  Annotating  videos  is  cumbersome,  expensive  and  not  scalable.  Yet,  many  strong  video  models  still  rely  on  manually  annotated  data.  With  the  recent  introduction  of  the  HowTo100M  dataset,  narrated  videos  now  offer  the  possibility  of  learning  video  representations  without  manual  supervision.  In  this  work  we  propose  a  new  learning  approach,  MIL-NCE,  capable  of  addressing  misalignments  inherent  to  narrated  videos.  With  this  approach  we  are  able  to  learn  strong  video  representations  from  scratch,  without  the  need  for  any  manual  annotation.  We  evaluate  our  representations  on  a  wide  range  of  four  downstream  tasks  over  eight  datasets:  action  recognition  (HMDB-51,  UCF-101,  Kinetics-700),  text-to-video  retrieval  (YouCook2,  MSR-VTT),  action  localization  (YouTube-8M  Segments,  CrossTask)  and  action  segmentation  (COIN).  Our  method  outperforms  all  published  self-supervised  approaches  for  these  tasks  as  well  as  several  fully  supervised  baselines.
0	Cfun  combining  faster  r  cnn  and  u  net  network  for  efficient  whole  heart  segmentation.  In  this  paper,  we  propose  a  novel  heart  segmentation  pipeline  Combining  Faster  R-CNN  and  U-net  Network  (CFUN).  Due  to  Faster  R-CNN's  precise  localization  ability  and  U-net's  powerful  segmentation  ability,  CFUN  needs  only  one-step  detection  and  segmentation  inference  to  get  the  whole  heart  segmentation  result,  obtaining  good  results  with  significantly  reduced  computational  cost.  Besides,  CFUN  adopts  a  new  loss  function  based  on  edge  information  named  3D  Edge-loss  as  an  auxiliary  loss  to  accelerate  the  convergence  of  training  and  improve  the  segmentation  results.  Extensive  experiments  on  the  public  dataset  show  that  CFUN  exhibits  competitive  segmentation  performance  in  a  sharply  reduced  inference  time.  Our  source  code  and  the  model  are  publicly  available  at  this  https  URL.
0	Depression  severity  estimation  from  multiple  modalities.  Depression  is  a  major  debilitating  disorder  which  can  affect  people  from  all  ages.  With  a  continuous  increase  in  the  number  of  annual  cases  of  depression,  there  is  a  need  to  develop  automatic  techniques  for  the  detection  of  the  presence  and  extent  of  depression.  In  this  AVEC  challenge  we  explore  different  modalities  (speech,  language  and  visual  features  extracted  from  face)  to  design  and  develop  automatic  methods  for  the  detection  of  depression.  In  psychology  literature,  the  PHQ-8  questionnaire  is  well  established  as  a  tool  for  measuring  the  severity  of  depression.  In  this  paper  we  aim  to  automatically  predict  the  PHQ-8  scores  from  features  extracted  from  the  different  modalities.  We  show  that  visual  features  extracted  from  facial  landmarks  obtain  the  best  performance  in  terms  of  estimating  the  PHQ-8  results  with  a  mean  absolute  error  (MAE)  of  4.66  on  the  development  set.  Behavioral  characteristics  from  speech  provide  an  MAE  of  4.73.  Language  features  yield  a  slightly  higher  MAE  of  5.17.  When  switching  to  the  test  set,  our  Turn  Features  derived  from  audio  transcriptions  achieve  the  best  performance,  scoring  an  MAE  of  4.11  (corresponding  to  an  RMSE  of  4.94),  which  makes  our  system  the  winner  of  the  AVEC  2017  depression  sub-challenge.
0	Generalized  sequential  tree  reweighted  message  passing.  This  paper  addresses  the  problem  of  approximate  MAP-MRF  inference  in  general  graphical  models.  Following  [36],  we  consider  a  family  of  linear  programming  relaxations  of  the  problem  where  each  relaxation  is  specified  by  a  set  of  nested  pairs  of  factors  for  which  the  marginalization  constraint  needs  to  be  enforced.  We  develop  a  generalization  of  the  TRW-S  algorithm  [9]  for  this  problem,  where  we  use  a  decomposition  into  junction  chains,  monotonic  w.r.t.  some  ordering  on  the  nodes.  This  generalizes  the  monotonic  chains  in  [9]  in  a  natural  way.  We  also  show  how  to  deal  with  nested  factors  in  an  efficient  way.  Experiments  show  an  improvement  over  min-sum  diffusion,  MPLP  and  subgradient  ascent  algorithms  on  a  number  of  computer  vision  and  natural  language  processing  problems.
0	Pyramid  scene  parsing  network.  Scene  parsing  is  challenging  for  unrestricted  open  vocabulary  and  diverse  scenes.  In  this  paper,  we  exploit  the  capability  of  global  context  information  by  different-region-based  context  aggregation  through  our  pyramid  pooling  module  together  with  the  proposed  pyramid  scene  parsing  network  (PSPNet).  Our  global  prior  representation  is  effective  to  produce  good  quality  results  on  the  scene  parsing  task,  while  PSPNet  provides  a  superior  framework  for  pixel-level  prediction  tasks.  The  proposed  approach  achieves  state-of-the-art  performance  on  various  datasets.  It  came  first  in  ImageNet  scene  parsing  challenge  2016,  PASCAL  VOC  2012  benchmark  and  Cityscapes  benchmark.  A  single  PSPNet  yields  new  record  of  mIoU  accuracy  85.4%  on  PASCAL  VOC  2012  and  accuracy  80.2%  on  Cityscapes.
0	Face  image  analysis  using  aam  gabor  lbp  and  wd  features  for  gender  age  expression  and  ethnicity  classification.  The  growth  in  electronic  transactions  and  human  machine  interactions  rely  on  the  information  such  as  gender,  age,  expression  and  ethnicity  provided  by  the  face  image.  In  order  to  obtain  these  information,  feature  extraction  plays  a  major  role.  In  this  paper,  retrieval  of  age,  gender,  expression  and  race  information  from  an  individual  face  image  is  analysed  using  different  feature  extraction  methods.  The  performance  of  four  major  feature  extraction  methods  such  as  Active  Appearance  Model  (AAM),  Gabor  wavelets,  Local  Binary  Pattern  (LBP)  and  Wavelet  Decomposition  (WD)  are  analyzed  for  gender  recognition,  age  estimation,  expression  recognition  and  racial  recognition  in  terms  of  accuracy  (recognition  rate),  time  for  feature  extraction,  neural  training  and  time  to  test  an  image.  Each  of  this  recognition  system  is  compared  with  four  feature  extractors  on  same  dataset  (training  and  validation  set)  to  get  a  better  understanding  in  its  performance.  Experiments  carried  out  on  FG-NET,  Cohn-Kanade,  PAL  face  database  shows  that  each  method  has  its  own  merits  and  demerits.  Hence  it  is  practically  impossible  to  define  a  method  which  is  best  at  all  circumstances  with  less  computational  complexity.  Further,  a  detailed  comparison  of  age  estimation  and  age  estimation  using  gender  information  is  provided  along  with  a  solution  to  overcome  aging  effect  in  case  of  gender  recognition.  An  attempt  has  been  made  in  obtaining  all  (i.e.  gender,  age  range,  expression  and  ethnicity)  information  from  a  test  image  in  a  single  go.
0	The  2018  pirm  challenge  on  perceptual  image  super  resolution.  This  paper  reports  on  the  2018  PIRM  challenge  on  perceptual  super-resolution  (SR),  held  in  conjunction  with  the  Perceptual  Image  Restoration  and  Manipulation  (PIRM)  workshop  at  ECCV  2018.  In  contrast  to  previous  SR  challenges,  our  evaluation  methodology  jointly  quantifies  accuracy  and  perceptual  quality,  therefore  enabling  perceptual-driven  methods  to  compete  alongside  algorithms  that  target  PSNR  maximization.  Twenty-one  participating  teams  introduced  algorithms  which  well-improved  upon  the  existing  state-of-the-art  methods  in  perceptual  SR,  as  confirmed  by  a  human  opinion  study.  We  also  analyze  popular  image  quality  measures  and  draw  conclusions  regarding  which  of  them  correlates  best  with  human  opinion  scores.  We  conclude  with  an  analysis  of  the  current  trends  in  perceptual  SR,  as  reflected  from  the  leading  submissions.
0	A  closed  form  solution  to  universal  style  transfer.  Universal  style  transfer  tries  to  explicitly  minimize  the  losses  in  feature  space,  thus  it  does  not  require  training  on  any  pre-defined  styles.  It  usually  uses  different  layers  of  VGG  network  as  the  encoders  and  trains  several  decoders  to  invert  the  features  into  images.  Therefore,  the  effect  of  style  transfer  is  achieved  by  feature  transform.  Although  plenty  of  methods  have  been  proposed,  a  theoretical  analysis  of  feature  transform  is  still  missing.  In  this  paper,  we  first  propose  a  novel  interpretation  by  treating  it  as  the  optimal  transport  problem.  Then,  we  demonstrate  the  relations  of  our  formulation  with  former  works  like  Adaptive  Instance  Normalization  (AdaIN)  and  Whitening  and  Coloring  Transform  (WCT).  Finally,  we  derive  a  closed-form  solution  named  Optimal  Style  Transfer  (OST)  under  our  formulation  by  additionally  considering  the  content  loss  of  Gatys.  Comparatively,  our  solution  can  preserve  better  structure  and  achieve  visually  pleasing  results.  It  is  simple  yet  effective  and  we  demonstrate  its  advantages  both  quantitatively  and  qualitatively.  Besides,  we  hope  our  theoretical  analysis  can  inspire  future  works  in  neural  style  transfer.  Code  is  available  at  this  https  URL.
0	Controllable  video  captioning  with  pos  sequence  guidance  based  on  gated  fusion  network.  In  this  paper,  we  propose  to  guide  the  video  caption  generation  with  Part-of-Speech  (POS)  information,  based  on  a  gated  fusion  of  multiple  representations  of  input  videos.  We  construct  a  novel  gated  fusion  network,  with  one  particularly  designed  cross-gating  (CG)  block,  to  effectively  encode  and  fuse  different  types  of  representations,  e.g.,  the  motion  and  content  features  of  an  input  video.  One  POS  sequence  generator  relies  on  this  fused  representation  to  predict  the  global  syntactic  structure,  which  is  thereafter  leveraged  to  guide  the  video  captioning  generation  and  control  the  syntax  of  the  generated  sentence.  Specifically,  a  gating  strategy  is  proposed  to  dynamically  and  adaptively  incorporate  the  global  syntactic  POS  information  into  the  decoder  for  generating  each  word.  Experimental  results  on  two  benchmark  datasets,  namely  MSR-VTT  and  MSVD,  demonstrate  that  the  proposed  model  can  well  exploit  complementary  information  from  multiple  representations,  resulting  in  improved  performances.  Moreover,  the  generated  global  POS  information  can  well  capture  the  global  syntactic  structure  of  the  sentence,  and  thus  be  exploited  to  control  the  syntactic  structure  of  the  description.  Such  POS  information  not  only  boosts  the  video  captioning  performance  but  also  improves  the  diversity  of  the  generated  captions.  Our  code  is  at:  this  https  URL.
0	Conditional  flow  variational  autoencoders  for  structured  sequence  prediction.  Prediction  of  future  states  of  the  environment  and  interacting  agents  is  a  key  competence  required  for  autonomous  agents  to  operate  successfully  in  the  real  world.  Prior  work  for  structured  sequence  prediction  based  on  latent  variable  models  imposes  a  uni-modal  standard  Gaussian  prior  on  the  latent  variables.  This  induces  a  strong  model  bias  which  makes  it  challenging  to  fully  capture  the  multi-modality  of  the  distribution  of  the  future  states.  In  this  work,  we  introduce  Conditional  Flow  Variational  Autoencoders  (CF-VAE)  using  our  novel  conditional  normalizing  flow  based  prior  to  capture  complex  multi-modal  conditional  distributions  for  effective  structured  sequence  prediction.  Moreover,  we  propose  two  novel  regularization  schemes  which  stabilizes  training  and  deals  with  posterior  collapse  for  stable  training  and  better  fit  to  the  target  data  distribution.  Our  experiments  on  three  multi-modal  structured  sequence  prediction  datasets  --  MNIST  Sequences,  Stanford  Drone  and  HighD  --  show  that  the  proposed  method  obtains  state  of  art  results  across  different  evaluation  metrics.
0	Silhonet  an  rgb  method  for  6d  object  pose  estimation.  Autonomous  robot  manipulation  involves  estimating  the  translation  and  orientation  of  the  object  to  be  manipulated  as  a  6-degree-of-freedom  (6D)  pose.  Methods  using  RGB-D  data  have  shown  great  success  in  solving  this  problem.  However,  there  are  situations  where  cost  constraints  or  the  working  environment  may  limit  the  use  of  RGB-D  sensors.  When  limited  to  monocular  camera  data  only,  the  problem  of  object  pose  estimation  is  very  challenging.  In  this  work,  we  introduce  a  novel  method  called  SilhoNet  that  predicts  6D  object  pose  from  monocular  images.  We  use  a  Convolutional  Neural  Network  (CNN)  pipeline  that  takes  in  Region  of  Interest  (ROI)  proposals  to  simultaneously  predict  an  intermediate  silhouette  representation  for  objects  with  an  associated  occlusion  mask  and  a  3D  translation  vector.  The  3D  orientation  is  then  regressed  from  the  predicted  silhouettes.  We  show  that  our  method  achieves  better  overall  performance  on  the  YCB-Video  dataset  than  two  state-of-the  art  networks  for  6D  pose  estimation  from  monocular  image  input.
0	Fully  convolutional  mesh  autoencoder  using  efficient  spatially  varying  kernels.  Learning  latent  representations  of  registered  meshes  is  useful  for  many  3D  tasks.  Techniques  have  recently  shifted  to  neural  mesh  autoencoders.  Although  they  demonstrate  higher  precision  than  traditional  methods,  they  remain  unable  to  capture  fine-grained  deformations.  Furthermore,  these  methods  can  only  be  applied  to  a  template-specific  surface  mesh,  and  is  not  applicable  to  more  general  meshes,  like  tetrahedrons  and  non-manifold  meshes.  While  more  general  graph  convolution  methods  can  be  employed,  they  lack  performance  in  reconstruction  precision  and  require  higher  memory  usage.  In  this  paper,  we  propose  a  non-template-specific  fully  convolutional  mesh  autoencoder  for  arbitrary  registered  mesh  data.  It  is  enabled  by  our  novel  convolution  and  (un)pooling  operators  learned  with  globally  shared  weights  and  locally  varying  coefficients  which  can  efficiently  capture  the  spatially  varying  contents  presented  by  irregular  mesh  connections.  Our  model  outperforms  state-of-the-art  methods  on  reconstruction  accuracy.  In  addition,  the  latent  codes  of  our  network  are  fully  localized  thanks  to  the  fully  convolutional  structure,  and  thus  have  much  higher  interpolation  capability  than  many  traditional  3D  mesh  generation  models.
0	Efficientps  efficient  panoptic  segmentation.  Understanding  the  scene  in  which  an  autonomous  robot  operates  is  critical  for  its  competent  functioning.  Such  scene  comprehension  necessitates  recognizing  instances  of  traffic  participants  along  with  general  scene  semantics  which  can  be  effectively  addressed  by  the  panoptic  segmentation  task.  In  this  paper,  we  introduce  the  Efficient  Panoptic  Segmentation  (EfficientPS)  architecture  that  consists  of  a  shared  backbone  which  efficiently  encodes  and  fuses  semantically  rich  multi-scale  features.  We  incorporate  a  new  semantic  head  that  aggregates  fine  and  contextual  features  coherently  and  a  new  variant  of  Mask  R-CNN  as  the  instance  head.  We  also  propose  a  novel  panoptic  fusion  module  that  congruously  integrates  the  output  logits  from  both  the  heads  of  our  EfficientPS  architecture  to  yield  the  final  panoptic  segmentation  output.  Additionally,  we  introduce  the  KITTI  panoptic  segmentation  dataset  that  contains  panoptic  annotations  for  the  popularly  challenging  KITTI  benchmark.  Extensive  evaluations  on  Cityscapes,  KITTI,  Mapillary  Vistas  and  Indian  Driving  Dataset  demonstrate  that  our  proposed  architecture  consistently  sets  the  new  state-of-the-art  on  all  these  four  benchmarks  while  being  the  most  efficient  and  fast  panoptic  segmentation  architecture  to  date.
0	Curriculum  manager  for  source  selection  in  multi  source  domain  adaptation.  The  performance  of  Multi-Source  Unsupervised  Domain  Adaptation  depends  significantly  on  the  effectiveness  of  transfer  from  labeled  source  domain  samples.  In  this  paper,  we  proposed  an  adversarial  agent  that  learns  a  dynamic  curriculum  for  source  samples,  called  Curriculum  Manager  for  Source  Selection  (CMSS).  The  Curriculum  Manager,  an  independent  network  module,  constantly  updates  the  curriculum  during  training,  and  iteratively  learns  which  domains  or  samples  are  best  suited  for  aligning  to  the  target.  The  intuition  behind  this  is  to  force  the  Curriculum  Manager  to  constantly  re-measure  the  transferability  of  latent  domains  over  time  to  adversarially  raise  the  error  rate  of  the  domain  discriminator.  CMSS  does  not  require  any  knowledge  of  the  domain  labels,  yet  it  outperforms  other  methods  on  four  well-known  benchmarks  by  significant  margins.  We  also  provide  interpretable  results  that  shed  light  on  the  proposed  method.
0	Agm  meets  abstract  argumentation  expansion  and  revision  for  dung  frameworks.  In  this  paper  we  combine  two  of  the  most  important  areas  of  knowledge  representation,  namely  belief  revision  and  (abstract)  argumentation.  More  precisely,  we  show  how  AGM-style  expansion  and  revision  operators  can  be  defined  for  Dung's  abstract  argumentation  frameworks  (AFs).  Our  approach  is  based  on  a  reformulation  of  the  original  AGM  postulates  for  revision  in  terms  of  monotonic  consequence  relations  for  AFs.  The  latter  are  defined  via  a  new  family  of  logics,  called  Dung  logics,  which  satisfy  the  important  property  that  ordinary  equivalence  in  these  logics  coincides  with  strong  equivalence  for  the  respective  argumentation  semantics.  Based  on  these  logics  we  define  expansion  as  usual  via  intersection  of  models.  We  show  the  existence  of  such  operators.  This  is  far  from  trivial  and  requires  to  study  realizability  in  the  context  of  Dung  logics.  We  then  study  revision  operators.  We  show  why  standard  approaches  based  on  a  distance  measure  on  models  do  not  work  for  AFs  and  present  an  operator  satisfying  all  postulates  for  a  specific  Dung  logic.
0	Implementation  of  depth  hog  based  human  upper  body  detection  on  a  mini  pc  using  a  low  cost  stereo  camera.  In  this  paper,  we  propose  a  human  upper  body  detection  using  a  depth  image  that  is  implemented  on  a  mini  PC  using  a  low  cost  stereo  camera.  The  camera,  named  Minoru  3D  webcam,  produces  depth  image  from  its  two  parallel  cameras.  A  pyramid-based  region  of  interest  (RoI)  is  applied  on  the  depth-image  frames  to  scan  the  possibility  of  human  upper  body  existence.  Simultaneously,  a  histogram  of  oriented  gradient  (HOG)  method  is  performed  in  the  ROI  to  extract  the  HOG  feature.  The  results  of  the  HOG  feature  extraction  are  then  classified  using  linear  support  vector  machine  (SVM).  Our  system  has  been  experimentally  tested  publicly  in  the  campus  environment.  The  detection  speed  obtained  from  a  computer  is  8.53  fps  and  the  Mini  PC  is  4.26  fps  under  non-threaded  programming.  The  result  of  object  detection  using  HOG  and  SVM  Classification  method  on  static  image  reaches  an  average  accuracy  of  78.71%.  Testing  the  system  for  real  implementation  has  average  error  accuracy  4.60%.  The  results  of  detection  for  two  human  objects  can  achieve  an  accuracy  71.00%.  Moving  image  testing  has  an  average  error  of  4.60%.
0	Using  ontology  for  providing  content  recommendation  based  on  learning  styles  inside  e  learning.  E-Learning  as  one  of  the  learning  support  facilities  provides  various  content  types  and  interaction  models  inside.  This  wide  range  of  content  types  inside  e-Learning  can  be  used  for  accommodating  differences  in  learning  styles  among  the  students.  In  this  research,  we  develop  concept  mapping  between  student  characteristics  and  categories  by  Felder-Silverman  Learning  Style  Model  and  appropriate  content  inside  a  Moodle-based  e-Learning.  This  mapping  is  represented  in  the  ontology  and  then  implemented  in  Moodle-based  e-Learning  system  for  giving  content  recommendation  to  students  based  on  their  learning  styles.  There  are  some  concepts  that  become  basic  definition  of  learning  styles  and  e-Learning  contents,  and  also  some  rules  that  is  used  for  inferring  content  recommendation  from  the  basic  definition.
0	Dropout  and  dropconnect  for  reliable  neuromorphic  inference  under  energy  and  bandwidth  constraints  in  network  connectivity.  DropOut  and  DropConnect  are  known  as  effective  methods  to  improve  on  the  generalization  performance  of  neural  networks,  by  either  dropping  states  of  neural  units  or  dropping  weights  of  synaptic  connections  randomly  selected  at  each  time  instance  throughout  the  training  process.  In  this  paper,  we  extend  on  the  use  of  these  methods  in  the  design  of  neuromorphic  spiking  neural  networks  (SNN)  hardware  to  improve  further  on  the  reliability  of  inference  as  impacted  by  resource  constrained  errors  in  network  connectivity.  Such  energy  and  bandwidth  constraints  arise  for  low-power  operation  in  the  communication  between  neural  units,  which  cause  dropped  spike  events  due  to  timeout  errors  in  the  transmission.  The  DropOut  and  DropConnect  processes  during  training  of  the  network  are  aligned  with  a  statistical  model  of  the  network  during  inference  that  accounts  for  these  random  errors  in  the  transmission  of  neural  states  and  synaptic  connections.  The  use  of  DropOut  and  DropConnect  during  training  hence  allows  to  simultaneously  meet  two  design  objectives:  maximizing  bandwidth,  while  minimizing  energy  of  inference  in  neuromorphic  hardware.  Simulations  of  the  model  with  a  5-layer  fully  connected  784-500-500-500-10  SNN  on  the  MNIST  task  show  a  5-fold  and  10-fold  improvement  in  bandwidth  during  inference  at  greater  than  98%  accuracy,  using  DropOut  and  DropConnect  respectively  during  backpropagation  training.
0	Balance  between  complexity  and  quality  local  search  for  minimum  vertex  cover  in  massive  graphs.  The  problem  of  finding  a  minimum  vertex  cover  (MinVC)  in  a  graph  is  a  well  known  NP-hard  problem  with  important  applications.  There  has  been  much  interest  in  developing  heuristic  algorithms  for  finding  a  "good"  vertex  cover  in  graphs.  In  practice,  most  heuristic  algorithms  for  MinVC  are  based  on  the  local  search  method.  Previously,  local  search  algorithms  for  MinVC  have  focused  on  solving  academic  benchmarks  where  the  graphs  are  of  relatively  small  size,  and  they  are  not  suitable  for  solving  massive  graphs  as  they  usually  have  highcomplexity  heuristics.  In  this  paper,  we  propose  a  simple  and  fast  local  search  algorithm  called  FastVC  for  solving  MinVC  in  massive  graphs,  which  is  based  on  two  low-complexity  heuristics.  Experimental  results  on  a  broad  range  of  real  world  massive  graphs  show  that  FastVC  finds  much  better  vertex  covers  (and  thus  also  independent  sets)  than  state  of  the  art  local  search  algorithms  for  MinVC.
0	Realistic  data  synthesis  using  enhanced  generative  adversarial  networks.  Real  data  with  privacy  and  confidentiality  concerns  are  not  often  available  or  are  too  expensive  to  afford  in  respect  of  both  time  and  money.  In  this  situation,  it  is  a  good  alternative  to  use  synthetic  data.  The  objective  of  this  research  is  to  generate  realistic  synthetic  data  so  that  people  can  use  it  freely.  We  propose  a  synthetic  data  generation  model  based  on  boundary-seeking  generative  adversarial  networks  (BGANs)–designated  as  medical  BGAN  or  medBGAN  and  compare  its  performances  with  an  existing  method  medical  GAN  (medGAN).  We  aim  to  perform  the  investigation  on  several  datasets  in  two  different  domains:  electronic  health  records  (EHRs)  in  the  medical  domain  and  a  crime  dataset  in  the  City  of  Los  Angeles  Police  Department.  Firstly,  we  train  the  models  and  generate  synthetic  data  by  using  these  trained  models.  We  then  analyze  and  compare  the  models'  performance  by  applying  some  statistical  methods  (dimension-wise  average  and  Kolmogorov-Smirnov  test)  and  two  machine  learning  tasks  (association  rule  mining  and  prediction).  The  comprehensive  analysis  of  this  study  shows  that  the  proposed  model  is  more  efficient  in  generating  realistic  synthetic  data  than  those  generated  using  medGAN.
0	Fuzzy  brain  storm  optimization  and  adaptive  thresholding  for  multimodal  vein  based  recognition  system.  Nowadays,  conventional  security  method  of  using  passwords  can  be  easily  forged  by  unauthorized  person.  Hence,  biometric  cues  such  as  fingerprints,  voice,  palm  print,  and  face  are  more  preferable  for  recognition  but  to  preserve  the  liveliness,  another  one  important  biometric  trait  is  vein  pattern,  which  is  formed  by  the  subcutaneous  blood  vessels  that  contain  all  the  achievable  recognition  properties.  Accordingly,  in  this  paper,  we  propose  a  multibiometric  system  using  palm  vein,  hand  vein,  and  finger  vein.  Here,  Holoentropy-based  thresholding  mechanism  is  newly  developed  for  extracting  the  vein  patterns.  Also,  Fuzzy  Brain  Storm  Optimization  (FBSO)  method  is  proposed  for  score  level  fusion  to  achieve  the  better  recognition  performance.  These  two  contributions  are  effectively  included  in  the  biometric  recognition  system  and  the  performance  analysis  of  the  proposed  method  is  carried  out  using  the  benchmark  datasets  of  palm  vein  image,  finger  vein  image,  and  hand  vein  image.  The  quantitative  results  are  analy...
0	A  regularized  locality  projection  based  sparsity  discriminant  analysis  for  face  recognition.  Manifold  learning  and  classifiers  based  on  sparse  representation  are  widely  used  in  pattern  recognition.  Most  of  the  conventional  manifold  learning  methods  are  subjected  to  the  choice  of  parameters.  In  this  paper,  we  present  a  Regularized  Locality  Projection  based  on  Sparsity  Discriminant  Analysis  (RLPSD)  method  for  Feature  Extraction  (FE)  to  understand  the  high-dimensional  data  such  as  face  images.  In  RLPSD,  firstly,  we  show  the  sparse  representation  of  training  samples  by  collaborative  representation-based  classification  (CRC).  Secondly,  the  idea  of  part  optimization  based  on  sparse  representation  is  used  to  ensure  the  within-class  compactness  which  combines  with  the  labels  of  measurements  and  the  weights  of  sparse  presentation  can  be  as  small  as  possible.  Finally,  whole  optimization  can  be  directly  obtained  without  the  iteration  of  local  optimization.  Meanwhile,  the  separability  information  of  between-class  can  be  well  discriminated  by  scatter  matrix  which  is  similar  to  Fisher  linear  discriminant  analy...
0	Form  factors  of  modeling  language  under  different  color  schemes  with  grey  relational  analysis  based  on  entropy  method.  This  paper  proposed  an  innovative  method  based  on  the  semantic  differential  of  Kansei  Engineering  and  the  correlation  analysis  of  machine  learning  in  the  case  of  small-size  samples,  which  could  ach...
0	The  modeling  method  of  a  vibrating  screen  efficiency  prediction  based  on  kpca  and  ls  svm.  A  vibrating  screen  efficiency  prediction  modeling  method  based  on  autoregressive  (AR)  model  and  least  square  support  vector  machine  (LS-SVM)  was  proposed.  The  vibration  signals  of  a  self-synchroniz...
0	Data  mining  in  market  segmentation  a  literature  review  and  suggestions.  The  importance  of  data  mining  techniques  for  market  segmentation  is  becoming  indispensable  in  the  field  of  marketing  research.  This  is  the  first  identified  academic  literature  review  of  the  available  data  mining  techniques  related  to  market  segmentation.  This  research  paper  provides  surveys  of  the  available  literature  on  data  mining  techniques  in  market  segmentation.  A  categorization  has  been  provided  based  on  the  available  data  mining  techniques  used  in  market  segmentation.  Eight  online  journal  databases  were  used  for  searching,  and  finally,  103  articles  were  selected  and  categorized  into  13  groups  based  on  data  mining  techniques.  The  utility  of  data  mining  techniques  and  suggestions  are  also  discussed.  The  findings  of  this  study  show  that  neural  networks  is  the  most  used  method,  and  kernel-based  method  is  the  most  promising  data  mining  techniques.  Our  research  work  provides  a  comprehensive  understanding  of  past,  present  as  well  as  future  research  trend  on  data  mining  techniques  in  market  segmentation.  We  hope  this  paper  provides  reasonable  insight  and  clear  understating  to  both  industry  as  well  as  academic  researchers.
0	Ga  based  scheduling  of  fms  using  roulette  wheel  selection  process.  FMS  Scheduling  problem  is  one  of  the  most  difficult  NP-hard  combinatorial  optimization  problems.
0	Time  varying  correlation  estimation  using  probabilistic  fuzzy  systems.  Accurate  financial  risk  analysis  has  drawn  considerable  attention  after  the  recent  financial  crisis.  Several  regulatory  agencies  recently  documented  the  need  for  proper  assessment  and  reporting  of  financial  risk  for  banks  and  other  financial  institutions.  It  is  stressed  that  risk  analysis  should  take  into  account  changing  risk  properties  over  time.  For  a  set  of  financial  assets,  risk  analysis  relies  on  the  correlation  and  covariance  structure  among  these  returns  from  these  assets.  Therefore  analyzing  changes  in  the  correlations  and  covariances  of  assets  is  essential  to  document  changing  risk  properties.  In  this  paper  we  show  that  a  PFS  can  be  used  to  model  unobserved  time-varying  correlation  between  financial  returns.  The  method  is  applied  to  simulated  data  and  real  data  of  daily  NASDAQ  and  HSI  stock  returns.  We  show  that  the  PFS  application  improves  over  the  conventional  moving  window  approximation  of  time-varying  correlation  by  decreasing  the  sensitivity  of  the  results  to  the  selection  of  the  window  length.
0	Experimental  study  on  response  time  of  magnetorheological  damper.  Magnetorheological  (MR)  damper  is  a  kind  of  superiorly  semi-active  control  device  whose  response  time  directly  affects  its  vibration  reduction.  This  paper  analyzed  the  mechanism  of  response  time  through  magnetorheological  fluid,  control  electric  circuit  and  magnetic  circuit,  and  the  flexibility  of  mechanism.  Based  on  above  analysis,  response  time  of  MR  damper  was  tested  using  the  Material  Testing  System  (MTS)  810  when  electric  current,  excited  frequency  and  excited  amplitude  were  changing.  And  the  experimental  data  were  analyzed  with  the  mathematical  statistical  method  and  the  variation  tendencies  of  the  response  time  following  the  changing  of  electric  current  and  velocity  were  shown,  which  could  provide  a  reference  for  the  improvement  of  MR  dampers  and  the  design  of  vibration  control  algorithm.
1	Using  timed  base  choice  coverage  criterion  for  testing  industrial  control  software.  The  base-choice  criterion  was  proposed  as  a  suitable  technique  for  testing  software  based  on  its  nominal  choice  of  input  parameters.  Test  cases  are  created  based  on  this  strategy  by  varying  the  values  of  one  input  parameter  at  a  time  while  keeping  the  values  of  the  other  parameters  fixed  on  the  base  choice.  However,  this  strategy  might  not  be  as  effective  when  used  on  industrial  control  software  for  testing  timed  behavior.  We  propose  to  incorporate  time  as  another  parameter  when  generating  and  executing  tests  by  defining  the  timed  base-choice  coverage  criterion.  We  performed  an  empirical  evaluation  using  11  industrial  programs  written  in  the  IEC  61131-3  programming  language.  We  found  that  tests  generated  for  timed  base-choice  criterion  show  better  code  coverage  (7%  improvement)  and  fault  detection  (27%  improvement)  in  terms  of  mutation  score  than  tests  satisfying  base-choice  coverage  criterion.  The  results  demonstrate  the  feasibility  of  applying  timed  base-choice  criterion  for  testing  industrial  control  software.
1	Requirements  for  testing  and  validating  the  industrial  internet  of  things.  The  latest  advances  in  industry  have  been  accomplished  within  the  4th  Industrial  Revolution,  mostly  noted  as  Industrie  4.0.  This  industrial  revolution  is  boosted  by  the  application  of  Internet  of  Things  (IoT)  technologies  into  the  industrial  contexts,  also  known  as  Industrial  Internet  of  Things  (IIoT),  which  is  being  supported  by  the  implementation  of  Cyber-Physical  Production  Systems  (CPPS).  In  this  context,  most  of  the  existing  work  concentrates  on  developing  IIoT  models  and  CPPS  architectures,  laking  the  identification  of  validation  requirements  of  these  platforms.  By  rushing  into  releasing  state-of-the-art  IIoT  applications,  developers  usually  forget  to  implement  methodologies  to  validate  these  applications.  In  this  paper,  we  propose  a  list  of  requirements  for  IIoT  platform  validation,  based  on  its  architecture,  as  well  as  in  requirements  established  by  the  industrial  reality.  A  CPPS  case  study  is  presented,  in  order  to  illustrate  some  of  these  requisites  and  how  validation  of  these  type  of  system  could  be  achieved.
1	Automatic  model  inference  of  web  applications  for  security  testing.  In  the  Internet  of  services  (IoS),  web  applications  are  the  most  common  way  to  provide  resources  to  the  users.  The  complexity  of  these  applications  grew  up  with  the  number  of  different  development  techniques  and  technologies  used.  Model-based  testing  (MBT)  has  proved  its  efficiency  in  software  testing  but  retrieving  the  corresponding  model  of  an  application  is  still  a  complex  task.  In  this  paper,  we  propose  an  automatic  and  vulnerability-driven  model  inference  approach  to  model  the  relevant  aspects  of  a  web  applications  by  combining  deep  web  crawling  and  model  inference  based  on  input  sequences.
1	An  experience  report  on  using  code  smells  detection  tools.  Detecting  code  smells  in  the  code  and  consequently  applying  the  right  refactoring  steps  when  necessary  is  very  important  to  improve  the  quality  of  the  code.  Different  tools  have  been  proposed  for  code  smell  detection,  each  one  characterized  by  particular  features.  The  aim  of  this  paper  is  to  describe  our  experience  on  using  different  tools  for  code  smell  detection.  We  outline  the  main  differences  among  them  and  the  different  results  we  obtained.
1	Applying  automated  test  case  generation  in  industry  a  retrospective.  Automated  test  case  generation  promises  to  reduce  the  high  effort  of  manually  developing  and  maintaining  test  cases,  to  improve  the  effectiveness  of  testing,  and  to  speed-up  testing  cycles.  Research  on  generating  test  cases  has  advanced  over  the  past  decades  and  today  a  wide  range  of  techniques  and  tools  are  available,  including  studies  showing  their  successful  evaluation  in  real-world  scenarios.  We  conducted  a  multi-firm  research  project  on  automated  software  testing  that  involved  the  application  of  automated  test  case  generation  approaches  in  industry  projects.  This  paper  provides  a  retrospective  on  the  related  activities.  It  reports  on  our  observations  and  insights  from  applying  automated  test  case  generation  in  practice,  identifies  pitfalls  and  gaps  in  current  research,  and  summarizes  lessons  learned  from  transferring  software  testing  research  results  to  industry.
1	A  process  to  increase  the  model  quality  in  the  context  of  model  based  testing.  In  the  past  years  model-based  testing  (MBT)  has  become  a  widely-used  approach  to  the  test  automation  in  the  industrial  context.  Until  now  the  application  of  MBT  has  been  limited  to  the  software  quality  engineers  with  very  good  modelling  skills.  In  order  to  guarantee  the  completeness  of  a  model  and  to  increase  its  precision  there  is  a  need  to  allow  the  usage  of  the  approach  by  other  project  stakeholders  such  as  requirements  engineers  as  well  as  software  quality  engineers  with  a  limited  modelling  experience.  In  this  contribution  we  share  the  challenges  discovered  during  the  several  years  of  the  application  of  a  certain  MBT  technique  in  a  SCRUM  project  with  particular  regard  to  the  definition  of  precise  and  complete  models.  A  process  which  involves  the  entire  software  project  team  into  the  model  definition  starting  at  the  very  early  stages  of  product  development  is  presented  along  with  its  concrete  implementation.  First  experiences  with  the  application  of  the  process  in  a  particular  project  are  presented.1
1	Identifying  failure  inducing  combinations  using  tuple  relationship.  Combinatorial  testing  (CT)  aims  at  detecting  interaction  failures  between  parameters  in  a  system.  Identifying  the  failure-inducing  combinations  of  a  failing  test  configuration  can  help  developers  find  the  cause  of  this  failure.  However,  most  studies  in  CT  focus  on  detecting  the  failures  rather  than  identifying  failure-inducing  combinations.  In  this  paper,  we  propose  the  notion  of  a  tuple  relationship  tree  (TRT)  to  describe  the  relationships  among  all  the  candidate  parameter  interactions.  TRT  reduces  additional  test  configurations  that  need  to  be  generated  in  the  fault  localization  process,  and  it  also  provides  a  clear  view  of  all  possible  candidate  interactions.  As  a  result,  our  approach  will  not  omit  any  possible  interaction  that  could  be  the  cause  of  a  failure.  In  particular,  we  can  identify  multiple  failure-inducing  combinations  that  overlap  with  each  other.  Moreover,  we  extend  our  approach  to  handle  the  case  where  additional  failure-inducing  combinations  may  be  introduced  by  newly  generated  test  configurations.
1	Software  based  remote  attestation  for  safety  critical  systems.  Assuring  system  integrity  to  a  remote  communication  partner  through  attestation  is  a  security  concept  which  also  is  very  important  for  safety-critical  systems  facing  security  threats.  Most  remote  attestation  methods  are  based  on  integrity  measurement  mechanisms  embedded  in  the  underlying  hardware  or  software  (e.g.  operating  system).  Alternatively,  the  application  software  can  measure  itself,  whereas  the  security  of  this  approach  relies  on  obscurity  of  the  measurement  mechanism.  There  are  several  tools  available  to  introduce  such  obscurity  through  automatic  code  transformations,  but  these  tools  cannot  be  applied  to  safety-critical  systems,  because  automatic  code  transformations  are  difficult  to  justify  during  safety  certification.  We  present  a  software-based  remote  attestation  concept  for  safety-critical  systems  and  apply  it  to  an  automation  system  case  study.  The  attestation  concept  utilizes  the  safety-related  black  channel  principle  to  allow  the  application  of  code  protection  tools  in  order  to  protect  the  attestation  mechanism  without  increasing  the  safety  certification  effort  for  the  system.
1	All  for  the  price  of  few.  We  present  a  simple  and  efficient  framework  for  automatic  verification  of  systems  with  a  parameteric  number  of  communicating  processes.  The  processes  may  be  organized  in  various  topologies  such  as  words,  multisets,  rings,  or  trees.  Our  method  needs  to  inspect  only  a  small  number  of  processes  in  order  to  show  correctness  of  the  whole  system.  It  relies  on  an  abstraction  function  that  views  the  system  from  the  perspective  of  a  fixed  number  of  processes.  The  abstraction  is  used  during  the  verification  procedure  in  order  to  dynamically  detect  cut-off  points  beyond  which  the  search  of  the  state  space  need  not  continue.  We  show  that  the  method  is  complete  for  a  large  class  of  well  quasi-ordered  systems  including  Petri  nets.  Our  experimentation  on  a  variety  of  benchmarks  demonstrate  that  the  method  is  highly  efficient  and  that  it  works  well  even  for  classes  of  systems  with  undecidable  verification  problems.
1	Sat  based  model  checking  without  unrolling.  A  new  form  of  SAT-based  symbolic  model  checking  is  described.  Instead  of  unrolling  the  transition  relation,  it  incrementally  generates  clauses  that  are  inductive  relative  to  (and  augment)  stepwise  approximate  reachability  information.  In  this  way,  the  algorithm  gradually  refines  the  property,  eventually  producing  either  an  inductive  strengthening  of  the  property  or  a  counterexample  trace.  Our  experimental  studies  show  that  induction  is  a  powerful  tool  for  generalizing  the  unreachability  of  given  error  states:  it  can  refine  away  many  states  at  once,  and  it  is  effective  at  focusing  the  proof  search  on  aspects  of  the  transition  system  relevant  to  the  property.  Furthermore,  the  incremental  structure  of  the  algorithm  lends  itself  to  a  parallel  implementation.
1	Towards  complete  reasoning  about  axiomatic  specifications.  To  support  verification  of  expressive  properties  of  functional  programs,  we  consider  algebraic  style  specifications  that  may  relate  multiple  user-defined  functions,  and  compare  multiple  invocations  of  a  function  for  different  arguments.  We  present  decision  procedures  for  reasoning  about  such  universally  quantified  properties  of  functional  programs,  using  local  theory  extensionmethodology.  We  establish  new  classes  of  universally  quantified  formulas  whose  satisfiability  can  be  checked  in  a  complete  way  by  finite  quantifier  instantiation.  These  classes  include  single-invocation  axioms  that  generalize  standard  function  contracts,  but  also  certain  many-invocation  axioms,  specifying  that  functions  satisfy  congruence,  injectivity,  or  monotonicity  with  respect  to  abstraction  functions,  as  well  as  conjunctions  of  some  of  these  properties.  These  many-invocation  axioms  can  specify  correctness  of  abstract  data  type  implementations  as  well  as  certain  information-flow  properties.  We  also  present  a  decidability-preserving  construction  that  enables  the  same  function  to  be  specified  using  different  classes  of  decidable  specifications  on  different  partitions  of  its  domain.
1	Towards  efficient  parameterized  synthesis.  Parameterized  synthesis  was  recently  proposed  as  a  way  to  circumvent  the  poor  scalability  of  current  synthesis  tools.  The  method  uses  cut-off  results  in  token  rings  to  reduce  the  problem  to  bounded  distributed  synthesis,  and  thus  ultimately  to  a  sequence  of  SMT  problems.  This  solves  the  problem  of  scalability  in  the  size  of  the  architecture,  but  experiments  show  that  the  size  of  the  specification  is  still  a  major  issue.  In  this  paper  we  propose  several  optimizations  of  the  approach.  First,  we  tailor  the  SMT  encoding  to  systems  with  isomorphic  processes  and  token-ring  architecture.  Second,  we  extend  the  cut-off  results  for  token  rings  and  refine  the  reduction,  using  modularity  and  abstraction  techniques.  Some  of  our  optimizations  also  apply  to  isomorphic  or  distributed  synthesis  in  arbitrary  architectures.  To  evaluate  these  optimizations,  we  developed  the  first  completely  automatic  implementation  of  parameterized  synthesis.  Experiments  show  a  speed-up  of  several  orders  of  magnitude,  compared  to  the  original  method.
1	A  cloud  adoption  decision  support  model  based  on  fuzzy  cognitive  maps.  Cloud  Computing  has  become  nowadays  a  significant  field  of  Information  and  Communication  Technology  (ICT).  Both  cloud  providers  and  customers  invest  time  and  resources  in  an  endeavor  of  the  former  to  serve  effectively  the  needs  of  the  latter  so  as  to  adopt  efficiently  such  cloud  services,  based  their  needs.  The  decision  to  adopt  cloud  services  falls  within  the  category  of  complex  and  difficult  to  model  real-world  problems.  Aiming  to  support  the  cloud  adoption  decision  process,  we  propose  in  this  paper  an  approach  based  on  Fuzzy  Cognitive  Maps  (FCM)  which  models  the  parameters  that  potentially  influence  such  a  decision.  The  construction  and  analysis  of  the  map  is  based  on  factors  reported  in  the  relevant  literature  and  the  utilization  of  experts’  opinion.  The  proposed  approach  is  evaluated  through  four  real-world  experimental  cases  and  the  suggestions  of  the  model  are  compared  with  the  customers’  final  decisions.  The  evaluation  indicated  that  the  proposed  approach  is  capable  of  capturing  the  dynamics  behind  the  interdependencies  of  the  participating  factors.
1	Minimizing  vehicular  travel  times  using  the  multi  agent  system  beejama.  We  present  and  evaluate  our  self-adaptive  and  distributed  vehicle  route  guidance  approach,  termed  BeeJamA,  which  provides  drivers  safely  with  routing  directions  well  before  each  intersection.  Our  approach  is  based  on  a  multi-agent  system  which  is  inspired  by  the  honey  bee  behavior  and  relies  on  a  decentralized  vehicle-to-infrastructure  architecture.  On  the  basis  of  microscopic  traffic  simulations  under  varying  penetration  rates  it  shows  that  BeeJamA  has  the  tendency  to  outperform  dynamic  shortest  path  algorithms  with  respect  to  (global)  travel  times.
1	Non  functional  requirements  documentation  in  agile  software  development  challenges  and  solution  proposal.  Non-functional  requirements  (NFRs)  are  determinant  for  the  success  of  software  projects.  However,  they  are  characterized  as  hard  to  define,  and  in  agile  software  development  (ASD),  are  often  given  less  priority  and  usually  not  documented.  In  this  paper,  we  present  the  findings  of  the  documentation  practices  and  challenges  of  NFRs  in  companies  utilizing  ASD  and  propose  guidelines  for  enhancing  NFRs  documentation  in  ASD.  We  interviewed  practitioners  from  four  companies  and  identified  that  epics,  features,  user  stories,  acceptance  criteria,  Definition  of  Done  (DoD),  product  and  sprint  backlogs  are  used  for  documenting  NFRs.  Wikis,  word  documents,  mockups  and  spreadsheets  are  also  used  for  documenting  NFRs.  In  smaller  companies,  NFRs  are  communicated  through  white  board  and  flip  chart  discussions  and  developers’  tacit  knowledge  is  prioritized  over  documentation.  However,  loss  of  traceability  of  NFRs,  the  difficulty  in  comprehending  NFRs  by  new  developers  joining  the  team  and  limitations  of  documentation  practices  for  NFRs  are  challenges  in  ASD.  In  this  regard,  we  propose  guidelines  for  documenting  NFRs  in  ASD.  The  proposed  guidelines  consider  the  diversity  of  the  NFRs  to  document  and  suggest  different  representation  artefacts  depending  on  the  NFRs  scope  and  level  of  detail.  The  representation  artefacts  suggested  are  among  those  currently  used  in  ASD  in  order  not  to  introduce  new  specific  ones  that  might  hamper  actual  adoption  by  practitioners.
1	Organizational  support  for  process  improvement  results  of  an  international  survey.  Organizational  support  for  process  improvement  initiatives  is  vital  to  the  success  of  these  improvements.  This  paper  describes  and  explores  the  steps  of  supporting  process  improvements  described  in  the  new  international  standard  ISO/IEC  33014  that  is  currently  being  developed.
1	A  case  study  on  the  need  to  consider  personality  types  for  software  team  formation.  Software  development  is  a  social  activity  and  the  formation  of  the  right  team  is  a  critical  success  factor.  Although  personality  types  in  software  teams  and  software  projects’  success  criterias  have  been  studied  before,  there  is  no  well  formed  methodology  for  establishing  software  teams  according  to  the  personality  types.  This  study  is  performed  to  search  the  relation  between  software  team  members’  personality  types  and  project  success.  To  achive  this  goal,  a  questionnaire  based  approach  is  developed  to  measure  project  success  and  personality  types.  Two  software  development  projects  are  assessed  with  a  questionnaire  that  assesses  project  success  in  different  aspects.  Also,  all  project  team  members  are  assessed  with  respect  to  their  personality  types.  Results  provide  insight  that,  personality  type  consideration  while  forming  software  teams  can  play  a  significant  role  in  project  success.
1	Supporting  the  validation  of  adequacy  in  requirements  based  hazard  mitigations.  [Context  and  motivation]  In  practice,  validating  functional  safety  requirements  is  mainly  done  by  means  of  reviews,  which  require  large  amounts  of  contextual  information  about  hazards,  such  as  safety  goals  or  the  operational  conditions  under  which  the  hazard  occurs.  [Question/problem]  This  information  is  often  scattered  across  a  plethora  of  artifacts  produced  particularly  during  requirements  engineering  and  safety  assessment.  In  consequence,  there  is  a  risk  that  not  all  relevant  information  is  considered  during  reviews,  leading  to  subjective  and  misjudged  results.  [Principal  ideas/results]  In  order  to  improve  the  consideration  of  all  relevant  information  necessary  to  validate  functional  safety  requirements,  we  propose  a  diagrammatic  representation  integrating  all  relevant  contextual  information.  [Contribution]  We  hypothesize  that  reviewers  are  more  likely  to  base  their  judgment  on  the  relevant  contextual  information  about  the  hazard,  which  increases  objectivity  and  confidence  in  review  results.  To  support  this  hypothesis,  we  report  preliminary  results  of  an  empirical  study.
1	Capturing  ambiguity  in  artifacts  to  support  requirements  engineering  for  self  adaptive  systems.  Self-adaptive  systems  (SAS)  automatically  adjust  their  behavior  at  runtime  in  order  to  manage  changes  in  their  user  requirements  and  operating  context.  To  achieve  this  goal,  a  SAS  needs  to  carry  knowledge  in  artifacts  (e.g.,  contextual  goal  models)  at  runtime.  However,  identifying,  representing,  and  refining  requirements  and  their  context  to  create  and  maintain  such  artifacts  at  runtime  is  a  challenging  task,  especially  if  the  runtime  environment  is  not  very  well  known.  In  this  short  paper,  we  present  an  early  concept  to  requirements  engineering  for  the  implementation  of  SAS  in  the  context  of  uncertainty.  Especially  the  wide  variety  of  knowledge  materialized  in  artifacts  created  during  software  engineering  activities  at  design  time  is  considered.  We  propose  to  start  with  a  list  of  ambiguous  requirements-or  under-specified  requirements  leaving  ng  the  ambiguity  in  the  requirements,  which  will  in  the  later  steps  be  resolved  further  as  more  information  is  known.  In  contrast  to  conventional  requirements  engineering  approaches,  not  all  ambiguous  requirements  will  be  resolved.  Instead,  ambiguities  serve  as  key  input  for  self-adaptation.  We  present  five  steps  for  the  resolution  of  the  ambiguity.  For  each  step,  we  describe  its  purpose,  identified  challenges,  and  resolution  ideas.
1	Supporting  learning  organisations  in  writing  better  requirements  documents  based  on  heuristic  critiques.  Context  &  motivation:  Despite  significant  advances  in  requirements  engineering  (RE)  research  and  practice,  software  developing  organisations  still  struggle  to  create  requirements  documentation  in  sufficient  quality  and  in  a  repeatable  way.  Question/problem:  The  notion  of  good-enough  quality  is  domain  and  project  specific.  Software  developing  organisations  need  concepts  that  i)  allow  adopting  a  suitable  set  of  RE  methods  for  their  domain  and  projects  and  ii)  allow  improving  these  methods  continuously.  Principal  ideas/results:  Automatic  analysis  of  requirements  documentation  can  support  a  process  of  organisational  learning.  Such  approaches  help  improve  requirements  documents,  but  can  also  start  a  discussion  about  its  desired  quality.  Contribution:  We  present  a  learning  model  based  on  heuristic  critiques.  The  paper  shows  how  this  concept  can  support  learning  on  both  the  organisational  and  individual  levels.
1	Trace  queries  for  safety  requirements  in  high  assurance  systems.  [Context  and  motivation]  Safety  critical  software  systems  pervade  almost  every  facet  of  our  lives.  We  rely  on  them  for  safe  air  and  automative  travel,  healthcare  diagnosis  and  treatment,  power  generation  and  distribution,  factory  robotics,  and  advanced  assistance  systems  for  special-needs  consumers.  [Question/Problem]  Delivering  demonstrably  safe  systems  is  difficult,  so  certification  and  regulatory  agencies  routinely  require  full  life-cycle  traceability  to  assist  in  evaluating  them.  In  practice,  however,  the  traceability  links  provided  by  software  producers  are  often  incomplete,  inaccurate,  and  ineffective  for  demonstrating  software  safety.  Also,  there  has  been  insufficient  integration  of  formal  method  artifacts  into  such  traceability.  [Principal  ideas/results]  To  address  these  weaknesses  we  propose  a  family  of  reusable  traceability  queries  that  serve  as  a  blueprint  for  traceability  in  safety  critical  systems.  In  particular  we  present  queries  that  consider  formal  artifacts,  designed  to  help  demonstrate  that:  1)  identified  hazards  are  addressed  in  the  safety-related  requirements,  and  2)  the  safety-related  requirements  are  realized  in  the  implemented  system.  We  model  these  traceability  queries  using  the  Visual  Trace  Modeling  Language,  which  has  been  shown  to  be  more  intuitive  than  the  defacto  SQL  standard.  [Contribution]  Practitioners  building  safety  critical  systems  can  use  these  trace  queries  to  make  their  traceability  efforts  more  complete,  accurate  and  effective.  This,  in  turn,  can  assist  in  building  safer  software  systems  and  in  demonstrating  their  adequate  handling  of  hazards.
1	Evaluation  of  techniques  to  detect  wrong  interaction  based  trace  links.  [Context  and  Motivation]  In  projects  where  trace  links  are  created  and  used  continuously  during  the  development,  it  is  important  to  support  developers  with  an  automatic  trace  link  creation  approach  with  high  precision.  In  our  previous  study  we  showed  that  our  interaction  based  trace  link  creation  approach  achieves  100%  precision  and  80%  relative  recall  and  thus  performs  better  than  traditional  IR  based  approaches.  [Question/problem]  In  this  study  we  wanted  to  confirm  our  previous  results  with  a  data  set  including  a  gold  standard  created  by  developers.  Moreover  we  planned  further  optimization  and  fine  tuning  of  our  trace  link  creation  approach.  [Principal  ideas/results]  We  performed  the  study  within  a  student  project.  It  turned  out  that  in  this  study  our  approach  achieved  only  50%  precision.  This  means  that  developers  also  worked  on  code  not  relevant  for  the  requirement  while  interactions  were  recorded.  In  order  to  improve  precision  we  evaluated  different  techniques  to  identify  relevant  trace  link  candidates  such  as  focus  on  edit  interactions  or  thresholds  for  frequency  and  duration  of  trace  link  candidates.  We  also  evaluated  different  techniques  to  identify  irrelevant  code  such  as  the  developer  who  created  the  code  or  code  which  is  not  related  to  other  code  in  an  interaction  log.  [Contribution]  Our  results  show  that  only  some  of  the  techniques  led  to  a  considerably  improvement  of  precision.  We  could  improve  precision  almost  up  to  70  %  while  keeping  recall  above  45%  which  is  much  better  than  IR-based  link  creation.  The  evaluations  show  that  the  full  benefits  of  an  interaction  based  approach  highly  depend  on  the  discipline  of  the  developers  when  recording  interactions  for  a  specific  requirement.  Further  research  is  necessary  how  to  support  the  application  of  our  approach  in  a  less  disciplined  context.
1	Joint  proceedings  of  the  refsq  2016  co  located  events.  Service-Based  Enterprise  Systems  represents  an  emerging  paradigm  in  software  development.  Over  the  years,  its  usage  has  increased  exponentially  in  fields  such  as  manufacturing,  banking,  telecommunication  and  healthcare  due  to  its  ability  to  promote  reusability,  loosely-coupled  and  scalability  for  software  development.  However  to  accommodate  the  evolution  of  business  requirements  in  SBES,  there  needs  to  be  in  place  a  standard  strategic  approach.  Moreover,  handling  business  requirements  evolution  in  natural  language  is  very  challenging  when  deluge  scale  of  requirements  are  expected.  Therefore,  we  propose  an  approach  to  automate  the  activity  of  evaluating  the  need  to  evolve  phase  which  firstly  conducted  in  service  evolution  life  cycle.  The  proposed  approach  follows  Change-oriented  Service  Life  Cycle  methodology  as  guideline  to  portray  different  phases  in  service  evolution.  As  proof  of  concept  we  will  select  a  few  SBES  projects  to  evaluate  the  feasibility  of  the  proposed  approach.
1	Towards  a  new  understanding  of  small  and  medium  sized  enterprises  in  requirements  engineering  research.  [Context  and  motivation]  Almost  worldwide  the  software  industry  mainly  consists  of  small  and  medium  software  enterprises.  From  a  requirements  engineering  perspective  these  companies  are  poorly  researched.  [Problem]  Though  RE  research  is  discovering  SMEs  as  an  interesting  field,  it  is  difficult  to  categorize  and  distinguish  these  companies  sufficiently.  This  leads  to  a)  weakly  classified  results  of  observational  studies  as  well  as  field  studies  and  empirical  research  and  b)  insufficient  mappings  between  methodical  improvements  and  the  companies  they  can  be  applied  to.  Therefore,  it  is  hard  for  researchers  and  enterprises  to  adopt  RE  state  of  the  art  to  an  enterprises  environment.  [Principal  ideas]  After  defining  the  problem,  initial  ideas  for  attributes  classifying  SMEs  are  presented  and  a  way  for  improving  and  clustering  these  attributes  is  shown.  [Contribution]  This  paper  raises  an  important  problem  statement  for  RE  research  and  shows  an  initial  way  towards  solving  this  problem.
1	Analyzing  the  tracing  of  requirements  and  source  code  during  software  development  a  research  preview.  [Context  and  motivation]  Traceability  links  between  requirements  and  code  are  often  created  after  development,  which  can,  for  example,  lead  to  higher  development  effort.  To  address  this  weakness,  we  developed  in  previous  work  an  approach  that  captures  traceability  links  between  requirements  and  code  as  the  development  progresses  by  using  artifacts  from  project  management  called  work  items.  [Question/problem]  It  is  important  to  investigate  empirically  what  is  the  best  way  to  capture  such  links  and  how  these  links  are  used  during  development.  [Principal  ideas/results]  In  order  to  link  requirements,  work  items  and  code  during  development,  we  extended  our  approach  from  previous  work  by  defining  three  traceability  link  creation  processes.  We  are  applying  these  processes  in  practice  in  a  software  development  project  conducted  with  undergraduate  students.  The  results  indicate  that  our  approach  creates  correct  traceability  links  between  requirements  and  code  with  high  precision/recall  during  development,  while  developers  mainly  used  the  third  process  to  link  work  items  after  implementation.  Furthermore,  the  students  used  a  subset  of  the  created  traceability  links  for  navigating  between  requirements  and  code  during  the  early  phase  of  the  development  project.  [Contribution]  In  this  paper,  we  report  on  preliminary  empirical  results  from  applying  our  approach  in  practice.
1	Use  case  and  requirements  analysis  in  a  remote  rural  context  in  mali.  [Context  &  motivation]  Few  studies  have  reported  on  a  systematic  use  case  and  requirements  analysis  of  low-tech,  low-resource  contexts  such  as  rural  Africa.  This,  despite  the  widespread  agreement  on  the  importance  of  Information  and  Communication  Technologies  (ICT)  for  social  and  rural  development,  and  despite  the  large  number  of  ICT  projects  targeting  underprivileged  communities.  [Question/problem]  Unfamiliarity  with  the  local  context  and  differences  in  cultural  and  educational  backgrounds  between  end-users  and  software  engineers  are  the  challenges  for  requirements  engineering  (RE)  we  encountered.  [Principal  ideas/results]  We  describe  a  systematic  approach  to  RE  in  developing  areas,  based  on  the  Living  Lab  methodology.  Our  approach  is  supported  by  extensive  field  research  and  based  on  co-creation  within  a  multi-disciplinary  and  multi-cultural  team  of  developers  and  users.  This  approach  creates  a  shared  understanding  of  the  problem  and  its  local  context,  and  optimizes  communication.  [Contribution]  We  illustrate  the  approach  using  a  case  study  of  web-  and  voice-based  communication  services,  that  we  developed  for  a  rural  context  in  Mali.
1	Diagnosing  unobserved  components  in  self  adaptive  systems.  Availability  is  an  increasingly  important  quality  for  today's  software-based  systems  and  it  has  been  successfully  addressed  by  the  use  of  closed-loop  control  systems  in  self-adaptive  systems.  Probes  are  inserted  into  a  running  system  to  obtain  information  and  the  information  is  fed  to  a  controller  that,  through  provided  interfaces,  acts  on  the  system  to  alter  its  behavior.  When  a  failure  is  detected,  pinpointing  the  source  of  the  failure  is  a  critical  step  for  a  repair  action.  However,  information  obtained  from  a  running  system  is  commonly  incomplete  due  to  probing  costs  or  unavailability  of  probes.  In  this  paper  we  address  the  problem  of  fault  localization  in  the  presence  of  incomplete  system  monitoring.  We  may  not  be  able  to  directly  observe  a  component  but  we  may  be  able  to  infer  its  health  state.  We  provide  formal  criteria  to  determine  when  health  states  of  unobservable  components  can  be  inferred  and  establish  formal  theoretical  bounds  for  accuracy  when  using  any  spectrum-based  fault  localization  algorithm.
1	Claims  and  supporting  evidence  for  self  adaptive  systems  a  literature  study.  Despite  the  vast  body  of  work  on  self-adaption,  no  systematic  study  has  been  performed  on  the  claims  associated  with  self-adaptation  and  the  evidence  that  exists  for  these  claims.  As  such  an  insight  is  crucial  for  researchers  and  engineers,  we  performed  a  literature  study  of  the  research  results  from  SEAMS  since  2006  and  the  associated  Dagstuhl  seminar  in  2008.  The  study  shows  that  the  primary  claims  of  self-adaptation  are  improved  flexibility,  reliability,  and  performance  of  the  system.  On  the  other  hand,  the  tradeoffs  implied  by  self-adaptation  have  not  received  much  attention.  Evidence  is  obtained  from  basic  examples,  or  simply  lacking.  Few  systematic  empirical  studies  have  been  performed,  and  no  industrial  evidence  is  reported.  From  the  study,  we  offer  the  following  recommendations  to  move  the  field  forward:  to  improve  evaluation,  researchers  should  make  their  assessment  methods,  tools  and  data  publicly  available;  to  deal  with  poor  discussion  of  limitations,  conferences/workshops  should  require  an  explicit  section  on  limitations  in  engineering  papers;  to  improve  poor  treatment  of  tradeoffs,  this  aspect  should  be  an  explicit  subject  of  reviews;  and  finally,  to  enhance  industrial  validation,  the  best  academy-industry  efforts  could  be  formally  recognized  by  the  community.
1	Self  adaptive  applications  on  the  development  of  personalized  web  tasking  systems.  Personalized  Web-Tasking  (PWT)  proposes  the  automation  of  user-centric  and  repetitive  web  interactions  to  assist  users  in  the  fulfilment  of  personal  goals  using  internet  systems.  In  PWT,  both  personal  goals  and  internet  systems  are  affected  by  unpredictable  changes  in  user  preferences,  situations,  system  infrastructures  and  environments.  Therefore,  self-adaptation  enhanced  with  dynamic  context  monitoring  is  required  to  guarantee  the  effectiveness  of  PWT  systems  that,  despite  context  uncertainty,  must  guarantee  the  accomplishment  of  personal  goals  and  deliver  pleasant  user  experiences.  This  position  paper  describes  our  approach  to  the  development  of  PWT  systems,  which  relies  on  self-adaptation  and  its  enabling  technologies.  In  particular,  it  presents  our  runtime  modelling  approach  that  is  comprised  of  our  PWT  Ontology  and  Goal-oriented  Context-sensitive  web-tasking  (GCT)  models,  and  the  way  we  exploit  previous  SEAMS  contributions  developed  in  our  research  group,  the  DYNAMICO  reference  model  and  the  SmarterContext  Monitoring  Infrastructure  and  Reasoning  Engine.  The  main  goal  of  this  paper  is  to  demonstrate  how  the  most  crucial  challenges  in  the  engineering  of  PWT  systems  can  be  addressed  by  implementing  them  as  self-adaptive  software.
1	Undersea  an  exemplar  for  engineering  self  adaptive  unmanned  underwater  vehicles.  Recent  advances  in  embedded  systems  and  underwater  communications  raised  the  autonomy  levels  in  unmanned  underwater  vehicles  (UUVs)  from  human-driven  and  scripted  to  adaptive  and  self-managing.  UUVs  can  execute  longer  and  more  challenging  missions,  and  include  functionality  that  enables  adaptation  to  unexpected  oceanic  or  vehicle  changes.  As  such,  the  simulated  UUV  exemplar  UNDERSEA  introduced  in  our  paper  facilitates  the  development,  evaluation  and  comparison  of  self-adaptation  solutions  in  a  new  and  important  application  domain.  UNDERSEA  comes  with  predefined  oceanic  surveillance  UUV  missions,  adaptation  scenarios,  and  a  reference  controller  implementation,  all  of  which  can  easily  be  extended  or  replaced.
1	Evolving  an  adaptive  industrial  software  system  to  use  architecture  based  self  adaptation.  Although  architecture-based  self-adaptation  has  been  widely  used,  there  is  still  little  understanding  about  the  validity  and  tradeoffs  of  incorporating  it  into  real-world  software-intensive  systems  which  already  feature  built-in  adaptation  mechanisms.  In  this  paper,  we  report  on  our  experience  in  integrating  Rainbow,  a  platform  for  architecture-based  self-adaptation,  and  an  industrial  middleware  employed  to  monitor  and  manage  highly  populated  networks  of  devices.  Specifically,  we  reflect  on  aspects  such  as  the  effort  required  for  framework  customization  and  legacy  code  refactoring,  performance  improvement,  and  the  impact  of  architecture-based  self-adaptation  on  system  evolution.
1	Data  driven  continuous  evolution  of  smart  systems.  As  Marc  Andreessen  said  in  his  Wall  Street  Journal  OpEd,  software  is  eating  the  world.  The  systems  that  we  are  building  today  and  in  the  near  future  will  exhibit  levels  of  autonomy  that  will  put  new  demands  on  the  engineering  of  such  systems.  Although  promising  examples  of  autonomous  systems  exist,  there  is  no  established  methodology  for  systematically  building  autonomous  systems  that  employ  modern  software  engineering  technology  such  as  continuous  deployment  and  data-driven  engineering.  The  contribution  of  this  paper  is  twofold.  First,  it  identifies  and  presents  the  challenge  of  continuous  evolution  of  autonomous  systems  as  a  well-defined  problem  that  needs  to  be  addressed  by  software  engineering  research.  Second,  it  presents  a  conceptual  solution  to  this  problem  that  integrates  the  development  of  new  software  for  autonomous  systems  by  R&D  teams  with  systematic  experimentation  by  autonomous  systems.
1	Delta  modeling  workflow.  In  previous  work  we  show  how  abstract  delta  modeling  can  be  used  to  model  product  lines.  The  formalism  assigns  a  functional  meaning  to  features  from  a  feature  model  and  provides  a  novel  mechanism  for  resolving  implementation  conflicts  without  code  duplication  or  overspecification.  But  in  the  vast  expressive  space  of  delta  modeling,  it  may  not  be  clear  to  a  developer  how  to  create  a  product  line  from  scratch.  The  formalism  was  descriptive  rather  than  prescriptive.  To  that  end,  we  propose  a  development  workflow  based  directly  on  Abstract  Delta  Modeling.  We  show  preservation  of  global  unambiguity  and  completeness  in  the  product  lines  resulting  from  this  workflow.  We  also  show  that  the  work-flow  naturally  supports  concurrent  development.
1	Mapping  features  to  automatically  identified  object  oriented  variability  implementations  the  case  of  argouml  spl.  In  Software  Product  Line  (SPL)  engineering,  mapping  domain  features  to  existing  code  assets  is  essential  for  variability  management.  When  variability  is  already  implemented  through  Object-Oriented  (OO)  techniques,  it  is  too  costly  and  error-prone  to  refactor  assets  in  terms  of  features  or  to  use  feature  annotations.  In  this  work,  we  delve  into  the  possible  usage  of  automatically  identified  variation  points  with  variants  in  an  OO  code  base  to  enable  feature  mapping  from  the  domain  level.  We  report  on  an  experiment  conducted  over  ArgoUML-SPL,  using  its  code  as  input  for  automatic  detection  through  the  symfinder  toolchain,  and  the  previously  devised  domain  features  as  a  ground  truth.  We  analyse  the  relevance  of  the  identified  variation  points  with  variants  w.r.t.  domain  features,  adapting  precision  and  recall  measures.  This  shows  that  the  approach  is  feasible,  that  an  automatic  mapping  can  be  envisaged,  and  also  that  the  symfinder  visualization  is  adapted  to  this  process  with  some  slight  additions.
1	The  three  layer  product  model  an  alternative  view  on  spls  and  variability.  Despite  more  than  20  years  on  research  on  software  product  lines  and  variability,  industry  continues  to  experience  the  topic  of  strategic  intra-organizational  reuse  and  variability  management  as  a  challenge.  This  challenge  is  exacerbated  by  the  adoption  of  software  ecosystems.  The  keynote  introduces  an  alternative  perspective  on  this  area  using  the  Three  Layer  Product  Model  (3LPM).  The  3LPM  categorizes  organizes  functionality  into  three  categories,  i.e.  commodity,  differentiating  and  innovative.  Our  research  shows  that  organizations,  on  average,  spend  80-90%  of  their  R&D  resources  on  commodity  functionality.  Also,  variability  in  the  commodity  layer  provides  no  or  little  business  value.  Consequently,  the  keynote  focuses  on  analyzing  this  challenge,  presenting  internal  strategies  and  ecosystem  strategies  to  address  this  and  industrial  experiences  of  using  3LPM.
1	Graph  transforming  java  data.  This  paper  introduces  an  approach  for  adding  graph  transformation-based  functionality  to  existing  Java  programs.  The  approach  relies  on  a  set  of  annotations  to  identify  the  intended  graph  structure,  as  well  as  on  user  methods  to  manipulate  that  structure,  within  the  user's  own  Java  class  declarations.  Other  ingredients  are  a  custom  transformation  language,  called  Chart,  and  a  compiler  from  Chart  to  Java.  The  generated  Java  code  runs  against  the  pre-existing,  annotated  code.    The  advantage  of  the  approach  is  that  it  allows  any  Java  program  to  be  enhanced,  non  invasively,  with  declarative  graph  rules,  improving  clarity,  conciseness  and  verifiability.
1	Computing  program  reliability  using  forward  backward  precondition  analysis  and  model  counting.  The  goal  of  probabilistic  static  analysis  is  to  quantify  the  probability  that  a  given  program  satisfies/violates  a  required  property  (assertion).  In  this  work,  we  use  a  static  analysis  by  abstract  interpretation  and  model  counting  to  construct  probabilistic  analysis  of  deterministic  programs  with  uncertain  input  data,  which  can  be  used  for  estimating  the  probabilities  of  assertions  (program  reliability).
1	Verifying  a  verifier  on  the  formal  correctness  of  an  lts  transformation  verification  technique.  Over  the  years,  various  formal  methods  have  been  proposed  and  further  developed  to  determine  the  functional  correctness  of  models  of  concurrent  systems.  Some  of  these  have  been  designed  for  application  in  a  model-driven  development  workflow,  in  which  model  transformations  are  used  to  incrementally  transform  initial  abstract  models  into  concrete  models  containing  all  relevant  details.  In  this  paper,  we  consider  an  existing  formal  verification  technique  to  determine  that  formalisations  of  such  transformations  are  guaranteed  to  preserve  functional  properties,  regardless  of  the  models  they  are  applied  on.  We  present  our  findings  after  having  formally  verified  this  technique  using  the  Coq  theorem  prover.  It  turns  out  that  in  some  cases  the  technique  is  not  correct.  We  explain  why,  and  propose  an  updated  technique  in  which  these  issues  have  been  fixed.
1	Introducing  b  sequenced  petri  nets  as  a  cpn  sub  class  for  safe  train  control.  Formalizing  system  specification  has  been  highly  valuable  in  demonstrating  safety  and  consistence  of  safety  critical  systems.  It  is  undoubtedly  the  case  in  railway  signalling,  especially  the  European  Rail  Traffic  Management  System/European  Train  Control  System  (ERTMS/ETCS).  However,  the  complexity  of  the  European  standard  specification,  especially  for  its  highest  level,  namely  level  3,  requires  a  significant  overtake  in  early  modelling  approaches  when  it  comes  to  clearly  expressing  system  functionalities  along  with  safety  requirements,  all  towards  a  concrete  safe  design.  In  this  regard,  our  research  introduces  a  Colored  Petri  net  (CPN)  sub-class  associated  to  an  Event-B  machine  and  annotated  by  mathematical  sequences,  which  are  ex-pressed  in  the  B-language,  all  in  the  view  of  enriching  the  modelling  techniques  intended  for  system  formal  specification  and  verification.  In  this  paper,  we  show  through  a  detailed  ERTMS  L3  case  study,  how  such  featured  CPNs  fit  in  the  progressive  formalization  and  verification  of  Movement  Authority  (MA)  computation.
1	An  automated  semantic  based  approach  for  creating  tasks  from  matlab  simulink  models.  The  approach  proposed  in  this  paper  forms  the  front-end  of  a  framework  for  the  complete  design  flow  from  specification  models  of  new  automotive  functions  captured  in  Matlab  Simulink  to  their  distributed  execution  on  hierarchical  bus-based  electronic  architectures  hosting  the  release  of  already  deployed  automotive  functions.  The  process  starts  by  deriving  a  task  structure  from  a  given  Matlab  Simulink  model.  Because  the  obtained  network  is  typically  unbalanced  in  the  sense  of  computational  node  weights,  nodes  are  melted  following  an  optimization  metric  called  cohesion  where  nodes  are  attracted  by  high  communication  density  and  repelled  by  high  node  weights.  This  reduces  task-switching  times  by  avoiding  too  lightweight  tasks  and  relieves  the  bus  by  keeping  inter-task  communication  low.  This  so-called  Task  Creation  encloses  the  translation  of  the  synchronous  block  diagram  model  of  Simulink  into  a  message-based  task  network  formalism  that  serves  as  semantic  base.
1	Randomised  testing  of  a  microprocessor  model  using  smt  solver  state  generation.  We  validate  a  HOL4  model  of  the  ARM  Cortex-M0  microcontroller  core  by  testing  the  model’s  behaviour  on  randomly  chosen  instructions  against  a  real  chip.
1	Timed  mobility  and  timed  communication  for  critical  systems.  We  present  a  simple  but  elegant  prototyping  language  for  describing  real-time  systems  including  specific  features  as  timeouts,  explicit  locations,  timed  migration  and  timed  communication.  The  parallel  execution  of  a  step  is  provided  by  multiset  labelled  transitions.  In  order  to  illustrate  its  features,  we  describe  a  railway  control  system.  Moreover,  we  define  some  behavioural  equivalences  matching  multisets  of  actions  that  could  happen  in  a  given  range  of  time  (up  to  a  timeout).  We  define  the  strong  time-bounded  bisimulation  and  the  strong  open  time-bounded  bisimulation,  and  prove  that  the  latter  one  is  a  congruence.  By  using  various  bisimulations  over  the  behaviours  of  real-time  systems,  we  can  check  which  behaviours  are  closer  to  an  optimal  and  safe  behaviour.
1	Compositional  model  checking  is  lively.  Compositional  model  checking  approaches  attempt  to  limit  state  space  explosion  by  iteratively  combining  behaviour  of  some  of  the  components  in  the  system  and  reducing  the  result  modulo  an  appropriate  equivalence  relation.  For  an  equivalence  relation  to  be  applicable,  it  should  be  a  congruence  for  parallel  composition  where  synchronisations  between  the  components  may  be  introduced.  An  equivalence  relation  preserving  both  safety  and  liveness  properties  is  divergence-preserving  branching  bisimulation  (DPBB).  It  is  generally  assumed  that  DPBB  is  a  congruence  for  parallel  composition,  even  in  the  context  of  synchronisations  between  components.  However,  so  far,  no  such  results  have  been  published.
1	Programming  dynamic  reconfigurable  systems.  DR-BIP  is  an  extension  of  the  BIP  component  framework  intended  for  programming  reconfigurable  systems  encompassing  various  aspects  of  dynamism.  It  relies  on  architectural  motifs  to  structure  the  architecture  of  a  system  and  to  coordinate  its  reconfiguration  at  runtime.  An  architectural  motif  defines  a  set  of  interacting  components  that  evolve  according  to  reconfiguration  rules.  With  DR-BIP,  the  dynamism  can  be  captured  as  the  interplay  of  dynamic  changes  in  three  independent  directions  (1)  the  organization  of  interactions  between  instances  of  components  in  a  given  configuration;  (2)  the  reconfiguration  mechanisms  allowing  creation/deletion  of  components  and  management  of  their  interaction  according  to  a  given  architectural  motif;  (3)  the  migration  of  components  between  predefined  architectural  motifs  which  characterizes  dynamic  execution  environments.  The  paper  lays  down  the  formal  foundation  of  DR-BIP,  illustrates  its  expressiveness  on  few  examples  and  discusses  avenues  for  dynamic  reconfigurable  system  design.
1	A  proposed  method  to  evaluate  and  compare  fault  predictions  across  studies.  Studies  on  fault  prediction  often  pay  little  attention  to  empirical  rigor  and  presentation.  Researchers  might  not  have  full  command  over  the  statistical  method  they  use,  full  understanding  of  the  data  they  have,  or  tend  not  to  report  key  details  about  their  work.  What  does  it  happen  when  we  want  to  compare  such  studies  for  building  a  theory  on  fault  prediction?  There  are  two  issues  that  if  not  addressed,  we  believe,  prevent  building  such  theory.  The  first  concerns  how  to  compare  and  report  prediction  performance  across  studies  on  different  data  sets.  The  second  regards  fitting  performance  of  prediction  models.  Studies  tend  not  to  control  and  report  the  performance  of  predictors  on  historical  data  underestimating  the  risk  that  good  predictors  may  poorly  perform  on  past  data.  The  degree  of  both  fitting  and  prediction  performance  determines  the  risk  managers  are  requested  to  take  when  they  use  such  predictors.  In  this  work,  we  propose  a  framework  to  compare  studies  on  categorical  fault  prediction  that  aims  at  addressing  the  two  issues.  We  propose  three  algorithms  that  automate  our  framework.  We  finally  review  baseline  studies  on  fault  prediction  to  discuss  the  application  of  the  framework.
1	An  adaptive  approach  with  active  learning  in  software  fault  prediction.  Background:  Software  quality  prediction  plays  an  important  role  in  improving  the  quality  of  software  systems.  By  mining  software  metrics,  predictive  models  can  be  induced  that  provide  software  managers  with  insights  into  quality  problems  they  need  to  tackle  as  effectively  as  possible.      Objective:  Traditional,  supervised  learning  approaches  dominate  software  quality  prediction.  Resulting  models  tend  to  be  project  specific.  On  the  other  hand,  in  situations  where  there  are  no  previous  releases,  supervised  learning  approaches  are  not  very  useful  because  large  training  data  sets  are  needed  to  develop  accurate  predictive  models.      Method:  This  paper  eases  the  limitations  of  supervised  learning  approaches  and  offers  good  prediction  performance.  We  propose  an  adaptive  approach  in  which  supervised  learning  and  active  learning  are  coupled  together.  NaiveBayes  classifier  is  used  as  the  base  learner.      Results:  We  track  the  performance  at  each  iteration  of  the  adaptive  learning  algorithm  and  compare  it  with  the  performance  of  supervised  learning.  Our  results  show  that  proposed  scheme  provides  good  fault  prediction  performance  over  time,  i.e.,  it  eventually  outperforms  the  corresponding  supervised  learning  approach.  On  the  other  hand,  adaptive  learning  classification  approach  reduces  the  variance  in  prediction  performance  in  comparison  with  the  corresponding  supervised  learning  algorithm.      Conclusion:  The  adaptive  approach  outperforms  the  corresponding  supervised  learning  approach  when  both  use  Naive-Bayes  as  base  learner.  Additional  research  is  needed  to  investigate  whether  this  observation  remains  valid  with  other  base  classifiers.
1	The  impact  of  parameter  tuning  on  software  effort  estimation  using  learning  machines.  Background:  The  use  of  machine  learning  approaches  for  software  effort  estimation  (SEE)  has  been  studied  for  more  than  a  decade.  Most  studies  performed  comparisons  of  different  learning  machines  on  a  number  of  data  sets.  However,  most  learning  machines  have  more  than  one  parameter  that  needs  to  be  tuned,  and  it  is  unknown  to  what  extent  parameter  settings  may  affect  their  performance  in  SEE.  Many  works  seem  to  make  an  implicit  assumption  that  parameter  settings  would  not  change  the  outcomes  significantly.      Aims:  To  investigate  to  what  extent  parameter  settings  affect  the  performance  of  learning  machines  in  SEE,  and  what  learning  machines  are  more  sensitive  to  their  parameters.      Method:  Considering  an  online  learning  scenario  where  learning  machines  are  updated  with  new  projects  as  they  become  available,  systematic  experiments  were  performed  using  five  learning  machines  under  several  different  parameter  settings  on  three  data  sets.      Results:  While  some  learning  machines  such  as  bagging  using  regression  trees  were  not  so  sensitive  to  parameter  settings,  others  such  as  multilayer  perceptrons  were  affected  dramatically.  Combining  learning  machines  into  bagging  ensembles  helped  making  them  more  robust  against  different  parameter  settings.  The  average  performance  of  k-NN  across  different  projects  was  not  so  much  affected  by  different  parameter  settings,  but  the  parameter  settings  that  obtained  the  best  average  performance  across  time  steps  were  not  so  consistently  the  best  throughout  time  steps  as  in  the  other  approaches.      Conclusions:  Learning  machines  that  are  more/less  sensitive  to  different  parameter  settings  were  identified.  The  different  sensitivity  obtained  by  different  learning  machines  shows  that  sensitivity  to  parameters  should  be  considered  as  one  of  the  criteria  for  evaluation  of  SEE  approaches.  A  good  learning  machine  for  SEE  is  not  only  one  which  is  able  to  achieve  superior  performance,  but  also  one  that  is  either  less  dependent  on  parameter  settings  or  to  which  good  parameter  choices  are  easy  to  make.
1	Cross  version  defect  prediction  using  cross  project  defect  prediction  approaches  does  it  work.  Background:  Specifying  and  removing  defects  before  release  deserve  extra  cost  for  the  success  of  software  projects.  Long-running  projects  experience  multiple  releases,  and  it  is  a  natural  choice  to  adopt  cross-version  defect  prediction  (CVDP)  that  uses  information  from  older  versions.  A  past  study  shows  that  feeding  multi  older  versions  data  may  have  a  positive  influence  on  the  performance.  The  study  also  suggests  that  cross-project  defect  prediction  (CPDP)  may  fit  the  situation  but  one  CPDP  approach  was  only  examined.Aims:  To  investigate  whether  feeding  multiple  older  versions  data  is  effective  for  CVDP  using  CPDP  approaches.  The  investigation  also  involves  performance  comparisons  of  the  CPDP  approaches  under  CVDP  situation.  Method:  We  chose  a  style  of  replication  of  the  comparative  study  on  CPDP  approaches  by  Herbold  et  al.  under  CVDP  situation.  Results:  Feeding  multiple  older  versions  had  a  positive  effect  for  more  than  a  half  CPDP  approaches.  However,  almost  all  of  the  CPDP  approaches  did  not  perform  significantly  better  than  a  simple  rule-based  prediction.  Although  the  best  CPDP  approach  could  work  better  than  it  and  with-in  project  defect  prediction,  we  found  no  effect  of  feeding  multiple  older  versions  for  it.  Conclusions:  Feeding  multiple  older  versions  could  improve  CPDP  approaches  under  CVDP  situation.  However,  it  did  not  work  for  the  best  CPDP  approach  in  the  study.
1	The  xsap  safety  analysis  platform.  This  paper  describes  the  xSAP  safety  analysis  platform.  xSAP  provides  several  model-based  safety  analysis  features  for  finite-  and  infinite-state  synchronous  transition  systems.  In  particular,  it  supports  library-based  definition  of  fault  modes,  an  automatic  model  extension  facility,  generation  of  safety  analysis  artifacts  such  as  Dynamic  Fault  Trees  and  Failure  Mode  and  Effects  Analysis  tables.  Moreover,  it  supports  probabilistic  evaluation  of  Fault  Trees,  failure  propagation  analysis  using  Timed  Failure  Propagation  Graphs,  and  Common  Cause  Analysis.  xSAP  has  been  used  in  several  industrial  projects  as  verification  back-end,  and  is  currently  being  evaluated  in  a  joint  R&D  Project  involving  FBK  and  The  Boeing  Company.
1	Omega  regular  objectives  in  model  free  reinforcement  learning.  We  provide  the  first  solution  for  model-free  reinforcement  learning  of  \(\omega  \)-regular  objectives  for  Markov  decision  processes  (MDPs).  We  present  a  constructive  reduction  from  the  almost-sure  satisfaction  of  \(\omega  \)-regular  objectives  to  an  almost-sure  reachability  problem,  and  extend  this  technique  to  learning  how  to  control  an  unknown  model  so  that  the  chance  of  satisfying  the  objective  is  maximized.  We  compile  \(\omega  \)-regular  properties  into  limit-deterministic  Buchi  automata  instead  of  the  traditional  Rabin  automata;  this  choice  sidesteps  difficulties  that  have  marred  previous  proposals.  Our  approach  allows  us  to  apply  model-free,  off-the-shelf  reinforcement  learning  algorithms  to  compute  optimal  strategies  from  the  observations  of  the  MDP.  We  present  an  experimental  evaluation  of  our  technique  on  benchmark  learning  problems.
1	Shepherding  hordes  of  markov  chains.  This  paper  considers  large  families  of  Markov  chains  (MCs)  that  are  defined  over  a  set  of  parameters  with  finite  discrete  domains.  Such  families  occur  in  software  product  lines,  planning  under  partial  observability,  and  sketching  of  probabilistic  programs.  Simple  questions,  like  ‘does  at  least  one  family  member  satisfy  a  property?’,  are  NP-hard.  We  tackle  two  problems:  distinguish  family  members  that  satisfy  a  given  quantitative  property  from  those  that  do  not,  and  determine  a  family  member  that  satisfies  the  property  optimally,  i.e.,  with  the  highest  probability  or  reward.  We  show  that  combining  two  well-known  techniques,  MDP  model  checking  and  abstraction  refinement,  mitigates  the  computational  complexity.  Experiments  on  a  broad  set  of  benchmarks  show  that  in  many  situations,  our  approach  is  able  to  handle  families  of  millions  of  MCs,  providing  superior  scalability  compared  to  existing  solutions.
1	Apte  an  algorithm  for  proving  trace  equivalence.  This  paper  presents  APTE,  a  new  tool  for  automatically  proving  the  security  of  cryptographic  protocols.  It  focuses  on  proving  trace  equivalence  between  processes,  which  is  crucial  for  specifying  privacy  type  properties  such  as  anonymity  and  unlinkability.
1	Incremental  analysis  of  evolving  alloy  models.  Alloy  is  a  well-known  tool-set  for  building  and  analyzing  software  designs  and  models.  Alloy’s  key  strengths  are  its  intuitive  notation  based  on  relational  logic,  and  its  powerful  analysis  engine  backed  by  propositional  satisfiability  (SAT)  solvers  to  help  users  find  subtle  design  flaws.  However,  scaling  the  analysis  to  the  designs  of  real-world  systems  remains  an  important  technical  challenge.  This  paper  introduces  a  new  approach,  iAlloy,  for  more  efficient  analysis  of  Alloy  models.  Our  key  insight  is  that  users  often  make  small  and  frequent  changes  and  repeatedly  run  the  analyzer  when  developing  Alloy  models,  and  the  development  cost  can  be  reduced  with  the  incremental  analysis  over  these  changes.  iAlloy  is  based  on  two  techniques  –  a  static  technique  based  on  a  lightweight  impact  analysis  and  a  dynamic  technique  based  on  solution  re-use  –  which  in  many  cases  helps  avoid  potential  costly  SAT  solving.  Experimental  results  show  that  iAlloy  significantly  outperforms  Alloy  analyzer  in  the  analysis  of  evolving  Alloy  models  with  more  than  50%  reduction  in  SAT  solver  calls  on  average,  and  up  to  7x  speedup.
1	Testor  a  modular  tool  for  on  the  fly  conformance  test  case  generation.  We  present  TESTOR,  a  tool  for  on-the-fly  conformance  test  case  generation,  guided  by  test  purposes.  Concretely,  given  a  formal  specification  of  a  system  and  a  test  purpose,  TESTOR  automatically  generates  test  cases,  which  assess  using  black  box  testing  techniques  the  conformance  to  the  specification  of  a  system  under  test.  In  this  context,  a  test  purpose  describes  the  goal  states  to  be  reached  by  the  test  and  enables  one  to  indicate  parts  of  the  specification  that  should  be  ignored  during  the  testing  process.  Compared  to  the  existing  tool  TGV,  TESTOR  has  a  more  modular  architecture,  based  on  generic  graph  transformation  components,  is  capable  of  extracting  a  test  case  completely  on  the  fly,  and  enables  a  more  flexible  expression  of  test  purposes,  taking  advantage  of  the  multiway  rendezvous.  TESTOR  has  been  implemented  on  top  of  the  CADP  verification  toolbox,  evaluated  on  three  published  case-studies  and  more  than  10000  examples  taken  from  the  non-regression  test  suites  of  CADP.
1	Rtd  finder  a  tool  for  compositional  verification  of  real  time  component  based  systems.  In  this  paper  we  present  RTD-Finder,  a  tool  which  applies  a  fully  compositional  and  automatic  method  for  the  verification  of  safety  properties  for  real-time  component-based  systems  modeled  in  the  RT-BIP  language.  The  core  method  is  based  on  the  compositional  computation  of  a  global  invariant  which  over-approximates  the  set  of  reachable  states  of  the  system.  The  verification  results  show  that  when  the  invariant  catches  the  safety  property,  the  verification  time  for  large  systems  is  drastically  reduced  in  comparison  with  exploration  techniques.  Nevertheless,  the  above  method  is  based  on  an  over-approximation  of  the  reachable  states  set  expressed  by  the  invariant,  hence  false  positives  may  occur  in  some  cases.  We  completed  our  compositional  verification  method  with  a  counterexample-based  invariant  refinement  algorithm  analyzing  iteratively  the  generated  counterexamples.  The  spurious  counterexamples  which  are  detected  serve  to  strengthen  incrementally  the  global  invariant  until  a  true  counterexample  is  found  or  until  it  is  proven  that  all  the  counterexamples  are  spurious.
1	Equivalence  checking  of  quantum  protocols.  Quantum  Information  Processing  (QIP)  is  an  emerging  area  at  the  intersection  of  physics  and  computer  science.  It  aims  to  establish  the  principles  of  communication  and  computation  for  systems  based  on  the  theory  of  quantum  mechanics.  Interesting  QIP  protocols  such  as  quantum  key  distribution,  teleportation,  and  blind  quantum  computation  have  already  been  realised  in  the  laboratory  and  are  now  in  the  realm  of  mainstream  industrial  applications.  The  complexity  of  these  protocols,  along  with  possible  inaccuracies  in  implementation,  demands  systematic  and  formal  analysis.  In  this  paper,  we  present  a  new  technique  and  a  tool,  with  a  high-level  interface,  for  verification  of  quantum  protocols  using  equivalence  checking.  Previous  work  by  Gay,  Nagarajan  and  Papanikolaou  used  model-checking  to  verify  quantum  protocols  represented  in  the  stabilizer  formalism,  a  restricted  model  which  can  be  simulated  efficiently  on  classical  computers.  Here,  we  are  able  to  go  beyond  stabilizer  states  and  verify  protocols  efficiently  on  all  input  states.
1	Firefighting  simulation  on  virtual  reality  platform.  We  propose  firefighting  scenarios  based  training  system  on  virtual  reality  platform.  The  main  objectives  of  our  work  are  to  provide  information  concerning  fire  incident  and  how  to  deal  with  this  critical  situation  as  realistic  as  possible.  The  players  interact  with  our  system  by  means  of  virtual  reality  head-mounted  display  and  our  custom  controller  that  mimic  the  fire  extinguisher.  The  simulated  fire  incidents  are  rendered  in  three  dimensional  first-person  point  of  view  style.  Users  have  to  deal  with  the  fire  incident  situation  that  may  occur  in  various  ways  and  learn  from  these  experiences.
1	Guidelines  for  coverage  based  comparisons  of  non  adequate  test  suites.  A  fundamental  question  in  software  testing  research  is  how  to  compare  test  suites,  often  as  a  means  for  comparing  test-generation  techniques  that  produce  those  test  suites.  Researchers  frequently  compare  test  suites  by  measuring  their  coverage.  A  coverage  criterion  C  provides  a  set  of  test  requirements  and  measures  how  many  requirements  a  given  suite  satisfies.  A  suite  that  satisfies  100p  of  the  feasible  requirements  is  called  C-adequate.  Previous  rigorous  evaluations  of  coverage  criteria  mostly  focused  on  such  adequate  test  suites:  given  two  criteria  C  and  C′,  are  C-adequate  suites  on  average  more  effective  than  C′-adequate  suites?  However,  in  many  realistic  cases,  producing  adequate  suites  is  impractical  or  even  impossible.      This  article  presents  the  first  extensive  study  that  evaluates  coverage  criteria  for  the  common  case  of  non-adequate  test  suites:  given  two  criteria  C  and  C′,  which  one  is  better  to  use  to  compare  test  suites?  Namely,  if  suites  T1,  T2,…,Tn  have  coverage  values  c1,  c2,…,cn  for  C  and  c1′,  c2′,…,cn′  for  C′,  is  it  better  to  compare  suites  based  on  c1,  c2,…,cn  or  based  on  c1′,  c2′,…,cn′?  We  evaluate  a  large  set  of  plausible  criteria,  including  basic  criteria  such  as  statement  and  branch  coverage,  as  well  as  stronger  criteria  used  in  recent  studies,  including  criteria  based  on  program  paths,  equivalence  classes  of  covered  statements,  and  predicate  states.  The  criteria  are  evaluated  on  a  set  of  Java  and  C  programs  with  both  manually  written  and  automatically  generated  test  suites.  The  evaluation  uses  three  correlation  measures.  Based  on  these  experiments,  two  criteria  perform  best:  branch  coverage  and  an  intraprocedural  acyclic  path  coverage.  We  provide  guidelines  for  testing  researchers  aiming  to  evaluate  test  suites  using  coverage  criteria  as  well  as  for  other  researchers  evaluating  coverage  criteria  for  research  use.
1	Verification  across  intellectual  property  boundaries.  In  many  industries,  the  importance  of  software  components  provided  by  third-party  suppliers  is  steadily  increasing.  As  the  suppliers  seek  to  secure  their  intellectual  property  (IP)  rights,  the  customer  usually  has  no  direct  access  to  the  suppliers’  source  code,  and  is  able  to  enforce  the  use  of  verification  tools  only  by  legal  requirements.  In  turn,  the  supplier  has  no  means  to  convince  the  customer  about  successful  verification  without  revealing  the  source  code.  This  article  presents  an  approach  to  resolve  the  conflict  between  the  IP  interests  of  the  supplier  and  the  quality  interests  of  the  customer.  We  introduce  a  protocol  in  which  a  dedicated  server  (called  the  “amanat”)  is  controlled  by  both  parties:  the  customer  controls  the  verification  task  performed  by  the  amanat,  while  the  supplier  controls  the  communication  channels  of  the  amanat  to  ensure  that  the  amanat  does  not  leak  information  about  the  source  code.  We  argue  that  the  protocol  is  both  practically  useful  and  mathematically  sound.  As  the  protocol  is  based  on  well-known  (and  relatively  lightweight)  cryptographic  primitives,  it  allows  a  straightforward  implementation  on  top  of  existing  verification  tool  chains.  To  substantiate  our  security  claims,  we  establish  the  correctness  of  the  protocol  by  cryptographic  reduction  proofs.
1	A  formal  model  for  automated  software  modularity  and  evolvability  analysis.  Neither  the  nature  of  modularity  in  software  design,  characterized  as  a  property  of  the  structure  of  dependencies  among  design  decisions,  or  its  economic  value  are  adequately  well  understood.  One  basic  problem  is  that  we  do  not  even  have  a  sufficiently  clear  definition  of  what  it  means  for  one  design  decision  to  depend  on  another.  The  main  contribution  of  this  work  is  one  possible  mathematically  precise  definition  of  dependency  based  on  an  augmented  constraint  network  model.  The  model  provides  an  end-to-end  account  of  the  connection  between  modularity  and  its  value  in  terms  of  options  to  make  adaptive  changes  in  uncertain  and  changing  design  spaces.  We  demonstrate  the  validity  and  theoretical  utility  of  the  model,  showing  that  it  is  consistent  with,  and  provides  new  insights  into,  several  previously  published  results  in  design  theory.
1	Impact  driven  process  model  repair.  The  abundance  of  event  data  in  today’s  information  systems  makes  it  possible  to  “confront”  process  models  with  the  actual  observed  behavior.  Process  mining  techniques  use  event  logs  to  discover  process  models  that  describe  the  observed  behavior,  and  to  check  conformance  of  process  models  by  diagnosing  deviations  between  models  and  reality.  In  many  situations,  it  is  desirable  to  mediate  between  a  preexisting  model  and  observed  behavior.  Hence,  we  would  like  to  repair  the  model  while  improving  the  correspondence  between  model  and  log  as  much  as  possible.  The  approach  presented  in  this  article  assigns  predefined  costs  to  repair  actions  (allowing  inserting  or  skipping  of  activities).  Given  a  maximum  degree  of  change,  we  search  for  models  that  are  optimal  in  terms  of  fitness—that  is,  the  fraction  of  behavior  in  the  log  not  possible  according  to  the  model  is  minimized.  To  compute  fitness,  we  need  to  align  the  model  and  log,  which  can  be  time  consuming.  Hence,  finding  an  optimal  repair  may  be  intractable.  We  propose  different  alternative  approaches  to  speed  up  repair.  The  number  of  alignment  computations  can  be  reduced  dramatically  while  still  returning  near-optimal  repairs.  The  different  approaches  have  been  implemented  using  the  process  mining  framework  ProM  and  evaluated  using  real-life  logs.
1	Modeling  and  verifying  hierarchical  real  time  systems  using  stateful  timed  csp.  Modeling  and  verifying  complex  real-time  systems  are  challenging  research  problems.  The  de  facto  approach  is  based  on  Timed  Automata,  which  are  finite  state  automata  equipped  with  clock  variables.  Timed  Automata  are  deficient  in  modeling  hierarchical  complex  systems.  In  this  work,  we  propose  a  language  called  Stateful  Timed  CSP  and  an  automated  approach  for  verifying  Stateful  Timed  CSP  models.  Stateful  Timed  CSP  is  based  on  Timed  CSP  and  is  capable  of  specifying  hierarchical  real-time  systems.  Through  dynamic  zone  abstraction,  finite-state  zone  graphs  can  be  generated  automatically  from  Stateful  Timed  CSP  models,  which  are  subject  to  model  checking.  Like  Timed  Automata,  Stateful  Timed  CSP  models  suffer  from  Zeno  runs,  that  is,  system  runs  that  take  infinitely  many  steps  within  finite  time.  Unlike  Timed  Automata,  model  checking  with  non-Zenoness  in  Stateful  Timed  CSP  can  be  achieved  based  on  the  zone  graphs.  We  extend  the  PAT  model  checker  to  support  system  modeling  and  verification  using  Stateful  Timed  CSP  and  show  its  usability/scalability  via  verification  of  real-world  systems.
1	A  stack  memory  abstraction  and  symbolic  analysis  framework  for  executables.  This  article  makes  three  contributions  regarding  reverse-engineering  of  executables.  First,  techniques  are  presented  for  recovering  a  precise  and  correct  stack-memory  model  in  executables  while  addressing  executable-specific  challenges  such  as  indirect  control  transfers.  Next,  the  enhanced  memory  model  is  employed  to  define  a  novel  symbolic  analysis  framework  for  executables  that  can  perform  the  same  types  of  program  analyses  as  source-level  tools.  Third,  a  demand-driven  framework  is  presented  to  enhance  the  scalability  of  the  symbolic  analysis  framework.  Existing  symbolic  analysis  frameworks  for  executables  fail  to  simultaneously  maintain  the  properties  of  correct  representation,  a  precise  stack-memory  model,  and  scalability.  Furthermore,  they  ignore  memory-allocated  variables  when  defining  symbolic  analysis  mechanisms.  Our  methods  do  not  use  symbolic,  relocation  or  debug  information,  which  are  usually  absent  in  deployed  binaries.  We  describe  our  framework,  highlighting  the  novel  intellectual  contributions  of  our  approach  and  demonstrating  its  efficacy  and  robustness.  Our  techniques  improve  the  precision  of  existing  stack-memory  models  by  25p,  enhance  scalability  of  our  basic  symbolic  analysis  mechanism  by  10×,  and  successfully  uncovers  five  previously  undiscovered  information-flow  vulnerabilities  in  several  widely  used  programs.
1	Computing  alignments  of  well  formed  process  models  using  local  search.  The  alignment  of  observed  and  modeled  behavior  is  an  essential  element  for  organizations,  since  it  opens  the  door  for  conformance  checking  and  enhancement  of  processes.  The  state-of-the-art  technique  for  computing  alignments  has  exponential  time  and  space  complexity,  hindering  its  applicability  for  medium  and  large  instances.  In  this  article,  a  novel  approach  is  presented  to  tackle  the  challenge  of  computing  an  alignment  for  large-problem  instances  that  correspond  to  well-formed  process  models.  Given  an  observed  trace,  first  it  uses  a  novel  replay  technique  to  find  an  initial  candidate  trace  in  the  model.  Then  a  local  search  framework  is  applied  to  try  to  improve  the  alignment  until  no  further  improvement  is  possible.  The  implementation  of  the  presented  technique  reveals  a  magnificent  reduction  both  in  computation  time  and  in  memory  usage.  Moreover,  although  the  proposed  technique  does  not  guarantee  the  derivation  of  an  alignment  with  minimal  cost,  the  experiments  show  that  in  practice  the  quality  of  the  obtained  solutions  is  close  to  optimal.
1	Using  cohesion  and  coupling  for  software  remodularization.  Refactoring  and,  in  particular,  remodularization  operations  can  be  performed  to  repair  the  design  of  a  software  system  and  remove  the  erosion  caused  by  software  evolution.  Various  approaches  have  b...
1	Status  quo  in  requirements  engineering  a  theory  and  a  global  family  of  surveys.  Requirements  Engineering  (RE)  has  established  itself  as  a  software  engineering  discipline  over  the  past  decades.  While  researchers  have  been  investigating  the  RE  discipline  with  a  plethora  of  empirical  studies,  attempts  to  systematically  derive  an  empirical  theory  in  context  of  the  RE  discipline  have  just  recently  been  started.  However,  such  a  theory  is  needed  if  we  are  to  define  and  motivate  guidance  in  performing  high  quality  RE  research  and  practice.We  aim  at  providing  an  empirical  and  externally  valid  foundation  for  a  theory  of  RE  practice,  which  helps  software  engineers  establish  effective  and  efficient  RE  processes  in  a  problem-driven  manner.We  designed  a  survey  instrument  and  an  engineer-focused  theory  that  was  first  piloted  in  Germany  and,  after  making  substantial  modifications,  has  now  been  replicated  in  10  countries  worldwide.  We  have  a  theory  in  the  form  of  a  set  of  propositions  inferred  from  our  experiences  and  available  studies,  as  well  as  the  results  from  our  pilot  study  in  Germany.  We  evaluate  the  propositions  with  bootstrapped  confidence  intervals  and  derive  potential  explanations  for  the  propositions.In  this  article,  we  report  on  the  design  of  the  family  of  surveys,  its  underlying  theory,  and  the  full  results  obtained  from  the  replication  studies  conducted  in  10  countries  with  participants  from  228  organisations.  Our  results  represent  a  substantial  step  forward  towards  developing  an  empirical  theory  of  RE  practice.  The  results  reveal,  for  example,  that  there  are  no  strong  differences  between  organisations  in  different  countries  and  regions,  that  interviews,  facilitated  meetings  and  prototyping  are  the  most  used  elicitation  techniques,  that  requirements  are  often  documented  textually,  that  traces  between  requirements  and  code  or  design  documents  are  common,  that  requirements  specifications  themselves  are  rarely  changed  and  that  requirements  engineering  (process)  improvement  endeavours  are  mostly  internally  driven.Our  study  establishes  a  theory  that  can  be  used  as  starting  point  for  many  further  studies  for  more  detailed  investigations.  Practitioners  can  use  the  results  as  theory-supported  guidance  on  selecting  suitable  RE  methods  and  techniques.
1	An  approach  for  modeling  architectural  design  rules  in  uml  and  its  application  to  embedded  software.  Current  techniques  for  modeling  software  architecture  do  not  provide  sufficient  support  for  modeling  architectural  design  rules.  This  is  a  problem  in  the  context  of  model-driven  development  in  which  it  is  assumed  that  major  design  artifacts  are  represented  as  formal  or  semi-formal  models.  This  article  addresses  this  problem  by  presenting  an  approach  to  modeling  architectural  design  rules  in  UML  at  the  abstraction  level  of  the  meaning  of  the  rules.  The  high  abstraction  level  and  the  use  of  UML  makes  the  rules  both  amenable  to  automation  and  easy  to  understand  for  both  architects  and  developers,  which  is  crucial  to  deployment  in  an  organization.  To  provide  a  proof-of-concept,  a  tool  was  developed  that  validates  a  system  model  against  the  architectural  rules  in  a  separate  UML  model.  To  demonstrate  the  feasibility  of  the  approach,  the  architectural  design  rules  of  an  existing  live  industrial-strength  system  were  modeled  according  to  the  approach.
1	Psc2code  denoising  code  extraction  from  programming  screencasts.  Programming  screencasts  have  become  a  pervasive  resource  on  the  Internet,  which  help  developers  learn  new  programming  technologies  or  skills.  The  source  code  in  programming  screencasts  is  an  important  and  valuable  information  for  developers.  But  the  streaming  nature  of  programming  screencasts  (i.e.,  a  sequence  of  screen-captured  images)  limits  the  ways  that  developers  can  interact  with  the  source  code  in  the  screencasts.  Many  studies  use  the  Optical  Character  Recognition  (OCR)  technique  to  convert  screen  images  (also  referred  to  as  video  frames)  into  textual  content,  which  can  then  be  indexed  and  searched  easily.  However,  noisy  screen  images  significantly  affect  the  quality  of  source  code  extracted  by  OCR,  for  example,  no-code  frames  (e.g.,  PowerPoint  slides,  web  pages  of  API  specification),  non-code  regions  (e.g.,  Package  Explorer  view,  Console  view),  and  noisy  code  regions  with  code  in  completion  suggestion  popups.  Furthermore,  due  to  the  code  characteristics  (e.g.,  long  compound  identifiers  like  ItemListener),  even  professional  OCR  tools  cannot  extract  source  code  without  errors  from  screen  images.  The  noisy  OCRed  source  code  will  negatively  affect  the  downstream  applications,  such  as  the  effective  search  and  navigation  of  the  source  code  content  in  programming  screencasts.  In  this  article,  we  propose  an  approach  named  psc2code  to  denoise  the  process  of  extracting  source  code  from  programming  screencasts.  First,  psc2code  leverages  the  Convolutional  Neural  Network  (CNN)  based  image  classification  to  remove  non-code  and  noisy-code  frames.  Then,  psc2code  performs  edge  detection  and  clustering-based  image  segmentation  to  detect  sub-windows  in  a  code  frame,  and  based  on  the  detected  sub-windows,  it  identifies  and  crops  the  screen  region  that  is  most  likely  to  be  a  code  editor.  Finally,  psc2code  calls  the  API  of  a  professional  OCR  tool  to  extract  source  code  from  the  cropped  code  regions  and  leverages  the  OCRed  cross-frame  information  in  the  programming  screencast  and  the  statistical  language  model  of  a  large  corpus  of  source  code  to  correct  errors  in  the  OCRed  source  code.  We  conduct  an  experiment  on  1,142  programming  screencasts  from  YouTube.  We  find  that  our  CNN-based  image  classification  technique  can  effectively  remove  the  non-code  and  noisy-code  frames,  which  achieves  an  F1-score  of  0.95  on  the  valid  code  frames.  We  also  find  that  psc2code  can  significantly  improve  the  quality  of  the  OCRed  source  code  by  truly  correcting  about  half  of  incorrectly  OCRed  words.  Based  on  the  source  code  denoised  by  psc2code,  we  implement  two  applications:  (1)  a  programming  screencast  search  engine;  (2)  an  interaction-enhanced  programming  screencast  watching  tool.  Based  on  the  source  code  extracted  from  the  1,142  collected  programming  screencasts,  our  experiments  show  that  our  programming  screencast  search  engine  achieves  the  precision@5,  10,  and  20  of  0.93,  0.81,  and  0.63,  respectively.  We  also  conduct  a  user  study  of  our  interaction-enhanced  programming  screencast  watching  tool  with  10  participants.  This  user  study  shows  that  our  interaction-enhanced  watching  tool  can  help  participants  learn  the  knowledge  in  the  programming  video  more  efficiently  and  effectively.
1	Testing  self  adaptive  systems  using  fault  injection  and  combinatorial  testing.  Verifying  and  validating  systems  that  can  adapt  their  behavior  at  runtime  is  still  a  research  challenge  that  deserves  great  attention.  In  order  to  assure  a  certain  behavior,  we  might  prove  that  a  self-adaptive  system  always  fulfills  certain  properties,  e.g.,  always  behaving  as  specified  in  a  given  range.  Such  verification  and  validation  techniques,  however,  assume  the  good  case,  i.e.,  that  the  system's  environment  is  working  as  expected,  i.e.,  that  no  fault  occurs.  In  this  paper,  we  relax  this  assumption,  and  consider  testing  self-adaptive  systems  in  case  of  faults.  In  particular,  we  show  how  fault  injection  techniques  and  combinatorial  testing  can  be  used  together  for  generating  tests  for  self-adaptive  systems.
1	A  deep  learning  method  for  judicial  decision  support.  With  the  development  and  innovation  of  machine  learning  and  deep  learning  technology,  more  and  more  fields  try  to  apply  artificial  intelligence  to  practical  scenarios.  We  try  to  use  deep  learning  model  to  assist  the  judgment  of  the  preliminary  case  results.  In  this  paper,  we  analyze  the  basic  description  of  the  case,  and  apply  deep  learning  model  to  predict  the  judgment  results  from  three  aspects:  penalty,  accusation  and  legal  provisions.  On  the  one  hand,  the  forecasting  results  can  help  the  judges  and  lawyers  to  make  decisions,  on  the  other  hand,  it  can  also  help  the  non-legal  professionals  to  have  a  basic  understanding  and  judgment  of  the  case.
1	Towards  a  model  of  testers  cognitive  processes  software  testing  as  a  problem  solving  approach.  Software  testing  is  a  complex,  intellectual  activity  based  (at  least)  on  analysis,  reasoning,  decision  making,  abstraction  and  collaboration  performed  in  a  highly  demanding  environment.  Naturally,  it  uses  and  allocates  multiple  cognitive  resources  in  software  testers.  However,  while  a  cognitive  psychology  perspective  is  increasingly  used  in  the  general  software  engineering  literature,  it  has  yet  to  find  its  place  in  software  testing.  To  the  best  of  our  knowledge,  no  theory  of  software  testers'  cognitive  processes  exists.  Here,  we  take  the  first  step  towards  such  a  theory  by  presenting  a  cognitive  model  of  software  testing  based  on  how  problem  solving  is  conceptualized  in  cognitive  psychology.  Our  approach  is  to  instantiate  a  general  problem  solving  process  for  the  specific  problem  of  creating  test  cases.  We  then  propose  an  experiment  for  testing  our  cognitive  test  design  model.  The  experiment  makes  use  of  verbal  protocol  analysis  to  understand  the  mechanisms  by  which  human  testers  choose,  design,  implement  and  evaluate  test  cases.  An  initial  evaluation  was  then  performed  with  five  software  engineering  master  students  as  subjects.  The  results  support  a  problem  solving-based  model  of  test  design  for  capturing  testers'  cognitive  processes.
1	Efficient  inclusion  checking  on  explicit  and  semi  symbolic  tree  automata.  The  paper  considers  several  issues  related  to  efficient  use  of  tree  automata  in  formal  verification.  First,  a  new  efficient  algorithm  for  inclusion  checking  on  non-deterministic  tree  automata  is  proposed.  The  algorithm  traverses  the  automaton  downward,  utilizing  antichains  and  simulations  to  optimize  its  run.  Results  of  a  set  of  experiments  are  provided,  showing  that  such  an  approach  often  very  significantly  outperforms  the  so  far  common  upward  inclusion  checking.  Next,  a  new  semi-symbolic  representation  of  non-deterministic  tree  automata,  suitable  for  automata  with  huge  alphabets,  is  proposed  together  with  algorithms  for  upward  as  well  as  downward  inclusion  checking  over  this  representation  of  tree  automata.  Results  of  a  set  of  experiments  comparing  the  performance  of  these  algorithms  are  provided,  again  showing  that  the  newly  proposed  downward  inclusion  is  very  often  better  than  upward  inclusion  checking.
1	Identifying  the  need  for  a  sustainable  architecture  maintenance  process.  Documentation  plays  a  significant  role  in  software  development  in  general  and  in  software  architecture  in  particular.  In  large  and  complex  systems,  many  changes  affecting  architecture  and  architectural  documentation  occur.  This  derives  the  need  for  constant  changes  within  architecture  documents  in  order  to  keep  them  up  to  date.  This  research  in-progress  aims  to  understand  the  current  state  of  architecture  maintenance  towards  proposing  a  solution  for  improving  this  practice  via  a  well-defined  process  and  supporting  tools.
1	A  new  cryptography  algorithm  to  protect  cloud  based  healthcare  services.  The  revolution  of  smart  devices  has  a  significant  and  positive  impact  on  the  lives  of  many  people,  especially  in  regard  to  elements  of  healthcare.  In  part,  this  revolution  is  attributed  to  technological  advances  that  enable  individuals  to  wear  and  use  medical  devices  to  monitor  their  health  activities,  but  remotely.  Also,  these  smart,  wearable  medical  devices  assist  health  care  providers  in  monitoring  their  patients  remotely,  thereby  enabling  physicians  to  respond  quickly  in  the  event  of  emergencies.  An  ancillary  advantage  is  that  health  care  costs  will  be  reduced,  another  benefit  that,  when  paired  with  prompt  medical  treatment,  indicates  significant  advances  in  the  contemporary  management  of  health  care.  However,  the  competition  among  manufacturers  of  these  medical  devices  creates  a  complexity  of  small  and  smart  wearable  devices  such  as  ECG  and  EMG.  This  complexity  results  in  other  issues  such  as  patient  security,  privacy,  confidentiality,  and  identity  theft.  In  this  paper,  we  discuss  the  design  and  implementation  of  a  hybrid  real-time  cryptography  algorithm  to  secure  lightweight  wearable  medical  devices.  The  proposed  system  is  based  on  an  emerging  innovative  technology  between  the  genomic  encryptions  and  the  deterministic  chaos  method  to  provide  a  quick  and  secure  cryptography  algorithm  for  real-time  health  monitoring  that  permits  for  threats  to  patient  confidentiality  to  be  addressed.  The  proposed  algorithm  also  considers  the  limitations  of  memory  and  size  of  the  wearable  health  devices.  The  experimental  results  and  the  encryption  analysis  indicate  that  the  proposed  algorithm  provides  a  high  level  of  security  for  the  remote  health  monitoring  system.
1	Secure  sequence  similarity  search  on  encrypted  genomic  data.  Genomic  data  is  being  produced  rapidly  by  both  individuals  and  enterprises  and  needs  to  be  outsourced  from  local  machines  to  a  cloud  for  better  flexibility.  Outsourcing  also  eliminates  the  local  storage  management  problem  for  data  owners.  However,  sensitive  data  must  be  encrypted  by  data  owners  before  outsourcing  to  protect  data  privacy  and  security  in  the  cloud.  As  genome  data  is  huge  in  volume,  it  is  challenging  to  execute  researchers'  query  securely  and  efficiently.  In  this  paper,  we  present  a  prefix  tree  based  indexing  algorithm  for  supporting  similar  sequence  search  query.  We  support  Hamming  distance  as  similarity  measure.  The  proposed  method  adopts  semi-honest  adversary  model  for  the  cloud  server.  The  security  of  the  shared  data  is  guaranteed  through  encryption  while  making  the  overall  computation  fast  and  scalable  enough  for  real-life  biomedical  applications.  We  evaluated  the  efficiency  of  our  proposed  model  on  a  database  of  Single-Nucleotide  Polymorphism  (SNP)  sequences  and  experimental  results  demonstrate  that  a  query  of  hamming  distance  k  =  2  in  a  database  of  10000  records,  where  each  record  contains  500  nucleotides,  takes  approximately  4  minutes.
1	Exploiting  partial  knowledge  for  efficient  model  analysis.  The  advancement  of  constraint  solvers  and  model  checkers  has  enabled  the  effective  analysis  of  high-level  formal  specification  languages.  However,  these  typically  handle  a  specification  in  an  opaque  manner,  amalgamating  all  its  constraints  in  a  single  monolithic  verification  task,  which  often  proves  to  be  a  performance  bottleneck.
1	Generic  emptiness  check  for  fun  and  profit.  We  present  a  new  algorithm  for  checking  the  emptiness  of  \(\omega  \)-automata  with  an  Emerson-Lei  acceptance  condition  (i.e.,  a  positive  Boolean  formula  over  sets  of  states  or  transitions  that  must  be  visited  infinitely  or  finitely  often).  The  algorithm  can  also  solve  the  model  checking  problem  of  probabilistic  positiveness  of  MDP  under  a  property  given  as  a  deterministic  Emerson-Lei  automaton.  Although  both  these  problems  are  known  to  be  NP-complete  and  our  algorithm  is  exponential  in  general,  it  runs  in  polynomial  time  for  simpler  acceptance  conditions  like  generalized  Rabin,  Streett,  or  parity.  In  fact,  the  algorithm  provides  a  unifying  view  on  emptiness  checks  for  these  simpler  automata  classes.  We  have  implemented  the  algorithm  in  Spot  and  PRISM  and  our  experiments  show  improved  performance  over  previous  solutions.
1	Verifying  communicating  multi  pushdown  systems.  Communicating  multi-pushdown  systems  model  networks  of  multi-threaded  recursive  programs  communicating  via  reliable  FIFO  channels.  Hence  their  verification  problems  are  undecidable  in  general.  The  behaviours  of  these  systems  can  be  represented  as  directed  graphs,  which  subsume  both  Message  Sequence  Charts  and  nested  words.  We  extend  the  notion  of  split-width  to  these  graphs,  defining  a  simple  algebra  to  compose/decompose  these  behaviours  using  two  natural  operations:  shuffle  and  merge.  We  obtain  simple,  uniform  and  optimal  decision  procedures  for  various  verification  problems  parametrized  by  split-width,  ranging  from  reachability  to  model-checking  against  MSO,  PDL  and  Temporal  Logics.
1	A  precise  style  for  business  process  modelling  results  from  two  controlled  experiments.  We  present  a  precise  style  for  the  modelling  of  business  processes  based  on  the  UML  activity  diagrams  and  two  controlled  experiments  to  compare  this  style  with  a  lighter  variant.  The  comparison  has  been  performed  with  respect  to  the  comprehensibility  of  business  processes  and  the  effort  to  comprehend  them.  The  first  experiment  has  been  conducted  at  the  Free  University  of  Bolzano-Bozen,  while  the  second  experiment  (i.e.,  a  differentiated  replication)  at  the  University  of  Genova.  The  participants  to  the  first  experiment  were  Master  students  and  so  more  experienced  than  the  participants  to  the  replication,  who  were  Bachelor  students.  The  results  indicate  that:  (a)  all  the  participants  achieved  a  significantly  better  comprehension  level  with  the  precise  style;  (b)  the  used  style  did  not  have  any  significant  impact  on  the  effort;  and  (c)  more  experienced  participants  benefited  more  from  the  precise  style.
1	An  industrial  system  engineering  process  integrating  model  driven  architecture  and  model  based  design.  We  present  an  industrial  model-driven  engineering  process  for  the  design  and  development  of  complex  distributed  embedded  systems.  We  outline  the  main  steps  in  the  process  and  the  evaluation  of  its  use  in  the  context  of  a  radar  application.  We  show  the  methods  and  tools  that  have  been  developed  to  allow  interoperability  among  requirements  management,  SysML  modeling  and  MBD  simulation  and  code  generation.
1	Specifying  and  verifying  secrecy  in  workflows  with  arbitrarily  many  agents.  Web-based  workflow  management  systems,  like  EasyChair,  HealthVault,  Ebay,  or  Amazon,  often  deal  with  confidential  information  such  as  the  identity  of  reviewers,  health  data,  or  credit  card  numbers.  Because  the  number  of  participants  in  the  workflow  is  in  principle  unbounded,  it  is  difficult  to  describe  the  information  flow  policy  of  such  systems  in  specification  languages  that  are  limited  to  a  fixed  number  of  agents.  We  introduce  a  first-order  version  of  HyperLTL,  which  allows  us  to  express  information  flow  requirements  in  workflows  with  arbitrarily  many  agents.  We  present  a  bounded  model  checking  technique  that  reduces  the  violation  of  the  information  flow  policy  to  the  satisfiability  of  a  first-order  formula.  We  furthermore  identify  conditions  under  which  the  resulting  satisfiability  problem  is  guaranteed  to  be  decidable.
1	Seamless  model  based  safety  engineering  from  requirement  to  implementation.  Development  of  embedded  automotive  systems  has  become  tremendously  complex  in  recent  years.  The  trend  of  replacing  traditional  mechanical  systems  with  modern  embedded  systems  enables  deployment  of  more  advanced  control  strategies.  This  provides  new  benefits  for  the  customer  and  environment,  but  at  the  same  time,  the  higher  degree  of  integration  and  safety-criticality  raise  new  challenges.  In  parallel  new  automotive  safety  standards,  such  as  ISO  26262,  and  the  introduction  of  automotive  multi-core  systems  require  efficient  and  consistent  product  development.  To  tackle  the  issues  of  mixed-critical  multi-core  systems  development  with  hard  real-time  constraints  and  provide  academical  methodologies  and  approaches  the  MEMCONS  project  was  launched.  Aim  of  this  paper  is  to  provide  an  overview  of  the  scientific  research  problem,  approaches  to  solve  the  problem  and  ways  to  evaluate  the  solution  found  by  the  project  related  PhD  thesis.
1	Heuristic  based  recommendation  for  metamodel  ocl  coevolution.  We  propose  a  novel  approach  for  solving  the  problem  of  coevolution  between  metamodels  and  OCL  constraints.  Unlike  existing  solutions,  our  approach  does  not  rely  on  predefined  update  rules  and  explicit  tracking  of  high  level  changes  to  the  metamodel.  Rather,  we  pose  it  as  a  multi-objective  optimization  problem,  exploring  the  space  of  possible  OCL  modifications  to  identify  solutions  that  (a)  do  not  violate  the  structure  of  the  new  version  of  the  metamodel,  (b)  minimize  changes  to  existing  constraints,  and  (c)  minimize  loss  of  information.  Finally,  we  recommend  an  appropriate  subset  of  solutions  to  the  user.  We  evaluate  our  approach  on  three  cases  of  metamodel  and  OCL  coevolution.  The  results  show  that  we  recommend  accurate  solutions  for  updating  OCL  constraints,  even  for  complex  evolution  changes.
1	Interoperability  of  software  engineering  metamodels.  Several  metamodels  have  been  proposed  in  the  software  engineering  literature  recently.  For  practical  usage,  it  is  important  to  ensure  that  these  metamodels  can  be  used  in  an  interoperable  fashion.  In  this  paper  we  present  an  approach  as  a  part  of  our  PhD  research  in  the  same  direction.  Our  methodology  is  based  on  the  study  of  analogous  characteristics  among  metamodels,  ontologies  and  schemas.  We  have  adopted  ontology  merging  and  schema  matching  techniques  and  apply  them  to  the  domain  of  metamodels  to  assist  in  creating  interoperable  metamodels.  This  methodology  is  applied  and  presented  here  with  an  illustrative  example  in  which  we  show  the  results  of  merging  two  of  the  OMG  metamodels:  the  Organization  Structure  Metamodel  (OSM)  and  the  Business  Process  Modelling  Notation  (BPMN).
1	Integrating  the  designer  in  the  loop  for  metamodel  model  co  evolution  via  interactive  computational  search.  Metamodels  evolve  even  more  frequently  than  programming  languages.  This  evolution  process  may  result  in  a  large  number  of  instance  models  that  are  no  longer  conforming  to  the  revised  meta-model.  On  the  one  hand,  the  manual  adaptation  of  models  after  the  metamodels'  evolution  can  be  tedious,  error-prone,  and  time-consuming.  On  the  other  hand,  the  automated  co-evolution  of  metamodels/models  is  challenging  especially  when  new  semantics  is  introduced  to  the  metamodels.  In  this  paper,  we  propose  an  interactive  multi-objective  approach  that  dynamically  adapts  and  interactively  suggests  edit  operations  to  developers  and  takes  their  feedback  into  consideration.  Our  approach  uses  NSGA-II  to  find  a  set  of  good  edit  operation  sequences  that  minimizes  the  number  of  conformance  errors,  maximizes  the  similarity  with  the  initial  model  (reduce  the  loss  of  information)  and  minimizes  the  number  of  proposed  edit  operations.  The  designer  can  approve,  modify,  or  reject  each  of  the  recommended  edit  operations,  and  this  feedback  is  then  used  to  update  the  proposed  rankings  of  recommended  edit  operations.  We  evaluated  our  approach  on  a  set  of  metamodel/model  coevolution  case  studies  and  compared  it  to  fully  automated  coevolution  techniques.
1	Query  based  access  control  for  secure  collaborative  modeling  using  bidirectional  transformations.  Large-scale  model-driven  system  engineering  projects  are  carried  out  collaboratively.  Engineering  artifacts  stored  in  model  repositories  are  developed  in  either  offline  (checkout-modify-commit)  or  online  (GoogleDoc-style)  scenarios.  Complex  systems  frequently  integrate  models  and  components  developed  by  different  teams,  vendors  and  suppliers.  Thus  confidentiality  and  integrity  of  design  artifacts  need  to  be  protected  by  access  control  policies.      We  propose  a  technique  for  secure  collaborative  modeling  where  (1)  fine-grained  access  control  for  models  can  be  defined  by  model  queries,  and  (2)  such  access  control  policies  are  strictly  enforced  by  bidirectional  model  transformations.  Each  collaborator  obtains  a  filtered  local  copy  of  the  model  containing  only  those  model  elements  which  they  are  allowed  to  read;  write  access  control  policies  are  checked  on  the  server  upon  submitting  model  changes.  We  illustrate  the  approach  and  carry  out  an  initial  scalability  assessment  using  a  case  study  of  the  MONDO  EU  project.
1	Measuring  the  quality  of  design  pattern  detection  results.  Detecting  design  patterns  in  large  software  systems  is  a  common  reverse  engineering  task  that  can  help  the  comprehension  process  of  the  system's  design.  While  several  design  pattern  detection  tools  presented  in  the  literature  are  capable  of  detecting  design  patterns  automatically,  evaluating  these  detection  results  is  usually  done  in  a  manual  and  subjective  fashion.  Differences  in  design  pattern  definitions,  as  well  as  pattern  instance  counting  and  presenting,  exacerbate  the  difficulty  of  evaluating  design  pattern  detection  results.  In  this  paper,  we  present  a  novel  approach  to  evaluating  and  comparing  design  pattern  detection  results.  Our  approach,  called  MoRe,  introduces  a  novel  way  to  present  design  pattern  instances  in  a  uniform  fashion.  Based  on  this  characterization  of  design  pattern  instances,  we  propose  four  measures  for  design  pattern  detection  evaluation  that  convey  a  concise  assessment  of  the  quality  of  the  results  produced  by  a  given  detection  method.  We  have  implemented  these  measures,  and  present  case  studies  that  showcase  their  usefulness.
1	The  python  c  api  evolution  usage  statistics  and  bug  patterns.  Python  has  become  one  of  the  most  popular  programming  languages  in  the  era  of  data  science  and  machine  learning,  especially  for  its  diverse  libraries  and  extension  modules.  Python  front-end  with  C/C++  native  implementation  achieves  both  productivity  and  performance,  almost  becoming  the  standard  structure  for  many  mainstream  software  systems.  However,  feature  discrepancies  between  two  languages  can  pose  many  security  hazards  in  the  interface  layer  using  the  Python/C  API.  In  this  paper,  we  applied  static  analysis  to  reveal  the  evolution  and  usage  statistics  of  the  Python/C  API,  and  provided  a  summary  and  classification  of  its  10  bug  patterns  with  empirical  bug  instances  from  Pillow,  a  widely  used  Python  imaging  library.  Our  toolchain  can  be  easily  extended  to  access  different  types  of  syntactic  bug-finding  checkers.  And  our  systematical  taxonomy  to  classify  bugs  can  guide  the  construction  of  more  highly  automated  and  high-precision  bug-finding  tools.
1	Automatic  generation  of  opaque  constants  based  on  the  k  clique  problem  for  resilient  data  obfuscation.  Data  obfuscations  are  program  transformations  used  to  complicate  program  understanding  and  conceal  actual  values  of  program  variables.  The  possibility  to  hide  constant  values  is  a  basic  building  block  of  several  obfuscation  techniques.  For  example,  in  XOR  Masking  a  constant  mask  is  used  to  encode  data,  but  this  mask  must  be  hidden  too,  in  order  to  keep  the  obfuscation  resilient  to  attacks.  In  this  paper,  we  present  a  novel  technique  based  on  the  k-clique  problem,  which  is  known  to  be  NP-complete,  to  generate  opaque  constants,  i.e.  values  that  are  difficult  to  guess  by  static  analysis.  In  our  experimental  assessment  we  show  that  our  opaque  constants  are  computationally  cheap  to  generate,  both  at  obfuscation  time  and  at  runtime.  Moreover,  due  to  the  NP-completeness  of  the  k-clique  problem,  our  opaque  constants  can  be  proven  to  be  hard  to  attack  with  state-of-the-art  static  analysis  tools.
1	Program  state  coverage  a  test  coverage  metric  based  on  executed  program  states.  In  software  testing,  different  metrics  are  proposed  to  predict  and  compare  test  suites  effectiveness.  In  this  regard,  Mutation  Score  (MS)  is  one  of  most  accurate  metrics.  However,  calculating  MS  needs  executing  test  suites  many  times  and  it  is  not  commonly  used  in  industry.  On  the  other  hand,  Line  Coverage  (LC)  is  a  widely  used  metric  which  is  calculated  by  executing  test  suites  only  once,  although  it  is  not  as  accurate  as  MS  in  terms  of  predicting  and  comparing  test  suites  effectiveness.  In  this  paper,  we  propose  a  novel  test  coverage  metric,  called  Program  State  Coverage  (PSC),  which  improves  the  accuracy  of  LC.  PSC  works  almost  the  same  as  LC  and  it  can  be  calculated  by  executing  test  suites  only  once.  However,  it  further  considers  the  number  of  distinct  program  states  in  which  each  line  is  executed.  Our  experiments  on  120  test  suites  from  four  packages  of  Apache  Commons  Math  and  Apache  Commons  Lang  show  that,  compared  to  LC,  PSC  is  more  strongly  correlated  with  normalized  MS.  As  a  result,  we  conclude  that  PSC  is  a  promising  test  coverage  metric.
1	Accurate  design  pattern  detection  based  on  idiomatic  implementation  matching  in  java  language  context.  Design  patterns  (DPs)  are  widely  accepted  as  solutions  to  recurring  problems  in  software  design.  While  numerous  approaches  and  tools  have  been  proposed  for  DP  detection  over  the  years,  the  neglect  of  language-specific  mechanism  that  underlies  the  implementation  idioms  of  DPs  leads  to  false  or  missing  DP  instances  since  language-specific  features  are  not  captured  and  similar  characteristics  are  not  distinguished.  However,  there  is  still  a  lack  of  research  that  emphasizes  the  idiomatic  implementation  in  the  context  of  a  specific  language.  A  vital  challenge  is  the  representation  of  software  systems  and  language  mechanism.  In  this  work,  we  propose  a  practical  approach  for  DP  detection  from  source  code,  which  exploits  idiomatic  implementation  in  the  context  of  Java  language.  DPs  are  formally  defined  under  the  blueprint  of  the  layered  knowledge  graph  (LKG)  that  models  both  language-independent  concepts  of  DPs  and  Java  language  mechanism.  Based  on  static  analysis  and  inference  techniques,  the  approach  enables  flexible  search  strategies  integrating  structural,  behavioral  and  semantic  aspects  of  DPs  for  the  detection.  Concerning  emerging  patterns  and  pattern  variants,  the  core  methodology  supports  pluggable  pattern  templates.  A  prototype  implementation  has  been  evaluated  on  five  open  source  software  systems  and  compared  with  three  other  approaches.  The  evaluation  results  show  that  the  proposed  approach  improves  the  accuracy  with  higher  precision  (85.7%)  and  recall  (93.8%).  The  runtime  performance  also  supports  its  practical  applicability.
1	Model  checking  for  protocols  using  verds.  We  present  the  techniques  of  ternary  boolean  diagram-based  model  checking  together  with  bounded  semantics  model  checking  used  in  the  model  checker  called  Verds,  which  is  developed  in  our  laboratory.  In  the  experiment  of  protocol  verification  under  different  scenarios,  we  compare  the  performance  of  Verds  against  those  of  model  checkers  CMurphi  and  NuSMV,  showing  that  Verds  overall  compares  favorably  to  NuSMV  and  CMurphi.
1	Data  flow  testing  in  the  large.  Data-flow  (DF)  testing  was  introduced  more  than  thirty  years  ago  aiming  at  extensively  evaluating  a  program  structure.  It  requires  tests  that  traverse  a  path  in  which  the  definition  of  a  variable  and  its  subsequent  use,  i.e.,  a  definition-use  association  (dua),  is  exercised.  While  control-flow  testing  tools  have  being  able  to  tackle  big  systems-large  and  long  running  programs,  DF  testing  tools  have  failed  to  do  so.  This  situation  is  in  part  due  to  the  costs  associated  with  tracking  duas  at  run-time.  Recently,  an  algorithm,  called  Bitwise  Algorithm  (BA),  which  uses  bit  vectors  and  bitwise  operations  for  tracking  intra-procedural  duas  at  run-time,  was  proposed.  This  paper  presents  the  implementation  of  BA  for  programs  compiled  into  bytecodes.  Previous  approaches  were  able  to  deal  with  small  to  medium  size  programs  with  high  penalties  in  terms  of  execution  and  memory.  Our  experimental  results  show  that  by  using  BA  we  are  able  to  tackle  large  systems  with  more  than  200  KLOCs  and  300K  required  duas.  Furthermore,  for  several  programs  the  execution  penalty  was  comparable  with  that  imposed  by  a  popular  control-flow  testing  tool.
1	Context  based  event  trace  reduction  in  client  side  javascript  applications.  Record-replay  techniques  are  developed  to  facilitate  debugging  client-side  JavaScript  application  failures.  They  faithfully  record  all  events  that  reveal  a  failure,  but  record  many  events  irrelevant  to  the  failure.  Delta  debugging  adopts  the  divide-and-conquer  algorithm  to  generate  a  minimal  event  subtrace  that  still  reveals  the  same  failure.  However,  delta  debugging  is  slow  because  it  may  generate  lots  of  syntactically  infeasible  candidate  event  subtraces  in  which  some  events  can  trigger  syntactical  errors  (e.g.,  ReferenceError  and  TypeError),  and  thus  cannot  be  replayed  as  expected.  Based  on  this  observation,  we  propose  EvMin,  an  effective  and  efficient  approach  to  remove  failure-irrelevant  events  from  an  event  trace.  We  use  the  variable  usage  information  (e.g.,  DOM  variable  usage)  in  an  event  to  model  the  event's  context.  We  require  that,  each  event  in  an  event  subtrace  has  the  compatible  context  with  its  corresponding  one  in  the  original  event  trace.  In  this  way,  we  avoid  generating  syntactically  infeasible  event  subtraces,  and  dramatically  speed  up  delta  debugging.  We  have  implemented  EvMin  and  evaluated  it  on  10  real-world  JavaScript  application  failures.  Our  evaluation  shows  that  EvMin  generates  72%  fewer  event  subtraces,  and  takes  84%  less  time  than  delta  debugging.
1	Modeling  and  verifying  topoguard  in  openflow  based  software  defined  networks.  Software  Defined  Networking  (SDN)  is  an  emerging  networking  paradigm,  which  provides  flexible  network  programmability  and  eases  the  complexity  of  network  control  and  management.  The  OpenFlow  protocol  is  the  best-known  southbound  interface  of  SDN.  As  the  core  of  a  software-defined  network,  a  controller  collects  topology  information  of  the  entire  network  in  order  to  manage  the  network  as  well  as  provide  services  to  topology-dependent  applications.  The  accuracy  of  topology  information  gained  by  a  controller  is  utmost  important.  However,  most  of  the  mainstream  OpenFlow  controllers  suffer  from  two  kinds  of  topology  poisoning  attacks:  Link  Fabrication  Attack  and  Host  Hijacking  Attack.  TopoGuard  is  the  most  famous  security  extension  to  traditional  OpenFlow  controllers,  providing  detection  of  the  two  attacks.  In  this  paper,  we  model  TopoGuard,  OpenFlow  switches,  hosts  and  two  kinds  of  attackers  using  Communication  Sequential  Processes  (CSP).  Moreover,  we  encode  the  proposed  model  into  Process  Analysis  Toolkit  (PAT),  a  model  checker.  Finally,  we  use  PAT  to  verify  whether  TopoGuard  is  able  to  detect  the  two  attacks  in  some  specific  scenarios.
1	Testing  obligation  policy  enforcement  using  mutation  analysis.  The  support  of  obligations  with  access  control  policies  allows  the  expression  of  more  sophisticated  requirements  such  as  usage  control,  availability  and  privacy.  In  order  to  enable  the  use  of  these  policies,  it  is  crucial  to  ensure  their  correct  enforcement  and  management  in  the  system.  For  this  reason,  this  paper  introduces  a  set  of  mutation  operators  for  obligation  policies.  The  paper  first  identifies  key  elements  in  obligation  policy  management,  then  presents  mutation  operators  which  injects  minimal  errors  which  affect  these  aspects.  Test  cases  are  qualified  w.r.t.  their  ability  in  detecting  problems,  simulated  by  mutation,  in  the  interactions  between  policy  management  and  the  application  code.  The  use  of  policy  mutants  as  substitutes  for  real  flaws  enables  a  first  investigation  of  testing  obligation  policies  in  a  system.  We  validate  our  work  by  providing  an  implementation  of  the  mutation  process:  the  experiments  conducted  on  a  Java  program  provide  insights  for  improving  test  selection.
1	Support  mechanisms  to  conduct  empirical  studies  in  software  engineering  a  systematic  mapping  study.  Context:  Empirical  studies  are  gaining  recognition  in  the  Software  Engineering  (SE)  research  community,  allowing  improved  quality  of  research  and  accelerating  the  adoption  of  new  technologies  in  the  software  market.  However,  empirical  studies  in  this  area  are  still  limited.  In  order  to  foster  empirical  research  in  SE,  it  is  essential  to  understand  the  resources  available  to  aid  these  studies.  Goal:  Identify  support  mechanisms  (methodology,  tool,  guideline,  process,  etc.)  used  to  conduct  empirical  studies  in  the  Empirical  Software  Engineering  (ESE)  community.  Method:  We  performed  a  systematic  mapping  study  that  included  all  full  papers  published  at  EASE,  ESEM  and  ESEJ  since  their  first  editions.  Were  selected  891  studies  between  1996  and  2013.  Results:  A  total  of  375  support  mechanisms  were  identified.  We  provide  the  full  list  of  mechanisms  and  the  strategies  that  uses  them.  Despite  this,  we  identified  a  high  number  of  studies  that  do  not  cite  any  mechanism  to  support  their  empirical  strategies:  433  studies  (48%).  Experiment  is  the  strategy  that  has  more  resources  to  support  their  activities.  And  guideline  was  the  most  used  type  of  mechanism.  Moreover  we  observed  that  the  most  mechanisms  used  as  reference  to  empirical  studies  are  not  specific  to  SE  area.  And  some  mechanisms  were  used  only  in  specific  activities  of  empirical  research,  such  as  statistical  and  qualitative  data  analysis.  Experiment  and  case  studies  are  the  strategies  most  applied.  Conclusions:  The  use  of  empirical  methods  in  SE  has  increased  over  the  years.  Despite  this,  many  studies  did  not  apply  these  methods  and  do  not  cite  any  resource  to  guide  their  research.  Therefore,  the  list  of  support  mechanisms,  where  and  how  they  were  applied  is  a  major  asset  to  the  SE  community.  Such  asset  can  encourage  empirical  studies  aiding  the  choice  regarding  which  strategies  and  mechanisms  to  use  in  a  research,  as  well  as  pointing  out  examples  where  they  were  used,  mainly  to  novice  researchers.  We  also  identified  new  perspectives  and  gaps  that  foster  other  research  for  the  improvement  of  empirical  research  in  this  area.
1	Approaches  for  prioritizing  feature  improvements  extracted  from  app  reviews.  App  reviews  contain  valuable  feedback  about  what  features  should  be  fixed  and  improved.  This  feedback  could  be  'mined'  to  facilitate  app  maintenance  and  evolution.  While  requirements  are  routinely  extracted  from  post-release  users'  feedback  in  traditional  projects,  app  reviews  are  often  generated  by  a  much  larger  client-base  with  competing  needs  and  priorities  and  ad  hoc  structure.  Although  there  has  been  interest  aimed  at  exploring  the  nature  of  issues  reported  in  app  reviews  (e.g.,  bugs  and  enhancement  requests),  prioritizing  these  outcomes  for  improving  and  evolving  apps  hasn't  received  much  attention.  In  this  preliminary  study  we  aim  to  bridge  this  gap  by  proposing  three  prioritization  approaches.  Driven  by  literature  in  other  domains,  we  identify  four  attributes  (frequency,  rating,  negative  emotions  and  deontics)  that  serve  as  the  base  constructs  for  prioritization.  Thereafter,  using  these  four  constructs,  we  develop  three  approaches  (individual  attribute-based  approach,  weighted  approach  and  regression-based  approach)  that  may  help  developers  to  prioritize  features  for  improvements.  We  evaluate  our  approaches  in  constructing  multiple  prioritized  lists  of  features  using  reviews  from  the  MyTracks  app.  It  is  anticipated  that  these  prioritized  lists  could  allow  developers  to  better  focus  their  efforts  in  deciding  which  aspects  of  their  apps  to  improve.
1	A  two  staged  survey  on  release  readiness.  Deciding  about  the  content  and  readiness  when  shipping  a  new  product  release  can  have  a  strong  impact  on  the  success  (or  failure)  of  the  product.  Having  formerly  analyzed  the  state-of-the  art  in  this  area,  the  objective  for  this  paper  was  to  better  understand  the  process  and  rationale  of  real-world  release  decisions  and  to  what  extent  research  on  release  readiness  is  aligned  with  industrial  needs.  We  designed  two  rounds  of  surveys  with  focus  on  the  current  (Survey-A)  and  the  desired  (Survey-B)  process  of  how  to  make  release  readiness  decisions.  We  received  49  and  40  valid  responses  for  Survey-A  and  Survey-B,  respectively.      In  total,  we  identified  12  main  findings  related  to  the  process,  the  rationale  and  the  tool  support  considered  for  making  release  readiness  decisions.  We  found  that  reasons  for  failed  releases  and  the  factors  considered  for  making  release  decisions  are  context  specific  and  vary  with  release  cycle  time.  Practitioners  confirmed  that  (i)  release  readiness  should  be  measured  and  continuously  monitored  during  the  whole  release  cycle,  (ii)  release  readiness  decisions  are  context-specific  and  should  not  be  based  solely  on  quality  considerations,  and  iii)  some  of  the  observed  reasons  for  failed  releases  such  as  low  functionality,  high  cost,  and  immature  service  are  not  adequately  studied  in  research  where  there  is  dominance  on  investigating  quality  and  testing  only.  In  terms  of  requested  tool  support,  dashboards  covering  multidimensional  aspects  of  the  status  of  release  development  were  articulated  as  key  requirements.
1	Testsage  regression  test  selection  for  large  scale  web  service  testing.  Regression  testing  is  an  important  but  expensive  activity  in  software  development.  Among  various  types  of  tests,  web  service  tests  are  usually  one  of  the  most  expensive  (due  to  network  communications)  but  widely  adopted  types  of  tests  in  commercial  software  development.  Regression  test  selection  (RTS)  aims  to  reduce  the  number  of  tests  which  need  to  be  retested  by  only  running  tests  that  are  affected  by  code  changes.  Although  a  large  number  of  RTS  techniques  have  been  proposed  in  the  past  few  decades,  these  techniques  have  not  been  adopted  on  large-scale  web  service  testing.  This  is  because  most  existing  RTS  techniques  either  require  direct  code  dependency  between  tests  and  code  under  test  or  cannot  be  applied  on  large  scale  systems  with  enough  efficiency.  In  this  paper,  we  present  a  novel  RTS  technique,  TestSage,  that  performs  RTS  for  web  service  tests  on  large  scale  commercial  software.  With  a  small  overhead,  TestSage  is  able  to  collect  fine  grained  (function  level)  dependency  between  test  and  service  under  test  that  do  not  directly  depend  on  each  other.  TestSage  has  also  been  successfully  applied  to  large  complex  systems  with  over  a  million  functions.  We  conducted  experiments  of  TestSage  on  a  large  scale  backend  service  at  Google.  Experimental  results  show  that  TestSage  reduces  34%  of  testing  time  when  running  all  AEC  (Analysis,  Execution  and  Collection)  phases,  50%  of  testing  time  while  running  without  collection  phase.  TestSage  has  been  integrated  with  internal  testing  framework  at  Google  and  runs  day-to-day  at  the  company.
1	On  the  evaluation  of  effort  estimation  models.  Background.  Using  accurate  effort  estimation  models  can  help  software  companies  plan,  monitor,  and  control  their  development  process  and  development  costs.  It  is  therefore  important  to  define  sound  accuracy  indicators  that  allow  practitioners  and  researchers  to  assess  and  rank  different  effort  estimation  models  so  that  practitioners  can  select  the  most  accurate,  and  therefore  useful  one.  Several  accuracy  indicators  exist,  with  different  advantages  and  disadvantages.      Objective.  We  propose  a  general  framework  for  building  sound  accuracy  indicators  for  effort  estimation  models.      Method.  The  accuracy  indicators  that  comply  with  our  proposal  are  built  by  means  of  a  comparison  between  a  reference  effort  estimation  model  and  the  specific  model  whose  accuracy  we  would  like  to  assess.  Several  existing  indicators  are  built  this  way:  we  develop  a  framework  so  new  indicators  can  be  defined  in  a  sound  way.      Results.  From  a  theoretical  point  of  view,  we  applied  our  approach  to  accuracy  indicators  based  on  the  square  of  the  residuals  and  the  absolute  value  of  the  residuals.  We  show  that  using  a  random  model  as  a  reference  model,  as  done  in  some  recent  literature,  sets  too  low  a  bar  in  terms  of  what  may  be  acceptable.  Instead,  we  use  reference  models  that  are  built  based  on  constant  functions.  From  a  practical  point  of  view,  we  applied  our  approach  to  datasets  containing  measures  of  industrial  software  development  projects.  With  the  proposed  method  we  were  able  to  derive  indications  both  according  to  criteria  already  proposed  in  the  literature  and  according  to  new  criteria.      Conclusions.  Our  method  can  be  used  to  define  sound  accuracy  indicators  for  effort  estimation  models.
1	Test  adequacy  evaluation  for  the  user  database  interaction  a  specification  based  approach.  Testing  a  database  application  is  a  challenging  process  where  both  the  database  and  the  user  interaction  have  to  be  considered  in  the  design  of  test  cases.  This  paper  describes  a  specification-based  approach  to  guide  the  design  of  test  inputs  (both  the  test  database  and  the  user  inputs)  for  a  database  application  and  to  automatically  evaluate  the  test  adequacy.  First,  the  system  specification  of  the  application  is  modelled:  (1)  the  structure  of  the  database  and  the  user  interface  are  represented  in  a  single  model,  called  Integrated  Data  Model  (IDM),  (2)  the  functional  requirements  are  expressed  as  a  set  of  business  rules,  written  in  terms  of  the  IDM.  Then,  a  MCDC-based  criterion  is  applied  over  the  business  rules  to  automatically  derive  the  situations  of  interest  to  be  tested  (test  requirements),  which  guide  the  design  of  the  test  inputs.  Finally,  the  adequacy  of  these  test  inputs  is  automatically  evaluated  to  determine  whether  the  test  requirements  are  covered.  The  approach  has  been  applied  to  the  TPC-C  benchmark.  The  results  show  that  it  allows  designing  test  cases  that  are  able  to  detect  interesting  faults  which  were  located  in  the  procedural  code  of  the  implementation.
1	An  assessment  of  the  quality  of  automated  program  operator  repair.  Automated  program  repair  (APR)  techniques  fix  faults  by  repeatedly  modifying  suspicious  code  until  a  program  passes  a  set  of  test  cases.  Although  generating  a  repair  is  the  goal  of  APR,  a  repair  can  have  negative  consequences.  The  quality  of  a  repair  is  reduced  when  the  repair  introduces  new  faults  and/or  degrades  maintainability  by  adding  irrelevant  but  functionally  benign  code.  We  used  two  APR  approaches  to  repair  faulty  binary  operators:  (1)  find  a  repair  in  existing  code  by  applying  a  genetic  algorithm  to  replace  suspicious  code  with  other  existing  code  as  done  by  GenProg,  and  (2)  mutate  suspicious  operators  within  a  genetic  algorithm.  Mutating  operators  was  clearly  more  effective  in  repairing  faulty  operators  than  using  existing  code  for  a  repair.  We  also  evaluated  the  approaches  in  terms  of  two  potential  negative  effects:  (1)  the  introduction  of  new  faults  and  (2)  a  reduction  of  program  maintainability.  We  found  that  repair  processes  that  use  tests  that  satisfy  branch  coverage  reduce  the  number  of  new  faults.  In  contrast,  repair  processes  using  tests  that  satisfy  statement  coverage  and  randomly  generated  tests  introduce  numerous  new  faults.  We  also  demonstrate  that  a  mutation  based  repair  process  produces  repairs  that  should  be  more  maintainable  compared  to  those  produced  using  existing  code.
1	An  empirical  study  on  the  use  of  defect  prediction  for  test  case  prioritization.  Test  case  prioritization  has  been  extensively  re-searched  as  a  means  for  reducing  the  time  taken  to  discover  regressions  in  software.  While  many  different  strategies  have  been  developed  and  evaluated,  prior  experiments  have  shown  them  to  not  be  effective  at  prioritizing  test  suites  to  find  real  faults.  This  paper  presents  a  test  case  prioritization  strategy  based  on  defect  prediction,  a  technique  that  analyzes  code  features  –  such  as  the  number  of  revisions  and  authors  —  to  estimate  the  likelihood  that  any  given  Java  class  will  contain  a  bug.  Intuitively,  if  defect  prediction  can  accurately  predict  the  class  that  is  most  likely  to  be  buggy,  a  tool  can  prioritize  tests  to  rapidly  detect  the  defects  in  that  class.  We  investigated  how  to  configure  a  defect  prediction  tool,  called  Schwa,  to  maximize  the  likelihood  of  an  accurate  prediction,  surfacing  the  link  between  perfect  defect  prediction  and  test  case  prioritization  effectiveness.  Using  6  real-world  Java  programs  containing  395  real  faults,  we  conducted  an  empirical  evaluation  comparing  this  paper's  strategy,  called  G-clef,  against  eight  existing  test  case  prioritization  strategies.  The  experiments  reveal  that  using  defect  prediction  to  prioritize  test  cases  reduces  the  number  of  test  cases  required  to  find  a  fault  by  on  average  9.48%  when  compared  with  existing  coverage-based  strategies,  and  10.4%  when  compared  with  existing  history-based  strategies.
1	Exhaustive  exploration  of  the  failure  oblivious  computing  search  space.  High-availability  of  software  systems  requires  automated  handling  of  crashes  in  presence  of  errors.  Failure-oblivious  computing  is  one  technique  that  aims  to  achieve  high  availability.  We  note  that  failure-obliviousness  has  not  been  studied  in  depth  yet,  and  there  is  very  few  study  that  helps  understand  why  failure-oblivious  techniques  work.  In  order  to  make  failure-oblivious  computing  to  have  an  impact  in  practice,  we  need  to  deeply  understand  failure-oblivious  behaviors  in  software.  In  this  paper,  we  study,  design  and  perform  an  experiment  that  analyzes  the  size  and  the  diversity  of  the  failure-oblivious  behaviors.  Our  experiment  consists  of  exhaustively  computing  the  search  space  of  16  field  failures  of  large-scale  open-source  Java  software.  The  outcome  of  this  experiment  is  a  much  better  understanding  of  what  really  happens  when  failure-oblivious  computing  is  used,  and  this  opens  new  promising  research  directions.
1	Formal  modeling  and  verification  of  sdn  openflow.  Software-Defined  Networking  (SDN)  is  a  network  architecture  where  a  controller  manages  flow  control  to  enable  intelligent  networking.  Currently,  a  popular  specification  for  creating  an  SDN  is  an  open  standard  called  OpenFlow.  The  behavior  of  the  SDN  OpenFlow  (SDN-OF)  is  critical  to  the  safety  of  the  network  system  and  its  correctness  must  be  proven  so  as  to  avoid  system  failures.  In  this  paper,  we  report  our  experience  in  applying  formal  techniques  for  modeling  and  analysis  of  SDN-OF.  The  formal  model  of  SDN-OF  is  described  in  detail  and  its  correctness  is  formalized  in  logical  formulas  based  on  the  informal  specification.  The  desired  properties  are  verified  over  the  model  using  VERSA  and  UPPAAL.  Our  work-in-progressinvolves  the  development  of  a  model  translation  tool  that  facilitates  automatic  conversion  of  the  verified  model  to  Python  for  modular  code  synthesis  on  the  application  platform
1	Automated  repair  of  internationalization  presentation  failures  in  web  pages  using  style  similarity  clustering  and  search  based  techniques.  Internationalization  enables  companies  to  reach  a  global  audience  by  adapting  their  websites  to  locale  specific  language  and  content.  However,  such  translations  can  often  introduce  Internationalization  Presentation  Failures  (IPFs)  —  distortions  in  the  intended  appearance  of  a  website.  It  is  challenging  for  developers  to  design  websites  that  can  inherently  adapt  to  varying  lengths  of  text  from  different  languages.  Debugging  and  repairing  IPFs  is  complicated  by  the  large  number  of  HTML  elements  and  CSS  properties  that  define  a  web  page's  appearance.  Tool  support  is  also  limited  as  existing  techniques  can  only  detect  IPFs,  with  the  repair  remaining  a  labor  intensive  manual  task.  To  address  this  problem,  we  propose  a  search-based  technique  for  automatically  repairing  IPFs  in  web  applications.  Our  empirical  evaluation  showed  that  our  approach  was  able  to  successfully  resolve  98%  of  the  reported  IPFs  for  23  real-world  web  pages.  In  a  user  study,  participants  rated  the  visual  quality  of  our  fixes  significantly  higher  than  the  unfixed  versions.
1	Virtualized  fault  injection  testing  a  machine  learning  approach.  We  introduce  a  new  methodology  for  virtualized  fault  injection  testing  of  safety  critical  embedded  systems.  This  approach  fully  automates  the  key  steps  of  test  case  generation,  fault  injection  and  verdict  construction.  We  use  machine  learning  to  reverse  engineer  models  of  the  system  under  test.  We  use  model  checking  to  generate  test  verdicts  with  respect  to  safety  requirements  formalised  in  temporal  logic.  We  exemplify  our  approach  by  implementing  a  tool  chain  based  on  integrating  the  QEMU  hardware  emulator,  the  GNU  debugger  GDB  and  the  LBTest  requirements  testing  tool.  This  tool  chain  is  then  evaluated  on  two  industrial  safety  critical  applications  from  the  automotive  sector.
1	An  empirical  analysis  of  reopened  bugs  based  on  open  source  projects.  Background:  Bug  fixing  is  a  long-term  and  time-consuming  activity.  A  software  bug  experiences  a  typical  life  cycle  from  newly  reported  to  finally  closed  by  developers,  but  it  could  be  reopened  afterwards  for  further  actions  due  to  reasons  such  as  unclear  description  given  by  the  bug  reporter  and  developer  negligence.  Bug  reopening  is  neither  desirable  nor  could  be  completely  avoided  in  practice,  and  it  is  more  likely  to  bring  unnecessary  workloads  to  already-busy  developers.  Aims:  To  the  best  of  our  knowledge,  there  has  been  a  little  previous  work  on  software  bug  reopening.  In  order  to  further  study  in  this  area,  we  perform  an  empirical  analysis  to  provide  a  comprehensive  understanding  of  this  special  area.  Method:  Based  on  four  open  source  projects  from  Eclipse  product  family,  they  are  CDT,  JDT,  PDE  and  Platform,  we  first  quantitatively  analyze  reopened  bugs  from  perspectives  of  proportion,  impacts  and  time  distribution.  After  initial  exploration  on  their  characteristics,  we  then  qualitatively  summarize  root  causes  for  bug  reopening,  this  is  carried  out  by  investigating  developer  discussions  recorded  in  Eclipse  Bugzilla.  Results:  Results  show  that  6%--10%  of  total  bugs  will  lead  to  reopening  eventually.  Over  93%  of  reopened  bugs  place  serious  influence  on  the  normal  operation  of  the  system  being  developed.  Several  key  reasons  for  bug  reopening  have  been  identified  in  our  empirical  study.  Conclusions:  Although  reopened  bugs  have  significant  impacts  on  both  end  users  and  developers,  it  is  quite  possible  to  reduce  bug  reopening  rate  through  the  adoption  of  appropriate  methods,  such  as  promoting  effective  and  efficient  communication  among  bug  reporters  and  developers,  which  is  supported  by  empirical  evidence  in  this  study.
1	Game  based  verification  of  contract  signing  protocols  with  minimal  messages.  A  multi-party  contract  signing  (MPCS)  protocol  is  used  for  a  group  of  signers  to  sign  a  digital  contract  over  a  network.  We  analyse  the  protocols  of  Mauw,  Radomirovic  and  Torabi  Dashti  (MRT),  using  the  finite-state  model  checker  Mocha.  Mocha  allows  for  the  specification  of  properties  in  alternating-time  temporal  logic  (ATL)  with  game  semantics,  and  the  model  checking  problem  for  ATL  requires  the  computation  of  winning  strategies.  This  gives  us  an  intuitive  interpretation  of  the  verification  problem  of  crucial  properties  of  MPCS  protocols.  MRT  protocols  can  be  generated  from  minimal  message  sequences,  depending  on  the  number  of  signers.  We  discover  an  attack  on  fairness  in  a  published  MRT  protocol  with  three  signers  and  a  general  attack  on  abuse-freeness  for  all  MRT  protocols.  For  both  attacks,  we  present  solutions.  The  abuse-freeness  attack  leads  us  to  a  revision  of  the  methodology  to  construct  an  MRT  protocol.  Following  this  revised  methodology,  we  design  a  number  of  MRT  protocols  using  minimal  message  sequences  for  three  and  four  signers,  all  of  whom  have  been  successfully  model  checked  in  Mocha.
1	Achieving  lossless  compression  of  audio  by  encoding  its  constituted  components  lcaec.  In  this  paper,  an  approach  has  been  made  to  produce  a  compressed  audio  without  losing  any  information.  Separating  amplitude  and  phase  components  followed  by  encoding  individual  components  into  suitable  pattern  is  done  in  this  technique.  Further,  binary  encoding  techniques  are  applied  on  these  generated  patterns  to  produce  more  compressed  representation  of  audio.  The  encoding  of  amplitude  is  done  by  adaptive  differential  pulse-code  modulation-based  technique  with  various  quantization  levels  considering  neighbouring  sampled  values  of  a  particular  block.  Both  encoded  phase  and  amplitude  signals  are  represented  with  less  volume  of  data.  Amplitude  signal  is  further  compressed  with  applying  Burrows–Wheeler  transform  and  Huffman  encoding  technique,  respectively.  Experimental  results  are  supported  by  statistical  parameter  (compression  ratio)  with  other  parameters  (encoding  time  and  decoding  time)  in  comparison  with  other  existing  techniques  to  justify  the  efficiency  of  the  present  compression  technique.
1	Prioritizing  test  scenarios  from  uml  communication  and  activity  diagrams.  Due  to  the  large  size  and  complexity  of  software,  exhaustive  testing  becomes  impossible.  Hence,  testing  must  be  done  in  an  optimized  way  keeping  in  mind  factors,  such  as  requirements  of  the  customer,  cost  and  time.  For  this,  there  is  a  need  to  generate  test  cases  and  exercise  them  to  gain  maximum  throughput  by  uncovering  defects.  Test  case/scenario  prioritization  is  a  well  known  and  efficient  technique  to  ensure  the  software  quality.  Prioritization  of  test  scenarios  helps  in  early  detection  of  bugs.  In  this  paper,  we  present  an  integrated  approach  and  a  prioritization  technique  to  generate  cluster-level  test  scenarios  from  UML  communication  and  activity  diagrams.  In  our  approach,  we  first  construct  a  tree  representation  of  communication  diagrams,  and  then  a  tree  representation  of  activity  diagrams.  We  convert  them  into  an  intermediate  tree  named  as  COMMACT  tree.  We,  then  carry  out  a  post-order  traversal  of  the  constructed  tree  for  selecting  conditional  predicates  from  the  intermediate  tree.  Then,  we  propose  an  algorithm  to  generate  test  scenarios  from  the  constructed  tree.  Next,  the  necessary  information,  such  as  method-activity  sequence,  associated  objects,  and  constraint  conditions  is  extracted  from  test  scenario.  The  test  sequences  are  a  set  of  theoretical  paths  starting  from  initialization  to  end,  while  taking  conditions  (pre-  and  post-condition)  into  consideration.  Each  generated  test  sequence  corresponds  to  a  particular  scenario  of  the  considered  use  case.  The  third  phase  is  to  generate  test  scenarios  from  the  tree  satisfying  the  message---activity  path  test  adequacy  criteria.  Preliminary  results  obtained  on  a  case-study  indicate  that  the  technique  is  effective  in  extracting  the  critical  scenarios  from  the  communication  and  activity  diagrams.  Our  approach  generates  redundant  test  scenarios  and  still  achieves  adequate  test  coverage.
1	Towards  optimal  software  engineering  learning  from  agile  practice.  In  essence,  optimal  software  engineering  means  creating  the  right  product,  through  the  right  process,  to  the  overall  satisfaction  of  everyone  involved.  Adopting  the  agile  approach  to  software  development  appears  to  have  helped  many  companies  make  substantial  progress  towards  that  goal.  The  purpose  of  this  paper  is  to  clarify  that  contribution  from  comparative  survey  information  gathered  in  2010  and  2012.  The  surveys  were  undertaken  in  software  development  companies  across  Northern  Ireland.  The  paper  describes  the  design  of  the  surveys  and  discusses  optimality  in  relation  to  the  results  obtained.  Both  surveys  aimed  to  achieve  comprehensive  coverage  of  a  single  region  rather  than  rely  on  a  voluntary  sample.  The  main  outcome  from  the  work  is  a  collection  of  insights  into  the  nature  and  advantages  of  agile  development,  suggesting  how  further  progress  towards  optimality  might  be  achieved.
1	What  attracts  newcomers  to  onboard  on  oss  projects  tl  dr  popularity.  Voluntary  contributions  play  an  important  role  in  maintaining  Open  Source  Software  (OSS)  projects  active.  New  volunteers  feel  motivated  to  contribute  to  OSS  projects  based  on  a  set  of  motivations.  In  this  study,  we  aim  to  understand  which  factors  OSS  projects  usually  maintain  that  might  influence  their  new  contributors’  onboarding.  Using  a  set  of  450  repositories,  we  investigated  mixed  factors,  such  as  the  project  age,  the  number  of  stars,  the  programming  language  used,  or  the  presence  of  text  files  that  aid  contributors  (e.g.,  templates  for  pull-requests  or  license  files).  We  used  a  K-Spectral  Centroid  (KSC)  clustering  algorithm  to  investigated  the  newcomers’  growth  rate  for  the  analyzed  projects.  We  could  found  three  common  patterns:  a  logarithmic,  an  exponential,  and  a  linear  growth  pattern.  Based  on  these  patterns,  we  used  a  Random  Forest  classifier  to  understand  how  each  factor  could  explain  the  growth  rates.  We  found  that  popularity  of  the  project  (in  terms  of  stars),  time  to  review  pull  requests,  project  age,  and  programming  languages  are  the  factors  that  best  explain  the  newcomers’  growth  patterns.
1	Scaling  and  internationalizing  an  agile  foss  project  lessons  learned.  This  paper  describes  problems  that  arose  with  the  scaling  and  internationalization  of  the  open  source  project  Catrobat.  The  problems  we  faced  were  the  lack  of  a  centralized  user  management,  insufficient  scaling  of  our  communication  channels,  and  the  necessity  to  adapt  agile  development  techniques  to  remote  collaboration.  To  solve  the  problems  we  decided  to  use  a  mix  of  open  source  tools  (Git,  IRC,  LDAP)  and  commercial  solutions  (Jira,  Confluence,  GitHub)  because  we  believe  that  this  mix  best  fits  our  needs.  Other  projects  can  benefit  from  the  lessons  we  learned  during  the  reorganization  of  our  knowledge  base  and  communication  tools,  as  infrastructure  changes  can  be  very  labor-intensive  and  time-consuming.
1	Exploring  the  role  of  outside  organizations  in  free  open  source  software  projects.  Free/Open  Source  Software  (FOSS)  projects  have  a  reputation  for  being  grass-roots  efforts  driven  by  individual  contributors  volunteering  their  time  and  effort.  While  this  may  be  true  for  a  majority  of  smaller  projects,  it  is  not  always  the  case  for  large  projects.  As  projects  grow  in  size,  importance  and  complexity,  many  come  to  depend  on  corporations,  universities,  NGO’s  and  governments,  for  support  and  contributions,  either  financially  or  through  seconded  staff.  As  outside  organizations  get  involved  in  projects,  how  does  this  affect  their  governance,  transparency  and  direction?  To  study  this  question  we  gathered  bug  reports  and  commit  logs  for  GCC  and  the  Linux  Kernel.  We  found  that  outside  organizations  contribute  a  majority  of  code  but  rarely  participate  in  bug  triaging.  Therefore  their  code  does  not  necessarily  address  the  needs  of  others  and  may  distort  governance  and  direction.  We  conclude  that  projects  should  examine  their  dependence  on  outside  organizations.
1	The  riscoss  platform  for  risk  management  in  open  source  software  adoption.  Managing  risks  related  to  OSS  adoption  is  a  must  for  organizations  that  need  to  smoothly  integrate  OSS-related  practices  in  their  development  processes.  Adequate  tool  support  may  pave  the  road  to  effective  risk  management  and  ensure  the  sustainability  of  such  activity.  In  this  paper,  we  present  the  RISCOSS  platform  for  managing  risks  in  OSS  adoption.  RISCOSS  builds  upon  a  highly  configurable  data  model  that  allows  customization  to  several  types  of  scopes.  It  implements  two  different  working  modes:  exploration,  where  the  impact  of  decisions  may  be  assessed  before  making  them;  and  continuous  assessment,  where  risk  variables  (and  their  possible  consequences  on  business  goals)  are  continuously  monitored  and  reported  to  decision-makers.  The  blackboard-oriented  architecture  of  the  platform  defines  several  interfaces  for  the  identified  techniques,  allowing  new  techniques  to  be  plugged  in.
1	Repairing  and  optimizing  hadoop  hashcode  implementations.  We  describe  how  contract  violations  in  JavaTM  hashCode  methods  can  be  repaired  using  novel  combination  of  semantics-preserving  and  generative  methods,  the  latter  being  achieved  via  Automatic  Improvement  Programming.  The  method  described  is  universally  applicable.  When  applied  to  the  Hadoop  platform,  it  was  established  that  it  produces  hashCode  functions  that  are  at  least  as  good  as  the  original,  broken  method  as  well  as  those  produced  by  a  widely-used  alternative  method  from  the  ‘Apache  Commons’  library.
1	Improving  the  performance  of  many  objective  software  refactoring  technique  using  dimensionality  reduction.  Software  quality  Assessment  involves  the  measurement  of  a  large  number  of  software  attributes  referred  to  as  quality  metrics.  In  most  searched-based  software  engineering  processes,  an  optimization  algorithm  is  used  to  evaluate  a  certain  number  of  maintenance  operations  by  minimizing  or  maximizing  these  quality  metrics.  One  such  process  is  software  refactoring.  When  the  solution  to  the  problem  includes  a  large  number  of  objectives,  various  difficulties  arise,  including  the  determination  of  the  Pareto-optimal  front,  and  the  visualization  of  the  solutions.  However,  in  some  refactoring  problem,  there  may  be  redundancies  among  any  two  or  more  objectives.  In  this  paper,  we  propose  a  new  software  refactoring  approach  named  PCA-NSGA-II  many-objective  refactoring.  This  approach  is  based  on  the  PCA-NSGA-II  evolutionary  multi-objective  algorithm,  and  can  overcome  the  curse  of  dimensionality  by  removing  redundancies  to  retain  conflicting  objectives  for  further  analysis.
1	Complexity  metrics  for  hierarchical  state  machines.  Automatically  generated  state  machines  are  constrained  by  their  complexity,  which  can  be  reduced  via  hierarchy  generation.  A  technique  has  been  demonstrated  for  hierarchy  generation,  although  evaluation  of  this  technique  has  proved  difficult.    There  are  a  variety  of  metrics  that  can  be  used  to  provide  indicators  of  how  complicated  a  state  machine  or  statechart  is,  one  such  example  is  cyclomatic  complexity  (the  number  of  edges  -  the  number  of  states  +  2).  Despite  this,  the  existing  complexity  metric  for  statecharts  does  not  operate  on  the  hierarchy,  instead  providing  an  equivalent  cyclomatic  complexity  for  statecharts  by  ignoring  it.    This  paper  defines  two  new  metrics;  Top  Level  Cyclomatic  Complexity  and  Hierarchical  Cyclomatic  Complexity.  These  metrics  assess  the  complexity  of  a  hierarchical  machine  directly,  as  well  as  allowing  for  comparison  between  the  original,  flat  state  machine  and  its  hierarchical  counterpart.
1	Mutation  based  generation  of  software  product  line  test  configurations.  Software  Product  Lines  (SPLs)  are  families  of  software  products  that  can  be  configured  and  managed  through  a  combination  of  features.  Such  products  are  usually  represented  with  a  Feature  Model  (FM).  Testing  the  entire  SPL  may  not  be  conceivable  due  to  economical  or  time  constraints  and,  more  simply,  because  of  the  large  number  of  potential  products.  Thus,  defining  methods  for  generating  test  configurations  is  required,  and  is  now  a  very  active  research  topic  for  the  testing  community.  In  this  context,  mutation  has  recently  being  advertised  as  a  promising  technique.  Mutation  evaluates  the  ability  of  the  test  suite  to  detect  defective  versions  of  the  FM,  called  mutants.  In  particular,  it  has  been  shown  that  existing  test  configurations  achieving  the  mutation  criterion  correlate  with  fault  detection.  Despite  the  potential  benefit  of  mutation,  there  is  no  approach  which  aims  at  generating  test  configurations  for  SPL  with  respect  to  the  mutation  criterion.  In  this  direction,  we  introduce  a  search-based  approach  which  explores  the  SPL  product  space  to  generate  product  test  configurations  with  the  aim  of  detecting  mutants.
1	Rewrite  based  statistical  model  checking  of  wmtl.  We  present  a  new  technique  for  verifying  Weighted  Metric  Temporal  Logic  (WMTL)  properties  of  Weighted  Timed  Automata.  Our  approach  relies  on  Statistical  Model  Checking  combined  with  a  new  monitoring  algorithm  based  on  rewriting  rules.  Contrary  to  existing  monitoring  approaches  for  WMTL  ours  is  exact.  The  technique  has  been  implemented  in  the  statistical  model  checking  engine  of  Uppaal  and  experiments  indicate  that  the  technique  performs  faster  than  existing  approaches  and  leads  to  more  accurate  results.
1	Recommending  posts  concerning  api  issues  in  developer  q  a  sites.  API  design  is  known  to  be  a  challenging  craft,  as  API  designers  must  balance  their  elegant  ideals  against  "real-world"  concerns,  such  as  utility,  performance,  backwards  compatibility,  and  unforeseen  emergent  uses.  However,  to  date,  there  is  no  principled  method  to  collect  or  analyze  API  usability  information  that  incorporates  input  from  typical  developers.  In  practice,  developers  often  turn  to  QaA  websites  such  as  stackoverflow.com  (SO)  when  seeking  expert  advice  on  API  use,  the  popularity  of  such  sites  has  thus  led  to  a  very  large  volume  of  unstructured  information  that  can  be  searched  with  diligence  for  answers  to  specific  questions.  The  collected  wisdom  within  such  sites  could,  in  principle,  be  of  great  help  to  API  designers  to  better  support  developer  needs,  if  only  it  could  be  collected,  analyzed,  and  distilled  for  practical  use.  In  this  paper,  we  present  a  methodology  that  combines  several  techniques,  including  social  network  analysis  and  topic  mining,  to  recommend  SO  posts  that  are  likely  to  concern  API  design-related  issues.  To  establish  a  comparison  baseline,  we  introduce  two  more  recommendation  approaches:  a  reputation-based  recommender  and  a  random  recommender.  We  have  found  that  when  applied  to  QaA  discussion  of  two  popular  mobile  platforms,  Android  and  iOS,  our  methodology  achieves  up  to  93%  accuracy  and  is  more  stable  with  its  recommendations  when  compared  to  the  two  baseline  techniques.
1	The  maven  dependency  graph  a  temporal  graph  based  representation  of  maven  central.  The  Maven  Central  Repository  provides  an  extraordinary  source  of  data  to  understand  complex  architecture  and  evolution  phenomena  among  Java  applications.  As  of  September  6,  2018,  this  repository  includes  2.8M  artifacts  (compiled  piece  of  code  implemented  in  a  JVM-based  language),  each  of  which  is  characterized  with  metadata  such  as  exact  version,  date  of  upload  and  list  of  dependencies  towards  other  artifacts.  Today,  one  who  wants  to  analyze  the  complete  ecosystem  of  Maven  artifacts  and  their  dependencies  faces  two  key  challenges:  (i)  this  is  a  huge  data  set;  and  (ii)  dependency  relationships  among  artifacts  are  not  modeled  explicitly  and  cannot  be  queried.  In  this  paper,  we  present  the  Maven  Dependency  Graph.  This  open  source  data  set  provides  two  contributions:  a  snapshot  of  the  whole  Maven  Central  taken  on  September  6,  2018,  stored  in  a  graph  database  in  which  we  explicitly  model  all  dependencies;  an  open  source  infrastructure  to  query  this  huge  dataset.
1	Googling  for  software  development  what  developers  search  for  and  what  they  find.  Developers  often  search  for  software  resources  on  the  web.  In  practice,  instead  of  going  directly  to  websites  (e.g.,  Stack  Overflow),  they  rely  on  search  engines  (e.g.,  Google).  Despite  this  being  a  common  activity,  we  are  not  yet  aware  of  what  developers  search  from  the  perspective  of  popular  software  development  websites  and  what  search  results  are  returned.  With  this  knowledge,  we  can  understand  real-world  queries,  developers’  needs,  and  the  query  impact  on  the  search  results.  In  this  paper,  we  provide  an  empirical  study  to  understand  what  developers  search  on  the  web  and  what  they  find.  We  assess  1.3M  queries  to  popular  programming  websites  and  we  perform  thousands  of  queries  on  Google  to  explore  search  results.  We  find  that  (i)  developers’  queries  typically  start  with  keywords  (e.g.,  Python,  Android,  etc.),  are  short  (3  words),  tend  to  omit  functional  words,  and  are  similar  among  each  other;  (ii)  minor  changes  to  queries  do  not  largely  affect  the  Google  search  results,  however,  some  cosmetic  changes  may  have  a  non-negligible  impact;  and  (iii)  search  results  are  dominated  by  Stack  Overflow,  but  YouTube  is  also  a  relevant  source  nowadays.  We  conclude  by  presenting  detailed  implications  for  researchers  and  developers.
1	Do  onboarding  programs  work.  Open  source  software  systems  rely  on  community  source  code  contributions  to  fix  bugs  and  develop  new  features.  Unfortunately,  it  is  often  difficult  to  become  an  effective  contributor  on  open-source  projects  due  to  the  complexity  of  the  tools  required  to  develop  and  test  new  patches  and  the  challenge  of  breaking  into  an  already-formed  social  organization.  To  help  new  contributors  learn  their  development  practices,  OSS  projects  have  created  on  boarding  programs  that,  for  example,  identify  easy  'first  bugs'  and  mentor  new  developers'  contributions.  However,  we  found  that  developers  who  join  an  organization  through  these  programs  are  half  as  likely  to  transition  into  long-term  community  members  than  developers  who  do  not  use  these  programs.  Measuring  the  impact  of  these  programs  is  important,  as  coordinating  and  staffing  on  boarding  projects  is  expensive.  This  paper  examines  on  boarding  programs  employed  by  Mozilla  and  demonstrates  that  they  are  not  as  effective  at  transitioning  new  developers  into  long-term  contributors  as  might  be  hoped,  although  developers  who  do  succeed  through  these  programs  find  them  valuable.
1	Why  are  android  apps  removed  from  google  play  a  large  scale  empirical  study.  To  ensure  the  quality  and  trustworthiness  of  the  apps  within  its  app  market  (i.e.,  Google  Play),  Google  has  released  a  series  of  policies  to  regulate  app  developers.  As  a  result,  policy-violating  apps  (e.g.,  malware,  low-quality  apps,  etc.)  have  been  removed  by  Google  Play  periodically.  In  reality,  we  have  found  that  the  number  of  removed  apps  are  actually  much  more  than  what  we  have  expected,  as  almost  half  of  all  the  apps  have  been  removed  or  replaced  from  Google  Play  during  a  two  year  period  from  2015  to  2017.  However,  despite  the  significant  number  of  removed  apps,  there  are  almost  no  study  on  the  characterization  of  these  removed  apps.  To  this  end,  this  paper  takes  the  first  step  to  understand  why  Android  apps  are  removed  from  Google  Play,  aiming  at  observing  promising  insights  for  both  market  maintainers  and  app  developers  towards  building  a  better  app  ecosystem.  By  leveraging  two  app  sets  crawled  from  Google  Play  in  2015  (over  1.5  million)  and  2017  (over  2.1  million),  we  have  identified  a  set  of  over  790K  removed  apps,  which  are  then  thoroughly  investigated  in  various  aspects.  The  experimental  results  have  revealed  various  interesting  findings,  as  well  as  insights  for  future  research  directions.
1	Using  large  scale  anomaly  detection  on  code  to  improve  kotlin  compiler.  In  this  work,  we  apply  anomaly  detection  to  source  code  and  byte-code  to  facilitate  the  development  of  a  programming  language  and  its  compiler.  We  define  anomaly  as  a  code  fragment  that  is  different  from  typical  code  written  in  a  particular  programming  language.  Identifying  such  code  fragments  is  beneficial  to  both  language  developers  and  end  users,  since  anomalies  may  indicate  potential  issues  with  the  compiler  or  with  runtime  performance.  Moreover,  anomalies  could  correspond  to  problems  in  language  design.  For  this  study,  we  choose  Kotlin  as  the  target  programming  language.  We  outline  and  discuss  approaches  to  obtaining  vector  representations  of  source  code  and  bytecode  and  to  the  detection  of  anomalies  across  vectorized  code  snippets.  The  paper  presents  a  method  that  aims  to  detect  two  types  of  anomalies:  syntax  tree  anomalies  and  so-called  compiler-induced  anomalies  that  arise  only  in  the  compiled  bytecode.  We  describe  several  experiments  that  employ  different  combinations  of  vectorization  and  anomaly  detection  techniques  and  discuss  types  of  detected  anomalies  and  their  usefulness  for  language  developers.  We  demonstrate  that  the  extracted  anomalies  and  the  underlying  extraction  technique  provide  additional  value  for  language  development.
1	Hydrochemical  facies  and  ionic  exchange  in  coastal  aquifers  of  puducherry  region  india  implications  for  seawater  intrusion.  Purpose  Salinization  of  groundwater  by  seawater  intrusion  is  a  major  concern  for  the  coastal  aquifers  worldwide.  Seawater  intrusion  occurs  mainly  due  to  overpumping  of  freshwater  and  sea-level  rise  which  causes  lateral  and  vertical  movements  of  seawater  into  the  coastal  aquifers.
1	A  multiple  comparative  study  of  test  with  development  product  changes  and  their  effects  on  team  speed  and  product  quality.  Researchers  have  typically  studied  the  effects  of  Test-First  Development  (TFD),  compared  to  Test-Last  Development  (TLD),  across  groups  or  projects,  and  for  relatively  short  durations.  We  defined  Test-With  Development  (TWD)  as  more  general  than  the  fine-grained  step  of  TFD,  but  also  in  contrast  to  the  large-grained  phase  of  TLD.  With  our  definition,  we  performed  a  multiple  comparative  study  to  explore  and  describe  TWD  product  changes,  and  the  effects  of  those  changes  on  two  attributes  related  to  team  speed  and  two  attributes  related  to  product  quality,  within  six  long-term  open-source  projects.  Our  results  indicate  that  when  developers  exercised  some  of  their  changes  with  automated  tests,  on  average  they  made  significantly  larger  changes  over  time  while  significantly  reducing  their  product's  complexity.  And,  when  they  exercised  all  of  their  changes  with  tests,  on  average  they  made  significantly  smaller  changes  over  time.  We  interpret  these  results  to  indicate  that  practicing  TWD  supports  faster  simplification  of  a  product.  Therefore,  we  conclude  that  teams  that  need  to  reduce  their  product's  complexity  can  benefit  from  practicing  TWD.
1	Integrating  conceptual  and  logical  couplings  for  change  impact  analysis  in  software.  The  paper  presents  an  approach  that  combines  conceptual  and  evolutionary  techniques  to  support  change  impact  analysis  in  source  code.  Conceptual  couplings  capture  the  extent  to  which  domain  concepts  and  software  artifacts  are  related  to  each  other.  This  information  is  derived  using  Information  Retrieval  based  analysis  of  textual  software  artifacts  that  are  found  in  a  single  version  of  software  (e.g.,  comments  and  identifiers  in  a  single  snapshot  of  source  code).  Evolutionary  couplings  capture  the  extent  to  which  software  artifacts  were  co-changed.  This  information  is  derived  from  analyzing  patterns,  relationships,  and  relevant  information  of  source  code  changes  mined  from  multiple  versions  in  software  repositories.  The  premise  is  that  such  combined  methods  provide  improvements  to  the  accuracy  of  impact  sets  compared  to  the  two  individual  approaches.  A  rigorous  empirical  assessment  on  the  changes  of  the  open  source  systems  Apache  httpd,  ArgoUML,  iBatis,  KOffice,  and  jEdit  is  also  reported.  The  impact  sets  are  evaluated  at  the  file  and  method  levels  of  granularity  for  all  the  software  systems  considered  in  the  empirical  evaluation.  The  results  show  that  a  combination  of  conceptual  and  evolutionary  techniques,  across  several  cut-off  points  and  periods  of  history,  provides  statistically  significant  improvements  in  accuracy  over  either  of  the  two  techniques  used  independently.  Improvements  in  F-measure  values  of  up  to  14%  (from  3%  to  17%)  over  the  conceptual  technique  in  ArgoUML  at  the  method  granularity,  and  up  to  21%  over  the  evolutionary  technique  in  iBatis  (from  9%  to  30%)  at  the  file  granularity  were  reported.
1	Metric  based  software  reliability  prediction  approach  and  its  application.  This  paper  proposes  a  software  reliability  prediction  approach  based  on  software  metrics.  Metrics  measurement  results  are  connected  to  quantitative  reliability  predictions  through  defect  information  and  consideration  of  the  operational  environments.  An  application  of  the  proposed  approach  to  a  safety  critical  software  deployed  in  a  nuclear  power  plant  is  discussed.  Results  show  that  the  proposed  prediction  approach  could  be  applied  using  a  variety  of  software  metrics  at  different  stages  of  the  software  development  life  cycle  and  could  be  used  as  an  indicator  of  software  quality.  Therefore  the  approach  could  also  guide  the  development  process  and  help  make  design  decisions.  Experiences  and  lessons  learned  from  the  application  are  also  discussed.
1	Runtime  enforcement  of  timed  properties.  Runtime  enforcement  is  a  powerful  technique  to  ensure  that  a  running  system  respects  some  desired  properties.  Using  an  enforcement  monitor,  an  (untrustworthy)  input  execution  (in  the  form  of  a  sequence  of  events)  is  modified  into  an  output  sequence  that  complies  to  a  property.  Runtime  enforcement  has  been  extensively  studied  over  the  last  decade  in  the  context  of  untimed  properties.
1	Patriot  policy  assisted  resilient  programmable  iot  system.  This  paper  presents  PatrIoT,  which  efficiently  monitors  the  behavior  of  a  programmable  IoT  system  at  runtime  and  suppresses  contemplated  actions  that  violate  a  given  declarative  policy.  Policies  in  PatrIoT  are  specified  in  effectively  propositional,  past  metric  temporal  logic  and  capture  the  system’s  expected  temporal  invariants  whose  violation  can  break  its  desired  security,  privacy,  and  safety  guarantees.  PatrIoT  has  been  instantiated  for  not  only  an  industrial  IoT  system  (EVA  ICS)  but  also  for  two  home  representative  automation  platforms:  one  proprietary  (SmartThings)  and  another  open-source  (OpenHAB).  Our  empirical  evaluation  shows  that,  while  imposing  only  a  moderate  runtime  overhead,  PatrIoT  can  effectively  detect  policy  violations.
1	Aspectocl  using  aspects  to  ease  maintenance  of  evolving  constraint  specification.  Constraints  play  an  important  role  in  Model-Driven  Software  Engineering.  Industrial  systems  commonly  exhibit  cross-cutting  behaviors  in  design  artifacts.  Aspect-orientation  is  a  well-established  approach  to  deal  with  cross-cutting  behaviors  and  has  been  successfully  used  for  programming  and  design  languages.  In  model-driven  software  engineering,  the  presence  of  cross-cutting  constraints  makes  it  difficult  to  maintain  constraints  defined  on  the  models  of  large-scale  industrial  systems.  In  this  work,  we  improve  our  previous  work  on  AspectOCL,  which  is  an  extension  of  OCL  that  allows  modeling  of  cross-cutting  constraints.  We  provide  the  abstract  and  concrete  syntax  of  the  language.  We  add  support  for  new  constructs  such  as  composite  aspects  and  invariant  specification  on  a  package.  We  also  provide  tool  support  for  writing  cross-cutting  constraints  using  AspectOCL.  To  evaluate  AspectOCL,  we  apply  it  on  benchmark  case  studies  from  the  OCL  repository.  The  results  show  that  by  separating  the  cross-cutting  constraints,  the  number  of  constructs  in  the  constraint  specifications  can  be  reduced  to  a  large  amount.  AspectOCL  reduces  the  maintenance  effort  by  up  to  55%  in  one  case  study.  To  explore  the  impact  on  maintenance  time  and  accuracy,  we  also  perform  a  controlled  experiment  with  90  student  subjects.  The  results  show  that  AspectOCL  has  a  small  magnitude  of  improvement  in  terms  of  maintenance  time  when  compared  to  OCL,  whereas  modifications  to  OCL  specification  are  more  accurate.  The  post-experiment  survey  indicates  that  the  majority  of  subjects  favored  AspectOCL,  but  faced  challenges  in  applying  aspect-orientation  to  constraint  specification  due  to  a  lack  of  prior  exposure.
1	A  high  resolution  mesoscale  model  approach  to  reproduce  super  typhoon  maysak  2015  over  northwestern  pacific  ocean.  In  this  study,  an  attempt  is  made  to  simulate  super  typhoon  Maysak,  which  occurred  over  the  northwest  Pacific  Ocean  in  2015  and  made  landfall  on  the  Philippines  coast.  The  aim  of  the  present  study  is  to  assess  the  various  atmospheric  conditions  during  the  life  cycle  of  Maysak  to  explore  the  associated  dynamics  and  behavior  over  the  ocean.  For  this  purpose,  the  advanced  research  core  of  the  weather  research  and  forecasting  (WRF)  mesoscale  model  is  adopted.  The  model  is  simulated  using  27-km  horizontal  grid  resolution  with  National  Centers  for  Environmental  Prediction  global  Final  analyses  (FNL)  initial  conditions.  The  relevant  parameters,  namely  storm  track,  intensity,  wind–vorticity,  rainfall,  minimum  sea  level  pressure,  relative  humidity,  and  maximum  reflectivity  etc.,  were  analyzed.  The  model  is  able  to  perform  reasonably  well  when  available  observations  over  the  region  compared  with  the  simulated  values  of  these  parameters.  The  present  study  is  able  to  demonstrate  the  capability  of  WRF  in  simulating  and  predicting  the  relevant  characteristic  features  of  typhoons  over  the  northwest  Pacific  Ocean  region  through  the  case  of  Maysak.
1	Evaluating  code  complexity  triggers  use  of  complexity  measures  and  the  influence  of  code  complexity  on  maintenance  time.  Code  complexity  has  been  studied  intensively  over  the  past  decades  because  it  is  a  quintessential  characterizer  of  code’s  internal  quality.  Previously,  much  emphasis  has  been  put  on  creating  code  complexity  measures  and  applying  these  measures  in  practical  contexts.  To  date,  most  measures  are  created  based  on  theoretical  frameworks,  which  determine  the  expected  properties  that  a  code  complexity  measure  should  fulfil.  Fulfilling  the  necessary  properties,  however,  does  not  guarantee  that  the  measure  characterizes  the  code  complexity  that  is  experienced  by  software  engineers.  Subsequently,  code  complexity  measures  often  turn  out  to  provide  rather  superficial  insights  into  code  complexity.  This  paper  supports  the  discipline  of  code  complexity  measurement  by  providing  empirical  insights  into  the  code  characteristics  that  trigger  complexity,  the  use  of  code  complexity  measures  in  industry,  and  the  influence  of  code  complexity  on  maintenance  time.  Results  of  an  online  survey,  conducted  in  seven  companies  and  two  universities  with  a  total  of  100  respondents,  show  that  among  several  code  characteristics,  two  substantially  increase  code  complexity,  which  subsequently  have  a  major  influence  on  the  maintenance  time  of  code.  Notably,  existing  code  complexity  measures  are  poorly  used  in  industry.
1	An  empirical  catalog  of  code  smells  for  the  presentation  layer  of  android  apps.  Software  developers,  including  those  of  the  Android  mobile  platform,  constantly  seek  to  improve  their  applications’  maintainability  and  evolvability.  Code  smells  are  commonly  used  for  this  purpose,  as  they  indicate  symptoms  of  design  problems.  However,  although  the  literature  presents  a  variety  of  code  smells,  such  as  God  Class  and  Long  Method,  characteristics  that  are  specific  to  the  underlying  technologies  are  not  taken  into  account.  The  presentation  layer  of  an  Android  app,  for  example,  implements  specific  architectural  decisions  from  the  Android  platform  itself  (such  as  the  use  of  Activities,  Fragments,  and  Listeners)  as  well  as  deal  with  and  integrate  different  types  of  resources  (such  as  layouts  and  images).  Through  a  three-step  study  involving  246  Android  developers,  we  investigated  code  smells  that  developers  perceive  for  this  part  of  Android  apps.  We  devised  20  specific  code  smells  and  collected  the  developers’  perceptions  of  their  frequency  and  importance.  We  also  implemented  a  tool  that  identifies  the  proposed  code  smells  and  studied  their  prevalence  in  619  open-source  Android  apps.  Our  findings  suggest  that:  1)  developers  perceive  smells  specific  to  the  presentation  layer  of  Android  apps;  2)  developers  consider  these  smells  to  be  of  high  importance  and  frequency;  and  3)  the  proposed  smells  occur  in  real-world  Android  apps.  Our  domain-specific  smells  can  be  leveraged  by  developers,  researchers,  and  tool  developers  for  searching  potentially  problematic  pieces  of  code.
1	Why  are  many  businesses  instilling  a  devops  culture  into  their  organization.  DevOps  can  be  defined  as  a  cultural  movement  to  improve  and  accelerate  the  delivery  of  business  value  by  making  the  collaboration  between  development  and  operations  effective.  Although  this  movement  is  relatively  recent,  there  exist  an  intensive  research  around  DevOps.  However,  the  real  reasons  why  companies  move  to  DevOps  and  the  results  they  expect  to  obtain  have  been  paid  little  attention  in  real  contexts.  This  paper  aims  to  help  practitioners  and  researchers  to  better  understand  the  context  and  the  problems  that  many  companies  face  day  to  day  in  their  organizations  when  they  try  to  accelerate  software  delivery  and  the  main  drivers  that  move  these  companies  to  adopting  DevOps.  We  conducted  an  exploratory  study  by  leveraging  in  depth,  semi-structured  interviews  to  relevant  stakeholders  of  30  multinational  software-intensive  companies,  together  with  industrial  workshops  and  observations  at  organizations’  facilities  that  supported  triangulation.  Additionally,  we  conducted  an  inter-coder  agreement  analysis,  which  is  not  usually  addressed  in  qualitative  studies  in  software  engineering,  to  increase  reliability  and  reduce  authors  bias  of  the  drawn  findings.  The  research  explores  the  problems  and  expected  outcomes  that  moved  companies  to  adopt  DevOps  and  reveals  a  set  of  patterns  and  anti-patterns  about  the  reasons  why  companies  are  instilling  a  DevOps  culture.  This  study  aims  to  strengthen  evidence  and  support  practitioners  in  making  better  informed  about  which  problems  trigger  a  DevOps  transition  and  most  common  expected  results.
1	Gerrit  software  code  review  data  from  android.  Over  the  past  decade,  a  number  of  tools  and  systems  have  been  developed  to  manage  various  aspects  of  the  software  development  lifecycle.  Until  now,  tool  supported  code  review,  an  important  aspect  of  software  development,  has  been  largely  ignored.  With  the  advent  of  open  source  code  review  tools  such  as  Gerrit  along  with  projects  that  use  them,  code  review  data  is  now  available  for  collection,  analysis,  and  triangulation  with  other  software  development  data.  In  this  paper,  we  extract  Android  peer  review  data  from  Gerrit.  We  describe  the  Android  peer  review  process,  the  reverse  engineering  of  the  Gerrit  JSON  API,  our  data  mining  and  cleaning  methodology,  database  schema,  and  provide  an  example  of  how  the  data  can  be  used  to  answer  an  empirical  software  engineering  question.  The  database  is  available  for  use  by  the  research  community.
1	World  of  code  an  infrastructure  for  mining  the  universe  of  open  source  vcs  data.  Open  source  software  (OSS)  is  essential  for  modern  society  and,  while  substantial  research  has  been  done  on  individual  (typically  central)  projects,  only  a  limited  understanding  of  the  periphery  of  the  entire  OSS  ecosystem  exists.  For  example,  how  are  tens  of  millions  of  projects  in  the  periphery  interconnected  through  technical  dependencies,  code  sharing,  or  knowledge  flows?  To  answer  such  questions  we  a)  create  a  very  large  and  frequently  updated  collection  of  version  control  data  for  FLOSS  projects  named  World  of  Code  (WoC)  and  b)  provide  basic  tools  for  conducting  research  that  depends  on  measuring  interdependencies  among  all  FLOSS  projects.  Our  current  WoC  implementation  is  capable  of  being  updated  on  a  monthly  basis  and  contains  over  12B  git  objects.  To  evaluate  its  research  potential  and  to  create  vignettes  for  its  usage,  we  employ  WoC  in  conducting  several  research  tasks.  In  particular,  we  find  that  it  is  capable  of  supporting  trend  evaluation,  ecosystem  measurement,  and  the  determination  of  package  usage.  We  expect  WoC  to  spur  investigation  into  global  properties  of  OSS  development  leading  to  increased  resiliency  of  the  entire  OSS  ecosystem.  Our  infrastructure  facilitates  the  discovery  of  key  technical  dependencies,  code  flow,  and  social  networks  that  provide  the  basis  to  determine  the  structure  and  evolution  of  the  relationships  that  drive  FLOSS  activities  and  innovation.
1	Mining  stackoverflow  to  turn  the  ide  into  a  self  confident  programming  prompter.  Developers  often  require  knowledge  beyond  the  one  they  possess,  which  often  boils  down  to  consulting  sources  of  information  like  Application  Programming  Interfaces  (API)  documentation,  forums,  Q&A  websites,  etc.  Knowing  what  to  search  for  and  how  is  non-  trivial,  and  developers  spend  time  and  energy  to  formulate  their  problems  as  queries  and  to  peruse  and  process  the  results.  We  propose  a  novel  approach  that,  given  a  context  in  the  IDE,  automatically  retrieves  pertinent  discussions  from  Stack  Overflow,  evaluates  their  relevance,  and,  if  a  given  confidence  threshold  is  surpassed,  notifies  the  developer  about  the  available  help.  We  have  implemented  our  approach  in  Prompter,  an  Eclipse  plug-in.  Prompter  has  been  evaluated  through  two  studies.  The  first  was  aimed  at  evaluating  the  devised  ranking  model,  while  the  second  was  conducted  to  evaluate  the  usefulness  of  Prompter.
1	Exploratory  study  of  slack  q  a  chats  as  a  mining  source  for  software  engineering  tools.  Modern  software  development  communities  are  increasingly  social.  Popular  chat  platforms  such  as  Slack  host  public  chat  communities  that  focus  on  specific  development  topics  such  as  Python  or  Ruby-on-Rails.  Conversations  in  these  public  chats  often  follow  a  Q&A  format,  with  someone  seeking  information  and  others  providing  answers  in  chat  form.  In  this  paper,  we  describe  an  exploratory  study  into  the  potential  use-fulness  and  challenges  of  mining  developer  Q&A  conversations  for  supporting  software  maintenance  and  evolution  tools.  We  designed  the  study  to  investigate  the  availability  of  information  that  has  been  successfully  mined  from  other  developer  communications,  particularly  Stack  Overflow.  We  also  analyze  characteristics  of  chat  conversations  that  might  inhibit  accurate  automated  analysis.  Our  results  indicate  the  prevalence  of  useful  information,  including  API  mentions  and  code  snippets  with  descriptions,  and  several  hurdles  that  need  to  be  overcome  to  automate  mining  that  information.
1	Co  evolution  of  logical  couplings  and  commits  for  defect  estimation.  Logical  couplings  between  files  in  the  commit  history  of  a  software  repository  are  instances  of  files  being  changed  together.  The  evolution  of  couplings  over  commits'  history  has  been  used  for  the  localization  and  prediction  of  software  defects  in  software  reliability.  Couplings  have  been  represented  in  class  graphs  and  change  histories  on  the  class-level  have  been  used  to  identify  defective  modules.  Our  new  approach  inverts  this  perspective  and  constructs  graphs  of  ordered  commits  coupled  by  common  changed  classes.  These  graphs,  thus,  represent  the  co-evolution  of  commits,  structured  by  the  change  patterns  among  classes.  We  believe  that  co-evolutionary  graphs  are  a  promising  new  instrument  for  detecting  defective  software  structures.  As  a  first  result,  we  have  been  able  to  correlate  the  history  of  logical  couplings  to  the  history  of  defects  for  every  commit  in  the  graph  and  to  identify  sub-structures  of  bug-fixing  commits  over  sub-structures  of  normal  commits.
1	Imprecisions  diagnostic  in  source  code  deltas.  Beyond  a  practical  use  in  code  review,  source  code  change  detection  (SCCD)  is  an  important  component  of  many  mining  software  repositories  (MSR)  approaches.  As  such,  any  error  or  imprecision  in  the  detection  may  result  in  a  wrong  conclusion  while  mining  repositories.  We  identified,  analyzed,  and  characterized  impressions  in  GumTree,  which  is  the  most  advanced  algorithm  for  SCCD.  After  analyzing  its  detection  accuracy  over  a  curated  corpus  of  107  C#  projects,  we  diagnosed  several  imprecisions.  Many  of  our  findings  confirm  that  a  more  language-aware  perspective  of  GumTree  can  be  helpful  in  reporting  more  precise  changes.
1	Software  process  evaluation  a  machine  learning  framework  with  application  to  defect  management  process.  Software  process  evaluation  is  important  to  improve  software  development  and  the  quality  of  software  products  in  a  software  organization.  Conventional  approaches  based  on  manual  qualitative  evaluations  (e.g.,  artifacts  inspection)  are  deficient  in  the  sense  that  (i)  they  are  time-consuming,  (ii)  they  usually  suffer  from  the  authority  constraints,  and  (iii)  they  are  often  subjective.  To  overcome  these  limitations,  this  paper  presents  a  novel  semi-automated  approach  to  software  process  evaluation  using  machine  learning  techniques.  In  this  study,  we  mainly  focus  on  the  procedure  aspect  of  software  processes,  and  formulate  the  problem  as  a  sequence  (with  additional  information,  e.g.,  time,  roles,  etc.)  classification  task,  which  is  solved  by  applying  machine  learning  algorithms.  Based  on  the  framework,  we  define  a  new  quantitative  indicator  to  evaluate  the  execution  of  a  software  process  more  objectively.  To  validate  the  efficacy  of  our  approach,  we  apply  it  to  evaluate  the  execution  of  a  defect  management  (DM)  process  in  nine  real  industrial  software  projects.  Our  empirical  results  show  that  our  approach  is  effective  and  promising  in  providing  a  more  objective  and  quantitative  measurement  for  the  DM  process  evaluation  task.  Furthermore,  we  conduct  a  comprehensive  empirical  study  to  compare  our  proposed  machine  learning  approach  with  an  existing  conventional  approach  (i.e.,  artifacts  inspection).  Finally,  we  analyze  the  advantages  and  disadvantages  of  both  approaches  in  detail.
1	How  effective  are  mutation  testing  tools  an  empirical  analysis  of  java  mutation  testing  tools  with  manual  analysis  and  real  faults.  Mutation  analysis  is  a  well-studied,  fault-based  testing  technique.  It  requires  testers  to  design  tests  based  on  a  set  of  artificial  defects.  The  defects  help  in  performing  testing  activities  by  measuring  the  ratio  that  is  revealed  by  the  candidate  tests.  Unfortunately,  applying  mutation  to  real-world  programs  requires  automated  tools  due  to  the  vast  number  of  defects  involved.  In  such  a  case,  the  effectiveness  of  the  method  strongly  depends  on  the  peculiarities  of  the  employed  tools.  Thus,  when  using  automated  tools,  their  implementation  inadequacies  can  lead  to  inaccurate  results.  To  deal  with  this  issue,  we  cross-evaluate  four  mutation  testing  tools  for  Java,  namely  PIT,  muJava,  Major  and  the  research  version  of  PIT,  PITRV,  with  respect  to  their  fault-detection  capabilities.  We  investigate  the  strengths  of  the  tools  based  on:  a)  a  set  of  real  faults  and  b)  manual  analysis  of  the  mutants  they  introduce.  We  find  that  there  are  large  differences  between  the  tools'  effectiveness  and  demonstrate  that  no  tool  is  able  to  subsume  the  others.  We  also  provide  results  indicating  the  application  cost  of  the  method.  Overall,  we  find  that  PITRV  achieves  the  best  results.  In  particular,  PITRV  outperforms  the  other  tools  by  finding  6%  more  faults  than  the  other  tools  combined.
1	Catalog  of  energy  patterns  for  mobile  applications.  Software  engineers  make  use  of  design  patterns  for  reasons  that  range  from  performance  to  code  comprehensibility.  Several  design  patterns  capturing  the  body  of  knowledge  of  best  practices  have  been  proposed  in  the  past,  namely  creational,  structural  and  behavioral  patterns.  However,  with  the  advent  of  mobile  devices,  it  becomes  a  necessity  a  catalog  of  design  patterns  for  energy  efficiency.  In  this  work,  we  inspect  commits,  issues  and  pull  requests  of  1027  Android  and  756  iOS  apps  to  identify  common  practices  when  improving  energy  efficiency.  This  analysis  yielded  a  catalog,  available  online,  with  22  design  patterns  related  to  improving  the  energy  efficiency  of  mobile  apps.  We  argue  that  this  catalog  might  be  of  relevance  to  other  domains  such  as  Cyber-Physical  Systems  and  Internet  of  Things.  As  a  side  contribution,  an  analysis  of  the  differences  between  Android  and  iOS  devices  shows  that  the  Android  community  is  more  energy-aware.
1	Smbfl  slice  based  cost  reduction  of  mutation  based  fault  localization.  Fault  localization  is  one  of  the  most  important  and  difficult  tasks  in  the  software  debugging  process.  Therefore,  several  methods  have  been  proposed  to  automate  and  improve  this  process.  Mutation-based  fault  localization  is  one  of  the  states  of  the  art  techniques  that  try  to  locate  faults  by  executing  different  mutants  of  the  faulty  program.  In  addition  to  favorable  results,  it  is  along  with  a  massive  increase  in  mutation  execution  cost.  In  this  paper,  we  propose  a  new  mutation-based  fault  localization  approach  called  SMBFL,  that  aim  to  reduce  the  execution  cost  by  reducing  the  number  of  statements  to  be  mutated.  As  fewer  mutants  execute  with  SMBFL,  the  whole  process  will  become  faster  and  the  cost  will  decrease.  SMBFL  only  examines  the  statements  in  the  dynamic  slice  of  the  program  under  test.  The  statements  that  present  in  the  dynamic  slice  have  a  direct  effect  on  the  execution  of  the  program  with  the  specified  test  case.  In  the  SMBFL  method,  the  suspiciousness  score  of  program  statements  is  measured  based  on  the  entropy  of  their  mutants.  The  proposed  formula,  MuEn,  determines  the  suspiciousness  score  based  on  the  result  of  executing  mutants  of  each  statement  of  the  program.  SMBFL  is  evaluated  during  a  series  of  tests.  The  results  show  a  relative  increase  in  the  accuracy  of  fault  localization,  by  an  average  of  14.2%,  and  a  decrease  in  the  execution  time  of  the  fault  localization  process,  by  an  average  of  24.3%.  Finally,  the  MuEn  formula  applies  the  least  execution  overhead  to  the  fault  localization  process.
1	Meteorological  drought  study  through  spi  in  three  drought  prone  districts  of  west  bengal  india.  Deficiency  in  rainfall  introduces  drought  phenomena  with  temporal  and  spatial  variability  in  terms  of  intensity  and  magnitude.  Study  of  drought  in  different  scales  is  necessary  for  successful  planning  in  a  country  such  as  India,  where  agricultural  sector  contributes  highest  in  economy.  Drought  indices  (DI)  have  a  tool  to  quantify  the  drought  nature  and  express  a  single  digit  which  is  helpful  to  recognise  a  drought  character.  Standardized  Precipitation  Index  (SPI)  is  a  tool  to  quantify  the  drought  characteristics,  widely  used  for  its  simplicity  and  variable  approaches  to  dignify  a  drought.  Therefore,  the  present  study  deals  with  SPI  to  analyse  drought  phenomena  in  pre-monsoon,  monsoon,  post-monsoon  and  monthly  time  steps  in  three  relatively  drought  prone  districts  (Purulia,  Bankura,  Midnapore)  of  West  Bengal  in  India  of  rainfall  data  of  117 years  (1901–2017).  From  SPI  values,  drought  frequency  is  analysed  using  Gumbel’s  type  1  distribution  and  trend  is  calculated  using  Mann–Kendal  test  (M–K  test).  Occurrence  of  drought  with  negative  SPI  values  is  frequent  in  these  districts  with  increasing  dry  events  and  decreasing  wet  and  normal  event.  More  intensive  study  in  hydrological  and  agricultural  drought  is  necessary  to  implement  any  plan  with  this  increasing  aggravation  of  drought.
1	An  execution  semantic  and  content  and  context  based  code  clone  detection  and  analysis.  This  paper  presents  a  code-clone  detection  and  its  analysis  method,  based  on  an  execution-semantic  and  arbitrarygranularity  model[8]  of  code  fragments  The  principal  goal  of  introducing  the  proposed  detection  method  is  to  provide  a  codeclone  detection  method  suitable  for  programming  languages,  where  software  developers  can  define  their  own  “control  sentences”  with  such  as  lambda  or  lazy  evaluation.  Code  clones  detected  with  the  proposed  method  are  a  kind  of  type-3  clone,  where  code  fragments  exist  across  boundaries  of  procedures  or  modules.  The  model  also  seems  useful  as  clone  metrics  (for  a  clone  triage)  based  on  the  contents  and  contexts  of  code  fragments  in  a  clone  class  and  extensible  to  a  unified  method  of  code-clone  detection  and  code  search.  This  paper  introduces  an  executionsemantic  and  content-and-context  based  code  clone,  describes  its  definition,  a  detection  method,  an  analysis  method,  and  a  prototype  implementation  of  a  tool  chain,  which  was  applied  to  two  open-source  products  as  an  preliminary  empirical  evaluation.
1	Toward  an  execution  system  for  self  healing  workflows  in  cyber  physical  systems.  Cyber-physical  systems  (CPS)  represent  a  new  class  of  information  system  that  also  takes  real-world  data  and  effects  into  account.  Software-controlled  sensors,  actuators  and  smart  objects  enable  a  close  coupling  of  the  cyber  and  physical  worlds.  Introducing  processes  into  CPS  to  automate  repetitive  tasks  promises  advantages  regarding  resource  utilization  and  flexibility  of  control  systems  for  smart  spaces.  However,  process  execution  systems  face  new  challenges  when  being  adapted  for  process  execution  in  CPS:  the  automated  processing  of  sensor  events  and  data,  the  dynamic  invocation  of  services,  the  integration  of  human  interaction,  and  the  synchronization  of  the  cyber  and  physical  worlds.  Current  workflow  engines  fulfill  these  requirements  only  to  a  certain  degree.  In  this  work,  we  present  PROtEUS—an  integrated  system  for  process  execution  in  CPS.  PROtEUS  integrates  components  for  event  processing,  data  routing,  dynamic  service  selection  and  human  interaction  on  the  modeling  and  execution  level.  It  is  the  basis  for  executing  self-healing  model-based  workflows  in  CPS.  We  demonstrate  the  applicability  of  PROtEUS  within  two  case  studies  from  the  Smart  Home  domain  and  discuss  its  feasibility  for  introducing  workflows  into  cyber-physical  systems.
1	Analysing  the  cognitive  effectiveness  of  the  webml  visual  notation.  WebML  is  a  domain-specific  language  used  to  design  complex  data-intensive  Web  applications  at  a  conceptual  level.  As  WebML  was  devised  to  support  design  tasks,  the  need  to  define  a  visual  notation  for  the  language  was  identified  from  the  very  beginning.  Each  WebML  element  is  consequently  associated  with  a  separate  graphical  symbol  which  was  mainly  defined  with  the  idea  of  providing  simple  and  expressive  modelling  artefacts  rather  than  by  adopting  a  rigorous  scientific  approach.  As  a  result,  the  graphical  models  defined  with  WebML  may  sometimes  prevent  proper  communication  from  taking  place  between  the  various  stakeholders.  In  fact,  this  is  a  common  issue  for  most  of  the  existing  model-based  proposals  that  have  emerged  during  the  last  few  years  under  the  umbrella  of  model-driven  engineering.  In  order  to  illustrate  this  issue  and  foster  in  using  a  scientific  basis  to  design,  evaluate,  improve  and  compare  visual  notations,  this  paper  analyses  WebML  according  to  a  set  of  solid  principles,  based  on  the  theoretical  and  empirical  evidence  concerning  the  cognitive  effectiveness  of  visual  notations.  As  a  result,  we  have  identified  a  set  of  possible  improvements,  some  of  which  have  been  verified  by  an  empirical  study.  Furthermore,  a  number  of  findings,  experiences  and  lessons  learnt  on  the  assessment  of  visual  notations  are  presented.
1	Applying  static  code  analysis  for  domain  specific  languages.  The  use  of  code  quality  control  platforms  for  analysing  source  code  is  increasingly  gaining  attention  in  the  developer  community.  These  platforms  are  prepared  to  parse  and  check  source  code  written  in  a  variety  of  general-purpose  programming  languages.  The  emergence  of  domain-specific  languages  enables  professionals  from  different  areas  to  develop  and  describe  problem  solutions  in  their  disciplines.  Thus,  source  code  quality  analysis  methods  and  tools  can  also  be  applied  to  software  artefacts  developed  with  a  domain-specific  language.  To  evaluate  the  quality  of  domain-specific  language  code,  every  software  component  required  by  the  quality  platform  to  parse  and  query  the  source  code  must  be  developed.  This  becomes  a  time-consuming  and  error-prone  task,  for  which  this  paper  describes  a  model-driven  interoperability  strategy  that  bridges  the  gap  between  the  grammar  formats  of  source  code  quality  parsers  and  domain-specific  text  languages.  This  approach  has  been  tested  on  the  most  widespread  platforms  for  designing  text-based  languages  and  source  code  analysis.  This  interoperability  approach  has  been  evaluated  on  a  number  of  specific  contexts  in  different  domain  areas.
1	Transactional  execution  of  hierarchical  reconfigurations  in  cyber  physical  systems.  Cyber-physical  systems  reconfigure  the  structure  of  their  software  architecture,  e.g.,  to  avoid  hazardous  situations  and  to  optimize  operational  conditions  like  their  energy  consumption.  These  reconfigurations  have  to  be  safe  so  that  the  systems  protect  their  users  or  environment  against  harmful  conditions  or  events  while  changing  their  structure.  As  software  architectures  are  typically  built  on  components,  reconfiguration  actions  need  to  take  into  account  the  component  structure.  This  structure  should  support  vertical  composition  to  enable  hierarchically  encapsulated  components.  While  many  reconfiguration  approaches  for  cyber-physical  and  embedded  real-time  systems  allow  the  use  of  hierarchically  embedded  components,  i.e.,  vertical  composition,  none  of  them  offers  a  modeling  and  verification  solution  to  take  hierarchical  composition,  i.e.,  encapsulation,  into  account  thus  limiting  reuse  and  compositional  verification.  In  this  paper,  we  present  an  extension  to  our  existing  modeling  language,  MechatronicUML,  to  enable  safe  hierarchical  reconfigurations.  The  three  extensions  are  (a)  an  adapted  variant  of  the  2-phase-commit  protocol  to  initiate  reconfigurations  that  maintain  component  encapsulation,  (b)  the  integration  of  feedback  controllers  during  reconfiguration,  and  (c)  a  verification  approach  based  on  (timed)  model  checking  for  instances  of  our  model.  We  illustrate  our  approach  on  a  case  study  in  the  area  of  smart  railway  systems  by  showing  two  different  use  cases  of  our  approach.  We  show  that  using  our  approach  the  systems  can  be  easily  designed  to  reconfigure  safely.
1	A  method  for  testing  and  validating  executable  statechart  models.  Statecharts  constitute  an  executable  language  for  modelling  event-based  reactive  systems.  The  essential  complexity  of  statechart  models  solicits  the  need  for  advanced  model  testing  and  validation  techniques.  In  this  article,  we  propose  a  method  aimed  at  enhancing  statechart  design  with  a  range  of  techniques  that  have  proven  their  usefulness  to  increase  the  quality  and  reliability  of  source  code.  The  method  is  accompanied  by  a  process  that  flexibly  accommodates  testing  and  validation  techniques  such  as  test-driven  development,  behaviour-driven  development,  design  by  contract,  and  property  statecharts  that  check  for  violations  of  behavioural  properties  during  statechart  execution.  The  method  is  supported  by  the  Sismic  tool,  an  open-source  statechart  interpreter  library  in  Python,  which  supports  all  the  aforementioned  techniques.  Based  on  this  tooling,  we  carry  out  a  controlled  user  study  to  evaluate  the  feasibility,  usefulness  and  adequacy  of  the  proposed  techniques  for  statechart  testing  and  validation.
1	Design  science  in  action  developing  a  modeling  technique  for  eliciting  requirements  on  business  process  management  bpm  tools.  Selecting  a  suitable  business  process  management  (BPM)  tool  to  build  a  business  process  support  system  for  a  particular  business  process  is  difficult.  There  are  a  number  of  BPM  tools  on  the  market  that  are  available  as  systems  to  install  locally  and  as  services  in  the  cloud.  These  tools  are  based  on  different  BPM  paradigms  (e.g.,  workflow  or  case  management)  and  provide  different  capabilities  (e.g.,  enforcement  of  the  control  flow,  shared  spaces,  or  a  collaborative  environment).  This  makes  it  difficult  for  an  organization  to  select  a  tool  that  would  fit  the  business  processes  at  hand.  The  paper  suggests  a  solution  for  this  problem.  The  core  of  the  solution  is  a  modeling  technique  for  business  processes  for  eliciting  their  requirements  for  a  suitable  BPM  tool.  It  produces  a  high-level,  business  process  model,  called  a  "step-relationship"  model  that  depicts  the  essential  characteristics  of  a  process  in  a  paradigm-independent  way.  The  solution  presented  in  this  paper  has  been  developed  based  on  the  paradigm  of  design  science  research,  and  the  paper  discusses  the  research  project  from  the  design  science  perspective.  The  solution  has  been  applied  in  two  case  studies  in  order  to  demonstrate  its  feasibility.
1	Toward  a  methodology  for  case  modeling.  Case  management  is  increasingly  used  to  capture  and  enact  flexible,  knowledge-intensive  processes  in  organizations.  None  of  the  existing  case  management  approaches  provides  a  methodology  for  case  model  elicitation  and  modeling.  In  this  contribution,  three  modeling  methods  for  fragment-based  case  management  are  presented:  one  which  focuses  on  the  control-flow  view,  the  process-first  method,  one  which  has  a  data-centric  view,  the  object  lifecycle-first  method,  and  one  which  focuses  on  the  goals  of  a  case,  the  goals-first  method.  Following  the  design  science  process,  each  of  the  three  methods  was  evaluated  in  two  case  modeling  workshops  with  two  different  stakeholder  groups  (PhD  students  and  secretaries),  resulting  in  a  total  of  six  workshops.  All  participants  were  novices  in  case  management  and  most  of  them  as  well  in  process  modeling.  The  results  indicate  that  the  process-first  method  can  be  quickly  learned  by  novices  and  it  might  be  useful  for  scenarios  where  the  focus  is  on  the  main  process  with  some  degree  of  flexibility.  The  object  lifecycle-first  method  yields  more  flexible  and  consistent  case  models,  but  requires  a  higher  initial  modeling  effort,  as  the  lifecycle  of  the  main  case  object  has  to  be  designed  first.  The  goals-first  method  leads  to  a  detailed  and  consistent  case  model  and  additionally  provides,  by  means  of  the  defined  goals,  a  checklist  what  needs  to  be  done  for  a  case.  This  method  requires  in  addition  to  the  process  modeling  notation  another  model  type,  the  goal  hierarchy,  and  therefore  is  less  suited  for  novice  modelers,  as  found  by  the  workshop  results.
1	A  framework  for  fmi  based  co  simulation  of  human  machine  interfaces.  A  framework  for  co-simulation  of  human–machine  interfaces  in  Cyber-Physical  Systems  (CPS)  is  presented.  The  framework  builds  on  formal  (i.e.  mathematical)  methods.  It  aims  to  support  the  work  of  formal  methods  experts  in  charge  of  modelling  and  analysing  safety-critical  aspects  of  user  interfaces  in  CPS.  To  carry  out  these  modelling  and  analysis  activities,  formal  methods  experts  usually  need  to  engage  with  domain  experts  that  may  not  fully  understand  the  mathematical  details  of  formal  analysis  results.  The  framework  presented  in  this  work  mitigates  this  communication  barrier  by  allowing  formal  methods  experts  to  create  interactive  prototypes  driven  by  formal  models.  The  prototypes  closely  resemble  the  visual  appearance  of  the  system  being  developed.  They  can  be  used  to  discuss  details  of  the  formal  analysis  effort  without  showing  any  mathematical  detail.  An  existing  prototyping  toolkit  based  on  formal  methods  is  used  as  baseline  technology.  Novel  functionalities  are  developed  for  automatic  generation  of  interactive  prototypes  supporting  the  Functional  Mockup  Interface  (FMI),  a  de-facto  standard  technology  for  simulation  of  complex  systems.  Using  the  FMI  interface,  the  prototypes  can  be  integrated  with  simulations  of  other  system  components.  The  architecture  of  the  framework  is  presented,  along  with  a  verification  of  core  aspects  of  its  functionalities.  A  case  study  based  on  a  medical  system  is  used  to  demonstrate  the  capabilities  of  the  framework.
1	A  transformation  contract  to  generate  aspects  from  access  control  policies.  Access  control  is  an  important  security  issue.  It  has  been  addressed  since  the  late  1960s  in  the  early  time-sharing  computer  systems.  Many  access  control  models  have  been  proposed  since  than  but  of  particular  interest  is  Ferraiolo  and  Khun's  role-based  access  control  model  (RBAC).  It  is  a  simple  and  yet  general  model  which  has  been  deeply  studied  and  applied  both  in  industry  and  in  academia.  A  variety  of  industrial  standards  have  been  proposed  based  on  this  model.  Generating  code  for  an  access  control  policy  is  an  interesting  challenge.  Understanding  access  control  as  a  non-functional  concern  that  cross-cuts  the  functional  part  of  a  system  raises  difficulties  quite  suitable  for  a  solution  based  on  aspect-oriented  programming.  In  this  paper,  we  address  the  problems  of  specification  and  validation  of  code  generation  for  access  control  policies  targeting  an  aspect-based  infra-structure.  We  propose  an  MDA  approach.  The  code  generator  is  a  transformation  from  SecureUML,  an  RBAC-based  modeling  language,  to  the  language  Aspects  for  Access  Control  (AAC),  an  aspect-oriented  modeling  language  proposed  in  this  paper.  Metamodels  are  used  to  represent  the  languages  and  to  specify  the  transformation.  A  metamodel  is  used  to  represent  the  abstract  syntax  of  a  language  and  the  constraints  that  a  given  instance  model  of  the  metamodel  must  fulfill.  We  also  use  a  metamodel  to  specify  the  code  generator.  This  transformation  metamodel,  together  with  all  the  constraints,  that  is,  from  both  languages  and  those  constraints  regarding  the  merge  of  the  two  languages,  we  call  a  transformation  contract.  It  merges  and  conservatively  extends  the  source  and  target  metamodels  of  the  model  transformation  it  represents.  In  the  context  of  code-generation  for  access  control  policies,  the  transformation  contract  specifies  the  relationships  between  the  abstract  syntaxes  of  SecureUML  and  AAC  and  constrains  the  two  languages.  The  validation  of  the  code  generator  also  uses  the  transformation  contract.  For  a  given  access  control  policy  and  aspect,  represented  as  instances  of  the  appropriate  metamodels,  with  aspects  produced  by  the  code  generator,  the  constraints  of  the  transformation  contract  must  hold.  We  have  prototyped  a  transformer  from  SecureUML  to  aspects  on  top  of  ITP/OCL,  an  OCL  interpreter  that  automatically  validates  the  generated  aspect  code  by  applying  the  constraints  of  the  transformation  contract.
1	Querying  process  models  by  behavior  inclusion.  Business  processes  are  vital  to  managing  organizations  as  they  sustain  a  company's  competitiveness.  Consequently,  these  organizations  maintain  collections  of  hundreds  or  thousands  of  process  models  for  streamlining  working  procedures  and  facilitating  process  implementation.  Yet,  the  management  of  large  process  model  collections  requires  effective  searching  capabilities.  Recent  research  focused  on  similarity  search  of  process  models,  but  querying  process  models  is  still  a  largely  open  topic.  This  article  presents  an  approach  to  querying  process  models  that  takes  a  process  example  as  input  and  discovers  all  models  that  allow  replaying  the  behavior  of  the  query.  To  this  end,  we  provide  a  notion  of  behavioral  inclusion  that  is  based  on  trace  semantics  and  abstraction.  Additional  to  deciding  a  match,  a  closeness  score  is  provided  that  describes  how  well  the  behavior  of  the  query  is  represented  in  the  model  and  can  be  used  for  ranking.  The  article  introduces  the  formal  foundations  of  the  approach  and  shows  how  they  are  applied  to  querying  large  process  model  collections.  An  experimental  evaluation  has  been  conducted  that  confirms  the  suitability  of  the  solution  as  well  as  its  applicability  and  scalability  in  practice.
1	Distributed  implementation  of  message  sequence  charts.  This  work  revisits  the  problem  of  program  synthesis  from  specifications  described  by  high-level  message  sequence  charts.  We  first  show  that  in  the  general  case,  synthesis  by  a  simple  projection  on  each  component  of  the  system  allows  more  behaviors  in  the  implementation  than  in  the  specification.  We  then  show  that  differences  arise  from  loss  of  ordering  among  messages  and  show  that  behaviors  can  be  preserved  by  addition  of  communication  controllers  that  intercept  messages  to  add  stamping  information  before  resending  them  and  deliver  messages  to  processes  in  the  order  described  by  the  specification.
1	Systematic  review  and  aggregation  of  empirical  studies  on  elicitation  techniques.  We  have  located  the  results  of  empirical  studies  on  elicitation  techniques  and  aggregated  these  results  to  gather  empirically  grounded  evidence.  Our  chosen  surveying  methodology  was  systematic  review,  whereas  we  used  an  adaptation  of  comparative  analysis  for  aggregation  because  meta-analysis  techniques  could  not  be  applied.  The  review  identified  564  publications  from  the  SCOPUS,  IEEEXPLORE,  and  ACM  DL  databases,  as  well  as  Google.  We  selected  and  extracted  data  from  26  of  those  publications.  The  selected  publications  contain  30  empirical  studies.  These  studies  were  designed  to  test  43  elicitation  techniques  and  50  different  response  variables.  We  got  100  separate  results  from  the  experiments.  The  aggregation  generated  17  pieces  of  knowledge  about  the  interviewing,  laddering,  sorting,  and  protocol  analysis  elicitation  techniques.  We  provide  a  set  of  guidelines  based  on  the  gathered  pieces  of  knowledge.
1	On  the  effectiveness  of  contracts  as  test  oracles  in  the  detection  and  diagnosis  of  functional  faults  in  concurrent  object  oriented  software.  Design  by  contract  (DbC)  is  a  software  development  methodology  that  focuses  on  clearly  defining  the  interfaces  between  components  to  produce  better  quality  object-oriented  software.  Though  there  exists  ample  support  for  DbC  for  sequential  programs,  applying  DbC  to  concurrent  programs  presents  several  challenges.  Using  Java  as  the  target  programming  language,  we  tackle  such  challenges  by  augmenting  the  Java  Modelling  Language  (JML)  and  modifying  the  JML  compiler  (jmlc)  to  generate  runtime  assertion  checking  code  to  support  DbC  in  concurrent  programs.  We  applied  our  solution  in  a  carefully  designed  case  study  on  a  highly  concurrent  industrial  software  system  from  the  telecommunications  domain  to  assess  the  effectiveness  of  contracts  as  test  oracles  in  detecting  and  diagnosing  functional  faults  in  concurrent  software.  Based  on  these  results,  clear  and  objective  requirements  are  defined  for  contracts  to  be  effective  test  oracles  for  concurrent  programs  whilst  balancing  the  effort  to  design  them.  Effort  is  measured  indirectly  through  the  contract  complexity  measure  (CCM),  a  measure  we  define.  Main  results  include  that  contracts  of  a  realistic  level  of  completeness  and  complexity  can  detect  around  76  percent  of  faults  and  reduce  the  diagnosis  effort  for  such  faults  tenfold.  We,  therefore,  show  that  DbC  can  be  applied  to  concurrent  software  and  can  be  a  valuable  tool  to  improve  the  economics  of  software  engineering.
1	Active  learning  and  effort  estimation  finding  the  essential  content  of  software  effort  estimation  data.  Background:  Do  we  always  need  complex  methods  for  software  effort  estimation  (SEE)?  Aim:  To  characterize  the  essential  content  of  SEE  data,  i.e.,  the  least  number  of  features  and  instances  required  to  capture  the  information  within  SEE  data.  If  the  essential  content  is  very  small,  then  1)  the  contained  information  must  be  very  brief  and  2)  the  value  added  of  complex  learning  schemes  must  be  minimal.  Method:  Our  QUICK  method  computes  the  euclidean  distance  between  rows  (instances)  and  columns  (features)  of  SEE  data,  then  prunes  synonyms  (similar  features)  and  outliers  (distant  instances),  then  assesses  the  reduced  data  by  comparing  predictions  from  1)  a  simple  learner  using  the  reduced  data  and  2)  a  state-of-the-art  learner  (CART)  using  all  data.  Performance  is  measured  using  hold-out  experiments  and  expressed  in  terms  of  mean  and  median  MRE,  MAR,  PRED(25),  MBRE,  MIBRE,  or  MMER.  Results:  For  18  datasets,  QUICK  pruned  69  to  96  percent  of  the  training  data  (median  =  89  percent).  K  =  1  nearest  neighbor  predictions  (in  the  reduced  data)  performed  as  well  as  CART's  predictions  (using  all  data).  Conclusion:  The  essential  content  of  some  SEE  datasets  is  very  small.  Complex  estimation  methods  may  be  overelaborate  for  such  datasets  and  can  be  simplified.  We  offer  QUICK  as  an  example  of  such  a  simpler  SEE  method.
1	Keyboard  control  method  for  virtual  reality  micro  robotic  cell  injection  training.  The  rapid  development  of  virtual  reality  offers  significant  potential  for  skills  training  applications.  Our  ongoing  work  proposes  virtual  reality  operator  training  for  the  micro-robotic  cell  injection  procedure.  The  interface  between  the  operator  and  the  system  can  be  achieved  in  many  different  ways.  The  computer  keyboard  is  ubiquitous  in  its  use  for  everyday  computing  applications  and  also  commonly  utilized  in  virtual  reality  systems.  Based  on  the  premise  that  most  people  have  experience  in  using  a  computer  keyboard,  as  opposed  to  more  sophisticated  input  devices,  this  paper  considers  the  feasibility  of  using  a  keyboard  to  control  the  micro-robot  for  cell  injection.  In  this  study,  thirteen  participants  underwent  the  experimental  evaluation.  The  participants  were  asked  to  perform  three  simulated  trial  sessions  in  a  virtual  micro-robotic  cell  injection  environment.  Each  session  consisted  of  ten  cell  injection  trials  and  relevant  data  for  each  trial  were  recorded  and  analyzed.  Results  showed  participants'  performance  improvement  after  the  three  sessions.  It  was  also  observed  that  participants  intuitively  controlled  multiple  axes  of  the  micro-robot  simultaneously  despite  the  absence  of  instruction  on  how  to  do  so.  This  continued  throughout  the  experiments  and  suggests  skills  transfer  from  other  keyboard  based  interactions.  Based  on  the  results  provided,  it  is  suggested  that  keyboard  control  is  a  feasible,  simple  and  low-cost  control  method  for  the  virtual  micro-robot.
1	A  systematic  literature  review  of  crowdsourcing  based  research  in  information  security.  Crowdsourcing  is  a  well-established  concept  in  several  application  areas  of  computer  science  and  information  systems.  While  crowdsourcing  is  favored  in  areas  such  as  information  sharing,  quality  management  or  data  acquisition,  only  little  attention  has  been  drawn  to  crowdsourcing  capabilities  for  information  security  in  the  past.  Since  a  few  years  an  increase  of  crowdsourcing-based  research  in  information  security  can  be  identified.  To  which  extend  remains  unclear  since  a  comprehensive  overview  of  applied  crowdsourcing  techniques  and  related  challenges  is  missing.  In  this  paper  we  try  to  shed  some  light  on  this  by  conducting  a  systematic  literature  review  based  on  the  snowballing  methodology.  It  delivered  23  relevant  papers  which  we  analyzed  with  respect  to  the  following  perspectives:  (a)  Bibliographic  information,  (b)  applied  research  methodology,  (c)  addressed  information  security  application  context,  (d)  applied  crowdsourcing  approach,  and  (e)  challenges  for  crowdsourcingbased  research  in  information  security.  Finally,  based  on  the  described  investigations,  we  give  a  comprehensive  overview,  and  identify  several  challenges  of  crowdsourcing  based  research  in  information  security.
1	Coping  with  uncertainty  in  systems  of  systems  architecture  modeling  on  the  iot  with  sosadl.  A  challenging  issue  in  the  architectural  design  of  a  System-of-Systems  (SoS)  is  how  to  cope  with  the  uncertainty  raised  by  the  limited  knowledge  of  the  operational  environment  where  the  SoS  will  actually  be  deployed  as  well  as  the  constituent  systems  which  will  concretely  participate  in  the  SoS  at  run-time.  It  is  especially  the  case  of  SoSs  being  architected  on  the  Internet-of-Things  (IoT).  Indeed,  due  to  the  open  and  dynamic  nature  of  the  IoT,  on  the  one  hand,  at  design-time,  most  often  the  SoS  architects  do  not  know  which  will  be  the  concrete  IoT  systems  that  will  become  constituents  of  an  SoS,  these  being  predominantly  identified  at  run-time;  on  the  other  hand,  the  correct  architecture  depends  not  only  on  the  constituent  IoT  systems  but  also,  largely,  on  the  operational  environment  where  the  SoS  will  be  positioned  on  the  IoT.  The  consequent  research  question  is  thereby  how  to  design  and  describe  the  SoS  architecture  in  a  way  that  is  flexible  enough  to  cope  with  these  different  uncertainties.  To  address  this  challenge,  this  paper  investigates  the  notion  of  uncertainty  in  SoS  and  presents  how  SosADL,  a  formal  SoS  Architecture  Description  Language  (ADL),  enables  to  cope  with  uncertainty  in  the  architecture  modeling  of  SoSs.  It  presents  the  concepts  and  constructs  that  makes  possible  to  describe  SoS  architectures  which  will  operate  in  unpredictable  environments  on  the  IoT  based  on  the  SosADL  support  for  dealing  with  partial  knowledge,  grounded  on  concurrent  constraints.
1	Invariant  based  automatic  testing  of  modern  web  applications.  Ajax-based  Web  2.0  applications  rely  on  stateful  asynchronous  client/server  communication,  and  client-side  runtime  manipulation  of  the  DOM  tree.  This  not  only  makes  them  fundamentally  different  from  traditional  web  applications,  but  also  more  error-prone  and  harder  to  test.  We  propose  a  method  for  testing  Ajax  applications  automatically,  based  on  a  crawler  to  infer  a  state-flow  graph  for  all  (client-side)  user  interface  states.  We  identify  Ajax-specific  faults  that  can  occur  in  such  states  (related  to,  e.g.,  DOM  validity,  error  messages,  discoverability,  back-button  compatibility)  as  well  as  DOM-tree  invariants  that  can  serve  as  oracles  to  detect  such  faults.  Our  approach,  called  Atusa,  is  implemented  in  a  tool  offering  generic  invariant  checking  components,  a  plugin-mechanism  to  add  application-specific  state  validators,  and  generation  of  a  test  suite  covering  the  paths  obtained  during  crawling.  We  describe  three  case  studies,  consisting  of  six  subjects,  evaluating  the  type  of  invariants  that  can  be  obtained  for  Ajax  applications  as  well  as  the  fault  revealing  capabilities,  scalability,  required  manual  effort,  and  level  of  automation  of  our  testing  approach.
1	Scaling  service  oriented  applications  into  geo  distributed  clouds.  With  the  significant  prevalence  of  cloud  computing,  more  and  more  data  centers  are  built  to  host  and  deliver  various  online  services.  However,  a  key  challenge  faced  by  service  providers  is  how  to  scale  their  applications  into  geo-distributed  data  centers  to  improve  application  performance  as  well  as  minimizing  the  operational  cost.  While  most  existing  deployment  methods  ignore  the  service  dependencies  in  an  application,  this  paper  proposes  a  general  dynamic  service  deployment  framework  to  bridge  this  gap,  in  which  a  deployment  manager  and  a  local  scheduler  are  designed  to  optimize  data  center  selection  and  auto-scale  the  service  instances  in  each  data  center  respectively.  More  specifically,  we  formulate  the  deployment  problem  across  multiple  data  centers  as  a  compact  minimization  model,  which  can  be  solved  efficiently  by  a  genetic  algorithm.  To  evaluate  the  performance  of  our  approach,  extensive  experiments  are  conducted  based  on  a  large-scale  real-world  latency  dataset.  The  experimental  results  show  that  our  approach  substantially  outperforms  the  other  existing  methods.
1	Servicepot  an  extensible  registry  for  choreography  governance.  The  Future  Internet  (FI)  vision  fosters  the  establishment  of  highly  dynamic  and  continuously  evolving  systems  in  which  different  organizations,  via  provided  e-services,  dynamically  cooperate  at  run-time,  and  possibly  just  for  a  single  application  level  transaction.  Service  choreographies  contribute  to  establish  the  FI  vision,  by  providing  support  for  the  description  of  complex  and  inter-organizational  service-based  applications.  Specifically,  the  choreography  paradigm  facilitates  the  dynamic  integration  and  interoperability  of  services  managed  and  made  available  by  different  organizations.  Nevertheless  the  real  take  off  of  choreography  based  solutions  asks  for  the  definition  and  development  of  suitable  supporting  frameworks  (i.e.  platforms  and  tools)  permitting  to  govern  the  whole  life-cycle  of  a  service  choreography.  In  this  paper,  we  have  introduced  the  main  challenges  and  requirements  for  a  software  infrastructure  supporting  choreography  adoption,  and  our  response  to  these  challenges:  Service  Pot.  Service  Pot  is  an  extensible  registry  for  choreography  basedsolutions  offering  choreography  lifecycle  management  and  governance  features.  The  registry  implements  all  the  fundamental  functionalities  for  choreography  support  and  it  has  a  plug-in  based  extensible  architecture  permitting  the  easy  introduction  of  additional  choreography  related  manipulation  activities.  A  reference  implementation  of  the  registry  is  also  introduced  and  discussed,  taking  into  account  choreographies  specifications  defined  using  the  BPMN  2.0  standard  notation.
1	Reconciling  service  orientation  with  the  cloud.  The  SaaS  market  is  growing  steadily.  In  the  face  of  it,  SaaS  providers  have  started  to  reflect  on  how  to  rationalize  their  internal  processes:  service  design,  management  and  delivery.  The  Service  Oriented  Architecture  (SOA)  principles  can  offer  valuable  insight  in  that  process  as  the  SOA  concept  has  several  key  traits  in  common  with  the  cloud  computing  paradigm.  The  latter  however  gives  prominence  to  the  economics  of  the  SaaS  application  delivery,  which  values  elasticity  in  the  sharing  of  resources  and  in  the  preservation  of  service-level  agreements,  characteristics  which  the  former  prospective  ignored.  This  work  continues  on  the  trail  of  previous  research  on  service-oriented  cloud  platforms,  by  presenting  a  new  enriched  SOA  paradigm  that  embraces  rapid  elasticity,  in  conformance  with  the  postulates  of  cloud  computing.  The  integration  of  our  SOA-based  PaaS  prototype,  PaaSSOA,  with  a  real  IaaS  instance,  Amazon  Web  Services,  provides  an  initial  confirmation  of  the  viability  of  the  extended  notion  of  service  model,  as  well  as  empirical  evidence  that  a  SOA-based  PaaS  can  effectively  operate  over  a  real  IaaS  instance.
1	Comments  on  researcher  bias  the  use  of  machine  learning  in  software  defect  prediction.  Shepperd et al. find  that  the  reported  performance  of  a  defect  prediction  model  shares  a  strong  relationship  with  the  group  of  researchers  who  construct  the  models.  In  this  paper,  we  perform  an  alternative  investigation  of  Shepperd et al.'s  data.  We  observe  that  (a)  research  group  shares  a  strong  association  with  other  explanatory  variables  (i.e.,  the  dataset  and  metric  families  that  are  used  to  build  a  model);  (b)  the  strong  association  among  these  explanatory  variables  makes  it  difficult  to  discern  the  impact  of  the  research  group  on  model  performance;  and  (c)  after  mitigating  the  impact  of  this  strong  association,  we  find  that  the  research  group  has  a  smaller  impact  than  the  metric  family.  These  observations  lead  us  to  conclude  that  the  relationship  between  the  research  group  and  the  performance  of  a  defect  prediction  model  are  more  likely  due  to  the  tendency  of  researchers  to  reuse  experimental  components  (e.g.,  datasets  and  metrics).  We  recommend  that  researchers  experiment  with  a  broader  selection  of  datasets  and  metrics  to  combat  any  potential  bias  in  their  results.
1	A  controlled  experiment  to  evaluate  the  effects  of  mindfulness  in  software  engineering.  Context.  Many  reports  support  the  fact  that  some  psycho--social  aspects  of  software  engineers  are  key  factors  for  the  quality  of  the  software  development  process  and  its  resulting  products.  Based  on  the  experience  of  some  of  the  authors  after  more  than  a  year  of  practising  mindfulness---a  meditation  technique  aimed  to  increase  clearness  of  mind  and  awareness---we  guessed  that  it  could  be  interesting  to  empirically  evaluate  whether  mindfulness  affects  positively  not  only  the  behaviour  but  also  the  professional  performance  of  software  engineers.      Goal.  In  this  paper,  we  present  a  quasi--experiment  carried  out  at  the  University  of  Seville  to  evaluate  whether  Software  Engineering  &  Information  Systems  students  enhance  their  conceptual  modelling  skills  after  the  continued  daily  practice  of  mindfulness  during  four  weeks.      Method.  Students  were  divided  into  two  groups:  one  group  practised  mindfulness,  and  the  other---the  control  group---were  trained  in  public  speaking.  In  order  to  study  the  possible  cause--and--effect  relationship,  effectiveness  (the  rate  of  model  elements  correctly  identified)  and  efficiency  (the  number  of  model  elements  correctly  identified  per  unit  of  time)  of  the  students  developing  conceptual  modelling  exercises  were  measured  before  and  after  taking  the  mindfulness  and  public  speaking  sessions.      Results.  The  experiment  results  have  revealed  that  the  students  who  practised  mindfulness  have  become  more  efficient  in  developing  conceptual  models  than  those  who  attended  the  public  speaking  sessions.  With  respect  to  effectiveness,  some  enhancement  have  been  observed,  although  not  as  significant  as  in  the  case  of  efficiency.      Conclusions.  This  rising  trend  in  effectiveness  suggests  that  the  number  of  sessions  could  have  been  insufficient  and  that  a  longer  period  of  sessions  could  have  also  enhanced  effectiveness  significantly.
1	Whence  to  learn  transferring  knowledge  in  configurable  systems  using  beetle.  As  software  systems  grow  in  complexity  and  the  space  of  possible  configurations  increases  exponentially,  finding  the  near-optimal  configuration  of  a  software  system  becomes  challenging.  Recent  approaches  address  this  challenge  by  learning  performance  models  based  on  a  sample  set  of  configurations.  However,  collecting  enough  sample  configurations  can  be  very  expensive  since  each  such  sample  requires  configuring,  compiling,  and  executing  the  entire  system  using  a  complex  test  suite.  When  learning  on  new  data  is  too  expensive,  it  is  possible  to  use  Transfer  Learning  to  “transfer”  old  lessons  to  the  new  context.  Traditional  transfer  learning  has  a  number  of  challenges,  specifically,  (a)  learning  from  excessive  data  takes  excessive  time,  and  (b)  the  performance  of  the  models  built  via  transfer  can  deteriorate  as  a  result  of  learning  from  a  poor  source.  To  resolve  these  problems,  we  propose  a  novel  transfer  learning  framework  called  BEETLE,  which  is  a  “bellwether”-based  transfer  learner  that  focuses  on  identifying  and  learning  from  the  most  relevant  source  from  amongst  the  old  data.  This  paper  evaluates  BEETLE  with  57  different  software  configuration  problems  based  on  five  software  systems  (a  video  encoder,  an  SAT  solver,  a  SQL  database,  a  high-performance  C-compiler,  and  a  streaming  data  analytics  tool).  In  each  of  these  cases,  BEETLE  found  configurations  that  are  as  good  as  or  better  than  those  found  by  other  state-of-the-art  transfer  learners  while  requiring  only  a  fraction  1/7th  of  the  measurements  needed  by  those  other  methods.  Based  on  these  results,  we  say  that  BEETLE  is  a  new  high-water  mark  in  optimally  configuring  software.
1	Keyword  search  for  building  service  based  systems.  With  the  fast  growth  of  applications  of  service-oriented  architecture  (SOA)  in  software  engineering,  there  has  been  a  rapid  increase  in  demand  for  building  service-based  systems  (SBSs)  by  composing  existing  Web  services.  Finding  appropriate  component  services  to  compose  is  a  key  step  in  the  SBS  engineering  process.  Existing  approaches  require  that  system  engineers  have  detailed  knowledge  of  SOA  techniques  which  is  often  too  demanding.  To  address  this  issue,  we  propose    K  eyword    S  earch  for    S  ervice-based    S  ystems  (KS3),  a  novel  approach  that  integrates  and  automates  the  system  planning,  service  discovery  and  service  selection  operations  for  building  SBSs  based  on  keyword  search.  KS3  assists  system  engineers  without  detailed  knowledge  of  SOA  techniques  in  searching  for  component  services  to  build  SBSs  by  typing  a  few  keywords  that  represent  the  tasks  of  the  SBSs  with  quality  constraints  and  optimisation  goals  for  system  quality,  e.g.,  reliability,  throughput  and  cost.  KS3  offers  a  new  paradigm  for  SBS  engineering  that  can  significantly  save  the  time  and  effort  during  the  system  engineering  process.  We  conducted  large-scale  experiments  using  two  real-world  Web  service  datasets  to  demonstrate  the  practicality,  effectiveness  and  efficiency  of  KS3.
1	On  the  adoption  usage  and  evolution  of  kotlin  features  in  android  development.  Background:  Google  announced  Kotlin  as  an  Android  official  programming  language  in  2017,  giving  developers  an  option  of  writing  applications  using  a  language  that  combines  object-oriented  and  functional  features.  Aims:  The  goal  of  this  work  is  to  understand  the  usage  of  Kotlin  features  considering  four  aspects:  i)  which  features  are  adopted,  ii)  what  is  the  degree  of  adoption,  iii)  when  are  these  features  added  into  Android  applications  for  the  first  time,  and  iv)  how  the  usage  of  features  evolves  along  with  applications'  evolution.  Method:  Exploring  the  source  code  of  387  Android  applications,  we  identify  the  usage  of  Kotlin  features  on  each  version  application's  version  and  compute  the  moment  that  each  feature  is  used  for  the  first  time.  Finally,  we  identify  the  evolution  trend  that  better  describes  the  usage  of  these  features.  Results:  15  out  of  26  features  are  used  on  at  least  50%  of  applications.  Moreover,  we  found  that  type  inference,  lambda  and  safe  call  are  the  most  used  features.  Also,  we  observed  that  the  most  used  Kotlin  features  are  those  first  included  on  Android  applications.  Finally,  we  report  that  the  majority  of  applications  tend  to  add  more  instances  of  24  out  of  26  features  along  with  their  evolution.  Conclusions:  Our  study  generates  7  main  findings.  We  present  their  implications,  which  are  addressed  to  developers,  researchers  and  tool  builders  in  order  to  foster  the  use  of  Kotlin  features  to  develop  Android  applications.
1	The  innovative  behaviour  of  software  engineers  findings  from  a  pilot  case  study.  Context:  In  the  workplace,  some  individuals  engage  in  the  voluntary  and  intentional  generation,  promotion,  and  realization  of  new  ideas  for  the  benefit  of  individual  performance,  group  effectiveness,  or  the  organization.  The  literature  classifies  this  phenomenon  as  innovative  behaviour.  Despite  its  importance  to  the  development  of  innovation,  innovative  behaviour  has  not  been  fully  investigated  in  software  engineering.  Objective:  To  understand  the  factors  that  support  or  inhibit  innovative  behaviour  in  software  engineering  practice.  Method:  We  conducted  a  pilot  case  study  in  a  Canadian  software  company  using  interviews  and  observations  as  data  collection  techniques.  Using  qualitative  analysis,  we  identified  relevant  factors  and  relationships  not  addressed  by  studies  from  other  areas.  Results:  Individual  innovative  behaviour  is  influenced  by  individual  attitudes  and  also  by  situational  factors  such  as  relationships  in  the  workplace,  organizational  characteristics,  and  project  type.  We  built  a  model  to  express  the  interacting  effects  of  these  factors.  Conclusions:  Innovative  behaviour  is  dependent  on  individual  and  contextual  factors.  Our  results  contribute  to  relevant  impacts  on  research  and  practice,  and  to  topics  that  deserve  further  study.
1	Revisiting  and  improving  szz  implementations.  Background:  The  SZZ  algorithm  was  proposed  to  identify  bug-introducing  changes,  i.e.,  changes  that  are  likely  to  induce  bugs.  Previous  studies  improved  its  implementation  and  evaluated  its  results.Aims:  To  address  existing  limitations  of  SZZ  to  improve  the  maturity  of  the  algorithm.  We  also  aim  to  verify  if  the  improvements  that  have  been  proposed  to  the  SZZ  algorithm  also  hold  in  different  datasets.Method:  We  re-evaluate  two  recent  SZZ  implementations  using  an  adaptation  of  the  Defects4J  dataset,  which  works  as  a  preprocessed  dataset  that  can  be  used  by  SZZ.  Furthermore,  we  revisit  the  limitations  of  RA-SZZ  (refactoring  aware  SZZ)  to  improve  the  precision  and  recall  of  the  algorithm.Results:  We  observe  that  a  median  of  44%  of  the  lines  that  are  flagged  by  the  improved  SZZ  are  very  likely  to  introduce  a  bug.  We  manually  analyze  the  SZZ-generated  data  and  observe  that  there  exist  refactoring  operations  (31.17%)  and  equivalent  changes  (13.64%)  that  are  still  misidentified  by  the  improved  SZZ.Conclusion:  By  preprocessing  the  dataset  that  is  used  as  input  by  SZZ,  the  accuracy  of  SZZ  may  be  considerably  improved.  For  example,  we  observe  that  SZZ  implementations  are  approximately  40%  more  accurate  if  only  valid  bug-fix  lines  are  used  as  the  input  for  SZZ.
1	Devops  in  an  iso  13485  regulated  environment  a  multivocal  literature  review.  Background:  Medical  device  development  projects  must  follow  proper  directives  and  regulations  to  be  able  to  market  and  sell  the  end-product  in  their  respective  territories.  The  regulations  describe  requirements  that  seem  to  be  opposite  to  efficient  software  development  and  short  time-to-market.  As  agile  approaches,  like  DevOps,  are  becoming  more  and  more  popular  in  software  industry,  a  discrepancy  between  these  modern  methods  and  traditional  regulated  development  has  been  reported.  Although  examples  of  successful  adoption  in  this  context  exist,  the  research  is  sparse.  Aims:  The  objective  of  this  study  is  twofold:  to  review  the  current  state  of  DevOps  adoption  in  regulated  medical  device  environment;  and  to  propose  a  checklist  based  on  that  review  for  introducing  DevOps  in  that  context.  Method:  A  multivocal  literature  review  is  performed  and  evidence  is  synthesized  from  sources  published  between  2015  to  March  of  2020  to  capture  the  opinions  of  experts  and  community  in  this  field.  Results:  Our  findings  reveal  that  adoption  of  DevOps  in  a  regulated  medical  device  environment  such  as  ISO  13485  has  its  challenges,  but  potential  benefits  may  outweigh  those  in  areas  such  as  regulatory,  compliance,  security,  organizational  and  technical.  Conclusion:  DevOps  for  regulated  medical  device  environments  is  a  highly  appealing  approach  as  compared  to  traditional  methods  and  could  be  particularly  suited  for  regulated  medical  development.  However,  an  organization  must  properly  anchor  a  transition  to  DevOps  in  top-level  management  and  be  supportive  in  the  initial  phase  utilizing  professional  coaching  and  space  for  iterative  learning;  as  such  an  initiative  is  a  complex  organizational  and  technical  task.
1	A  large  scale  comparative  analysis  of  coding  standard  conformance  in  open  source  data  science  projects.  Background:  Meeting  the  growing  industry  demand  for  Data  Science  requires  cross-disciplinary  teams  that  can  translate  machine  learning  research  into  production-ready  code.  Software  engineering  teams  value  adherence  to  coding  standards  as  an  indication  of  code  readability,  maintainability,  and  developer  expertise.  However,  there  are  no  large-scale  empirical  studies  of  coding  standards  focused  specifically  on  Data  Science  projects.  Aims:  This  study  investigates  the  extent  to  which  Data  Science  projects  follow  code  standards.  In  particular,  which  standards  are  followed,  which  are  ignored,  and  how  does  this  differ  to  traditional  software  projects?  Method:  We  compare  a  corpus  of  1048  Open-Source  Data  Science  projects  to  a  reference  group  of  1099  non-Data  Science  projects  with  a  similar  level  of  quality  and  maturity.  Results:  Data  Science  projects  suffer  from  a  significantly  higher  rate  of  functions  that  use  an  excessive  numbers  of  parameters  and  local  variables.  Data  Science  projects  also  follow  different  variable  naming  conventions  to  non-Data  Science  projects.  Conclusions:  The  differences  indicate  that  Data  Science  codebases  are  distinct  from  traditional  software  codebases  and  do  not  follow  traditional  software  engineering  conventions.  Our  conjecture  is  that  this  may  be  because  traditional  software  engineering  conventions  are  inappropriate  in  the  context  of  Data  Science  projects.
1	Translation  of  atl  to  agt  and  application  to  a  code  generator  for  simulink.  Analysing  and  reasoning  on  model  transformations  has  become  very  relevant  for  various  applications  such  as  ensuring  the  correctness  of  transformations.  ATL  is  a  model  transformation  language  with  rich  semantics  and  a  focus  on  usability,  making  its  analysis  not  straightforward.  Conversely,  algebraic  graph  transformation  (AGT)  is  an  approach  with  strong  theoretical  foundations  allowing  for  formal  analyses  that  would  be  valuable  in  the  context  of  ATL.  In  this  paper,  we  propose  a  translation  of  ATL  to  the  AGT  framework  in  the  objective  of  bringing  theoretical  analyses  of  AGT  to  ATL  transformations.  We  show  that  this  transformation  supports  a  sufficient  subset  of  ATL  to  be  used  on  an  industrial  application  example:  QGen,  a  qualifiable  Simulink\(^{\circledR  }\)  to  source  code  generator  developed  at  AdaCore.  In  addition  to  this  example,  we  validate  our  proposal  by  translating  a  set  of  feature-rich  ATL  transformations  to  the  Henshin  AGT  framework.  We  execute  the  ATL  and  AGT  versions  on  the  same  set  of  models  and  verify  that  the  result  is  the  same.
1	Benchmarking  bidirectional  transformations  theory  implementation  application  and  assessment.  Bidirectional  transformations  (bx)  are  relevant  for  a  wide  range  of  application  domains.  While  bx  problems  may  be  solved  with  unidirectional  languages  and  tools,  maintaining  separate  implementations  of  forward  and  backward  synchronizers  with  mutually  consistent  behavior  can  be  difficult,  laborious,  and  error-prone.  To  address  the  challenges  involved  in  handling  bx  problems,  dedicated  languages  and  tools  for  bx  have  been  developed.  Due  to  their  heterogeneity,  however,  the  numerous  and  diverse  approaches  to  bx  are  difficult  to  compare,  with  the  consequence  that  fundamental  differences  and  similarities  are  not  yet  well  understood.  This  motivates  the  need  for  suitable  benchmarks  that  facilitate  the  comparison  of  bx  approaches.  This  paper  provides  a  comprehensive  treatment  of  benchmarking  bx,  covering  theory,  implementation,  application,  and  assessment.  At  the  level  of  theory,  we  introduce  a  conceptual  framework  that  defines  and  classifies  architectures  of  bx  tools.  At  the  level  of  implementation,  we  describe  Benchmarx,  an  infrastructure  for  benchmarking  bx  tools  which  is  based  on  the  conceptual  framework.  At  the  level  of  application,  we  report  on  a  wide  variety  of  solutions  to  the  well-known  Families-to-Persons  benchmark,  which  were  developed  and  compared  with  the  help  of  Benchmarx.  At  the  level  of  assessment,  we  reflect  on  the  usefulness  of  the  Benchmarx  approach  to  benchmarking  bx,  based  on  the  experiences  gained  from  the  Families-to-Persons  benchmark.
1	Identifying  strategies  for  study  selection  in  systematic  reviews  and  maps.  Study  selection  in  systematic  reviews  is  prone  to  bias  and  there  exist  no  commonly  defined  strategies  of  how  to  reduce  the  bias  and  resolve  disagreement  between  researchers.  This  study  aims  at  identifying  strategies  for  bias  reduction  and  disagreement  resolution.  A  review  of  existing  systematic  reviews  is  conducted  for  study  selection  strategy  identification.  In  total  13  different  strategies  have  been  identified.
1	Evaluating  the  fittest  automated  testing  tools  an  industrial  case  study.  This  paper  aims  at  evaluating  a  set  of  automated  tools  of  the  FITTEST  EU  project  within  an  industrial  case  study.  The  case  study  was  conducted  at  the  IBM  Research  lab  in  Haifa,  by  a  team  responsible  for  building  the  testing  environment  for  future  development  versions  of  an  IBM  system  management  product.  The  main  function  of  that  product  is  resource  management  in  a  networked  environment.  This  case  study  has  investigated  whether  current  IBM  Research  testing  practices  could  be  improved  or  complemented  by  using  some  of  the  automated  testing  tools  that  were  developed  within  the  FITTEST  EU  project.  Although  the  existing  Test  Suite  from  IBM  Research  (TSibm)  that  was  selected  for  comparison  is  substantially  smaller  than  the  Test  Suite  generated  by  FITTEST  (TSfittest),  the  effectiveness  of  TSfittest,  measured  by  the  injected  faults  coverage  is  significantly  higher  (50%  vs  70%).  With  respect  to  efficiency,  by  normalizing  the  execution  times,  we  found  the  TSfittest  runs  faster  (9.18  vs.  6.99).  This  is  due  to  the  fact  that  the  TSfittest  includes  shorter  tests.  Within  IBM  Research  and  for  the  testing  of  the  target  product  in  the  simulated  environment:  the  FITTEST  tools  can  increase  the  effectiveness  of  the  current  practice  and  the  test  cases  automatically  generated  by  the  FITTEST  tools  can  help  in  more  efficient  identification  of  the  source  of  the  identified  faults.  Moreover,  the  FITTEST  tools  have  shown  the  ability  to  automate  testing  within  a  real  industry  case.
1	Design  of  an  iot  bim  gis  based  risk  management  system  for  hospital  basic  operation.  There  has  been  a  significant  increase  in  emergency  incidents  involving  hospital  basic  operation  (non-clinical  side  of  hospital  daily  running  and  maintaining),  which  adversely  affects  the  functioning  of  hospitals  and  poses  threat  to  the  staff  members  and  patients.  To  cope  with  these  emergencies,  our  group  proposes  a  design  of  risk  management  system  based  on  technologies  of  Internet  of  Things  (IoT),  building  information  model  (BIM),  and  geographic  information  system  (GIS),  aiming  to  realize  real-time  risk  factors  identification  and  more  effective  and  more  efficient  coordinated  response.  In  this  paper,  the  system  architecture,  key  technologies,  and  simulated  cases  are  also  presented  respectively.
1	Model  based  management  of  service  composition.  Promoted  by  the  Service  Computing  paradigm,  service  composition  has  played  an  important  role  in  modern  software  development.  Currently,  available  services  have  covered  a  wide  spectrum  of  heterogeneity,  including  SOAP  services,  Restful  services  and  other  data  services.  The  composite  services  should  continuously  serve  for  a  large  number  of  users.  The  heterogeneity  and  open  dynamic  network  environment  bring  grand  challenges  to  the  management  of  service  composition.  Based  upon  our  previous  work  on  service  composition  middleware  -  Star  link,  and  a  runtime  system  management  tool  -  SM@RT,  this  paper  proposes  a  model-based  approach  to  service  composition  management  at  run  time.  A  runtime  model  enables  casual  connections  between  applications  and  supporting  platforms,  provides  a  global  view  of  a  running  system,  abstracts  underlying  technical  details,  and  performs  automated  generation  of  management  code.  By  constructing  the  runtime  model  of  Star  link  and  using  the  SM@RT  tool  to  generate  synchronization  between  model  and  running  composition,  our  approach  makes  the  following  contributions  to  service  composition  management:  (1)  a  more  comprehensive  view  of  service  composition  management,  (2)  an  easy-of-use  manner  to  perform  management  operations  at  model  level  without  underlying  tedious  details,  (3)  an  on-the-fly  effect  on  running  system  by  means  of  synchronization  between  the  model  and  composite  services.  We  demonstrate  that  our  approach  can  tackle  the  challenge  of  service  composition  management  by  using  a  case  study  of  a  photo  sharing  composite  service  application.
1	The  making  of  a  system  of  systems  ontology  reveals  the  true  nature  of  emergence.  A  consistent,  cohesive,  and  predictive  framework  that  harbors  the  ontology  and  taxonomy  of  systems  and  system  of  systems  is  described  that  exposes  the  true  nature  of  emergence  as  resulting  from  interactions  that  change  physical  objects.  It  is  shown  that  the  part-whole  relationship  is  represented  through  the  language  syntax  and  semantics  of  integration  and  interoperability  for  systems  and  system  of  systems  to  build  ontologies  of  objects  and  processes  with  primitives  for  objects  of  objects,  functions,  and  behaviors;  and  for  processes  with  primitives  of  cognitions,  developing  procedures,  and  modeling.
1	Predicting  the  vector  impact  of  change  an  industrial  case  study  at  brightsquid.  Background:  Understanding  and  controlling  the  impact  of  change  decides  about  the  success  or  failure  of  evolving  products.  The  problem  magnifies  for  start-ups  operating  with  limited  resources.  Their  usual  focus  is  on  Minimum  Viable  Product  (MVP's)  providing  specialized  functionality,  thus  have  little  expense  available  for  handling  changes.  Aims:  Change  Impact  Analysis  (CIA)  refers  to  the  identification  of  source  code  files  impacted  when  implementing  a  change  request.  We  extend  this  question  to  predict  not  only  affected  files,  but  also  the  effort  needed  for  implementing  the  change,  and  the  duration  necessary  for  that.  Method:  This  study  evaluates  the  performance  of  three  textual  similarity  techniques  for  CIA  based  on  Bag  of  words  in  combination  with  either  topic  modeling  or  file  coupling.  Results:  The  approaches  are  applied  on  data  from  two  industrial  projects.  The  data  comes  as  part  of  an  industrial  collaboration  project  with  Brightsquid,  a  Canadian  start-up  company  specializing  in  secure  communication  solutions.  Performance  analysis  shows  that  combining  textual  similarity  with  file  coupling  improves  impact  prediction,  resulting  in  Recall  of  67%.  Effort  and  duration  can  be  predicted  with  84%  and  72%  accuracy  using  textual  similarity  only.  Conclusions:  The  relative  effort  invested  into  CIA  for  predicting  impacted  files  can  be  reduced  by  extending  its  applicability  to  multiple  dimensions  which  include  impacted  files,  effort,  and  duration.
1	When  and  who  leaves  matters  emerging  results  from  an  empirical  study  of  employee  turnover.  Background:  Employee  turnover  in  GSD  is  an  extremely  important  issue,  especially  in  Western  companies  offshoring  to  emerging  nations.  Aims:  In  this  case  study  we  investigated  an  offshore  vendor  company  and  in  particular  whether  the  employees'  retention  is  related  with  their  experience.  Moreover,  we  studied  whether  we  can  identify  a  threshold  associated  with  the  employees'  tendency  to  leave  the  particular  company.  Method:  We  used  a  case  study,  applied  and  presented  descriptive  statistics,  contingency  tables,  results  from  Chi-Square  test  of  association  and  post  hoc  tests.  Results:  The  emerging  results  showed  that  employee  retention  and  company  experience  are  associated.  In  particular,  almost  90%  of  the  employees  are  leaving  the  company  within  the  first  year,  where  the  percentage  within  the  second  year  is  50-50%.  Thus,  there  is  an  indication  that  the  2  years'  time  is  the  retention  threshold  for  the  investigated  offshore  vendor  company.  Conclusions:  The  results  are  preliminary  and  lead  us  to  the  need  for  building  a  prediction  model  which  should  include  more  inherent  characteristics  of  the  projects  to  aid  the  companies  avoiding  massive  turnover  waves.
1	Clone  management  for  evolving  software.  Recent  research  results  suggest  a  need  for  code  clone  management.  In  this  paper,  we  introduce  JSync,  a  novel  clone  management  tool.  JSync  provides  two  main  functions  to  support  developers  in  being  ...
1	Mechanical  properties  comfort  and  bio  function  of  pla  knits  treated  with  microcapsules  of  citrus  unshiu  s  essential  oil.  This  study  aimed  to  investigate  the  mechanical  properties,  comfort  attributes  such  as  water  absorption,  water-vapor  permeability,  as  well  as  air  permeability,  fragrance  release,  and  antimicrobial  activity  of  PLA  knits  treated  with  microcapsules  containing  Citrus  Unshiu`s  essential  oil  (MIC-CUEO).  As  for  the  mechanical  properties  by  KES,  tensile  and  bending  rigidity  were  slightly  increased  while  surface  roughness  and  compressional  energy  were  reduced.  Comfort  attributes  including  water  absorption,  water-vapor  permeability,  and  air  permeability  were  also  declined  but  not  significant.  Fragrance  release  rates  were  very,  which  means  that  the  treated  PLA  could  preserve  its  fragrance  feasibly.  Finally,  lower  capsule  and  binder  concentrations  such  as  3%  were  shown  to  provide  excellent  antimicrobial  activity  to  PLA  knits  even  after  10  repeated  launders.
1	Effect  of  wet  spinning  and  heat  treatment  on  the  structure  and  mechanical  properties  of  polyhydroxyamide  fibers  i  coagulation  behavior  at  various  coagulation  conditions.  Abstract:  Polyhydroxyamide  (PHA)  was  synthesized  using  low  temperature  solution  polymerization  of  3,3'-dihydrox-ybenzidine  (DHB)  and  isophthaloyl  chloride  (IPC)  in  N,N-dimethylacetamide  (DMAc).  In  order  to  study  wet-spinningof  PHA  fibers,  the  diffusion  property  of  DMAc  in  various  coagulants  and  the  effect  of  coagulation  bath  temperaturewere  evaluated.  The  initial  diffusion  rate  of  DMAc,  and  the  SEM  images  and  mechanical  properties  of  PHA  fibers,demonstrated  that  the  coagulation  in  ethanol  at  20  o  C  was  the  most  optimal  among  all  the  conditions  examined.  Thetensile  strength  and  initial  modulus  of  PHA  fibers  increased,  while  its  breaking  strain  decreased  with  increasing  spindraw  ratio  (SDR).  The  wide-angle  X-ray  diffraction  (WAXD)  experiment  revealed  that  the  crystallinity  of  PHA  fibersincreased  with  increasing  SDR.  The  process-structure-property  relationship  among  PHA  fibers  under  various  coagula-tion  conditions  was  also  investigated.  Keywords:  PHA  fibers,  spinning  dope,  coagulation  bath  temperature,  coagulants,  initial  diffusion  rate,  spin  draw  ratio
1	Verifying  the  evolution  of  probability  distributions  governed  by  a  dtmc.  We  propose  a  new  probabilistic  temporal  logic,  iLTL,  which  captures  properties  of  systems  whose  state  can  be  represented  by  probability  mass  functions  (pmfs).  Using  iLTL,  we  can  specify  reachability  to  a  state  (i.e.,  a  pmf),  as  well  as  properties  representing  the  aggregate  (expected)  behavior  of  a  system.  We  then  consider  a  class  of  systems  whose  transitions  are  governed  by  a  Markov  Chain-in  this  case,  the  set  of  states  a  system  may  be  in  is  specified  by  the  transitions  of  pmfs  from  all  potential  initial  states  to  the  final  state.  We  then  provide  a  model  checking  algorithm  to  check  iLTL  properties  of  such  systems.  Unlike  existing  model  checking  techniques,  which  either  compute  the  portions  of  the  computational  paths  that  satisfy  a  specification  or  evaluate  properties  along  a  single  path  of  pmf  transitions,  our  model  checking  technique  enables  us  to  do  a  complete  analysis  on  the  expected  behaviors  of  large-scale  systems.  Desirable  system  parameters  may  also  be  found  as  a  counterexample  of  a  negated  goal.  Finally,  we  illustrate  the  usefulness  of  iLTL  model  checking  by  means  of  two  examples:  assessing  software  reliability  and  ensuring  the  results  of  administering  a  drug.
1	A  lightweight  system  for  detecting  and  tolerating  concurrency  bugs.  Along  with  the  prevalence  of  multi-threaded  programs,  concurrency  bugs  have  become  one  of  the  most  important  sources  of  software  bugs.  Even  worse,  due  to  the  non-deterministic  nature  of  concurrency  bugs,  these  bugs  are  both  difficult  to  detect  and  fix  even  after  the  detection.  As  a  result,  it  is  highly  desired  to  develop  an  all-around  approach  that  is  able  to  not  only  detect  them  during  the  testing  phase  but  also  tolerate  undetected  bugs  during  production  runs.  However,  existing  bug-detecting  and  bug-tolerating  tools  are  usually  either    1)    constrained  in  types  of  bugs  they  can  handle  or    2)    requiring  specific  hardware  supports  for  achieving  an  acceptable  overhead.  In  this  paper,  we  present  a  novel  program  invariant,  name  Anticipating  Invariant  (  Ai  ),  that  can  detect  most  types  of  concurrency  bugs.  More  importantly,    Ai    can  be  used  to  anticipate  many  concurrency  bugs  before  any  irreversible  changes  have  been  made.  Thus  it  enables  us  to  develop  a  software-only  system  that  is  able  to  forestall  failures  with  a  simple  thread  stalling  technique,  which  does  not  rely  on  execution  roll-back  and  hence  has  good  performance.  Experiments  with  35  real-world  concurrency  bugs  demonstrate  that    Ai    is  capable  of  detecting  and  tolerating  many  important  types  of  concurrency  bugs,  including  both  atomicity  and  order  violations.  It  has  also  exposed  two  new  bugs  (confirmed  by  developers)  that  were  never  reported  before  in  the  literature.  Performance  evaluation  with  6  representative  parallel  programs  shows  that    Ai    incurs  negligible  overhead  (      $              )  for  many  nontrivial  desktop  and  server  applications.
1	Mining  likely  analogical  apis  across  third  party  libraries  via  large  scale  unsupervised  api  semantics  embedding.  Establishing  API  mappings  between  third-party  libraries  is  a  prerequisite  step  for  library  migration  tasks.  Manually  establishing  API  mappings  is  tedious  due  to  the  large  number  of  APIs  to  be  examined.  Having  an  automatic  technique  to  create  a  database  of  likely  API  mappings  can  significantly  ease  the  task.  Unfortunately,  existing  techniques  either  adopt  supervised  learning  mechanism  that  requires  already-ported  or  functionality  similar  applications  across  major  programming  languages  or  platforms,  which  are  difficult  to  come  by  for  an  arbitrary  pair  of  third-party  libraries,  or  cannot  deal  with  lexical  gap  in  the  API  descriptions  of  different  libraries.  To  overcome  these  limitations,  we  present  an  unsupervised  deep  learning  based  approach  to  embed  both  API  usage  semantics  and  API  description  (name  and  document)  semantics  into  vector  space  for  inferring  likely  analogical  API  mappings  between  libraries.  Based  on  deep  learning  models  trained  using  tens  of  millions  of  API  call  sequences,  method  names  and  comments  of  2.8  millions  of  methods  from  135,127  GitHub  projects,  our  approach  significantly  outperforms  other  deep  learning  or  traditional  information  retrieval  (IR)  methods  for  inferring  likely  analogical  APIs.  We  implement  a  proof-of-concept  website  (  https://similarapi.appspot.com  )  which  can  recommend  analogical  APIs  for  583,501  APIs  of  111  pairs  of  analogical  Java  libraries  with  diverse  functionalities.  This  scale  of  third-party  analogical-API  database  has  never  been  achieved  before.
1	Classification  and  definition  of  an  enterprise  architecture  analyses  language.  Enterprise  Architecture  Management  (EAM)  deals  with  the  assessment  and  development  of  business  processes  and  IT  components.  Through  the  analysis  of  as-is  and  to-be  states  the  information  flow  in  organizations  is  optimized.  Thus  EAM  analyses  are  an  essential  part  in  the  EAM  cycle.  To  cover  the  needs  of  an  architect  the  analyses  pursue  different  goals  and  utilize  different  techniques.  In  this  work  we  examine  the  different  EA  analysis  approaches  according  to  their  characteristics  and  requirements.  For  that  purpose  we  design  a  generic  analysis  language  which  can  be  used  for  their  description.  In  order  to  manage  the  numerous  approaches  from  literature  we  develop  a  categorization.  The  categories  are  created  based  on  the  goals,  constructs  and  kind  of  results.  We  propose  a  two-dimensional  classification  into  functional  and  technical  categories.  The  goal  is  to  provide  a  common  description  for  EA  analyses  for  an  easy  access  to  their  goals  and  execution  requirements.
1	Smt  constrained  symbolic  execution  for  eclipse  cdt  codan.  This  paper  presents  a  symbolic  execution  plug-in  extension  for  Eclipse  CDT/Codan,  which  serves  to  reason  about  satisfiable  paths  of  C  programs.  Programs  are  translated  into  the  SMT-LIB  sublogic  of  arrays,  uninterpreted  functions  and  nonlinear  integer  and  real  arithmetic  (AUFNIRA),  and  path  satisfiability  is  automatically  examined  with  an  SMT  solver.  The  presented  plug-in  can  serve  as  a  basis  for  path-sensitive  static  bug  detection  with  bounded  or  unrestricted  context,  where  the  presence  of  bugs  is  decided  with  the  solver.  An  interface  provides  notifications  and  context  information  for  checker  classes.  With  a  buffer  bound  checker  the  symbolic  execution  plug-in  is  shown  capable  of  accurately  detecting  bugs  with  currently  36  of  the  39  C  flow  variants  of  the  NSA's  Juliet  test  suite  for  static  analyzers.
1	Isabelle  dof  design  and  implementation.  DOF  is  a  novel  framework  for  defining  ontologies  and  enforcing  them  during  document  development  and  document  evolution.  A  major  goal  of  DOF  is  the  integrated  development  of  formal  certification  documents  (e.  g.,  for  Common  Criteria  or  CENELEC  50128)  that  require  consistency  across  both  formal  and  informal  arguments.
1	Formal  analysis  of  the  dns  bandwidth  amplification  attack  and  its  countermeasures  using  probabilistic  model  checking.  The  DNS  Bandwidth  Amplification  Attack  (BAA)  is  a  distributed  denial-of-service  attack  in  which  a  network  of  computers  floods  a  DNS  server  with  responses  to  requests  that  have  never  been  made.  Amplification  enters  into  the  attack  by  virtue  of  the  fact  that  a  small  60-byte  request  can  be  answered  by  a  substantially  larger  response  of  4,000  bytes  or  more  in  size.  We  use  the  PRISM  probabilistic  model  checker  to  introduce  a  Continuous  Time  Markov  Chain  model  of  the  DNS  BAA  and  three  recently  proposed  countermeasures,  and  to  perform  an  extensive  cost-benefit  analysis  of  the  countermeasures.  Our  analysis,  which  is  applicable  to  both  DNS  and  DNSSec  (a  security  extension  of  DNS),  is  based  on  objective  metrics  that  weigh  the  benefits  for  a  server  in  terms  of  the  percentage  increase  in  the  processing  of  legitimate  packets  against  the  cost  incurred  by  incorrectly  dropping  legitimate  traffic.  The  results  we  obtain,  gleaned  from  more  than  450  PRISM  runs,  demonstrate  significant  differences  between  the  countermeasures  as  reflected  by  their  respective  net  benefits.  Our  results  also  reveal  that  DNSSec  is  more  vulnerable  than  DNS  to  a  BAA  attack,  and,  relatedly,  DNSSec  derives  significantly  less  benefit  from  the  countermeasures.
1	Theory  of  altruism  on  software  development  practices  patterns.  Software  development  is  not  just  about  the  cost  or  quality  of  the  software,  but  it  is  also  about  people  who  work  to  produce  software.  In  this  research,  I  intend  to  provide  empirical  and  reproducible  experiments  to  explore  the  most  suitable  forms  to  allow  programmers  to  develop  software,  either  solo  programming,  pair  programming  or  mob  programming.  My  overall  goal  is  not  only  to  reduce  the  software  development  cost  but  also  to  improve  programmers  life  quality.  As  a  more  concrete  objective,  I  will  propose  a  catalog  with  recommendations  on  how  to  organize  the  work  of  programmers.  Towards  reaching  my  goals,  I  am  carrying  out  application  examples  to  produce  raw  data.  After,  to  analyze  these  data,  I  will  use  Grounded  Theory  techniques  to  look  for  an  auditable  theory  that  explains  the  findings.  Simultaneously,  I  will  model  a  Theory  of  Altruism  based  on  the  curve  of  Pareto  and  Game  Theory  looking  for  a  Nash  equilibrium  over  the  organization  of  the  work  of  software  developers.  The  theory  of  altruism  fits  tapping  into  the  developer's  selfless  concern  for  the  well-being  of  others  could  be  exploitative  in  a  commercial  setting  considering  the  importance  of  sharing  knowledge  broadly  in  the  company,  learning,  producing,  and  profiting  as  a  team.  A  mathematical  theory  similar  to  the  Pareto  curve,  however,  a  Modeling  based  on  Agents,  to  analyze  the  influence  of  altruist  behavior  of  the  agents'  in  the  whole  context  involved.  The  apex  to  be  reached  is  based  on  Game  Theory,  inspired  by  The  Prisoners'  Dilemma,  using  the  raw  data  produced,  interpreted  using  Grounded  Theory  techniques,  and  in  a  triangulation  research  strategy  regarding  the  theory  of  altruism,  toward  permit  in  the  future  cultures  changing.
1	A  practitioner  s  guide  to  software  based  soft  error  mitigation  using  an  codes.  Arithmetic  error  coding  schemes  (AN  codes)  are  a  well  known  and  effective  technique  for  soft  error  mitigation.  Although  coding  theory  being  a  rich  area  of  mathematics,  their  implementation  seems  to  be  fairly  easy.  However,  compliance  with  the  theory  can  be  lost  easily  while  moving  towards  an  actual  implementation  -  finally  jeopardizing  the  aspired  fault-tolerance  characteristics.  In  this  paper,  we  present  our  experiences  and  lessons  learned  from  implementing  AN  codes  in  the  Cored  dependable  voter.  We  focus  on  the  challenges  and  pitfalls  in  the  transition  from  maths  to  machine  code  for  a  binary  computer  from  a  systems  perspective.  Our  results  show,  that  practical  misconceptions  (such  as  the  use  of  prime  numbers)  and  architecture-dependent  implementation  glitches  occur  on  every  stage  of  this  transition.  We  identify  typical  pitfalls  and  describe  practical  measures  to  find  and  resolve  them.  Our  measures  eliminate  all  remaining  SDCs  in  the  Cored  voter,  which  is  validated  by  an  extensive  fault-injection  campaign  that  covers  100  percent  of  the  fault  space  for  1-bit  and  2-bit  errors.
1	Using  tool  supported  model  based  safety  analysis  progress  and  experiences  in  saml  development.  Software  controls  in  technical  systems  are  becoming  more  and  more  important  and  complex.  Model  based  safety  analysis  can  give  provably  correct  and  complete  results,  often  in  a  fully  automatic  way.  These  methods  can  answer  both  logical  and  probabilistic  questions.  In  common  practice,  the  needed  models  must  be  specified  in  different  input  languages  of  different  tools  depending  on  the  chosen  verification  tool  for  the  desired  aspect.  This  is  time  consuming  and  error-prone.  To  cope  with  this  problem  we  developed  the  safety  analysis  modeling  language  (SAML).  In  this  paper,  we  present  a  new  tool  to  intuitively  create  probabilistic,  non-deterministic  and  deterministic  specifications  for  formal  analysis.  The  goal  is  to  give  tool-support  during  modeling  and  thus  make  building  a  formal  model  less  error-prone.  The  model  is  then  automatically  transformed  into  the  input  language  of  state  of  the  art  verification  engines.  We  illustrate  the  approach  on  a  case-study  from  nuclear  power  plant  domain.
1	Symbolic  visibly  pushdown  automata.  Nested  words  model  data  with  both  linear  and  hierarchical  structure  such  as  XML  documents  and  program  traces.  A  nested  word  is  a  sequence  of  positions  together  with  a  matching  relation  that  connects  open  tags  (calls)  with  the  corresponding  close  tags  (returns).  Visibly  Pushdown  Automata  are  a  restricted  class  of  pushdown  automata  that  process  nested  words,  and  have  many  appealing  theoretical  properties  such  as  closure  under  Boolean  operations  and  decidable  equivalence.  However,  like  any  classical  automata  models,  they  are  limited  to  finite  alphabets.  This  limitation  is  restrictive  for  practical  applications  to  both  XML  processing  and  program  trace  analysis,  where  values  for  individual  symbols  are  usually  drawn  from  an  unbounded  domain.  With  this  motivation,  we  introduce  Symbolic  Visibly  Pushdown  Automata  (SVPA)  as  an  executable  model  for  nested  words  over  infinite  alphabets.  In  this  model,  transitions  are  labeled  with  predicates  over  the  input  alphabet,  analogous  to  symbolic  automata  processing  strings  over  infinite  alphabets.  A  key  novelty  of  SVPAs  is  the  use  of  binary  predicates  to  model  relations  between  open  and  close  tags  in  a  nested  word.  We  show  how  SVPAs  still  enjoy  the  decidability  and  closure  properties  of  Visibly  Pushdown  Automata.  We  use  SVPAs  to  model  XML  validation  policies  and  program  properties  that  are  not  naturally  expressible  with  previous  formalisms  and  provide  experimental  results  for  our  implementation.
1	The  deepsec  prover.  In  this  paper  we  describe  the  DeepSec  prover,  a  tool  for  security  protocol  analysis.  It  decides  equivalence  properties  modelled  as  trace  equivalence  of  two  processes  in  a  dialect  of  the  applied  pi  calculus.
1	Avatar  the  architecture  for  first  order  theorem  provers.  This  paper  describes  a  new  architecture  for  first-order  resolution  and  superposition  theorem  provers  called  AVATAR  (Advanced  Vampire  Architecture  for  Theories  and  Resolution).  Its  original  motivation  comes  from  a  problem  well-studied  in  the  past    dealing  with  problems  having  clauses  containing  propositional  variables  and  other  clauses  that  can  be  split  into  components  with  disjoint  sets  of  variables.  Such  clauses  are  common  for  problems  coming  from  applications,  for  example  in  program  verification  and  program  analysis,  where  many  ground  literals  occur  in  the  problems  and  even  more  are  generated  during  the  proof-search.    This  problem  was  previously  studied  by  adding  various  versions  of  splitting.  The  addition  of  splitting  resulted  in  some  improvements  in  performance  of  theorem  provers.  However,  even  with  various  versions  of  splitting,  the  performance  of  superposition  theorem  provers  is  nowhere  near  SMT  solvers  on  variable-free  problems  or  SAT  solvers  on  propositional  problems.    This  paper  describes  a  new  architecture  for  superposition  theorem  provers,  where  a  superposition  theorem  prover  is  tightly  integrated  with  a  SAT  or  an  SMT  solver.  Its  implementation  in  our  theorem  prover  Vampire  resulted  in  drastic  improvements  over  all  previous  implementations  of  splitting.  Over  four  hundred  TPTP  problems  previously  unsolvable  by  any  modern  prover,  including  Vampire  itself,  have  been  proved,  most  of  them  with  short  runtimes.  Nearly  all  problems  solved  with  one  of  481  variants  of  splitting  previously  implemented  in  Vampire  can  also  be  solved  with  AVATAR.    We  also  believe  that  AVATAR  is  an  important  step  towards  efficient  reasoning  with  both  quantifiers  and  theories,  which  is  one  of  the  key  areas  in  modern  applications  of  theorem  provers  in  program  analysis  and  verification.
1	Security  aware  synthesis  using  delayed  action  games.  Stochastic  multiplayer  games  (SMGs)  have  gained  attention  in  the  field  of  strategy  synthesis  for  multi-agent  reactive  systems.  However,  standard  SMGs  are  limited  to  modeling  systems  where  all  agents  have  full  knowledge  of  the  state  of  the  game.  In  this  paper,  we  introduce  delayed-action  games  (DAGs)  formalism  that  simulates  hidden-information  games  (HIGs)  as  SMGs,  where  hidden  information  is  captured  by  delaying  a  player’s  actions.  The  elimination  of  private  variables  enables  the  usage  of  SMG  off-the-shelf  model  checkers  to  implement  HIGs.  Furthermore,  we  demonstrate  how  a  DAG  can  be  decomposed  into  subgames  that  can  be  independently  explored,  utilizing  parallel  computation  to  reduce  the  model  checking  time,  while  alleviating  the  state  space  explosion  problem  that  SMGs  are  notorious  for.  In  addition,  we  propose  a  DAG-based  framework  for  strategy  synthesis  and  analysis.  Finally,  we  demonstrate  applicability  of  the  DAG-based  synthesis  framework  on  a  case  study  of  a  human-on-the-loop  unmanned-aerial  vehicle  system  under  stealthy  attacks,  where  the  proposed  framework  is  used  to  formally  model,  analyze  and  synthesize  security-aware  strategies  for  the  system.
1	Tartar  a  timed  automata  repair  tool.  We  present  TarTar,  an  automatic  repair  analysis  tool  that,  given  a  timed  diagnostic  trace  (TDT)  obtained  during  the  model  checking  of  a  timed  automaton  model,  suggests  possible  syntactic  repairs  of  the  analyzed  model.  The  suggested  repairs  include  modified  values  for  clock  bounds  in  location  invariants  and  transition  guards,  adding  or  removing  clock  resets,  etc.  The  proposed  repairs  guarantee  that  the  given  TDT  is  no  longer  feasible  in  the  repaired  model,  while  preserving  the  overall  functional  behavior  of  the  system.  We  give  insights  into  the  design  and  architecture  of  TarTar,  and  show  that  it  can  successfully  repair  69%  of  the  seeded  errors  in  system  models  taken  from  a  diverse  suite  of  case  studies.
1	Interpolation  based  semantic  gate  extraction  and  its  applications  to  qbf  preprocessing.  We  present  a  new  semantic  gate  extraction  technique  for  propositional  formulas  based  on  interpolation.  While  known  gate  detection  methods  are  incomplete  and  rely  on  pattern  matching  or  simple  semantic  conditions,  this  approach  can  detect  any  definition  entailed  by  an  input  formula.
1	Yicesa2  2.  Yices  is  an  SMT  solver  developed  by  SRI  International.  The  first  version  of  Yices  was  released  in  2006  and  has  been  continuously  updated  since  then.  In  2007,  we  started  a  complete  re-implementation  of  the  solver  to  improve  performance  and  increase  modularity  and  flexibility.  We  describe  the  latest  release  of  Yices,  namely,  Yices  2.2.  We  present  the  tool's  architecture  and  discuss  the  algorithms  it  implements,  and  we  describe  recent  developments  such  as  support  for  the  SMT-LIBa2.0  notation  and  various  performance  improvements.
1	Under  approximating  cut  sets  for  reachability  in  large  scale  automata  networks.  In  the  scope  of  discrete  finite-state  models  of  interacting  components,  we  present  a  novel  algorithm  for  identifying  sets  of  local  states  of  components  whose  activity  is  necessary  for  the  reachability  of  a  given  local  state.  If  all  the  local  states  from  such  a  set  are  disabled  in  the  model,  the  concerned  reachability  is  impossible.    Those  sets  are  referred  to  as  cut  sets  and  are  computed  from  a  particular  abstract  causality  structure,  so-called  Graph  of  Local  Causality,  inspired  from  previous  work  and  generalised  here  to  finite  automata  networks.  The  extracted  sets  of  local  states  form  an  under-approximation  of  the  complete  minimal  cut  sets  of  the  dynamics:  there  may  exist  smaller  or  additional  cut  sets  for  the  given  reachability.    Applied  to  qualitative  models  of  biological  systems,  such  cut  sets  provide  potential  therapeutic  targets  that  are  proven  to  prevent  molecules  of  interest  to  become  active,  up  to  the  correctness  of  the  model.  Our  new  method  makes  tractable  the  formal  analysis  of  very  large  scale  networks,  as  illustrated  by  the  computation  of  cut  sets  within  a  Boolean  model  of  biological  pathways  interactions  gathering  more  than  9000  components.
1	Exploring  parameter  space  of  stochastic  biochemical  systems  using  quantitative  model  checking.  We  propose  an  automated  method  for  exploring  kinetic  parameters  of  stochastic  biochemical  systems.  The  main  question  addressed  is  how  the  validity  of  an  a  priori  given  hypothesis  expressed  as  a  temporal  logic  property  depends  on  kinetic  parameters.  Our  aim  is  to  compute  a  landscape  function  that,  for  each  parameter  point  from  the  inspected  parameter  space,  returns  the  quantitative  model  checking  result  for  the  respective  continuous  time  Markov  chain.  Since  the  parameter  space  is  in  principle  dense,  it  is  infeasible  to  compute  the  landscape  function  directly.  Hence,  we  design  an  effective  method  that  iteratively  approximates  the  lower  and  upper  bounds  of  the  landscape  function  with  respect  to  a  given  accuracy.  To  this  end,  we  modify  the  standard  uniformization  technique  and  introduce  an  iterative  parameter  space  decomposition.  We  also  demonstrate  our  approach  on  two  biologically  motivated  case  studies.
1	Modelling  cost  effectiveness  of  defenses  in  industrial  control  systems.  Industrial  Control  Systems  (ICS)  play  a  critical  role  in  controlling  industrial  processes.  Wide  use  of  modern  IT  technologies  enables  cyber  attacks  to  disrupt  the  operation  of  ICS.  Advanced  Persistent  Threats  (APT)  are  the  most  threatening  attacks  to  ICS  due  to  their  long  persistence  and  destructive  cyber-physical  effects  to  ICS.  This  paper  considers  a  simulation  of  attackers  and  defenders  of  an  ICS,  where  the  defender  must  consider  the  cost-effectiveness  of  implementing  defensive  measures  within  the  system  in  order  to  create  an  optimal  defense.  The  aim  is  to  identify  the  appropriate  deployment  of  a  specific  defensive  strategy,  such  as  defense-in-depth  or  critical  component  defense.  The  problem  is  represented  as  a  strategic  competitive  optimisation  problem,  which  is  solved  using  a  co-evolutionary  particle  swarm  optimisation  algorithm.  Through  the  development  of  optimal  defense  strategy,  it  is  possible  to  identify  when  each  specific  defensive  strategies  is  most  appropriate;  where  the  optimal  defensive  strategy  depends  on  the  resources  available  and  the  relative  effectiveness  of  those  resources.
1	Rare  event  simulation  for  dynamic  fault  trees.  Fault  trees  (FT)  are  a  popular  industrial  method  for  reliability  engineering,  for  which  Monte  Carlo  simulation  is  an  important  technique  to  estimate  common  dependability  metrics,  such  as  the  system  reliability  and  availability.  A  severe  drawback  of  Monte  Carlo  simulation  is  that  the  number  of  simulations  required  to  obtain  accurate  estimations  grows  extremely  large  in  the  presence  of  rare  events,  i.e.,  events  whose  probability  of  occurrence  is  very  low,  which  typically  holds  for  failures  in  highly  reliable  systems.    This  paper  presents  a  novel  method  for  rare  event  simulation  of  dynamic  fault  trees  with  complex  repairs  that  requires  only  a  modest  number  of  simulations,  while  retaining  statistically  justified  confidence  intervals.  Our  method  exploits  the  importance  sampling  technique  for  rare  event  simulation,  together  with  a  compositional  state  space  generation  method  for  dynamic  fault  trees.    We  demonstrate  our  approach  using  two  parameterized  sets  of  case  studies,  showing  that  our  method  can  handle  fault  trees  that  could  not  be  evaluated  with  either  existing  analytical  techniques,  nor  with  standard  simulation  techniques.
1	A  preliminary  fault  injection  framework  for  evaluating  multicore  systems.  Multicore  processors  are  becoming  more  and  more  attractive  in  embedded  and  safety-critical  domains  because  they  allow  increasing  the  performance  by  ensuring  reduced  power  consumption.  However,  moving  to  multicore  systems  raises  novel  dependability  challenges:  the  number  of  cores,  concurrency  issues,  shared  resources  and  interconnections  among  cores  make  it  hard  to  develop  and  validate  software  deployed  on  the  top  of  multicore  processors.    This  paper  discusses  a  preliminary  fault  injection  framework,  which  aims  to  investigate  dependability  properties  of  multicore-based  systems.  The  proposed  framework  leverages  the  error  reporting  architecture  provided  by  modern  processors  and  has  been  instantiated  in  the  context  of  the  Intel  Core  i7  processor.  Fault  injection  campaigns  have  been  conducted  under  the  Linux  OS  to  show  the  benefits  of  the  framework.
1	Exploiting  trust  in  deterministic  builds.  Deterministic  builds,  where  the  compile  and  build  processes  are  reproducible,  can  be  used  to  achieve  increased  trust  in  distributed  binaries.  As  the  trust  can  be  distributed  across  a  set  of  builders,  where  all  provide  their  own  signature  of  a  byte-to-byte  identical  binary,  all  have  to  cooperate  in  order  to  introduce  unwanted  code  in  the  binary.  On  the  other  hand,  if  an  attacker  manages  to  incorporate  malicious  code  in  the  source,  and  make  this  remain  undetected  during  code  reviews,  the  deterministic  build  provides  additional  opportunities  to  introduce  e.g.,  a  backdoor.  The  impact  of  such  a  successful  attack  would  be  serious  since  the  actual  trust  model  is  exploited.  In  this  paper,  the  problem  of  crafting  such  hidden  code  that  is  difficult  to  detect,  both  during  code  reviews  of  the  source  code  as  well  as  static  analysis  of  the  binary  executable  is  addressed.  It  is  shown  that  the  displacement  and  immediate  fields  of  an  instruction  can  be  used  the  embed  hidden  code  directly  from  the  C  programming  language.
1	Mcmas  slk  a  model  checker  for  the  verification  of  strategy  logic  specifications.  Model  checking  has  come  of  age.  A  number  of  techniques  are  increasingly  used  in  industrial  setting  to  verify  hardware  and  software  systems,  both  against  models  and  concrete  implementations.  While  it  is  generally  accepted  that  obstacles  still  remain,  notably  handling  infinite  state  systems  efficiently,  much  of  current  work  involves  refining  and  improving  existing  techniques  such  as  predicate  abstraction.
1	A  safety  condition  monitoring  system.  In  any  safety  argument,  belief  in  the  top-level  goal  depends  upon  a  variety  of  assumptions  that  derive  from  the  system  development  process,  the  operating  context,  and  the  system  itself.  If  an  assumption  is  false  or  becomes  false  at  any  point  during  the  lifecycle,  the  rationale  for  belief  in  the  safety  goal  might  be  invalidated  and  the  safety  of  the  associated  system  compromised.  Assurance  that  assumptions  actually  hold  when  they  are  supposed  to  is  not  guaranteed,  and  so  monitoring  of  assumptions  might  be  required.  In  this  paper,  we  describe  the  Safety  Condition  Monitoring  System,  a  system  that  permits  comprehensive  yet  flexible  monitoring  of  assumptions  throughout  the  entire  lifecycle  together  with  an  alert  infrastructure  that  allows  tailored  responses  to  violations  of  assumptions.  An  emphasis  of  the  paper  is  the  approach  used  to  run-time  monitoring  of  assumptions  derived  from  software  where  the  software  cannot  be  easily  changed.
1	A  stamp  analysis  on  the  china  yongwen  railway  accident.  Traditional  accident  models  regard  accidents  as  resulting  from  a  linear  chain  of  events.  They  are  limited  in  their  ability  to  handle  accidents  in  complex  systems  including  non-linear  interactions  among  components,  software  errors,  human  decision-making,  and  organizational  factors.  A  new  accident  model  called  Systems-theoretic  Accident  Modeling  and  Processes  (STAMP)  has  been  developed  by  Leveson  to  explain  such  accidents.  In  this  paper,  we  will  use  the  STAMP  accident  model  to  analyze  the  China-Yongwen  railway  accident  for  a  more  comprehensive  view  of  the  accident  and  propose  some  improvement  measures  to  prevent  similar  accidents  in  the  future.
1	Generating  reproducible  and  replayable  bug  reports  from  android  application  crashes.  Manually  reproducing  bugs  is  time-consuming  and  tedious.  Software  maintainers  routinely  try  to  reproduce  unconfirmed  issues  using  incomplete  or  noninformative  bug  reports.  Consequently,  while  reproducing  an  issue,  the  maintainer  must  augment  the  report  with  information---such  as  a  reliable  sequence  of  descriptive  steps  to  reproduce  the  bug---to  aid  developers  with  diagnosing  the  issue.  This  process  encumbers  issue  resolution  from  the  time  the  bug  is  entered  in  the  issue  tracking  system  until  it  is  reproduced.  This  paper  presents    crashdroid  ,  an  approach  for  automating  the  process  of  reproducing  a  bug  by  translating  the  call  stack  from  a  crash  report  into  expressive  steps  to  reproduce  the  bug  and  a  kernel  event  trace  that  can  be  replayed  on-demand.    crashdroid    manages  traceability  links  between  scenarios'  natural  language  descriptions,  method  call  traces,  and  kernel  event  traces.  We  evaluated    crashdroid    on  several  open-source  Android  applications  infected  with  errors.  Given  call  stacks  from  crash  reports,    crashdroid    was  able  to  generate  expressive  steps  to  reproduce  the  bugs  and  automatically  replay  the  crashes.  Moreover,  users  were  able  to  confirm  the  crashes  faster  with    crashdroid    than  manually  reproducing  the  bugs  or  using  a  stress-testing  tool.
1	Industrial  case  study  on  supporting  the  comprehension  of  system  behaviour  under  load.  Large-scale  software  systems  achieve  concurrency  on  enormous  scales  using  a  number  of  different  design  patterns.  Many  of  these  design  patterns  are  based  on  pools  of  pre-existing  and  reusable  threads  that  facilitate  incoming  service  requests.  Thread  pools  limit  thread  lifecycle  overhead  (thread  creation  and  destruction)  and  resource  thrashing  (thread  proliferation).  Despite  their  potential  for  scalability,  thread  pools  are  hard  to  configure  and  test  because  of  concurrency  risks  like  synchronization  errors  and  dead  lock,  and  thread  pool-specific  risks  like  resource  thrashing  and  thread  leakage.  Addressing  these  challenges  requires  a  thorough  understanding  of  the  behaviour  of  the  threads  in  the  thread  pool.  We  argue  for  a  methodology  to  automatically  identify  and  rank  deviations  in  the  behaviour  of  threads  based  on  resource  usage.
1	Sdexplorer  a  generic  toolkit  for  smoothly  exploring  massive  scale  sequence  diagram.  To  understand  program's  behavior,  using  reverse-engineered  sequence  diagram  is  a  valuable  technique.  In  practice,  researchers  usually  record  execution  traces  and  generate  a  sequence  diagram  according  to  them.  However,  the  diagram  can  be  too  large  to  read  while  treating  real-world  software  due  to  the  massiveness  of  execution  traces.  Several  studies  on  minimizing/compressing  sequence  diagrams  have  been  proposed;  however,  the  resulting  diagram  may  be  either  still  large  or  losing  important  information.  Besides,  existing  tools  are  highly  customized  for  a  certain  research  purpose.  To  address  these  problems,  we  present  a  generic  toolkit  SDExplorer  in  this  paper,  which  is  a  flexible  and  lightweight  tool  to  effectively  explore  a  massive-scale  sequence  diagram  in  a  highly  scalable  manner.  Additionally,  SDExplorer  supports  popular  features  of  existing  tools  (i.e.  search,  filter,  grouping,  etc.).  We  believe  it  is  an  easy-to-use  and  promising  tool  in  future  research  to  evaluate  and  compare  the  minimizing/compressing  techniques  in  real  maintenance  tasks.  SDExplorer  is  available  at  https://lyukx.github.io/SDExplorer/.
1	An  empirical  study  of  quick  remedy  commits.  Software  systems  are  continuously  modified  to  implement  new  features,  to  fix  bugs,  and  to  improve  quality  attributes.  Most  of  these  activities  are  not  atomic  changes,  but  rather  the  result  of  several  related  changes  affecting  different  parts  of  the  code.  For  this  reason,  it  may  happen  that  developers  omit  some  of  the  needed  changes  and,  as  a  consequence,  leave  a  task  partially  unfinished,  introduce  technical  debt  or,  in  the  worst  case  scenario,  inject  bugs.  Knowing  the  changes  that  are  mistakenly  omitted  by  developers  can  help  in  designing  recommender  systems  able  to  automatically  identify  risky  situations  in  which,  for  example,  the  developer  is  likely  to  be  pushing  an  incomplete  change  to  the  software  repository.  We  present  a  qualitative  study  investigating  "quick  remedy  commits"  performed  by  developers  with  the  goal  of  implementing  changes  omitted  in  previous  commits.  With  quick  remedy  commits  we  refer  to  commits  that  (i)  quickly  follow  a  commit  performed  by  the  same  developer  in  the  same  repository,  and  (ii)  aim  at  remedying  issues  introduced  as  the  result  of  code  changes  omitted  in  the  previous  commit  (e.g.,  fix  references  to  code  components  that  have  been  broken  as  a  consequence  of  a  rename  refactoring).  Through  a  manual  analysis  of  500  quick  remedy  commits,  we  define  a  taxonomy  categorizing  the  types  of  changes  that  developers  tend  to  omit.  The  defined  taxonomy  can  guide  the  development  of  tools  aimed  at  detecting  omitted  changes,  and  possibly  autocomplete  them.
1	Agile  software  assessment  invited  paper.  Informed  decision  making  is  a  critical  activity  in  software  development,  but  it  is  poorly  supported  by  common  development  environments,  which  focus  mainly  on  low-level  programming  tasks.  We  posit  the  need  for  agile  software  assessment,  which  aims  to  support  decision  making  by  enabling  rapid  and  effective  construction  of  software  models  and  custom  analyses.  Agile  software  assessment  entails  gathering  and  exploiting  the  broader  context  of  software  information  related  to  the  system  at  hand  as  well  as  the  ecosystem  of  related  projects,  and  beyond  to  include  “big  software  data”.  Finally,  informed  decision  making  entails  continuous  assessment  by  monitoring  the  evolving  system  and  its  architecture.  We  identify  several  key  research  challenges  in  supporting  agile  software  assessment  by  focusing  on  customization,  context  and  continuous  assessment.
1	Trustrace  improving  automated  trace  retrieval  through  resource  trust  analysis.  Traceability  is  a  task  to  create/recover  traceability  links  among  different  software  artifacts.  It  uses  resources,  such  as  an  expert,  source  and  target  document,  and  traceability  approach,  to  create/recover  traceability  links.  However,  it  does  not  provide  any  guidance  that  how  much  we  can  trust  on  available  resources.  We  propose  Trustrace,  a  trust-based  traceability  recovery  process,  to  improve  expert  trust  on  a  recovered  link  and  trust  over  the  traceability  inputs.  Trustrace  has  three  sub  components,  in  particular,  Link  trust  improver  (LTI),  traceability  factor  controller  (TFC),  and  a  hybrid  traceability  approach  (HTA).  LTI  uses  various  source  of  information,  such  as  temporal  information,  design  documents,  source  code  structure,  and  so  on,  to  increase  experts'  trust  over  a  link.  To  develop  TFC,  we  will  perform  a  systematic  literature  review  and  empirical  studies  to  find  out  which  factors  impact  the  traceability-process  inputs  and  document  these  factors  in  a  trust  pattern.  TFC  trust  pattern  will  help  practitioner  and  researchers  to  know  which  steps  they  can  take  to  avoid/control  these  factors  to  improve  their  trust  on  these  inputs.  In  the  HTA,  we  will  combine  different  traceability  recovery  approaches.  All  approaches  have  different  positive  and  negative  points,  we  will  combine  all  the  positive  points  of  different  approaches  to  increase  experts'  trust  over  the  HTA.  In  Trustrace,  HTA  will  implement  the  LTI  model  following  TFC  instructions  to  improve  the  expert  trust  over  recovered  link  as  well  as  precision  and  recall.
1	Exploring  tools  and  strategies  used  during  regular  expression  composition  tasks.  Regular  expressions  are  frequently  found  in  programming  projects.  Studies  have  found  that  developers  can  accurately  determine  whether  a  string  matches  a  regular  expression.  However,  we  still  do  not  know  the  challenges  associated  with  composing  regular  expressions.  We  conduct  an  exploratory  case  study  to  reveal  the  tools  and  strategies  developers  use  during  regular  expression  composition.  In  this  study,  29  students  are  tasked  with  composing  regular  expressions  that  pass  unit  tests  illustrating  the  intended  behavior.  The  tasks  are  in  Java  and  the  Eclipse  IDE  was  set  up  with  JUnit  tests.  Participants  had  one  hour  to  work  and  could  use  any  Eclipse  tools,  web  search,  or  web-based  tools  they  desired.  Screen-capture  software  recorded  all  interactions  with  browsers  and  the  IDE.  We  analyzed  the  videos  quantitatively  by  transcribing  logs  and  extracting  personas.  Our  results  show  that  participants  were  30%  successful  (28  of  94  attempts)  at  achieving  a  100%  pass  rate  on  the  unit  tests.  When  participants  used  tools  frequently,  as  in  the  case  of  the  novice  tester  and  the  knowledgeable  tester  personas,  or  when  they  guess  at  a  solution  prior  to  searching,  they  are  more  likely  to  pass  all  the  unit  tests.  We  also  found  that  compile  errors  often  arise  when  participants  searched  for  a  result  and  copy/pasted  the  regular  expression  from  another  language  into  their  Java  files.  These  results  point  to  future  research  into  making  regular  expression  composition  easier  for  programmers,  such  as  integrating  visualization  into  the  IDE  to  reduce  context  switching  or  providing  language  migration  support  when  reusing  regular  expressions  written  in  another  language  to  reduce  compile  errors.
1	Toward  refactoring  evaluation  with  code  naturalness.  Refactoring  evaluation  is  a  challenging  research  topic  because  right  and  wrong  of  refactoring  depend  on  various  aspects  of  development  context  such  as  developers'  skills,  development  cost,  deadline  and  so  on.  Many  techniques  have  been  proposed  to  evaluate  refactoring  objectively.  However,  those  techniques  do  not  consider  individual  contexts  of  software  development.  Currently,  the  authors  are  trying  to  evaluate  refactoring  automatically  and  objectively  with  considering  development  contexts.  In  this  paper,  we  propose  to  evaluate  refactoring  with  code  naturalness.  Our  technique  is  based  on  a  hypothesis:  if  a  given  refactoring  raises  the  naturalness  of  existing  code,  the  refactoring  is  beneficial.  In  this  paper,  we  also  report  our  pilot  study  on  open  source  software.
1	Codese  fast  deserialization  via  code  generation.  Many  tools  for  automated  testing,  model  checking,  and  debugging  store  and  restore  program  states  multiple  times.  Storing/restoring  a  program  state  is  commonly  done  with  serialization/deserialization.  Traditionally,  the  format  for  stored  states  is  based  on  data:  serialization  generates  the  data  that  encodes  the  state,  and  deserialization  interprets  this  data  to  restore  the  state.  We  propose  a  new  approach,  called  CoDeSe,  where  the  format  for  stored  states  is  based  on  code:  serialization  generates  code  whose  execution  restores  the  state,  and  deserialization  simply  executes  the  code.  We  implemented  CoDeSe  in  Java  and  performed  a  number  of  experiments  on  deserialization  of  states.  CoDeSe  provides  on  average  more  than  6X  speedup  over  the  highly  optimized  deserialization  from  the  standard  Java  library.  Our  new  format  also  allows  simple  parallel  deserialization  that  can  provide  additional  speedup  on  top  of  the  sequential  CoDeSe  but  only  for  larger  states.
1	Tests  from  traces  automated  unit  test  extraction  for  r.  Unit  tests  are  labor-intensive  to  write  and  maintain.  This  paper  looks  into  how  well  unit  tests  for  a  target  software  package  can  be  extracted  from  the  execution  traces  of  client  code.  Our  objective  is  to  reduce  the  effort  involved  in  creating  test  suites  while  minimizing  the  number  and  size  of  individual  tests,  and  maximizing  coverage.  To  evaluate  the  viability  of  our  approach,  we  select  a  challenging  target  for  automated  test  extraction,  namely  R,  a  programming  language  that  is  popular  for  data  science  applications.  The  challenges  presented  by  R  are  its  extreme  dynamism,  coerciveness,  and  lack  of  types.  This  combination  decrease  the  efficacy  of  traditional  test  extraction  techniques.  We  present  Genthat,  a  tool  developed  over  the  last  couple  of  years  to  non-invasively  record  execution  traces  of  R  programs  and  extract  unit  tests  from  those  traces.  We  have  carried  out  an  evaluation  on  1,545  packages  comprising  1.7M  lines  of  R  code.  The  tests  extracted  by  Genthat  improved  code  coverage  from  the  original  rather  low  value  of  267,496  lines  to  700,918  lines.  The  running  time  of  the  generated  tests  is  1.9  times  faster  than  the  code  they  came  from
1	Fluccs  using  code  and  change  metrics  to  improve  fault  localization.  Fault  localization  aims  to  support  the  debugging  activities  of  human  developers  by  highlighting  the  program  elements  that  are  suspected  to  be  responsible  for  the  observed  failure.  Spectrum  Based  Fault  Localization  (SBFL),  an  existing  localization  technique  that  only  relies  on  the  coverage  and  pass/fail  results  of  executed  test  cases,  has  been  widely  studied  but  also  criticized  for  the  lack  of  precision  and  limited  effort  reduction.  To  overcome  restrictions  of  techniques  based  purely  on  coverage,  we  extend  SBFL  with  code  and  change  metrics  that  have  been  studied  in  the  context  of  defect  prediction,  such  as  size,  age  and  code  churn.  Using  suspiciousness  values  from  existing  SBFL  formulas  and  these  source  code  metrics  as  features,  we  apply  two  learn-to-rank  techniques,  Genetic  Programming  (GP)  and  linear  rank  Support  Vector  Machines  (SVMs).  We  evaluate  our  approach  with  a  ten-fold  cross  validation  of  method  level  fault  localization,  using  210  real  world  faults  from  the  Defects4J  repository.  GP  with  additional  source  code  metrics  ranks  the  faulty  method  at  the  top  for  106  faults,  and  within  the  top  five  for  173  faults.  This  is  a  significant  improvement  over  the  state-of-the-art  SBFL  formulas,  the  best  of  which  can  rank  49  and  127  faults  at  the  top  and  within  the  top  five,  respectively.
1	Combining  model  checking  and  testing  with  an  application  to  reliability  prediction  and  distribution.  Testing  provides  a  probabilistic  assurance  of  system  correctness.  In  general,  testing  relies  on  the  assumptions  that  the  system  under  test  is  deterministic  so  that  test  cases  can  be  sampled.  However,  a  challenge  arises  when  a  system  under  test  behaves  non-deterministiclly  in  a  dynamic  operating  environment  because  it  will  be  unknown  how  to  sample  test  cases.          In  this  work,  we  propose  a  method  combining  hypothesis  testing  and  probabilistic  model  checking  so  as  to  provide  the  ``assurance"  and  quantify  the  error  bounds.  The  idea  is  to  apply  hypothesis  testing  to  deterministic  system  components  and  use  probabilistic  model  checking  techniques  to  lift  the  results  through  non-determinism.  Furthermore,  if  a  requirement  on  the  level  of  ``assurance"  is  given,  we  apply  probabilistic  model  checking  techniques  to  push  down  the  requirement  through  non-determinism  to  individual  components  so  that  they  can  be  verified  using  hypothesis  testing.  We  motivate  and  demonstrate  our  method  through  an  application  of  system  reliability  prediction  and  distribution.  Our  approach  has  been  realized  in  a  toolkit  named  RaPiD,  which  has  been  applied  to  investigate  two  real-world  systems.
1	Cooperative  types  for  controlling  thread  interference  in  java.  Multithreaded  programs  are  notoriously  prone  to  unintended  interference  between  concurrent  threads.  To  address  this  problem,  we  argue  that  yield  annotations  in  the  source  code  should  document  all  thread  interference,  and  we  present  a  type  system  for  verifying  the  absence  of  undocumented  interference  in  Java  programs.  Under  this  type  system,  well-typed  programs  behave  as  if  context  switches  occur  only  at  yield  annotations.  Thus,  well-typed  programs  can  be  understood  using  intuitive  sequential  reasoning,  except  where  yield  annotations  remind  the  programmer  to  account  for  thread  interference.          Experimental  results  show  that  yield  annotations  describe  thread  interference  more  precisely  than  prior  techniques  based  on  method-level  atomicity  specifications.  In  particular,  yield  annotations  reduce  the  number  of  interference  points  one  must  reason  about  by  an  order  of  magnitude.  The  type  system  is  also  more  precise  than  prior  methods  targeting  race  freedom.  Moreover,  yield  annotations  serve  to  highlight  all  known  concurrency  defects  in  our  benchmark  suite.
1	Trade  offs  in  continuous  integration  assurance  security  and  flexibility.  Continuous  integration  (CI)  systems  automate  the  compilation,  building,  and  testing  of  software.  Despite  CI  being  a  widely  used  activity  in  software  engineering,  we  do  not  know  what  motivates  developers  to  use  CI,  and  what  barriers  and  unmet  needs  they  face.  Without  such  knowledge,  developers  make  easily  avoidable  errors,  tool  builders  invest  in  the  wrong  direction,  and  researchers  miss  opportunities  for  improving  the  practice  of  CI.  We  present  a  qualitative  study  of  the  barriers  and  needs  developers  face  when  using  CI.  We  conduct  semi-structured  interviews  with  developers  from  different  industries  and  development  scales.  We  triangulate  our  findings  by  running  two  surveys.  We  find  that  developers  face  trade-offs  between  speed  and  certainty  (Assurance),  between  better  access  and  information  security  (Security),  and  between  more  configuration  options  and  greater  ease  of  use  (Flexi-  bility).  We  present  implications  of  these  trade-offs  for  developers,  tool  builders,  and  researchers.
1	Selection  and  presentation  practices  for  code  example  summarization.  Code  examples  are  an  important  source  for  answering  questions  about  software  libraries  and  applications.  Many  usage  contexts  for  code  examples  require  them  to  be  distilled  to  their  essence:  e.g.,  when  serving  as  cues  to  longer  documents,  or  for  reminding  developers  of  a  previously  known  idiom.  We  conducted  a  study  to  discover  how  code  can  be  summarized  and  why.  As  part  of  the  study,  we  collected  156  pairs  of  code  examples  and  their  summaries  from  16  participants,  along  with  over  26  hours  of  think-aloud  verbalizations  detailing  the  decisions  of  the  participants  during  their  summarization  activities.  Based  on  a  qualitative  analysis  of  this  data  we  elicited  a  list  of  practices  followed  by  the  participants  to  summarize  code  examples  and  propose  empirically-supported  hypotheses  justifying  the  use  of  specific  practices.  One  main  finding  was  that  none  of  the  participants  exclusively  extracted  code  verbatim  for  the  summaries,  motivating  abstractive  summarization.  The  results  provide  a  grounded  basis  for  the  development  of  code  example  summarization  and  presentation  technology.
1	A  feasibility  study  of  using  automated  program  repair  for  introductory  programming  assignments.  Despite  the  fact  an  intelligent  tutoring  system  for  programming  (ITSP)  education  has  long  attracted  interest,  its  widespread  use  has  been  hindered  by  the  difficulty  of  generating  personalized  feedback  automatically.  Meanwhile,  automated  program  repair  (APR)  is  an  emerging  new  technology  that  automatically  fixes  software  bugs,  and  it  has  been  shown  that  APR  can  fix  the  bugs  of  large  real-world  software.  In  this  paper,  we  study  the  feasibility  of  marrying  intelligent  programming  tutoring  and  APR.  We  perform  our  feasibility  study  with  four  state-of-the-art  APR  tools  (GenProg,  AE,  Angelix,  and  Prophet),  and  661  programs  written  by  the  students  taking  an  introductory  programming  course.  We  found  that  when  APR  tools  are  used  out  of  the  box,  only  about  30%  of  the  programs  in  our  dataset  are  repaired.  This  low  repair  rate  is  largely  due  to  the  student  programs  often  being  significantly  incorrect  -  in  contrast,  professional  software  for  which  APR  was  successfully  applied  typically  fails  only  a  small  portion  of  tests.  To  bridge  this  gap,  we  adopt  in  APR  a  new  repair  policy  akin  to  the  hint  generation  policy  employed  in  the  existing  ITSP.  This  new  repair  policy  admits  partial  repairs  that  address  part  of  failing  tests,  which  results  in  84%  improvement  of  repair  rate.  We  also  performed  a  user  study  with  263  novice  students  and  37  graders,  and  identified  an  understudied  problem;  while  novice  students  do  not  seem  to  know  how  to  effectively  make  use  of  generated  repairs  as  hints,  the  graders  do  seem  to  gain  benefits  from  repairs.
1	Assertion  guided  symbolic  execution  of  multithreaded  programs.  Symbolic  execution  is  a  powerful  technique  for  systematic  testing  of  sequential  and  multithreaded  programs.  However,  its  application  is  limited  by  the  high  cost  of  covering  all  feasible  intra-thread  paths  and  inter-thread  interleavings.  We  propose  a  new  assertion  guided  pruning  framework  that  identifies  executions  guaranteed  not  to  lead  to  an  error  and  removes  them  during  symbolic  execution.  By  summarizing  the  reasons  why  previously  explored  executions  cannot  reach  an  error  and  using  the  information  to  prune  redundant  executions  in  the  future,  we  can  soundly  reduce  the  search  space.  We  also  use  static  concurrent  program  slicing  and  heuristic  minimization  of  symbolic  constraints  to  further  reduce  the  computational  overhead.  We  have  implemented  our  method  in  the  Cloud9  symbolic  execution  tool  and  evaluated  it  on  a  large  set  of  multithreaded  C/C++  programs.  Our  experiments  show  that  the  new  method  can  reduce  the  overall  computational  cost  significantly.
1	Do  the  dependency  conflicts  in  my  project  matter.  Intensive  dependencies  of  a  Java  project  on  third-party  libraries  can  easily  lead  to  the  presence  of  multiple  library  or  class  versions  on  its  classpath.  When  this  happens,  JVM  will  load  one  version  and  shadows  the  others.  Dependency  conflict  (DC)  issues  occur  when  the  loaded  version  fails  to  cover  a  required  feature  (e.g.,  method)  referenced  by  the  project,  thus  causing  runtime  exceptions.  However,  the  warnings  of  duplicate  classes  or  libraries  detected  by  existing  build  tools  such  as  Maven  can  be  benign  since  not  all  instances  of  duplication  will  induce  runtime  exceptions,  and  hence  are  often  ignored  by  developers.  In  this  paper,  we  conducted  an  empirical  study  on  real-world  DC  issues  collected  from  large  open  source  projects.  We  studied  the  manifestation  and  fixing  patterns  of  DC  issues.  Based  on  our  findings,  we  designed  Decca,  an  automated  detection  tool  that  assesses  DC  issues'  severity  and  filters  out  the  benign  ones.  Our  evaluation  results  on  30  projects  show  that  Decca  achieves  a  precision  of  0.923  and  recall  of  0.766  in  detecting  high-severity  DC  issues.  Decca  also  detected  new  DC  issues  in  these  projects.  Subsequently,  20  DC  bug  reports  were  filed,  and  11  of  them  were  confirmed  by  developers.  Issues  in  6  reports  were  fixed  with  our  suggested  patches.
1	Machine  translation  testing  via  pathological  invariance.  Machine  translation  software  has  become  heavily  integrated  into  our  daily  lives  due  to  the  recent  improvement  in  the  performance  of  deep  neural  networks.  However,  machine  translation  software  has  been  shown  to  regularly  return  erroneous  translations,  which  can  lead  to  harmful  consequences  such  as  economic  loss  and  political  conflicts.  Additionally,  due  to  the  complexity  of  the  underlying  neural  models,  testing  machine  translation  systems  presents  new  challenges.  To  address  this  problem,  we  introduce  a  novel  methodology  called  PatInv.  The  main  intuition  behind  PatInv  is  that  sentences  with  different  meanings  should  not  have  the  same  translation.  Under  this  general  idea,  we  provide  two  realizations  of  PatInv  that  given  an  arbitrary  sentence,  generate  syntactically  similar  but  semantically  different  sentences  by:  (1)  replacing  one  word  in  the  sentence  using  a  masked  language  model  or  (2)  removing  one  word  or  phrase  from  the  sentence  based  on  its  constituency  structure.  We  then  test  whether  the  returned  translations  are  the  same  for  the  original  and  modified  sentences.  We  have  applied  PatInv  to  test  Google  Translate  and  Bing  Microsoft  Translator  using  200  English  sentences.  Two  language  settings  are  considered:  English-Hindi  (En-Hi)  and  English-Chinese  (En-Zh).  The  results  show  that  PatInv  can  accurately  find  308  erroneous  translations  in  Google  Translate  and  223  erroneous  translations  in  Bing  Microsoft  Translator,  most  of  which  cannot  be  found  by  the  state-of-the-art  approaches.
1	Heterogeneous  cross  company  defect  prediction  by  unified  metric  representation  and  cca  based  transfer  learning.  Cross-company  defect  prediction  (CCDP)  learns  a  prediction  model  by  using  training  data  from  one  or  multiple  projects  of  a  source  company  and  then  applies  the  model  to  the  target  company  data.  Existing  CCDP  methods  are  based  on  the  assumption  that  the  data  of  source  and  target  companies  should  have  the  same  software  metrics.  However,  for  CCDP,  the  source  and  target  company  data  is  usually  heterogeneous,  namely  the  metrics  used  and  the  size  of  metric  set  are  different  in  the  data  of  two  companies.  We  call  CCDP  in  this  scenario  as  heterogeneous  CCDP  (HCCDP)  task.  In  this  paper,  we  aim  to  provide  an  effective  solution  for  HCCDP.  We  propose  a  unified  metric  representation  (UMR)  for  the  data  of  source  and  target  companies.  The  UMR  consists  of  three  types  of  metrics,  i.e.,  the  common  metrics  of  the  source  and  target  companies,  source-company  specific  metrics  and  target-company  specific  metrics.  To  construct  UMR  for  source  company  data,  the  target-company  specific  metrics  are  set  as  zeros,  while  for  UMR  of  the  target  company  data,  the  source-company  specific  metrics  are  set  as  zeros.  Based  on  the  unified  metric  representation,  we  for  the  first  time  introduce  canonical  correlation  analysis  (CCA),  an  effective  transfer  learning  method,  into  CCDP  to  make  the  data  distributions  of  source  and  target  companies  similar.  Experiments  on  14  public  heterogeneous  datasets  from  four  companies  indicate  that:  1)  for  HCCDP  with  partially  different  metrics,  our  approach  significantly  outperforms  state-of-the-art  CCDP  methods;  2)  for  HCCDP  with  totally  different  metrics,  our  approach  obtains  comparable  prediction  performances  in  contrast  with  within-project  prediction  results.  The  proposed  approach  is  effective  for  HCCDP.
1	Automating  presentation  changes  in  dynamic  web  applications  via  collaborative  hybrid  analysis.  Web  applications  are  becoming  increasingly  popular  nowadays.  During  the  development  and  evolution  of  a  web  application,  a  typical  type  of  tasks  is  to  change  the  presentation  of  the  web  application,  such  as  correcting  display  errors,  adding  user-interface  controls,  or  changing  appearance  styles.  To  change  the  presentation  of  a  static  web  page,  developers  are  able  to  modify  the  HTML  text  of  the  web  page  using  a  graphical  web-page  editor.  However,  to  change  the  presentation  of  a  dynamic  web  application,  instead  of  using  a  graphical  web-page  editor  to  directly  modify  generated  web  pages,  developers  need  to  modify  the  code  that  generates  the  web  pages.  As  manually  performing  presentation  changes  in  dynamic  web  applications  is  tedious  and  error-prone,  we  propose  a  novel  approach  based  on  collaborative  hybrid  analysis  that  combines  static  analysis  and  dynamic  analysis  to  facilitate  developers  to  perform  presentation  changes  in  dynamic  web  applications.  Our  approach  includes  two  parts.  The  first  part  takes  as  input  the  presentation  change  to  be  performed  on  a  generated  web  page  (with  proper  runtime  information),  and  uses  dynamic  string-origin  analysis  to  locate  the  source-code  segment  that  generates  the  changed  part  of  the  web  page.  The  second  part  checks  unexpected  impact  of  directly  performing  the  change  on  the  source-code  segment,  and  asks  for  human  intervention  when  unexpected  impact  exists.  We  implemented  our  approach  for  the  PHP  language  and  carried  out  an  empirical  study  on  39  presentation-change  tasks  identified  from  600  bug  reports  of  three  real-world  dynamic  web  applications  (in  total  more  than  148  KLOC).  Among  the  39  tasks,  our  approach  is  able  to  correctly  locate  the  place  to  modify  in  each  presentation-change  task  and  correctly  perform  the  presentation  change  on  the  source  code  in  more  than  half  of  the  tasks.
1	A  fast  causal  profiler  for  task  parallel  programs.  This  paper  proposes  TASKPROF,  a  profiler  that  identifies  parallelism  bottlenecks  in  task  parallel  programs.  It  leverages  the  structure  of  a  task  parallel  execution  to  perform  fine-grained  attribution  of  work  to  various  parts  of  the  program.  TASKPROF’s  use  of  hardware  performance  counters  to  perform  fine-grained  measurements  minimizes  perturbation.  TASKPROF’s  profile  execution  runs  in  parallel  using  multi-cores.  TASKPROF’s  causal  profile  enables  users  to  estimate  improvements  in  parallelism  when  a  region  of  code  is  optimized  even  when  concrete  optimizations  are  not  yet  known.  We  have  used  TASKPROF  to  isolate  parallelism  bottlenecks  in  twenty  three  applications  that  use  the  Intel  Threading  Building  Blocks  library.  We  have  designed  parallelization  techniques  in  five  applications  to  increase  parallelism  by  an  order  of  magnitude  using  TASKPROF.  Our  user  study  indicates  that  developers  are  able  to  isolate  performance  bottlenecks  with  ease  using  TASKPROF.
1	Jstar  eclipse  an  ide  for  automated  verification  of  java  programs.  jStar  is  a  tool  for  automatically  verifying  Java  programs.  It  uses  separation  logic  to  support  abstract  reasoning  about  object  specifications.  jStar  can  verify  a  number  of  challenging  design  patterns,  including  Subject/Observer,  Visitor,  Factory  and  Pooling.  However,  to  use  jStar  one  has  to  deal  with  a  family  of  command-line  tools  that  expect  specifications  in  separate  files  and  diagnose  the  errors  by  inspecting  the  text  output  from  these  tools.      In  this  paper  we  present  a  plug-in,  called  jStar-eclipse,  allowing  programmers  to  use  jStar  from  within  Eclipse  IDE.  Our  plug-in  allows  writing  method  contracts  in  Java  source  files  in  form  of  Java  annotations.  It  automatically  translates  Java  annotations  into  jStar  specifications  and  propagates  errors  reported  by  jStar  back  to  Eclipse,  pinpointing  the  errors  to  the  locations  in  source  files.  This  way  the  plug-in  ensures  an  overall  better  user  experience  when  working  with  jStar.  Our  end  goal  is  to  make  automated  verification  based  on  separation  logic  accessible  to  a  broader  audience.
1	Ossmeter  a  software  measurement  platform  for  automatically  analysing  open  source  software  projects.  Deciding  whether  an  open  source  software  (OSS)  project  meets  the  required  standards  for  adoption  in  terms  of  quality,  maturity,  activity  of  development  and  user  support  is  not  a  straightforward  process  as  it  involves  exploring  various  sources  of  information.  Such  sources  include  OSS  source  code  repositories,  communication  channels  such  as  newsgroups,  forums,  and  mailing  lists,  as  well  as  issue  tracking  systems.  OSSMETER  is  an  extensible  and  scalable  platform  that  can  monitor  and  incrementally  analyse  a  large  number  of  OSS  projects.  The  results  of  this  analysis  can  be  used  to  assess  various  aspects  of  OSS  projects,  and  to  directly  compare  different  OSS  projects  with  each  other.
1	Automated  extraction  of  security  policies  from  natural  language  software  documents.  Access  Control  Policies  (ACP)  specify  which  principals  such  as  users  have  access  to  which  resources.  Ensuring  the  correctness  and  consistency  of  ACPs  is  crucial  to  prevent  security  vulnerabilities.  However,  in  practice,  ACPs  are  commonly  written  in  Natural  Language  (NL)  and  buried  in  large  documents  such  as  requirements  documents,  not  amenable  for  automated  techniques  to  check  for  correctness  and  consistency.  It  is  tedious  to  manually  extract  ACPs  from  these  NL  documents  and  validate  NL  functional  requirements  such  as  use  cases  against  ACPs  for  detecting  inconsistencies.  To  address  these  issues,  we  propose  an  approach,  called  Text2Policy,  to  automatically  extract  ACPs  from  NL  software  documents  and  resource-access  information  from  NL  scenario-based  functional  requirements.  We  conducted  three  evaluations  on  the  collected  ACP  sentences  from  publicly  available  sources  along  with  use  cases  from  both  open  source  and  proprietary  projects.  The  results  show  that  Text2Policy  effectively  identifies  ACP  sentences  with  the  precision  of  88.7%  and  the  recall  of  89.4%,  extracts  ACP  rules  with  the  accuracy  of  86.3%,  and  extracts  action  steps  with  the  accuracy  of  81.9%.
1	Fairway  a  way  to  build  fair  ml  software.  Machine  learning  software  is  increasingly  being  used  to  make  decisions  that  affect  people's  lives.  But  sometimes,  the  core  part  of  this  software  (the  learned  model),  behaves  in  a  biased  manner  that  gives  undue  advantages  to  a  specific  group  of  people  (where  those  groups  are  determined  by  sex,  race,  etc.).  This  "algorithmic  discrimination"  in  the  AI  software  systems  has  become  a  matter  of  serious  concern  in  the  machine  learning  and  software  engineering  community.  There  have  been  works  done  to  find  "algorithmic  bias"  or  "ethical  bias"  in  the  software  system.  Once  the  bias  is  detected  in  the  AI  software  system,  the  mitigation  of  bias  is  extremely  important.  In  this  work,  we  a)explain  how  ground-truth  bias  in  training  data  affects  machine  learning  model  fairness  and  how  to  find  that  bias  in  AI  software,b)propose  a  method  Fairway  which  combines  pre-processing  and  in-processing  approach  to  remove  ethical  bias  from  training  data  and  trained  model.  Our  results  show  that  we  can  find  bias  and  mitigate  bias  in  a  learned  model,  without  much  damaging  the  predictive  performance  of  that  model.  We  propose  that  (1)  testing  for  bias  and  (2)  bias  mitigation  should  be  a  routine  part  of  the  machine  learning  software  development  life  cycle.  Fairway  offers  much  support  for  these  two  purposes.
1	Efficient  customer  incident  triage  via  linking  with  system  incidents.  In  cloud  service  systems,  customers  will  report  the  service  issues  they  have  encountered  to  cloud  service  providers.  Despite  many  issues  can  be  handled  by  the  support  team,  sometimes  the  customer  issues  can  not  be  easily  solved,  thus  raising  customer  incidents.  Quick  troubleshooting  of  a  customer  incident  is  critical.  To  this  end,  a  customer  incident  should  be  assigned  to  its  responsible  team  accurately  in  a  timely  manner.  Our  industrial  experiences  show  that  linking  customer  incidents  with  detected  system  incidents  can  help  the  customer  incident  triage.  In  particular,  our  empirical  study  on  7  real  cloud  service  systems  shows  that  with  the  additional  information  about  the  system  incidents  (i.e.,  incident  reports  generated  by  system  monitors),  the  triage  time  of  customer  incidents  can  be  accelerated  13.1×  on  average.  Based  on  this  observation,  in  this  paper,  we  propose  LinkCM,  a  learning  based  approach  to  automatically  link  customer  incidents  to  monitor  reported  system  incidents.  LinkCM  incorporates  a  novel  learning-based  model  that  effectively  extracts  related  information  from  two  resources,  and  a  transfer  learning  strategy  is  proposed  to  help  LinkCM  achieve  better  performance  without  huge  amount  of  data.  The  experimental  results  indicate  that  LinkCM  is  able  to  achieve  accurate  link  prediction.  Furthermore,  case  studies  are  presented  to  demonstrate  how  LinkCM  can  help  the  customer  incident  triage  procedure  in  real  production  cloud  service  systems.
1	On  decomposing  a  deep  neural  network  into  modules.  Deep  learning  is  being  incorporated  in  many  modern  software  systems.  Deep  learning  approaches  train  a  deep  neural  network  (DNN)  model  using  training  examples,  and  then  use  the  DNN  model  for  prediction.  While  the  structure  of  a  DNN  model  as  layers  is  observable,  the  model  is  treated  in  its  entirety  as  a  monolithic  component.  To  change  the  logic  implemented  by  the  model,  e.g.  to  add/remove  logic  that  recognizes  inputs  belonging  to  a  certain  class,  or  to  replace  the  logic  with  an  alternative,  the  training  examples  need  to  be  changed  and  the  DNN  needs  to  be  retrained  using  the  new  set  of  examples.  We  argue  that  decomposing  a  DNN  into  DNN  modules—  akin  to  decomposing  a  monolithic  software  code  into  modules—can  bring  the  benefits  of  modularity  to  deep  learning.  In  this  work,  we  develop  a  methodology  for  decomposing  DNNs  for  multi-class  problems  into  DNN  modules.  For  four  canonical  problems,  namely  MNIST,  EMNIST,  FMNIST,  and  KMNIST,  we  demonstrate  that  such  decomposition  enables  reuse  of  DNN  modules  to  create  different  DNNs,  enables  replacement  of  one  DNN  module  in  a  DNN  with  another  without  needing  to  retrain.  The  DNN  models  formed  by  composing  DNN  modules  are  at  least  as  good  as  traditional  monolithic  DNNs  in  terms  of  test  accuracy  for  our  problems.
1	Hidden  truths  in  dead  software  paths.  Approaches  and  techniques  for  statically  finding  a  multitude  of  issues  in  source  code  have  been  developed  in  the  past.  A  core  property  of  these  approaches  is  that  they  are  usually  targeted  towards  finding  only  a  very  specific  kind  of  issue  and  that  the  effort  to  develop  such  an  analysis  is  significant.  This  strictly  limits  the  number  of  kinds  of  issues  that  can  be  detected.  In  this  paper,  we  discuss  a  generic  approach  based  on  the  detection  of  infeasible  paths  in  code  that  can  discover  a  wide  range  of  code  smells  ranging  from  useless  code  that  hinders  comprehension  to  real  bugs.  Code  issues  are  identified  by  calculating  the  difference  between  the  control-flow  graph  that  contains  all  technically  possible  edges  and  the  corresponding  graph  recorded  while  performing  a  more  precise  analysis  using  abstract  interpretation.  We  have  evaluated  the  approach  using  the  Java  Development  Kit  as  well  as  the  Qualitas  Corpus  (a  curated  collection  of  over  100  Java  Applications)  and  were  able  to  find  thousands  of  issues  across  a  wide  range  of  categories.
1	Automatically  deriving  pointer  reference  expressions  from  binary  code  for  memory  dump  analysis.  Given  a  crash  dump  or  a  kernel  memory  snapshot,  it  is  often  desirable  to  have  a  capability  that  can  traverse  its  pointers  to  locate  the  root  cause  of  the  crash,  or  check  their  integrity  to  detect  the  control  flow  hijacks.  To  achieve  this,  one  key  challenge  lies  in  how  to  locate  where  the  pointers  are.  While  locating  a  pointer  usually  requires  the  data  structure  knowledge  of  the  corresponding  program,  an  important  advance  made  by  this  work  is  that  we  show  a  technique  of  extracting  address-independent  data  reference  expressions  for  pointers  through  dynamic  binary  analysis.  This  novel  pointer  reference  expression  encodes  how  a  pointer  is  accessed  through  the  combination  of  a  base  address  (usually  a  global  variable)  with  certain  offset  and  further  pointer  dereferences.  We  have  applied  our  techniques  to  OS  kernels,  and  our  experimental  results  with  a  number  of  real  world  kernel  malware  show  that  we  can  correctly  identify  the  hijacked  kernel  function  pointers  by  locating  them  using  the  extracted  pointer  reference  expressions  when  only  given  a  memory  snapshot.
1	Detecting  numerical  bugs  in  neural  network  architectures.  Detecting  bugs  in  deep  learning  software  at  the  architecture  level  provides  additional  benefits  that  detecting  bugs  at  the  model  level  does  not  provide.  This  paper  makes  the  first  attempt  to  conduct  static  analysis  for  detecting  numerical  bugs  at  the  architecture  level.  We  propose  a  static  analysis  approach  for  detecting  numerical  bugs  in  neural  architectures  based  on  abstract  interpretation.  Our  approach  mainly  comprises  two  kinds  of  abstraction  techniques,  i.e.,  one  for  tensors  and  one  for  numerical  values.  Moreover,  to  scale  up  while  maintaining  adequate  detection  precision,  we  propose  two  abstraction  techniques:  tensor  partitioning  and  (elementwise)  affine  relation  analysis  to  abstract  tensors  and  numerical  values,  respectively.  We  realize  the  combination  scheme  of  tensor  partitioning  and  affine  relation  analysis  (together  with  interval  analysis)  as  DEBAR,  and  evaluate  it  on  two  datasets:  neural  architectures  with  known  bugs  (collected  from  existing  studies)  and  real-world  neural  architectures.  The  evaluation  results  show  that  DEBAR  outperforms  other  tensor  and  numerical  abstraction  techniques  on  accuracy  without  losing  scalability.  DEBAR  successfully  detects  all  known  numerical  bugs  with  no  false  positives  within  1.7–2.3  seconds  per  architecture.  On  the  real-world  architectures,  DEBAR  reports  529  warnings  within  2.6–135.4  seconds  per  architecture,  where  299  warnings  are  true  positives.
1	Semi  automatic  repair  of  over  constrained  models  for  combinatorial  robustness  testing.  Combinatorial  robustness  testing  is  an  approach  to  generate  separate  test  inputs  for  positive  and  negative  test  scenarios.  The  test  model  is  enriched  with  semantic  information  to  distinguish  valid  from  invalid  values  and  value  combinations.  Unfortunately,  it  is  easy  to  create  over-constrained  models  and  invalid  values  or  invalid  value  combinations  do  not  appear  in  the  final  test  suite.  In  this  paper,  we  extend  previous  work  on  manual  repair  and  develop  a  technique  to  semi-automatically  repair  over-constrained  models.  The  technique  is  evaluated  with  benchmark  models  and  the  results  indicate  a  small  computational  overhead.
1	An  experimentation  platform  for  the  automatic  parallelization  of  r  programs.  We  present  our  ALCHEMY  platform  that  supports  the  automatic  parallelization  of  R  programs  during  execution.  Parallelization  occurs  fully  transparent  to  the  user.  Different  parallelization  techniques  can  be  implemented  as  modules,  linked  into  the  platform,  and  combined  with  each  other.  The  parallelization  analysis  modules  and  code  transformation  modules  use  a  new  intermediate  representation  for  sequential  and  parallelized  R  code.  Successfully  parallelized  parts  of  the  R  program  are  executed  on  a  multicore  processor,  the  results  and  the  remaining  sequential  parts  are  fed  back  into  the  standard  R  interpreter  and  evaluated  to  completion.  This  way,  an  R  user  can  benefit  from  multiprocessor  performance  without  writing  a  single  line  of  parallel  code.  At  this  stage  of  the  research  project,  the  main  goal  is  to  enable  ample  experimentation  with  different  approaches  to  the  automatic  parallelization  of  scripting  languages  such  as  R.
1	Towards  a  hybrid  framework  for  detecting  input  manipulation  vulnerabilities.  Input  manipulation  vulnerabilities  such  as  SQL  Injection,  Cross-site  scripting,  Buffer  Overflow  vulnerabilities  are  highly  prevalent  and  pose  critical  security  risks.  As  a  result,  many  methods  have  been  proposed  to  apply  static  analysis,  dynamic  analysis  or  a  combination  of  them,  to  detect  such  security  vulnerabilities.  Most  of  the  existing  methods  classify  vulnerabilities  into  safe  and  unsafe.  They  have  both  false-positive  and  false-negative  cases.  In  general,  security  vulnerability  can  be  classified  into  three  cases:  (1)  provable  safe,  (2)  provable  unsafe,  (3)  unsure.  In  this  paper,  we  propose  a  hybrid  framework-Detecting  Input  Manipulation  Vulnerabilities  (DIMV),  to  verify  the  adequacy  of  security  vulnerability  defenses  for  input  manipulation  vulnerabilities  by  integrating  formal  verification  with  vulnerability  prediction  in  a  seamless  way.  The  verification  part  takes  into  account  sink  predicates  and  effect  of  domain  and  custom  specifications  for  detecting  input  manipulation  vulnerabilities.  Proving  from  specification  is  used  as  far  as  possible.  Cases  that  cannot  be  proved  are  then  predicted  from  the  signatures  mined.  Our  evaluation  shows  the  practicality  of  the  proposed  framework.
1	Empirical  analysis  of  fault  proneness  in  methods  by  focusing  on  their  comment  lines.  This  paper  focuses  on  comments  described  in  Java  programs,  and  conducts  an  empirical  analysis  about  relationships  between  comments  and  fault-proneness  in  the  programs.  The  types  of  comments  analyzed  in  this  paper  are  comments  described  inside  a  method  body  (inner  comments),  and  comments  followed  by  a  method  declaration  (documentation  comments).  Although  both  of  them  play  important  roles  in  the  program  comprehension,  they  seem  to  be  described  in  different  purposes,  The  inner  comments  are  often  added  to  present  tips  about  code  fragments,  while  the  documentation  comments  usually  work  as  a  programmer's  manual.  In  the  field  of  code  refactoring,  well-written  inner  comments  are  said  to  be  related  to  "code  smell"  since  they  may  cover  a  lack  of  readability  in  a  complicated  code  fragment.  This  paper  analyzes  the  associations  of  comments  with  the  code  quality  from  the  aspect  of  fault-proneness,  with  using  four  popular  open  source  products.  The  empirical  results  show  that  a  method  having  inner  comments  tends  to  be  1.8  --  3.0  times  likely  to  be  faulty.  The  key  contribution  of  this  work  is  to  reveal  the  usefulness  of  inner  comments  to  point  at  faulty  methods.
1	Visualization  of  defect  inflow  and  resolution  cycles  before  during  and  after  transfer.  The  link  between  maintenance  and  product  quality,  as  well  as  the  high  cost  of  software  maintenance,  highlights  the  importance  of  efficient  maintenance  processes.  Sustaining  maintenance  work  efficiency  in  a  global  software  development  setting  that  involves  a  transfer  is  a  challenging  endeavor.  Studies  report  on  the  negative  effect  of  transfers  on  efficiency.  However,  empirical  evidence  on  the  magnitude  of  the  change  in  efficiency  is  scarce.  In  this  study  we  used  a  lean  indicator  to  visualize  variances  in  defect  resolution  cycles  for  two  large  products  during  evolution,  before,  during  and  after  a  transfer.  Focus  group  meetings  were  also  held  for  each  product.  Study  results  show  that  during  and  immediately  after  the  transfer  the  defect  inflow  is  higher,  bottlenecks  are  more  visible,  and  defect  resolution  cycles  are  longer,  as  compared  to  before  the  transfer.  Furthermore  we  highlight  the  factors  that  influenced  the  change  in  defect  resolution  cycles  before,  during,  and  after  the  transfer.
1	Towards  a  better  understanding  of  testing  if  conditionals.  Fault  based  testing  is  a  technique  for  choosing  test  cases  to  reveal  certain  classes  of  faults.  Due  to  limited  resources  and  time,  testing  professionals  use  their  personal  experience  to  (1)  "guess"  which  fault  classes  are  most  likely  to  be  present  and,  then,  (2)  select  appropriate  testing  methods  to  reveal  such  fault  classes.  The  quality  of  the  software  depends  on  whether  they  can  make  a  good  "guess"  about  the  type  of  faults  present  and  then  choose  the  right  testing  methods  to  reveal  those  faults.  However,  there  is  little  empirical  evidence  available  in  the  open  literature  to  support  these  intuitions.  For  example,  there  is  no  empirical  evidence  about  which  types  of  faults  are  most  commonly  made  by  software  developers.  By  examining  the  source  code  changes  when  faults  were  fixed  in  seven  open  source  software  artifacts,  we  propose  to  classify  bug  fix  patterns  into  fault  classes,  and  recorded  the  relative  frequencies  of  those  fault  classes.  This  paper  reports  our  findings  related  to  "ifconditional"  fixes.
1	Preffinder  getting  the  right  preference  in  configurable  software  systems.  Highly  configurable  software,  such  as  web  browsers,databases  or  office  applications,  have  a  large  number  of  preferences  that  the  user  can  customize,  but  documentation  of  them  may  be  scarce  or  distributed.  A  user,  tester  or  service  technician  may  have  to  search  through  hundreds  or  thousands  of  choices  in  multiple  documents  when  trying  to  identify  which  preference  will  modify  a  particular  system  behavior.  In  this  paper  we  present  PrefFinder,  a  natural  language  framework  that  finds  (and  changes)  user  preferences.  It  is  tied  into  an  application's  preference  system  and  static  documentation.  We  have  instantiated  PrefFinder  as  a  plugin  on  two  open  source  applications,  and  as  a  stand-alone  GUI  for  an  industrial  application.  PrefFinder  finds  thecorrect  answer  between  76-96%  of  the  time  on  more  than  175  queries.  When  compared  to  asking  questions  on  a  help  forum  or  through  the  company's  service  center,  we  can  potentially  save  days  or  even  weeks  of  time.
1	Proving  mcapi  executions  are  correct  using  smt.  Asynchronous  message  passing  is  an  important  paradigm  in  writing  applications  for  embedded  heterogeneous  multicore  systems.  The  Multicore  Association  (MCA),  an  industry  consortium  promoting  multicore  technology,  is  working  to  standardize  message  passing  into  a  single  API,  MCAPI,  for  bare  metal  implementation  and  portability  across  platforms.  Correctness  in  such  an  API  is  difficult  to  reason  about  manually,  and  testing  against  reference  solutions  is  equally  difficult  as  reference  solutions  implement  an  unknown  set  of  allowed  behaviors,  and  programmers  have  no  way  to  directly  control  API  internals  to  expose  or  reproduce  errors.  This  paper  provides  a  way  to  encode  an  MCAPI  execution  as  a  Satisfiability  Modulo  Theories  (SMT)  problem,  which  if  satisfiable,  yields  a  feasible  execution  schedule  on  the  same  trace,  such  that  it  resolves  non-determinism  in  the  MCAPI  runtime  in  a  way  that  it  now  fails  user  provided  assertions.  The  paper  proves  the  problem  is  NP-complete.  The  encoding  is  useful  for  test,  debug,  and  verification  of  MCAPI  program  execution.  The  novelty  in  the  encoding  is  the  direct  use  of  match  pairs  (potential  send  and  receive  couplings).  Match-pair  encoding  for  MCAPI  executions,  when  compared  to  other  encoding  strategies,  is  simpler  to  reason  about,  results  in  significantly  fewer  terms  in  the  SMT  problem,  and  captures  feasible  behaviors  that  are  ignored  in  previously  published  techniques.  Further,  to  our  knowledge,  this  is  the  first  SMT  encoding  that  is  able  to  run  in  infinite-buffer  semantics,  meaning  the  runtime  has  unlimited  internal  buffering  as  opposed  to  no  internal  buffering.  Results  demonstrate  that  the  SMT  encoding,  restricted  to  zero-buffer  semantics,  uses  fewer  clauses  when  compared  to  another  zero-buffer  technique,  and  it  runs  faster  and  uses  less  memory.  As  a  result  the  encoding  scales  well  for  programs  with  high  levels  of  non-determinism  in  how  sends  and  receives  may  potentially  match.
1	On  essential  configuration  complexity  measuring  interactions  in  highly  configurable  systems.  Quality  assurance  for  highly-configurable  systems  is  challenging  due  to  the  exponentially  growing  configuration  space.  Interactions  among  multiple  options  can  lead  to  surprising  behaviors,  bugs,  and  security  vulnerabilities.  Analyzing  all  configurations  systematically  might  be  possible  though  if  most  options  do  not  interact  or  interactions  follow  specific  patterns  that  can  be  exploited  by  analysis  tools.  To  better  understand  interactions  in  practice,  we  analyze  program  traces  to  characterize  and  identify  where  interactions  occur  on  control  flow  and  data.  To  this  end,  we  developed  a  dynamic  analysis  for  Java  based  on  variability-aware  execution  and  monitor  executions  of  multiple  small  to  medium-sized  programs.  We  find  that  the  essential  configuration  complexity  of  these  programs  is  indeed  much  lower  than  the  combinatorial  explosion  of  the  configuration  space  indicates.  However,  we  also  discover  that  the  interaction  characteristics  that  allow  scalable  and  complete  analyses  are  more  nuanced  than  what  is  exploited  by  existing  state-of-the-art  quality  assurance  strategies.
1	Sauml  a  tool  for  symbolic  analysis  of  uml  rt  models.  Model  Driven  Development  (MDD)  is  an  approach  to  software  development  built  around  the  notion  of  models.  One  of  its  implementation  is  the  IBM  RSA  RTE,  which  uses  the  UML-RT  modeling  language.  In  this  paper  we  introduce  the  tool  SAUML  (Symbolic  Analysis  of  UML-RT  Models)  that  enhances  the  current  practice  of  MDD  with  the  analyses  of  UML-RT  models.  The  implemented  technique  is  based  on  symbolic  execution,  features  modularity  and  supports  the  reuse  of  analysis  results.  The  paper  gives  an  overview  of  this  technique  and  its  implementation  in  the  IBM  RSA  RTE  tool.
1	Empirically  assessing  opportunities  for  prefetching  and  caching  in  mobile  apps.  Network  latency  in  mobile  software  has  a  large  impact  on  user  experience,  with  potentially  severe  economic  consequences.  Prefetching  and  caching  have  been  shown  effective  in  reducing  the  latencies  in  browser-based  systems.  However,  those  techniques  cannot  be  directly  applied  to  the  emerging  domain  of  mobile  apps  because  of  the  differences  in  network  interactions.  Moreover,  there  is  a  lack  of  research  on  prefetching  and  caching  techniques  that  may  be  suitable  for  the  mobile  app  domain,  and  it  is  not  clear  whether  such  techniques  can  be  effective  or  whether  they  are  even  feasible.  This  paper  takes  the  first  step  toward  answering  these  questions  by  conducting  a  comprehensive  study  to  understand  the  characteristics  of  HTTP  requests  in  over  1,000  popular  Android  apps.  Our  work  focuses  on  the  prefetchability  of  requests  using  static  program  analysis  techniques  and  cacheability  of  resulting  responses.  We  find  that  there  is  a  substantial  opportunity  to  leverage  prefetching  and  caching  in  mobile  apps,  but  that  suitable  techniques  must  take  into  account  the  nature  of  apps'  network  interactions  and  idiosyncrasies  such  as  untrustworthy  HTTP  header  information.  Our  observations  provide  guidelines  for  developers  to  utilize  prefetching  and  caching  schemes  in  app  development,  and  motivate  future  research  in  this  area.
1	Cloud  software  development  platforms  a  comparative  overview.  Doing  things  “in  the  cloud”  has  become  ubiquitous,  and  the  cloud  has  become  a  rich  environment  for  the  use  of  technology  anywhere  and  anytime  to  solve  problems,  connect  people,  and  improve  lives.  Software  engineering  paradigms  have  been  shifting  during  the  last  decade  from  “Software-as-a-Product  (SaaP)”  to  “Software-as-a-Service  (SaaS)”  provided  “in  the  cloud”.  The  SaaS  software  paradigm  comes  with  new  capabilities  and  technical  challenges  for  cloud  software  development,  as  compared  to  mobile  and  stand-alone  software  development.  Distributed  multitiered  architecting,  design,  and  programming  for  cloud  software  development  require  new  strategies  specifically  motivated  by  the  SaaS  paradigm.  This  paper  aims  to  compare  and  contrast  cloud  software  development  platforms  and  frameworks,  namely,  Java  platform,  Enterprise  Edition  (Java  EE);  Microsoft  platform,  .NET  framework;  Ruby  on  Rails  framework;  Zend  framework;  Node.js  framework;  and  Django  (Python)  framework.  A  comparative  overview  is  presented  to  help  cloud  software  engineers  select  an  appropriate  platform  /  framework  to  solve  a  complex  problem.
1	Brain  tumor  segmention  based  on  dilated  convolution  refine  networks.  A  brain  tumor  is  a  growth  of  abnormal  cells  in  the  tissues  of  the  brain,  which  is  difficult  for  treatment  and  severely  affects  patients'  cognitive  ability.  Recent  year  magnetic  resonance  imaging  (MRI)  has  been  widely  used  imaging  technique  to  assess  brain  tumors.  However  manual  segmentation  and  artificial  extracting  features  block  MRI's  practice  when  facing  with  the  huge  amount  of  data  produced  by  MRI.  An  efficient  and  automatic  image  segmentation  of  brain  tumor  is  still  needed.  In  this  paper,  a  novel  automatic  segmentation  framework  of  brain  tumors,  which  have  5  parts  and  resnet-50  use  as  a  backbone,  is  proposed  based  on  convolutional  neural  network.  A  dilated  convolution  refine  (DCR)  structure  is  introduced  to  extract  the  local  features  and  global  features.  After  investigating  different  parameters  of  our  framework,  it  is  proved  that  DCR  is  an  efficient  and  robust  method  in  Brain  Tumor  Segmentation.  The  experiments  are  evaluated  by  Multimodal  Brain  Tumor  Image  Segmentation  (BRATS  2015)  dataset.  The  results  show  that  our  framework  in  complete  tumor  segmentation  achieved  excellent  results  with  a  DEC  score  of  0.87  and  a  PPV  score  of  0.92.  (GitHub:  https://github.com/wei-lab/DCR)
1	Strsolve  solving  string  constraints  lazily.  Reasoning  about  strings  is  becoming  a  key  step  at  the  heart  of  many  program  analysis  and  testing  frameworks.  Stand-alone  string  constraint  solving  tools,  called  decision  procedures,  have  been  the  focus  of  recent  research  in  this  area.  The  aim  of  this  work  is  to  provide  algorithms  and  implementations  that  can  be  used  by  a  variety  of  program  analyses  through  a  well-defined  interface.  This  separation  enables  independent  improvement  of  string  constraint  solving  algorithms  and  reduces  client  effort.
1	Safe  asynchronous  multicore  memory  operations.  Asynchronous  memory  operations  provide  a  means  for  coping  with  the  memory  wall  problem  in  multicore  processors,  and  are  available  in  many  platforms  and  languages,  e.g.,  the  Cell  Broadband  Engine,  CUDA  and  OpenCL.  Reasoning  about  the  correct  usage  of  such  operations  involves  complex  analysis  of  memory  accesses  to  check  for  races.  We  present  a  method  and  tool  for  proving  memory-safety  and  race-freedom  of  multicore  programs  that  use  asynchronous  memory  operations.  Our  approach  uses  separation  logic  with  permissions,  and  our  tool  automates  this  method,  targeting  a  C-like  core  language.  We  describe  our  solutions  to  several  challenges  that  arose  in  the  course  of  this  research.  These  include:  syntactic  reasoning  about  permissions  and  arrays,  integration  of  numerical  abstract  domains,  and  utilization  of  an  SMT  solver.  We  demonstrate  the  feasibility  of  our  approach  experimentally  by  checking  absence  of  DMA  races  on  a  set  of  programs  drawn  from  the  IBM  Cell  SDK.
1	Cross  contract  static  analysis  for  detecting  practical  reentrancy  vulnerabilities  in  smart  contracts.  Reentrancy  bugs,  one  of  the  most  severe  vulnerabilities  in  smart  contracts,  have  caused  huge  financial  loss  in  recent  years.  Researchers  have  proposed  many  approaches  to  detecting  them.  However,  empirical  studies  have  shown  that  these  approaches  suffer  from  undesirable  false  positives  and  false  negatives,  when  the  code  under  detection  involves  the  interaction  between  multiple  smart  contracts.  In  this  paper,  we  propose  an  accurate  and  efficient  cross-contract  reentrancy  detection  approach  in  practice.  Rather  than  design  rule-of-thumb  heuristics,  we  conduct  a  large  empirical  study  of  11714  real-world  contracts  from  Etherscan  against  three  well-known  general-purpose  security  tools  for  reentrancy  detection.  We  manually  summarized  the  reentrancy  scenarios  where  the  state-of-the-art  approaches  cannot  address.  Based  on  the  empirical  evidence,  we  present  Clairvoyance,  a  cross-function  and  cross-contract  static  analysis  to  detect  reentrancy  vulnerabilities  in  real  world  with  significantly  higher  accuracy.  To  reduce  false  negatives,  we  enable,  for  the  first  time,  a  cross-contract  call  chain  analysis  by  tracking  possibly  tainted  paths.  To  reduce  false  positives,  we  systematically  summarized  five  major  path  protective  techniques  (PPTs)  to  support  fast  yet  precise  path  feasibility  checking.  We  implemented  our  approach  and  compared  Clairvoyance  with  five  state-of-the-art  tools  on  17770  real-worlds  contracts.  The  results  show  that  Clairvoyance  yields  the  best  detection  accuracy  among  all  the  five  tools  and  also  finds  101  unknown  reentrancy  vulnerabilities.
1	Randomizing  regression  tests  using  game  theory.  As  software  evolves,  the  number  of  test-cases  in  the  regression  test  suites  continues  to  increase,  requiring  testers  to  prioritize  their  execution.  Usually  only  a  subset  of  the  test  cases  is  executed  due  to  limited  testing  resources.  This  subset  is  often  known  to  the  developers  who  may  try  to  "game"  the  system  by  committing  insufficiently  tested  code  for  parts  of  the  software  that  will  not  be  tested.  In  this  new  ideas  paper,  we  propose  a  novel  approach  for  randomizing  regression  test  scheduling,  based  on  Stackelberg  games  for  deployment  of  scarce  resources.  We  apply  this  approach  to  randomizing  test  cases  in  such  a  way  as  to  maximize  the  testers'  expected  payoff  when  executing  the  test  cases.  Our  approach  accounts  for  resource  limitations  (e.g.,  number  of  testers)  and  provides  a  probabilistic  distribution  for  scheduling  test  cases.  We  provide  an  example  application  of  our  approach  showcasing  the  idea  of  using  Stackelberg  games  for  randomized  regression  test  scheduling.
1	Fib  squeezing  loop  invariants  by  interpolation  between  forward  backward  predicate  transformers.  Loop  invariant  generation  is  a  fundamental  problem  in  program  analysis  and  verification.  In  this  work,  we  propose  a  new  approach  to  automatically  constructing  inductive  loop  invariants.  The  key  idea  is  to  aggressively  squeeze  an  inductive  invariant  based  on  Craig  interpolants  between  forward  and  backward  reachability  analysis.  We  have  evaluated  our  approach  by  a  set  of  loop  benchmarks,  and  experimental  results  show  that  our  approach  is  promising.
1	Current  challenges  in  automatic  software  repair.  The  abundance  of  defects  in  existing  software  systems  is  unsustainable.  Addressing  them  is  a  dominant  cost  of  software  maintenance,  which  in  turn  dominates  the  life  cycle  cost  of  a  system.  Recent  research  has  made  significant  progress  on  the  problem  of  automatic  program  repair,  using  techniques  such  as  evolutionary  computation,  instrumentation  and  run-time  monitoring,  and  sound  synthesis  with  respect  to  a  specification.  This  article  serves  three  purposes.  First,  we  review  current  work  on  evolutionary  computation  approaches,  focusing  on  GenProg,  which  uses  genetic  programming  to  evolve  a  patch  to  a  particular  bug.  We  summarize  algorithmic  improvements  and  recent  experimental  results.  Second,  we  review  related  work  in  the  rapidly  growing  subfield  of  automatic  program  repair.  Finally,  we  outline  important  open  research  challenges  that  we  believe  should  guide  future  research  in  the  area.
1	Eclipse  api  usage  the  good  and  the  bad.  Today,  when  constructing  software  systems,  many  developers  build  their  systems  on  top  of  frameworks.  Eclipse  is  such  a  framework  that  has  been  in  existence  for  over  a  decade.  Like  many  other  evolving  software  systems,  the  Eclipse  platform  has  both  stable  and  supported  interfaces  ("good")  and  unstable,  discouraged  and  unsupported  interfaces  ("bad").  In  this  study,  we  investigate  Eclipse  interface  usage  by  Eclipse  third-party  plug-ins  (ETPs)  based  on  whether  they  use  bad  interfaces  or  not.  The  investigations,  based  on  empirical  analysis  present  the  following  observations.  First,  we  discovered  that  44  %  of  the  512  analyzed  Eclipse  third-party  plug-ins  depend  on  "bad"  interfaces  and  that  developers  continue  to  use  "bad"  interfaces.  Second,  we  have  observed  that  plug-ins  that  use  or  extend  at  least  one  "bad"  interface  are  comparatively  larger  and  use  more  functionality  from  Eclipse  than  those  that  use  only  "good"  interfaces.  Third,  the  findings  show  that  the  ETPs  use  a  diverse  set  of  "bad"  interfaces.  Fourth,  we  observed  that  the  reason  why  the  bad  interfaces  are  being  eliminated  from  the  ETPs'  source  code  is,  because  (ETP  developers  believe)  these  non-APIs  will  cause  incompatibilities  when  a  version  of  the  ETP  is  ported  to  new  Eclipse  SDK  release.  Finally,  we  observed  that  when  developers  eliminate  problematic  "bad"  interfaces,  they  either  re-implement  the  same  functionality  in  their  own  API,  find  equivalent  SDK  good  interfaces,  or  completely  delete  the  entities  in  the  ETPs'  source  code  that  use  the  functionality  from  the  "bad"  interfaces.
1	Empirical  analysis  of  factors  affecting  confirmation  bias  levels  of  software  engineers.  Confirmation  bias  is  defined  as  the  tendency  of  people  to  seek  evidence  that  verifies  a  hypothesis  rather  than  seeking  evidence  to  falsify  it.  Due  to  the  confirmation  bias,  defects  may  be  introduced  in  a  software  product  during  requirements  analysis,  design,  implementation  and/or  testing  phases.  For  instance,  testers  may  exhibit  confirmatory  behavior  in  the  form  of  a  tendency  to  make  the  code  run  rather  than  employing  a  strategic  approach  to  make  it  fail.  As  a  result,  most  of  the  defects  that  have  been  introduced  in  the  earlier  phases  of  software  development  may  be  overlooked  leading  to  an  increase  in  software  defect  density.  In  this  paper,  we  quantify  confirmation  bias  levels  in  terms  of  a  single  derived  metric.  However,  the  main  focus  of  this  paper  is  the  analysis  of  factors  affecting  confirmation  bias  levels  of  software  engineers.  Identification  of  these  factors  can  guide  project  managers  to  circumvent  negative  effects  of  confirmation  bias,  as  well  as  providing  guidance  for  the  recruitment  and  effective  allocation  of  software  engineers.  In  this  empirical  study,  we  observed  low  confirmation  bias  levels  among  participants  with  logical  reasoning  and  hypothesis  testing  skills.
1	Apogen  automatic  page  object  generator  for  web  testing.  Modern  web  applications  are  characterized  by  ultra-rapid  development  cycles,  and  web  testers  tend  to  pay  scant  attention  to  the  quality  of  their  automated  end-to-end  test  suites.  Indeed,  these  quickly  become  hard  to  maintain,  as  the  application  under  test  evolves.  As  a  result,  end-to-end  automated  test  suites  are  abandoned,  despite  their  great  potential  for  catching  regressions.  The  use  of  the  Page  Object  pattern  has  proven  to  be  very  effective  in  end-to-end  web  testing.  Page  objects  are  facade  classes  abstracting  the  internals  of  web  pages  into  high-level  business  functions  that  can  be  invoked  by  the  test  cases.  By  decoupling  test  code  from  web  page  details,  web  test  cases  are  more  readable  and  maintainable.  However,  the  manual  development  of  such  page  objects  requires  substantial  coding  effort,  which  is  paid  off  only  later,  during  software  evolution.  In  this  paper,  we  describe  a  novel  approach  for  the  automatic  generation  of  page  objects  for  web  applications.  Our  approach  is  implemented  in  the  tool  Apogen,  which  automatically  derives  a  testing  model  by  reverse  engineering  the  target  web  application.  It  combines  clustering  and  static  analysis  to  identify  meaningful  page  abstractions  that  are  automatically  turned  into  Java  page  objects  for  Selenium  WebDriver.  Our  evaluation  on  an  open-source  web  application  shows  that  our  approach  is  highly  promising:  Automatically  generated  page  object  methods  cover  most  of  the  application  functionalities  and  result  in  readable  and  meaningful  code,  which  can  be  very  useful  to  support  the  creation  of  more  maintainable  web  test  suites.
1	Generating  model  transformation  rules  from  examples  using  an  evolutionary  algorithm.  We  propose  an  evolutionary  approach  to  automatically  generate  model  transformation  rules  from  a  set  of  examples.  To  this  end,  genetic  programming  is  adapted  to  the  problem  of  model  transformation  in  the  presence  of  complex  input/output  relationships  (i.e.,  models  conforming  to  meta-models)  by  generating  declarative  programs  (i.e.,  transformation  rules  in  this  case).  Our  approach  does  not  rely  on  prior  transformation  traces  for  the  model-example  pairs,  and  directly  generates  executable,  many-to-many  rules  with  complex  conditions.  The  applicability  of  the  approach  is  illustrated  with  the  well-known  problem  of  transforming  UML  class  diagrams  into  relational  schemas,  using  examples  collected  from  the  literature.
1	Exploring  caching  for  efficient  collection  operations.  Many  useful  programs  operate  on  collection  types.  Extensive  libraries  are  available  in  many  programming  languages,  such  as  the  C++  Standard  Template  Library,  which  make  programming  with  collections  convenient.  Extending  programming  languages  to  provide  collection  queries  as  first  class  constructs  in  the  language  would  not  only  allow  programmers  to  write  queries  explicitly  in  their  programs  but  it  would  also  allow  compilers  to  leverage  the  wealth  of  experience  available  from  the  database  domain  to  optimize  such  queries.  This  paper  describes  an  approach  to  reducing  the  run  time  of  programs  involving  explicit  collection  queries  by  leveraging  a  cache  to  store  previously  computed  results.  We  propose  caching  the  results  of  join  (sub)queries  which  allows  queries  that  miss  the  cache  entirely  to  be  answered  partially  from  the  cache  thereby  improving  the  query  execution  time.  We  also  describe  an  effective  cache  policy  to  determine  which  join  (sub)queries  to  cache.  The  cache  is  maintained  incrementally,  when  the  underlying  collections  change,  and  use  of  the  cache  space  is  optimized  by  a  cache  replacement  policy.
1	Applying  the  mahalanobis  taguchi  strategy  for  software  defect  diagnosis.  The  Mahalanobis-Taguchi  (MT)  strategy  combines  mathematical  and  statistical  concepts  like  Mahalanobis  distance,  Gram-Schmidt  orthogonalization  and  experimental  designs  to  support  diagnosis  and  decision-making  based  on  multivariate  data.  The  primary  purpose  is  to  develop  a  scale  to  measure  the  degree  of  abnormality  of  cases,  compared  to  "normal"  or  "healthy"  cases,  i.e.  a  continuous  scale  from  a  set  of  binary  classified  cases.  An  optimal  subset  of  variables  for  measuring  abnormality  is  then  selected  and  rules  for  future  diagnosis  are  defined  based  on  them  and  the  measurement  scale.  This  maps  well  to  problems  in  software  defect  prediction  based  on  a  multivariate  set  of  software  metrics  and  attributes.  In  this  paper,  the  MT  strategy  combined  with  a  cluster  analysis  technique  for  determining  the  most  appropriate  training  set,  is  described  and  applied  to  well-known  datasets  in  order  to  evaluate  the  fault-proneness  of  software  modules.  The  measurement  scale  resulting  from  the  MT  strategy  is  evaluated  using  ROC  curves  and  shows  that  it  is  a  promising  technique  for  software  defect  diagnosis.  It  compares  favorably  to  previously  evaluated  methods  on  a  number  of  publically  available  data  sets.  The  special  characteristic  of  the  MT  strategy  that  it  quantifies  the  level  of  abnormality  can  also  stimulate  and  inform  discussions  with  engineers  and  managers  in  different  defect  prediction  situations.
1	Improving  missing  issue  commit  link  recovery  using  positive  and  unlabeled  data.  Links  between  issue  reports  and  corresponding  fix  commits  are  widely  used  in  software  maintenance.  The  quality  of  links  directly  affects  maintenance  costs.  Currently,  such  links  are  mainly  maintained  by  error-prone  manual  efforts,  which  may  result  in  missing  links.  To  tackle  this  problem,  automatic  link  recovery  approaches  have  been  proposed  by  building  traditional  classifiers  with  positive  and  negative  links.  However,  these  traditional  classifiers  may  not  perform  well  due  to  the  inherent  characteristics  of  missing  links.  Positive  links,  which  can  be  used  to  build  link  recovery  model,  are  quite  limited  as  the  result  of  missing  links.  Since  the  construction  of  negative  links  depends  on  the  number  of  positive  links  in  many  existing  approaches,  the  available  negative  links  also  become  restricted.  In  this  paper,  we  point  out  that  it  is  better  to  consider  the  missing  link  problem  as  a  model  learning  problem  by  using  positive  and  unlabeled  data,  rather  than  the  construction  of  traditional  classifier.  We  propose  PULink,  an  approach  that  constructs  the  link  recovery  model  with  positive  and  unlabeled  links.  Our  experiment  results  show  that  compared  to  existing  state-of-the-art  technologies  built  on  traditional  classifier,  PULink  can  achieve  competitive  performance  by  utilizing  only  70%  positive  links  that  are  used  in  those  approaches.
1	An  evolutionary  approach  for  analyzing  alloy  specifications.  Formal  methods  use  mathematical  notations  and  logical  reasoning  to  precisely  define  a  program's  specifications,  from  which  we  can  instantiate  valid  instances  of  a  system.  With  these  techniques  we  can  perform  a  multitude  of  tasks  to  check  system  dependability.  Despite  the  existence  of  many  automated  tools  including  ones  considered  lightweight,  they  still  lack  a  strong  adoption  in  practice.  At  the  crux  of  this  problem,  is  scalability  and  applicability  to  large  real  world  applications.  In  this  paper  we  show  how  to  relax  the  completeness  guarantee  without  much  loss,  since  soundness  is  maintained.  We  have  extended  a  popular  lightweight  analysis,  Alloy,  with  a  genetic  algorithm.  Our  new  tool,  EvoAlloy,  works  at  the  level  of  finite  relations  generated  by  Kodkod  and  evolves  the  chromosomes  based  on  the  failed  constraints.  In  a  feasibility  study  we  demonstrate  that  we  can  find  solutions  to  a  set  of  specifications  beyond  the  scope  where  traditional  Alloy  fails.  While  small  specifications  take  longer  with  EvoAlloy,  the  scalability  means  we  can  handle  larger  specifications.  Our  future  vision  is  that  when  specifications  are  small  we  can  maintain  both  soundness  and  completeness,  but  when  this  fails,  EvoAlloy  can  switch  to  its  genetic  algorithm.
1	Quality  oriented  decision  support  for  maintaining  architectures  of  fault  tolerant  space  systems.  Due  to  hostile  environments,  space  systems  are  equipped  with  hardware  redundancies  to  guarantee  proper  operation.  For  reconfigurations  beyond  redundancies,  manual  decision  making  is  needed,  which  results  in  down  times,  communication  efforts  and  man  hours  in  maintenance  phases.      We  investigate  automated  reconfiguration  decision  support  that  determines  Pareto-optimal  architectures  w.r.t.  variable  hardware  availability  and  quality  properties.  Reconfiguration  options  for  control  software  according  to  available  sensing  and  actuation  hardware  are  derived  and  prioritised  w.r.t.  predicted  qualitative  impacts.  The  knowledge  about  relations  of  the  system's  variations  is  persisted  in  a  decision  model  at  design  time  on  the  level  of  software  architectures.  Upon  a  resources  fault,  the  model  is  traversed  for  an  alternative  architecture.  This  promotes  a  transparent  analysis  of  available  deployments  as  well  as  an  acceleration  of  the  reconfiguration  process  during  maintenance.  We  provide  tool  support  for  analysis  and  a  concept  for  reconfigurations  during  operation.      For  evaluation,  we  inspect  a  reengineered  extension  of  the  attitude  control  system  of  the  TET-1  micro  satellite.
1	Optimizing  decomposition  of  software  architecture  for  local  recovery.  The  increasing  size  and  complexity  of  software  systems  has  led  to  an  amplified  number  of  potential  failures  and  as  such  makes  it  harder  to  ensure  software  reliability.  Since  it  is  usually  hard  to  prevent  all  the  failures,  fault  tolerance  techniques  have  become  more  important.  An  essential  element  of  fault  tolerance  is  the  recovery  from  failures.  Local  recovery  is  an  effective  approach  whereby  only  the  erroneous  parts  of  the  system  are  recovered  while  the  other  parts  remain  available.  For  achieving  local  recovery,  the  architecture  needs  to  be  decomposed  into  separate  units  that  can  be  recovered  in  isolation.  Usually,  there  are  many  different  alternative  ways  to  decompose  the  system  into  recoverable  units.  It  appears  that  each  of  these  decomposition  alternatives  performs  differently  with  respect  to  availability  and  performance  metrics.  We  propose  a  systematic  approach  dedicated  to  optimizing  the  decomposition  of  software  architecture  for  local  recovery.  The  approach  provides  systematic  guidelines  to  depict  the  design  space  of  the  possible  decomposition  alternatives,  to  reduce  the  design  space  with  respect  to  domain  and  stakeholder  constraints  and  to  balance  the  feasible  alternatives  with  respect  to  availability  and  performance.  The  approach  is  supported  by  an  integrated  set  of  tools  and  illustrated  for  the  open-source  MPlayer  software.
1	Using  model  based  assurance  to  strengthen  diagnostic  procedures.  In  previous  work  we  described  Diagnostic  Tree  for  Verification  (DTV),  a  partially  automated  software  engineering  technique  by  which  diagnostic  trees  generated  from  system  models  are  used  to  help  check  out  diagnostic  procedures.  Diagnostic  procedures  are  instructions  used  to  isolate  failures  during  operations.  Assuring  such  procedures  manually  is  time-consuming  and  costly.  This  paper  reports  our  recent  experience  in  applying  DTV  to  diagnostic  procedures  for  lighting  failures  in  NASA's  Habitat  Demonstration  Unit  (HDU),  a  prototype  for  astronauts'  living  quarters.  DTV  identified  missing  and  inconsistent  instructions,  as  well  as  more-efficient  sequences  of  diagnostic  steps.  Unexpectedly,  the  most  significant  benefit  was  finding  assumptions  that  will  not  remain  true  as  the  system  evolves.  We  describe  both  the  challenges  faced  in  applying  DTV  and  how  its  independent  perspective  helped  in  assuring  the  procedures'  adequacy  and  quality.  Finally,  the  paper  discusses  more  generally  how  software  systems  that  are  model-based,  rapidly  evolving  and  safety-critical  appear  most  likely  to  benefit  from  this  approach.
1	Fragment  based  spreadsheet  debugging.  Faults  in  spreadsheets  can  represent  a  major  risk  for  businesses.  To  minimize  such  risks,  various  automated  testing  and  debugging  approaches  for  spreadsheets  were  proposed.  In  such  approaches,  often  one  main  assumption  is  that  the  spreadsheet  developer  is  able  to  indicate  if  the  outcomes  of  certain  calculations  correspond  to  the  intended  values.  This,  however,  might  require  that  the  user  performs  calculations  manually,  a  process  which  can  easily  become  tedious  and  error-prone  for  more  complex  spreadsheets.  In  this  work,  we  propose  an  interactive  spreadsheet  algorithmic  debugging  method,  which  is  based  on  partitioning  the  spreadsheet  into  fragments.  Test  cases  can  then  be  automatically  or  manually  created  for  each  of  these  smaller  fragments,  whose  correctness  or  faultiness  can  be  easier  assessed  by  users  than  test  cases  that  cover  the  entire  spreadsheet.  The  annotated  test  cases  are  then  fed  into  an  algorithmic  debugging  technique,  which  returns  a  set  of  formulas  that  could  have  caused  any  observed  failures,  i.e.,  discrepancies  between  the  expected  and  computed  calculation  outcomes.  Simulation  experiments  demonstrate  that  the  suggested  decomposition  approach  can  speed  up  the  algorithmic  debugging  process  and  significantly  reduce  the  number  of  fault  candidates  returned  by  the  algorithm.  An  additional  laboratory  study  shows  that  fragmenting  a  spreadsheet  with  our  method  furthermore  reduces  the  time  needed  by  users  for  creating  test  cases  for  a  spreadsheet.
1	Improving  the  decision  making  process  of  self  adaptive  systems  by  accounting  for  tactic  volatility.  When  self-adaptive  systems  encounter  changes  within  their  surrounding  environments,  they  enact  tactics  to  perform  necessary  adaptations.  For  example,  a  self-adaptive  cloud-based  system  may  have  a  tactic  that  initiates  additional  computing  resources  when  response  time  thresholds  are  surpassed,  or  there  may  be  a  tactic  to  activate  a  specific  security  measure  when  an  intrusion  is  detected.  In  real-world  environments,  these  tactics  frequently  experience  tactic  volatility  which  is  variable  behavior  during  the  execution  of  the  tactic.Unfortunately,  current  self-adaptive  approaches  do  not  account  for  tactic  volatility  in  their  decision-making  processes,  and  merely  assume  that  tactics  do  not  experience  volatility.  This  limitation  creates  uncertainty  in  the  decision-making  process  and  may  adversely  impact  the  system's  ability  to  effectively  and  efficiently  adapt.  Additionally,  many  processes  do  not  properly  account  for  volatility  that  may  effect  the  system's  Service  Level  Agreement  (SLA).  This  can  limit  the  system's  ability  to  act  proactively,  especially  when  utilizing  tactics  that  contain  latency.To  address  the  challenge  of  sufficiently  accounting  for  tactic  volatility,  we  propose  a  Tactic  Volatility  Aware  (TVA)  solution.  Using  Multiple  Regression  Analysis  (MRA),  TVA  enables  self-adaptive  systems  to  accurately  estimate  the  cost  and  time  required  to  execute  tactics.  TVA  also  utilizes  Autoregressive  Integrated  Moving  Average  (ARIMA)  for  time  series  forecasting,  allowing  the  system  to  proactively  maintain  specifications.
1	Lifting  inter  app  data  flow  analysis  to  large  app  sets.  Mobile  apps  process  increasing  amounts  of  private  data,  giving  rise  to  privacy  concerns.  Such  concerns  do  not  arise  only  from  single  apps,  which  might—accidentally  or  intentionally—leak  private  information  to  untrusted  parties,  but  also  from  multiple  apps  communicating  with  each  other.  Certain  combinations  of  apps  can  create  critical  data  flows  not  detectable  by  analyzing  single  apps  individually.  While  sophisticated  tools  exist  to  analyze  data  flows  inside  and  across  apps,  none  of  these  scale  to  large  numbers  of  apps,  given  the  combinatorial  explosion  of  possible  (inter-app)  data  flows.  We  present  a  scalable  approach  to  analyze  data  flows  across  Android  apps.  At  the  heart  of  our  approach  is  a  graph-based  data  structure  that  represents  inter-app  flows  efficiently.  Following  ideas  from  product-line  analysis,  the  data  structure  exploits  redundancies  among  flows  and  thereby  tames  the  combinatorial  explosion.  Instead  of  focusing  on  specific  installations  of  app  sets  on  mobile  devices,  we  lift  traditional  data-flow  analysis  approaches  to  analyze  and  represent  data  flows  of  all  possible  combinations  of  apps.  We  developed  the  tool  Sifta  and  applied  it  to  several  existing  app  benchmarks  and  real-world  app  sets,  demonstrating  its  scalability  and  accuracy.
1	Migrating  cascading  style  sheets  to  preprocessors  by  introducing  mixins.  Cascading  Style  Sheets  (CSS)  is  the  standard  language  for  styling  web  documents  and  is  extensively  used  in  the  industry.  However,  CSS  lacks  constructs  that  would  allow  code  reuse  (e.g.,  functions).  Consequently,  maintaining  CSS  code  is  often  a  cumbersome  and  error-prone  task.  Preprocessors  (e.g.,  Less  and  Sass)  have  been  introduced  to  fill  this  gap,  by  extending  CSS  with  the  missing  constructs.  Despite  the  clear  maintainability  benefits  coming  from  the  use  of  preprocessors,  there  is  currently  no  support  for  migrating  legacy  CSS  code  to  preprocessors.  In  this  paper,  we  propose  a  technique  for  automatically  detecting  duplicated  style  declarations  in  CSS  code  that  can  be  migrated  to  preprocessor  functions  (i.e.,  mixins).  Our  technique  can  parameterize  differences  in  the  style  values  of  duplicated  declarations,  and  ensure  that  the  migration  will  not  change  the  presentation  semantics  of  the  web  documents.  The  evaluation  has  shown  that  our  technique  is  able  to  detect  98%  of  the  mix-ins  that  professional  developers  introduced  in  websites  and  Style  Sheet  libraries,  and  can  safely  migrate  real  CSS  code.
1	Temperature  dependence  of  magnetic  resonance  sensors  for  embedding  into  constructed  wetlands.  Constructed  wetlands  are  an  environmentally  considerate  means  of  water  purification.  Automating  parameters  such  as  heating  and  aeration  may  extend  the  lifetime  of  constructed  wetlands  and  allow  for  superior  waste-water  treatment.  One  critical  parameter  to  monitor  in  a  wetland  system  is  clogging  of  pores  within  the  gravel  matrix,  as  this  limits  the  viable  lifetime  of  the  system.  It  has  previously  been  observed  in  a  laboratory  setting  that  magnetic  resonance  (MR)  relaxation  measurements,  T1  and  T2eff,  can  be  used  to  characterise  the  clogging  state.  Various  open-geometry  MR  sensors  have  been  constructed  using  permanent  neodymium  magnets  with  the  view  of  long-term  embedding  as  part  of  the  EU  FP7  project  ARBI  (Automated  Reed  Bed  Installations).  The  ultimate  aim  is  to  monitor  clogging  levels  over  the  lifetime  of  the  reed  bed  using  MR  techniques.  One  issue  with  taking  various  MR  measurements  over  such  an  extreme  time  scale,  in  this  case  years,  is  that  temperature  fluctuations  will  significantly  alter  the  magnetic  field  strength  produced  by  the  sensors  constituent  magnets.  While  the  RF  transmit-receive  circuit  has  been  built  so  that  MR  can  still  be  conducted  at  a  range  of  frequencies  without  altering  the  tuning  or  matching  of  the  circuit,  this  will  result  in  poor  RF  excitation  if  the  magnetic  field  strength  shifts  significantly.  This  work  investigates  the  effect  that  temperature  has  on  the  a  MR  sensor  intended  for  embedding,  to  determine  whether  received  signal  intensity  is  compromised  significantly  at  large  temperature  changes.
1	Software  architecture  and  reference  architecture  of  software  intensive  systems  and  systems  of  systems  contributions  to  the  state  of  the  art.  Complex  software-intensive  systems  are  more  and  more  required  as  a  solution  for  diverse  critical  application  domains;  at  the  same  time,  software  architecture  and  also  reference  architecture  have  attracted  attention  as  means  to  more  adequately  produce  and  evolve  such  systems.  The  main  goal  of  this  paper  is  to  summarize  our  principal  contributions  in  software  architecture  and  reference  architecture  of  software-intensive  systems,  including  Systems-of-Systems.  We  intend  this  work  can  also  inspire  the  opening  of  other  related  research  lines  towards  founding  the  sustainability  of  such  software-intensive  systems.
1	Suitability  of  software  architecture  decision  making  methods  for  group  decisions.  Software  architecture  design  decisions  are  central  to  the  architecting  process.  Hence,  the  software  architecture  community  has  been  constantly  striving  towards  making  the  decision-making  process  robust  and  reliable  to  create  high-quality  architectures.  Surveys  of  practitioners  has  demonstrated  that  most  decisions  made  by  them  are  group  decisions.  Hence,  for  any  tool  or  method  to  be  useful  to  them,  it  must  include  provision  for  making  group  decisions.
1	Relating  health  to  platform  success  exploring  three  e  commerce  ecosystems.  The  market  for  e-commerce  systems  is  saturated  with  more  than  60  different  solutions  that  compete  and  try  to  accommodate  different  needs.  However,  it  appears  that  three  main  platforms  account  for  roughly  half  of  the  market  share:  Magento,  PrestaShop,  and  WooCommerce.  This  paper  evaluates  these  platforms  from  a  software  ecosystems  health  perspective.  By  shedding  light  on  the  success  factors  of  these  ecosystems,  we  aim  at  establishing  what  makes  an  e-commerce  ecosystem  healthy.  This  knowledge  provides  ecosystem  orchestrators  with  an  overview  of  the  current  health  of  these  ecosystems,  and  researchers  with  an  application  of  ecosystem  health  assessment.
1	Reducing  architectural  knowledge  vaporization  by  applying  the  repertory  grid  technique.  The  architecture  of  a  software-intensive  system  is  the  composition  of  architectural  design  decisions.  These  decisions  are  an  important  part  of  Architectural  Knowledge  (AK).  Failure  to  document  architectural  design  decisions  can  lead  to  AK  vaporization  and  higher  maintenance  costs.  To  reduce  AK  vaporization,  we  propose  to  apply  the  Repertory  Grid  Technique  (RGT)  to  make  tacit  knowledge  about  architecture  decisions  explicit.  An  architect  can  use  the  RGT  to  elicit  decision  alternatives  and  concerns,  and  to  rank  each  alternative  against  concerns.  To  validate  our  approach,  we  conducted  a  survey  with  graduate  students.  In  the  survey,  participants  documented  decisions  using  the  RGT.  We  compared  these  decisions  with  decisions  documented  using  a  basic  decision  template.  Our  results  suggest  that  RGT  leads  to  less  AK  vaporization,  compared  to  conventional  ways  of  documenting  decisions.
1	A  quantitative  approach  for  the  assessment  of  microservice  architecture  deployment  alternatives  by  automated  performance  testing.  Microservices  have  emerged  as  an  architectural  style  for  developing  distributed  applications.  Assessing  the  performance  of  architectural  deployment  alternatives  is  challenging  and  must  be  aligned  with  the  system  usage  in  the  production  environment.  In  this  paper,  we  introduce  an  approach  for  using  operational  profiles  to  generate  load  tests  to  automatically  assess  scalability  pass/fail  criteria  of  several  microservices  deployment  alternatives.  We  have  evaluated  our  approach  with  different  architecture  deployment  alternatives  using  extensive  lab  studies  in  a  large  bare  metal  host  environment  and  a  virtualized  environment.  The  data  presented  in  this  paper  supports  the  need  to  carefully  evaluate  the  impact  of  increasing  the  level  of  computing  resources  on  performance.  Specifically,  for  the  case  study  presented  in  this  paper,  we  observed  that  the  evaluated  performance  metric  is  a  non-increasing  function  of  the  number  of  CPU  resources  for  one  of  the  environments  under  study.
1	Collecting  service  based  maintainability  metrics  from  restful  api  descriptions  static  analysis  and  threshold  derivation.  While  many  maintainability  metrics  have  been  explicitly  designed  for  service-based  systems,  tool-supported  approaches  to  automatically  collect  these  metrics  are  lacking.  Especially  in  the  context  of  microservices,  decentralization  and  technological  heterogeneity  may  pose  challenges  for  static  analysis.  We  therefore  propose  the  modular  and  extensible  RAMA  approach  (RESTful  API  Metric  Analyzer)  to  calculate  such  metrics  from  machine-readable  interface  descriptions  of  RESTful  services.  We  also  provide  prototypical  tool  support,  the  RAMA  CLI,  which  currently  parses  the  formats  OpenAPI,  RAML,  and  WADL  and  calculates  10  structural  service-based  metrics  proposed  in  scientific  literature.  To  make  RAMA  measurement  results  more  actionable,  we  additionally  designed  a  repeatable  benchmark  for  quartile-based  threshold  ranges  (green,  yellow,  orange,  red).  In  an  exemplary  run,  we  derived  thresholds  for  all  RAMA  CLI  metrics  from  the  interface  descriptions  of  1,737  publicly  available  RESTful  APIs.  Researchers  and  practitioners  can  use  RAMA  to  evaluate  the  maintainability  of  RESTful  services  or  to  support  the  empirical  evaluation  of  new  service  interface  metrics.
1	The  skills  that  employers  look  for  in  software  testers.  Software  testing  is  an  integral  part  of  software  development  that  provides  better-quality  products  and  user  experiences  and  helps  build  the  reputation  of  software  companies.  Though  software  testers  perform  a  role  that  requires  specific  tasks  and  skills,  in-depth  studies  of  software  testers  lag  behind  research  studies  of  other  roles  within  software  development  teams.  In  this  paper,  we  aim  to  create  a  profile  of  testers  by  presenting  an  empirical  analysis  of  the  skills  the  industry  currently  needs.  We  analysed  data  from  400  job  adverts  in  33  countries.  We  mapped  the  skills  on  a  taxonomy  comprising  test-related,  technical,  and  domain-specific  skills.  In  addition,  we  looked  at  the  demand  for  educational  attainment,  relevant  certifications,  and  previous  experience  requirements.  Our  findings  show  that  employers  are  mostly  interested  in  skills  related  to  test  planning  and  design,  test  automation,  functional  testing,  performance  testing,  and  progress  reporting.  One  third  of  the  job  advertisers  were  interested  in  people  with  the  skills  to  operate  test  execution  tools.  Selenium  was  the  testing  tool  most  in  demand.  The  testers  must  have  strong  technical  abilities,  including  programming  skills  in  Java,  C#,  and  SQL.  Also,  they  must  handle  project  management  tasks  such  as  estimation,  risk  management,  and  quality  assurance.  Employers  do  not  emphasise  domain-specific  knowledge,  which  indicates  that  they  consider  testing  skills  portable  across  industries.  One  in  seven  job  adverts  asks  for  a  software  testing  certification.  Our  study  helps  clarify  the  complexity  of  the  testing  job  and  outlines  the  capabilities  one  needs  to  fulfil  a  software  tester’s  responsibilities.
1	Monitoring  memory  related  software  aging  an  exploratory  study.  The  accumulating  effects  of  software  aging  have  direct  influence  to  the  rate  of  aging-related  failures.  So  far,  the  most  investigated  software  aging  effects  are  memory  related,  such  as  memory  leak  and  memory  fragmentation  problems.  In  this  work,  we  present  a  practical  body  of  knowledge  to  support  the  solid  understanding  of  the  most  important  issues  in  monitoring  memory-related  software  aging  effects,  focusing  on  memory  leak  problems.  We  discuss  important  drawbacks  of  using  well  known  system-wide  and  application-specific  aging  indicators,  as  well  as  propose  effective  solutions  for  both  cases.
1	A  novel  hybridization  of  artificial  neural  networks  and  arima  models  for  forecasting  resource  consumption  in  an  iis  web  server.  Software  aging  has  been  observed  in  a  long  running  software  application.  A  technique  named  rejuvenation  is  proposed  to  counteract  this  problem.  The  key  to  the  aging  and  rejuvenation  problem  is  how  to  analyze/forecast  the  resource  consumption  of  software  system.  In  this  paper,  we  propose  a  methodology  of  hybrid  ARIMA  and  artificial  neural  networks  to  forecast  resource  consumption  in  an  IIS  web  server  which  is  a  running  commercial  server  and  subjected  to  software  aging.  The  proposed  hybrid  method  consists  of  two  steps.  In  the  first  step,  an  ARIMA  model  is  used  to  analyze  the  linear  component  of  the  data.  In  the  second  step,  an  artificial  neural  network  model  is  developed  to  model  the  residuals  from  ARIMA  model.  The  results  show  that  the  proposed  hybrid  model  can  be  a  good  trade-off  to  forecast  resource  consumption.
1	Evaluating  regression  test  selection  opportunities  in  a  very  large  open  source  ecosystem.  Regression  testing  in  very  large  software  ecosystems  is  notoriously  costly,  requiring  computational  resources  that  even  large  corporations  struggle  to  cope  with.  Very  large  ecosystems  contain  thousands  of  rapidly  evolving,  interconnected  projects  where  client  projects  transitively  depend  on  library  projects.  Regression  test  selection  (RTS)  reduces  regression  testing  costs  by  rerunning  only  tests  whose  pass/fail  behavior  may  flip  after  code  changes.  For  single  projects,  researchers  showed  that  class-level  RTS  is  more  effective  than  lower  method-or  statement-level  RTS.  Meanwhile,  several  very  large  ecosystems  in  industry,  e.g.,  at  Facebook,  Google,  and  Microsoft,  perform  project-level  RTS,  rerunning  tests  in  a  changed  library  and  in  all  its  transitive  clients.  However,  there  was  no  previous  study  of  the  comparative  benefits  of  class-level  and  project-level  RTS  in  such  ecosystems.  We  evaluate  RTS  opportunities  in  the  MAVEN  Central  opensource  ecosystem.  There,  some  popular  libraries  have  up  to  924589  clients;  in  turn,  clients  can  depend  on  up  to  11190  libraries.  We  sampled  408  popular  projects  and  found  that  202  (almost  half)  cannot  update  to  latest  library  versions  without  breaking  compilation  or  tests.  If  developers  want  to  detect  these  breakages  earlier,  they  need  to  run  very  many  tests.  We  compared  four  variants  of  class-level  RTS  with  project-level  RTS  in  MAVEN  Central.  The  results  showed  that  class-level  RTS  may  be  an  order  of  magnitude  less  costly  than  project-level  RTS  in  very  large  ecosystems.  Specifically,  various  class-level  RTS  variants  select,  on  average,  7.8%-17.4%  of  tests  selected  by  project-level  RTS.
1	Compositional  verification  of  sensor  software  using  uppall.  Verification  of  wireless  sensor  networks  has  long  been  performed  for  communication  protocols  and  for  network-level  behavior  over  multiple  nodes,  but  not  for  the  basic  properties  that  should  hold  at  a  single  node.  Testing  sensor  networks,  however,  is  extremely  hard  due  to  the  lack  of  controllability,  and  complex  simulation  setups  are  often  too  expensive  to  undertake.  Thus,  verification  of  properties  for  a  sensor  node  is  desirable.  We  created  a  verification  methodology  that  extracts  timed  models  of  the  high-level  behavior  of  a  wireless  sensor  and  then  uses  UPPAAL  to  verify  both  functional  and  non-functional  (timed)  properties  for  the  sensor.  This  verification  capability  will  enhance  the  trustworthiness  of  deployed  sensor  networks.
1	Safe  ar  reducing  risk  while  augmenting  reality.  Augmented  reality  (AR)  systems  excel  at  offering  users  real-time,  situation-aware  information  to  support  users'  decision  making.  With  AR,  rich  visualizations  of  relevant  data  can  be  displayed  to  users  without  blocking  their  view  of  the  real  world.  For  example,  an  AR-enabled  automotive  windshield  can  display  a  red  outline  around  a  pedestrian  to  alert  a  driver  starting  a  turn  into  that  cross  street.  Other  critical  uses  of  AR  applications  that  are  or  will  soon  be  deployed  include  surgery,  emergency  response,  vehicle  maintenance,  and  pilot  training.  Many  of  these  applications  can  enhance  operational  safety.  However,  developing  risk  analysis  methods  to  handle  failure  modes  in  the  melded  virtual  and  physical  realities  remains  an  open  problem.  This  paper  proposes  a  risk  analysis  method  with  which  to  study  computer-generated  AR  visualizations  of  system  and  environment  states.  The  analysis  framework  incorporates  three  levels  at  which  AR  interfaces  with  the  user:  perception,  comprehension,  and  decision-making.  This  method  enables  broader  risk  analysis  of  the  entire  cyber-physical-human  system  that  an  AR  application  may  indirectly  control.  Preliminary  results  show  that  this  method  yields  improved  coverage  of  user-involved  failure  modes  over  current  approaches.  While  the  focus  here  is  on  safety,  the  method  also  appears  applicable  to  AR  security  risks.
1	Checkable  safety  cases  enabling  automated  consistency  checks  between  safety  work  products.  In  the  automotive  domain,  the  employment  of  agile  development  is  currently  hindered  by  the  fact  that  the  safety  lifecycle,  which  implies  the  creation  and  maintenance  of  safety  work  products,  is  manually  executed,  being  a  complex  and  expensive  process.  Given  a  change  in  the  system  under  consideration,  ISO  26262  recommends  that  the  impact  of  that  change  on  the  safety  case  of  the  system  shall  be  assessed  and  that  the  safety  case  shall  be  correspondingly  updated.  To  this  end,  in  this  paper,  while  assuming  a  model-based  system  and  safety  engineering  context,  we  propose  checkable  safety  case  models,  which  are  semantically  rich  safety  case  models  integrated  with  system  and  safety  engineering  models  (i.e.,  work  products  of  a  model-based  safety  lifecycle).  The  semantically  rich  specification  and  the  model  integration  allow  for  automated  consistency  checks  between  the  safety  case  and  the  system,  specifically  its  engineering  models.  We  exemplify  our  contributions  via  an  in-vehicle  driver  assistance  system  for  driving  through  intersections.
1	Handling  language  variations  in  open  source  bug  reporting  systems.  Natural  language  plays  a  critical  role  in  the  design,  development  and  maintenance  of  software  systems.  For  example,  bug  reporting  systems  allow  users  to  submit  reports  describing  observed  anomalies  in  free  form  English.  However,  the  free  form  aspect  makes  the  detection  of  duplicate  reports  a  challenge  due  to  the  breadth  and  diversity  of  language  used  by  individual  reporters.  Tokenization,  stemming  and  stop  word  removal  are  commonly  used  techniques  to  normalize  and  reduce  the  language  space.  However,  the  impact  of  typographical  errors  and  alternate  spellings  has  not  been  analyzed  in  the  research  literature.  Our  research  indicates  that  handling  language  problems  during  automated  bug  triage  analysis  can  lead  to  a  boost  in  performance.  We  show  that  the  language  used  in  software  problem  reporting  is  too  specialized  to  benefit  from  domain  independent  spell  checkers  or  lexical  databases.  Therefore,  we  present  a  novel  approach  using  word  distance  and  neighbor  word  likelihood  measures  for  detecting  and  resolving  language-based  issues  in  open-source  software  problem  reporting.  We  evaluate  our  approach  using  the  complete  Firefox  repository  until  March  2012.  Our  results  indicate  measurable  improvements  in  duplicate  detection  results,  while  reducing  the  language  space  for  most  frequently  used  words  by  30%.  Moreover,  our  method  is  language-agnostic  and  does  not  require  a  pre-built  dictionary,  thus  making  it  suitable  for  use  in  a  variety  of  systems.
1	Class  point  approach  for  software  effort  estimation  using  various  support  vector  regression  kernel  methods.  Accurate  effort  estimation  in  early  stage  of  software  development  life  cycle  is  a  major  challenge  for  many  software  industries.  The  use  of  various  optimization  techniques  helps  in  improving  the  effort  estimation  accuracy.  The  Support  Vector  Regression  (SVR)  is  one  of  different  soft-computing  techniques,  that  helps  in  getting  optimal  estimated  values.  The  idea  of  SVR  is  based  upon  the  computation  of  a  linear  regression  function  in  a  high  dimensional  feature  space  where  the  input  data  are  mapped  via  a  nonlinear  function.  Further,  the  SVR  kernel  methods  can  be  applied  in  transforming  the  input  data  and  then  based  on  these  transformations,  an  optimal  boundary  between  the  possible  outputs  can  be  obtained.  The  main  objective  of  the  research  work  carried  out  in  this  paper  is  to  estimate  the  software  effort  using  class  point  approach.  Then,  an  attempt  has  been  made  to  optimize  the  results  obtained  from  class  point  analysis  using  various  SVR  kernel  methods  to  achieve  better  prediction  accuracy.  A  performance  comparison  of  the  models  obtained  using  various  SVR  kernel  methods  has  also  been  presented  in  order  to  highlight  performance  achieved  by  each  method.
1	Comic  books  graphic  novels  and  a  novel  approach  to  teaching  anatomy  and  surgery.  Art  has  been  an  integral  component  of  the  exploration  of  the  human  body  for  thousands  of  years,  and  drawing  part  of  our  evolutionary  history  and  one  our  best  adaptations  for  communicating  and  acquiring  knowledge.  As  a  biology  educator  and  artist,  I  had  been  using  drawing  for  my  entire  career  to  bring  students  closer  to  the  natural  world  and  assist  them  in  deepening  their  understanding  of  form.  I  have  used  a  variety  of  teaching  techniques  in  biology  and  anatomy.  More  recently  I  discovered  that  I  could  transfer  my  visual  narrative  of  biology  using  drawing  and  comic  books  to  my  anatomy  classes  and  to  surgical  residents  and  medical  students.  This  merger  of  fine  art  and  life  sciences  resulted  in  a  fine  arts  show,  two  published  papers,  and  pedagogy  of  a  comic  book  and  graphic  novel  genre  that  taught  anatomy  drawing  to  medical  and  anatomy  students  through  surgical  narrative.
1	A  runtime  verification  based  trace  oriented  monitoring  framework  for  cloud  systems.  Cloud  computing  provides  a  new  paradigm  for  resource  utilization  and  sharing.  However,  the  reliability  problems,  like  system  failures,  often  happen  in  cloud  systems  and  bring  enormous  loss.  Trace-oriented  monitoring  is  an  important  runtime  method  to  improve  the  reliability  of  cloud  systems.  In  this  paper,  we  propose  to  bring  runtime  verification  into  trace-oriented  monitoring,  to  facilitate  the  specification  of  monitoring  requirements  and  to  improve  the  efficiency  of  monitoring  cloud  systems.  Based  on  a  data  set  collected  from  a  cloud  storage  system  in  a  real  environment,  we  validate  our  approach  by  monitoring  the  critical  properties  of  the  storage  system.  The  preliminary  experimental  results  indicate  the  promise  of  our  approach.
1	Using  error  information  to  improve  software  quality.  Problem  Definition:  To  help  ensure  high-quality  software  artifacts,  researchers  and  practitioners  have  developed  various  techniques  for  identifying  and  repairing  problems  early  in  the  software  lifecycle  (e.g.,  requirements  and  design  documents).  Most  of  these  techniques  are  fault-based,  and  have  been  empirically  validated.  However,  results  show  that  even  when  developers  faithfully  apply  these  techniques,  they  are  not  able  to  identify  all  types  of  problems  and  that  40-50%  of  effort  is  spent  on  fixing  these  early  problems  later  in  the  development  process.  The  studies  have  revealed  the  inadequacy  of  fault-based  approaches,  which  treats  the  symptoms  of  software  defects,  not  their  underlying  causes.  Prior  research  that  only  analyzed  a  sample  of  faults  to  determine  their  causes  and  suggest  process  improvements  (e.g.,  RCA,  ODC)  overlooked  many  errors  due  to  a  lack  of  underlying  cognitive  theory.  Proposed  Solution:  Our  solution  applies  results  from  human  error  research  to  address  the  defects  made  during  development.  Human  error  research  focuses  on  the  psychological  processes  that  produce  errors  in  human  behavior.  The  process  of  applying  human  error  research  to  software  development  begins  by  collecting  data  about  errors,  finding  common  failures,  and  interpreting  those  errors  in  light  of  human  information  processing  limitations  and  known  error  patterns.  A  taxonomy  of  errors  can  be  used  to  make  developers  more  effective  during  the  software  inspection  process  to  catch  costly  mistakes  early.
1	Arguing  software  compliance  with  iso  26262.  ISO  26262  is  a  safety  standard  for  electrical  and/or  electronic  systems  in  automobiles  and  includes  specific  requirements  for  software.  Compliance  with  the  standard  requires  a  safety  case.  In  this  paper  we  present  an  approach  to  structuring  a  software  assurance  case  that  complies  with  ISO  26262  and  argues  explicitly  that  the  subject  software  meets  appropriate  dependability  goals.  The  resulting  assurance  case  integrates  conveniently  into  a  safety  case  for  the  subject  system.
1	Conducting  robots  bridging  the  gap  between  science  technology  and  the  arts  in  the  undergraduate  curriculum.  While  many  have  explored  multidisciplinary  approaches  to  course  content  delivery  in  computer  science  and  engineering,  very  few  have  combined  engineering  with  fundamentally  different  disciplines  such  as  the  arts,  humanities,  or  social  science.  This  paper  presents  a  multidisciplinary  undergraduate  seminar  entitled  “Conducting  Robots”  that  brings  together  majors  from  four  disparate  disciplines:  computer  science,  mechanical  engineering,  music,  and  interactive  multimedia.  The  goal  of  the  course  is  to  teach  and  support  interdisciplinary  teamwork  while  student  teams  build  an  artificial  system  that  can  conduct  the  college  orchestra.  The  end-of-semester  survey  shows  that  students  found  the  course  interesting  and  challenging,  motivating  them  to  collaborate  with  peers  across  disciplines.
1	On  hardware  variability  and  the  relation  to  software  variability.  In  mechatronic  and  embedded  systems,  variability  stretches  from  customer-visible  features  to  implementation  features,  which  manifest  in  software,  hardware,  and  mechanical  parts.  A  good  example  are  automotive  systems,  which  are  usually  implemented  as  product  lines.  There  are  close  connections  between  hardware  and  software  during  the  development  of  such  product  lines.  For  example,  software  usually  needs  to  be  heavily  tuned  towards  processors  characteristics  or  optimized  for  a  specific  memory  size.  The  problem  is  that  different  lifecycles  of  hardware  and  software  make  it  difficult  to  maintain  all  variability  in  a  single  model.  In  this  paper,  the  notion  of  hardware  variability  is  discussed.  We  suggest  that  software  and  hardware  variability  should  be  kept  in  separate  models.  We  argue  that  hardware  variability  and  software  variability  models  should  only  be  loosely  coupled.  This  allows  an  easier  exchange  of  hardware  platforms  and  variants  as  well  as  a  test  during  the  configuration  whether  hardware  and  software  fit  to  each  other.  To  address  this,  we  propose  an  approach  that  distinguishes  between  software  and  hardware  variants  by  using  separate  variability  models.  Therefore,  we  introduce  a  hardware  variability  model,  which  has  a  strong  focus  on  the  description  of  hardware  properties.  Furthermore,  we  introduce  a  concept  for  modeling  the  dependencies  between  hardware  and  software  variants  to  combine  them  during  the  configuration.
1	A  comparative  study  of  vectorization  based  static  test  case  prioritization  methods.  To  enhance  the  efficiency  of  software  testing,  researchers  have  studied  various  test  case  prioritization  (TCP)  methods.  A  topic  model-based  TCP  is  one  of  the  promising  methods,  which  expresses  test  cases  by  topic  vectors  and  prioritizes  them  in  the  order  such  that  the  set  of  already-prioritized  test  cases  have  the  maximum  dispersion  in  the  vector  space.  However,  the  topic  model  is  not  the  only  option  available  for  vectorizing  test  cases.  Moreover,  the  distance  metric  in  the  vector  space  and  the  scheme  to  prioritize  test  cases  (the  way  to  find  the  test  case  that  is  the  farthest  from  the  set  of  already-prioritized  ones)  also  have  some  available  options.  Because  the  combinations  of  the  above  options  have  not  been  well-discussed  in  the  past,  this  paper  conducts  a  comparative  study  of  36  TCP  methods,  which  are  the  combinations  of  (1)  three  vectorization  methods,  (2)  three  distance  metrics,  and  (3)  four  prioritization  schemes  (36=3x3x4).  The  empirical  results  show  the  following  findings.  The  choice  of  the  vectorization  method  has  a  significant  impact  on  the  testing  efficiency:  a  promising  option  is  Doc2Vec  (PVDBoW).  The  combination  with  the  distance  metric  may  also  be  impactful:  a  useful  combination  is  Doc2Vec  (PV-DBoW)  and  Euclidean  distance.  The  third  aspect,  i.e.,  the  choice  of  the  scheme  to  find  the  farthest  test  case,  is  not  always  influential.
1	A  balanced  risk  treatment  for  construction  projects.  Risk  management  is  an  integral  part  of  a  successful  project  planning  and  control  mechanism.  Standards,  e.g.,  AS/NZS  ISO  31000:2009,  establish  frameworks  on  how  to  perform  comprehensive  risk  management  process.  However,  there  remain  gaps  in  enacting  such  standards  in  reality,  one  of  which  is  balancing  risk  treatment  with  the  associated  costs  to  risk-bearing  project  stakeholders.  Although  many  studies  were  carried  out  to  identify  the  range  of  factors  representing  project  risk  events  and  the  recommended  responses,  very  little  has  addressed  the  means  of  making  such  decisions.  In  this  context,  the  guided  search  capabilities  of  evolutionary  algorithms  can  play  a  role.  After  discussing  and  modeling  the  costs  and  benefits  of  alternative  risk  treatment  strategies,  the  paper  introduces  ant  colony  as  a  capable  algorithm  for  the  balanced  selection  of  such  strategies.  The  research  is  being  applied  in  the  pipeline  construction  sector  and  made  use  of  professional  knowledge  and  project  records  from  a  mega  construction  company  in  the  Middle  East.
1	A  smart  classroom  of  wireless  sensor  networks  for  students  time  attendance  system.  Today,  Wireless  Sensor  Networks  (WSNs)  have  been  included  in  many  researches  to  form  smart  environments.  IoT  is  becoming  increasingly  integrated  in  our  daily  life  at  homes,  streets,  schools  and  everywhere.  Using  IoT  based  on  WSNs  in  the  educational  filed  is  an  important  resource  for  better  preparing  students  for  the  future  digital  world.  Digital  students  attendance  system  in  schools  or  universities  is  an  example  for  case  that  technology  can  facilitate  the  traditional  ways  for  checking  students  attendance  instead  of  using  the  manual  paper  sheets  which  consume  time  and  resources  with  higher  probability  of  failure.  In  literature,  there  are  many  time  attendance  systems  that  proposed  to  enhance  the  manual  traditional  methods  for  taking  and  calculating  students  attendance  in  smart  ways.  In  this  paper,  we  proposed  a  user  friendly  students  time  attendance  system  that  can  be  applicable  in  different  schools  or  universities  in  order  to  form  a  smart  classroom  based  on  WSNs  and  IoT  technologies.  We  proposed  intelligent  chairs  that  can  be  identified  as  the  sources  of  information,  which  integrated  with  four  50  kg  load  sensors  and  HX711  amplifier  that  measure  the  students  weights  and  send  the  digital  signals  to  a  receiver  in  order  to  recognize  the  student  presence  during  class  schedule.  This  smart  classroom  is  also  installed  with  one  ZKTeco  ZK4500  fingerprint  reader  in  order  to  increase  the  identification  for  students.  The  transmitted  signals  from  the  intelligent  chairs  will  be  connected  by  Android  application  which  will  be  installed  on  the  lecturers’  smart  phones.  So,  teachers  will  get  details  and  summary  report  of  students  attendance  through  the  Android  app.  We  implemented  our  proposed  idea  using  java  language,  database  and  android  system.
1	Guiding  testing  activities  by  predicting  defect  prone  parts  using  product  and  inspection  metrics.  Product  metrics,  such  as  size  or  complexity,  are  often  used  to  identify  defect-prone  parts  or  to  focus  quality  assurance  activities.  In  contrast,  quality  information  that  is  available  early,  such  as  information  provided  by  inspections,  is  usually  not  used.  Currently,  only  little  experience  is  documented  in  the  literature  on  whether  data  from  early  defect  detection  activities  can  support  the  identification  of  defect  prone  parts  later  in  the  development  process.  This  article  compares  selected  product  and  inspection  metrics  commonly  used  to  predict  defect-prone  parts.  Based  on  initial  experience  from  two  case  studies  performed  in  different  environments,  the  suitability  of  different  metrics  for  predicting  defect-prone  parts  is  illustrated.  These  studies  revealed  that  inspection  defect  data  seems  to  be  a  suitable  predictor,  and  a  combination  of  certain  inspection  and  product  metrics  led  to  the  best  prioritizations  in  our  contexts.
1	A  computation  and  storage  trade  off  strategy  for  cost  efficient  video  transcoding  in  the  cloud.  Video  transcoding  refers  to  the  process  of  converting  a  compressed  digital  video  from  one  format  to  another.  Since  it  is  a  compute-intensive  operation,  transcoding  of  a  large  number  of  on-demand  videos  requires  a  large  scale  cluster  of  transcoding  servers.  Moreover,  storage  of  multiple  transcoded  versions  of  each  source  video  requires  a  large  amount  of  disk  space.  Infrastructure  as  a  Service  (IaaS)  clouds  provide  virtual  machines  (VMs)  for  creating  a  dynamically  scalable  cluster  of  servers.  Likewise,  a  cloud  storage  service  may  be  used  to  store  a  large  number  of  transcoded  videos.  Moreover,  it  may  be  possible  to  reduce  the  total  IaaS  cost  by  trading  storage  for  computation,  or  vice  versa.  In  this  paper,  we  present  a  computation  and  storage  trade-off  strategy  for  cost-efficient  video  transcoding  in  the  cloud  called  cost  and  popularity  score  based  strategy.  The  proposed  strategy  estimates  computation  cost,  storage  cost,  and  video  popularity  of  individual  transcoded  videos  and  then  uses  this  information  to  make  decisions  on  how  long  a  video  should  be  stored  or  how  frequently  it  should  be  re-transcoded  from  a  given  source  video.  It  is  demonstrated  in  a  discrete-event  simulation  and  is  evaluated  in  a  series  of  experiments  involving  semi  synthetic  and  realistic  load  patterns.
1	Autonomic  configuration  adaptation  based  on  simulation  generated  state  transition  models.  Configuration  management  is  a  complex  task,  even  for  experienced  system  administrators,  which  makes  self-managing  systems  a  particularly  desirable  solution.  This  paper  describes  a  novel  contribution  to  self-managing  systems,  including  an  autonomic  configuration  self-optimization  methodology.  Our  solution  involves  a  systematic  simulation  method  that  develops  a  state-transition  model  of  the  behavior  of  a  service-oriented  system  in  terms  of  its  configuration  and  performance.  At  run  time,  the  system's  behavior  is  monitored  and  classified  in  one  of  the  model  states.  If  this  state  may  lead  to  futures  that  violate  service  level  agreements,  the  system  configuration  is  changed  toward  a  safer  future  state.  Similarly,  a  satisfactory  state  that  is  over-provisioned  may  be  transitioned  to  a  more  economical  satisfactory  state.  Aside  from  the  typical  benefits  of  self-optimization,  our  approach  includes  an  intuitive,  explainable  decision  model,  the  ability  to  predict  the  future  with  some  accuracy  avoiding  trial-and-error,  offline  training,  and  the  ability  to  improve  the  model  at  run-time.  We  demonstrate  this  methodology  in  an  experiment  where  Amazon  EC2  instances  are  added  and  removed  to  handle  changing  request  volumes  to  a  real  service-oriented  application.  We  show  that  a  knowledge  base  generated  entirely  in  simulation  can  be  used  to  make  accurate  changes  to  a  real-world  application.
1	Improving  reusability  of  model  transformations  by  automating  their  composition.  Model  based  software  design  relies  on  model  transformations.  In  order  to  ease  their  reuse  and  maintenance,  transformations  can  be  broken  down  into  smaller  transformation  units  that  are  to  be  composed.  When  reusing  transformation  units  in  different  projects,  it  is  necessary  to  identify  how  to  compose  them  in  order  to  obtain  a  valid  transformation.  A  valid  transformation  is  a  chain  of  transformation  units  where  the  output  model  of  a  unit  meets  the  conditions  of  applicability  of  the  next  unit.  When  the  conditions  of  applicability  of  transformation  units  is  complex,  the  construction  of  valid  chains  becomes  error  prone  and  time  consuming.  In  a  previous  work,  we  proposed  to  automate  the  identification  of  model  transformations  chains  by  formalizing  models  and  transformations  using  Alloy.  The  complexity  of  this  identification  grows  rapidly  with  the  number  of  elements  in  the  input  model,  the  number  of  transformation  units  to  apply.  This  is  even  more  difficult  when  they  transformations  are  endogenous.  In  this  paper,  we  present  optimizations  for  the  identification  of  transformation  chains.  Performance  evaluation  shows  significant  improvements,  and  alleviates  the  scalability  problem  identified  in  our  previous  work.  In  addition,  the  method  we  propose  in  this  paper  helps  the  identification  of  valid  sub-chains,  where  only  a  subset  of  transformation  units  is  correctly  chained.
1	Archetypical  approaches  of  fast  software  development  and  slow  embedded  projects.  This  paper  describes  the  problem  context  of  software  development  for  mass-produced  embedded  systems,  with  distinguishing  factors  such  as  the  co-design  of  software  and  hardware,  strong  focus  on  manufacturing  aspects,  supplier  involvement  and  safety-critical  functionality.  In  this  context  there  is  a  need  for  a  holistic  model  to  explain  the  failures  and  successes  industrial  projects,  where  just  investigating  a  single  dimension,  e.g.  chosen  ways-of-working  or  architecture  is  not  sufficient.  The  main  contribution  is  a  holistic  model  consisting  of  five  archetypical  approaches  to  embedded  software  development,  based  on  a  mapping  study  over  industrial  cases  in  literature.  The  approaches  range  from  "traditional"  stage-gate  projects  focusing  on  product  qualities  and  large  integration  efforts,  to  fast  development  in  short  loops  by  autonomous  teams  based  on  a  compos  able  software  platform.  The  model  aligns  the  processes  with  the  architecture  of  the  embedded  software,  and  the  implications  on  the  business  and  the  organisation.  The  model  allows  an  research  &  development(R&D)  organisation  to  identify  where  it  is  positioned  and  to  evolve  its  software  development  approach.  The  model  is  elucidated  by  two  empirical  cases  from  a  Swedish  company.
1	Success  dimensions  in  selecting  cloud  software  services.  Cloud  computing  is  promoted  by  providers  as  a  service  offering  to  satisfy  the  modern  information  system  needs  of  the  business  stakeholder.  These  services  are  presented  in  a  way  that  can  be  elastic,  scalable,  cost-effective  and  delivered  via  the  internet  on  a  pay-for-usage  pricing  model.  These  services  are  now  within  the  grasp  of  the  small  and  medium-sized  enterprise  (SME).  Businesses  in  general,  and  SMEs  in  particular,  may  not  have  the  technical  capability  to  explicitly  state  their  service  needs  or  expectations  or  to  assess  risks.  This  paper  highlights  some  of  the  unseen  technical  hurdles  faced  by  SMEs  in  selecting  and  identifying  software-as-a-service  offerings.  The  research  was  undertaken  through  an  analysis  of  providers  considered  by  businesses,  the  expertise  the  businesses  sought,  and  an  ethnographic  observation  of  a  service  selection.  The  results  are  used  to  propose  indicative  success  dimensions  for  cloud  service  selection  and  a  need  for  more  detailed  research  to  support  SME  in  service  selection
1	An  adaptive  control  model  for  non  functional  feature  interactions.  Many  systems,  especially  distributed  embedded  systems,  have  very  strong  emphasis  on  non-functional  properties,  which  are  often  cross-cutting  and  difficult  to  capture  in  a  modular  way.  Here,  we  consider  non-functional  feature  interactions,  which  occur  if  two  features  show  unexpected  behavior  regarding  non-functional  properties.  The  goal  is  to  handle  non-functional  properties  and  interactions  in  a  modular  and  flexible  way  on  a  separate  control  layer.  On  this  control  layer,  we  can  adapt  control  components  to  different  feature  interactions.  We  use  state  charts  to  describe  control  models  and  use  state  chart  refinement  to  make  interactions  explicit.  We  present  our  approach  by  two  examples  with  several  non-functional  feature  interactions  and  argue  that  the  control  layer  can  address  these.  The  main  advantages  are  modular  control  of  non-functional  properties  and  explicit  modeling  of  non-functional  feature  interactions  on  a  separate  control  layer.
1	A  comparison  of  cross  versus  single  company  effort  prediction  models  for  web  projects.  Background:  In  order  to  address  the  challenges  in  companies  having  no  or  limited  effort  datasets  of  their  own,  cross-company  models  have  been  a  focus  of  interest  for  previous  studies.  Further,  a  particular  domain  of  investigation  has  been  Web  projects.  Aim:  This  study  investigates  to  what  extent  effort  predictions  obtained  using  cross-company  (CC)  datasets  are  effective  in  relation  to  the  predictions  obtained  using  single-company  (SC)  datasets  within  the  domain  of  web  projects.  Method:  This  study  uses  the  Tukutuku  database.  We  employed  data  on  125  projects  from  eight  different  companies  and  built  cross  and  single-company  models  with  stepwise  linear  regression  (SWR)  with  and  without  relevancy  filtering.  We  also  benchmarked  these  models  against  mean  and  median  based  models.  We  report  a  case-by-case  analysis  per  company  as  well  as  a  meta-analysis  of  the  findings.  Results:  Results  showed  that  CC  models  provided  poor  predictions  and  performed  significantly  worse  than  SC  models.  However,  relevancy  filtered  CC  models  yielded  comparable  results  to  that  of  SC  models.  These  results  corroborate  with  previous  research.  An  interesting  result  was  that  the  median-based  models  were  consistently  better  than  other  models.  Conclusions:  We  conclude  that  companies  that  carry  out  Web  development  may  use  a  median-based  CC  model  for  prediction  until  it  is  possible  for  the  company  to  build  its  own  SC  model,  which  can  be  used  by  itself  or  in  combination  with  median-based  estimations.
1	Current  state  of  research  on  continuous  experimentation  a  systematic  mapping  study.  The  systematic  evaluation  of  ideas  by  experiments  are  the  foundation  of  continuous  experimentation.  It  allows  to  assess  the  value  of  an  idea,  remove  guessing  and  subjective  opinions  from  the  discussion.  The  enormous  interest  of  it  by  practitioners  and  researchers  let  the  body  of  knowledge  consistently  grow.  New  framework,  methods  and  techniques  are  developed  and  its  application  is  constantly  expanded  to  new  fields  like  cyber-physical  systems  or  social  networks.  In  this  paper  we  present  a  systematic  mapping  study  to  characterize  the  current  state  of  research  on  continuous  experimentation.  Our  study  analyzes  the  following  aspects:  intensity  of  research  activity  and  industry-academia  collaboration,  influential  authors  and  publications,  frequent  research  types  and  topics,  kind  of  contributions  and  terms  used  for  continuous  experimentation.  Our  findings  show  amongst  others  that  the  intensity  of  research  activities  increases  consistently,  the  collaboration  between  industry  and  academia  is  high  and  the  most  cited  publications  are  experience  reports  from  practitioners.
1	Dynamic  adaptation  of  cloud  computing  applications.  Cloud-based  applications  are  composed  of  services  offered  by  distinct  third-party  cloud  providers.  As  most  cloudrelated  information  (i.e.  properties  of  the  services  such  as  price,  availability,  response  time,  etc.)  of  the  services  are  dynamic  and  may  change  any  time  during  the  application  execution,  it  is  essential  to  adapt  the  application  upon  the  detection  of  QoS  violations  that  affect  the  application  requirements.  In  this  paper  we  present  a  dynamic  adaptation  approach  managed  by  an  autonomic  control  loop  that  takes  place  when  a  service  becomes  unavailable  or  when  QoS  parameters  are  degraded.  Our  dynamic  adaptation  approach  relies  on  dynamic  aspect-oriented  programming  (DAOP)  to:  (i)  encapsulate  the  dynamic  adaptation  (removal  and/or  insertion  of  services)  as  an  aspect  that  contains  join  points  that  specify  where  each  aspect  must  act,  and;  (ii)  easily  change  the  application  by  dynamically  removing  a  service  and  inserting  a  new  one.  Keywords-Cloud  Computing,  Software  Product  Lines,  AspectOriented  Programming,  Dynamic  Adaptation,  Autonomic
1	Proposing  a  software  process  model  for  follow  the  sun  development.  Many  software  organizations  are  restructuring  their  software  development  groups  by  extending  operations  to  offshore  software  development  centers.  Thus,  a  Follow  the  Sun  (FTS)  development  is  a  potential  strategy  for  these  organizations.  FTS  can  help  with  reducing  the  software  development  life  cycle  duration  and  consequently  the  time-to-market.  However,  while  the  FTS  concept  looks  promising  in  theory,  it  appears  to  be  difficult  in  practice  and  software  organizations  have  a  pressing  need  for  support  in  how  to  successfully  implement  FTS  in  a  global  software  environment.  In  this  paper,  we  combine  the  results  from  prior  work  in  FTS  and  a  design  validation  method  conducted  by  experts  to  propose  a  software  process  model  for  FTS  development,  named  FTS-SPM  (Follow  the  Sun  Software  Process  Model).  Our  paper  describes  how  we  built  the  FTS-SPM  and  draws  recommendations  for  software  organizations  interested  in  practicing  FTS.
1	Causes  of  architecture  changes  an  empirical  study  through  the  communication  in  oss  mailing  lists.  Understanding  the  causes  of  architecture  changes  allows  us  to  devise  means  to  prevent  architecture  knowledge  vaporization  and  architecture  degeneration.  But  the  causes  are  not  always  known,  especially  in  open  source  software  (OSS)  development.  This  makes  it  very  hard  to  understand  the  underlying  reasons  for  the  architecture  changes  and  design  appropriate  modifications.  Architecture  information  is  communicated  in  development  mailing  lists  of  OSS  projects.  To  explore  the  possibility  of  identifying  and  understanding  the  causes  of  architecture  changes,  we  conducted  an  empirical  study  to  analyze  architecture  information  (i.e.,  architectural  threads)  communicated  in  the  development  mailing  lists  of  two  popular  OSS  projects:  Hibernate  and  ArgoUML,  verified  architecture  changes  with  source  code,  and  identified  the  causes  of  architecture  changes  from  the  communicated  architecture  information.  The  main  findings  of  this  study  are:  (1)  architecture  information  communicated  in  OSS  mailing  lists  does  lead  to  architecture  changes  in  code;  (2)  the  major  cause  for  architecture  changes  in  both  Hibernate  and  ArgoUML  is  preventative  changes.  (3)  more  than  45%  of  architecture  changes  in  both  projects  happened  before  the  first  stable  version  was  released,  which  indicates  that  the  architectures  of  the  investigated  OSS  projects  are  relatively  stable  after  the  first  stable  release.  Keywords-architecture  change;  cause  of  change;  open  source  software;  mailing  list;  communication
1	Reusing  functional  testing  in  order  to  decrease  performance  and  stress  testing  costs.  This  work  presents  an  experimental  study  of  an  idea  related  to  the  automatic  generation  of  performance  and  stress  testing  by  reusing  functional  testing.  The  idea  was  implemented  in  a  tool  named  FERRARE  GT.  This  tool  is  able  to  generate  both  test  scripts  as  well  as  the  data  required  for  their  execution.  In  this  study  we  verified  that  the  use  of  the  method  can  generate  benefits  related  to  cost  reduction,  from  the  reduction  of  test  effort  and,  at  the  same  time,  benefits  related  to  test  quality,  from  the  improvement  of  the  test  relevance  for  the  software  development.
1	Secure  outsourcing  algorithm  of  polynomials  in  cloud  computing.  In  the  era  of  information  explosion,  people  have  to  deal  with  huge  amount  of  data.  It  is  a  great  computation  burden  for  the  resource-  constrained  clients.  Cloud  computing  connects  large  amounts  of  network  resources,  and  forms  a  vast  pool  of  resources.  It  provides  much  con-  venience  for  people.  Clients  can  outsource  the  complex  computation  task  to  the  powerful  cloud  server.  In  this  way,  the  computation  burden  of  clients  can  be  greatly  reduced.  In  this  paper  a  new  algorithm  of  secure  outsourcing  for  polyno-  mials  is  proposed.  In  the  computation  process,  the  computation  polynomial  is  hidden  to  cloud  server,  and  the  inputs  and  outputs  of  polynomi-  als  will  not  revealed.  In  addition,  clients  can  ver-  ify  the  result  easily.
1	A  comparative  study  of  different  strategies  for  predicting  software  quality.  Various  methods  have  been  developed  for  improving  the  quality  of  a  software  product,  especially  for  high-assurance  and  missioncritical  software  systems.  One  commonly  used  approach  is  software  quality  modeling,  in  which  software  practitioners  utilize  software  metrics  and  defect  data  collected  during  the  software  development  process  to  build  defect  prediction  models  that  will  help  to  find  poor-quality  program  modules.  Those  modules  predicted  to  be  fault-prone  will  receive  more  inspection  and  testing,  thereby  improving  their  quality.  Efficacy  of  defect  prediction  models  is  influenced  by  relevance  between  software  metrics  and  fault  data.  Usually  not  all  software  metrics  in  data  repositories  contribute  equally  to  the  occurrence  of  faults.  Choosing  the  most  important  metrics  (features)  prior  to  the  model  training  process  is  needed  to  improve  the  effectiveness  of  defect  predictors.  In  this  paper,  we  study  18  filter-based  feature  selection  techniques  and  evaluate  their  effectiveness  through  a  case  study  performed  on  16  different  software  data  sets.  Among  the  18  techniques,  six  of  them  are  standard  filter-based  methods,  while  11  of  them  are  threshold-based  feature  selection  (TBFS)  techniques  proposed  by  our  research  team  recently.  The  last  one  is  based  on  signal  to  noise  ratio  (S2N),  which  is  a  widely  used  concept  in  electrical  and  communication  engineering,  but  which  is  rarely  used  in  feature  selection.  The  experimental  results  demonstrate  that  the  TBFS  techniques  perform  similarly  to  the  standard  techniques  and  the  S2N  technique  shows  significantly  better  performance  than  the  other  17  approaches.
1	Reasoning  at  runtime  using  time  distorted  contexts  a  models  run  time  based  approach.  Intelligent  systems  continuously  analyze  their  context  to  autonomously  take  corrective  actions.  Building  a  proper  knowledge  representation  of  the  context  is  the  key  to  take  adequate  actions.  This  requires  numerous  and  complex  data  models,  for  example  formalized  as  ontologies  or  meta-models.  As  these  systems  evolve  in  a  dynamic  context,  reasoning  processes  typically  need  to  analyze  and  compare  the  current  context  with  its  history.  A  common  approach  consists  in  a  temporal  discretization,  which  regularly  samples  the  context  (snapshots)  at  specific  timestamps  to  keep  track  of  the  history.  Reasoning  processes  would  then  need  to  mine  a  huge  amount  of  data,  extract  a  relevant  view,  and  finally  analyze  it.  This  would  require  lots  of  computational  power  and  be  time-consuming,  conflicting  with  the  near  real-time  response  time  requirements  of  intelligent  systems.  This  paper  introduces  a  novel  temporal  modeling  approach  together  with  a  time-relative  navigation  between  context  concepts  to  overcome  this  limitation.  Similarly  to  time  distortion  theory,  our  approach  enables  building  time-distorted  views  of  a  context,  composed  by  elements  coming  from  different  times,  which  speeds  up  the  reasoning.  We  demonstrate  the  efficiency  of  our  approach  with  a  smart  grid  load  prediction  reasoning  engine.  Keywords—Temporal  data,  Time-aware  context  modeling,  Knowledge  representation,  Reactive  systems,  Intelligent  systems
1	There  s  never  enough  time  doing  requirements  under  resource  constraints  and  what  requirements  engineering  can  learn  from  agile  development.  While  Requirements  Engineering  textbooks  state  that  a  requirements  specification  must  be  complete,  in  real-life  projects  we  are  always  starting  too  late,  with  too  few  resources,  so  we  can't  do  everything.  The  software  development  community  has  solved  a  similar  problem  (not  having  enough  resources  to  implement  everything  that  was  asked  for)  by  introducing  agile  development  methods,  which  offer  ways  of  segmenting  the  overall  project,  and  choosing  which  parts  to  allocate  resources  to.  This  paper  is  about  how  insights  from  that  agile  development  community  can  be  applied  to  requirements  engineering  activities  for  any  (agile  or  non-agile)  development  project.  Key  terms  in  agile  development,  such  as  “working  product”  and  “user  story”,  must  be  mapped  intelligently  to  terms  in  requirements  engineering  —  and  not  simply  copied:  the  “product”  of  requirements  engineering  is  not  the  same  as  the  “product”  being  implemented  by  developers.
1	Requirements  elicitation  and  derivation  of  security  policy  templates  an  industrial  case  study.  The  technical  or  organizational  enforcement  of  security  policies  is  a  necessity  for  modern  enterprises  such  as  DATEV  eG.  However,  security  policy  specification  is  challenging,  especially  for  users  inexperienced  in  security.  The  provision  of  project-and  domain-specific  security  policy  templates  can  support  users  in  the  specification  of  security  policies.  However,  existing  elicitation  approaches  focus  on  general  security  requirements  or  risk  assessment  and  do  not  support  domain-specific  policy  template  derivation.  In  this  paper,  we  present  a  methodology  for  eliciting  and  deriving  such  security  policy  templates.  We  use  and  adapt  established  techniques  known  from  requirements  engineering  to  elicit  assets,  threats,  and  countermeasures.  The  policy  templates,  derived  from  the  gathered  information,  are  used  to  instantiate  domain-specific  security  requirements  at  run-time.  We  successfully  applied  our  method  in  an  industrial  case  study  at  DATEV  eG  to  show  its  principle  applicability.
1	Non  functional  requirements  for  machine  learning  challenges  and  new  directions.  Machine  Learning  (ML)  provides  approaches  which  use  big  data  to  enable  algorithms  to  "learn",  producing  outputs  which  would  be  difficult  to  obtain  otherwise.  Despite  the  advances  allowed  by  ML,  much  recent  attention  has  been  paid  to  certain  qualities  of  ML  solutions,  particularly  fairness  and  transparency,  but  also  qualities  such  as  privacy,  security,  and  testability.  From  a  requirements  engineering  (RE)  perspective,  such  qualities  are  also  known  as  non-functional  requirements  (NFRs).  In  RE,  the  meaning  of  certain  NFRs,  how  to  refine  those  NFRs,  and  how  to  use  NFRs  for  design  and  runtime  decision  making  over  traditional  software  is  relatively  well  established  and  understood.  However,  in  a  context  where  the  solution  involves  ML,  much  of  our  knowledge  about  NFRs  no  longer  applies.  First,  the  types  of  NFRs  we  are  concerned  with  undergo  a  shift:  NFRs  like  fairness  and  transparency  become  prominent,  whereas  other  NFRs  such  as  modularity  may  become  less  relevant.  The  meanings  and  interpretations  of  NFRs  in  an  ML  context  (e.g.,  maintainability,  interoperability,  and  usability)  must  be  rethought,  including  how  these  qualities  are  decomposed  into  sub-qualities.  Trade-offs  between  NFRs  in  an  ML  context  must  be  re-examined.  Beyond  the  changing  landscape  of  NFRs,  we  can  ask  if  our  known  approaches  to  understanding,  formalizing,  modeling,  and  reasoning  over  NFRs  at  design  and  runtime  must  also  be  adjusted,  or  can  be  applied  as-is  to  this  new  area?  Given  these  questions,  this  work  outlines  challenges  and  a  proposed  research  agenda  for  the  exploration  of  NFRs  for  ML-based  solutions.
1	Symboleo  towards  a  specification  language  for  legal  contracts.  Legal  contracts  specify  the  terms  and  conditions  (in  essence,  requirements)  that  apply  to  business  transactions.  Smart  contracts  are  software  systems  that  monitor  and  control  the  execution  of  contracts  to  ensure  compliance.  This  paper  proposes  a  formal  specification  language  for  contracts,  called  Symboleo,  where  contracts  consist  of  collections  of  obligations  and  powers  that  define  the  legal  contract’s  compliant  executions.  The  formal  semantics  of  Symboleo  is  based  on  an  extension  of  an  ontology  for  Law  and  is  described  in  terms  of  logical  axioms  on  statecharts  that  describe  the  lifetimes  of  contracts,  obligations  and  powers.  Our  proposal  includes  a  preliminary  evaluation  through  the  specification  of  a  real  life-inspired  Sale-of-Goods  contract,  with  a  prototype  execution  engine.  We  envision  this  language  to  enable  formally  verifying  contracts  to  detect  requirements-level  issues  and  to  generate  executable  smart  contracts  (e.g.,  on  blockchain  technology).
1	Openargue  supporting  argumentation  to  evolve  secure  software  systems.  When  software  systems  are  verified  against  security  requirements,  formal  and  informal  arguments  provide  a  structure  for  organizing  the  software  artifacts.  Our  recent  work  on  the  evolution  of  security-critical  software  systems  demonstrates  that  our  argumentation  technique  is  useful  in  limiting  the  scope  of  change  and  in  identifying  changes  to  security  properties.  In  support  of  this  work,  we  have  developed  OpenArgue,  a  tool  for  syntax  checking,  visualizing,  formalizing,  and  reasoning  about  incremental  arguments.  OpenArgue  has  been  integrated  with  requirements  engineering  tools  for  Problem  Frames  and  i∗,  and  applied  to  an  Air  Traffic  Management  (ATM)  case  study.
1	Learning  based  testing  for  reactive  systems  using  term  rewriting  technology.  We  show  how  the  paradigm  of  learning-based  testing  (LBT)  can  be  applied  to  automate  specification-based  black-box  testing  of  reactive  systems  using  term  rewriting  technology.  A  general  model  for  a  reactive  system  can  be  given  by  an  extended  Mealy  automata  (EMA)  over  an  abstract  data  type  (ADT).  A  finite  state  EMA  over  an  ADT  can  be  efficiently  learned  in  polynomial  time  using  the  CGE  regular  inference  algorithm,  which  builds  a  compact  representation  as  a  complete  term  rewriting  system.  We  show  how  this  rewriting  system  can  be  used  to  model  check  the  learned  automaton  against  a  temporal  logic  specification  by  means  of  narrowing.  Combining  CGE  learning  with  a  narrowing  model  checker  we  obtain  a  new  and  general  architecture  for  learningbased  testing  of  reactive  systems.  We  compare  the  performance  of  this  LBT  architecture  against  random  testing  using  a  case  study.
1	Synchronizing  heuristics  speeding  up  the  slowest.  Computing  a  shortest  synchronizing  word  of  an  automaton  is  an  NP–hard  problem.  Therefore,  heuristics  are  used  to  compute  short  synchronizing  words.  SynchroP  is  among  the  best  heuristics  in  the  literature  in  terms  of  word  lengths.  The  heuristic  and  its  variants  such  as  SynchroPL  have  been  frequently  used  as  a  baseline  to  judge  the  quality  of  the  words  generated  by  the  new  heuristics.  Although,  its  quality  is  good,  the  heuristics  are  significantly  slow  especially  compared  to  much  cheaper  heuristics  such  as  Greedy  and  Cycle.  This  makes  them  infeasible  for  large-scale  automatons.  In  this  paper,  we  show  how  one  can  improve  the  time  performance  of  SynchroP  and  its  variants  by  avoiding  unnecessary  computations  which  makes  these  heuristics  more  competitive  than  they  already  are.  Our  experimental  results  show  that  for  2500  states,  SynchroP  can  be  made  70–\(160\times  \)  faster,  via  the  proposed  optimizations.  In  particular,  for  2500  states  and  32  letters,  the  SynchroP  execution  reduces  to  66  s  from  4745  s.  Furthermore,  the  suggested  optimizations  become  more  effective  as  the  number  of  states  in  the  automata  increase.
1	Interactive  voice  uncertainties  for  emergency  communication  suspends  automation.  Freedom  Fone  (FF)  is  an  Interactive  Voice  Response  (IVR)  System  that  integrates  with  the  Global  System  for  Mobile  (GSM)  telecommunications  [1].  Sahana  is  a  disaster  information  management  expert  system  working  with  Internet  technologies  [2].  The  Project  intent  was  to  mediate  information  between  the  FF  and  Sahana  through  the  Emergency  Data  Exchange  Language  (EDXL)  interoperable  content  standard  [3].  It  goal  was  to  equip  Sarvodaya,  Sri  Lanka's  largest  humanitarian  organization,  with  voice-enabled  disaster  communication.  The  3.52  Mean  Opinion  Score  (MOS)  for  voice  quality  was  an  early  automation  challenge  in  introducing  Automatic  Speech  Recognition  (ASR).  A  4.0  MOS  was  determined  as  a  cut-point  for  classifying  reliable  voice  data  [4].  The  Percent  Difficult  (PD)  in  an  emulated  speaker-independent  scenario  was  29.44%  and  a  speaker-dependent  scenario  was  13.24%.  Replacing  human  operators  with  ASR  software  proved  inefficient  [5]  and  [6].  This  paper  discusses  uncertainties  that  are  barriers  to  integrating  voice  enabled  automated  emergency  communication  services  for  response  resource  analysis  and  decision  support.
1	The  most  promising  scheduling  algorithm  to  provide  guaranteed  qos  to  all  types  of  traffic  in  multiservice  4g  wireless  networks.  There  is  a  growing  interest  of  wireless  operators  to  migrate  their  existing  3G  networks  to  different  4G  technologies  such  as  WiMAX  and  LTE.  In  this  heterogeneous  environment  of  wireless  networks  and  architectures,  one  of  the  major  concerns  is  how  to  allocate  network  resources  efficiently  to  diverse  traffic  classes  with  different  QoS  constraints.  Further  it  has  been  convincingly  demonstrated  through  numerous  high  quality  studies  that  multimedia  traffic  found  in  modern  wireless  networks  exhibits  Long-Range  Dependence  (LRD)  and  self-similarity,  a  phenomenon  which  cant'  be  captured  by  traditional  traffic  modeling  based  on  simplistic  Poisson  model.  Unlike  most  existing  studies  that  are  primarily  based  on  simplistic  Poisson  model  and  traditional  scheduling  algorithms,  this  research  presents  an  analytical  performance  model  for  multiple  queue  systems  with  self-similar  traffic  input  scheduled  by  a  novel  and  promising  scheduling  mechanism.  Our  proposed  model  is  substantiated  on  G/M/1  queuing  system  that  considers  multiple  classes  of  traffic  exhibiting  long-range  dependence  and  self-similar  characteristics.  We  analyze  the  model  on  the  basis  of  newly  proposed  scheduling  scheme.  We  present  closed  form  expressions  of  expected  waiting  times  for  multiple  traffic  classes.  We  develop  a  finite  queue  Markov  chain  for  the  proposed  scheduling  scheme.  We  develop  a  discrete  event  simulator  to  understand  the  behavior  of  multiple  classes  of  self-similar  traffic  under  this  newly  proposed  scheduling  mechanism.  The  results  indicate  that  our  proposed  scheduling  algorithm  provides  preferential  treatment  to  real-time  applications  such  as  voice  and  video  but  not  to  that  extent  that  data  applications  are  starving  for  bandwidth  and  outperforms  all  other  scheduling  schemes  that  are  available  in  the  market.
1	A  review  paper  on  human  computer  interaction.  The  advancement  in  the  development  of  computer  technology  has  led  to  the  idea  of  human  computer  interaction.  Research  experiments  in  human  computer  interaction  involves  the  young  age  group  of  people  that  are  educated  and  technically  knowledgeable.  This  paper  focuses  on  the  mental  model  in  Human  Computer  Interaction.  There  are  various  approaches  of  this  review  paper  and  one  of  them  is  highlighting  current  approach,  results  and  the  trends  in  the  human  computer  interaction  and  the  second  approach  is  to  find  out  the  research  that  have  been  invented  a  long  time  before  and  are  currently  lagging  behind.  This  paper  also  focuses  on  the  emotional  intelligence  of  a  user  to  become  more  user  like,  fidelity  prototyping.  The  development  and  design  of  an  automated  system  that  perform  such  task  is  still  being  accomplished.
1	Fault  based  testing  for  refinement  in  csp.  The  process  algebra  CSP  has  been  studied  as  a  modeling  notation  for  test  derivation.  Work  has  been  developed  using  its  trace  and  failure  semantics,  and  their  refinement  notions  as  conformance  relations.  In  this  paper,  we  propose  a  procedure  for  online  test  generation  for  selection  of  finite  test  sets  for  traces  refinement  from  CSP  models,  based  on  the  notion  of  fault  domains,  that  is,  focusing  on  the  set  of  faulty  implementations  of  interest.  We  investigate  scenarios  where  the  verdict  of  a  test  campaign  can  be  reached  after  a  finite  number  of  test  executions.  We  illustrate  the  usage  of  the  procedure  with  a  small  case  study.
1	Using  knapsack  problem  model  to  design  a  resource  aware  test  architecture  for  adaptable  and  distributed  systems.  This  work  focuses  on  testing  the  consistency  of  distributed  and  adaptable  systems.  In  this  context,  Runtime  Testing  which  is  carried  out  on  the  final  execution  environment  is  emerging  as  a  new  solution  for  quality  assurance  and  validation  of  these  systems.  This  activity  can  be  costly  and  resource  consuming  especially  when  execution  environment  is  shared  between  the  software  system  and  the  test  system.  To  overcome  this  challenging  problem,  we  propose  a  new  approach  to  design  a  resource  aware  test  architecture.  We  consider  the  best  usage  of  available  resources  (such  as  CPU  load,  memory,  battery  level,  etc.)  in  the  execution  nodes  while  assigning  the  test  components  to  them.  Hence,  this  work  describes  basically  a  method  for  test  component  placement  in  the  execution  environment  based  on  an  existing  model  called  Multiple  Multidimensional  Knapsack  Problem.  A  tool  based  on  the  constraint  programming  Choco  library  has  been  also  implemented.
1	The  intruder  detection  system  for  rapid  transit  using  cctv  surveillance  based  on  histogram  shapes.  This  paper  presents  the  intruder  detection  system  for  rapid  transit  using  CCTV  surveillance  based  on  the  histogram  shapes.  In  this  paper,  researchers  proposed  intruder  detection  algorithm  of  yellow  line  located  on  the  ground  next  to  rapid  transit  railway  for  preventing  passengers  from  any  harmful  train  incidents  by  using  CCTV  surveillance  system  based  histogram  shape  that  is  incredibly  convenient  technique  for  image  analysis.  The  histogram  shapes  of  trespass  and  non-trespass  are  different.  Therefore,  it  can  be  used  to  alarm  as  a  warning  system  to  the  passengers  who  invade  the  yellow  line.  The  good  advantages  of  the  histogram  shapes  method  are;  flexibility  in  use  and  stability  in  light  changing.  This  research  is  suitable  for  CCTV  surveillance  system  used  in  observation  mode  for  intruder  detection.  The  system  can  be  worked  both  on  real  time  and  offline  mode.  The  experimental  results  show  the  error  of  the  system  that  is  less  than  5%.
1	Dynamic  load  balancing  on  gpu  clusters  for  large  scale  k  means  clustering.  K-Means  is  the  clustering  algorithm  which  is  widely  used  in  many  areas  such  as  information  retrieval,  computer  vision  and  pattern  recognition.  With  the  recent  advance  in  General  Purpose  Graphics  Processing  Unit  (GPGPU),  we  can  use  a  modern  GPU  which  is  capable  to  do  computation  up  to  Tflops  to  calculate  K-Means  clustering  on  average  problems.  However,  due  to  the  exponential  growth  of  data,  the  K-Means  clustering  on  a  single  GPU  will  not  be  adequate  for  large  datasets  in  the  near  future.  In  this  paper,  we  present  the  design  and  implementation  of  an  efficient  large-scale  parallel  K-Means  on  GPU  clusters.  We  utilize  the  massive  parallelism  in  GPUs  to  speed  up  the  most  time  consuming  part  of  K-Means  clustering  in  each  node.  We  employ  the  dynamic  load  balancing  to  distribute  workload  equally  on  different  GPUs  installed  in  the  clusters  so  as  to  improve  the  performance  of  the  parallel  K-Means  at  the  inter-node  level.  We  also  take  advantage  from  software  distributed  shared  memory  to  simplify  the  communication  and  collaboration  among  nodes.  The  result  of  the  evaluation  shows  the  performance  improvement  of  the  parallel  K-Means  by  maintaining  load  balance  on  GPU  clusters.
1	Implementing  multiplayer  pervasive  installations  based  on  mobile  sensing  devices  field  experience  and  user  evaluation  from  a  public  showcase.  In  this  work  we  discuss  Fun  in  Numbers,  a  software  platform  for  implementing  multiplayer  games  and  interactive  installations,  that  are  based  on  the  use  of  ad  hoc  mobile  sensing  devices.  We  utilize  a  detailed  log  of  a  three-day  long  public  showcase  as  a  basis  to  discuss  the  implementation  issues  related  to  a  set  of  games  and  installations,  which  are  examples  of  this  unique  category  of  applications,  utilizing  a  blend  of  technologies.  We  discuss  their  fundamental  concepts  and  features,  also  arguing  that  they  have  many  aspects  and  potential  uses.  The  architecture  of  the  platform  and  implementation  details  are  highlighted  in  this  work,  along  with  detailed  descriptions  of  the  protocols  used.  Our  experiments  shed  light  on  a  number  of  key  issues,  such  as  network  scaling  and  real-time  performance,  and  we  provide  experiments  regarding  cross-layer  software  issues.  We  additionally  provide  data  showing  that  such  games  and  installations  can  be  efficiently  supported  by  our  platform,  with  as  many  as  50  concurrent  players  in  the  same  physical  space.  These  results  are  backed  up  by  a  user  evaluation  study  from  a  large  sample  of  136  visitors,  which  shows  that  such  applications  can  be  seriously  fun.
1	Practitioner  perceptions  of  open  source  software  in  the  embedded  systems  area.  There  is  a  growing  body  of  research  to  show  that,  with  the  advent  of  so-called  professional  Open  Source,  attitudes  within  many  organisations  towards  adopting  Open  Source  software  have  changed.  However,  there  have  been  conflicting  reports  on  the  extent  to  which  this  is  true  of  the  embedded  software  systems  sector-a  large  sector  in  Europe.  This  paper  reports  on  attitudes  towards  Open  Source  software  within  that  sector.  Our  results  show  a  high  level  of  acceptance  of  Open  Source  products  with  large,  well  established  communities,  and  not  only  at  the  level  of  the  operating  system.  Control  over  the  software  is  seen  as  fundamentally  important.  Other  key  perceptions  with  Open  Source  are  an  easing  of  long-term  maintenance  problems  and  ready  availability  of  support.  The  classical  strengths  of  Open  Source,  namely  mass  inspection,  ease  of  conducting  trials,  longevity  and  source  code  access  for  debugging,  were  at  the  forefront  of  thinking.  However,  there  was  an  acknowledgement  that  more  guidelines  are  needed  for  assessing  Open  Source  software  and  incorporating  it  into  products.
1	Comprehensible  software  fault  and  effort  prediction.  HighlightsWe  argue  that  comprehensibility  is  crucial  in  software  effort  and  fault  prediction.We  extracted  new  datasets  based  on  the  Android  repository.ALPA  extracts  a  tree  that  mimics  the  performance  of  the  complex  model.The  extracted  trees  are  not  only  comprehensible  but  also  more  accurate.  Software  fault  and  effort  prediction  are  important  tasks  to  minimize  costs  of  a  software  project.  In  software  effort  prediction  the  aim  is  to  forecast  the  effort  needed  to  complete  a  software  project,  whereas  software  fault  prediction  tries  to  identify  fault-prone  modules.  In  this  research  both  tasks  are  considered,  thereby  using  different  data  mining  techniques.  The  predictive  models  not  only  need  to  be  accurate  but  also  comprehensible,  demanding  that  the  user  can  understand  the  motivation  behind  the  model's  prediction.  Unfortunately,  to  obtain  predictive  performance,  comprehensibility  is  often  sacrificed  and  vice  versa.  To  overcome  this  problem,  we  extract  trees  from  well  performing  Random  Forests  (RFs)  and  Support  Vector  Machines  for  regression  (SVRs)  making  use  of  a  rule  extraction  algorithm  ALPA.  This  method  builds  trees  (using  C4.5  and  REPTree)  that  mimic  the  black-box  model  (RF,  SVR)  as  closely  as  possible.  The  proposed  methodology  is  applied  to  publicly  available  datasets,  complemented  with  new  datasets  that  we  have  put  together  based  on  the  Android  repository.  Surprisingly,  the  trees  extracted  from  the  black-box  models  by  ALPA  are  not  only  comprehensible  and  explain  how  the  black-box  model  makes  (most  of)  its  predictions,  but  are  also  more  accurate  than  the  trees  obtained  by  working  directly  on  the  data.
1	Setz  logistics  models  and  system  framework  for  manufacturing  and  exporting  large  engineering  assets.  Given  the  dynamic  and  increasingly  competitive  nature  of  international  commerce,  manufacturing  companies  must  plan  global  logistics  operations  for  sustainable  competitive  advantage.  Many  enterprises  build  collaborative  manufacturing  networks  across  multinational  regions  to  reduce  production  costs  and  gain  access  to  new  and  often  unfamiliar  markets.  Recognizing  the  strategic  importance  of  globalization,  government  agencies  are  developing  Special  Economic  Trade  Zones  (SETZ).  These  zones  are  regulated  industrial  districts  which  encourage  manufacturing  by  offering  incentives  such  as  new  logistic  designs  linked  with  the  latest  information  technologies.  The  objective  of  this  paper  is  to  analyze  and  design  SETZ  logistics  hub  models  and  system  framework  for  linking  manufacturers.  By  defining  the  characteristics  of  the  different  types  of  specialized  trade  zones,  and  briefly  discussing  the  older  types  of  trade  models  that  are  no  longer  competitive,  this  research  uses  the  case  of  a  Taiwan  power  transformer  manufacturer  to  analyze  the  supply  chain  logistics  processes  for  manufacturing  and  exporting  large  engineering  assets  within  a  SETZ.  The  logistics  models  and  information  system  framework  developed  provide  a  general  reference  for  other  governments,  companies,  and  industrial  sectors  that  intend  to  design  export-oriented  industrial  parks  incorporating  IT-centric  and  globally  oriented  SETZ  techniques.
1	A  survey  of  schedulability  analysis  techniques  for  rate  dependent  tasks.  Abstract  In  automotive  embedded  real-time  systems,  such  as  the  engine  control  unit,  there  are  tasks  that  are  activated  whenever  the  crankshaft  arrives  at  a  specific  angular  position.  As  a  consequence  the  frequency  of  activation  changes  with  the  crankshaft’s  angular  speed  (i.e.,  engine  rpm).  Additionally,  execution  times  and  deadlines  may  also  depend  on  angular  speeds  and  positions.  This  paper  provides  a  survey  on  schedulability  analysis  techniques  for  tasks  with  this  rate-dependent  behaviour.  It  covers  different  task-models  and  analysis  methods  for  both  fixed  priority  and  earliest  deadline  first  scheduling.  A  taxonomy  of  the  different  analysis  methods,  classifying  them  according  to  the  assumptions  made  and  the  precision  of  the  analysis,  is  provided  at  the  end  of  the  paper.
1	Parallel  construction  of  interprocedural  memory  ssa  form.  Abstract  Interprocedural  memory  SSA  form,  which  provides  a  sparse  data-flow  representation  for  indirect  memory  operations,  paves  the  way  for  many  advanced  program  analyses.  Any  performance  improvement  for  memory  SSA  construction  benefits  for  a  wide  range  of  clients  (e.g.,  bug  detection  and  compiler  optimisations).  However,  its  construction  is  much  more  expensive  than  that  for  scalar-based  SSA  form.  The  memory  objects  distinguished  at  a  pointer  dereference  significantly  increases  the  number  of  variables  that  need  to  be  put  on  SSA  form,  resulting  in  considerable  analysis  overhead  when  analyzing  large  programs  (e.g.,  millions  of  lines  of  code).  This  paper  presents  ParSSA  ,  a  fully  parameterised  approach  for  parallel  construction  of  interprocedural  memory  SSA  form  by  utilising  multi-core  computing  resources.  ParSSA  partitions  whole-program  memory  objects  into  uniquely  identified  memory  regions.  The  indirect  memory  accesses  in  a  function  are  fully  parameterised  using  partitioned  memory  regions,  so  that  the  memory  SSA  construction  of  a  parameterised  function  is  readily  parallelised.  We  implemented  ParSSA  in  LLVM  using  Intel  Threading  Building  Block  (TBB)  for  creating  parallel  tasks.  We  evaluated  ParSSA  using  15  large  applications.  ParSSA  achieves  up  to  6.9 ×   speedup  against  the  sequential  version  on  an  8-core  machine.
1	The  state  of  practice  in  model  driven  engineering.  Despite  lively  debate  over  the  past  decade  on  the  benefits  and  drawbacks  of  model-driven  engineering  (MDE),  there  have  been  few  industry-wide  studies  of  MDE  in  practice.  A  new  study  that  surveyed  450  MDE  practitioners  and  performed  in-depth  interviews  with  22  more  suggests  that  although  MDE  might  be  more  widespread  than  commonly  believed,  developers  rarely  use  it  to  generate  whole  systems.  Rather,  they  apply  MDE  to  develop  key  parts  of  a  system.
1	Listen  then  use  ears.  Applying  the  Easy  Approach  to  Requirements  Syntax  (EARS)  template  can  result  in  a  simple,  clear  requirement.  However,  to  be  able  to  write  a  simple  statement,  you  must  first  understand  what  you  want  the  system  to  do,  which  might  be  difficult.  The  simplicity  of  the  EARS  templates  prevents  engineers  from  hiding  behind  ambiguous  statements  of  what  the  system  must  do.
1	Analyzing  the  harmful  effect  of  god  class  refactoring  on  power  consumption.  Energy  efficiency  and  other  sustainability  issues  are  common  concerns  in  the  material  production  industries  but  rarely  addressed  in  software  development  efforts.  Instead,  traditional  software  development  life  cycles  and  methodologies  place  an  emphasis  on  maintainability  and  other  intrinsic  software  quality  features.  One  standard  practice  is  to  improve  maintainability  by  detecting  bad  smells  in  a  system's  architecture  and  then  applying  refactoring  transformations  to  deal  with  those  smells.  The  refactoring  research  area  is  sufficiently  mature  for  most  techniques  to  achieve  more  maintainable  system  architectures,  but  the  authors  argue  that  they  can  also  lead  to  both  decreased  sustainability  and  increased  power  consumption.  Accordingly,  this  article  analyzes  the  relationship  between  architecture  sustainability  and  maintainability  by  providing  empirical  evidence  of  how  power  consumption  increases  after  refactoring.
1	A  manifesto  for  energy  aware  software.  According  to  recent  estimates,  computing  and  communications  could  account  for  20%  of  energy  usage  globally  by  2025.1  This  trend  shows  no  sign  of  slowing.  The  annual  growth  in  power  consumption  of  Internet-connected  devices  is  20%.  Data  centers  alone  are  now  accounting  for  more  than  3%  of  global  emissions.  Even  if  you  are  not  worried  about  this  trend  on  the  mega  scale,  you  are  likely  concerned  with  the  power  consumption  of  the  devices  in  your  pocket,  on  your  wrist,  and  in  your  ears.  Software,  hardware,  and  network  attributes  all  contribute  to  power  usage,  but  little  attention  has  been  given  to  this  topic  by  the  information  and  communications  technology  (ICT)  community.
1	Remote  pair  programming  in  virtual  reality.  There  are  many  benefits  to  pair  programming,  including  increased  knowledge  transfer,  higher  quality  code,  increased  code  comprehension,  and  team  bonding.  Unfortunately,  when  programmers  work  remotely,  it  becomes  more  challenging  to  collaborate.  Virtual  Reality  (VR)  technology  has  become  increasingly  popular  in  domains  outside  of  software  engineering.  It  allows  humans  to  experience  increased  social  presence,  a  key  for  collaboration,  while  still  working  remotely.  In  the  last  few  years,  and  even  more  so  recently,  remote  work  has  become  very  common  for  software  engineers.  In  this  paper,  we  explore  remote  pair  programming  and  code  comprehension  in  virtual  reality.  We  conducted  remote  pair  programming  experiments  with  a  total  of  40  professional  programmers.  Half  of  the  participants  pair  programmed  using  VR  as  a  tool  and  the  other  half  used  a  screen  sharing  system  to  collaborate.  We  found  that  programmers  solved  almost  twice  as  many  bugs  using  VR.  We  also  found  that  the  time  taken  to  solve  bugs  was,  on  average,  reduced  when  working  in  the  VR  environment  than  using  the  state  of  the  art
1	Failure  patterns  in  operating  systems  an  exploratory  and  observational  study.  Abstract      Sophisticated  critical  computer  applications  need  to  run  on  top  of  operating  system  (OS)  software.  Given  the  natural  intrinsic  dependency  of  user  applications  on  the  OS  software,  OS  failures  can  severely  impact  even  the  most  reliable  applications.  Thus,  it  is  essential  to  understand  how  OS  failures  occur  in  order  to  improve  software  reliability.  In  this  paper,  we  present  an  exploratory  and  observational  study  on  OS  failure  patterns.  We  analyze  7007  real  OS  failures  collected  from  566  computers  used  in  different  workplaces.  We  start  with  a  general  characterization  of  the  failure  dataset  examined  in  this  study,  where  interesting  findings  are  presented,  e.g.,  the  most  frequent  failure  types  per  period  of  a  day  and  per  different  workplaces.  Next,  we  investigate  the  existence  of  failure  patterns.  For  this  purpose,  we  introduce  an  OS  failure  pattern  discovery  protocol  that  identifies  failure  patterns  exhibiting  consistency  across  different  computers  used  in  the  same  as  well  as  different  workplaces.  In  total,  we  discovered  45  failure  patterns  with  153,511  occurrences.  Based  on  these  patterns,  we  found  that  the  most  prevalent  failures  were  related  to  the  software  updates  of  the  OS  components.  The  main  causes  of  these  failures  involved  infrastructural  and  environmental  factors  such  as  disk-space  unavailability  and  concurrent  execution  of  OS  services.  Empirical  evidence  of  time-correlated  failures  of  these  OS  components  is  also  discussed  in  this  paper.  Other  findings  include  the  OS  components  that  contributed  more  to  create  the  discovered  failure  patterns  and  the  most  prevalent  combination  of  failure  events  and  their  temporal  order.  This  study  aims  at  to  contribute  to  a  better  understanding  of  the  mechanisms  behind  OS  failures.
1	Architecture  enforcement  concerns  and  activities  an  expert  study.  Software  architecture  provides  the  high-level  design  of  software  systems  with  the  most  critical  decisions.  The  source  code  of  a  system  has  to  conform  to  the  architectural  decisions  to  guarantee  the  systems’  success  in  terms  of  quality  properties.  Therefore,  architects  have  to  continuously  ensure  that  architecture  decisions  are  implemented  correctly  to  prevent  architecture  erosion.  This  is  the  main  goal  of  Architecture  Enforcement.  For  an  effective  enforcement,  architects  have  to  be  aware  of  the  most  important  enforcement  concerns  and  activities.  Unfortunately,  current  state  of  the  art  does  not  provide  a  concrete  structure  on  how  the  process  of  architecture  enforcement  is  actually  applied  in  industry.  Therefore,  we  conducted  an  empirical  study  in  order  to  gain  insight  in  the  industrial  practice  of  architecture  enforcement.  For  this,  we  interviewed  12  experienced  software  architects  from  different  companies.  As  a  result,  we  identified  the  most  important  concerns  that  software  architects  care  about  during  architecture  enforcement.  Additionally,  we  investigated  which  activities  architects  usually  apply  in  order  to  enforce  those  concerns.
1	Cyber  physical  modelling  in  modelica  with  model  reduction  techniques.  Abstract  Object-oriented  modelling  of  cyber-physical  systems  with  Modelica  and  similar  environments  has  brought  many  advantages,  especially  the  efficient  re-use  of  models  and  thus  the  possibility  of  creating  powerful  multi-domain  libraries.  Unfortunately,  the  models  have  become  highly  complex,  which  causes  serious  problems  during  processing  and  execution.  Consequently,  verification  and  debugging  is  becoming  an  increasingly  challenging  task.  The  continuous  investigation  of  simplifications  and  reductions  in  all  phases  of  model  developments  is  thus  urgent.  The  present  paper  deals  with  reduction  methods  based  on  metric  ranking  and  preserve  realisation,  which  means  that  the  structure  and  the  parameters  of  the  model  remain  physically  interpretable.  Two  model-reduction  methods  are  described  and  implemented  in  Open  Modelica.  The  first  operates  on  a  set  of  differential-algebraic  equations,  and  the  second  is  based  on  modified  bond-graphs-reduction  techniques.  The  latter  approach  is  suitable  for  component-based  models  in  Modelica  that  are  usually  represented  graphically  with  object  diagrams.  The  paper  briefly  describes  the  research  area,  the  problems  of  the  adoption  of  the  developed  model  reduction  techniques  to  the  Modelica  environments,  and  the  final  implementation.  Both  proposed  approaches  are  tested  on  the  model  of  a  car  suspension  system  and  briefly  discussed.
1	Network  aware  embedding  of  virtual  machine  clusters  onto  federated  cloud  infrastructure.  A  virtual  machine  cluster  embedding  algorithm  for  federated  cloud  is  suggested.Suggested  algorithm  optimizes  network  delay  and  bandwidth  allocation.A  framework  for  modeling  of  resource  allocation  in  federated  cloud  is  presented.  Federated  clouds  are  continuously  developing  as  the  demands  of  cloud  users  get  more  complicated.  Contemporary  cloud  management  technologies  like  Open-Stack  (Sefraouiźetźal.,  2012)  and  OpenNebula  (Milojicic  etźal.,  2011)  allow  users  to  define  network  topologies  among  virtual  machines  that  are  requested.  Therefore,  federated  clouds  currently  face  the  challenge  of  network  topology  mapping  in  addition  to  conventional  resource  allocation  problems.  In  this  paper,  topology  based  mapping  of  virtual  machine  clusters  onto  the  federated  cloud  infrastructures  is  studied.  A  novel  algorithm  is  presented  to  perform  the  mapping  operation  that  work  towards  minimizing  network  latency  and  optimizing  bandwidth  utilization.  To  realize  and  evaluate  the  algorithm,  a  widely  used  cloud  simulation  environment,  CloudSim  (Calheiros  etźal.,  2011),  is  extended  to  support  several  additional  capabilities  in  network  and  cost  modeling.  Evaluation  is  performed  by  comparing  the  proposed  algorithm  to  a  number  of  conventional  heuristics  such  as  least  latency  first  and  round-robin.  Results  under  different  request  characteristics  indicate  that  the  proposed  algorithm  performs  significantly  better  than  the  compared  conventional  approaches  regarding  various  QoS  parameters  such  as  inter-cloud  latency  and  throughput.
1	Shorter  hash  based  signatures.  We  give  a  description  of  a  hash-based  signature  scheme  with  shorter  signature  footprint  and  better  processing  times.We  argue  that  our  signature  scheme  is  suitable  for  the  Internet  of  Things.We  describe  an  efficient  implementation  of  the  scheme  for  a  very  constrained  8-bit  AVR  ATmega128l  microcontroller.We  provide  detailed  benchmarks  of  time,  memory  and  energy  for  the  constrained  microcontroller.  We  describe  an  efficient  hash-based  signature  scheme  that  yields  shorter  signatures  than  the  state  of  the  art.  Signing  and  verification  are  faster  as  well,  and  the  overall  scheme  is  suitable  for  constrained  platforms  typical  of  the  Internet  of  Things.  We  describe  an  efficient  implementation  of  our  improved  scheme  and  show  memory,  time,  and  energy  consumption  benchmarks  over  a  real  device,  i.e.  the  ATmega128l  8-bit  AVR  microcontroller  embedded  in  MICAz,  a  typical  sensor  node  used  in  wireless  sensor  networks.
1	A  context  awareness  framework  for  cross  platform  distributed  applications.  HighlightsA  method  for  obtaining  homogenous  contextual  information  in  webinos.A  context  DB  schema  for  Context  Objects,  a  unit  for  storing  contextual  information.A  personal  cloud  storage  where  the  ownership  of  context  data  remains  to  the  end-user.A  secure,  lightweight  platform  to  create  context-aware  webinos-enabled  applications.  With  the  introduction  of  interconnected  cross-platform  middleware,  a  new  area  of  opportunities  for  ubiquitous/pervasive  computing  has  emerged.  Context  aware  applications  can  be  enhanced  to  practically  and  realistically  incorporate  multiple  facets  of  human-machine  interactions  in  everyday  life  that  are  not  limited  to  a  device-centered  model  for  deducing  context.  In  this  paper,  we  propose  that  they  can  rather  extend  this  model  to  a  human-centered,  device  and  platform  independent  model,  based  on  a  personal  distributed  application  and  data  cloud  ecosystem.  For  this  to  be  achieved,  webinos,  a  set  of  web  runtime  extensions  that  enable  web  applications  and  services  to  be  used  and  shared  consistently  and  securely  over  a  broad  spectrum  of  converged  and  connected  devices,  is  used  to  provide  this  ecosystem.  The  webinos  Context  Awareness  Framework  described  here  is  accessible  to  each  webinos-enabled  application.  After  strict  policy  enforcement,  it  can  collect  contextual  information,  either  via  an  automatic  mechanism  that  intercepts  native  calls  made  by  webinos  applications  through  the  various  webinos  APIs,  via  an  automatic  polling  mechanism  to  these  APIs,  or  via  custom,  application-specific  context  schema  extensions.  It  can  then  distribute  the  contextual  information  from  its  own  personal  cloud  storage  mechanism,  in  the  form  of  simple,  manageable  and  intuitive  Context  Objects,  to  and  from  all  webinos-enabled  devices  owned  by  the  same  user,  or  even  other,  authorized  users.
1	A  systematic  mapping  study  on  text  analysis  techniques  in  software  architecture.  Abstract  Context  Information  from  artifacts  in  each  phase  of  the  software  development  life  cycle  can  potentially  be  mined  to  enhance  architectural  knowledge.  Many  text  analysis  techniques  have  been  proposed  for  mining  such  artifacts.  However,  there  is  no  comprehensive  understanding  of  what  artifacts  these  text  analysis  techniques  analyze,  what  information  they  are  able  to  extract  or  how  they  enhance  architecting  activities.  Objective  This  systematic  mapping  study  aims  to  study  text  analysis  techniques  for  mining  architecture-related  artifacts  and  how  these  techniques  have  been  used,  and  to  identify  the  benefits  and  limitations  of  these  techniques  and  tools  with  respect  to  enhancing  architecting  activities.  Method  We  conducted  a  systematic  mapping  study  and  defined  five  research  questions.  We  analyzed  the  results  using  descriptive  statistics  and  qualitative  analysis  methods.  Results  Fifty-five  studies  were  finally  selected  with  the  following  results:  (1)  Current  text  analysis  research  emphasizes  on  architectural  understanding  and  recovery.  (2)  A  spectrum  of  text  analysis  techniques  have  been  used  in  textual  architecture  information  analysis.  (3)  Five  categories  of  benefits  and  three  categories  of  limitations  were  identified.  Conclusions  This  study  shows  a  steady  interest  in  textual  architecture  information  analysis.  The  results  give  clues  for  future  research  directions  on  improving  architecture  practice  through  using  these  text  analysis  techniques.
1	Towards  energy  efficient  scheduling  for  real  time  tasks  under  uncertain  cloud  computing  environment.  HighlightsWe  develop  an  uncertainty-aware  architecture  for  scheduling  real-time  tasks  in  cloud  computing  environment.A  novel  algorithm  named  PRS  that  combines  proactive  with  reactive  scheduling  methods  is  proposed  to  schedule  real-time  tasks.Three  system  scaling  strategies  according  to  dynamic  workloads  are  developed  to  improve  the  resource  utilization  and  reduce  energy  consumption.  Green  cloud  computing  has  become  a  major  concern  in  both  industry  and  academia,  and  efficient  scheduling  approaches  show  promising  ways  to  reduce  the  energy  consumption  of  cloud  computing  platforms  while  guaranteeing  QoS  requirements  of  tasks.  Existing  scheduling  approaches  are  inadequate  for  real-time  tasks  running  in  uncertain  cloud  environments,  because  those  approaches  assume  that  cloud  computing  environments  are  deterministic  and  pre-computed  schedule  decisions  will  be  statically  followed  during  schedule  execution.  In  this  paper,  we  address  this  issue.  We  introduce  an  interval  number  theory  to  describe  the  uncertainty  of  the  computing  environment  and  a  scheduling  architecture  to  mitigate  the  impact  of  uncertainty  on  the  task  scheduling  quality  for  a  cloud  data  center.  Based  on  this  architecture,  we  present  a  novel  scheduling  algorithm  (PRS11Proactive  and  Reactive  Scheduling.)  that  dynamically  exploits  proactive  and  reactive  scheduling  methods,  for  scheduling  real-time,  aperiodic,  independent  tasks.  To  improve  energy  efficiency,  we  propose  three  strategies  to  scale  up  and  down  the  system's  computing  resources  according  to  workload  to  improve  resource  utilization  and  to  reduce  energy  consumption  for  the  cloud  data  center.  We  conduct  extensive  experiments  to  compare  PRS  with  four  typical  baseline  scheduling  algorithms.  The  experimental  results  show  that  PRS  performs  better  than  those  algorithms,  and  can  effectively  improve  the  performance  of  a  cloud  data  center.
1	Fault  aware  management  protocols  for  multi  component  applications.  Abstract  Nowadays,  applications  are  composed  by  multiple  heterogeneous  components,  whose  management  must  be  suitably  coordinated  by  taking  into  account  inter-component  dependencies  and  potential  failures.  In  this  paper,  we  first  present  fault-aware  management  protocols,  which  allow  to  model  the  management  behaviour  of  application  components,  and  we  then  illustrate  how  such  protocols  can  be  composed  to  analyse  and  automate  the  overall  management  of  a  multi-component  application.  We  also  show  how  to  recover  applications  that  got  stuck  because  a  fault  was  not  handled  properly,  or  because  a  component  is  behaving  differently  than  expected.  To  illustrate  the  feasibility  of  our  approach,  we  present  Barrel  ,  a  proof-of-concept  application  that  permits  editing  and  analysing  fault-aware  management  protocols  in  multi-component  applications.  We  also  discuss  the  usefulness  of  Barrel  by  showing  how  it  was  fruitfully  exploited  it  in  a  concrete  case  study  and  in  a  controlled  experiment.
1	Examining  the  effects  of  developer  familiarity  on  bug  fixing.  Abstract  Background:  In  modern  software  systems’  maintenance  and  evolution,  how  to  fix  software  bugs  efficiently  and  effectively  becomes  increasingly  more  essential.  A  deep  understanding  of  developers’/assignees’  familiarity  with  bugs  could  help  project  managers  make  a  proper  allotment  of  maintenance  resources.  However,  to  our  knowledge,  the  effects  of  developer  familiarity  on  bug  fixing  have  not  been  studied.  Aims:  Inspired  by  the  understanding  of  developers’/assignees’  familiarity  with  bugs,  we  aim  to  investigate  the  effects  of  familiarity  on  efficiency  and  effectiveness  of  bug  fixing.  Method:  Based  on  evolution  history  of  buggy  code  lines,  we  propose  three  metrics  to  evaluate  the  developers’/assignees’  familiarity  with  bugs.  Additionally,  we  conduct  an  empirical  study  on  6  well-known  Apache  Software  Foundation  projects  with  more  than  9000  confirmed  bugs.  Results:  We  observe  that  (a)  familiarity  is  one  of  the  common  factors  in  cases  of  bug  fixing:  the  developers  are  more  likely  to  be  assigned  to  fix  the  bugs  introduced  by  themselves;  (b)  familiarity  has  complex  effects  on  bug  fixing:  although  the  developers  fix  the  bugs  introduced  by  themselves  more  quickly  (with  high  efficiency),  they  are  more  likely  to  introduce  future  bugs  when  fixing  the  current  bugs  (with  worse  effectiveness).  Conclusion:  We  put  forward  the  following  suggestions:  (a)  managers  should  assign  some  “outsiders”  to  participate  in  bug  fixing.  (b)  when  developers  deal  with  his  own  code,  managers  should  assign  more  maintenance  resource  (e.g.,  more  inspection)  to  developers.
1	Sla  aware  multiple  migration  planning  and  scheduling  in  sdn  nfv  enabled  clouds.  Abstract  In  Software-Defined  Networking  (SDN)-enabled  cloud  data  centers,  live  migration  is  a  key  approach  used  for  the  reallocation  of  Virtual  Machines  (VMs)  and  Virtual  Network  Functions  (VNFs).  Using  live  migration,  cloud  providers  can  address  their  dynamic  resource  management  and  fault  tolerance  objectives  without  interrupting  the  service  of  users.  However,  performing  multiple  live  migrations  in  arbitrary  order  can  lead  to  service  degradation.  Therefore,  efficient  migration  planning  is  essential  to  reduce  the  impact  of  live  migration  overheads.  In  addition,  to  prevent  Quality  of  Service  (QoS)  degradations  and  Service  Level  Agreement  (SLA)  violations,  it  is  necessary  to  set  priorities  for  different  live  migration  requests  with  various  urgency.  In  this  paper,  we  propose  SLAMIG,  a  set  of  algorithms  that  composes  deadline-aware  multiple  migration  grouping  algorithm  and  on-line  migration  scheduling  to  determine  the  sequence  of  VM/VNF  migrations.  The  experimental  results  show  that  our  approach  with  reasonable  algorithm  runtime  can  efficiently  reduce  the  number  of  deadline  misses  and  has  a  good  migration  performance  compared  with  the  one-by-one  scheduling  and  two  state-of-the-art  algorithms  in  terms  of  total  migration  time,  average  execution  time,  downtime,  and  transferred  data.  We  also  evaluate  and  analyze  the  impact  of  multiple  migrations  on  QoS  and  energy  consumption.
1	Release  early  release  often  and  watch  your  users  emotions  lessons  from  emotional  patterns.  App  stores  are  highly  competitive  markets,  and  unexpected  app  changes  might  incite  even  loyal  users  to  explore  alternative  apps.  In  this  article,  we  present  five  release  lessons,  from  emotional  patterns  identified  using  sentiment  analysis  tools,  to  assist  app  vendors  maintain  positive  emotions  and  gain  competitive  advantages.
1	Injecting  value  thinking  into  prioritization  decisions.  A  proposed  approach  injects  value-thinking  into  feature  prioritization,  using  story  mapping.  The  Web  extra  at  http://youtu.be/Xm5VqODvVZE  is  an  audio  podcast  in  which  author  Jane  Cleland-Huang  provides  an  audio  recording  of  the  Requirements  column,  in  which  she  discusses  an  approach  that  injects  value-thinking  into  feature  prioritization  by  using  story  mapping.
1	Gcad  a  near  miss  clone  genealogy  extractor  to  support  clone  evolution  analysis.  Understanding  the  evolution  of  code  clones  is  important  for  both  developers  and  researchers  to  understand  the  maintenance  implications  of  clones  and  to  design  robust  clone  management  systems.  Generally,  a  study  of  clone  evolution  starts  with  extracting  clone  genealogies  across  multiple  versions  of  a  program  and  classifying  them  according  to  their  change  patterns.  Although  these  tasks  are  straightforward  for  exact  clones,  extracting  the  history  of  near-miss  clones  and  classifying  their  change  patterns  automatically  is  challenging  due  to  the  potential  diverse  variety  of  clone  fragments  even  in  the  same  clone  class.  In  this  tool  demonstration  paper  we  describe  the  design  and  implementation  of  a  near-miss  clone  genealogy  extractor,  gCad,  that  can  extract  and  classify  both  exact  and  near-miss  clone  genealogies.  Developers  and  researchers  can  compute  a  wide  range  of  popular  metrics  regarding  clone  evolution  by  simply  post  processing  the  gCad  results.  gCad  scales  well  to  large  subject  systems,  works  for  different  granularities  of  clones,  and  adapts  easily  to  popular  clone  detection  tools.
1	Which  feature  location  technique  is  better.  Feature  location  is  a  fundamental  step  in  software  evolution  tasks  such  as  debugging,  understanding,  and  reuse.  Numerous  automated  and  semi-automated  feature  location  techniques  (FLTs)  have  been  proposed,  but  the  question  remains:  How  do  we  objectively  determine  which  FLT  is  most  effective?  Existing  evaluations  frequently  use  bug  fix  data,  which  includes  the  location  of  the  fix,  but  not  what  other  code  needs  to  be  understood  to  make  the  fix.  Existing  evaluation  measures  such  as  precision,  recall,  effectiveness,  mean  average  precision  (MAP),  and  mean  reciprocal  rank  (MRR)  will  not  differentiate  between  a  FLT  that  ranks  higher  these  related  elements  over  completely  irrelevant  ones.  We  propose  an  alternative  measure  of  relevance  based  on  the  likelihood  of  a  developer  finding  the  bug  fix  locations  from  a  ranked  list  of  results.  Our  initial  evaluation  shows  that  by  modeling  user  behavior,  our  proposed  evaluation  methodology  can  compare  and  evaluate  FLTs  fairly.
1	A  cost  model  based  on  software  maintainability.  In  this  paper  we  present  a  maintainability  based  model  for  estimating  the  costs  of  developing  source  code  in  its  evolution  phase.  Our  model  adopts  the  concept  of  entropy  in  thermodynamics,  which  is  used  to  measure  the  disorder  of  a  system.  In  our  model,  we  use  maintainability  for  measuring  disorder  (i.e.  entropy)  of  the  source  code  of  a  software  system.  We  evaluated  our  model  on  three  proprietary  and  two  open  source  real  world  software  systems  implemented  in  Java,  and  found  that  the  maintainability  of  these  evolving  software  is  decreasing  over  time.  Furthermore,  maintainability  and  development  costs  are  in  exponential  relationship  with  each  other.  We  also  found  that  our  model  is  able  to  predict  future  development  costs  with  high  accuracy  in  these  systems.
1	Useful  software  engineering  research  leading  a  double  agent  life.  Though  software  engineering  is  in  essence  an  engineering  discipline,  that  is  a  discipline  whose  aim  is  “the  construction  of  machinery  and  other  artifacts  for  use  by  society”,  software  engineering  research  has  always  been  struggling  to  demonstrate  impact.  This  is  reflected  in  part  by  the  funding  challenges  that  the  discipline  faces  in  many  countries,  the  difficulties  we  have  to  attract  industrial  participants  to  our  conferences,  and  the  scarcity  of  papers  reporting  industrial  case  studies.
1	An  experience  report  on  applying  passive  learning  in  a  large  scale  payment  company.  Passive  learning  techniques  infer  graph  models  on  the  behavior  of  a  system  from  large  trace  logs.  The  research  community  has  been  dedicating  great  effort  in  making  passive  learning  techniques  more  scalable  and  ready  to  use  by  industry.  However,  there  is  still  a  lack  of  empirical  knowledge  on  the  usefulness  and  applicability  of  such  techniques  in  large  scale  real  systems.  To  that  aim,  we  conducted  action  research  over  nine  months  in  a  large  payment  company.  Throughout  this  period,  we  iteratively  applied  passive  learning  techniques  with  the  goal  of  revealing  useful  information  to  the  development  team.  In  each  iteration,  we  discussed  the  findings  and  challenges  to  the  expert  developer  of  the  company,  and  we  improved  our  tools  accordingly.  In  this  paper,  we  present  evidence  that  passive  learning  can  indeed  support  development  teams,  a  set  of  lessons  we  learned  during  our  experience,  a  proposed  guide  to  facilitate  its  adoption,  and  current  research  challenges.
1	Efficient  evolutionary  security  analysis  of  interacting  android  apps.  In  parallel  with  the  increasing  popularity  of  mobile  software,  an  alarming  escalation  in  the  number  and  sophistication  of  security  threats  is  observed  on  mobile  platforms,  remarkably  Android  as  the  dominant  platform.  Such  mobile  software,  further,  evolves  incrementally,  and  especially  so  when  being  maintained  after  it  has  been  deployed.  Yet,  most  security  analysis  techniques  lack  the  ability  to  efficiently  respond  to  incremental  system  changes.  Instead,  every  time  the  system  changes,  the  entire  security  analysis  has  to  be  repeated  from  scratch,  making  it  too  expensive  for  practical  use,  given  the  frequency  with  which  apps  are  updated,  installed,  and  removed  in  such  volatile  environments  as  the  Android  ecosystem.  To  address  this  limitation,  we  present  a  novel  technique,  dubbed  FLAIR,  for  efficient,  yet  formally  precise,  security  analysis  of  Android  apps  in  response  to  incremental  system  changes.  Leveraging  the  fact  that  the  changes  are  likely  to  impact  only  a  small  fraction  of  the  prior  analysis  results,  FLAIR  recomputes  the  analysis  only  where  required,  thereby  greatly  improving  analysis  performance  without  sacrificing  the  soundness  and  completeness  thereof.  Our  experimental  results  using  numerous  bundles  of  real-world  apps  corroborate  that  FLAIR  can  provide  an  order  of  magnitude  speedup  over  prior  techniques.
1	Mining  software  contracts  for  software  evolution.  Maintenance  and  evolution  are  important  parts  for  all  successful  software  projects.  In  recent  years,  version  control  systems  have  played  a  key  role  in  software  development  process.  Not  only  do  they  provide  a  means  to  coordinate  programmers,  organize  and  manage  source  code,  but  they  also  persist  the  evolution  history  of  the  source  code  into  their  software  repositories.  Mining  software  repositories  has  provided  many  insights  on  the  evolution  of  software,  both  for  researchers  and  practitioners.  In  this  paper  we  propose  that  versioned  software  contracts--mined  from  software  repositories--can  be  a  powerful  tool  for  better  understanding  and  supporting  software  evolution.  Tooling  support  is  critical,  due  to  the  complexities  of  configuring,  compiling,  and  running  the  software  to  produce  meaningful  inferred  contracts.  This  paper  contributes  both  techniques  and  tool  support  for  downloading,  building,  and  analyzing  open  source  software  from  social  coding  sites  like  GitHub.  The  tool  automatically  produces  a  description  of  software  evolution  represented  by  versions  of  program  invariants.
1	Visual  storytelling  of  development  sessions.  Most  development  activities,  like  program  understanding,  source  code  navigation  and  editing,  are  supported  by  Integrated  Development  Environments  (IDEs).  They  provide  different  tools  and  user  interfaces  (UI)  to  interact  with  the  source  code,  such  as  browsers,  debuggers,  and  inspectors.  It  is  uncertain  how  and  when  programmers  use  different  UI  elements  of  an  IDE  and  to  what  extent  they  appropriately  support  development.  Previously  we  developed  DFLOW,  a  tool  that  seamlessly  records  and  processes  interaction  data.  Our  long-term  goal  is  to  assess  to  what  extent  the  UIs  of  IDEs  support  the  workflow  of  developers  and  whether  they  can  be  improved.  As  a  first  step  we  present  our  approach  to  analyze  development  sessions  in  the  form  of  visual  storytelling.  We  illustrate  our  initial  catalogue  of  visualizations  through  two  development  stories.
1	Understanding  log  lines  using  development  knowledge.  Logs  are  generated  by  output  statements  that  developers  insert  into  the  code.  By  recording  the  system  behaviour  during  runtime,  logs  play  an  important  role  in  the  maintenance  of  large  software  systems.  The  rich  nature  of  logs  has  introduced  a  new  market  of  log  management  applications  (e.g.,  Splunk,  XpoLog  and  log  stash)  that  assist  in  storing,  querying  and  analyzing  logs.  Moreover,  recent  research  has  demonstrated  the  importance  of  logs  in  operating,  understanding  and  improving  software  systems.  Thus  log  maintenance  is  an  important  task  for  the  developers.  However,  all  too  often  practitioners  (i.e.,  operators  and  administrators)  are  left  without  any  support  to  help  them  unravel  the  meaning  and  impact  of  specific  log  lines.  By  spending  over  100  human  hours  and  manually  examining  all  the  email  threads  in  the  mailing  list  for  three  open  source  systems  (Hadoop,  Cassandra  and  Zookeeper)  and  performing  web  search  on  sampled  logging  statements,  we  found  15  email  inquiries  and  73  inquiries  from  web  search  about  different  log  lines.  We  identified  that  five  types  of  development  knowledge  that  are  often  sought  from  the  logs  by  practitioners:  meaning,  cause,  context,  impact  and  solution.  Due  to  the  frequency  and  nature  of  log  lines  about  which  real  customers  inquire,  documenting  all  the  log  lines  or  identifying  which  ones  to  document  is  not  efficient.  Hence  in  this  paper  we  propose  an  on-demand  approach,  which  associates  the  development  knowledge  present  in  various  development  repositories  (e.g.,  code  commits  and  issues  reports)  with  the  log  lines.  Our  case  studies  show  that  the  derived  development  knowledge  can  be  used  to  resolve  real-life  inquiries  about  logs.
1	A  tiered  approach  towards  an  incremental  bpel  to  bpmn  2  0  migration.  This  report  describes  the  challenges  and  experiences  with  the  incremental  migration  of  a  BPEL  to  a  BPMN  2.0  process  engine.  The  transition  is  motivated  by  a  strategic  reorientation  towards  the  new  standard  as  well  as  end  of  life  of  the  previous  product.  The  solution  reflects  the  preliminary  steps  of  integrating  the  new  platform  into  the  existing  application  and  support  for  parallel  operation.  This  paper  further  describes  the  incrementally  executed  reverse  engineering  of  process  definitions  and  migration  of  instances  by  applying  four  different,  tiered  strategies  in  an  economically  viable  way.  The  report  concludes  by  detailing  the  lessons  learned  to  provide  additional  guidance  for  attempts  to  apply  the  detailed  approach.
1	Locating  performance  improvement  opportunities  in  an  industrial  software  as  a  service  application.  The  goal  of  performance  maintenance  is  to  improve  the  performance  of  a  software  system  after  delivery.  As  the  performance  of  a  system  is  often  characterized  by  unexpected  combinations  of  metric  values,  manual  analysis  of  performance  is  hard  in  complex  systems.  In  this  paper,  we  extend  our  previous  work  on  performance  anomaly  detection  with  a  technique  that  helps  performance  experts  locate  spots  —  so-called  performance  improvement  opportunities  (PIOs)  —,  for  possible  performance  improvements.  PIOs  give  performance  experts  a  starting  point  for  performance  improvements,  e.g.,  by  pinpointing  the  bottleneck  component.  The  technique  uses  a  combination  of  association  rules  and  several  visualizations,  such  as  heat  maps,  which  were  implemented  in  an  open  source  tool  called  Wedjat.  In  this  paper,  we  evaluate  our  technique  and  Wedjat  in  a  field  user  study  with  three  performance  experts  from  industry  using  data  from  a  large-scale  industrial  application.  From  our  field  study  we  conclude  that  our  technique  is  useful  for  speeding  up  the  performance  maintenance  process  and  that  heat  maps  are  a  valuable  way  of  visualizing  performance  data.
1	Dead  code  elimination  for  web  systems  written  in  php  lessons  learned  from  an  industry  case.  Web  systems  undergo  constant  evolution.  This  makes  them  prone  to  accumulating  dead  code.  In  turn,  dead  code  is  commonly  understood  to  inhibit  software  evolution.  The  only  way  out  of  this  vicious  circle  is  the  careful  analysis  of  the  web  system,  identifying  unused  features,  and  eliminating  them.  However,  modern  web  systems  are  often  built  with  server  side  scripting  languages  such  as  PHP.  Their  inherent  dynamic  features  render  traditional  static  dead  code  identification  approaches  useless.  We  describe  the  technical  issues  involved  in  detecting  dead  PHP  code,  and  propose  an  identification  and  removal  approach  based  on  dynamic  analysis.  Further,  we  describe  the  examination  of  our  approach  in  an  industry-scale  web  system,  and  discuss  our  lessons  learned.
1	Refactoring  in  the  presence  of  annotations.  Current-day  programming  languages  include  constructs  to  embed  meta-data  in  a  program's  source  code  in  the  form  of  annotations.  More  than  mere  documentation,  these  annotations  are  used  in  modern  frameworks  to  map  source-level  entities  to  domain-specific  ones.  A  common  example  being  the  Hibernate  Object-Relational  Mapping  framework  that  relies  on  annotations  to  declare  persistence  configurations.  While  the  presence  of  annotations  extends  the  base  semantics  of  the  language,  it  also  imposes  restrictions  on  the  annotated  program.  In  this  paper  we  consider  the  manner  in  which  annotations  affect  automated  refactorings,  and  in  particular  how  they  break  their  behavior  preservation.  As  refactorings,  during  their  condition  checking  phase,  ignore  the  annotation's  restrictions  they  can  no  longer  guarantee  the  preservation  of  the  domain-specific  mappings.  To  address  this  problem,  we  propose  to  make  the  restrictions  of  the  annotations  explicit,  and  use  them  to  steer  the  refactoring  process.  A  prototype  extension  of  the  Eclipse  IDE's  refactoring  engine  is  used  to  demonstrate  our  approach  on  three  annotation  libraries:  Java  Persistence  API,  AspectJ5  and  Simple  XML  serialization.
1	An  efficient  method  for  uncertainty  propagation  in  robust  software  performance  estimation.  Abstract  Software  engineers  often  have  to  estimate  the  performance  of  a  software  system  before  having  full  knowledge  of  the  system  parameters,  such  as  workload  and  operational  profile.  These  uncertain  parameters  inevitably  affect  the  accuracy  of  quality  evaluations,  and  the  ability  to  judge  if  the  system  can  continue  to  fulfil  performance  requirements  if  parameter  results  are  different  from  expected.  Previous  work  has  addressed  this  problem  by  modelling  the  potential  values  of  uncertain  parameters  as  probability  distribution  functions,  and  estimating  the  robustness  of  the  system  using  Monte  Carlo-based  methods.  These  approaches  require  a  large  number  of  samples,  which  results  in  high  computational  cost  and  long  waiting  times.  To  address  the  computational  inefficiency  of  existing  approaches,  we  employ  Polynomial  Chaos  Expansion  (PCE)  as  a  rigorous  method  for  uncertainty  propagation  and  further  extend  its  use  to  robust  performance  estimation.  The  aim  is  to  assess  if  the  software  system  is  robust,  i.e.,  it  can  withstand  possible  changes  in  parameter  values,  and  continue  to  meet  performance  requirements.  PCE  is  a  very  efficient  technique,  and  requires  significantly  less  computations  to  accurately  estimate  the  distribution  of  performance  indices.  Through  three  very  different  case  studies  from  different  phases  of  software  development  and  heterogeneous  application  domains,  we  show  that  PCE  can  accurately  ( > 97%)  estimate  the  robustness  of  various  performance  indices,  and  saves  up  to  225 h  of  performance  evaluation  time  when  compared  to  Monte  Carlo  Simulation.
1	Antecedents  to  it  personnel  s  intentions  to  leave  a  systematic  literature  review.  This  paper  undertakes  a  systematic  review  to  gain  insight  into  existing  studies  on  the  turnover  of  information  technology  (IT)  personnel.  Our  systematic  review  of  72  studies  from  1980  to  2008  examines  the  background  and  trend  of  research  into  IT  personnel's  intentions  to  leave  their  workplaces,  in  addition  to  providing  a  taxonomy  of  the  determinants  of  their  intentions  to  quit  as  captured  in  IT  literature.  We  note  a  huge  growth  in  the  number  of  academic  papers  on  the  topic  since  1998.  Moreover,  most  of  the  research  on  IT  turnover  has  been  undertaken  in  North  America,  followed  by  Asia.  Based  on  the  72  extracted  studies,  we  found  a  total  of  70  conceptually  distinct  IT  turnover  drivers.  We  classified  them  into  the  5  broad  categories  of  individual,  organisational,  job-related,  psychological,  and  environmental,  each  containing  three  to  four  sub-categories.  Finally,  this  paper  presents  insightful  recommendations  for  IT  practitioners  as  well  as  for  the  research  community.
1	How  effectively  can  spreadsheet  anomalies  be  detected  an  empirical  study.  Abstract      While  spreadsheets  are  widely  used,  they  have  been  found  to  be  error-prone.  Various  techniques  have  been  proposed  to  detect  anomalies  in  spreadsheets,  with  varying  scopes  and  effectiveness.  Nevertheless,  there  is  no  empirical  study  comparing  these  techniques’  practical  usefulness  and  effectiveness.  In  this  work,  we  conducted  a  large-scale  empirical  study  of  three  state-of-the-art  techniques  on  their  effectiveness  in  detecting  spreadsheet  anomalies.  Our  study  focused  on  the  precision,  recall  rate,  efficiency  and  scope.  We  found  that  one  technique  outperforms  the  other  two  in  precision  and  recall  rate  of  spreadsheet  anomaly  detection.  Efficiency  of  the  three  techniques  is  acceptable  for  most  spreadsheets,  but  they  may  not  be  scalable  to  large  spreadsheets  with  complex  formulas.  Besides,  they  have  different  scopes  for  detecting  different  spreadsheet  anomalies,  thus  complementing  to  each  other.  We  also  discussed  limitations  of  these  three  techniques.  Based  on  our  findings,  we  give  suggestions  for  future  spreadsheet  research.
1	Links  between  the  personalities  styles  and  performance  in  computer  programming.  The  five-factors  of  personality  are  examined.The  composite  of  code  quality,  project  quality  and  programmer  quality  is  examined.Conscientiousness  affects  depth-first;  Openness  to  Experience  affects  breadth-first.Depth-first  and  large-revisions  affect  performance  in  programming.Programming  styles  help  to  explain  the  influence  of  personality  in  programming.  There  are  repetitive  patterns  in  strategies  of  manipulating  source  code.  For  example,  modifying  source  code  before  acquiring  knowledge  of  how  a  code  works  is  a  depth-first  style  and  reading  and  understanding  before  modifying  source  code  is  a  breadth-first  style.  To  the  extent  we  know  there  is  no  study  on  the  influence  of  personality  on  them.  The  objective  of  this  study  is  to  understand  the  influence  of  personality  on  programming  styles.  We  did  a  correlational  study  with  65  programmers  at  the  University  of  Stuttgart.  Academic  achievement,  programming  experience,  attitude  towards  programming  and  five  personality  factors  were  measured  via  self-assessed  survey.  The  programming  styles  were  asked  in  the  survey  or  mined  from  the  software  repositories.  Performance  in  programming  was  composed  of  bug-proneness  of  programmers  which  was  mined  from  software  repositories,  the  grades  they  got  in  a  software  project  course  and  their  estimate  of  their  own  programming  ability.  We  did  statistical  analysis  and  found  that  Openness  to  Experience  has  a  positive  association  with  breadth-first  style  and  Conscientiousness  has  a  positive  association  with  depth-first  style.  We  also  found  that  in  addition  to  having  more  programming  experience  and  better  academic  achievement,  the  styles  of  working  depth-first  and  saving  coarse-grained  revisions  improve  performance  in  programming.
1	Score  optimization  and  template  updating  in  a  biometric  technique  for  authentication  in  mobiles  based  on  gestures.  This  article  focuses  on  the  evaluation  of  a  biometric  technique  based  on  the  performance  of  an  identifying  gesture  by  holding  a  telephone  with  an  embedded  accelerometer  in  his/her  hand.  The  acceleration  signals  obtained  when  users  perform  gestures  are  analyzed  following  a  mathematical  method  based  on  global  sequence  alignment.  In  this  article,  eight  different  scores  are  proposed  and  evaluated  in  order  to  quantify  the  differences  between  gestures,  obtaining  an  optimal  EER  result  of  3.42%  when  analyzing  a  random  set  of  40  users  of  a  database  made  up  of  80  users  with  real  attempts  of  falsification.  Moreover,  a  temporal  study  of  the  technique  is  presented  leeding  to  the  need  to  update  the  template  to  adapt  the  manner  in  which  users  modify  how  they  perform  their  identifying  gesture  over  time.  Six  updating  schemes  have  been  assessed  within  a  database  of  22  users  repeating  their  identifying  gesture  in  20  sessions  over  4  months,  concluding  that  the  more  often  the  template  is  updated  the  better  and  more  stable  performance  the  technique  presents.
1	Prow  a  pairwise  algorithm  with  constraints  order  and  weight.  Abstract      Testing  systems  with  many  variables  and/or  values  is  often  quite  expensive  due  to  the  huge  number  of  possible  combinations  to  be  tested.  There  are  several  criteria  available  to  combine  test  data  and  produce  scalable  test  suites.  One  of  them  is  pairwise.  With  the  pairwise  criterion,  each  pair  of  values  of  any  two  parameters  is  included  in  at  least  one  test  case.  Although  this  is  a  widely-used  coverage  criterion,  two  main  characteristics  improve  considerably  pairwise:  constraints  handling  and  prioritisation.    This  paper  presents  an  algorithm  and  a  tool.  The  algorithm  (called  PROW:  Pairwise  with  constRaints,  Order  and  Weight)  handles  constraints  and  prioritisation  for  pairwise  coverage.  The  tool  called  CTWeb  adds  functionalities  to  execute  PROW  in  different  contexts,  one  of  them  is  product  sampling  in  Software  Product  Lines  via  importing  feature  models.  Software  Product  Line  (SPL)  development  is  a  recent  paradigm,  where  a  family  of  software  systems  is  constructed  by  means  of  the  reuse  of  a  set  of  common  functionalities  and  some  variable  functionalities.  An  essential  artefact  of  a  SPL  is  the  feature  model,  which  shows  the  features  offered  by  the  product  line,  jointly  with  the  relationships  (includes  and  excludes)  among  them.  Pairwise  testing  could  be  used  to  obtain  the  product  sampling  to  test  in  a  SPL,  using  features  as  pairwise  parameters.  In  this  context,  the  constraint  handling  becomes  essential.  As  a  difference  with  respect  to  other  tools,  CTWeb  does  not  require  SAT  solvers.    This  paper  describes  the  PROW  algorithm,  also  analysing  its  complexity  and  efficiency.  The  CTWeb  tool  is  presented,  including  two  examples  of  the  PROW  application  to  two  real  environments:  the  first  corresponds  to  the  migration  of  the  subsystem  of  transactions  processing  of  a  credit  card  management  system  from  AS400  to  Oracle  with .NET;  the  second  applies  both  the  algorithm  and  the  tool  to  a  SPL  that  monitors  and  controls  some  parameters  of  the  load  in  trucks.
1	Achieving  functional  and  non  functional  interoperability  through  synthesized  connectors.  An  automated  connectors  synthesis  approach  for  application  interoperability.A  connector  adaptation  process  to  preserve  the  connector  non  functional  adequacy.A  stochastic  model-based  implementation  of  performance  and  dependability  analysis.  Our  everyday  life  is  pervaded  by  the  use  of  a  number  of  heterogeneous  systems  that  are  continuously  and  dynamically  available  in  the  networked  environment  to  interoperate  to  achieve  some  goal.  Goals  may  include  both  functional  and  non  functional  aspects  and  the  evolving  nature  of  such  environment  requires  automated  solutions  as  means  to  reach  the  needed  level  of  flexibility.  Achieving  interoperability  in  such  environment  is  a  challenging  problem.  Even  though  some  of  such  systems  may  in  principle  interact  since  they  have  compatible  functionalities  and  similar  interaction  protocols,  mismatches  in  their  protocols  and  non  functional  issues  arising  from  the  environment  may  undermine  their  seamless  interoperability.  In  this  paper,  we  propose  an  approach  for  the  automated  synthesis  of  application  layer  connectors  between  heterogeneous  networked  systems  (NSs)  addressing  both  functional  and  some  non  functional  interoperability.  Our  contributions  are:  (i)  an  automated  connectors  synthesis  approach  for  NSs  interoperability  taking  into  account  functional,  performance  and  dependability  aspects  spanning  pre-deployment  time  and  run-time;  (ii)  a  connector  adaptation  process,  related  to  the  performance  and  dependability  aspects;  and  (iii)  a  stochastic  model-based  implementation  of  the  performance  and  dependability  analysis.  In  addition,  we  implemented,  analyzed,  and  critically  discussed  a  case  study.
1	10  years  of  software  architecture  knowledge  management.  A  retrospective  analysis  of  state  of  the  art  of  AKM.AKM  practice  from  models  to  tools  around  three  different  generations.Our  results  of  AK  practice  in  industry,  barriers  and  remedies.Use  of  AK  in  different  software  development  contexts.A  comparison  of  extended  capabilities  of  AKM  tools.  The  importance  of  architectural  knowledge  (AK)  management  for  software  development  has  been  highlighted  over  the  past  ten  years,  where  a  significant  amount  of  research  has  been  done.  Since  the  first  systems  using  design  rationale  in  the  seventies  and  eighties  to  the  more  modern  approaches  using  AK  for  designing  software  architectures,  a  variety  of  models,  approaches,  and  research  tools  have  leveraged  the  interests  of  researchers  and  practitioners  in  AK  management  (AKM).  Capturing,  sharing,  and  using  AK  has  many  benefits  for  software  designers  and  maintainers,  but  the  cost  to  capture  this  relevant  knowledge  hampers  a  widespread  use  by  software  companies.  However,  as  the  improvements  made  over  the  last  decade  didn't  boost  a  wider  adoption  of  AKM  approaches,  there  is  a  need  to  identify  the  successes  and  shortcomings  of  current  AK  approaches  and  know  what  industry  needs  from  AK.  Therefore,  as  researchers  and  promoters  of  many  of  the  AK  research  tools  in  the  early  stages  where  AK  became  relevant  for  the  software  architecture  community,  and  based  on  our  experience  and  observations,  we  provide  in  this  research  an  informal  retrospective  analysis  of  what  has  been  done  and  the  challenges  and  trends  for  a  future  research  agenda  to  promote  AK  use  in  modern  software  development  practices.
1	Versatile  workload  aware  power  management  performability  analysis  of  server  virtualized  systems.  Abstract      The  widespread  integration  of  virtualization  technologies  in  data  centers  has  enabled  in  the  last  few  years  several  benefits  in  terms  of  operating  costs  and  flexibility.  These  benefits  maybe  boosted  through  join  optimization  of  power  management  (PM)  and  dependability  for  virtualized  systems.  This  indeed  involves  developing  appropriate  models  to  better  understand  their  performability  behavior  whenever  they  are  exposed  to  predictable  (e.g.  rejuvenation)  and  unpredictable  breakdowns.  We  propose  in  this  paper  a  performability  analysis  of  server  virtualized  systems  (SVSs)  using  a  workload-aware  PM  mechanism  based  on  non-Markovian  Stochastic  Reward  Nets  (SRNs)  modeling  approach.  This  analysis  investigates  interactions  and  correlations  between  several  modules  involving  workload-aware  PM  mechanism,  dynamic  speed  scaling  processing,  virtual  machine  (VM)  and  virtual  machine  monitor  (VMM)  both  subject  to  software  aging,  failure  and  rejuvenation.  We  show  through  numerical  results,  using  quantitative  and  qualitative  metrics,  how  performance,  power  usage  and  efficiency  are  impacted  by  workload-aware  PM  mechanism.  We  show  also  how  judicious  choice  of  tunable  attribute  (i.e.    Timeout  )  of  the  proposed  PM  mechanism  with  respect  to  workload  can  lead  to  a  good  power-performance  trade-off.
1	Evaluating  the  perceived  and  estimated  quality  in  use  of  web  2  0  applications.  Web  2.0  refers  to  a  new  generation  of  web  applications  where  individuals  are  able  to  participate,  collaborate,  and  share  created  artefacts.  Despite  the  fact  that  Web  2.0  applications  are  widely  used  for  both  educational  and  professional  purposes,  a  consolidated  methodology  for  their  evaluation  is  still  not  available.  This  paper  presents  and  discusses  the  results  of  two  empirical  studies  on  the  case  of  mind  mapping  and  diagramming  Web  2.0  applications.  Both  studies  employed  logging  actual  use  method  to  measure  the  estimated  quality  in  use,  while  the  retrospective  thinking  aloud  method  and  an  online  questionnaire  were  applied  to  assess  the  perceived  quality  in  use.  Achieved  analytical  results  showed  that  the  results  of  the  estimated  and  the  perceived  quality  in  use  match  partially,  which  indicates  that  quality  in  use  should  be  measured  with  both  subjective  and  objective  instruments.  The  work  presented  in  this  paper  is  the  first  step  towards  a  comprehensive  methodology  for  evaluating  the  quality  in  use  of  Web  2.0  applications.  Consequently,  the  usage  of  the  proposed  quality  in  use  model  for  other  types  of  Web  2.0  applications  as  well  as  contexts  of  use  needs  to  be  investigated  in  order  to  draw  generalizable  conclusions.
1	Software  engineering  research  and  industry  a  symbiotic  relationship  to  foster  impact.  Software  engineering  is  not  only  an  increasingly  challenging  endeavor  that  goes  beyond  the  intellectual  capabilities  of  any  single  individual  engineer  but  also  an  intensely  human  one.  Tools  and  methods  to  develop  software  are  employed  by  engineers  of  varied  backgrounds  within  a  large  variety  of  organizations  and  application  domains.  As  a  result,  the  variation  in  challenges  and  practices  in  system  requirements,  architecture,  and  quality  assurance  is  staggering.  Human,  domain,  and  organizational  factors  define  the  context  within  which  software  engineering  methodologies  and  technologies  are  to  be  applied  and  therefore  the  context  that  research  needs  to  account  for,  if  it  is  to  be  impactful.  This  article  provides  an  assessment  of  the  current  challenges  faced  by  software  engineering  research  in  achieving  its  potential,  a  description  of  the  root  causes  of  such  challenges,  and  a  proposal  for  the  field  to  move  forward  and  become  more  impactful  through  collaborative  research  and  innovation  between  public  research  and  industry.  This  article  is  part  of  a  theme  issue  on  software  engineering’s  50th  anniversary.
1	Spocks  and  kirks  in  the  requirements  universe.  Many  people  make  decisions  based  on  emotion,  then  retrospectively  justify  these  decisions  with  logic.  Requirements  work  can  exploit  this  desire  for  emotional  relationships.
1	Managing  evolving  services.  Services  are  subject  to  constant  change  and  variation,  leading  to  continuous  redesign  and  improvement.  However,  service  changes  shouldn't  be  disruptive  by  requiring  radical  modifications  or  by  altering  the  way  that  business  is  conducted.  In  this  article,  we  discuss  a  causal  model  of  service  changes  that  addresses  the  effects  of  both  shallow  and  deep  changes.  This  article  is  largely  based  on  concepts  and  definitions  found  in  previous  work.  The  definitions  used  have  been  revised  and  amended  on  the  basis  of  formalization  and  compatibility  analysis,  prototype  implementation,  comparison  with  functionality  offered  by  open  standards,  and  an  empirical  in  depth  investigation  using  an  industrial  strength  case  study.
1	Long  term  product  line  sustainability  with  planned  staged  investments.  Software  product  lines  (SPL)  are  long-living  systems  that  enable  systematic  reuse  in  application  engineering.  Product-specific  changes  over  time  can  result  in  architecture  drift,  which  requires  updating  assumptions  made  in  the  SPL's  reuse  infrastructure.  A  model  called  planned  staged  investments  supports  SPL  rearchitecting  in  a  sustainable  way.  The  key  idea  is  to  use  two  different  operational  phases--investment  and  harvesting--to  coordinate  the  competing,  parallel  needs  of  redesign  and  reuse.
1	Developing  a  scheduler  with  difference  bound  matrices  and  the  floyd  warshall  algorithm.  A  study  of  difference-bound  matrices  and  the  Floyd-Warshall  algorithm  in  the  development  of  an  online  scheduler  provides  the  backdrop  for  a  comparison  of  software  practice  and  algorithmic  theory.
1	Product  line  engineering.  Product  line  engineering  (PLE)  is  one  of  the  few  industry-ready  methods  to  manage  reuse  and  variability  in  a  defined  way  and  thus  bring  software  development  maturity  to  a  more  advanced  stage.  The  goal  is  to  deliver  specific  product  variants  with  fast  cycle  times  at  a  manageable  life-cycle  cost  with  a  defined  quality  level.  Many  IT  and  software  organizations  have  started  PLE  but  fail  in  industrializing  the  concepts  and  thus  do  not  achieve  sustainable  benefits.  Authors  Klaus  Schmid  and  Eduardo  Santana  de  Almeida  look  at  current  technology  for  modeling  and  managing  variation  and  thus  facilitate  PLE.  The  Web  extra  at  http://youtu.be/R1gybFwAy10  is  a  video  interview  with  David  Weiss  discussing  the  benefits  of  using  software  product  line  engineering  to  produce  software  families.
1	Automatic  recovery  of  missing  issue  type  labels.  Ag  ile  software  organizations  empower  developers  to  make  appropriate  decisions  rather  than  enforce  adherence  to  a  process,  resulting  in  incomplete  and  noisy  data  in  software  archives.  Since  software  analytics  techniques  are  trained  using  this  data,  automated  techniques  are  required  to  recover  it.
1	What  happens  when  software  developers  are  un  happy.  Abstract  The  growing  literature  on  affect  among  software  developers  mostly  reports  on  the  linkage  between  happiness,  software  quality,  and  developer  productivity.  Understanding  happiness  and  unhappiness  in  all  its  components  –  positive  and  negative  emotions  and  moods  –  is  an  attractive  and  important  endeavor.  Scholars  in  industrial  and  organizational  psychology  have  suggested  that  understanding  happiness  and  unhappiness  could  lead  to  cost-effective  ways  of  enhancing  working  conditions,  job  performance,  and  to  limiting  the  occurrence  of  psychological  disorders.  Our  comprehension  of  the  consequences  of  (un)happiness  among  developers  is  still  too  shallow,  being  mainly  expressed  in  terms  of  development  productivity  and  software  quality.  In  this  paper,  we  study  what  happens  when  developers  are  happy  and  unhappy  while  developing  software.  Qualitative  data  analysis  of  responses  given  by  317  questionnaire  participants  identified  42  consequences  of  unhappiness  and  32  of  happiness.  We  found  consequences  of  happiness  and  unhappiness  that  are  beneficial  and  detrimental  for  developers’  mental  well-being,  the  software  development  process,  and  the  produced  artifacts.  Our  classification  scheme,  available  as  open  data  enables  new  happiness  research  opportunities  of  cause-effect  type,  and  it  can  act  as  a  guideline  for  practitioners  for  identifying  damaging  effects  of  unhappiness  and  for  fostering  happiness  on  the  job.
1	Dynamic  service  placement  and  replication  framework  to  enhance  service  availability  using  team  formation  algorithm.  The  motivation  of  this  work  is  to  reduce  the  complexity  in  managing  and  administering  services  in  the  ever  growing  distributed  environment  via  automated  service  placement  and  replication  with  team  formation  algorithm.  The  team  formation  algorithm  is  designed  in  a  way  that  it  would  continuously  search  for  resources  with  better  performance  and  pool  resources  together  to  achieve  better  availability.  The  main  intention  of  this  work  is  not  to  replace  the  human  administrators  but  to  provide  a  better  alternative  in  managing  services  in  dynamic  distributed  environment.  Instructions  from  the  administrators  are  still  required  but  at  a  different  level.  Administrators  are  freed  from  making  low  level  decisions  such  as  to  decide  the  actual  placement  of  the  services  and  design  the  failover  capabilities  for  each  of  the  services.  The  evaluation  results  showed  that  the  framework  is  capable  of  managing  resources  according  to  the  requirements  given  by  administrator,  even  during  in  the  event  of  multiple  consecutive  resources  failure.  The  proposed  solution  had  the  probability  of  72.1%  of  its  services  that  are  still  available  after  83.3%  of  the  available  resources  were  shut  down  while  conventional  failover  solution  using  three  redundant  units  had  only  the  probability  of  40.8%  of  services  that  are  still  available.
1	Resource  discovery  in  a  grid  system  directing  requests  to  trustworthy  virtual  organizations  based  on  global  trust  values.  Abstract:  This  paper  studies  the  resource  discovery  problem  in  a  Grid  system,  in  which  global  trust  values  play  a  crucial  role.  The  proposed  mechanism  suggests  that  routers  and  resources  comprise  virtual  organizations  (VOs)  within  a  Grid  system,  where  a  router  controls  locally  a  number  of  resources  in  each  virtual  organization.  Global  trust  values  are  assigned  to  the  system's  VOs.  These  trust  values  show  whether  a  VO  and  subsequently  its  local  resources  are  trustworthy  or  not.  Our  primary  goal  is  to  discover  the  appropriate  resource  for  a  specific  request  and  then  effectively  direct  this  request  to  a  trustworthy  VO  that  controls  locally  the  appropriate  resource.  Furthermore,  the  trust-aware  resource  discovery  mechanism  also  manages  the  cases  of  dynamic  changes  in  the  trustworthiness  of  VOs.  For  instance,  VOs  that  in  the  past  were  untrustworthy  could  now  be  trustworthy.  The  proposed  mechanism  is  capable  of  detecting  these  dynamic  changes,  so  that  the  directing  of  requests  occurs  in  an  up-to-date  way.  Finally,  this  paper  presents  the  performance  evaluation  of  the  proposed  trust-aware  resource  discovery  mechanism  by  providing  a  number  of  simulation  tests  in  Grid  systems  of  different  sizes.
1	Evolving  models  in  model  driven  engineering.  A  conceptual  overview  of  the  problem  of  model  evolution.A  survey  of  the  state-of-the-art  in  model  evolution  and  migration.Open  and  ongoing  research  challenges  in  model  evolution  and  migration.  The  artefacts  used  in  Model-Driven  Engineering  (MDE)  evolve  as  a  matter  of  course:  models  are  modified  and  updated  as  part  of  the  engineering  process;  metamodels  change  as  a  result  of  domain  analysis  and  standardisation  efforts;  and  the  operations  applied  to  models  change  as  engineering  requirements  change.  MDE  artefacts  are  inter-related,  and  simultaneously  constrain  each  other,  making  evolution  a  challenge  to  manage.  We  discuss  some  of  the  key  problems  of  evolution  in  MDE,  summarise  the  key  state-of-the-art,  and  look  forward  to  new  challenges  in  research  in  this  area.
1	Supporting  runtime  software  architecture  a  bidirectional  transformation  based  approach.  Runtime  software  architectures  (RSA)  are  architecture-level,  dynamic  representations  of  running  software  systems,  which  help  monitor  and  adapt  the  systems  at  a  high  abstraction  level.  The  key  issue  to  support  RSA  is  to  maintain  the  causal  connection  between  the  architecture  and  the  system,  ensuring  that  the  architecture  represents  the  current  system,  and  the  modifications  on  the  architecture  cause  proper  system  changes.  The  main  challenge  here  is  the  abstraction  gap  between  the  architecture  and  the  system.  In  this  paper,  we  investigate  the  synchronization  mechanism  between  architecture  configurations  and  system  states  for  maintaining  the  causal  connections.  We  identify  four  required  properties  for  such  synchronization,  and  provide  a  generic  solution  satisfying  these  properties.  Specifically,  we  utilize  bidirectional  transformation  to  bridge  the  abstraction  gap  between  architecture  and  system,  and  design  an  algorithm  based  on  it,  which  addresses  issues  such  as  conflicts  between  architecture  and  system  changes,  and  exceptions  of  system  manipulations.  We  provide  a  generative  tool-set  that  helps  developers  implement  this  approach  on  a  wide  class  of  systems.  We  have  successfully  applied  our  approach  on  JOnAS  JEE  system  to  support  it  with  C2-styled  runtime  software  architecture,  as  well  as  some  other  cases  between  practical  systems  and  typical  architecture  models.
1	Automated  generation  of  f  ltl  oracles  for  testing  and  debugging.  Abstract  For  being  able  to  draw  on  automated  reasoning  that  helps  us  in  improving  the  quality  of  some  software  artifact  or  cyber-physical  system,  we  have  to  express  desired  system  traits  in  precise  formal  requirements.  Verifying  that  a  system  adheres  to  these  requirements  allows  us  then  to  gain  the  crucial  level  of  confidence  in  its  capabilities  and  quality.  Complementing  related  methods  like  model  checking  or  runtime  monitors,  for  testing  and  most  importantly  debugging  recognized  problems,  we  would  certainly  be  interested  in  automated  oracles.  These  oracles  would  allow  us  to  judge  whether  observed  (test)  data  really  adhere  to  desired  properties,  and  also  to  derive  program  spectra  that  have  been  shown  to  be  an  effective  reasoning  basis  for  debugging  purposes.  In  this  paper,  we  show  how  to  automatically  derive  such  an  oracle  as  a  dedicated  satisfiability  encoding  that  is  specifically  tuned  to  the  considered  test  data  at  hand.  In  particular,  we  instantiate  a  dedicated  SAT  problem  in  conjunctive  normal  form  directly  from  the  requirements  and  a  test  case’s  execution  data.  Our  corresponding  experiments  illustrate  that  our  approach  shows  attractive  performance  and  can  be  fully  automated.
1	A  systematic  review  on  the  engineering  of  software  for  ubiquitous  systems.  A  systematic  literature  review  on  engineering  software  for  ubiquitous  systems.We  identified  132  approaches  addressing  issues  on  different  phases  of  the  software  engineering  cycle  for  ubiquitous  systems.Implementation,  evolution/maintenance,  and  feedback  phases  have  been  the  most  studied.The  testing  phase  needs  to  receive  more  attention,  especially  in  what  respect  to  simulations.  Context:  Software  engineering  for  ubiquitous  systems  has  experienced  an  important  and  rapid  growth,  however  the  vast  research  corpus  makes  it  difficult  to  obtain  valuable  information  from  it.Objective:  To  identify,  evaluate,  and  synthesize  research  about  the  most  relevant  approaches  addressing  the  different  phases  of  the  software  development  life  cycle  for  ubiquitous  systems.Method:  We  conducted  a  systematic  literature  review  of  papers  presenting  and  evaluating  approaches  for  the  different  phases  of  the  software  development  life  cycle  for  ubiquitous  systems.  Approaches  were  classified  according  to  the  phase  of  the  development  cycle  they  addressed,  identifying  their  main  concerns  and  limitations.Results:  We  identified  128  papers  reporting  132  approaches  addressing  issues  related  to  different  phases  of  the  software  development  cycle  for  ubiquitous  systems.  Most  approaches  have  been  aimed  at  addressing  the  implementation,  evolution/maintenance,  and  feedback  phases,  while  others  phases  such  as  testing  need  more  attention  from  researchers.Conclusion:  We  recommend  to  follow  existing  guidelines  when  conducting  case  studies  to  make  the  studies  more  reproducible  and  closer  to  real  life  cases.  While  some  phases  of  the  development  cycle  have  been  extensively  explored,  there  is  still  room  for  research  in  other  phases,  toward  a  more  agile  and  integrated  cycle,  from  requirements  to  testing  and  feedback.
1	An  experimental  comparison  of  different  real  time  schedulers  on  multicore  systems.  Highlights?  Experimental  comparison  among  RM  and  EDF  on  multi-processors.  ?  Comparison  made  with  partitioned,  clustered  and  global  policies.  ?  Random  workloads  of  synthetic  periodic  tasks.  ?  Experimentation  carried  out  on  a  48-core  machine  with  Linux.  ?  Overheads  achieved  in  the  various  scenarios  are  reported.  ?  Global  and  clustered  real-time  algorithms  prove  to  be  a  viable  solution.  In  this  work,  an  experimental  comparison  among  the  Rate  Monotonic  (RM)  and  Earliest  Deadline  First  (EDF)  multiprocessor  real-time  schedulers  is  performed,  with  a  focus  on  soft  real-time  systems.  We  generated  random  workloads  of  synthetic  periodic  task  sets  and  executed  them  on  a  big  multi-core  machine,  using  Linux  as  Operating  System,  gathering  an  extensive  amount  of  data  related  to  their  exhibited  performance  under  various  real-time  scheduling  strategies.  The  comparison  involves  the  fixed-priority  scheduler  for  multiprocessors  as  available  in  the  Linux  kernel  (with  priorities  set  so  as  to  achieve  RM),  and  on  our  own  implementation  of  EDF,  both  configured  in  global,  partitioned  and  clustered  mode.  The  impact  of  the  various  scheduling  strategies  on  the  performance  of  the  applications,  as  well  as  the  generated  scheduling  overheads,  are  compared  presenting  an  extensive  set  of  experimental  results.  These  provide  a  comprehensive  view  of  the  performance  achievable  by  the  different  schedulers  under  various  workload  conditions.
1	Multi  criteria  scheduling  of  bag  of  tasks  applications  on  heterogeneous  interlinked  clouds  with  simulated  annealing.  We  design  and  implement  two  adaptations  of  the  simulated  annealing  meta-heuristic  for  use  in  Bag-of-Tasks  scheduling  in  multiple  clouds  and  with  multiple  scheduling  criteria.We  implement  and  study  a  multi-cloud  model  with  heterogeneous  characteristics  both  in  terms  of  cost,  performance  and  size.Performance  and  cost  analysis  results  of  the  model  demonstrate  the  benefits  of  the  use  of  simulated  annealing  in  the  scheduling  of  heterogeneous  multi-cloud  models.  Cloud  computing  has  spurred  the  creation  of  a  multitude  of  services  that  use  the  cloud  to  deliver  their  products  on-demand.  Behind  it,  stand  multiple  "Cloud  Providers"  that  in  the  past  few  years  have  created  data-centers,  spread  around  the  world,  creating  a  mesh  of  distributed  resources  that  can  meet  high  availability  and  quality  of  service  requirements.  The  growing  number  of  cloud  clients  demand  reliability,  performance  and  better  cost-to-performance  ratios.  Recently,  scientific  research  has  focused  on  the  optimization  of  interlinked  cloud  systems,  an  aim  which  requires  strategies  for  allocation  of  resources  and  distribution  of  computing  tasks  between  them,  while  also  considering  their  cost  along  with  any  factors  that  may  differentiate  them.  In  this  study,  we  have  evaluated  the  use  of  simulated  annealing  and  thermodynamic  simulated  annealing  in  the  scheduling  of  a  dynamic  multi-cloud  system  with  virtual  machines  of  heterogeneous  performance  serving  Bag-of-Tasks  applications.  The  scheduling  heuristics  applied,  consider  multiple  criteria  when  scheduling  said  applications  and  try  to  optimize  both  for  performance  and  cost,  while  also  taking  into  account  the  heterogeneity  of  the  virtual  machines.  Simulation  results  indicate  that  the  use  of  these  heuristics  can  have  a  significant  impact  in  performance  while  maintaining  a  good  cost-performance  trade-off.
1	Mulapi  improving  api  method  recommendation  with  api  usage  location.  Abstract  During  the  evolution  of  a  software  system,  a  large  number  of  feature  requests  are  continuously  proposed  by  users.  To  implement  these  feature  requests,  developers  often  utilize  existing  third-party  libraries  and  make  use  of  Application  Programming  Interfaces  (APIs)  to  accelerate  the  feature  implementation  process.  However,  it  is  not  always  obvious  which  API  methods  are  suitable  and  where  these  API  methods  can  be  used  in  the  target  program.  In  this  paper,  we  propose  an  approach,  MULAPI  (Method  Usage  and  Location  for  API),  to  recommend  API  methods  and  figure  out  the  API  usage  location  where  these  API  methods  would  be  used.  MULAPI  employs  feature  location  to  identify  feature  related  files  as  API  usage  location.  Further,  these  feature  related  files  are  taken  into  account  to  recommend  API  methods  by  exploring  the  source  code  repository  and  API  libraries  as  well.  We  evaluate  MULAPI  on  more  than  1000  feature  requests  of  eight  Java  projects  (Axis/Java,  CXF,  Hadoop  Common,  Hbase,  Struts2,  Hadoop  HDFS,  Hive  and  Hadoop  Map/Reduce),  and  recommend  API  methods  from  ten  third-party  libraries.  The  empirical  results  show  that  MULAPI  can  accurately  recommend  API  methods  and  usage  location,  and  moreover,  MULAPI  improves  the  effectiveness  of  API  method  recommendation,  compared  with  the  state-of-the-art  approach.
1	A  taxonomy  of  service  identification  approaches  for  legacy  software  systems  modernization.  Abstract  The  success  of  modernizing  legacy  software  systems  to  Service-Oriented  Architecture  (SOA)  depends  on  Service  Identification  Approaches  (SIAs),  which  identify  reusable  functionalities  that  could  become  services.  The  literature  describes  several  SIAs.  However,  the  selection  of  an  identification  approach  that  is  suitable  for  a  practitioner  is  difficult  because  it  depends  on  several  factors,  including  the  goal  of  modernization,  the  available  legacy  artifacts,  the  organization’s  development  process,  the  desired  output,  and  the  usability  of  the  approach.  Accordingly,  to  select  a  suitable  service  identification  approach,  a  practitioner  must  have  a  comprehensive  view  of  existing  techniques.  We  report  a  systematic  literature  review  (SLR)  that  covers  41  SIAs  based  on  software-systems  analyses.  Based  on  this  SLR,  we  create  a  taxonomy  of  SIAs  and  build  a  multi-layer  classification  of  existing  identification  approaches.  We  start  from  a  high-level  classification  based  on  the  used  inputs,  the  applied  processes,  the  given  outputs,  and  the  usability  of  the  SIAs.  We  then  divide  each  category  into  a  fine-grained  taxonomy  that  helps  practitioners  in  selecting  a  suitable  approach  for  identifying  services  in  legacy  software  systems.  We  build  our  SLR  based  on  our  experience  with  legacy  software  modernization,  on  discussions  and  experiences  working  with  industrial  partners,  and  analyses  of  existing  SIAs.  We  validate  the  correctness  and  the  coverage  of  our  review  with  industrial  experts  who  modernize(d)  legacy  software  systems  to  SOA.  The  results  show  that  our  classification  conforms  to  the  industrial  experts’  experiences.  We  also  show  that  most  of  the  studied  SIAs  are  still  at  their  infancy.  Finally,  we  identify  the  main  challenges  that  SIAs  need  to  address,  to  improve  their  quality.
1	Black  box  adversarial  sample  generation  based  on  differential  evolution.  Abstract  Deep  Neural  Networks  (DNNs)  are  being  used  in  various  daily  tasks  such  as  object  detection,  speech  processing,  and  machine  translation.  However,  it  is  known  that  DNNs  suffer  from  robustness  problems  —  perturbed  inputs  called  adversarial  samples  leading  to  misbehaviors  of  DNNs.  In  this  paper,  we  propose  a  black-box  technique  called  Black-box  Momentum  Iterative  Fast  Gradient  Sign  Method  (BMI-FGSM)  to  test  the  robustness  of  DNN  models.  The  technique  does  not  require  any  knowledge  of  the  structure  or  weights  of  the  target  DNN.  Compared  to  existing  white-box  testing  techniques  that  require  accessing  model  internal  information  such  as  gradients,  our  technique  approximates  gradients  through  Differential  Evolution  and  uses  approximated  gradients  to  construct  adversarial  samples.  Experimental  results  show  that  our  technique  can  achieve  100%  success  in  generating  adversarial  samples  to  trigger  misclassification,  and  over  95%  success  in  generating  samples  to  trigger  misclassification  to  a  specific  target  output  label.  It  also  demonstrates  better  perturbation  distance  and  better  transferability.  Compared  to  the  state-of-the-art  black-box  technique,  our  technique  is  more  efficient.  Furthermore,  we  conduct  testing  on  the  commercial  Aliyun  API  and  successfully  trigger  its  misbehavior  within  a  limited  number  of  queries,  demonstrating  the  feasibility  of  real-world  black-box  attack.
1	Software  industry  business  models.  Software  companies  can  leverage  successful  firms'  business  and  revenue  models  to  create  a  competitive  advantage.
1	Trends  in  agile  from  operational  to  strategic  agility  practitioners  digest.  Reports  on  meetings  and  events  that  were  part  of  the  2018  Agile  Conference  hat  took  place  August  6-10  in  San  Diego,  CA.
1	How  do  open  source  software  contributors  perceive  and  address  usability  valued  factors  practices  and  challenges.  Usability  is  an  increasing  concern  in  open  source  software  (OSS).  Given  the  recent  changes  in  the  OSS  landscape,  it  is  imperative  to  examine  the  OSS  contributors’  current  valued  factors,  practices,  and  challenges  concerning  usability.  We  accumulated  this  knowledge  through  a  survey  with  a  wide  range  of  contributors  to  OSS  applications.  Through  analyzing  84  survey  responses,  we  found  that  many  participants  recognized  the  importance  of  usability.  While  most  relied  on  issue  tracking  systems  to  collect  user  feedback,  a  few  participants  also  adopted  typical  user-centered  design  methods.  However,  most  participants  demonstrated  a  system-centric  rather  than  a  user-centric  view.  Understanding  the  diverse  needs  and  consolidating  various  feedback  of  end-users  posed  unique  challenges  for  the  OSS  contributors  when  addressing  usability  in  the  most  recent  development  context.  Our  work  provided  important  insights  for  OSS  practitioners  and  tool  designers  in  exploring  ways  for  promoting  a  user-centric  mindset  and  improving  usability  practice  in  the  current  OSS  communities.
1	Outsourced  offshored  software  testing  practice  vendor  side  experiences.  In  the  era  of  globally  distributed  software  engineering,  the  practice  of  outsourced,  off  shored  software  testing  (OOST)  has  witnessed  increasing  adoption.  Although  there  have  been  ethnographic  studies  of  the  development  aspects  of  global  software  engineering  and  of  the  in-house  practice  of  testing,  there  have  been  fewer  studies  of  OOST,  which  to  succeed,  can  require  dealing  with  unique  challenges.  To  address  this  limitation  of  the  existing  studies,  we  conducted  --  and,  in  this  paper,  report  the  findings  of  --  an  ethnographically-informed  study  of  three  vendor  testing  teams  involved  in  OOST  practice.  Specifically,  we  studied  how  test  engineers  perform  their  tasks  under  deadline  pressures,  the  challenges  that  they  encounter,  and  their  strategies  for  coping  with  the  challenges.  Our  study  provides  insights  into  the  differences  and  similarities  between  in-house  testing  and  OOST,  the  influence  of  team  structures  on  the  degree  of  pressure  experienced  by  test  engineers  in  the  OOST  setup,  and  the  factors  that  influence  quality  and  productivity  under  OOST.
1	Towards  integrated  variant  management  in  global  software  engineering  an  experience  report.  In  the  automotive  domain,  customer  demands  and  market  constraints  are  progressively  realized  by  electric/electronic  components  and  corresponding  software.  Variant  trace  ability  in  SPL  is  crucial  in  the  context  of  different  tasks,  like  change  impact  analysis,  especially  in  complex  global  software  projects.  In  addition,  trace  ability  concepts  must  be  extended  by  partly  automated  variant  configuration  mechanisms  to  handle  restrictions  and  dependencies  between  variants.  Such  variant  configuration  mechanism  helps  to  reduce  complexity  when  configuring  a  valid  variant  and  to  establish  an  explicit  documentation  of  dependencies  between  components.  However,  integrated  variant  management  has  not  been  sufficiently  addressed  so  far.  Especially,  the  increasing  number  of  software  variants  requires  an  examination  of  traceable  and  configurable  software  variants  over  the  software  lifecycle.  This  paper  emphasizes  variant  trace  ability  achievements  in  a  large  global  software  engineering  project,  elaborates  existing  challenges,  and  evaluates  an  industrial  usage  of  an  integrated  variant  management  based  on  experiences.
1	Focused  crawler  for  the  acquisition  of  health  articles.  The  health  intervention  by  using  technology  can  be  the  alternative  to  the  doctor,  especially  for  common  health  problem.  To  support  the  technology,  we  need  health  knowledge  base  as  the  foundation.  The  artificial  intelligence  and  hardware  development  nowadays  support  this  requirement.  The  big  picture  of  our  research  is  building  the  application  that  can  utilize  the  health  knowledge  base  to  provide  health  intervention.  As  the  first  step,  we  collect  the  articles  related  to  health.  To  realize  it,  we  build  the  focused  crawler  that  implements  multithreaded  programming,  Larger-Sites-First  algorithm  and  also  Naive  Bayes  classifier.  We  find  that  the  articles  acquisition  is  going  to  saturate  along  with  the  increment  of  threads.  Furthermore,  the  implementation  of  Larger-Sites-First  algorithm  do  increase  the  number  of  crawled  articles,  but  it  is  not  significant.  In  addition,  Naive  Bayes  recognizes  ≥  90  percent  articles  in  perfect  condition  for  both  health  and  non-health  category.  However,  the  performance  goes  down  when  recognizing  the  non-health  articles  which  contain  health  keywords.
1	Input  injection  detection  in  java  code.  Input  Injections  are  considered  as  the  most  common  and  effective  vulnerabilities  to  exploit  in  many  software  systems  (esp.  web  apps).  In  this  paper,  we  propose  a  way  to  detect  such  vulnerabilities,  such  as  SQL  injection,  command  injection,  and  cross-site  scripting.  Input  injection  is  caused  by  executing  user  inputs  which  have  not  been  validated  or  sanitized,  so  that  the  purpose  of  execution  is  changed  by  malicious  agents  into  their  advantages.  The  input  injection  detector  is  done  by  extending  an  existing  static  analysis  tool,  namely  FindBugs.  The  detection  uses  a  dataflow  analysis  to  monitor  user-contaminated  variables.  To  improve  accuracy,  reducing  false  positives  and  false  negatives,  dataflow  analysis  is  used  to  monitor  variables  that  have  been  validated  or  sanitized  by  developers.  Our  detector  has  only  few  false  positives  and  false  negatives  based  on  our  testing  using  our  test  cases  and  existing  applications,  i.e.  WebGoat  and  ADempiere.
1	Aspect  sentiment  classification  in  opinion  mining  using  the  combination  of  rule  based  and  machine  learning.  Most  online  marketplaces  in  Indonesia  provide  review  or  feedback  feature  in  order  to  enhance  customer's  satisfaction.  However,  there  is  a  large  number  of  unstructured  opinions  and  every  opinion  can  discuss  one  or  more  aspects.  In  this  paper,  we  propose  a  combination  of  rule-based  and  machine  learning  approach  to  classify  aspect  and  its  sentiment  of  online  marketplace  opinions.  We  use  Support  Vector  Machine  and  Naive  Bayes  Classifier  for  classifying  opinions.  The  evaluation  uses  2960  reviews  from  various  categories  collected  from  Indonesian  online  marketplace  site.  The  best  method  for  quality,  accuracy,  service,  communication,  and  delivery  aspect  is  machine  learning  SVM  with  rule-based  as  one  of  the  features  while  the  best  method  for  packaging  and  price  aspect  is  using  rule-based  only.  The  average  f-measures  for  all  aspects  ranging  from  78.9%  to  92%.
1	Authenticating  multiple  user  defined  spatial  queries.  Multiple  user  decision  making  is  important  in  today's  location-based  service  scenarios.  Existing  query  services  such  as  kNN  and  Skyline  queries  only  consider  single  user  and  do  not  consider  user's  preferences.  In  this  paper,  we  introduce  a  novel  query  type  called  multiple  user-defined  spatial  queries  (MUSQ),  which  return  the  best  answers  for  a  group  of  users  considering  both  their  locations  and  preferences.  We  design  an  authenticated  query  processing  framework  based  on  MRtree.  Comprehensive  experiments  are  conducted  to  evaluate  the  proposed  methods.  The  results  show  the  effectiveness  and  robustness  of  our  methods  under  various  parameter  settings.
1	Privacy  preserving  two  party  k  means  clustering  in  malicious  model.  In  data  mining,  clustering  is  a  well-known  and  useful  technique.  One  of  the  most  powerful  and  frequently  used  techniques  is  k-means  clustering.  Most  of  the  privacy-preserving  solutions  based  on  cryptography  proposed  by  different  researchers  in  recent  years  are  in  semi-honest  model,  where  participating  parties  always  follow  the  protocol.  This  model  is  realistic  in  many  cases.  But  providing  stonger  solutions  considering  malicious  model  would  be  more  useful  for  many  practical  applications  because  it  tries  to  protect  a  protocol  from  arbitrary  malicious  behavior  using  cryptographic  tools.  In  this  paper,  we  have  proposed  a  new  protocol  for  privacy-preserving  two-party  k-means  clustering  in  malicious  model.  We  have  used  threshold  homomorphic  encryption  and  non-interactive  zero  knowledge  protocols  to  construct  our  protocol  according  to  real/ideal  world  paradigm.
1	Examining  privacy  concern  in  social  driven  location  sharing  an  empirical  study  on  chinese  popular  snss.  Sharing  location  in  SNSs  has  witnessed  rapid  development  in  recent  years.  Privacy  is  undoubtedly  a  barrier  to  the  adoption  of  such  location  sharing  services.  In  this  paper,  we  investigate  what  factors  affect  users'  privacy  concerns  and  how  privacy  concerns  may  in  turn  affect  users'  location  sharing  behaviors.  We  conducted  one  quantitative  survey  and  8  interviews  on  Chinese  popular  SNSs.  We  identified  1)  the  factors  that  users  care  more  about  their  privacy  when  posting  location  information  on  social  networks,  2)  what  types  of  privacy  information  do  they  think  might  be  exposed  through  location  sharing,  and  3)  users'  behavior  patterns  in  cope  with  privacy  concerns.  We  also  discuss  the  theoretical  and  practical  implication  on  the  improvement  of  location  sharing  services.
1	Towards  container  orchestration  in  fog  computing  infrastructures.  The  Cloud  Computing  paradigm  promoted  the  outsourcing  of  IT  infrastructure  and  enterprise  applications  paving  the  way  to  save  costs  of  building  and  maintaining  computing  infrastructures  on-premise.  In  this  environment,  scale  up  of  applications  to  attend  demands  in  high  peaks  become  easier  and  highly  automated.  Virtualization  was  a  key  technology  to  enable  these  characteristics.  Nowadays,  Container  technology  became  popular  as  an  alternative  to  Virtual  Machines,  and  is  being  widely  applied,  as  a  consequence,  Orchestration  tools  are  being  extensively  applied  in  the  Cloud  environment.  Despite  its  success,  when  it  comes  to  the  Internet  of  Things  (IoT),  Cloud  Computing  falls  short  to  meet  several  requirements.  Fog  Computing  appear  as  a  complimentary  technology  to  the  Cloud  to  deliver  the  missing  requirements  in  the  IoT  scene.  Managing  services  deployed  in  a  Fog  Environment  is  a  complex  task  and  infrastructure  management  and  orchestration  tools  can  make  it  seamless.  In  this  paper,  we  evaluate  how  Containers  can  affect  the  overall  performance  of  applications  in  Fog  Nodes.  We  analyze  different  Container  Orchestration  tools  and  how  they  meet  Fog  requirements  to  run  applications.  We  also  propose  a  Container  Orchestration  Framework  for  Fog  Computing  infrastructures.
1	The  concepts  and  ontology  of  sisl  a  situation  centric  specification  language.  The  concept  of  situation  proposed  in  Situ,  a  context  aware  service-centric  model,  is  the  instant  status  of  a  software  system  environment,  including  context  values  and  user's  actions,  as  well  as  the  predicated  user's  desires,  which  are  critical  for  service  evolution  because  they  usually  reflect  user's  requirements  on  the  system.  For  better  applying  the  situation  theory  in  practice,  it  is  essential  to  provide  formal  specification  and  explicit  description  for  situations.  In  this  paper,  we  present  a  situation-centric  specification  language  --  SiSL,  which  formalizes  the  situation-theoretic  approach  to  humanintention-driven  service  evolution  proposed  in  Situ  using  second  order  logic.  SiSL  classifies  the  entities  and  their  relations  that  are  used  to  reason  about  user's  behaviors  in  the  software  system  domains,  and  it  supports  formal  description  of  situations  and  intentions.  SiSL  provides  a  set  of  axioms  which  establish  a  theoretical  framework  for  inferring  pre-defined  use's  desires  and  detecting  new  desires  based  on  observations  of  user's  actions  and  context  values.  A  case  study  of  paper  submission  and  review  system  -  PaperSR  -  is  used  to  explain  the  concepts  and  ontology  of  SiSL.
1	Causal  consistency  for  distributed  data  stores  and  applications  as  they  are.  There  have  been  proposed  protocols  to  achieve  causal  consistency  with  a  distributed  data  store  that  does  not  make  safety  guarantees.  Such  a  protocol  works  with  an  unmodified  data  store  if  it  is  implemented  as  middleware  or  a  shim  layer  while  it  can  be  implemented  inside  a  data  store.  But  the  middleware  approach  has  required  modifications  to  applications.  Applications  have  to  specify  explicitly  data  dependency  to  be  managed.  On  the  contrary,  our  Letting-It-Be  protocol  handles  all  the  implicit  dependency  naturally  resulting  from  data  accesses  though  it  is  implemented  as  middleware.  Our  protocol  does  not  require  any  modifications  to  either  data  stores  or  applications.  It  trades  performance  for  the  merit  to  some  extent.  Throughput  declines  from  a  bare  data  store  were  21%  in  the  best  case  and  78%  in  the  worst  case.
1	The  samba  approach  for  self  adaptive  model  based  online  testing  of  services  orchestrations.  Service  Oriented  Architecture  (SOA)  is  a  popular  design  pattern  that  allows  building  applications  composed  of  loosely-coupled  and  autonomous  services.  Such  services  may  evolve  and  change  at  runtime,  often  outside  the  control  of  the  owner  of  the  application.  Consequently,  typical  validation  approaches,  like  offline  testing  performed  before  services  deployment,  are  necessary  but  not  sufficient:  offline  testing  cannot  assure  the  correct  behavior  of  the  SOA  during  its  execution.  To  cope  with  the  evolution  of  services  and  their  orchestrations,  in  this  paper  we  present  a  Self-Adaptive  Model-BAsed  online  testing  framework  called  SAMBA.  SAMBA  aims  to  assess  the  proper  behavior  of  a  SOA  during  its  lifecycle  executing  model-based  online  testing  at  runtime,  under  the  coordination  of  a  MAPE-K  control  loop.  SAMBA  is  assessed  in  a  case  study,  where  its  detection  capability  are  proved  through  functional,  mutation  and  fault  injection  tests.
1	Improving  the  smartness  of  cloud  management  via  machine  learning  based  workload  prediction.  Cloud  computing  has  been  widely  adopted  by  many  companies  and  government  entities.  To  ensure  high  quality  computing  resource  provisioning,  cloud  platforms  should  offer  smart  resource  management  solutions.  An  important  step  toward  better  resource  management  is  to  accurately  predict  the  workloads  of  the  applications  running  on  the  cloud.  Many  existing  workload  prediction  methods  are  regression  based,  which  require  the  workloads  of  the  applications  show  clear  seasonality  and  trend.  However,  it  is  difficult  to  use  these  methods  for  tasks  which  may  not  have  such  recurring  workload  patterns.  From  careful  analysis  of  the  workloads  in  a  real-world  cloud,  we  found  that  many  tasks  have  busty  workloads  that  are  very  difficult  to  predict  using  regression-based  prediction.  Instead,  we  consider  a  job-pool  based  approach,  where  the  knowledge  about  the  workloads  of  a  large  pool  of  tasks  is  used  to  help  predict  the  workloads  of  new  tasks.  In  particular,  we  develop  a  clustering-based  learning  approach  to  realize  the  job-pool  based  concept.  The  pool  of  jobs  are  clustered  based  on  their  workloads,  and  a  neuralnet  is  used  to  learn  the  characteristics  of  the  workloads  in  each  cluster.  When  a  new  job  arrives,  we  use  its  initial  workload  pattern  and  submission  parameters  to  find  the  cluster  it  belongs  to.  Then,  the  corresponding  neuralnet  is  used  to  predict  the  workload  of  the  new  job  far  into  the  future.  Based  on  this  predicted  long-term  workload,  smart  resource  management  decisions  can  be  made  to  reduce  the  potential  overhead  in  scaling  and  migration.  We  also  consider  a  non-clustering  based  learning  solution  and  compare  it  with  the  clustering-based  learning  solution.  Experimental  results  show  that  the  clustering-based  learning  approach  can  predict  the  workload  more  accurately.
1	Real  time  data  intensive  telematics  functionalities  at  the  extreme  edge  of  the  network  experience  with  the  prestocloud  project.  In  recent  years,  use  of  different  sensors  connected  to  vehicles  is  dramatically  increasing  in  order  to  enhance  transportation  efficiency.  The  current  Big  Data  technologies  are  predominantly  used  to  store  large  amount  of  telematics  data  especially  in  the  cloud,  and  they  are  only  able  to  perform  simple  querying  for  the  purpose  of  reporting.  While  all  the  data  is  stored  in  the  cloud-centric  datacenters,  these  telematics  systems  are  not  capable  of  exploiting  other  functionalities  offered  by  advanced  real-time  analytics  such  as  run-time  anomaly  detection.  In  this  paper,  we  propose  an  advanced  telematics  system  orchestrated  upon  an  edge  computing  framework  in  the  context  of  the  PrEstoCloud  (Proactive  Cloud  Resources  Management  at  the  Edge  for  Efficient  Real-Time  Big  Data  Processing)  project.  This  telematics  system  is  a  real-time  data-intensive  application  running  at  the  extreme  edge  of  the  network  for  drivers'  behavior  profiling  and  triggering  run-time  alerts.  Such  functionalities  may  be  useful  in  order  to  notify  stakeholders  for  example  drivers  and  logistic  centers  on  situations  where  a  possible  accident  may  occur  or  attention  is  required.
1	Delivering  dependable  reusable  components  by  expressing  and  enforcing  design  decisions.  A  component  is  usually  complemented  with  guidelines  expressing  its  proper  use,  e.g.  the  appropriate  order  of  calls  that  clients  should  conform  to.  During  reuse,  clients  could  easily  alter  such  an  order,  and  this  could  result  in  reused  components  that  become  unreliable,  since  architectural  guidelines  have  not  been  honoured.  Sometimes  architectural  guidelines  are  simply  unknown,  hence  whether  components  are  misused  by  clients  is  uncertain.  This  paper  proposes  an  approach  to  document  the  architectural  guidelines  that  client  classes  should  comply  with  when  reusing  a  component.  We  empower  component  developers  to  provide  such  guidelines,  conveying  design  decisions,  along  with  the  code  of  components  in  such  a  way  to  be  apt  to  automatic  checks.  Then,  clients  compliance  with  architectural  guidelines  of  reused  components  can  be  automatically  checked  by  an  aspect-based  tool.  As  a  result,  proper  usage  of  reused  components  can  be  ensured,  and  in  turn  the  behaviour  of  components  should  be  correct.  This  strengthen  the  reliability  of  the  resulting  system.  Especially  for  continuous  evolution,  having  automatic  conformance  checks  is  paramount  for  obtaining  the  correct  behaviour  of  reused  components.
1	On  the  effectiveness  of  link  addition  for  improving  robustness  of  multiplex  networks  against  layer  node  based  attack.  Recent  research  trends  in  network  science  are  shifting  from  the  analysis  of  single-layer  networks  to  the  analysis  of  multilayer  networks.  In  particular,  the  robustness  of  multilayer  networks  has  been  actively  studied.  There  exist  two  popular  multilayer  network  models:  one  is  interdependent  network,  and  the  other  is  multiplex  network.  We  aim  to  construct  a  methodology  for  effectively  improving  the  robustness  of  multiplex  networks  against  layer  node-based  attack.  As  the  first  step  to  achieve  this  goal,  in  this  paper,  we  examine  the  effectiveness  of  existing  link  addition  strategies,  which  are  proposed  for  interdependent  networks,  for  improving  the  robustness  of  multiplex  networks.  Through  the  network  attack  simulations,  we  show  that  the  strategic  link  addition  can  effectively  improve  the  robustness  of  multiplex  networks.  Moreover,  link  addition  strategies  are  suggested  to  be  effective  particularly  when  a  large  number  of  nodes  are  attacked.
1	An  extended  cp  abe  based  access  control  model  for  data  outsourced  in  the  cloud.  This  paper  proposes  an  access  control  scheme  called  Collaborative  Cipher  text-Policy  Attribute  Role  Based  Encryption  (C-CP-ARBE).  Our  C-CP-ARBE  integrates  Role-based  Access  Control  (RBAC)  into  a  Cipher  text-Policy  Attribute-based  Encryption  (CP-ABE).  The  proposed  model  provides  high  expressiveness  of  access  control  policy,  scalable  user  management,  and  less  user  revocation  cost  compared  to  the  existing  approach.  In  addition,  our  model  supports  both  read  and  write  access  control  in  a  more  complex  data  sharing  in  collaborative  cloud  storage  where  there  are  multi-owner,  multi-user,  and  multi-authority.  For  the  evaluation,  we  develop  the  access  control  tool  and  set  up  test  cases  to  validate  the  functionality  of  our  proposed  scheme.  We  also  conduct  the  performance  evaluation  and  compare  the  revocation  cost  of  our  C-CP-ARBE  and  CP-ABE  scheme  to  demonstrate  that  our  revocation  method  incurs  less  computation  cost  and  efficient  in  practice  for  supporting  a  larger  scale  of  users.
1	Internet  of  things  based  framework  to  facilitate  indoor  localization  education.  Internet  of  Things  (IoT)  is  becoming  a  hot  topic  in  Computer  Science,  especially  with  the  advent  of  smart  appliances  such  as  Nest  thermostats,  connected  light  bulbs,  and  other  IP-enabled  systems.  In  this  paper  we  are  proposing  IoT-based  framework  to  facilitate  education  of  indoor  localization.  The  proposed  framework  employs  Arduino  microcontroller  boards  connected  with  XBee  radio  modules  to  perform  indoor  localization.  The  framework  uses  a  master-slave  architecture  and  RSSI-based  triangulation  to  facilitate  student  learning  of  localization  algorithms.  Indoor  localization  is  a  field  where  more  research  is  still  needed,  since  current  methods  are  far  from  perfect  and  have  limitations  in  accuracy  and  precision.  Helping  students  spark  interest  in  indoor  localization  has  the  potential  to  bring  in  more  young  researchers  into  the  field  as  well  as  bring  in  much  needed  new  ideas.
1	Adaptive  agile  performance  modeling  and  testing.  Rising  industry  trends  in  acceptance  for  Agile  approaches  in  software  development  provides  opportunities  as  well  as  challenges  in  the  ever-changing  environment  of  software  development.  On  the  one  hand,  fast  feedback  of  working  code  combined  with  close  customer  collaboration  enables  increased  software  development  productivity  by  providing  greater  accuracy  of  where  to  target  the  development  effort.  On  the  other  hand,  the  growing  interdependence  of  the  product  software  components  within  complex  systems  gradually  evolves  considerable  challenge  in  terms  of  assessing  system  performance  and  planning  for  future  system  expansion.  In  addition,  and  equally  important,  establishing  and  maintaining  performance  modeling  becomes  more  and  more  costly  due  to  the  increasing  volatility  of  requirements.  We  should  also  consider  the  growing  complexity  of  systems  being  built,  integration  with  third  party  subsystems,  and  at  the  same  time  the  fact  that  engineering  teams  face  tight  project  budgets  as  businesses  are  looking  to  reduce  capital  expenditures.  To  address  these  concerns,  an  adaptive  performance  modeling  approach  supported  by  automated  performance  analysis  is  proposed.  This  approach  combines  predictive  engineering  techniques,  experimental  feedback  obtained  through  testing,  and  continuous  data  acquisition  including  knowledge-based  assessments  provided  by  the  engineers.
1	Technological  module  for  unsupervised  personalized  cardiac  rehabilitation  exercising.  Cardiac  Rehabilitation  (CR)  can  significantly  improve  mortality  and  morbidity  rates  from  Cardiovascular  Diseases  (CVD).  Nevertheless,  traditional  CR  is  diminished  by  low  subsequent  adherence  rates.  Thus,  in  this  paper,  an  e-Health  technological  module  for  human  motion  analysis  and  user  modelling  is  proposed,  in  order  to  address  the  requirements  of  unsupervised,  tele-rehabilitation  systems  for  CVD,  by  evaluating  and  personalizing  prescribed  physical  CR  programs.  The  proposed  module  consists  of  a)  an  exercise  capturing  and  evaluation  component,  and  b)  a  user  modelling  and  decision  support  system  for  personalization  of  cardiac  rehabilitation  programs.  In  particular,  the  module  monitors  and  analyses  the  body  movements  of  the  patient  when  exercising  in  real-time,  while  based  on  this  analysis  and  the  heart-rate  measurements,  it  is  capable  of  short-term  and  long-term  CR  session  adaptation.  The  proposed  module  constitutes  a  significant  tool  for  internet-enabled  sensor-based  home  exercise  platforms.
1	A  novel  activity  detection  system  using  plantar  pressure  sensors  and  smartphone.  Physical  activities  detection  plays  a  vital  role  to  healthcare  professionals  who  would  like  to  monitor  patients  remotely  and  to  develop  context-sensitive  systems.  Major  number  of  physical  activity  detection  systems  use  accelerometers  to  collect  data  from  different  parts  of  the  body.  Since  those  approaches  have  limitations  from  users'  point  of  view,  we  have  used  smart  phones  that  are  coming  with  built-in  accelerometers  and  gyroscopes.  We  have  proposed  and  developed  three  novel  approaches  for  activity  recognition.  Firstly,  we  have  developed  a  multimodal  system  where  we  used  pressure  sensor  data  from  shoes  along  with  accelerometers  and  gyroscope  data  from  smart  phone.  Again,  we  have  presented  the  details  of  our  novel  activity  detection  system  along  with  evaluation.  In  the  second  approach,  we  considered  our  sensor  data  as  time  series  shapelets  and  apply  recently  developed  algorithms  to  differentiate  those  shapelets.  Finally,  we  applied  Gaussian  Mixture  Models  with  time-delay  embedding  for  detecting  different  activities.
1	Eliciting  relations  from  natural  language  requirements  documents  based  on  linguistic  and  statistical  analysis.  Requirements  are  usually  presented  as  Natural  Language  based  documents.  In  the  conceptual  modeling  phase,  requirements  are  collected  from  different  stakeholders  and  analyzed  by  requirement  engineers.  However,  the  size  of  the  requirements  documents  can  become  very  large,  and  the  modeling  process  is  quite  time  consuming  and  resource  consuming.  In  order  to  solve  this  problem,  much  has  been  written  on  the  processing  of  requirements  documents  to  yield  conceptual  models.  In  this  paper,  we  proposed  an  approach  for  identifying  and  extracting  relations  in  a  range  of  requirements  documents  with  three  steps:  text  analysis,  entity  extraction  and  relation  mapping.  If  the  entities  in  the  relation  are  quite  close  to  each  other,  for  example,  in  the  strategic  dependency  relationship,  we  will  define  a  set  of  linguistic  patterns  used  for  identifying  relations  and  propose  a  matching  algorithm  of  semantic  automata  to  extract  the  relation.  Based  on  this  approach,  we  developed  a  system  to  automatically  generate  the  strategic  dependency  model  of  i  framework  and  the  activity  model  from  Chinese  requirements  documents.  A  series  of  experiments  were  conducted  to  evaluate  the  performance  of  the  automated  requirements  analysis  system.  The  results  show  that  the  system  achieves  high  recall  with  a  consistent  improvement  in  precision,  which  demonstrates  the  applicability  of  our  approach.
1	Implementation  of  video  and  medical  image  services  in  cloud.  The  main  subject  of  this  paper  is  how  to  construct  virtualization  in  the  cloud  for  implementing  video  and  medical  image  services.  The  framework  of  cloud  service  contains  the  infrastructure,  OS,  virtual  machines,  platform,  cloud  web  application  services,  and  cloud  devices.  We  build  medical  image  and  video  services  on  cloud  IaaS  environment,  which  integrates  KVM  and  OpenNebula  open  sources  to  provide  a  cloud  virtual  environment  for  end  users.  Also,  Hadoop  open  source,  as  cloud  PaaS,  is  used  for  these  two  cloud  services.  This  paper  realizes  medical  image  and  video  services  that  are  easy  for  users  to  understand,  access,  and  operate  with  them  in  the  cloud.  The  proposed  system  can  improve  medical  imaging  storage,  transmission  stability,  and  reliability  while  providing  an  easy-to-operate  management  interface.
1	Semantic  smells  and  errors  in  access  control  models  a  case  study  in  php.  Access  control  models  implement  mechanisms  to  restrict  access  to  sensitive  data  from  unprivileged  users.  Access  controls  typically  check  privileges  that  capture  the  semantics  of  the  operations  they  protect.  Semantic  smells  and  errors  in  access  control  models  stem  from  privileges  that  are  partially  or  totally  unrelated  to  the  action  they  protect.  This  paper  presents  a  novel  approach,  partly  based  on  static  analysis  and  information  retrieval  techniques,  for  the  automatic  detection  of  semantic  smells  and  errors  in  access  control  models.  Investigation  of  the  case  study  application  revealed  31  smells  and  2  errors.  Errors  were  reported  to  developers  who  quickly  confirmed  their  relevance  and  took  actions  to  correct  them.  Based  on  the  obtained  results,  we  also  propose  three  categories  of  semantic  smells  and  errors  to  lay  the  foundations  for  further  research  on  access  control  smells  in  other  systems  and  domains.
1	U  can  touch  this  touchifying  an  ide.  Touch  gestures  are  not  only  often  very  intuitive,  but  their  direct  manipulation  characteristics  also  help  to  reduce  the  cognitive  load.  Since  software  development  poses  complex  cognitive  demands,  our  goal  is  to  exploit  the  advantages  of  direct  manipulation  to  support  professional  software  engineering  processes.  In  this  paper,  we  demonstrate  how  touch  gestures  can  be  used  within  a  professional  integrated  development  environment.  As  for  that,  we  have  enriched  the  Eclipse  IDE  with  common  and  invertible  multi-touch  gestures  which  can  be  used  for  both  controlling  the  graphical  user  interface  and  triggering  built-in  refactoring  tools.  The  design  of  our  extensions  was  informed  by  an  early  user  study  revealing  problems  of  using  the  Eclipse  IDE  with  the  default  touch  support  provided  by  the  operating  system.  By  using  the  emerging  prototype  during  its  implementation,  we  were  able  to  iteratively  improve  the  prototype  based  on  our  own  experience  and  gain  first  insights  into  the  potential  of  using  direct  manipulation  methods  within  the  IDE.  First  results  suggest  that  using  an  additional  touch  device  within  the  classical  desktop  setup  enables  a  precise  and  fast  work  flow.
1	Software  analytics  achievements  and  challenges.  A  huge  wealth  of  various  data  exist  in  the  practice  of  software  development.  Further  rich  data  are  produced  by  modern  software  and  services  in  operation,  many  of  which  tend  to  be  data-driven  and/or  data-producing  in  nature.  Hidden  in  the  data  is  information  about  the  quality  of  software  and  services  or  the  dynamics  of  software  development.  Software  analytics  is  to  utilize  a  data-driven  approach  to  enable  software  practitioners  to  perform  data  exploration  and  analysis  in  order  to  obtain  insightful  and  actionable  information;  such  information  is  used  for  completing  various  tasks  around  software  systems,  software  users,  and  software  development  process.  This  tutorial  presents  achievements  and  challenges  of  research  and  practice  on  principles,  techniques,  and  applications  of  software  analytics,  highlighting  success  stories  in  industry,  research  achievements  that  are  transferred  to  industrial  practice,  and  future  research  and  practice  directions  in  software  analytics.
1	A  novel  method  on  software  structure  evaluation.  The  software  is  becoming  much  more  complex  and  large,  thus  it  is  difficult  to  examine  and  evaluate  the  structure  of  software  containing  millions  of  lines  of  code  and  thousands  of  functions  or  objects.  In  large-scale  software,  the  structure  is  one  of  the  most  important  factors  for  people  to  design,  develop  and  maintain  trusted  software.  In  this  paper,  we  concentrate  on  dynamic  analysis  instead  of  static  analysis  and  the  Calling  Network  is  proposed  to  represent  the  dynamic  structure  of  software.  Then,  the  community  and  hierarchical  structure  of  the  software  are  detected  from  the  Calling  Network  using  Newman  Fast  algorithm  and  Bi-Breadth-First  Search  algorithm.  Finally,  the  Partition  Matching  Ratio  is  proposed  to  evaluate  the  rationality  of  software  structure  by  comparing  the  theoretical  community  and  hierarchical  structure  with  the  physical  partition.  In  the  experiments,  we  evaluate  two  Java  applications,  JPetStore  and  Jforum,  and  find  the  structure  of  JPetStore  is  better  than  Jforum  which  is  same  as  the  users'  evaluation.
1	Model  checking  of  security  critical  applications  in  a  model  driven  approach.  This  paper  illustrates  the  integration  of  model  checking  in  SecureMDD,  a  model-driven  approach  for  the  development  of  security-critical  applications.  In  addition  to  a  formal  model  for  interactive  verification  as  well  as  executable  code,  a  formal  system  specification  for  model  checking  is  generated  automatically  from  a  UML  model.  Model  checking  is  used  to  find  attacks  automatically  and  interactive  verification  is  used  by  an  expert  to  guarantee  security  properties.  We  use  AVANTSSAR  for  model  checking  and  KIV  for  interactive  verification.  The  integration  of  AVANTSSAR  in  SecureMDD  and  the  advantages  and  disadvantages  over  interactive  verification  with  KIV  are  demonstrated  with  a  smart  card  based  electronic  ticketing  example.
1	Finding  and  analyzing  compiler  warning  defects.  Good  compiler  diagnostic  warnings  facilitate  software  development  as  they  indicate  likely  programming  mistakes  or  code  smells.  However,  due  to  compiler  bugs,  the  warnings  may  be  erroneous,  superfluous  or  missing,  even  for  mature  production  compilers  like  GCC  and  Clang.  In  this  paper,  we  (1)  propose  the  first  randomized  differential  testing  technique  to  detect  compiler  warning  defects  and  (2)  describe  our  extensive  evaluation  in  finding  warning  defects  in  widely-used  C  compilers.      At  the  high  level,  our  technique  starts  with  generating  random  programs  to  trigger  compilers  to  emit  a  variety  of  compiler  warnings,  aligns  the  warnings  from  different  compilers,  and  identifies  inconsistencies  as  potential  bugs.  We  develop  effective  techniques  to  overcome  three  specific  challenges:  (1)  How  to  generate  random  programs,  (2)  how  to  align  textual  warnings,  and  (3)  how  to  reduce  test  programs  for  bug  reporting?      Our  technique  is  very  effective  ---  we  have  found  and  reported  60  bugs  for  GCC  (38  confirmed,  assigned  or  fixed)  and  39  for  Clang  (14  confirmed  or  fixed).  This  case  study  not  only  demonstrates  our  technique's  effectiveness,  but  also  highlights  the  need  to  continue  improving  compilers'  warning  support,  an  essential,  but  rather  neglected  aspect  of  compilers.
1	Symbolic  assume  guarantee  reasoning  through  bdd  learning.  Both  symbolic  model  checking  and  assume-guarantee  reasoning  aim  to  circumvent  the  state  explosion  problem.  Symbolic  model  checking  explores  many  states  simultaneously  and  reports  numerous  erroneous  traces.  Automated  assume-guarantee  reasoning,  on  the  other  hand,  infers  contextual  assumptions  by  inspecting  spurious  erroneous  traces.  One  would  expect  that  their  integration  could  further  improve  the  capacity  of  model  checking.  Yet  examining  numerous  erroneous  traces  to  deduce  contextual  assumptions  can  be  very  time-consuming.  The  integration  of  symbolic  model  checking  and  assume-guarantee  reasoning  is  thus  far  from  clear.  In  this  paper,  we  present  a  progressive  witness  analysis  algorithm  for  automated  assume-guarantee  reasoning  to  exploit  a  multitude  of  traces  from  BDD-based  symbolic  model  checkers.  Our  technique  successfully  integrates  symbolic  model  checking  with  automated  assume-guarantee  reasoning  by  directly  inferring  BDD's  as  implicit  assumptions.  It  outperforms  monolithic  symbolic  model  checking  in  four  benchmark  problems  and  an  industrial  case  study  in  experiments.
1	Recommending  developers  with  supplementary  information  for  issue  request  resolution.  Software  changes,  new  features  and  bugs  are  generally  reported  as  issue  requests  which  need  to  be  quickly  and  efficiently  resolved.  A  large  amount  of  approaches  have  been  proposed  to  recommend  suitable  developers  to  resolve  software  issues  [1,  2,  6,  3].  These  techniques  tend  to  recommend  senior  developers  who  have  luxuriant  developing  experience,  which  prejudice  someone  who  just  joined  the  team.  But  when  the  senior  developers  are  not  available,  these  approaches  cannot  effectively  help  select  an  alternative  suitable  developer  (maybe  a  junior  developer)  to  implement  the  issue.  On  the  other  hand,  the  junior  developers  may  be  not  skilled  to  the  issue  request  and  target  system.  They  may  also  need  to  refer  to  other  software  repositories  in  understanding  the  changing  task,  which  is  costly  and  time-consuming.
1	Zero  downtime  sql  database  schema  evolution  for  continuous  deployment.  When  a  web  service  or  application  evolves,  its  database  schema  ---  tables,  constraints,  and  indices  ---  often  need  to  evolve  along  with  it.  Depending  on  the  database,  some  of  these  changes  require  a  full  table  lock,  preventing  the  service  from  accessing  the  tables  under  change.  To  deal  with  this,  web  services  are  typically  taken  offline  momentarily  to  modify  the  database  schema.  However  with  the  introduction  of  concepts  like  Continuous  Deployment,  web  services  are  deployed  into  their  production  environments  every  time  the  source  code  is  modified.  Having  to  take  the  service  offline  ---  potentially  several  times  a  day  ---  to  perform  schema  changes  is  undesirable.  In  this  paper  we  introduce  QuantumDB---  a  tool-supported  approach  that  abstracts  this  evolution  process  away  from  the  web  service  without  locking  tables.  This  allows  us  to  redeploy  a  web  service  without  needing  to  take  it  offline  even  when  a  database  schema  change  is  necessary.  In  addition  QuantumDB  puts  no  restrictions  on  the  method  of  deployment,  supports  schema  changes  to  multiple  tables  using  changesets,  and  does  not  subvert  foreign  key  constraints  during  the  evolution  process.  We  evaluate  QuantumDB  by  applying  19  synthetic  and  95  industrial  evolution  scenarios  to  our  open  source  implementation  of  QuantumDB.  These  experiments  demonstrate  that  QuantumDB  realizes  zerodowntime  migrations  at  the  cost  of  acceptable  overhead,  and  is  applicable  in  industrial  continuous  deployment  contexts.
1	Proceedings  of  the  international  conference  on  software  and  system  process.  Welcome  to  the  International  Conference  on  Software  and  Systems  Process  (ICSSP)  held  in  Zurich,  Switzerland,  on  June  2nd  and  3rd,  2012.  Having  grown  from  a  Software  Process  Workshop  in  2005  to  the  International  Conference  on  Software  Process  (ICSP)  in  2007,  and  finally  to  the  International  Conference  on  Software  and  Systems  Process  (ICSSP)  in  2011,  this  conference  series  has  established  its  place  in  the  software  engineering  community  as  a  respected  conference  focusing  on  research  and  practice  related  to  software  and  systems  development  process.    Software  and  software  systems  development  use  processes  as  one  major  means  for  achieving  and  ensuring  the  quality  of  the  final  product.  This  leads  to  specific  challenges  regarding  software  processes  such  as  simplicity,  adaptability,  standard  conformance,  the  coordination  of  multiple  domains  and  lifecycles,  etc.  Processes  from  other  domains  (e.g.,  health  care,  manufacturing,  business,  aerospace,  automotive  systems,  and  others)  face  similar  challenges.  Yet,  they  share  interesting  similarities  with  and  differences  to  software  processes.  The  question  is:  How  can  we  in  the  software  domain  learn  from  other  domains  to  meet  our  software  process  engineering  challenges?    The  ICSSP  conferences  hence  connect  the  traditional  software  and  systems  process  community  with  outside  views  and  insights,  broadening  its  scope  of  software  development  processes  to  system  development  and  explicitly  including  processes  of  other  domains  such  as  health  care,  business,  and  manufacturing.  By  sharing  process  development  theories  and  practices  from  such  domains,  ICSSP  2012  is  investigating  novel  solutions  to  today's  process  challenges.  Correspondingly,  the  main  theme  of  ICSSP  2012  is  "Beyond  Software  and  Software  Systems  Processes",  signaling  our  goal  of  learning  from  domains  other  than  the  traditional  software  process  community.
1	Poster  precooked  developer  dashboards  what  to  show  and  how  to  use.  Designing  an  effective  and  useful  dashboard  is  expensive  and  it  would  be  important  to  determine  if  it  is  possible  to  elaborate  a  "generic"  useful  and  effective  dashboard,  usable  in  a  variety  of  circumstances.  To  determine  if  it  is  possible  to  develop  such  dashboard  and,  if  so,  its  structure  we  interviewed  67  software  engineers  from  44  different  companies.  Their  answers  made  us  confident  in  the  possibility  of  building  such  dashboard.
1	Contracting  agile  developments  for  mission  critical  systems  in  the  public  sector.  Although  Agile  is  a  well  established  software  development  paradigm,  major  concerns  arise  when  it  comes  to  contracting  issues  between  a  software  consumer  and  a  software  producer.  How  to  contractualize  the  Agile  production  of  software,  especially  for  security  &  mission  critical  organizations,  which  typically  outsource  software  projects,  has  been  a  major  concern  since  the  beginning  of  the  ""Agile  Era"".  In  literature,  little  has  been  done,  from  a  foundational  point  of  view  regarding  the  formalization  of  such  contracts.  Indeed,  when  the  development  is  outsourced,  the  management  of  the  contractual  life  is  non–trivial.  This  happens  because  the  interests  of  the  two  parties  are  typically  not  aligned.  In  these  situations,  software  houses  strive  for  the  minimization  of  the  effort,  while  the  customer  commonly  expects  high  quality  artifacts.  This  structural  asymmetry  can  hardly  be  overcome  with  traditional  ""Waterfall""  contracts.  In  this  work,  we  propose  a  foundational  approach  to  the  Law  &  Economics  of  Agile  contracts.  Moreover,  we  explore  the  key  elements  of  the  Italian  procurement  law  and  outline  a  suitable  solution  to  merge  some  basic  legal  constraints  with  Agile  requirements.  Finally,  a  case  study  is  presented,  describing  how  Agile  contracting  has  been  concretely  implemented  in  the  Italian  Defense  Acquisition  Process.  This  work  is  intended  to  be  a  framework  for  Agile  contracts  for  the  Italian  public  sector  of  critical  systems,  according  to  the  new  contractual  law  (Codice  degli  Appalti).
1	Providing  a  baseline  in  software  process  improvement  education  with  lego  scrum  simulations.  A  critical  aspect  of  software  process  education  in  general  and  software  process  improvement  (SPI)  education  in  particular  is  to  give  students  the  chance  to  experience  processes  and  issues  associated  with  process  at  first  hand.  This  is,  however,  often  difficult  in  an  educational  setting  since  providing  a  meaningful  project  in  which  to  apply  a  process  can  take  away  time  and  focus  from  the  intended  learning  objectives.  Instead,  miniatures  and  simulations  can  be  used  to  create  an  environment  in  which  students  can  interact  with  processes  directly  without  taking  up  large  parts  of  the  curriculum.  In  this  paper,  we  report  on  our  experience  of  using  Lego  Scrum  simulations  in  an  SPI  course  at  the  Bachelor  level.  The  simulations  are  used  both  to  introduce  a  baseline  for  the  students  to  let  them  experience  process  issues  directly,  create  an  improvement  plan  that  addresses  observed  issues,  and  to  apply  and  evaluate  the  plan  in  a  second  simulation.  This  allows  students  to  engage  with  SPI  methods  practically,  instead  of  purely  theoretically,  and  allows  the  teacher  to  refer  to  the  shared  experience  throughout  the  course.  The  collected  data  shows  that  the  approach  is  suitable,  but  that  students  struggle  with  the  demand  of  putting  an  improvement  plan  into  practice.  We  show  which  issues  commonly  occur  in  the  simulations  and  thus  allow  teachers  who  adopt  the  practice  to  scaffold  it  and  react  accordingly,  in  particular  to  empower  the  students  to  take  on  responsibility  for  the  improvement  of  the  process.
1	An  auto  scaling  system  for  api  gateway  based  on  kubernetes.  The  micro-service  approach  is  a  new  term  in  software  architecture  patterns  which  is  gaining  popularity  due  to  its  flexibility,  granular  approach  and  loosely  coupled  services  [1].  In  this  paper,  first,  we  present  an  API  Gateway  System  as  the  entrance  to  backend  services.  It  can  effectively  decrease  the  amount  of  remote  calls  between  applications  and  backend  services,  and  simplify  the  complexity  of  internal  services  calling  each  other.  Secondly  this  paper  designs  an  auto  scaling  system  for  API  Gateway  System  based  on  Kubernetes  and  Prometheus  which  can  dynamically  adjust  the  number  of  application  instances  according  to  its  own  load.  It  can  improve  the  utilization  of  system  resources  while  ensuing  the  high  availability  and  quality  of  the  application's  service.
1	The  mind  is  a  powerful  place  how  showing  code  comprehensibility  metrics  influences  code  understanding.  Static  code  analysis  tools  and  integrated  development  environments  present  developers  with  quality-related  software  metrics,  some  of  which  describe  the  understandability  of  source  code.  Software  metrics  influence  overarching  strategic  decisions  that  impact  the  future  of  companies  and  the  prioritization  of  everyday  software  development  tasks.  Several  software  metrics,  however,  lack  in  validation:  we  just  choose  to  trust  that  they  reflect  what  they  are  supposed  to  measure.  Some  of  them  were  even  shown  to  not  measure  the  quality  aspects  they  intend  to  measure.  Yet,  they  influence  us  through  biases  in  our  cognitive-driven  actions.  In  particular,  they  might  anchor  us  in  our  decisions.  Whether  the  anchoring  effect  exists  with  software  metrics  has  not  been  studied  yet.  We  conducted  a  randomized  and  double-blind  experiment  to  investigate  the  extent  to  which  a  displayed  metric  value  for  source  code  comprehensibility  anchors  developers  in  their  subjective  rating  of  source  code  comprehensibility,  whether  performance  is  affected  by  the  anchoring  effect  when  working  on  comprehension  tasks,  and  which  individual  characteristics  might  play  a  role  in  the  anchoring  effect.  We  found  that  the  displayed  value  of  a  comprehensibility  metric  has  a  significant  and  large  anchoring  effect  on  a  developer's  code  comprehensibility  rating.  The  effect  does  not  seem  to  affect  the  time  or  correctness  when  working  on  comprehension  questions  related  to  the  code  snippets  under  study.  Since  the  anchoring  effect  is  one  of  the  most  robust  cognitive  biases,  and  we  have  limited  understanding  of  the  consequences  of  the  demonstrated  manipulation  of  developers  by  non-validated  metrics,  we  call  for  an  increased  awareness  of  the  responsibility  in  code  quality  reporting  and  for  corresponding  tools  to  be  based  on  scientific  evidence.
1	Chaordic  learning  a  case  study.  Software  engineering  is  an  interactive,  collaborative  and  creative  activity  that  cannot  be  entirely  planned.  Inspection  and  adaption  are  required  to  cope  with  changes  during  the  development  process.  Software  engineering  education  requires  practical  application  of  knowledge,  but  it  is  challenging  and  time  consuming  for  instructors  to  evaluate  the  creation  of  innovative  solutions  to  problems.  Current  higher  education  practices  lead  to  a  multitude  of  rules,  guidelines  and  order.  Instructors  see  deviations  of  students  as  failures  and  limit  the  creative  thinking  processes  of  students.      In  this  paper  we  describe  chaordic  learning,  a  self-organizing,  adaptive  and  nonlinear  learning  approach,  to  stimulate  the  creative  thinking  of  students.  Instructors  provide  structure  and  guidance,  but  also  integrate  freedom  for  self-organization  and  self-guided  learning  and  embrace  innovation  and  creativity.  Deviations  are  seen  as  opportunities  and  failures  as  possibilities  for  students  to  learn  and  improve.  We  introduced  chaordic  learning  into  a  games  development  course  and  a  joint  advanced  student  school  and  describe  the  chaordic  process  of  these  courses  as  case  studies.  Students  in  these  courses  report  about  an  increased  intrinsic  motivation,  a  higher  level  of  self-organization  and  more  room  for  creativity  leading  to  an  improved  learning  experience  and  more  fun.
1	Taming  google  scale  continuous  testing.  Growth  in  Google's  code  size  and  feature  churn  rate  has  seen  increased  reliance  on  continuous  integration  (CI)  and  testing  to  maintain  quality.  Even  with  enormous  resources  dedicated  to  testing,  we  are  unable  to  regression  test  each  code  change  individually,  resulting  in  increased  lag  time  between  code  check-ins  and  test  result  feedback  to  developers.  We  report  results  of  a  project  that  aims  to  reduce  this  time  by:  (1)  controlling  test  workload  without  compromising  quality,  and  (2)  distilling  test  results  data  to  inform  developers,  while  they  write  code,  of  the  impact  of  their  latest  changes  on  quality.  We  model,  empirically  understand,  and  leverage  the  correlations  that  exist  between  our  code,  test  cases,  developers,  programming  languages,  and  code-change  and  test-execution  frequencies,  to  improve  our  CI  and  development  processes.  Our  findings  show:  very  few  of  our  tests  ever  fail,  but  those  that  do  are  generally  "closer"  to  the  code  they  test,  certain  frequently  modified  code  and  certain  users/tools  cause  more  breakages,  and  code  recently  modified  by  multiple  developers  (more  than  3)  breaks  more  often.
1	Scalable  approaches  for  test  suite  reduction.  Test  suite  reduction  approaches  aim  at  decreasing  software  regression  testing  costs  by  selecting  a  representative  subset  from  large-size  test  suites.  Most  existing  techniques  are  too  expensive  for  handling  modern  massive  systems  and  moreover  depend  on  artifacts,  such  as  code  coverage  metrics  or  specification  models,  that  are  not  commonly  available  at  large  scale.  We  present  a  family  of  novel  very  efficient  approaches  for  similarity-based  test  suite  reduction  that  apply  algorithms  borrowed  from  the  big  data  domain  together  with  smart  heuristics  for  finding  an  evenly  spread  subset  of  test  cases.  The  approaches  are  very  general  since  they  only  use  as  input  the  test  cases  themselves  (test  source  code  or  command  line  input).  We  evaluate  four  approaches  in  a  version  that  selects  a  fixed  budget  B  of  test  cases,  and  also  in  an  adequate  version  that  does  the  reduction  guaranteeing  some  fixed  coverage.  The  results  show  that  the  approaches  yield  a  fault  detection  loss  comparable  to  state-of-the-art  techniques,  while  providing  huge  gains  in  terms  of  efficiency.  When  applied  to  a  suite  of  more  than  500K  real  world  test  cases,  the  most  efficient  of  the  four  approaches  could  select  B  test  cases  (for  varying  B  values)  in  less  than  10  seconds.
1	Automated  fine  grained  requirements  to  code  traceability  link  recovery.  Problem:  Existing  approaches  for  requirements-to-code  traceability  link  recovery  rely  on  text  retrieval  to  trace  requirements  to  coarse-grained  code  documents  (e.g.,  methods,  files,  classes,  etc.),  while  suffering  from  low  accuracy  problems.  Hypotheses:  The  salient  information  in  most  requirements  is  expressed  as  functional  constraints,  which  can  be  automatically  identified  and  categorized.  Moreover,  people  use  recognizable  discourse  patterns  when  describing  them  and  developers  use  well-defined  patterns  for  implementing  them.  Contributions:  Recasting  the  requirements-to-code  traceability  link  problem  as  an  accurate  matching  between  functional  constraints  and  their  implementation.
1	Free  hugs  praising  developers  for  their  actions.  Developing  software  is  a  complex,  intrinsically  intellectual,  and  therefore  ephemeral  activity,  also  due  to  the  intangible  nature  of  the  end  product,  the  source  code.  There  is  a  thin  red  line  between  a  productive  development  session,  where  a  developer  actually  does  something  useful  and  productive,  and  a  session  where  the  developer  essentially  produces  "fried  air",  pieces  of  code  whose  quality  and  usefulness  are  doubtful  at  best.  We  believe  that  well-thought  mechanisms  of  gamification  built  on  fine-grained  interaction  information  mined  from  the  IDE  can  crystallize  and  reward  good  coding  behavior.      We  present  our  preliminary  experience  with  the  design  and  implementation  of  a  micro-gamification  layer  built  into  an  object-oriented  IDE,  which  at  the  end  of  each  development  session  not  only  helps  the  developer  to  understand  what  he  actually  produced,  but  also  praises  him  in  case  the  development  session  was  productive.  Building  on  this,  we  envision  an  environment  where  the  IDE  reflects  on  the  deeds  of  the  developers  and  by  providing  a  historical  view  also  helps  to  track  and  reward  long-term  growth  in  terms  of  development  skills,  not  dissimilar  from  the  mechanics  of  role-playing  games.
1	Crunch  time  the  reasons  and  effects  of  unpaid  overtime  in  the  games  industry.  The  games  industry  is  notorious  for  its  intense  work  ethics  with  uncompensated  overtime  and  weekends  at  the  office,  also  known  as  crunch  or  crunch  time.  Since  crunch  time  is  so  common  within  the  industry,  is  it  possible  that  the  benefits  of  crunch  time  outweigh  the  disadvantages?  By  studying  postmortems  and  conducting  interviews  with  employees  in  the  industry,  we  aim  to  characterise  crunch  time  and  discover  its  effects  on  the  industry.  We  provide  a  classification  of  crunch,  i.e.,  four  types  of  crunch  which  all  have  distinct  characteristics  and  affect  the  product,  employees  and  schedule  in  various  ways.  One  of  the  crunch  types  stands  out  from  the  others  by  only  having  positive  effects  on  product  and  schedule.  A  characteristic  that  all  of  the  types  have  in  common  is  an  increase  in  stress  levels  amongst  the  employees.  We  identify  a  set  of  reasons  for  crunch  and  show  that  crunch  is  less  pronounced  in  game  studios  where  prioritisation  of  features  is  a  regular  practice.
1	The  hardware  design  and  implementation  of  four  channel  uhf  rfid  reader  based  on  impinj  r2000.  It  is  well  known  that  UHF  RFID,  one  of  the  key  technologies  of  Internet  of  Things,  is  becoming  the  mainstream  research  of  RFID  applications.  In  a  RFID  system,  reader  plays  an  important  role,  however,  RFID  reader  with  only  one  channel  often  cannot  satisfy  the  application  requirements  due  to  the  complicated  working  environment.  This  study  introduces  a  novel  design  solution  of  Four-channel  RFID  reader  based  on  Impinj  R2000,  Atmel's  AT91SAM7S256-MU  is  chosen  as  the  microcontroller.  Moreover,  concepts  and  methods  of  the  hardware  design,  such  as  components  selection,  design  of  schematic  circuit  diagram  and  PCB,  etc.  are  expatiated  in  the  paper.  According  to  the  testing  results,  when  the  transmit  power  is  set  to  be  25dBm,  the  reader  can  achieve  reading  in  a  round-robin  manner  excellently  and  the  readable  range  for  passive  tags  can  reach  2.5m.
1	Mind  the  gap  assessing  the  conformance  of  software  traceability  to  relevant  guidelines.  Many  guidelines  for  safety-critical  industries  such  as  aeronautics,  medical  devices,  and  railway  communications,  specify  that  traceability  must  be  used  to  demonstrate  that  a  rigorous  process  has  been  followed  and  to  provide  evidence  that  the  system  is  safe  for  use.  In  practice,  there  is  a  gap  between  what  is  prescribed  by  guidelines  and  what  is  implemented  in  practice,  making  it  difficult  for  organizations  and  certifiers  to  fully  evaluate  the  safety  of  the  software  system.  In  this  paper  we  present  an  approach,  which  parses  a  guideline  to  extract  a  Traceability  Model  depicting  software  artifact  types  and  their  prescribed  traces.  It  then  analyzes  the  traceability  data  within  a  project  to  identify  areas  of  traceability  failure.  Missing  traceability  paths,  redundant  and/or  inconsistent  data,  and  other  problems  are  highlighted.  We  used  our  approach  to  evaluate  the  traceability  of  seven  safety-critical  software  systems  and  found  that  none  of  the  evaluated  projects  contained  traceability  that  fully  conformed  to  its  relevant  guidelines.
1	Is  spreadsheet  ambiguity  harmful  detecting  and  repairing  spreadsheet  smells  due  to  ambiguous  computation.  Spreadsheets  are  widely  used  by  end  users  for  numerical  computation  in  their  business.  Spreadsheet  cells  whose  computation  is  subject  to  the  same  semantics  are  often  clustered  in  a  row  or  column.  When  a  spreadsheet  evolves,  these  cell  clusters  can  degenerate  due  to  ad  hoc  modifications  or  undisciplined  copy-and-pastes.  Such  degenerated  clusters  no  longer  keep  cells  prescribing  the  same  computational  semantics,  and  are  said  to  exhibit  ambiguous  computation  smells.  Our  empirical  study  finds  that  such  smells  are  common  and  likely  harmful.  We  propose  AmCheck,  a  novel  technique  that  automatically  detects  and  repairs  ambiguous  computation  smells  by  recovering  their  intended  computational  semantics.  A  case  study  using  AmCheck  suggests  that  it  is  useful  for  discovering  and  repairing  real  spreadsheet  problems.
1	Manual  refactoring  changes  with  automated  refactoring  validation.  Refactoring,  the  practice  of  applying  behavior-preserving  changes  to  existing  code,  can  enhance  the  quality  of  software  systems.  Refactoring  tools  can  automatically  perform  and  check  the  correctness  of  refactorings.  However,  even  when  developers  have  these  tools,  they  still  perform  about  90%  of  refactorings  manually,  which  is  error-prone.  To  address  this  problem,  we  propose  a  technique  called  GhostFactor  separating  transformation  and  correctness  checking:  we  allow  the  developer  to  transform  code  manually,  but  check  the  correctness  of  her  transformation  automatically.  We  implemented  our  technique  as  a  Visual  Studio  plugin,  then  evaluated  it  with  a  human  study  of  eight  software  developers;  GhostFactor  improved  the  correctness  of  manual  refactorings  by  67%.
1	Extracting  conceptual  interoperability  constraints  from  api  documentation  using  machine  learning.  Successfully  using  a  software  web-service/platform  API  requires  satisfying  its  conceptual  interoperability  constraints  that  are  stated  within  its  shared  documentation.  However,  manual  and  unguided  analysis  of  text  in  API  documents  is  a  tedious  and  time  consuming  task.  In  this  work,  we  present  our  empirical-based  methodology  of  using  machine  learning  techniques  for  automatically  identifying  conceptual  interoperability  constraints  from  natural  language  text.  We  also  show  some  initial  promising  results  of  our  research.
1	A  collaboration  based  testing  model  for  composite  components.  In  this  paper,  we  present  a  testing  model  to  support  the  verification  and  validation  for  composite  components.  The  testing  model  supports  two  types  of  components:  atomic  and  composite.  Atomic  components  are  non-decomposable  software  units,  while  composite  components  are  developed  by  composing  existing  components.  Our  main  contribution  presented  in  this  paper  focusing  on  two  issues:  (1)  to  provide  a  testing  model  for  atomic  components  while  these  components  are  being  designed  and  developed;  and  (2)  to  provide  a  mechanism  to  construct  new  testing  models  for  composite  components  by  composing  existing  testing  models  of  atomic  components.  In  addition,  we  address  how  our  integrating  testing  model  can  support  both  centralized  and  distributed  testing  of  composite  components.
1	Automated  design  of  self  adaptive  software  with  control  theoretical  formal  guarantees.  Self-adaptation  enables  software  to  execute  successfully  in  dynamic,  unpredictable,  and  uncertain  environments.          Control  theory  provides  a  broad  set  of  mathematically  grounded  techniques  for  adapting  the  behavior  of  dynamic  systems.  While  it  has  been  applied  to  specific  software  control  problems,  it  has  proved  difficult  to  define  methodologies  allowing  non-experts  to  systematically  apply  control  techniques  to  create  adaptive  software.  These  difficulties  arise  because  computer  systems  are  usually  non-linear,  with  varying  workloads  and  heterogeneous  components,  making  it  difficult  to  model  software  as  a  dynamic  system;  i.e.,  by  means  of  differential  or  difference  equations.          This  paper  proposes  a  broad  scope  methodology  for  automatically  constructing  both  an  approximate  dynamic  model  of  a  software  system  and  a  suitable  controller  for  managing  its  non-functional  requirements.  Despite  its  generality,  this  methodology  provides  formal  guarantees  concerning  the  system's  dynamic  behavior  by  keeping  its  model  continuously  updated  to  compensate  for  changes  in  the  execution  environment  and  effects  of  the  initial  approximation.          We  apply  the  methodology  to  three  case  studies,  demonstrating  its  generality  by  tackling  different  domains  (and  different  non-functional  requirements)  with  the  same  approach.  Being  broadly  applicable  and  fully  automated,  this  methodology  may  allow  the  adoption  of  control  theoretical  solutions  (and  their  formal  properties)  for  a  wide  range  of  software  adaptation  problems.
1	Sentiment  analysis  in  jira  software  repositories.  Mining  Software  Repositories  is  a  field  of  study  whose  main  task  is  to  extract  valuable  information  from  a  large  amount  of  data  available  within  software  repositories.  This  information  about  systems  and  projects  can  be  exploited  in  different  ways  improving  the  development  processes  in  Software  Engineering.  A  new  research  area  makes  use  of  this  data  for  analyzing  the  software  professionals’  emotional  state  and  their  relationship  with  different  factors  such  as  productivity  and  quality  in  tasks.  In  our  study,  we  applied  a  supervised  classification  model  to  predict  sentiments  contained  in  issue  comments  in  open  source  projects  hosted  in  Jira  repositories.  The  main  objective  is  verifying  if  sentiments  are  related  to  issue  comments  and  factors  such  as  issue  resolution  types  (Resolved  or  Unresolved)  as  well  as  the  time  of  day  and  the  day  of  the  week,  in  which  the  comments  were  written.  Our  results  show  that  comments  in  unresolved  issues  tend  to  express  less  positive  and  more  negative  sentiments  regarding  the  comments  in  resolved  issues.  In  addition,  we  also  obtained  interesting  results  regarding  the  sentiments  in  comments  and  the  time/day  of  the  week  of  publication.
1	Improving  code  recommendations  by  combining  neural  and  classical  machine  learning  approaches.  Code  recommendation  systems  for  software  engineering  are  designed  to  accelerate  the  development  of  large  software  projects.  A  classical  example  is  code  completion  or  next  token  prediction  offered  by  modern  integrated  development  environments.  A  particular  challenging  case  for  such  systems  are  dynamic  languages  like  Python  due  to  limited  type  information  at  editing  time.  Recently,  researchers  proposed  machine  learning  approaches  to  address  this  challenge.  In  particular,  the  Probabilistic  Higher  Order  Grammar  technique  (Bielik  et  al.,  ICML  2016)  uses  a  grammar-based  approach  with  a  classical  machine  learning  schema  to  exploit  local  context.  A  method  by  Li  et  al.,  (IJCAI  2018)  uses  deep  learning  methods,  in  detail  a  Recurrent  Neural  Network  coupled  with  a  Pointer  Network.  We  compare  these  two  approaches  quantitatively  on  a  large  corpus  of  Python  files  from  GitHub.  We  also  propose  a  combination  of  both  approaches,  where  a  neural  network  decides  which  schema  to  use  for  each  prediction.  The  proposed  method  achieves  a  slightly  better  accuracy  than  either  of  the  systems  alone.  This  demonstrates  the  potential  of  ensemble-like  methods  for  code  completion  and  recommendation  tasks  in  dynamically  typed  languages.
1	Stuck  and  frustrated  or  in  flow  and  happy  sensing  developers  emotions  and  progress.  Software  developers  working  on  change  tasks  commonly  experience  a  broad  range  of  emotions,  ranging  from  happiness  all  the  way  to  frustration  and  anger.  Research,  primarily  in  psychology,  has  shown  that  for  certain  kinds  of  tasks,  emotions  correlate  with  progress  and  that  biometric  measures,  such  as  electro-dermal  activity  and  electroencephalography  data,  might  be  used  to  distinguish  between  emotions.  In  our  research,  we  are  building  on  this  work  and  investigate  developers'  emotions,  progress  and  the  use  of  biometric  measures  to  classify  them  in  the  context  of  software  change  tasks.  We  conducted  a  lab  study  with  17  participants  working  on  two  change  tasks  each.  Participants  were  wearing  three  biometric  sensors  and  had  to  periodically  assess  their  emotions  and  progress.  The  results  show  that  the  wide  range  of  emotions  experienced  by  developers  is  correlated  with  their  perceived  progress  on  the  change  tasks.  Our  analysis  also  shows  that  we  can  build  a  classifier  to  distinguish  between  positive  and  negative  emotions  in  71.36%  and  between  low  and  high  progress  in  67.70%  of  all  cases.  These  results  open  up  opportunities  for  improving  a  developer's  productivity.  For  instance,  one  could  use  such  a  classifier  for  providing  recommendations  at  opportune  moments  when  a  developer  is  stuck  and  making  no  progress.
1	On  demand  feature  recommendations  derived  from  mining  public  product  descriptions.  We  present  a  recommender  system  that  models  and  recommends  product  features  for  a  given  domain.  Our  approach  mines  product  descriptions  from  publicly  available  online  specifications,  utilizes  text  mining  and  a  novel  incremental  diffusive  clustering  algorithm  to  discover  domain-specific  features,  generates  a  probabilistic  feature  model  that  represents  commonalities,  variants,  and  cross-category  features,  and  then  uses  association  rule  mining  and  the  k-Nearest-Neighbor  machine  learning  strategy  to  generate  product  specific  feature  recommendations.  Our  recommender  system  supports  the  relatively  labor-intensive  task  of  domain  analysis,  potentially  increasing  opportunities  for  re-use,  reducing  time-to-market,  and  delivering  more  competitive  software  products.  The  approach  is  empirically  validated  against  20  different  product  categories  using  thousands  of  product  descriptions  mined  from  a  repository  of  free  software  applications.
1	Supporting  architectural  design  decisions  evolution  through  model  driven  engineering.  Architectural  design  decisions  (i.e.,  those  decisions  made  when  architecting  software  systems)  are  considered  an  essential  piece  of  knowledge  to  be  carefully  documented  and  maintained.  As  any  other  artifact,  architectural  design  decisions  may  evolve,  having  an  impact  on  other  design  decisions,  or  on  related  artifacts  (like  requirements  and  architectural  elements).  It  is  therefore  important  to  document  and  analyze  the  impact  of  an  evolving  decision  on  other  related  decisions  or  artifacts.  In  this  work  we  propose  an  approach  based  on  a  notation-independent  metamodel  that  becomes  a  means  for  systematically  defining  traceability  links,  enabling  inter-decision  and  extra-decision  evolution  impact  analysis.  The  purpose  of  such  an  analysis  is  to  check  the  presence  of  inconsistencies  that  may  occur  during  evolution.  An  Eclipse  plugin  has  been  realized  to  implement  the  approach.
1	Bridging  the  divide  between  software  developers  and  operators  using  logs.  There  is  a  growing  gap  between  the  software  development  and  operation  worlds.  Software  developers  rarely  divulge  development  knowledge  about  the  software  to  operators,  while  operators  rarely  communicate  field  knowledge  to  developers.  To  improve  the  quality  and  reduce  the  operational  cost  of  large-scale  software  systems,  bridging  the  gap  between  these  two  worlds  is  essential.  This  thesis  proposes  the  use  of  logs  as  mechanism  to  bridge  the  gap  between  these  two  worlds.  Logs  are  messages  generated  from  statements  inserted  by  developers  in  the  source  code  and  are  often  used  by  operators  for  monitoring  the  field  operation  of  a  system.  However,  the  rich  knowledge  in  logs  has  not  yet  been  fully  used  because  of  their  non-structured  nature,  their  large  scale,  and  the  use  of  the  ad  hoc  log  analysis  techniques.  Through  case  studies  on  large  commercial  and  open  source  systems,  we  plan  to  demonstrate  the  value  of  logs  as  a  tool  to  support  developers  and  operators.
1	Computational  alignment  of  goals  and  scenarios  for  complex  systems.  The  purpose  of  requirements  validation  is  to  determine  whether  a  large  requirements  set  will  lead  to  the  achievement  of  system-related  goals  under  different  conditions  -  a  task  that  needs  automation  if  it  is  to  be  performed  quickly  and  accurately.  One  reason  for  the  current  lack  of  software  tools  to  undertake  such  validation  is  the  absence  of  the  computational  mechanisms  needed  to  associate  scenario,  system  specification  and  goal  analysis  tools.  Therefore,  in  this  paper,  we  report  first  research  experiments  in  developing  these  new  capabilities,  and  demonstrate  them  with  a  non-trivial  example  associated  with  a  Rolls  Royce  aircraft  engine  software  component.
1	Pac  learning  based  verification  and  model  synthesis.  We  introduce  a  novel  technique  for  verification  and  model  synthesis  of  sequential  programs.  Our  technique  is  based  on  learning  an  approximate  regular  model  of  the  set  of  feasible  paths  in  a  program,  and  testing  whether  this  model  contains  an  incorrect  behavior.  Exact  learning  algorithms  require  checking  equivalence  between  the  model  and  the  program,  which  is  a  difficult  problem,  in  general  undecidable.  Our  learning  procedure  is  therefore  based  on  the  framework  of  probably  approximately  correct  (PAC)  learning,  which  uses  sampling  instead,  and  provides  correctness  guarantees  expressed  using  the  terms  error  probability  and  confidence.  Besides  the  verification  result,  our  procedure  also  outputs  the  model  with  the  said  correctness  guarantees.  Obtained  preliminary  experiments  show  encouraging  results,  in  some  cases  even  outperforming  mature  software  verifiers.
1	Preliminary  interdependency  analysis  pia  method  and  tool  support.  One  of  the  greatest  challenges  in  enhancing  the  protection  of  Critical  Infrastructures  (CIs)  against  accidents,  natural  disasters,  and  acts  of  terrorism  is  establishing  and  maintaining  an  understanding  of  the  interdependencies  between  infrastructures.  Understanding  interdependencies  is  a  challenge  both  for  governments  and  for  infrastructure  owners/operators.  Both,  to  a  different  extent,  have  an  interest  in  services  and  tools  that  can  enhance  their  risk  assessment  and  management  to  mitigate  large  failures  that  may  propagate  across  infrastructures.  The  abstract  presents  an  approach  (the  method  and  tool  support)  to  interdependency  analysis  developed  recently  by  Centre  for  Software  Reliability,  City  University  London.  The  method  progresses  from  a  qualitative  phase  during  which  a  fairly  abstract  model  is  built  of  interacting  infrastructures.  Via  steps  of  incremental  refinement  more  detailed  models  are  built,  which  allow  for  quantifying  interdependencies.  The  tool  support  follows  the  methodology  and  allows  analysts  to  build  quickly  models  at  the  appropriate  level  of  abstraction  (qualitative  or  very  detailed  including  deterministic  models  specific  for  the  particular  domain,  e.g.  various  flow  models).  The  approach  was  successfully  applied  to  a  large  scale  case-study  with  more  than  800  modeling  elements.
1	A  formal  model  for  constraint  based  deployment  calculation  and  analysis  for  fault  tolerant  systems.  In  many  embedded  systems  like  in  the  automotive  domain,  safety-critical  features  are  increasingly  realized  by  software.  Some  of  these  features  are  often  required  to  behave  fail-operational,  meaning  that  they  must  stay  alive  even  in  the  presence  of  random  hardware  failures.
1	Studying  and  suggesting  logging  locations  in  code  blocks.  Developers  write  logging  statements  to  generate  logs  and  record  system  execution  behaviors  to  assist  in  debugging  and  software  maintenance.  However,  there  exists  no  practical  guidelines  on  where  to  write  logging  statements.  On  one  hand,  adding  too  many  logging  statements  may  introduce  superfluously  trivial  logs  and  performance  overheads.  On  the  other  hand,  logging  too  little  may  miss  necessary  runtime  information.  Thus,  properly  deciding  the  logging  location  is  a  challenging  task  and  a  finer-grained  under-standing  of  where  to  write  logging  statements  is  needed  to  assist  developers  in  making  logging  decisions.  In  this  paper,  we  conduct  a  comprehensive  study  to  uncover  guidelines  on  logging  locations  at  the  code  block  level.  We  analyze  logging  statements  and  their  surrounding  code  by  combining  both  deep  learning  techniques  and  manual  investigations.  From  our  preliminary  results,  we  find  that  our  deep  learning  models  achieve  over  90%  in  precision  and  recall  when  trained  using  the  syntactic  (e.g.,  nodes  in  abstract  syntax  tree)  and  semantic  (e.g.,  variable  names)  features.  However,  cross-system  models  trained  using  semantic  features  only  have  45.6%  in  precision  and  73.2%  in  recall,  while  models  trained  using  syntactic  features  still  have  over  90%  precision  and  recall.  Our  current  progress  high-lights  that  there  is  an  implicit  syntactic  logging  guideline  across  systems,  and  such  information  may  be  leveraged  to  uncover  general  logging  guidelines.
1	Identification  of  cultural  influences  on  requirements  engineering  activities.  Requirements  Engineering  (RE)  involves  critical  activities  to  ensure  the  accurate  elicitation  and  documentation  of  clients'  requirements.  RE  is  a  socio-technical  activity  and  requires  intensive  communication  with  several  clients.  RE  activities  might  be  considerably  influenced  by  individuals'  cultural  background  because  culture  has  a  deep  impact  on  the  way  in  which  people  communicate.  However,  there  has  been  limited  exploration  of  this  issue.  We  present  a  framework  that  identifies  and  analyses  cultural  influences  on  RE  activities.  To  build  the  framework,  we  employed  Hofstede's  cultural  model  and  a  mixed-methods  design  comprising  two  case  studies  involving  two  cultures:  Saudi  Arabia  and  Australia.  The  evaluation  highlighted  that  the  framework  provides  high  accuracy  to  identify  cultural  influences  in  other  cultures  as  well.
1	Testing  central  processing  unit  scheduling  algorithms  using  metamorphic  testing.  Central  Processing  Unit  (CPU)  scheduling  is  used  to  allocate  CPU  for  multiple  processes.  CPU  is  one  of  the  most  important  resources  in  the  computer  system,  and  its  scheduling  is  vital  and  influential  in  operating  systems.  Thus,  it  is  necessary  to  ensure  the  correctness  of  the  CPU  scheduling  program.  However,  testing  the  correctness  of  a  scheduling  program  is  difficult  because  it  is  hard  to  verify  the  correctness  of  its  output,  which  is  known  as  the  test  oracle  problem  in  software  testing.  Metamorphic  Testing  (MT)  which  has  been  recently  proposed  to  alleviate  the  test  oracle  problem,  is  applied  to  test  the  CPU  scheduling  program.  In  this  paper,  we  use  MT  to  test  the  Highest  Response  Ratio  Next  (HRRN)  scheduling  algorithm.  Two  simulators  of  HRRN  scheduler  are  used  in  the  evaluation  of  our  method.  Surprisingly,  some  real  life  faults  in  one  open  source  simulator  are  detected  by  MT.  Further  experiments  are  performed  based  on  mutants,  and  the  experimental  results  show  that  MT  is  an  effective  strategy  to  test  CPU  scheduler.
1	Yoda  young  and  newcomer  developer  assistant.  Mentoring  project  newcomers  is  a  crucial  activity  in  software  projects,  and  requires  to  identify  people  having  good  communication  and  teaching  skills,  other  than  high  expertise  on  specific  technical  topics.  In  this  demo  we  present  Yoda  (Young  and  newcOmer  Developer  Assistant),  an  Eclipse  plugin  that  identifies  and  recommends  mentors  for  newcomers  joining  a  software  project.  Yoda  mines  developers'  communication  (e.g.,  mailing  lists)  and  project  versioning  systems  to  identify  mentors  using  an  approach  inspired  to  what  ArnetMiner  does  when  mining  advisor/student  relations.  Then,  it  recommends  appropriate  mentors  based  on  the  specific  expertise  required  by  the  newcomer.  The  demo  shows  Yoda  in  action,  illustrating  how  the  tool  is  able  to  identify  and  visualize  mentoring  relations  in  a  project,  and  suggest  appropriate  mentors  for  a  developer  who  is  going  to  work  on  certain  source  code  files,  or  on  a  given  topic.  Demo  URL:  http://youtu.be/4yrbYT-LAXA.
1	Metamorphic  fault  tolerance  an  automated  and  systematic  methodology  for  fault  tolerance  in  the  absence  of  test  oracle.  A  system  may  fail  due  to  an  internal  bug  or  a  fault  in  its  execution  environment.  Incorporating  fault  tolerance  strategies  enables  such  system  to  complete  its  function  despite  the  failure  of  some  of  its  parts.  Prior  to  the  execution  of  some  fault  tolerance  strategies,  failure  detection  is  needed.  Detecting  incorrect  output,  for  instance,  assumes  the  existence  of  an  oracle  to  check  the  correctness  of  program  outputs  given  an  input.  However,  in  many  practical  situations,  oracle  does  not  exist  or  is  extremely  difficult  to  apply.  Such  an  oracle  problem  is  a  major  challenge  in  the  context  of  software  testing.  In  this  paper,  we  propose  to  apply  metamorphic  testing,  a  software  testing  method  that  alleviates  the  oracle  problem,  into  fault  tolerance.  The  proposed  technique  supports  failure  detection  without  the  need  of  oracles.
1	A  practical  guide  for  using  statistical  tests  to  assess  randomized  algorithms  in  software  engineering.  Randomized  algorithms  have  been  used  to  successfully  address  many  different  types  of  software  engineering  problems.  This  type  of  algorithms  employ  a  degree  of  randomness  as  part  of  their  logic.  Randomized  algorithms  are  useful  for  difficult  problems  where  a  precise  solution  cannot  be  derived  in  a  deterministic  way  within  reasonable  time.  However,  randomized  algorithms  produce  different  results  on  every  run  when  applied  to  the  same  problem  instance.  It  is  hence  important  to  assess  the  effectiveness  of  randomized  algorithms  by  collecting  data  from  a  large  enough  number  of  runs.  The  use  of  rigorous  statistical  tests  is  then  essential  to  provide  support  to  the  conclusions  derived  by  analyzing  such  data.  In  this  paper,  we  provide  a  systematic  review  of  the  use  of  randomized  algorithms  in  selected  software  engineering  venues  in  2009.  Its  goal  is  not  to  perform  a  complete  survey  but  to  get  a  representative  snapshot  of  current  practice  in  software  engineering  research.  We  show  that  randomized  algorithms  are  used  in  a  significant  percentage  of  papers  but  that,  in  most  cases,  randomness  is  not  properly  accounted  for.      This  casts  doubts  on  the  validity  of  most  empirical  results  assessing  randomized  algorithms.  There  are  numerous  statistical  tests,  based  on  different  assumptions,  and  it  is  not  always  clear  when  and  how  to  use  these  tests.  We  hence  provide  practical  guidelines  to  support  empirical  research  on  randomized  algorithms  in  software  engineering
1	A  significant  approach  for  cloud  database  using  shared  disk  architecture.  A  Cloud  database  is  a  database  that  relies  on  cloud  technology.  Both  the  database  and  most  of  its  DBMS  reside  remotely,  "in  the  cloud,"  while  its  applications  are  both  developed  by  programmers  and  later  maintained  and  utilized  by  (application's)  end-users  through  a  Web  browser  and  Open  APIs.  More  and  more  such  database  products  are  emerging,  both  of  new  vendors  and  by  virtually  all  established  database  vendors  are  increasing  drastically.  Previously,  there  are  many  database  architecture  viz.,  shared-nothing,  shared  cache,  nonsql.  Proposed  for  maintaining  data  in  different  storage  systems  like  Oracle,  IBM  DB2,  Microsoft  SQL  Server,  Microsoft  Access,  PostgreSQL  and  MySQL.  The  paper  discusses  on  the  effective  usage  of  database  sharing  and  lays  more  emphasis  on  the  perfect  handling  of  data  that  resides  in  various  remote  places.  The  vital  role  of  the  data  that  stores  in  databases  has  more  security,  time  consuming  problems  in  the  cloud  computing.  But,  among  them  the  shared  disk  architecture  is  well  suited  for  cloud  environment  since  the  data  is  stored  in  remote  place.  The  shared-disk  database  architecture  is  ideally  suites  to  cloud  computing.  The  shared-disk  architecture  requires  fewer  and  lower-cost  servers,  it  provides  high-availability,  it  reduces  maintenance  costs  by  eliminating  partitioning,  and  it  delivers  dynamic  scalability  in  cloud.
1	Fault  tolerant  interactive  cockpits  for  critical  applications  overall  approach.  The  deployment  of  interactive  facilities  in  avionic  digital  cockpits  for  critical  applications  is  a  challenge  today.  The  dependability  of  the  user  interface  and  its  related  supporting  software  must  be  consistent  with  the  criticality  of  the  functions  to  be  controlled.  The  approach  proposed  in  this  paper  aims  at  describing  how  fault  prevention  and  fault  tolerance  techniques  can  be  combined  to  address  this  challenge.  Following  the  ARINC  661  standard,  a  model-based  development  of  interactive  objects  (namely  widgets  and  layers)  aims  at  providing  zero-default  software.  Regarding  remaining  software  faults  in  the  underlying  runtime  support  and  also  physical  faults,  the  approach  is  based  on  fault  tolerance  design  patterns,  like  self-checking  components  and  replication  techniques.  The  proposed  solution  relies  on  the  space  and  time  partitioning  provided  by  the  executive  support  following  the  ARINC  653  standard.  Defining  and  designing  resilient  interactive  cockpits  is  a  necessity  in  the  near  future  as  these  command  and  control  systems  provide  a  great  opportunity  to  improve  maintenance  and  evolutivity  of  avionic  systems.
1	Medusa  blockchain  powered  log  storage  system.  Blockchain  is  one  of  the  most  heavily  invested  technologies  in  recent  years.  Due  to  its  tamper-proof  and  decentralization  properties,  blockchain  has  become  an  ideal  utility  for  data  storage  that  is  applicable  in  many  real  world  industrial  scenarios.  One  important  scenario  is  web  log,  which  is  treated  as  sources  of  technical  significance  and  commercial  revenues  in  major  internet  companies.  In  this  paper,  we  illustrate  our  design  of  a  web  log  storage  system  based  on  HyperLedger.  HyperLedger  yields  higher  throughput  and  lower  latency  compared  with  other  blockchain  systems.  Alongside  its  efficiency  advantages.,  HyperLeger  is  a  permissioned  blockchain,  which  is  an  ideal  fit  for  enterprise  software  design  scenario.
1	The  holy  grail  of  quantum  artificial  intelligence  major  challenges  in  accelerating  the  machine  learning  pipeline.  We  discuss  the  synergetic  connection  between  quantum  computing  and  artificial  intelligence.  After  surveying  current  approaches  to  quantum  artificial  intelligence  and  relating  them  to  a  formal  model  for  machine  learning  processes,  we  deduce  four  major  challenges  for  the  future  of  quantum  artificial  intelligence:  (i)  Replace  iterative  training  with  faster  quantum  algorithms,  (ii)  distill  the  experience  of  larger  amounts  of  data  into  the  training  process,  (iii)  allow  quantum  and  classical  components  to  be  easily  combined  and  exchanged,  and  (iv)  build  tools  to  thoroughly  analyze  whether  observed  benefits  really  stem  from  quantum  properties  of  the  algorithm.
1	Model  based  testing  under  parametric  variability  of  uncertain  beliefs.  Modern  software  systems  operate  in  complex  and  changing  environments  and  are  exposed  to  multiple  sources  of  uncertainty.  Considering  uncertainty  as  a  first-class  concern  in  software  testing  is  currently  on  an  uptrend.  This  paper  introduces  a  novel  methodology  to  deal  with  testing  under  uncertainty.  Our  proposal  combines  the  usage  of  parametric  model  checking  at  design-time  and  online  model-based  testing  algorithms  to  gather  runtime  evidence  and  detect  requirements  violations.  As  modeling  formalism,  we  adopt  parametric  Markov  Decision  Processes  where  transition  probabilities  are  not  fixed,  but  are  possibly  given  as  a  set  of  uncertain  parameters.  The  design-time  phase  aims  at  analyzing  the  parameter  space  to  identify  the  constraints  for  requirements  satisfaction.  Then,  the  testing  activity  applies  a  Bayesian  inference  process  to  identify  violations  of  pre-computed  constraints.  An  extensive  empirical  evaluation  shows  that  the  proposed  technique  is  effective  in  discovering  violations  and  is  cheaper  than  existing  testing  under  uncertainty  methods.
1	What  makes  a  great  software  engineer.  Good  software  engineers  are  essential  to  the  creation  of  good  software.  However,  most  of  what  we  know  about  software-engineering  expertise  are  vague  stereotypes,  such  as  'excellent  communicators'  and  'great  teammates'.  The  lack  of  specificity  in  our  understanding  hinders  researchers  from  reasoning  about  them,  employers  from  identifying  them,  and  young  engineers  from  becoming  them.  Our  understanding  also  lacks  breadth:  what  are  all  the  distinguishing  attributes  of  great  engineers  (technical  expertise  and  beyond)?  We  took  a  first  step  in  addressing  these  gaps  by  interviewing  59  experienced  engineers  across  13  divisions  at  Microsoft,  uncovering  53  attributes  of  great  engineers.  We  explain  the  attributes  and  examine  how  the  most  salient  of  these  impact  projects  and  teams.  We  discuss  implications  of  this  knowledge  on  research  and  the  hiring  and  training  of  engineers.
1	Teamscale  software  quality  control  in  real  time.  When  large  software  systems  evolve,  the  quality  of  source  code  is  essential  for  successful  maintenance.  Controlling  code  quality  continuously  requires  adequate  tool  support.  Current  quality  analysis  tools  operate  in  batch-mode  and  run  up  to  several  hours  for  large  systems,  which  hampers  the  integration  of  quality  control  into  daily  development.  In  this  paper,  we  present  the  incremental  quality  analysis  tool  Teamscale,  providing  feedback  to  developers  within  seconds  after  a  commit  and  thus  enabling  real-time  software  quality  control.  We  evaluated  the  tool  within  a  development  team  of  a  German  insurance  company.  A  video  demonstrates  our  tool:  http://www.youtube.com/watch?v=nnuqplu75Cg.
1	Inferring  likely  mappings  between  apis.  Software  developers  often  need  to  port  applications  written  for  a  source  platform  to  a  target  platform.  In  doing  so,  a  key  task  is  to  replace  an  application's  use  of  methods  from  the  source  platform  API  with  corresponding  methods  from  the  target  platform  API.  However,  this  task  is  challenging  because  developers  must  manually  identify  mappings  between  methods  in  the  source  and  target  APIs,  e.g.,  using  API  documentation.          We  develop  a  novel  approach  to  the  problem  of  inferring  mappings  between  the  APIs  of  a  source  and  target  platform.  Our  approach  is  tailored  to  the  case  where  the  source  and  target  platform  each  have  independently-developed  applications  that  implement  similar  functionality.  We  observe  that  in  building  these  applications,  developers  exercised  knowledge  of  the  corresponding  APIs.  We  develop  a  technique  to  systematically  harvest  this  knowledge  and  infer  likely  mappings  between  the  APIs  of  the  source  and  target  platform.  The  output  of  our  approach  is  a  ranked  list  of  target  API  methods  or  method  sequences  that  likely  map  to  each  source  API  method  or  method  sequence.  We  have  implemented  this  approach  in  a  prototype  tool  called  Rosetta,  and  have  applied  it  to  infer  likely  mappings  between  the  Java2  Mobile  Edition  (JavaME)  and  Android  graphics  APIs.
1	What  make  long  term  contributors  willingness  and  opportunity  in  oss  community.  To  survive  and  succeed,  software  projects  need  to  attract  and  retain  contributors.  We  model  the  individual's  chances  to  become  a  valuable  contributor  through  their  capacity,  willingness,  and  the  opportunity  to  contribute  at  the  time  of  joining.  Using  issue  tracking  data  of  Mozilla  and  Gnome,  we  find  that  the  probability  for  a  new  joiner  to  become  a  Long  Term  Contributor  (LTC)  is  associated  with  her  willingness  and  environment.  Specifically,  during  their  first  month,  future  LTCs  tend  to  be  more  active  and  show  more  community-oriented  attitude  than  other  joiners.  Joiners  who  start  by  commenting  on  instead  of  reporting  an  issue  or  ones  who  succeed  to  get  at  least  one  reported  issue  to  be  fixed,  more  than  double  their  odds  of  becoming  an  LTC.  The  macro-climate  with  high  project  relative  sociality  and  the  micro-climate  with  a  large,  productive,  and  clustered  peer  group  increase  the  odds.  On  the  contrary,  the  macro-climate  with  high  project  popularity  and  the  micro-climate  with  low  attention  from  peers  reduce  the  odds.  This  implies  that  the  interaction  between  individual's  attitude  and  project's  climate  are  associated  with  the  odds  that  an  individual  would  become  a  valuable  contributor  or  disengage  from  the  project.  Our  findings  may  provide  a  basis  for  empirical  approaches  to  design  a  better  community  architecture  and  to  improve  the  experience  of  contributors.
1	A  large  scale  evaluation  of  a  model  for  the  evaluation  of  games  for  teaching  software  engineering.  In  order  to  adopt  games  for  Software  Engineering  (SE)  education  effectively  it  is  essential  to  obtain  sound  evidence  on  their  quality.  A  prominent  evaluation  model  is  MEEGA  (Model  for  the  Evaluation  of  Educational  Games),  which  provides  a  systematic  support  to  evaluate  the  game's  quality  in  terms  of  motivation,  user  experience  and  learning.  To  facilitate  its  application,  the  model  provides  a  questionnaire  for  collecting  data  on  the  perception  of  the  students  after  they  played  an  educational  game  in  a  case  study  with  a  one-shot  post-test  design.  However,  in  order  to  assure  a  valid  feedback  on  the  game's  quality  an  important  issue  is  the  reliability  and  validity  of  the  questionnaire.  In  this  respect,  this  article  presents  a  large-scale  evaluation  of  the  MEEGA  questionnaire  in  terms  of  reliability  and  construct  validity.  The  analysis  is  based  on  data  collected  in  43  case  studies,  evaluating  20  different  SE  games,  involving  a  population  of  723  students.  Our  analysis  indicates  that  the  MEEGA  questionnaire  can  be  considered  reliable  (Cronbach's  alpha  α=.915).  In  terms  of  construct  validity,  there  exists  evidence  of  convergent  validity  through  an  acceptable  degree  of  correlation  of  almost  all  item  pairs  within  each  dimension.  Yet,  we  identified  a  need  for  the  re-grouping  of  items  based  on  the  results  of  a  factor  analysis,  mainly  with  respect  to  items  related  to  motivation  and  user  experience.  These  results  allow  SE  researchers  and  instructors  to  rely  on  the  MEEGA  questionnaire  in  order  to  evaluate  SE  games  and,  thus,  contribute  to  their  improvement  and  to  direct  an  effective  and  efficient  adoption  for  SE  education.
1	Interactive  refinement  of  combinatorial  test  plans.  Combinatorial  test  design  (CTD)  is  an  effective  test  planning  technique  that  reveals  faulty  feature  interactions  in  a  given  system.  The  test  space  is  modeled  by  a  set  of  parameters,  their  respective  values,  and  restrictions  on  the  value  combinations.  A  subset  of  the  test  space  is  then  automatically  constructed  so  that  it  covers  all  valid  value  combinations  of  every  $t$  parameters,  where  $t$  is  a  user  input.          When  applying  CTD  to  real-life  testing  problems,  it  can  often  occur  that  the  result  of  CTD  cannot  be  used  as  is,  and  manual  modifications  to  the  tests  are  performed.  One  example  is  very  limited  resources  that  significantly  reduce  the  number  of  tests  that  can  be  used.  Another  example  is  complex  restrictions  that  are  not  captured  in  the  model  of  the  test  space.  The  main  concern  is  that  manually  modifying  the  result  of  CTD  might  potentially  introduce  coverage  gaps  that  the  user  is  unaware  of.  In  this  paper  we  present  a  tool  that  supports  interactive  modification  of  a  combinatorial  test  plan,  both  manually  and  with  tool  assistance.  For  each  modification,  the  tool  displays  the  new  coverage  gaps  that  will  be  introduced,  and  enables  the  user  to  take  educated  decisions  on  what  to  include  in  the  final  set  of  tests.
1	Dynamic  generation  of  likely  invariants  for  multithreaded  programs.  We  propose  a  new  method  for  dynamically  generat-  ing  likely  invariants  from  multithreaded  programs.While  existing  invariant  generation  tools  work  well  on  sequential  programs,  they  are  ineffective  at  reasoning  about  multithreaded  programs  both  in  terms  of  the  number  of  real  invariants  generated  and  in  terms  of  their  usefulness  in  helping  programmers.  We  address  this  issue  by  developing  a  new  dynamic  invariant  generator  consisting  of  an  LLVM  based  code  instrumentation  front  end,  a  systematic  thread  interleaving  explorer,  and  a  customized  invariant  inference  engine.  We  show  that  efficient  interleaving  exploration  strategies  can  be  used  to  generate  a  diversified  set  of  executions  with  little  runtime  overhead.  Furthermore,  we  show  that  focusing  on  a  small  subset  of  thread-local  transition  invariants  is  often  sufficient  for  reasoning  about  the  concurrency  behavior  of  programs.  We  have  evaluated  our  new  method  on  a  set  of  open-source  multithreaded  C/C++  benchmarks.  Our  experiments  show  that  our  method  can  generate  invariants  that  are  significantly  higher  in  quality  than  the  previous  state-of-the-art.
1	Competence  confidence  gap  a  threat  to  female  developers  contribution  on  github.  On  GitHub,  contributing  to  a  new  project  is  crucial  for  a  developer  to  gain  personal  growth  and  maximize  impact  in  the  community.  It  is  known  that  female  developers  are  often  hesitant  to  explore  the  opportunities  to  contribute  to  new  projects  even  when  they  possess  the  competence  to  make  valuable  contributions.  Drawing  from  the  literature  of  the  competence-confidence  gap,  we  develop  a  fresh  explanation  for  this  phenomenon.  We  validate  the  theoretical  explanation  through  an  empirical  study  using  GitHub's  historical  data.  In  this  study,  we  identify  all  female  developers  ranking  in  top  5,000  GitHub  users.  Using  the  Granger  Causality  Test,  we  find  that,  for  the  majority  of  identified  female  developers,  initiating  a  pull  request  to  a  new  repository  is  "Granger"  caused  by  the  quick  increase  of  followers  in  the  preceding  couple  of  weeks.  For  most  male  developers,  our  observations  show  that  their  new  pull  requests  have  no  relationship  with  the  dynamics  of  follower  numbers.  The  results  indicate  that  the  competence-confidence  gap  is  a  threat  to  female  developers'  contribution  on  GitHub.  The  research  suggests  that  helping  female  developers  to  overcome  the  competence-confidence  gap  is  critical  for  encouraging  female's  contribution  open  source  development,  as  well  as  growing  their  reputations  and  impacts  in  the  community.
1	Guileak  tracing  privacy  policy  claims  on  user  input  data  for  android  applications.  The  Android  mobile  platform  supports  billions  of  devices  across  more  than  190  countries  around  the  world.  This  popularity  coupled  with  user  data  collection  by  Android  apps  has  made  privacy  protection  a  well-known  challenge  in  the  Android  ecosystem.  In  practice,  app  producers  provide  privacy  policies  disclosing  what  information  is  collected  and  processed  by  the  app.  However,  it  is  difficult  to  trace  such  claims  to  the  corresponding  app  code  to  verify  whether  the  implementation  is  consistent  with  the  policy.  Existing  approaches  for  privacy  policy  alignment  focus  on  information  directly  accessed  through  the  Android  platform  (e.g.,  location  and  device  ID),  but  are  unable  to  handle  user  input,  a  major  source  of  private  information.  In  this  paper,  we  propose  a  novel  approach  that  automatically  detects  privacy  leaks  of  user-entered  data  for  a  given  Android  app  and  determines  whether  such  leakage  may  violate  the  app's  privacy  policy  claims.  For  evaluation,  we  applied  our  approach  to  120  popular  apps  from  three  privacy-relevant  app  categories:  finance,  health,  and  dating.  The  results  show  that  our  approach  was  able  to  detect  21  strong  violations  and  18  weak  violations  from  the  studied  apps.
1	Big  data  software  analytics  with  apache  spark.  At  the  beginning  of  every  research  effort,  researchers  in  empirical  software  engineering  have  to  go  through  the  processes  of  extracting  data  from  raw  data  sources  and  transforming  them  to  what  their  tools  expect  as  inputs.  This  step  is  time  consuming  and  error  prone,  while  the  produced  artifacts  (code,  intermediate  datasets)  are  usually  not  of  scientific  value.  In  the  recent  years,  Apache  Spark  has  emerged  as  a  solid  foundation  for  data  science  and  has  taken  the  big  data  analytics  domain  by  storm.  We  believe  that  the  primitives  exposed  by  Apache  Spark  can  help  software  engineering  researchers  create  and  share  reproducible,  high-performance  data  analysis  pipelines.  In  our  technical  briefing,  we  discuss  how  researchers  can  profit  from  Apache  Spark,  through  a  hands-on  case  study.
1	When  teams  go  crazy  an  environment  to  experience  group  dynamics  in  software  project  management  courses.  Software  development  consists  to  a  large  extend  of  human-based  processes  with  continuously  increasing  demands  regarding  interdisciplinary  team  work.  Understanding  the  dynamics  of  software  teams  can  be  seen  as  highly  important  to  successful  project  execution.  Hence,  for  future  project  managers,  knowledge  about  non-technical  processes  in  teams  is  significant.  In  this  paper,  we  present  a  course  unit  that  provides  an  environment  in  which  students  can  learn  and  experience  the  impact  of  group  dynamics  on  project  performance  and  quality.  The  course  unit  uses  the  Tuckman  model  as  theoretical  framework,  and  borrows  from  controlled  experiments  to  organize  and  implement  its  practical  parts  in  which  students  then  experience  the  effects  of,  e.g.,  time  pressure,  resource  bottlenecks,  staff  turnover,  loss  of  key  personnel,  and  other  stress  factors.  We  provide  a  detailed  design  of  the  course  unit  to  allow  for  implementation  in  further  software  project  management  courses.  Furthermore,  we  provide  experiences  obtained  from  two  instances  of  this  unit  conducted  in  Munich  and  Karlskrona  with  36  graduate  students.  We  observed  students  building  awareness  of  stress  factors  and  developing  counter  measures  to  reduce  impact  of  those  factors.  Moreover,  students  experienced  what  problems  occur  when  teams  work  under  stress  and  how  to  form  a  performing  team  despite  exceptional  situations.
1	Strategies  for  prioritizing  test  cases  generated  through  model  based  testing  approaches.  Software  testing  is  expensive  and  time  consuming,  especially  for  complex  software.  In  order  to  deal  with  the  cost  of  testing,  researchers  develop  Model-Based  Testing  (MBT).  In  MBT,  test  cases  are  generated  automatically  and  a  drawback  is  a  huge  generated  test  suite.  Our  research  aims  at  studying  the  Test  Case  Prioritization  problem  in  MBT  context.      So  far,  we  already  evaluated  the  influence  of  the  model  structure  and  the  characteristics  of  the  test  cases  that  fail.  Results  suggest  that  the  former  does  not  affect  significantly  the  performance  of  techniques,  however,  the  latter  indeed  represents  a  major  impact.  Therefore,  a  worthy  information  in  this  context  might  be  an  expert  who  knows  the  crucial  parts  of  the  software,  thus  we  propose  the  first  version  of  a  prioritization  technique  that  considers  hints  from  the  expert  and  the  distance  notion  in  order  to  prioritize  test  cases.  Evaluation  and  tuning  of  the  technique  are  ongoing,  but  preliminary  evaluation  reveals  promising  results.
1	Codeshovel  constructing  method  level  source  code  histories.  Source  code  histories  are  commonly  used  by  developers  and  researchers  to  reason  about  how  software  evolves.  Through  a  survey  with  42  professional  software  developers,  we  learned  that  developers  face  significant  mismatches  between  the  output  provided  by  developers'  existing  tools  for  examining  source  code  histories  and  what  they  need  to  successfully  complete  their  historical  analysis  tasks.  To  address  these  shortcomings,  we  propose  CodeShovel,  a  tool  for  uncovering  method  histories  that  quickly  produces  complete  and  accurate  change  histories  for  90%  methods  (including  97%  of  all  method  changes)  outperforming  leading  tools  from  both  research  (e.g,  FinerGit)  and  practice  (e.g.,  IntelliJ  /  git  log).  CodeShovel  helps  developers  to  navigate  the  entire  history  of  source  code  methods  so  they  can  better  understand  how  the  method  evolved.  A  field  study  on  industrial  code  bases  with  16  industrial  developers  confirmed  our  empirical  findings  of  CodeShovel's  correctness,  low  runtime  overheads,  and  additionally  showed  that  the  approach  can  be  useful  for  a  wide  range  of  industrial  development  tasks.
1	Automatically  predicting  bug  severity  early  in  the  development  process.  Bug  severity  is  an  important  factor  in  prioritizing  which  bugs  to  fix  first.  The  process  of  triaging  bug  reports  and  assigning  a  severity  requires  developer  expertise  and  knowledge  of  the  underlying  software.  Methods  to  automate  the  assignment  of  bug  severity  have  been  developed  to  reduce  the  developer  cost,  however,  many  of  these  methods  require  70-90%  of  the  project’s  bug  reports  as  training  data  and  delay  their  use  until  later  in  the  development  process.  Not  being  able  to  automatically  predict  a  bug  report’s  severity  early  in  a  project  can  greatly  reduce  the  benefits  of  automation.  We  have  developed  a  new  bug  report  severity  prediction  method  that  leverages  how  bug  reports  are  written  rather  than  what  the  bug  reports  contain.  Our  method  allows  for  the  prediction  of  bug  severity  at  the  beginning  of  the  project  by  using  an  organization’s  historical  data,  in  the  form  of  bug  reports  from  past  projects,  to  train  the  prediction  classier.  In  validating  our  approach,  we  conducted  over  1000  experiments  on  a  dataset  of  five  NASA  robotic  mission  software  projects.  Our  results  demonstrate  that  our  method  was  not  only  able  to  predict  the  severity  of  bugs  earlier  in  development,  but  it  was  also  able  to  outperform  an  existing  keyword-based  classifier  for  a  majority  of  the  NASA  projects.Ccs  Concepts•  Software  and  its  engineering  →  Software  maintenance  tools;  Maintaining  software;  Software  testing  and  debugging;  •  Computing  methodologies  →  Machine  learning.
1	Zero  watermarking  for  text  on  www  using  semantic  approach.  Information  security  can  be  achieved  by  different  standard  methodologies  like  Steganography,  Cryptography  and  Digital  Watermarking.  In  this  research  semantic  watermarking  approach  has  been  used  to  offer  security  for  online  content.  The  proposed  approach  uses  a  secret  key  based  on  the  idea  of  public  key  cryptography.  The  proposed  method  is  developed  by  embedding  semantic  watermarking  with  a  standard  cryptographic  method.
1	Sentiment  classification  from  online  customer  reviews  using  lexical  contextual  sentence  structure.  Sentiment  analysis  is  the  procedure  by  which  information  is  extracted  from  the  opinions,  appraisals  and  emotions  of  people  in  regards  to  entities,  events  and  their  attributes.  In  decision  making,  the  opinions  of  others  have  a  significant  effect  on  customers,  ease  in  making  choices  regards  to  online  shopping,  choosing  events,  products,  entities,  etc.  When  an  important  decision  needs  to  be  made,  consumers  usually  want  to  know  the  opinion,  sentiment  and  emotion  of  others.  With  rapidly  growing  online  resources  such  as  online  discussion  groups,  forums  and  blogs,  people  are  commentating  via  the  Internet.  As  a  result,  a  vast  amount  of  new  data  in  the  form  of  customer  reviews,  comments  and  opinions  about  products,  events  and  entities  are  being  generated  more  and  more.  So  it  is  desired  to  develop  an  efficient  and  effective  sentiment  analysis  system  for  online  customer  reviews  and  comments.  In  this  paper,  the  rule  based  domain  independent  sentiment  analysis  method  is  proposed.  The  proposed  method  classifies  subjective  and  objective  sentences  from  reviews  and  blog  comments.  The  semantic  score  of  subjective  sentences  is  extracted  from  SentiWordNet  to  calculate  their  polarity  as  positive,  negative  or  neutral  based  on  the  contextual  sentence  structure.  The  results  show  the  effectiveness  of  the  proposed  method  and  it  outperforms  the  word  level  and  machine  learning  methods.  The  proposed  method  achieves  an  accuracy  of  97.8%  at  the  feedback  level  and  86.6%  at  the  sentence  level.
1	Towards  natural  interaction  with  wheelchair  using  nintendo  wiimote  controller.  The  importance  of  natural  interaction  increases  when  the  subjects  are  disabled  people.  It  has  been  found  that  embedded  solutions  for  impaired  people  lack  in  fluid  interaction  properties.  This  work  investigates  the  interface  design  of  a  wheelchair  controller  according  to  the  rules  of  physicality.  Our  aim  is  to  provide  further  ease  of  use  to  the  impaired  whilst  strengthening  link  between  embedded  software  engineering  and  human-computer  interaction.  We  propose  an  improved  multi-function  interface  design  by  using  Nintendo’s  Wiimote  that  is  more  intuitive,  robust,  low  cost,  and  most  importantly,  natural  to  use.
1	Fractal  analysis  of  surface  electromyography  emg  signal  for  identify  hand  movements  using  critical  exponent  analysis.  Recent  advances  in  non-linear  analysis  have  led  to  understand  the  complexity  and  self-similarity  of  surface  electromyography  (sEMG)  signal.  This  research  paper  examines  usage  of  critical  exponent  analysis  method  (CEM),  a  fractal  dimension  (FD)  estimator,  to  study  properties  of  the  sEMG  signal  and  to  use  these  properties  to  identify  various  kinds  of  hand  movements  for  prosthesis  control  and  human-machine  interface.  The  sEMG  signals  were  recorded  from  ten  healthy  subjects  with  seven  hand  movements  and  eight  muscle  positions.  Mean  values  and  coefficient  of  variations  of  the  FDs  for  all  the  experiments  show  that  there  are  larger  variations  between  hand  movement  types  but  there  is  small  variation  within  hand  movement.  It  also  shows  that  the  FD  related  to  the  self-affine  property  for  the  sEMG  signal  extracted  from  different  hand  activities  1.944~2.667.  These  results  have  also  been  evaluated  and  displayed  as  a  box  plot  and  analysis-of-variance  (p  value).  It  demonstrates  that  the  FD  value  is  suitable  for  using  as  an  EMG  feature  extraction  to  characterize  the  sEMG  signals  compared  to  the  commonly  and  popular  sEMG  feature,  i.e.,  root  mean  square  (RMS).  The  results  also  indicate  that  the  p  values  of  the  FDs  for  six  muscle  positions  was  less  than  0.0001  while  that  of  the  RMS,  a  candidate  feature,  ranged  between  0.0003-0.1195.  The  FD  that  is  computed  by  the  CEM  can  be  applied  to  be  used  as  a  feature  for  different  kinds  of  sEMG  application.
1	The  development  of  software  evaluation  and  selection  framework  for  supporting  cots  based  systems  the  theoretical  framework.  As  a  result  of  increasing  demands  on  COTS  technology,  there  is  an  increasingly  huge  market  of  COTS  software.  Therefore,  one  of  the  most  critical  activities  in  COTS-based  system  development  is  the  COTS  evaluation  and  selection.  Unfortunately,  most  existing  methods  that  have  been  proposed  in  previous  studies  for  evaluating  and  selecting  COTS  software  are  still  have  many  limitations  to  be  applicable  and  used  in  the  industry.  So  without  an  effective  method  to  select  and  evaluate  COTS  software,  the  time  spent  for  selecting  the  correct  COTS  software  may  offset  the  advantages  of  using  it.  This  paper  outlines  and  discusses  the  common  problems  in  existing  methods  and  the  main  processes  and  criteria  (non-functional  requirements)  that  are  required  for  evaluating  and  selecting  COTS  software  through  theoretical  and  empirical  studies  which  goal  is  to  develop  new  framework  to  evaluate  and  select  COTS  software.
1	Splba  an  interaction  strategy  for  testing  software  product  lines  using  the  bat  inspired  algorithm.  Software  product  lines  (SPLs)  represent  an  engineering  method  for  creating  a  portfolio  of  similar  software  systems  for  a  shared  set  of  software  product  assets.  Owing  to  the  significant  growth  of  SPLs,  there  is  a  need  for  systematic  approach  for  ensuring  the  quality  of  the  resulting  product  derivatives.  Combinatorial  t-way  testing  (where  t  indicates  the  interaction  strength)  has  been  known  to  be  effective  especially  when  the  number  of  product's  features  and  constraints  in  the  SPLs  of  interest  are  huge.  In  line  with  the  recent  emergence  of  Search  based  Software  Engineering  (SBSE),  this  article  presents  a  novel  strategy  for  SPLs  tests  reduction  using  Bat-inspired  algorithm  (BA),  called  SPLBA.  Our  experience  with  SPLBA  has  been  promising  as  the  strategy  performed  well  against  existing  strategies  in  the  literature.
2	Designing  interfaces  for  explicit  preference  elicitation  a  user  centered  investigation  of  preference  representation  and  elicitation  process.  Two  problems  may  arise  when  an  intelligent  (recommender)  system  elicits  users'  preferences.  First,  there  may  be  a  mismatch  between  the  quantitative  preference  representations  in  most  preference  models  and  the  users'  mental  preference  models.  Giving  exact  numbers,  e.g.,  such  as  "I  like  30  days  of  vacation  2.5  times  better  than  28  days"  is  difficult  for  people.  Second,  the  elicitation  process  can  greatly  influence  the  acquired  model  (e.g.,  people  may  prefer  different  options  based  on  whether  a  choice  is  represented  as  a  loss  or  gain).  We  explored  these  issues  in  three  studies.  In  the  first  experiment  we  presented  users  with  different  preference  elicitation  methods  and  found  that  cognitively  less  demanding  methods  were  perceived  low  in  effort  and  high  in  liking.  However,  for  methods  enabling  users  to  be  more  expressive,  the  perceived  effort  was  not  an  indicator  of  how  much  the  methods  were  liked.  We  thus  hypothesized  that  users  are  willing  to  spend  more  effort  if  the  feedback  mechanism  enables  them  to  be  more  expressive.  We  examined  this  hypothesis  in  two  follow-up  studies.  In  the  second  experiment,  we  explored  the  trade-off  between  giving  detailed  preference  feedback  and  effort.  We  found  that  familiarity  with  and  opinion  about  an  item  are  important  factors  mediating  this  trade-off.  Additionally,  affective  feedback  was  preferred  over  a  finer  grained  one-dimensional  rating  scale  for  giving  additional  detail.  In  the  third  study,  we  explored  the  influence  of  the  interface  on  the  elicitation  process  in  a  participatory  set-up.  People  considered  it  helpful  to  be  able  to  explore  the  link  between  their  interests,  preferences  and  the  desirability  of  outcomes.  We  also  confirmed  that  people  do  not  want  to  spend  additional  effort  in  cases  where  it  seemed  unnecessary.  Based  on  the  findings,  we  propose  four  design  guidelines  to  foster  interface  design  of  preference  elicitation  from  a  user  view.
2	Exploring  online  video  watching  behaviors.  Laptop  and  desktop  computers  are  frequently  used  to  watch  online  videos  from  a  wide  variety  of  services.  From  short  YouTube  clips,  to  television  programming,  to  full-length  films,  users  are  increasingly  moving  much  of  their  video  viewing  away  from  television  sets  towards  computers.  But  what  are  they  watching,  and  when?  We  set  out  to  understand  current  video  use  on  computers  through  analyzing  full  browsing  histories  from  a  diverse  set  of  online  Americans,  finding  some  temporal  differences  in  genres  watched,  yet  few  differences  in  the  length  of  videos  watched  by  hour.  We  also  explore  topics  of  videos,  how  users  arrive  at  online  videos  through  referral  links,  and  conclude  with  several  implications  for  the  design  of  online  video  services  that  focus  on  the  types  of  content  people  are  actually  watching  online.
2	Envdash  an  environment  aware  dynamic  adaptive  streaming  over  http  system.  The  recent  advances  in  adaptive  video  streaming  technologies  including  Dynamic  Adaptive  Streaming  over  HTTP  (DASH)  are  capable  to  adjust  video  streams  to  rapidly  changing  network  conditions.  Our  system,  EnvDASH,  differs  from  those  standard  implementations  as  it  extends  DASH  with  mechanisms  that  allow  sensing  the  environmental  conditions  of  a  device.  EnvDASH  leverages  that  users  in  mobile  situations  are  often  distracted  from  watching  a  video  or  that  viewing  conditions  are  severely  degraded  by  adapting  the  video  to  reduce  the  generated  network  traffic.  The  system  senses  if  the  user  is  interested  in  watching  a  video,  if  the  displaying  device  is  held  stable  and  if  the  ambient  noise  level  allows  listening  to  an  audio  track  of  a  video.  This  is  especially  helpful  as  mobile  devices  usually  use  capped  data  volume  contracts  for  the  network  access.
2	A  nonformal  interactive  therapeutic  multisensory  environment  for  people  with  cerebral  palsy.  A  new  multisensory  system  that  aims  at  fostering  the  interaction  of  people  with  cerebral  palsy  is  presented.  This  article  describes  the  strategies  and  technologies  used  to  provide  people  who  have  moderate  to  severe  cerebral  palsy  with  playful  and  fun  activities  designed  according  to  their  abilities.  These  activities  are  based  on  interactive  systems  that  use  computer  vision  and  generate  graphics  and  sounds  in  real  time.  The  well-being  that  is  achieved  through  the  use  of  these  activities  is  the  result  of  gaining  a  significant  degree  of  autonomy  by  the  users.  The  presented  system  was  first  developed  in  the  Cerebral  Palsy  Centre  of  Tarragona,  Spain.  Its  motivation  came  from  the  low  rate  of  users  able  to  interact  with  computers.  Although  several  assistive  technology  gadgets  and  special  software  applications  (e.g.,  cause–effect  and  educational  activities,  simple  navigation  environments,  etc.)  were  used,  most  users  simply  did  not  understand  the  interaction  mechanisms.  It  was  thought  that  a  highly  interactive  act...
2	Using  distributed  user  interfaces  in  collaborative  secure  and  privacy  preserving  software  environments.  In  complex,  ad  hoc  constituted  situations,  people  with  different  intentions,  experiences,  and  expertise  need  or  want  to  cooperate  to  cope  with  the  domain-specific  challenges  they  face.  These  situations  can  occur  in  both  a  professional  and  a  leisure-life  context.  Cooperative  systems  providing  enhanced  interaction  facilities  in  the  user  interface  (e.g.,  direct  manipulation  techniques)  could  substantially  support  cooperation  especially  for  geographically  distributed  cooperating  participants.  In  many  cases,  sensitive  information  has  to  be  shared  in  a  common  workspace  requiring  different  handling  procedures  according  to  the  different  types  of  participants  involved  in  these  ad  hoc  processes.  This  article  proposes  the  use  of  a  common,  multilaterally  secure  distributed  user  interface  to  support  collaboration  for  distributed  groups  of  process  participants.  The  system  combines  a  collaborative  multipointer  system  with  an  anonymous  credential  security  system  to  provide  users  with  an  easy  way  to  share  and  access  infor...
2	Understanding  user  experience  with  computer  based  applications  with  different  use  purposes.  The  concept  of  user  experience  emphasizes  the  importance  of  understanding  users  for  applications  that  have  various  contextual  features.  To  address  this  issue,  this  study  examines  the  changes  in  the  relationships  among  user  satisfaction  and  users'  perceptions  of  usability  and  aesthetics  according  to  use  situations.  For  data,  an  experiment  was  conducted  using  15  existing  websites  with  similar  levels  of  usability  and  aesthetics.  Forty-five  engineering  students  participated  in  the  experiment.  The  results  indicated  that  the  relationships  among  perceived  usability,  perceived  aesthetics,  and  user  satisfaction  could  be  dependent  on  how  users  perceived  the  use  purposes  and  interaction  types  of  the  websites.  Specifically,  the  relationship  between  perceived  usability  and  user  satisfaction  was  stronger  for  websites  requiring  users'  goal-directed  activities,  whereas  the  relationship  between  perceived  aesthetics  and  user  satisfaction  was  stronger  for  the  websites  mainly  providing  useful  information,  regardless  of  the  e...
2	Website  usability  in  asia  from  within  an  overview  of  a  decade  of  literature.  As  the  number  of  website  users  in  Asia  grows,  there  is  an  increasing  need  to  gain  an  overview  of  human–computer  interaction  (HCI)  research  about  users  and  websites  in  that  context.  This  article  presents  an  overview  of  HCI  research  on  website  usability  in  Asia  “from  within,”  which  outlines  the  articles  written  by  researchers  with  affiliations  to  universities  in  that  part  of  the  world.  Based  on  a  key  word  approach  to  major  HCI  research  outlets,  60  articles  from  2001  to  2011  were  identified  and  analyzed.  Results  indicate  that  academic  websites,  e-commerce  websites,  and  tourism  websites  were  the  most  studied  website  domains  in  Asia.  Typically,  university  graduates  were  used  as  participants  in  a  laboratory  setup  and  asked  to  navigate  and  find  information  on  a  website.  No  systematic  use  of  cultural  variables  or  theories  to  code,  analyze,  and  interpret  data  and  findings  was  found.  The  article  discusses  the  results  and  the  need  for  a  greater  sensitivity  to  what  is  “local”  and  “from  within”  in  HCI  research  and  wha...
2	The  system  usability  scale  past  present  and  future.  The  System  Usability  Scale  (SUS)  is  the  most  widely  used  standardized  questionnaire  for  the  assessment  of  perceived  usability.  This  review  of  the  SUS  covers  its  early  history  from  inception  in  the  ...
2	Interactive  persuasive  systems  a  perspective  on  theory  and  evaluation.  Interactive  systems  are  increasingly  used  in  a  growing  number  of  domains  to  encourage  users  to  perform  certain  actions  or  take  a  certain  position  toward  a  given  subject.  This  article  offers  a  perspective  on  interactive  persuasive  systems  that  emphasizes  theories  and  concepts  that  can  guide  their  design  and  evaluation.  The  goal  is  not  to  champion  a  theoretical  model  over  others  or  to  compare  the  effectiveness  of  different  persuasive  strategies  but  rather  to  depict  the  current  conceptual  landscape  in  this  field  and  highlight  emerging  concepts  and  positions  that  have  recently  appeared  in  it.  The  article  includes  a  discussion  of  some  representative,  general,  and  domain-specific  constructs  that  can  be  used  to  evaluate  the  effectiveness  of  interactive  persuasive  systems.
2	Robot  transparency  and  team  orientation  effects  on  human  robot  teaming.  Human–robot  team  members  often  have  to  interact  in  a  situation  when  the  team  members  are  not  physically  collocated  requiring  effective  communication  to  establish  and  maintain  effective  human–robot  ...
2	To  please  in  a  pod  employing  an  anthropomorphic  agent  interlocutor  to  enhance  trust  and  user  experience  in  an  autonomous  self  driving  vehicle.  Recognising  that  one  of  the  aims  of  conversation  is  to  build,  maintain  and  strengthen  positive  relationships  with  others,  the  study  explores  whether  passengers  in  an  autonomous  vehicle  display  similar  behaviour  during  transactions  with  an  on-board  conversational  agent-interface;  moreover,  whether  related  attributes  (e.g.  trust)  transcend  to  the  vehicle  itself.  Employing  a  counterbalanced,  within-subjects  design,  thirty-four  participants  were  transported  in  a  self-driving  pod  using  an  expansive  testing  arena.  Participants  undertook  three  journeys  with  an  anthropomorphic  agent-interlocutor  (via  Wizard-of-Oz),  a  voice-command  interface,  or  a  traditional  touch-surface;  each  delivered  equivalent  task-related  information.  Results  show  that  the  agent-interlocutor  was  the  most  preferred  interface,  attracting  the  highest  ratings  of  trust,  and  significantly  enhancing  the  pleasure  and  sense  of  control  over  the  journey  experience,  despite  the  inclusion  of  'trust  challenges'  as  part  of  the  design.  The  findings  can  help  support  the  design  and  development  of  in-vehicle  agent-based  voice  interfaces  to  enhance  trust  and  user  experience  in  autonomous  cars.
2	Low  cost  5  dof  haptic  stylus  interaction  using  two  phantom  omni  devices.  This  paper  introduces  a  haptic  interface  providing  5-DOF  stylus  interaction  for  applications  requiring  3-DOF  force  and  2-DOF  torque  feedback.  The  interface  employs  two  coupled  Phantom  Omni  devices  each  offering  3-DOF  force  feedback  and  6-DOF  position  sensing.  The  interface  uses  an  inexpensive  lightweight  coupling  and  no  additional  actuators  enabling  the  interface  to  maintain  low  inertia  and  stylus  interaction,  both  similar  to  the  original  Phantom  Omni  device.  The  interface  also  maintains  unconstrained  rotation  about  the  stylus’  longitudinal  axis  aiding  in  handheld  manipulation.  Kinematic  analysis  of  the  5-DOF  interface  is  presented  and  the  usable  workspace  of  the  device  is  demonstrated.
2	Interaction  based  dynamic  measurement  of  haptic  characteristics  of  control  elements.  The  force-displacement  curve  is  typically  used  today  to  haptically  characterize  control  elements.  Elements  with  the  same  curve,  however,  may  still  lead  to  quite  different  percepts  because  the  curve  describes  only  static  information.  To  overcome  this  limitation,  a  new  dynamic  measurement  method  is  introduced.  It  can  directly  measure  dynamic  interaction  signals  between  a  finger  mimicking  measurement  device  and  control  elements.  Using  this  measurement  method,  also  novel  control  elements  like  touchpads  with  active  haptic  feedback  can  be  technically  specified  and  evaluated  for  the  first  time.
2	Haptics  1  preliminary  results  from  the  first  stiffness  jnd  identification  experiment  in  space.  On  July  28th  2014,  23:47  UTC,  the  European  Space  Agency  launched  the  Haptics-1  Kit  to  the  International  Space  Station  ISS  on  its  last  Automated  Transfer  Vehicle  ATV-5.  The  Kit  reached  the  station  two  weeks  later,  marking  the  first  haptic  master  device  to  enter  the  ISS.  The  first  force-feedback  and  human  perceptual  motor  performance  tests  started  to  take  place  on  December  30th  2014,  and  are  the  first  of  their  kind  in  the  history  of  spaceflight.  Three  astronauts  participated  in  the  Haptics-1  experiment  until  November  2015,  allowing  the  investigation  of  the  effects  of  microgravity  on  various  psycho-motor  performance  metrics  related  with  the  usage  of  haptic  feedback.  Experiments  are  conducted  following  full  adaptation  to  the  space  environment  after  3  months  in  space.  This  paper  introduces  the  Haptics-1  experiment  and  associated  hardware.  Detailed  experimental  results  are  reported  from  a  first  stiffness  just  noticeable  difference  JND  experimental  study  in  space,  carried  out  on  the  ISS  and  pre-flight  on  ground  with  3  astronauts.  The  first  findings  from  the  experiment  show  no  major  alterations  in-flight,  when  compared  to  on-ground  data,  if  the  manipulandum  is  secured  in  flight  against  a  sufficiently  stiff  reference  structure.
2	Hapti  o  physical  i  o  node  over  the  internet.  Along  with  the  development  of  information  technology  and  tactile  technology,  it  has  become  possible  to  share  human  haptic  experiences  via  the  Internet.  In  this  research,  we  propose  HaptI/O  a  technology  that  allows  easy  sharing  and  communication  of  haptic  experiences.  HaptI/O  devices  are  physical  network  nodes  that  can  perform  as  gateways  to  both  input  or  output  the  haptic  information  from  a  source  such  as  the  human  body  or  a  tangible  object.  HaptI/O  proposed  in  this  research  focuses  on  the  (1)  ease  of  sharing  haptic  information  with  a  focus  on  usability  (2)  share  Haptic  information  among  multiple  users  (3)  usage  of  the  HaptI/O  as  a  mobile  device.  As  a  result  of  user  testing  of  the  implemented  HaptI/O,  it  was  confirmed  that  user’s  information  perception  in  remote  communication  was  improved.
2	Study  of  middleware  for  internet  of  healthcare  things  and  their  applications.  The  rapid  proliferation  and  miniaturization  of  the  wireless  and  embedded  devices  has  led  to  the  invasion  of  the  Internet  of  Things  in  many  domains  and  has  reached  the  healthcare  sector  to  form  what  is  called  the  Internet  of  Healthcare  Things  (IoHT)  .  The  growing  number  of  the  applications  in  the  Internet  of  Healthcare  Things  as  well  as  the  overwhelming  number  of  heterogeneous  medical  devices  that  should  interact  in  this  network  has  put  the  researchers  and  developers  in  front  of  lot  of  challenges:  How  to  facilitate  the  implementation  of  the  various  healthcare  applications?  And  how  to  ease  the  integration  of  new  devices  and  make  their  interoperation  a  transparent  task  for  the  developers?  To  fulfill  these  requirements,  lot  of  middleware  have  been  proposed.  In  this  paper  we  provide  a  complete  study  on  the  existing  middleware  for  IoHT  and  specify  their  applications,  we  propose  a  taxonomy  for  them  and  we  present  their  main  advantages  and  drawbacks.
2	Evaluation  framework  for  smart  technology  mental  health  interventions.  Evaluations  of  technological  mental  health  interventions  are  often  too  narrow  to  support  the  uptake  of  such  technologies.  This  report  describes  the  evaluation  framework  used  for  a  mobile  technology  study,  the  Mental  Health  Engagement  Network  MHEN.  The  framework  presented  here  includes  four  types  of  analyses:  effectiveness,  economic,  policy,  and  ethics  analysis.  When  technological  mental  health  interventions  are  evaluated  in  each  of  these  four  areas,  research  can  be  more  comprehensive  and  set  the  stage  for  spreading  the  innovation.  Particularly,  inclusion  of  economic  analysis  may  speak  to  potential  funders,  ethical  analysis  may  encourage  adoption  in  clinical  settings,  and  policy  analysis  may  encourage  uptake  from  decision-makers.  This  report  provides  a  framework  that  can  be  adapted  to  a  variety  of  technological  mental  health  interventions  to  assess  and  compare  not  only  effectiveness,  but  also  economic,  policy  and  ethical  challenges  and  opportunities.Findings  from  the  MHEN  study  are  presented  as  a  case  study  of  the  applied  framework.
2	A  wavelet  feature  based  mechanomyography  classification  system  for  a  wearable  rehabilitation  system  for  the  elderly.  This  paper  proposes  a  pattern  recognition  based  system  for  identification  of  the  forearm  movements  using  Mechanomyography(MMG)  for  the  rehabilitation  of  the  elderly.  The  system  is  used  to  assist  in  the  relearning  and  rehabilitation  of  the  movements  of  the  wrist  and  the  hand.  Surface  MMG  signals  acquired  from  the  flexor  carpi  ulnaris,  brachioradialis  supinator  and  abductor  pollicis  longus.  The  MMG  is  processed  and  wavelet  based  features  are  extracted  which  are  classified  into  eight  different  forearm  movements  using  a  multilayer  perceptron  (MLP)  classifier.  A  classification  efficiency  of  90.2  %  is  achieved  using  the  MLP  classifier.  The  MMG  system  is  designed  to  measure  data  using  accelerometers  built  into  the  assistive  device  and,  hence,  doesn’t  require  any  active  involvement  of  the  elderly.
2	Shelfie  a  framework  for  designing  material  representations  of  physical  activity  data.  Self-monitoring  devices  are  becoming  increasingly  popular  in  the  support  of  physical  activity  experiences.  These  devices  mostly  represent  on-screen  data  using  numbers  and  graphs  and  in  doing  so,  they  may  miss  multi-sensorial  methods  for  engaging  with  data.  Embracing  the  opportunity  for  pleasurable  interactions  with  one's  own  data  through  the  use  of  different  materials  and  digital  fabrication  technology,  we  designed  and  studied  three  systems  that  turn  this  data  into  3D-printed  plastic  artifacts,  sports  drinks,  and  3D-printed  chocolate  treats.  We  utilize  the  insights  gained  from  associated  studies,  related  literature,  and  our  experiences  in  designing  these  systems  to  develop  a  conceptual  framework,  “Shelfie.”  The  “Shelfie”  framework  has  13  cards  that  convey  key  themes  for  creating  material  representations  of  physical  activity  data.  Through  this  framework,  we  present  a  conceptual  understanding  of  relationships  between  material  representation  and  physical  activity  data  and  contribute  guidelines  to  the  design  of  meaningful  material  representations  of  physical  activity  data.
2	Bubbleview  an  interface  for  crowdsourcing  image  importance  maps  and  tracking  visual  attention.  In  this  article,  we  present  BubbleView,  an  alternative  methodology  for  eye  tracking  using  discrete  mouse  clicks  to  measure  which  information  people  consciously  choose  to  examine.  BubbleView  is  a  mouse-contingent,  moving-window  interface  in  which  participants  are  presented  with  a  series  of  blurred  images  and  click  to  reveal  “bubbles”  --  small,  circular  areas  of  the  image  at  original  resolution,  similar  to  having  a  confined  area  of  focus  like  the  eye  fovea.  Across  10  experiments  with  28  different  parameter  combinations,  we  evaluated  BubbleView  on  a  variety  of  image  types:  information  visualizations,  natural  images,  static  webpages,  and  graphic  designs,  and  compared  the  clicks  to  eye  fixations  collected  with  eye-trackers  in  controlled  lab  settings.  We  found  that  BubbleView  clicks  can  both  (i)  successfully  approximate  eye  fixations  on  different  images,  and  (ii)  be  used  to  rank  image  and  design  elements  by  importance.  BubbleView  is  designed  to  collect  clicks  on  static  images,  and  works  best  for  defined  tasks  such  as  describing  the  content  of  an  information  visualization  or  measuring  image  importance.  BubbleView  data  is  cleaner  and  more  consistent  than  related  methodologies  that  use  continuous  mouse  movements.  Our  analyses  validate  the  use  of  mouse-contingent,  moving-window  methodologies  as  approximating  eye  fixations  for  different  image  and  task  types.
2	Using  context  to  reveal  factors  that  affect  physical  activity.  There  are  many  physical  activity  awareness  systems  available  in  today's  market.  These  systems  show  physical  activity  information  (e.g.,  step  counts,  energy  expenditure,  heart  rate)  which  is  sufficient  for  many  self-knowledge  needs,  but  information  about  the  factors  that  affect  physical  activity  may  be  needed  for  deeper  self-reflection  and  increased  self-knowledge.  We  explored  the  use  of  contextual  information,  such  as  events,  places,  and  people,  to  support  reflection  on  the  factors  that  affect  physical  activity.  We  present  three  findings  from  our  studies.  First,  users  make  associations  between  physical  activity  and  contextual  information  that  help  them  become  aware  of  factors  that  affect  their  physical  activity.  Second,  reflecting  on  physical  activity  and  context  can  increase  people's  awareness  of  opportunities  for  physical  activity.  Lastly,  automated  tracking  of  physical  activity  and  contextual  information  benefits  long-term  reflection,  but  may  have  detrimental  effects  on  immediate  awareness.
2	Quantifying  semantic  proximity  between  contexts.  Autonomic  Agents  in  Open  intelligent  space  face  a  wide  diversity  of  Context  providers  and  formats.  With  a  micro  approach  to  Context-awareness,  individual  Agents  perform  their  own  assessment  of  individual  Context  relevance.  This  assessment  relies  in  part  on  the  semantic  proximity  between  requested  and  candidate  Contexts.  We  present  a  quantitative  semantic  distance  function  that  supports  subjective  Context  relevance  assessment  in  Agents.
2	A  sustainable  design  fiction.  In  this  article,  we  argue  that  an  approach  informed  by  practice  theory  coupled  with  design  fiction  provides  useful  insights  into  the  role  of  interaction  design  with  respect  to  environmental  sustain...
2	Up  close  and  personal  collaborative  work  on  a  high  resolution  multitouch  wall  display.  Multitouch  wall-sized  displays  afford  new  forms  of  collaboration:  They  can  be  used  up  close  by  several  users  simultaneously,  offer  high  resolution,  and  provide  sufficient  space  for  intertwining  individual  and  joint  work.  The  difference  to  displays  without  these  capabilities  is  not  well  understood.  To  better  understand  the  collaboration  of  groups  around  high-resolution  multitouch  wall  displays,  we  conducted  an  exploratory  study.  Pairs  collaborated  on  a  problem-solving  task  using  a  2.8m  ×  1.2m  multitouch  display  with  24.8  megapixels.  The  study  examines  how  participants  collaborate;  navigate  relative  to  the  display  and  to  each  other;  and  interact  with  and  share  the  display.  Participants  physically  navigated  among  different  parts  of  the  display,  switched  fluidly  between  parallel  and  joint  work,  and  shared  the  display  evenly.  The  results  contrast  earlier  research  that  suggests  difficulties  in  sharing  and  collaborating  around  wall  displays.  The  study  suggests  that  multitouch  wall  displays  can  support  different  collaboration  styles  and  fluid  transitions  in  group  work.
2	Communication  and  aac  in  the  lives  of  adults  with  autism  the  stories  of  their  older  parents.  The  aim  of  this  study  was  to  explore  the  communication  experiences,  particularly  those  related  to  augmentative  and  alternative  communication  (AAC),  of  older  parents  who  had  an  adult  son  or  daughter  with  autism.  A  narrative  analysis  of  in-depth  interviews  with  16  older  parents  indicated  that  the  majority  had  rarely  spontaneously  mentioned  AAC  or  other  communication  interventions.  Most  did  not  express  the  need  for  such  services.  Yet,  communication  breakdown  featured  prominently  in  parents’  narratives  about  interactions  with  their  son  or  daughter.  The  quality  of  the  communication  between  older  parents  and  their  offspring  with  autism  constituted  important  sources  of  both  gratification  and  strain  in  parents’  roles  as  caregivers.  Reasons  for  the  current  lack  of  communication  interventions  are  discussed,  along  with  implications  for  communication  and  AAC  service  provision.
2	Quantifying  the  creativity  support  of  digital  tools  through  the  creativity  support  index.  Creativity  support  tools  help  people  engage  creatively  with  the  world,  but  measuring  how  well  a  tool  supports  creativity  is  challenging  since  creativity  is  ill-defined.  To  this  end,  we  developed  the  Creativity  Support  Index  (CSI),  which  is  a  psychometric  survey  designed  for  evaluating  the  ability  of  a  creativity  support  tool  to  assist  a  user  engaged  in  creative  work.  The  CSI  measures  six  dimensions  of  creativity  support:  Exploration,  Expressiveness,  Immersion,  Enjoyment,  Results  Worth  Effort,  and  Collaboration.  The  CSI  allows  researchers  to  understand  not  just  how  well  a  tool  supports  creative  work  overall,  but  what  aspects  of  creativity  support  may  need  attention.  In  this  article,  we  present  the  CSI,  along  with  scenarios  for  how  it  can  be  deployed  in  a  variety  of  HCI  research  settings  and  how  the  CSI  scores  can  help  target  design  improvements.  We  also  present  the  iterative,  rigorous  development  and  validation  process  used  to  create  the  CSI.
2	Hypertendril  visual  analytics  for  user  driven  hyperparameter  optimization  of  deep  neural  networks.  To  mitigate  the  pain  of  manually  tuning  hyperparameters  of  deep  neural  networks,  automated  machine  learning  (AutoML)  methods  have  been  developed  to  search  for  an  optimal  set  of  hyperparameters  in  large  combinatorial  search  spaces.  However,  the  search  results  of  AutoML  methods  significantly  depend  on  initial  configurations,  making  it  a  non-trivial  task  to  find  a  proper  configuration.  Therefore,  human  intervention  via  a  visual  analytic  approach  bears  huge  potential  in  this  task.  In  response,  we  propose  HyperTendril,  a  web-based  visual  analytics  system  that  supports  user-driven  hyperparameter  tuning  processes  in  a  model-agnostic  environment.  HyperTendril  takes  a  novel  approach  to  effectively  steering  hyperparameter  optimization  through  an  iterative,  interactive  tuning  procedure  that  allows  users  to  refine  the  search  spaces  and  the  configuration  of  the  AutoML  method  based  on  their  own  insights  from  given  results.  Using  HyperTendril,  users  can  obtain  insights  into  the  complex  behaviors  of  various  hyperparameter  search  algorithms  and  diagnose  their  configurations.  In  addition,  HyperTendril  supports  variable  importance  analysis  to  help  the  users  refine  their  search  spaces  based  on  the  analysis  of  relative  importance  of  different  hyperparameters  and  their  interaction  effects.  We  present  the  evaluation  demonstrating  how  HyperTendril  helps  users  steer  their  tuning  processes  via  a  longitudinal  user  study  based  on  the  analysis  of  interaction  logs  and  in-depth  interviews  while  we  deploy  our  system  in  a  professional  industrial  environment.
2	Peakvizor  visual  analytics  of  peaks  in  video  clickstreams  from  massive  open  online  courses.  Massive  open  online  courses  (MOOCs)  aim  to  facilitate  open-access  and  massive-participation  education.  These  courses  have  attracted  millions  of  learners  recently.  At  present,  most  MOOC  platforms  record  the  web  log  data  of  learner  interactions  with  course  videos.  Such  large  amounts  of  multivariate  data  pose  a  new  challenge  in  terms  of  analyzing  online  learning  behaviors.  Previous  studies  have  mainly  focused  on  the  aggregate  behaviors  of  learners  from  a  summative  view;  however,  few  attempts  have  been  made  to  conduct  a  detailed  analysis  of  such  behaviors.  To  determine  complex  learning  patterns  in  MOOC  video  interactions,  this  paper  introduces  a  comprehensive  visualization  system  called  PeakVizor.  This  system  enables  course  instructors  and  education  experts  to  analyze  the  "peaks"  or  the  video  segments  that  generate  numerous  clickstreams.  The  system  features  three  views  at  different  levels:  the  overview  with  glyphs  to  display  valuable  statistics  regarding  the  peaks  detected;  the  flow  view  to  present  spatio-temporal  information  regarding  the  peaks;  and  the  correlation  view  to  show  the  correlation  between  different  learner  groups  and  the  peaks.  Case  studies  and  interviews  conducted  with  domain  experts  have  demonstrated  the  usefulness  and  effectiveness  of  PeakVizor,  and  new  findings  about  learning  behaviors  in  MOOC  platforms  have  been  reported.
2	Automatic  constraint  detection  for  2d  layout  regularization.  In  this  paper,  we  address  the  problem  of  constraint  detection  for  layout  regularization.  The  layout  we  consider  is  a  set  of  two-dimensional  elements  where  each  element  is  represented  by  its  bounding  box.  Layout  regularization  is  important  in  digitizing  plans  or  images,  such  as  floor  plans  and  facade  images,  and  in  the  improvement  of  user-created  contents,  such  as  architectural  drawings  and  slide  layouts.  To  regularize  a  layout,  we  aim  to  improve  the  input  by  detecting  and  subsequently  enforcing  alignment,  size,  and  distance  constraints  between  layout  elements.  Similar  to  previous  work,  we  formulate  layout  regularization  as  a  quadratic  programming  problem.  In  addition,  we  propose  a  novel  optimization  algorithm  that  automatically  detects  constraints.  We  evaluate  the  proposed  framework  using  a  variety  of  input  layouts  from  different  applications.  Our  results  demonstrate  that  our  method  has  superior  performance  to  the  state  of  the  art.
2	Dynamic  influence  networks  for  rule  based  models.  We  introduce  the  Dynamic  Influence  Network  (DIN),  a  novel  visual  analytics  technique  for  representing  and  analyzing  rule-based  models  of  protein-protein  interaction  networks.  Rule-based  modeling  has  proved  instrumental  in  developing  biological  models  that  are  concise,  comprehensible,  easily  extensible,  and  that  mitigate  the  combinatorial  complexity  of  multi-state  and  multi-component  biological  molecules.  Our  technique  visualizes  the  dynamics  of  these  rules  as  they  evolve  over  time.  Using  the  data  produced  by  KaSim,  an  open  source  stochastic  simulator  of  rule-based  models  written  in  the  Kappa  language,  DINs  provide  a  node-link  diagram  that  represents  the  influence  that  each  rule  has  on  the  other  rules.  That  is,  rather  than  representing  individual  biological  components  or  types,  we  instead  represent  the  rules  about  them  (as  nodes)  and  the  current  influence  of  these  rules  (as  links).  Using  our  interactive  DIN-Viz  software  tool,  researchers  are  able  to  query  this  dynamic  network  to  find  meaningful  patterns  about  biological  processes,  and  to  identify  salient  aspects  of  complex  rule-based  models.  To  evaluate  the  effectiveness  of  our  approach,  we  investigate  a  simulation  of  a  circadian  clock  model  that  illustrates  the  oscillatory  behavior  of  the  KaiC  protein  phosphorylation  cycle.
2	An  interactive  method  to  improve  crowdsourced  annotations.  In  order  to  effectively  infer  correct  labels  from  noisy  crowdsourced  annotations,  learning-from-crowds  models  have  introduced  expert  validation.  However,  little  research  has  been  done  on  facilitating  the  validation  procedure.  In  this  paper,  we  propose  an  interactive  method  to  assist  experts  in  verifying  uncertain  instance  labels  and  unreliable  workers.  Given  the  instance  labels  and  worker  reliability  inferred  from  a  learning-from-crowds  model,  candidate  instances  and  workers  are  selected  for  expert  validation.  The  influence  of  verified  results  is  propagated  to  relevant  instances  and  workers  through  the  learning-from-crowds  model.  To  facilitate  the  validation  of  annotations,  we  have  developed  a  confusion  visualization  to  indicate  the  confusing  classes  for  further  exploration,  a  constrained  projection  method  to  show  the  uncertain  labels  in  context,  and  a  scatter-plot-based  visualization  to  illustrate  worker  reliability.  The  three  visualizations  are  tightly  integrated  with  the  learning-from-crowds  model  to  provide  an  iterative  and  progressive  environment  for  data  validation.  Two  case  studies  were  conducted  that  demonstrate  our  approach  offers  an  efficient  method  for  validating  and  improving  crowdsourced  annotations.
2	Stainedview  variable  intensity  light  attenuation  display  with  cascaded  spatial  color  filtering  for  improved  color  fidelity.  We  present  StainedView,  an  optical  see-through  display  that  spatially  filters  the  spectral  distribution  of  light  to  form  an  image  with  improved  color  fidelity.  Existing  light-attenuation  displays  have  limited  color  fidelity  and  contrast,  resulting  in  a  degraded  appearance  of  virtual  images.  To  use  these  displays  to  present  virtual  images  that  are  more  consistent  with  the  real  world,  we  require  three  things:  intensity  modulation  of  incoming  light,  spatial  color  filtering  with  narrower  bandwidth,  and  appropriate  light  modulation  for  incoming  light  with  an  arbitrary  spectral  distribution.  In  StainedView,  we  address  the  three  requirements  by  cascading  two  phase-only  spatial  light  modulators  (PSLMs),  a  digital  micromirror  device,  and  polarization  optics  to  control  both  light  intensity  and  spectrum  distribution.  We  show  that  our  design  has  a  1.8  times  wider  color  gamut  fidelity  (75.8%  fulfillment  of  sRGB  color  space)  compared  to  the  existing  single-PSLM  approach  (41.4%)  under  a  reference  white  light.  We  demonstrated  the  design  with  a  proof-of-concept  display  system.  We  further  introduce  our  optics  design  and  pixel-selection  algorithm  for  the  given  light  input,  evaluate  the  spatial  color  filter,  and  discuss  the  limitation  of  the  current  prototype.
2	Vassl  a  visual  analytics  toolkit  for  social  spambot  labeling.  Social  media  platforms  are  filled  with  social  spambots.  Detecting  these  malicious  accounts  is  essential,  yet  challenging,  as  they  continually  evolve  to  evade  detection  techniques.  In  this  article,  we  present  VASSL,  a  visual  analytics  system  that  assists  in  the  process  of  detecting  and  labeling  spambots.  Our  tool  enhances  the  performance  and  scalability  of  manual  labeling  by  providing  multiple  connected  views  and  utilizing  dimensionality  reduction,  sentiment  analysis  and  topic  modeling,  enabling  insights  for  the  identification  of  spambots.  The  system  allows  users  to  select  and  analyze  groups  of  accounts  in  an  interactive  manner,  which  enables  the  detection  of  spambots  that  may  not  be  identified  when  examined  individually.  We  present  a  user  study  to  objectively  evaluate  the  performance  of  VASSL  users,  as  well  as  capturing  subjective  opinions  about  the  usefulness  and  the  ease  of  use  of  the  tool.
2	Blood  flow  clustering  and  applications  invirtual  stenting  of  intracranial  aneurysms.  Understanding  the  hemodynamics  of  blood  flow  in  vascular  pathologies  such  as  intracranial  aneurysms  is  essential  for  both  their  diagnosis  and  treatment.  Computational  fluid  dynamics  (CFD)  simulations  of  blood  flow  based  on  patient-individual  data  are  performed  to  better  understand  aneurysm  initiation  and  progression  and  more  recently,  for  predicting  treatment  success.  In  virtual  stenting,  a  flow-diverting  mesh  tube  (stent)  is  modeled  inside  the  reconstructed  vasculature  and  integrated  in  the  simulation.  We  focus  on  steady-state  simulation  and  the  resulting  complex  multiparameter  data.  The  blood  flow  pattern  captured  therein  is  assumed  to  be  related  to  the  success  of  stenting.  It  is  often  visualized  by  a  dense  and  cluttered  set  of  streamlines.We  present  a  fully  automatic  approach  for  reducing  visual  clutter  and  exposing  characteristic  flow  structures  by  clustering  streamlines  and  computing  cluster  representatives.  While  individual  clustering  techniques  have  been  applied  before  to  streamlines  in  3D  flow  fields,  we  contribute  a  general  quantitative  and  a  domain-specific  qualitative  evaluation  of  three  state-of-the-art  techniques.  We  show  that  clustering  based  on  streamline  geometry  as  well  as  on  domain-specific  streamline  attributes  contributes  to  comparing  and  evaluating  different  virtual  stenting  strategies.  With  our  work,  we  aim  at  supporting  CFD  engineers  and  interventional  neuroradiologists.
2	2  5d  cartoon  hair  modeling  and  manipulation.  This  paper  addresses  a  challenging  single-view  modeling  and  animation  problem  with  cartoon  images.  Our  goal  is  to  model  the  hairs  in  a  given  cartoon  image  with  consistent  layering  and  occlusion,  so  that  we  can  produce  various  visual  effects  from  just  a  single  image.  We  propose  a  novel  2.5D  modeling  approach  to  deal  with  this  problem.  Given  an  input  image,  we  first  segment  the  hairs  of  the  cartoon  character  into  regions  of  hair  strands.  Then,  we  apply  our  novel  layering  metric,  which  is  derived  from  the  Gestalt  psychology,  to  automatically  optimize  the  depth  ordering  among  the  hair  strands.  After  that,  we  employ  our  hair  completion  method  to  fill  the  occluded  part  of  each  hair  strand,  and  create  a  2.5D  model  of  the  cartoon  hair.  By  using  this  model,  we  can  produce  various  visual  effects,  e.g.,  we  develop  a  simplified  fluid  simulation  model  to  produce  wind  blowing  animations  with  the  2.5D  hairs.  To  further  demonstrate  the  applicability  and  versatility  of  our  method,  we  compare  our  results  with  real  cartoon  hair  animations,  and  also  apply  our  model  to  produce  a  wide  variety  of  hair  manipulation  effects,  including  hair  editing  and  hair  braiding.
2	Towards  zero  waste  furniture  design.  In  traditional  design,  shapes  are  first  conceived,  and  then  fabricated.  While  this  decoupling  simplifies  the  design  process,  it  can  result  in  unwanted  material  wastage,  especially  where  off-cut  pieces  are  hard  to  reuse.  In  absence  of  explicit  feedback  on  material  usage,  the  designer  remains  helpless  to  effectively  adapt  the  design  –  even  when  design  variabilities  exist.  We  investigate    waste  minimizing  furniture  design    wherein  based  on  the  current  design,  the  user  is  presented  with  design  variations  that  result  in  less  wastage  of  materials.  Technically,  we  dynamically  analyze  material  space  layout  to  determine    which    parts  to  change  and    how    ,  while  maintaining  original  design  intent  specified  in  the  form  of  design  constraints.  We  evaluate  the  approach  on  various  design  scenarios,  and  demonstrate  effective  material  usage  that  is  difficult,  if  not  impossible,  to  achieve  without  computational  support.
2	Pattern  driven  navigation  in  2d  multiscale  visualizations  with  scalable  insets.  We  present  Scalable  Insets  ,  a  technique  for  interactively  exploring  and  navigating  large  numbers  of  annotated  patterns  in  multiscale  visualizations  such  as  gigapixel  images,  matrices,  or  maps.  Exploration  of  many  but  sparsely-distributed  patterns  in  multiscale  visualizations  is  challenging  as  visual  representations  change  across  zoom  levels,  context  and  navigational  cues  get  lost  upon  zooming,  and  navigation  is  time  consuming.  Our  technique  visualizes  annotated  patterns  too  small  to  be  identifiable  at  certain  zoom  levels  using  insets,  i.e.,  magnified  thumbnail  views  of  the  annotated  patterns.  Insets  support  users  in  searching,  comparing,  and  contextualizing  patterns  while  reducing  the  amount  of  navigation  needed.  They  are  dynamically  placed  either  within  the  viewport  or  along  the  boundary  of  the  viewport  to  offer  a  compromise  between  locality  and  context  preservation.  Annotated  patterns  are  interactively  clustered  by  location  and  type.  They  are  visually  represented  as  an  aggregated  inset  to  provide  scalable  exploration  within  a  single  viewport.  In  a  controlled  user  study  with  18  participants,  we  found  that  Scalable  Insets  can  speed  up  visual  search  and  improve  the  accuracy  of  pattern  comparison  at  the  cost  of  slower  frequency  estimation  compared  to  a  baseline  technique.  A  second  study  with  6  experts  in  the  field  of  genomics  showed  that  Scalable  Insets  is  easy  to  learn  and  provides  first  insights  into  how  Scalable  Insets  can  be  applied  in  an  open-ended  data  exploration  scenario.
2	Simplifly  a  methodology  for  simplification  and  thematic  enhancement  of  trajectories.  Movement  data  sets  collected  using  today’s  advanced  tracking  devices  consist  of  complex  trajectories  in  terms  of  length,  shape,  and  number  of  recorded  positions.  Multiple  additional  attributes  characterizing  the  movement  and  its  environment  are  often  also  included  making  the  level  of  complexity  even  higher.  Simplification  of  trajectories  can  improve  the  visibility  of  relevant  information  by  reducing  less  relevant  details  while  maintaining  important  movement  patterns.  We  propose  a  systematic  stepwise  methodology  for  simplifying  and  thematically  enhancing  trajectories  in  order  to  support  their  visual  analysis.  The  methodology  is  applied  iteratively  and  is  composed  of:  (a)  a  simplification  step  applied  to  reduce  the  morphological  complexity  of  the  trajectories,  (b)  a  thematic  enhancement  step  which  aims  at  accentuating  patterns  of  movement,  and  (c)  the  representation  and  interactive  exploration  of  the  results  in  order  to  make  interpretations  of  the  findings  and  further  refinement  to  the  simplification  and  enhancement  process.  We  illustrate  our  methodology  through  an  analysis  example  of  two  different  types  of  tracks,  aircraft  and  pedestrian  movement.
2	Diderot  a  domain  specific  language  for  portable  parallel  scientific  visualization  and  image  analysis.  Many  algorithms  for  scientific  visualization  and  image  analysis  are  rooted  in  the  world  of  continuous  scalar,  vector,  and  tensor  fields,  but  are  programmed  in  low-level  languages  and  libraries  that  obscure  their  mathematical  foundations.  Diderot  is  a  parallel  domain-specific  language  that  is  designed  to  bridge  this  semantic  gap  by  providing  the  programmer  with  a  high-level,  mathematical  programming  notation  that  allows  direct  expression  of  mathematical  concepts  in  code.  Furthermore,  Diderot  provides  parallel  performance  that  takes  advantage  of  modern  multicore  processors  and  GPUs.  The  high-level  notation  allows  a  concise  and  natural  expression  of  the  algorithms  and  the  parallelism  allows  efficient  execution  on  real-world  datasets.
2	Translucent  radiosity  efficiently  combiningdiffuse  inter  reflection  andsubsurface  scattering.  It  is  hard  to  efficiently  model  the  light  transport  in  scenes  with  translucent  objects  for  interactive  applications.  The  inter-reflection  between  objects  and  their  environments  and  the  subsurface  scattering  through  the  materials  intertwine  to  produce  visual  effects  like  color  bleeding,  light  glows,  and  soft  shading.  Monte-Carlo  based  approaches  have  demonstrated  impressive  results  but  are  computationally  expensive,  and  faster  approaches  model  either  only  inter-reflection  or  only  subsurface  scattering.  In  this  paper,  we  present  a  simple  analytic  model  that  combines  diffuse  inter-reflection  and  isotropic  subsurface  scattering.  Our  approach  extends  the  classical  work  in  radiosity  by  including  a  subsurface  scattering  matrix  that  operates  in  conjunction  with  the  traditional  form  factor  matrix.  This  subsurface  scattering  matrix  can  be  constructed  using  analytic,  measurement-based  or  simulation-based  models  and  can  capture  both  homogeneous  and  heterogeneous  translucencies.  Using  a  fast  iterative  solution  to  radiosity,  we  demonstrate  scene  relighting  and  dynamically  varying  object  translucencies  at  near  interactive  rates.
2	Interactive  high  relief  reconstruction  for  organic  and  double  sided  objects  from  a  photo.  We  introduce  an  interactive  user-driven  method  to  reconstruct  high-relief  3D  geometry  from  a  single  photo.  Particularly,  we  consider  two  novel  but  challenging  reconstruction  issues:  i)  common  non-rigid  objects  whose  shapes  are  organic  rather  than  polyhedral/symmetric,  and  ii)  double-sided  structures,  where  front  and  back  sides  of  some  curvy  object  parts  are  revealed  simultaneously  on  image.  To  address  these  issues,  we  develop  a  three-stage  computational  pipeline.  First,  we  construct  a  2.5D  model  from  the  input  image  by  user-driven  segmentation,  automatic  layering,  and  region  completion,  handling  three  common  types  of  occlusion.  Second,  users  can  interactively  mark-up  slope  and  curvature  cues  on  the  image  to  guide  our  constrained  optimization  model  to  inflate  and  lift  up  the  image  layers.  We  provide  real-time  preview  of  the  inflated  geometry  to  allow  interactive  editing.  Third,  we  stitch  and  optimize  the  inflated  layers  to  produce  a  high-relief  3D  model.  Compared  to  previous  work,  we  can  generate  high-relief  geometry  with  large  viewing  angles,  handle  complex  organic  objects  with  multiple  occluded  regions  and  varying  shape  profiles,  and  reconstruct  objects  with  double-sided  structures.  Lastly,  we  demonstrate  the  applicability  of  our  method  on  a  wide  variety  of  input  images  with  human,  animals,  flowers,  etc.
2	E  festa  ensemble  feature  exploration  with  surface  density  estimates.  We  propose  surface  density  estimate  (SDE)  to  model  the  spatial  distribution  of  surface  features—isosurfaces,  ridge  surfaces,  and  streamsurfaces—in  3D  ensemble  simulation  data.  The  inputs  of  SDE  computation  are  surface  features  represented  as  polygon  meshes,  and  no  field  datasets  are  required  (e.g.,  scalar  fields  or  vector  fields).  The  SDE  is  defined  as  the  kernel  density  estimate  of  the  infinite  set  of  points  on  the  input  surfaces  and  is  approximated  by  accumulating  the  surface  densities  of  triangular  patches.  We  also  propose  an  algorithm  to  guide  the  selection  of  a  proper  kernel  bandwidth  for  SDE  computation.  An  e  nsemble  F  eature  E  xploration  method  based  on  S  urface  densi  T  y  Estim  A  tes  (  e  FESTA)  is  then  proposed  to  extract  and  visualize  the  major  trends  of  ensemble  surface  features.  For  an  ensemble  of  surface  features,  each  surface  is  first  transformed  into  a  density  field  based  on  its  contribution  to  the  SDE,  and  the  resulting  density  fields  are  organized  into  a  hierarchical  representation  based  on  the  pairwise  distances  between  them.  The  hierarchical  representation  is  then  used  to  guide  visual  exploration  of  the  density  fields  as  well  as  the  underlying  surface  features.  We  demonstrate  the  application  of  our  method  using  isosurface  in  ensemble  scalar  fields,  Lagrangian  coherent  structures  in  uncertain  unsteady  flows,  and  streamsurfaces  in  ensemble  fluid  flows.
2	Measuring  implicit  science  learning  with  networks  of  player  game  interactions.  Visualizing  player  behavior  in  complex  problem  solving  tasks  such  as  games  is  important  for  both  assessing  learning  and  for  the  design  of  content.  We  collected  data  from  195  high  school  students  playing  an  optics  puzzle  game,  Quantum  Spectre,  and  modeled  their  game  play  as  an  interaction  network,  examining  errors  hypothesized  to  be  related  to  a  lack  of  implicit  understanding  of  the  science  concepts  embedded  in  the  game.  We  found  that  the  networks  were  useful  for  visualization  of  student  behavior,  identifying  areas  of  student  misconceptions  and  locating  regions  of  the  network  where  students  become  stuck.  Preliminary  regression  analyses  show  a  negative  relationship  between  the  science  misconceptions  identified  during  gameplay  and  implicit  science  learning.
2	Surface  and  contour  preserving  origamic  architecture  paper  pop  ups.  Origamic  architecture  (OA)  is  a  form  of  papercraft  that  involves  cutting  and  folding  a  single  sheet  of  paper  to  produce  a  3D  pop-up,  and  is  commonly  used  to  depict  architectural  structures.  Because  of  the  strict  geometric  and  physical  constraints,  OA  design  requires  considerable  skill  and  effort.  In  this  paper,  we  present  a  method  to  automatically  generate  an  OA  design  that  closely  depicts  an  input  3D  model.  Our  algorithm  is  guided  by  a  novel  set  of  geometric  conditions  to  guarantee  the  foldability  and  stability  of  the  generated  pop-ups.  The  generality  of  the  conditions  allows  our  algorithm  to  generate  valid  pop-up  structures  that  are  previously  not  accounted  for  by  other  algorithms.  Our  method  takes  a  novel  image-domain  approach  to  convert  the  input  model  to  an  OA  design.  It  performs  surface  segmentation  of  the  input  model  in  the  image  domain,  and  carefully  represents  each  surface  with  a  set  of  parallel  patches.  Patches  are  then  modified  to  make  the  entire  structure  foldable  and  stable.  Visual  and  quantitative  comparisons  of  results  have  shown  our  algorithm  to  be  significantly  better  than  the  existing  methods  in  the  preservation  of  contours,  surfaces,  and  volume.  The  designs  have  also  been  shown  to  more  closely  resemble  those  created  by  real  artists.
2	Globe  browsing  contextualized  spatio  temporal  planetary  surface  visualization.  Results  of  planetary  mapping  are  often  shared  openly  for  use  in  scientific  research  and  mission  planning.  In  its  raw  format,  however,  the  data  is  not  accessible  to  non-experts  due  to  the  difficulty  in  grasping  the  context  and  the  intricate  acquisition  process.  We  present  work  on  tailoring  and  integration  of  multiple  data  processing  and  visualization  methods  to  interactively  contextualize  geospatial  surface  data  of  celestial  bodies  for  use  in  science  communication.  As  our  approach  handles  dynamic  data  sources,  streamed  from  online  repositories,  we  are  significantly  shortening  the  time  between  discovery  and  dissemination  of  data  and  results.  We  describe  the  image  acquisition  pipeline,  the  pre-processing  steps  to  derive  a  2.5D  terrain,  and  a  chunked  level-of-detail,  out-of-core  rendering  approach  to  enable  interactive  exploration  of  global  maps  and  high-resolution  digital  terrain  models.  The  results  are  demonstrated  for  three  different  celestial  bodies.  The  first  case  addresses  high-resolution  map  data  on  the  surface  of  Mars.  A  second  case  is  showing  dynamic  processes,  such  as  concurrent  weather  conditions  on  Earth  that  require  temporal  datasets.  As  a  final  example  we  use  data  from  the  New  Horizons  spacecraft  which  acquired  images  during  a  single  flyby  of  Pluto.  We  visualize  the  acquisition  process  as  well  as  the  resulting  surface  data.  Our  work  has  been  implemented  in  the  OpenSpace  software  [8]  ,  which  enables  interactive  presentations  in  a  range  of  environments  such  as  immersive  dome  theaters,  interactive  touch  tables,  and  virtual  reality  headsets.
2	Problematizing  cultural  appropriation.  Cultural  appropriation  in  games  entails  the  taking  of  knowledge,  artifacts  or  expression  from  a  culture  and  recontextualizing  it  within  game  structures.  While  cultural  appropriation  is  a  pervasive  practice  in  games,  little  attention  has  been  given  to  the  ethical  issues  that  emerge  from  such  practices  with  regards  to  how  culture  is  portrayed.  This  paper  problematizes  cultural  appropriation  in  the  context  of  a  serious  game  for  children  inspired  by  Dia  de  los  Muertos,  a  Mexican  festival  focused  on  remembrance  of  the  dead.  Taking  a  research  through  design  approach,  we  demonstrate  that  recontextualised  cultural  elements  can  retain  their  basic,  original  meaning.  However,  we  also  find  that  cultural  appropriation  is  inevitable  and  its  ethical  implications  can  be  far  reaching.  In  our  context,  ethical  concerns  arose  as  a  result  of  children's  beliefs  that  death  affects  prominent  others  and  their  destructive  ways  of  coping  with  death.  We  argue  that  revealing  emergent  ethical  concerns  is  imperative  before  deciding  how  and  in  what  way  to  encourage  culturally  authentic  narratives.
2	Fastc  accelerated  fixed  rate  texture  encoding.  We  present  a  new  algorithm  for  encoding  low  dynamic  range  images  into  fixed-rate  texture  compression  formats.  Our  approach  provides  orders  of  magnitude  improvements  in  speed  over  existing  publicly-available  compressors,  while  generating  high  quality  results.  The  algorithm  is  applicable  to  any  fixed-rate  texture  encoding  scheme  based  on  Block  Truncation  Coding  and  we  use  it  to  compress  images  into  the  OpenGL  BPTC  format.  The  underlying  technique  uses  an  axis-aligned  bounding  box  to  estimate  the  proper  partitioning  of  a  texel  block  and  performs  a  generalized  cluster  fit  to  compute  the  endpoint  approximation.  This  approximation  can  be  further  refined  using  simulated  annealing.  The  algorithm  is  inherently  parallel  and  scales  with  the  number  of  processor  cores.  We  highlight  its  performance  on  low-frequency  game  textures  and  the  high  frequency  Kodak  Test  Image  Suite.
2	Sound  localization  and  multi  modal  steering  for  autonomous  virtual  agents.  With  the  increasing  realism  of  interactive  applications,  there  is  a  growing  need  for  harnessing  additional  sensory  modalities  such  as  hearing.  While  the  synthesis  and  propagation  of  sounds  in  virtual  environments  has  been  explored,  there  has  been  little  work  that  addresses  sound  localization  and  its  integration  into  behaviors  for  autonomous  virtual  agents.  This  paper  develops  a  framework  that  enables  autonomous  virtual  agents  to  localize  sounds  in  dynamic  virtual  environments,  subject  to  distortion  effects  due  to  attenuation,  reflection  and  diffraction  from  obstacles,  as  well  as  interference  between  multiple  audio  signals.  We  additionally  integrate  hearing  into  standard  predictive  collision  avoidance  techniques  and  couple  it  with  vision  to  allow  agents  to  react  to  what  they  see  and  hear,  while  navigating  in  virtual  environments.
2	Increasing  top  20  search  results  diversity  through  recommendation  post  processing.  This  paper  presents  three  different  methods  for  diversifying  search  results,  that  were  developed  as  part  of  our  user  modelling  research.  All  three  methods  focus  on  post-processing  search  results  provided  by  the  baseline  recommender  systems  and  increase  the  diversity  (measured  with  ILD@20)  at  the  cost  of  final  precision  (measured  with  F@20).  The  authors  feel  that  these  methods  have  potential  yet  require  further  development  and  testing.
2	A  virtual  reality  environment  for  prospective  memory  training.  Prospective  Memory  (PM),  or  remembering  to  perform  tasks  in  the  future,  is  of  crucial  importance  for  everyday  life.  Stroke  survivors  often  have  impaired  prospective  memory,  which  can  interfere  with  their  independent  living.  In  2011,  we  started  working  on  computer-based  training  for  improving  prospective  memory  in  stroke  patients.  The  primary  goal  of  our  project  is  to  develop  an  effective  PM  treatment  that  could  be  used  without  the  input  of  clinicians.  Our  approach  combines  the  use  of  visual  imagery  with  practice  in  a  Virtual  Reality  (VR)  environment.  In  this  paper,  we  present  the  VR  environment  and  the  user  modelling  approach  implemented.
2	Looking  beyond  transfer  models  finding  other  sources  of  power  for  student  models.  Student  modeling  plays  an  important  role  in  educational  research.  Many  techniques  have  been  developed  focusing  on  accurately  estimating  student  performances.  In  this  paper,  using  Performance  Factors  Analysis  as  our  framework,  we  examine  what  components  of  the  model  enable  us  to  better  predict,  and  consequently  better  understand,  student  performance.  Using  transfer  models  to  predict  is  very  common  across  different  student  modeling  techniques,  as  student  proficiencies  on  those  required  skills  are  believed,  to  a  large  degree,  to  determine  student  performance.  However,  we  found  that  problem  difficulty  is  an  even  more  important  predictor  than  student  knowledge  of  the  required  skills.  In  addition,  we  found  that  using  student  proficiencies  across  all  skills  works  better  than  just  using  those  skills  thought  relevant  by  the  transfer  model.  We  tested  our  proposed  models  with  two  transfer  models  of  fine-  and  coarse-grain  sizes;  the  results  suggest  that  the  improvement  is  not  simply  an  illusion  due  to  possible  mistakes  in  associating  skills  with  problems.
2	A  user  interface  for  semantic  competence  profiles.  Competence  management  systems  are  increasingly  based  on  ontologies  representing  competences  within  a  certain  domain.  Most  of  these  systems  represent  a  user's  competence  profile  by  means  of  an  ontological  structure.  Such  semantic  competence  profiles,  often  structured  as  a  hierarchy  of  competences,  are  difficult  to  navigate  for  self-assessment  purposes.  The  more  competences  a  user  profile  holds,  the  more  challenging  the  comprehensive  presentation  of  profile  data  is.  In  this  paper,  we  present  an  integrated  user  interface  that  supports  users  during  competence  self-assessment  and  facilitates  a  clear  presentation  of  their  semantic  competence  profiles.  For  evaluation,  we  conducted  a  usability  study  with  19  students  at  university.  The  results  show  that  users  were  mostly  satisfied  with  the  usability  of  the  interface  that  also  represents  a  promising  approach  for  efficient  competence  self-assessment.
2	Omg  how  did  it  know  that  reactions  to  highly  personalized  ads.  In  this  paper,  we  explore  the  question  "would  people  be  willing  to  share  their  personal  data  in  exchange  for  highly-personalized  online  ads?"  through  a  Wizard-of-Oz  deception  study.  Our  volunteers  were  exposed  via  a  web  browser  to  three  different  highly-  personalized  ads,  designed  by  people  who  knew  them  well.  They  were  made  believe  that  the  ads  had  been  generated  automatically  by  an  Artificial  Intelligence  engine  on  the  basis  of  their  browsing  &  location  history  and/or  personal  traits.  The  participants'  reactions  were  surprisingly  favorable:  in  more  than  50%  of  the  cases,  the  ads  triggered  spontaneous  positive  emotional  reactions;  almost  90%  of  participants  would  share  at  least  two  of  the  three  data  sources  with  advertisers;  and  about  50%  would  share  all  data  sources.  Our  results  provide  evidence  that  highly-personalized  ads  may  offset  the  concerns  that  people  have  about  sharing  their  personal  data.  Thus  further  efforts  in  building  increasingly  personalized  online  ads  would  represent  a  worthwhile  endeavour.
2	Embedding  knowledge  graphs  for  semantics  aware  recommendations  based  on  dbpedia.  In  this  paper  we  present  a  semantics-aware  recommendation  strategy  that  uses  graph  embedding  techniques  to  learn  a  vector  space  reresentation  of  the  items  to  be  recommended.  Such  a  representation  relies  on  the  tripartite  graph  which  connects  users,  items  and  entities  gathered  from  DBpedia,  thus  it  encodes  both  collaborative  and  content-based  information.  These  embeddings  are  then  used  to  feed  with  positive  and  negative  examples  (the  items  the  user  liked  and  those  she  did  not  like)  a  classification  model,  which  is  finally  exploited  to  classify  new  items  as  interesting  or  not  interesting  for  the  target  user.  In  the  experimental  evaluation  we  evaluate  the  effectiveness  of  our  method  on  varying  of  different  graph  embedding  techniques  and  on  several  topologies  of  the  graph.  Results  show  that  the  embeddings  learnt  by  combining  collaborative  data  points  with  the  information  gathered  from  DBpedia  led  to  the  best  results  and  also  beat  several  state-of-the-art  techniques.
2	Age  and  computer  self  efficacy  in  the  use  of  digital  technologies  an  investigation  of  prototypes  for  public  self  service  terminals.  Previous  research  suggests  that  self-efficacy  (SE),  i.e.  the  belief  'in  one's  capabilities  to  organize  and  execute  the  courses  of  action  required  to  produce  given  attainments'  (Bandura),  plays  an  important  role  in  the  usage  of  self-service  technologies  especially  for  elder  customers.  Two  experiments  with  different  prototypes  of  ticket  vending  machines  (TVM)  were  conducted.  Participants  were  selected  according  to  their  age  (half  of  the  participants  aged  55  or  older)  and  levels  of  general  computer  self-efficacy  (CSE).  The  first  experiment  shows  that  CSE  contributes  to  both  the  user´s  performance  and  ratings  of  task-specific  SE,  while  age  affects  the  performance  only.  The  second  experiment  indicates  that  using  the  novel  TVM  increases  the  user's  task-specific  SE  except  for  elder  users  with  low  CSE  ---  who  faced  critical  problems  in  the  more  complex  tasks.  Results  indicate  that  future  research  on  digital  inclusion  should  focus  on  elder  users  with  low  CSE.
2	Experimental  environments  for  dismounted  human  robot  multimodal  communications.  The  goal  for  multimodal  communication  (MMC)  is  to  facilitate  the  conveyance  of  information  through  various  modalities,  such  as  auditory,  visual,  and  tactile.  MMC  has  become  a  major  focus  for  enabling  human-robot  teaming,  but  it  is  often  the  case  that  the  technological-state  of  robot  capabilities  is  limited  for  research  and  development.  Currently,  robots  often  serve  a  single  role,  not  equipped  to  interact  dynamically  with  human  team  members.  However,  before  that  functionality  is  developed,  it  is  important  to  understand  what  robot  capability  is  needed  for  effective  collaboration.  Through  the  use  of  simulations,  controlled  systematic  evaluation  of  MMC  input  and  output  devices  can  be  evaluated  to  garner  a  better  understanding  of  how  to  apply  MMC  with  respect  to  user’s  abilities  and  preferences,  as  well  as  assess  the  communication  hardware  and  software  functionality.  An  experiment  will  be  presented  and  discussed  to  illustrate  this  approach.
2	Remote  evaluation  of  wcag  2  0  techniques  by  web  users  with  visual  disabilities.  The  Web  Content  Accessibility  Guidelines  represent  an  opportunity  to  provide  concrete,  structured  guidance  for  designers  and  developers  regarding  how  to  build  accessible  web  pages.  However,  there  is  currently  a  lack  of  evidence  regarding  which  techniques  contained  within  WCAG  2.0  produce  accessible  websites.  This  paper  presents  a  methodology  for  evaluating  implementation  techniques  with  remote  users  and  demonstrates  its  use  in  evaluation  techniques  for  one  Success  Criterion  of  WCAG  2.0.
2	Confucius  computer  bridging  intergenerational  communication  through  illogical  and  cultural  computing.  Confucius  Computer  is  a  new  form  of  illogical  cultural  computing  based  on  the  Eastern  paradigms  of  balance  and  harmony.  The  system  uses  new  media  to  revive  and  model  ancient  Eastern  and  Confucius  philosophies  and  teachings,  presenting  them  in  new  contexts,  such  as  online  social  chat,  music  and  food.  Based  on  the  model  of  Eastern  mind  and  teaching,  the  system  enables  users  to  have  meaningful  social  network  communication  with  a  virtual  Confucius.  The  Confucius  Computer  system  offers  a  new  artistic  playground  for  interactive  music-painting  creation  based  on  our  Confucius  music  filters  and  the  ancient  model  of  Cycles  of  Balance.  Confucius  Computer  also  allows  users  to  explore  the  traditional  Chinese  medicine  concept  of  Yin-Yang  through  interactive  recipe  creation.  Detailed  descriptions  of  the  systems  are  presented  in  this  paper.  Our  user  studies  showed  that  users  gave  positive  feedbacks  to  their  experience  of  interacting  with  Confucius  Computer.  They  believed  that  this  media  could  improve  intergenerational  interaction  and  promote  a  sense  of  calmness.
2	Game  based  speech  rehabilitation  for  people  with  parkinson  s  disease.  Neurodegenerative  syndromes  such  as  Parkinson’s  disease  usually  lead  to  speech  impairments.  Reduced  intelligibility  of  spoken  language  is  treatable  with  Speech  and  Language  Therapy.  A  successful  speech  therapy  implements  the  principles  of  frequency,  intensity  and  repetition.  Consequently,  patients  need  to  be  highly  motivated  for  the  exercises  to  keep  up  with  their  training.  We  argue  that  game-based  technology  are  prone  to  support  patients  in  partaking  in  a  self-sustained  high  frequency  training.  Furthermore,  studies  demonstrate  that  game-based  interventions  have  the  potential  to  enhance  motivation  for  rehabilitative  exercising  in  patients  with  neurological  disorders.  Building  on  these  insights  we  apply  successful  principles  of  gamification  to  enhance  impaired  speech  in  patients  with  neurogenerative  syndromes.  With  the  ISi-Speech  project  (‘Individualisierte  Spracherkennung  in  der  Rehabilitation  fur  Menschen  mit  Beeintrachtigung  in  der  Sprechverstandlichkeit’  (in  German)  [individual  speech  recognition  in  therapy  for  people  with  motor  speech  disorders])  we  further  integrate  psychological  motivation  theory  (self-determination)  and  user  driven  design  into  the  developmental  process  of  a  rehabilitation  tool  for  patients  with  Parkinson’s  disease.
2	Command  and  control  collaboration  sand  table  c2  cst.  A  Command  and  Control  (C2)  display  system  using  the  Microsoft  HoloLens  and  the  Intelligent  Multi-UxV  Planner  with  Adaptive  Collaborative  Control  Technologies  (IMPACT)  has  been  developed  as  a  demonstration  of  a  new  advanced  user  interface.  This  allows  for  human-to-human-to-machine  collaboration  for  situational  awareness,  decision  making,  and  C2  planning  and  execution  of  simulated  multi-unmanned  heterogeneous  autonomous  vehicles.  The  advanced  user  interface  allows  multiple  operators  to  collaborate  across  a  shared  holographic  sand  table  and  control  multiple  vehicles.  Multiple  networking  frameworks  were  used  to  offload  the  computation  of  vehicle  autonomy  and  planning  algorithms  to  allow  the  HoloLens  to  run  efficiently  for  an  improved  user  experience.  Additionally,  the  concept  of  pseudo-classified  information  filtering  allows  for  tiers  of  classification  levels  for  each  HoloLens  user  derived  from  a  ‘need-to-know’  classification  basis.
2	Towards  ambient  intelligence  in  the  classroom.  This  paper  discusses  an  education-centric  approach  towards  ambient  intelligence  in  the  classroom,  raising  fundamental  requirements  that  should  be  taken  into  consideration,  in  order  to  efficiently  provide  genuine  students'  education  enhancement.  These  requirements  are  addressed  by  an  integrated  architecture  for  pervasive  computing  environments,  named  ClassMATE,  which  facilitates  all  necessary  mechanisms  for  context  -  aware  ubiquitous  computing  in  the  classroom.  Furthermore,  a  smart  classroom  prototype,  incorporating  the  ClassMATE's  infrastructure,  is  presented  constituting  the  first  test  -  bed  for  the  study  of  the  educational  process  in  intelligent  classrooms.
2	A  cross  cultural  comparison  of  salient  perceptual  characteristics  of  height  channels  for  a  virtual  auditory  environment.  Perceptual  characteristics  of  virtual  auditory  environments  from  three  listener  groups  were  compared.  To  generate  convincing  and  pleasing  virtual  auditory  environments,  acoustic  impulse  responses  were  measured  in  two  venues  using  an  innovative  microphone  array  and  convolved  with  two  anechoic  recordings.  Subsequently,  the  convolved  sound  sources  were  assigned  to  loudspeakers  (five  horizontal  channels  and  four  height  channels),  and  inter-channel  level  balances  were  optimized.  The  authors  conducted  a  controlled  listening  test  with  two  variables:  height-channel  configurations  (eight  conditions)  and  stimuli  (four  conditions--two  musical  selections  times  and  two  target  venues)  to  determine  the  influence  of  (1)  two  control  variables  on  the  perceived  appropriateness  of  virtual  auditory  environments  and  (2)  the  cultural  background  of  three  listener  groups  composed  of  participants  from  Canada  (group  1,  11  subjects),  the  USA  (group  2,  12  subjects),  and  Japan  (group  3,  14  subjects).  The  data  analysis  revealed  that  the  configuration  variable  (the  height  position  of  the  loudspeakers)  has  a  greater  influence  on  perceived  appropriateness  than  the  stimulus  variable  for  all  three  groups.  In  addition,  the  results  showed  that  although  group  1  data  had  a  similar  listening  response  pattern  to  group  2,  the  response  of  group  3  was  different.  A  subsequent  analysis  of  reported  descriptors  found  that  groups  1  and  2  chose  height  configurations  that  generated  a  "frontal"  and  "narrow"  impression  as  a  more  appropriate  virtual  auditory  environment,  while  group  3  chose  the  same  characteristics  but  as  a  less  appropriate  environment.  Groups  1  and  2  also  described  a  less  appropriate  auditory  environment  with  "wide,  spacious,  and  surrounding"  images  that  again  were  described  by  group  3  as  more  appropriate.  While  room  acoustics  and  loudspeaker  size  also  contributed  to  the  overall  modulation  of  listeners'  judgment,  the  findings  support  the  idea  that  cultural  background  affects  perceptual  responses  to  spatial  sound  and  is  therefore  important  in  rendering  a  homogeneous  experience  of  a  virtual  auditory  environment  for  listeners  in  remote  spaces.
2	Dancing  with  physio  a  mobile  game  with  physiologically  aware  virtual  humans.  This  study  presents  an  evaluation  of  a  mobile  game  with  physiologically  aware  virtual  humans  as  an  approach  to  modulate  the  participant's  affective  and  physiological  state.  We  developed  a  mobile  version  of  a  virtual  reality  scenario  where  the  participants  were  able  to  interact  with  virtual  human  characters  through  their  psychophysiological  activity.  Music  was  played  in  the  background  of  the  scenario  and,  depending  on  the  experimental  condition,  the  virtual  humans  were  initially  either  barely  dancing  or  dancing  very  euphorically.  The  task  of  the  participants  was  to  encourage  the  apathetic  virtual  humans  to  dance  or  to  calm  down  the  frenetically  dancing  characters,  through  the  modulation  of  their  own  mood  and  physiological  activity.  Results  from  our  study  show  that  by  using  this  mobile  game  with  the  physiologically  aware  and  affective  virtual  humans  the  participants  were  able  to  emotionally  arouse  themselves  in  the    Activation    condition  and  were  able  to  relax  themselves  in  the    Relaxation      condition,  during  the  same  session  with  only  a  brief  break  between  conditions.  The  self-reported  affective  data  was  also  corroborated  by  the  physiological  data  (heart  rate,  respiration  and  skin  conductance)  which  significantly  differed  between  the    Activation    and    Relaxation    conditions.
2	Improving  manual  tracking  of  systems  with  oscillatory  dynamics.  This  paper  examines  the  manual  control  of  systems  with  oscillatory  dynamics.  Tracking  performance  is  improved  by  using  input  shaping  to  suppress  command-induced  oscillation.  An  operator  study  tested  tracking  behavior  using  controlled  elements  with  both  low-frequency  (1.25  rad/s)  and  high-frequency  (5  rad/s)  oscillatory  modes.  After  each  experimental  trial,  measures  of  tracking  performance  and  subjective  task  difficulty  were  recorded,  and  frequency-domain  control  characteristics  were  computed.  Results  showed  that  the  high-frequency  oscillatory  mode  did  not  greatly  decrease  the  tracking  performance  from  the  nonoscillatory  case;  thus,  input  shaping  did  not  produce  a  significant  improvement  in  the  tracking  performance.  However,  input  shaping  did  cause  a  decrease  in  the  average  subjective  task  difficulty  and  made  the  system  closely  resemble  McRuer's  “crossover  model.”  For  the  low-frequency  case,  the  addition  of  input  shaping  significantly  improved  the  tracking  performance  and  reduced  the  tracking  difficulty.  These  results  demonstrate  that  input  shaping  can  greatly  improve  the  continuous  tracking  ability  of  human-machine  systems  that  have  oscillatory  modes.
2	Micro  and  macro  facial  expression  recognition  using  advanced  local  motion  patterns.  In  this  paper,  we  develop  a  new  method  that  recognizes  facial  expressions,  on  the  basis  of  an  innovative  Local  Motion  Patterns  (LMP)  feature.  The  LMP  feature  analyzes  locally  the  motion  distribution  in  order  to  separate  consistent  mouvement  patterns  from  noise.  Indeed,  facial  motion  extracted  from  the  face  is  generally  noisy  and  without  specific  processing,  it  can  hardly  cope  with  expression  recognition  requirements  especially  for  micro-expressions.  Direction  and  magnitude  statistical  profiles  are  jointly  analyzed  in  order  to  filter  out  noise.  This  work  presents  three  main  contributions.  The  first  one  is  the  analysis  of  the  face  skin  temporal  elasticity  and  face  deformations  during  expression.  The  second  one  is  a  unified  approach  for  both  macro  and  micro  expression  recognition  leading  the  way  to  supporting  a  wide  range  of  expression  intensities.  The  third  one  is  the  step  forward  towards  in-the-wild  expression  recognition,  dealing  with  challenges  such  as  various  intensity  and  various  expression  activation  patterns,  illumination  variations  and  small  head  pose  variations.  Our  method  outperforms  state-of-the-art  methods  for  micro  expression  recognition  and  positions  itself  among  top-ranked  state-of-the-art  methods  for  macro  expression  recognition.
2	A  novel  taxi  dispatch  system  for  smart  city.  Taxis  as  a  kind  of  public  transit  have  been  taken  by  citizens  thousands  of  times  every  day  in  urban  areas.  However,  it  is  economically  inefficient  for  vacant  taxis  to  randomly  cruise  around  to  seek  for  passengers.  In  this  paper,  we  propose  a  dynamic  taxi  dispatch  system  for  smart  city  which  dispatches  routes  with  high  probability  to  encounter  passengers  for  vacant  taxis.  In  the  system,  a  dynamic  probabilistic  model  has  been  established,  which  considers  the  impact  of  time  on  passenger  appearance  and  the  effect  of  different  vacant  taxis  traveling  route  on  each  other's  pick-up  probability.  Specifically,  a  novel  feedback  system  has  been  introduced  in  the  system,  which  utilizes  the  information  about  where  taxis  pick  up  passengers  to  amend  system  probabilistic  model.  Moreover,  extensive  trace-driven  simulations  based  on  real  digital  map  of  Shanghai  and  historical  data  of  over  2,000  taxis  demonstrate  the  good  performance  of  our  system.
2	Editorial  state  of  the  journal.  With  this  fourth  year  of  the  IEEE  Transactions  on  Affective  Computing  (TAC),  the  field  of  affective  computing  is  strong  and  vibrant.  In  2013  we  will  mark  the  fifth  International  Conferences  on  Affective  Computing  in  Geneva,  Switzerland,  which  will  be  cochaired  by  our  associate  editor,  Catherine  Pelachaud.  Over  the  last  year,  TAC  published  43  articles  over  four  issues,  up  from  26  articles  in  2011.  We  expect  to  maintain  this  publication  rate  over  the  next  year  and  focus  on  attracting  high-quality  articles.  The  journal  continues  to  encourage  interdisciplinary  research  and  we've  attracted  articles  from  recognized  names  in  both  the  computational  and  social  sciences  of  affect.  After  these  four  years  of  growth,  as  Editor-in-Chief  (EiC)  I  feel  confident  in  stating  that  TAC  is  the  premier  journal  for  research  on  the  topic  of  affective  computing.  The  editorial  board  has  remained  steady  over  the  last  year,  but  to  handle  our  increasing  paper  load  we  have  added  one  editor  focusing  on  human-robot  interaction.  I  welcome  Bilge  Mutlu  from  the  University  of  Wisconsin,  Madison,  who's  bio  and  photo  are  provided.  In  the  coming  year,  my  primary  goal  continues  to  be  to  increase  the  visibility  of  the  journal  and  for  this  I  need  your  help.  Please  help  me  in  spreading  awareness  of  the  journal.
2	The  evolution  of  ijhcs  and  chi  a  quantitative  analysis.  Abstract  In  this  paper  we  focus  on  the  International  Journal  of  Human-Computer  Studies  (IJHCS)  as  a  domain  of  analysis,  to  gain  insights  about  its  evolution  in  the  past  50  years  and  what  this  evolution  tells  us  about  the  research  landscape  associated  with  the  journal.  To  this  purpose  we  use  techniques  from  the  field  of  Science  of  Science  and  analyse  the  relevant  scholarly  data  to  identify  a  variety  of  phenomena,  including  significant  geopolitical  patterns,  the  key  trends  that  emerge  from  a  topic-centric  analysis,  and  the  insights  that  can  be  drawn  from  an  analysis  of  citation  data.  Because  the  area  of  Human-Computer  Interaction  (HCI)  has  always  been  a  central  focus  for  IJHCS,  we  also  include  in  the  analysis  the  CHI  conference,  which  is  the  premiere  scientific  venue  in  HCI.  Analysing  both  venues  provides  more  data  points  to  our  study  and  allows  us  to  consider  two  alternative  viewpoints  on  the  evolution  of  HCI  research.
2	Using  smartwatches  to  facilitate  a  group  dynamics  based  statewide  physical  activity  intervention.  Abstract  Physical  inactivity  is  a  major  cause  of  disease,  both  in  the  United  States  and  globally.  Physical  activity  interventions  often  use  a  multi-level  and  community-based  approach  combining  individual  and  group-based  behavioral  strategies  to  promote  physical  activity  and  influence  social  norms.  Such  interventions  can  increase  their  impact  by  adopting  technology-based  solutions  to  facilitate  the  underlying  behavioral  strategies.  Current  technologies  for  persuading  physical  activity  primarily  focus  on  facilitating  individual-level  behavioral  strategies  and  de-emphasizing  interpersonal  aspects.  This  article  focuses  on  the  design  and  evaluation  of  technology  aimed  at  facilitating  group  dynamics-based  strategies  for  promoting  physical  activity  within  small  socially-connected  teams.  This  work  introduces  a  multi-component  smartwatch-centered  system  called  FitAware  that  uses  sensors  to  automatically  track  physical  activity  and  leverage  the  advantages  of  the  watch  form  factor  to  facilitate  both  group  and  individual  level  behavioral  strategies  via  non-interruptive,  glanceable,  and  frequent  updates.  This  article  describes  the  design  and  evaluation  of  FitAware  in  the  context  of  an  8-week  statewide  physical  activity  community-based  intervention.
2	A  comparative  study  of  user  dependent  and  independent  accelerometer  based  gesture  recognition  algorithms.  In  this  paper,  we  introduce  an  evaluation  of  accelerometer-based  gesture  recognition  algorithms  in  user  dependent  and  independent  cases.  Gesture  recognition  has  many  algorithms  and  this  evaluation  includes  Hidden  Markov  Models,  Support  Vector  Machine,  K-nearest  neighbor,  Artificial  Neural  Net-work  and  Dynamic  Time  Warping.  Recognition  results  are  based  on  acceleration  data  collected  from  12  users.  We  evaluated  the  algorithms  based  on  the  recognition  accuracy  related  to  different  number  of  gestures  from  two  datasets.  Evaluation  results  show  that  the  best  accuracy  for  8  and  18  gestures  is  achieved  with  dynamic  time  warping  and  K-nearest  neighbor  algorithms.
2	Knock  knock  who  s  there  putting  the  user  in  control  of  managing  interruptions.  The  abundance  of  communication  technology,  such  as  the  omnipresence  of  cell  phones,  has  not  only  increased  our  ability  to  reach  people  anytime  anywhere,  but  also  the  likelihood  of  being  interrupted.  As  a  result,  there  is  value  in  understanding  how  to  design  technology  so  that  gains  are  realized  from  desired  interruptions,  while  the  losses  from  unwanted  interruptions  are  minimized.  This  paper  presents  the  findings  of  two  complementary  field  studies,  one  quantitative  and  the  other  qualitative,  exploring  how  the  provision  of  additional  incoming  cell  phone  call  information  impacts  people?s  interruption  decision  making.  These  studies  were  enabled  by,  Telling  Calls,  a  research  application  built  to  enable  users  to  provide  and  receive  information  such  as  what  the  call  is  about  and  the  caller?s  circumstances.  The  qualitative  study  showed  how  the  additional  call  information  helps  people  make  informed  call  handling  decisions  and  acts  as  an  aid  to  effective  conversation.  The  quantitative  study  elucidated  these  findings  and  showed  that  reducing  the  uncertainty  about  the  nature  of  an  incoming  call  improves  people?s  ability  to  predict  the  value  of  an  interruption.  By  combining  these  diverse  research  approaches:  (1)  theory  instantiation  through  tool  building;  (2)  context-aware  surveys;  and  (3)  semi-structured  interviews,  we  were  able  to  gain  unique  insights  into  the  nature  of  interruption  management  in  the  wild,  and  related  design  implications.  Presents  2  field  studies  of  people?s  interruption  handling  behavior  in  cell  phones.Explores  the  value  of  providing  additional  caller  and  call  related  information.Having  additional  information  enables  informed  call  handling  decisions.Provides  implications  for  design  of  interpersonal  interruption  management  tools.
2	Emotion  detection  from  touch  interactions  during  text  entry  on  smartphones.  There  are  different  modes  of  interaction  with  a  software  keyboard  on  a  smartphone,  such  as  typing  and  swyping.  Patterns  of  such  touch  interactions  on  a  keyboard  may  reflect  emotions  of  a  user.  Since  users  may  switch  between  different  touch  modalities  while  using  a  keyboard,  therefore,  automatic  detection  of  emotion  from  touch  patterns  must  consider  both  modalities  in  combination  to  detect  the  pattern.  In  this  paper,  we  focus  on  identifying  different  features  of  touch  interactions  with  a  smartphone  keyboard  that  lead  to  a  personalized  model  for  inferring  user  emotion.  Since  distinguishing  typing  and  swyping  activity  is  important  to  record  the  correct  features,  we  designed  a  technique  to  correctly  identify  the  modality.  The  ground  truth  labels  for  user  emotion  are  collected  directly  from  the  user  by  periodically  collecting  self-reports.  We  jointly  model  typing  and  swyping  features  and  correlate  them  with  user  provided  self-reports  to  build  a  personalized  machine  learning  model,  which  detects  four  emotion  states  (happy,  sad,  stressed,  relaxed).  We  combine  these  design  choices  into  an  Android  application  TouchSense  and  evaluate  the  same  in  a  3-week  in-the-wild  study  involving  22  participants.  Our  key  evaluation  results  and  post-study  participant  assessment  demonstrate  that  it  is  possible  to  predict  these  emotion  states  with  an  average  accuracy  (AUCROC)  of  73%  (std  dev.  6%,  maximum  87%)  combining  these  two  touch  interactions  only.
2	Human  values  and  digital  citizen  science  interactions.  Abstract  Sustained  participation  is  critical  to  the  success  of  digital  citizen-science  initiatives,  yet  much  of  the  current  literature  focuses  on  mapping  people’s  motives  to  engage  without  considering  the  extent  to  which  participation  is  sustained  over  time.  We  conducted  a  year-long  experimental  study  (  n  =  85  )  “in-thewild”  to  explore  the  effects  of  human-value  orientations  on  the  use  of  digital  citizen-science  tools.  Participants  took  part  in  both  the  co-design  and  use  of  digital  citizen-science  tools  in  Lappeenranta,  Finland  from  2018–2019.  Our  statistical  analysis  finds  evidence  of  relations  between  value  orientations,  sustained  participation,  and  the  number  and  quality  of  digital  interactions.  Specifically,  we  find  that  value  orientations  are  linked  with  different  usage  patterns.  For  instance,  people  with  a  stronger  openness-to-change  (OTC)  values  tended  to  use  the  mobile  application  to  check  others’  submissions,  even  when  they  had  nothing  to  submit,  whereas  people  with  stronger  security  values  mostly  used  the  application  when  they  had  something  relevant  to  submit.  Further  understanding  the  influence  of  human  values  in  digital  citizen  science  is  a  promising  area  for  future  research  that  could  contribute  to  a)  guide  the  design  of  incentive  mechanisms,  b)  understand  user  experiences  in  online  communities,  and  c)  inform  the  design  and  evaluation  of  digital  citizen-science  technologies.
2	Strengthening  gamification  studies  current  trends  and  future  opportunities  of  gamification  research.  Abstract  Gamification  is  now  a  well-established  technique  in  Human-Computer  Interaction.  However,  research  on  gamification  still  faces  a  variety  of  empirical  and  theoretical  challenges.  Firstly,  studies  of  gamified  systems  typically  focus  narrowly  on  understanding  individuals.  short-term  interactions  with  the  system,  ignoring  more  difficult  to  measure  outcomes.  Secondly,  academic  research  on  gamification  has  been  slow  to  improve  the  techniques  through  which  gamified  applications  are  designed.  Third,  current  gamification  research  lacks  a  critical  lens  capable  of  exploring  unintended  consequences  of  designs.  The  14  articles  published  in  this  special  issue  face  these  challenges  with  great  methodological  rigor.  We  summarize  them  by  identifying  three  main  themes:  the  determination  to  improve  the  quality  and  usefulness  of  theory  in  the  field  of  gamification,  the  improvements  in  design  practice,  and  the  adoption  of  a  critical  gaze  to  uncover  side-effects  of  gamification  designs.  We  conclude  by  providing  an  overview  of  the  questions  that  we  feel  must  be  addressed  by  future  work  in  gamification.  Gamification  studies  would  benefit  from  a  wider  use  of  theories  to  account  for  the  complexity  of  human  behavior,  a  more  thorough  exploration  of  the  many  opportunities  coming  from  the  world  of  games,  and  an  ethical  reflection  on  the  use  of  game  design  elements  in  serious  domains.
2	Twitter  the  best  of  bot  worlds  for  automated  wit.  Language  affords  a  great  many  opportunities  for  the  intelligent  reuse  of  linguistic  content.  Rather  than  always  putting  our  own  thoughts  into  our  own  words,  we  often  convey  feelings  through  the  words  of  others,  by  citing,  quoting,  mimicking,  borrowing,  varying  or  ironically  echoing  what  others  have  already  said.  Social  networking  platforms  such  as  Twitter  elevate  linguistic  reuse  into  an  integral  norm  of  digital  interaction.  On  such  platforms,  who  you  follow  and  what  you  re-tweet  can  say  as  much  about  you  as  the  clothes  you  wear  or  the  art  you  hang  on  your  walls.  But  not  everyone  that  is  worth  following  is  human,  and  not  everything  that  is  worth  re-tweeting  was  first  coined  by  a  real  person.  More  and  more  of  the  witty  and  thought-provoking  content  on  Twitter  is  generated  by  bots,  artificial  systems  that  write  their  own  material  and  vie  for  our  attention  just  as  humans  do.  Real  people  knowingly  follow  artificial  bots  for  reasons  that  are  subtle  and  diverse,  but  a  significant  reason  is  surely  Twitter  itself.  This  paper  explores  Twitter  as  a  smart  environment  for  automated  wit,  and  describes  the  mechanics  of  a  wittily  inventive  new  Twitterbot  named  @MetaphorMagnet.
2	Human  like  rewards  to  train  a  reinforcement  learning  controller  for  planar  arm  movement.  High-level  spinal  cord  injury  (SCI)  in  humans  causes  paralysis  below  the  neck.  Functional  electrical  stimulation  (FES)  technology  applies  electrical  current  to  nerves  and  muscles  to  restore  movement,  and  controllers  for  upper  extremity  FES  neuroprostheses  calculate  stimulation  patterns  to  produce  desired  arm  movement.  However,  currently  available  FES  controllers  have  yet  to  restore  natural  movements.  Reinforcement  learning  (RL)  is  a  reward-driven  control  technique;  it  can  employ  user-generated  rewards,  and  human  preferences  can  be  used  in  training.  To  test  this  concept  with  FES,  we  conducted  simulation  experiments  using  computer-generated  “pseudo-human”  rewards.  Rewards  with  varying  properties  were  used  with  an  actor-critic  RL  controller  for  a  planar  two-degree-of-freedom  biomechanical  human  arm  model  performing  reaching  movements.  Results  demonstrate  that  sparse,  delayed  pseudo-human  rewards  permit  stable  and  effective  RL  controller  learning.  The  frequency  of  reward  is  proportional  to  learning  success,  and  human-scale  sparse  rewards  permit  greater  learning  than  exclusively  automated  rewards.  Diversity  of  training  task  sets  did  not  affect  learning.  Long-term  stability  of  trained  controllers  was  observed.  Using  human-generated  rewards  to  train  RL  controllers  for  upper-extremity  FES  systems  may  be  useful.  Our  findings  represent  progress  toward  achieving  human–machine  teaming  in  control  of  upper-extremity  FES  systems  for  more  natural  arm  movements  based  on  human  user  preferences  and  RL  algorithm  learning  capabilities.
2	To  move  or  not  to  move  analyzing  motion  cueing  in  vehicle  simulators  by  means  of  massive  simulations.  Motion  platforms  and  motion  cueing  algorithms  (MCA)  have  been  included  in  virtual  reality  applications  for  several  decades.  They  are  necessary  to  provide  suitable  inertial  cues  in  vehicle  simulators.  However,  the  great  number  of  operational  constraints  that  these  devices  and  algorithms  suffer,  namely  limited  physical  space,  elevated  costs,  absence  of  sufficient  power,  difficulty  of  tuning  and  lack  of  standardized  assessment  methods,  have  hindered  their  widespread  use.  This  work  tries  to  clarify  open  questions  in  the  field,  such  as:  How  important  is  MCA  tuning?  How  much  does  size,  number  of  DOF  and  power/latency  matter?  Can  the  absence  of  motion  be  better  than  poor  motion  cueing?  What  are  the  key  factors  that  should  be  addressed  to  enhance  the  design  of  these  devices?  Although  absolute  certain  answers  cannot  be  given,  this  paper  tries  to  clarify  these  research  questions  by  performing  massive  experiments  with  simulated  motion  platforms  of  different  types,  sizes  and  powers.  The  information  obtained  from  these  experiments  will  be  important  to  customize  the  design  of  real  devices  for  this  particular  use.  Ideally,  subjective  experiments  with  human  experts  would  have  been  preferred.  However,  the  use  of  simulated  devices  allows  comparing  many  different  motion  platforms.  In  this  paper,  forty  of  these  devices  are  simulated,  optimized  by  means  of  a  heuristic  algorithm  and  compared  with  objective  indicators  in  order  to  measure  their  relative  performance  using  the  classical  MCA,  something  that  would  require  an  unreasonable  amount  of  effort  with  real  users  and  real  devices.  The  obtained  results  show  that  MCA  tuning  is  of  the  utmost  importance  in  motion  cueing.  They  also  suggest  that  high  power  can  usually  compensate  for  lack  of  size  and  that  a  6-DOF  motion  platform  slightly  improves  the  performance  of  a  3-DOF  motion  platform.
2	An  evaluation  of  the  game  changer  password  system  a  new  approach  to  password  security.  Abstract      We  propose  –  and  experimentally  test  –  a  mnemonic  variant  of  password  security  that  uses  game  positions  as  passwords.  In  Experiment  1,  we  report  accuracy  and  reaction  time  data  when  high  school  student,  younger  adult,  and  older  adult  participants  remembered  and  entered  one  game-based  password,  using  chess  or  Monopoly.  In  Experiment  2,  we  report  accuracy  and  reaction  time  data  from  participants'  use  of  five  game-based  passwords  across  24  sessions  over  10  weeks.  All  five  passwords  were  stored  in  chess  or  Monopoly  for  the  initial  20  sessions,  and  changed  (from  chess  to  Monopoly  or  vice  versa)  for  the  remaining  sessions.  This  new  approach  to  password  security  is  both  mathematically  robust  and  user-friendly.
2	What  s  in  it  for  me  self  serving  versus  other  oriented  framing  in  messages  advocating  use  of  prosocial  peer  to  peer  services.  Abstract  We  present  a  study  that  investigates  the  effectiveness  of  self-serving  versus  other-oriented  motivational  framing  of  messages  designed  to  persuade  people  to  sign  up  for  a  prosocial  peer-to-peer  (P2P)  service.  As  part  of  the  study,  volunteer  message  senders  were  incentivized  to  recruit  people  to  sign  up  for  one  of  three  types  of  prosocial  P2P  services.  Senders  were  given  an  option  of  choosing  one  of  four  pre-designed  invitation  messages  to  send  to  their  contacts,  two  framed  for  self-serving  motivations  and  two  framed  for  other-oriented  motivations.  We  found  that  recipients  were  more  attracted  to  click  on  messages  emphasizing  self-serving  benefits.  This  may  not  match  the  expectation  of  senders,  who  generally  prioritized  other-oriented  motives  for  participating  in  prosocial  P2P  services.  However,  after  recipients  clicked  the  messages  to  investigate  further,  effects  of  self  versus  other-framing  messages  depended  on  the  nature  of  the  service.  Our  findings  suggest  that,  even  for  prosocial  services,  messages  offering  self-serving  motivations  are  more  effective  than  altruistic  ones  on  inspiring  interests.  But  the  overall  persuasive  effect  on  conversion  may  be  more  nuanced,  where  the  persuasion  context  (service  type)  appears  to  be  a  critical  moderator.
2	Dynamic  time  warping  for  music  retrieval  using  time  series  modeling  of  musical  emotions.  Musical  signals  have  rich  temporal  information  not  only  at  the  physical  level  but  at  the  emotion  level.  The  listeners  may  wish  to  find  music  excerpts  that  have  similar  sequence  patterns  of  musical  emotions  with  given  excerpts.  Most  state-of-the-art  systems  for  emotion-based  music  retrieval  concentrate  on  static  analysis  of  musical  emotions,  and  ignore  dynamic  analysis  and  modeling  of  musical  emotions  over  time.  This  paper  presents  a  novel  approach  to  perform  music  retrieval  based  on  time-varying  musical  emotion  dynamics.  A  three-dimensional  musical  emotion  model—Resonance-Arousal-Valence  (RAV)—is  used,  and  emotions  of  a  piece  of  music  are  represented  by  musical  emotion  dynamics  in  a  time  series.  A  multiple  dynamic  textures  (MDT)  model  is  proposed  to  model  music  and  emotion  dynamics  over  time,  and  expectation  maximization  (EM)  algorithm  along  with  Kalman  filtering  and  smoothing  is  used  to  estimate  model  parameters.  Two  smoothing  methods—Rauch-Tung-Striebel  (RTS)  and  minimum-variance  smoothing  (MVS)—to  robust  model  are  investigated  and  compared  to  find  an  optimal  solution  to  enhance  prediction.  To  find  similar  sequence  patterns  of  musical  emotions,  subsequence  dynamic  time  warping  (DTW)  for  emotion  dynamics  matching  is  presented.  Experimental  results  demonstrate  the  benefits  of  MDT  to  predict  time-varying  musical  emotions,  and  our  proposed  method  for  music  retrieval  based  on  emotion  dynamics  outperforms  retrieval  methods  based  on  acoustic  features.
2	The  effect  of  applied  normal  force  on  the  electrovibration.  Electrovibration  has  become  one  of  the  promising  approaches  for  adding  tactile  feedback  on  touchscreen.  Previous  studies  revealed  that  the  normal  force  applied  on  the  touchscreen  by  the  finger  affects  significantly  the  electrostatic  force.  It  is  obvious  that  the  normal  force  affects  the  electrostatic  force  if  it  changes  the  contact  area  between  the  finger  and  the  touchscreen.  However,  it  is  unclear  whether  the  normal  force  affects  the  electrostatic  force  when  the  apparent  contact  area  is  constant.  In  this  paper,  we  estimated  the  electrostatic  force  via  measuring  the  tangential  force  of  the  finger  sliding  on  a  3M  touchscreen  at  different  normal  forces  under  the  constant  apparent  contact  area.  We  found  that  the  electrostatic  force  increases  significantly  as  the  normal  force  increases  from  0.5  to  4.5N.  We  explained  the  experimental  results  using  the  most  recently  proposed  electrostatic  force  model,  which  considers  the  effect  of  air  gap.  We  estimated  the  averaged  air  gap  thickness  using  the  electrostatic  force  model.  The  results  showed  that  the  relationship  between  the  air  gap  thickness  and  the  normal  force  follows  a  power  function.  Our  experiment  suggests  that  the  normal  force  has  a  significant  effect  on  the  air  gap  thickness,  thus  require  consideration  in  the  design  of  tactile  feedback.
2	Mental  resource  demands  prediction  as  a  key  element  for  future  assistant  systems  in  military  helicopters.  This  work  presents  an  approach  to  enhance  knowledge-based  assistant  systems  in  the  domain  of  military  helicopter  missions  with  the  ability  to  prevent  the  pilot  from  being  overtaxed.  Therefore,  an  estimation  method  for  residual  mental  capacity  and  current  subjective  workload  is  proposed.  This  estimation  enables  the  assistant  system  to  deduce  the  pilots'  specific  needs  of  support.  As  a  result  the  assistant  system  shall  be  enabled  to  cooperate  with  the  pilot  by  resource  adaptive  information  exchange.  First  evaluation  experiments  of  the  prototype,  conducted  in  our  research  helicopter  mission  simulator,  will  be  described.
2	How  can  a  future  safety  net  successfully  detect  conflicting  atc  clearances  yet  remain  inconspicuous  to  the  tower  runway  controller  first  results  from  a  sesar  exercise  at  hamburg  airport.  To  increase  runway  safety  a  new  safety  net  for  Tower  Runway  Controllers  was  developed  which  detects  if  controllers  give  a  clearance  to  an  aircraft  or  vehicle  contradictory  to  another  clearance  already  given  to  another  mobile.  In  a  shadow  mode  validation  exercise  with  eleven  controllers  at  the  operational  environment  of  the  airport  Hamburg  (Germany)  operational  feasibility  was  tested  in  order  to  clarify  if  operational  requirements  in  terms  of  usability  are  fulfilled.  At  the  same  time  operational  improvements  regarding  safety  were  studied  e.g.  if  the  new  safety  net  detects  all  conflicts  and  if  nuisance  alerts  are  suppressed.
2	Rendered  and  characterized  closed  loop  accuracy  of  impedance  type  haptic  displays.  Impedance-type  kinesthetic  haptic  displays  aim  to  render  arbitrary  desired  dynamics  to  a  human  operator  using  force  feedback.  To  render  realistic  virtual  environments,  the  difference  between  desired  and  rendered  dynamics  must  be  small.  In  this  paper,  we  analyze  the  closed-loop  dynamics  of  haptic  displays  for  three  common  virtual  environments:  a  spring,  a  damper,  and  a  spring-damper,  including  the  effects  of  time  delay  and  low-pass  filtering.  Using  a  linear  model,  we  identify  important  parameters  for  the  rendered  dynamics  in  terms  of  effective  impedances,  a  conceptual  tool  that  decomposes  the  displays  closed-loop  impedance  into  components  with  physical  analogs.  Our  results  establish  bandwidth  limits  for  rendering  effective  stiffness  and  damping.  The  effective  stiffness  bandwidth  is  limited  by  the  virtual  stiffness  and  device  mass,  and  the  effective  damping  bandwidth  is  limited  by  the  cut-off  frequency  of  the  low-pass  filter  which  filters  the  device  velocity  estimate.  We  show  that  a  general  system  impedance  can  be  characterized  by  a  mass,  damper,  and  spring  optimally  by  the  solution  to  a  convex  optimization  problem,  and  we  present  a  quantitative  metric,  the  Average  Distortion  Error  (ADE),  to  describe  the  fidelity  of  this  model.  Time  delay  has  no  significant  effect  on  characterized  stiffness,  and  reduces  characterized  damping  by  the  product  of  virtual  stiffness  and  total  time  delay.  Reducing  the  low-pass  filter  cut-off  frequency  reduces  the  characterized  damping.  Experimental  data  gathered  with  a  Phantom  Premium  1.5  validates  the  theoretical  analysis.  We  also  conducted  human  user  experiments  to  investigate  the  effects  of  time  delay  and  low-pass  filtering  on  perceived  stiffness  and  damping.  Similar  to  the  characterized  dynamics  results,  we  observed  no  significant  effect  of  time  delay  on  perceived  stiffness,  and  increasing  time  delay  resulted  in  reduced  perceived  damping.  Lower  filter  cut-off  frequencies  resulted  in  lower  perceived  damping.  This  work  informs  haptic  display  design  by  presenting  how  closed-loop  behavior  changes  with  key  parameters.
2	Midair  haptic  pursuit.  In  human  vision,  smooth  pursuit  eye  movement  is  the  basic  ability  to  visually  follow  a  moving  object  by  keeping  it  at  the  sight  center.  In  this  study,  we  validate  that  a  human  hand  has  a  similar  ability  to  track  a  midair  haptic  stimulus,  i.e.,  a  human  palm  exposed  to  a  point  vibration  by  a  noncontact  ultrasound  tactile  display  can  follow  the  continuous  movement  of  the  stimulation  point.  The  experimental  results  show  that  the  trackable  velocity  limit  is  10  cm/s  for  motion  parallel  to  the  palm,  when  the  initial  velocity  is  zero.  This  ability  of  motion  tracking  by  hand  can  be  applied  to  haptic  guidance  for  visually  impaired  people  or  for  evacuation  navigation,  where  no  devices  are  needed  to  be  equipped  by  users.
2	Haptic  guidance  on  demand  a  grip  force  based  scheduling  of  guidance  forces.  In  haptic  shared  control  systems  (HSC),  a  fixed  strength  of  guidance  force  equates  to  a  fixed  level  of  control  authority,  which  can  be  insufficient  for  complex  tasks.  An  adaptable  control  authority  based  on  operator  input  can  allow  the  HSC  system  to  better  assist  the  operator  under  varied  conditions.  In  this  paper,  we  experimentally  investigate  (  $n  =  8$  )  an  adaptable  authority  HSC  system  that  provides  the  operator  with  a  direct  way  to  adjust  the  control  authority  based  on  applied  grip  force.  This  system  can  serve  as  an  intuitive  ‘manual  override’  function  in  case  of  HSC  system  malfunction.  In  a  position  tracking  task,  we  explore  two  opposite  approaches  to  adapt  the  control  authority:  increasing  versus  decreasing  guidance  strength  with  operator  grip.  These  approaches  were  compared  with  unassisted  control  and  two  levels  of  fixed-level  haptic  guidance.  Results  show  that  the  grip-adaptable  approach  allowed  the  operators  to  increase  performance  over  unassisted  control  and  over  a  weak  guidance.  At  the  same  time,  the  approach  substantially  reduced  the  operator  physical  control  effort  required  to  cope  with  HSC  system  disturbances.  Predictions  based  on  the  formalized  model  of  the  complete  human-in-the-loop  system  corresponded  to  the  experimental  results,  implying  that  such  validated  formalization  can  be  used  for  model-based  analysis  and  design  of  guidance  systems.
2	Haptic  orientation  guidance  using  two  parallel  double  gimbal  control  moment  gyroscopes.  This  paper  presents  a  system  of  two  double-gimbal  control  moment  gyroscopes  (CMGs)  for  providing  ungrounded  kinesthetic  haptic  feedback.  By  spinning  a  second  flywheel  opposite  the  first,  and  rotating  them  through  opposite  trajectories,  undesired  gyroscopic  effects  can  be  eliminated,  isolating  a  single  torque  axis.  This  produces  a  moment  pulse  proportional  to  the  flywheel  spin  speed  and  rotation  speed.  Rotating  the  CMG  gimbals  quickly  in  one  direction,  then  resetting  them  more  slowly  generates  repeated  torque  pulses  indicating  a  clear  direction  cue.  We  present  the  mathematical  model  for  moments  produced  by  this  system  and  verify  that  the  performance  of  our  device  matches  this  model.  Using  these  asymmetric  moment  pulses,  we  provide  haptic  cues  to  participants  in  two  studies.  In  the  first  study,  users  simply  identify  the  direction  of  torque  cues.  In  the  second  study,  we  use  the  torque  pulses  to  guide  users  to  target  orientations.  Performance  in  both  studies  shows  that  this  system  has  the  potential  to  provide  useful  guidance  for  applications  where  ungrounded  haptic  feedback  is  desired.
2	An  approach  to  optimal  text  placement  on  images.  In  deciding  where  to  place  a  text  block  on  an  image,  there  are  two  major  factors:  aesthetic  of  the  design  composition,  and  the  visual  attention  that  the  text  block  naturally  attracts.  We  propose  a  computational  model  to  address  this  problem  based  on  the  principles  of  visual  balance  and  the  diagonal  method  of  placing  emphasis.  A  between-subject  study  with  seven  participants  was  conducted  to  validate  our  model  with  subjective  ratings.  Eight  color  photographs  were  used  to  generate  a  set  of  text-overlaid  images  as  the  stimuli.  Participants  rated  the  stimuli  for  aesthetic  appeal  on  a  seven-point  likert  scale.  Results  show  that  the  participants  preferred  text-overlaid  images  generated  by  our  method  of  text  placement  over  random  text  placement.
2	Experimental  assessment  of  absolute  stability  in  bilateral  teleoperation.  Absolute  stability  analysis  of  bilateral  teleoperation  systems  are  typically  model-based.  Under  borderline  conditions  of  absolute  stability,  depending  on  the  degree  of  uncertainty  in  the  dynamic  model  of  the  teleoperator  and  existing  noise,  the  system  may  behave  as  potentially  unstable  when  the  model-based  analysis  predicts  otherwise.  In  this  article,  we  propose  a  methodology  to  experimentally  verify  the  absolute  stability  of  master-slave  teleoperation  systems.  Since  absolute  stability  demands  bounds  of  all  possible  environments,  we  achieve  this  by  conducting  only  three  experiments  that  are  often  experienced  in  teleoperation:  free  slave,  mass-carrying  slave  and  locked  slave  (rigid  environment).  We  will  validate  and  compare  our  proposed  method  with  the  benchmark  Llewellyns  absolute  stability  criterion.  Furthermore,  we  will  examine  the  robustness  of  the  proposed  method  and  will  provide  guidelines  for  choosing  the  mass  for  the  mass-carrying  load  condition.
2	Mid  air  tactile  feedback  co  located  with  virtual  touchscreen  improves  dual  task  performance.  The  use  of  haptic  technology  has  recently  become  essential  in  Human-Computer  Interaction  to  improve  performance  and  user  experience.  Mid-air  tactile  feedback  co-located  with  virtual  touchscreen  displays  have  a  great  potential  to  improve  the  performance  in  dual-task  situations,  such  as  when  using  a  phone  while  walking  or  driving.  The  purpose  of  this  article  is  to  investigate  the  effects  of  augmenting  virtual  touchscreen  with  mid-air  tactile  feedback  to  improve  dual-task  performance  where  the  primary  task  is  driving  in  a  simulation  environment  and  the  secondary  task  involves  interacting  with  a  virtual  touchscreen.  Performance  metrics  included  primary  task  performance  in  terms  of  velocity  error,  deviation  from  the  middle  of  the  road,  number  of  collisions,  and  the  number  of  off-road  glances,  secondary  task  performance  including  the  interaction  time  and  the  reach  time,  and  quality  of  user  experience  for  perceived  difficulty  and  satisfaction.  Results  demonstrate  that  adding  mid-air  tactile  feedback  to  virtual  touchscreen  resulted  in  statistically  significant  improvement  in  the  primary  task  performance  (the  average  speed  error,  spatial  deviation,  and  the  number  of  off-road  glances),  the  secondary  task  (reach  time),  and  the  perceived  difficulty.  These  results  provide  a  great  motivation  for  augmenting  virtual  touchscreens  with  mid-air  tactile  feedback  in  dual-task  human-computer  interaction  applications.
2	Interactive  transfer  function  design  on  large  multiresolution  volumes.  Interactive  transfer  function  design  techniques  seek  to  leverage  user  knowledge  to  facilitate  the  discovery  of  data  salience.  In  this  process,  interactive  volume  rendering  is  typically  a  necessity.  Interactive  volume  rendering  of  large-scale  data  on  workstations  is  often  accomplished  through  the  use  of  level  of  detail  techniques,  prioritizing  information  deemed  to  be  salient  over  information  deemed  to  be  unimportant.  If  salience  is  not  known  a  priori,  and  interactive  transfer  function  design  techniques  that  depend  on  volume  rendering  are  to  be  applied  to  large-scale  data  using  level  of  detail  selection,  then  there  is  a  cyclic  dependency.  Techniques  must  be  applied  that  can  support  simultaneous  development  of  salience  both  for  the  transfer  function  design  technique  and  the  level  of  detail  selection  technique.  Building  on  recent  work  in  LOD  selection,  we  propose  an  interactive  transfer  function  design  technique  that  enables  incremental  salience  discovery  to  support  simultaneous  construction  of  transfer  functions  and  LOD  selections  on  large-scale  data.
2	10  000  gold  for  20  dollars  an  exploratory  study  of  world  of  warcraft  gold  buyers.  Buying  virtual  currencies  with  real  money  from  a  third-party  often  violates  the  terms  of  use  of  online  games.  This  study  quantitatively  investigates  players  who  buy  in-game  gold  from  a  third-party  in  World  of  Warcraft.  A  cross-cultural  survey  dataset  of  2865  players  reveals  that  differences  between  Asian  and  Western  players  are  negligible  compared  to  differences  across  genders,  job  categories,  and  play  motivations.  Our  findings  have  implications  for  the  design  and  study  of  interactions  between  players  and  virtual  currencies.
2	Data  intensive  analysis  for  scientific  experiments  at  the  large  scale  data  facility.  The  Large  Scale  Data  Facility  (LSDF)  was  conceived  and  launched  at  the  Karlsruhe  Institute  of  Technology  (KIT)  end  of  2009  to  address  the  growing  need  for  value-added  storage  services  for  data  intensive  experiments.  The  LSDF  main  focus  is  to  support  scientific  experiments  producing  large  data  sets  reaching  into  the  petabyte  range  with  adequate  storage,  support  and  value  added  services  for  data  management,  processing  and  preservation.  In  this  work  we  describe  the  approach  taken  to  perform  data  analysis  in  LSDF,  as  well  as  for  data  management  of  the  scientific  datasets.
2	Deep  unsupervised  multi  view  detection  of  video  game  stream  highlights.  We  consider  the  problem  of  automatic  highlight-detection  in  video  game  streams.  Currently,  the  vast  majority  of  highlight-detection  systems  for  games  are  triggered  by  the  occurrence  of  hard-coded  game  events  (e.g.,  score  change,  end-game),  while  most  advanced  tools  and  techniques  are  based  on  detection  of  highlights  via  visual  analysis  of  game  footage.  We  argue  that  in  the  context  of  game  streaming,  events  that  may  constitute  highlights  are  not  only  dependent  on  game  footage,  but  also  on  social  signals  that  are  conveyed  by  the  streamer  during  the  play  session  (e.g.,  when  interacting  with  viewers,  or  when  commenting  and  reacting  to  the  game).  In  this  light,  we  present  a  multi-view  unsupervised  deep  learning  methodology  for  novelty-based  highlight  detection.  The  method  jointly  analyses  both  game  footage  and  social  signals  such  as  the  players  facial  expressions  and  speech,  and  shows  promising  results  for  generating  highlights  on  streams  of  popular  games  such  as  Player  Unknown's  Battlegrounds.
2	Programming  in  game  space  how  to  represent  parallel  programming  concepts  in  an  educational  game.  Concurrent  and  parallel  programming  (CPP)  skills  are  increasingly  important  in  today's  world  of  parallel  hardware.  However,  the  conceptual  leap  from  deterministic  sequential  programming  to  CPP  is  notoriously  challenging  to  make.  Our  educational  game  Parallel  is  designed  to  support  the  learning  of  CPP  core  concepts  through  a  game-based  learning  approach,  focusing  on  the  connection  between  gameplay  and  CPP.  Through  a  10-week  user  study  (n  25)  in  an  undergraduate  concurrent  programming  course,  the  first  empirical  study  for  a  CPP  educational  game,  our  results  show  that  Parallel  offers  both  CPP  knowledge  and  student  engagement.  Furthermore,  we  provide  a  new  framework  to  describe  the  design  space  for  programming  games  in  general.
2	Virtual  character  behavior  architecture  using  cyclic  scheduling.  A  story-based  video  game  contains  many  characters.  The  majority  are  virtual  characters  controlled  by  artificial  intelligence.  In  recent  years,  virtual  character  artificial  intelligence  has  developed  slower  than  other  aspects  of  video  games,  such  as  graphics,  mainly  due  to  the  cost  of  scripting  complex  and  believable  virtual  characters.  To  tackle  this  bottleneck  in  content  creation,  this  research  proposes  a  new  Tiered  Behavior  Architecture  model  for  controlling  the  behaviors  of  virtual  characters.  For  local  scenes,  techniques  such  as  Behavior  Capture  with  Hidden  Markov  Models,  which  has  been  evaluated  by  user  studies  that  validated  its  success  in  generating  fine-grained  behaviors,  can  be  used  to  fulfill  the  roles.  At  a  larger  scale,  a  hierarchical  cyclic  scheduler  determines  the  general  circumstances,  schedules,  and  objectives  of  the  virtual  characters  as  well  as  the  roles  that  will  accomplish  these  objectives.  This  paper  describes  experiments  and  user  studies  that  validate  this  model.
2	Effects  of  sharing  real  time  multi  sensory  heart  rate  feedback  in  different  immersive  collaborative  virtual  environments.  Collaboration  is  an  important  application  area  for  virtual  reality  (VR).  However,  unlike  in  the  real  world,  collaboration  in  VR  misses  important  empathetic  cues  that  can  make  collaborators  aware  of  each  other’s  emotional  states.  Providing  physiological  feedback,  such  as  heart  rate  or  respiration  rate,  to  users  in  VR  has  been  shown  to  create  a  positive  impact  in  single  user  environments.  In  this  paper,  through  a  rigorous  mixed-factorial  user  experiment,  we  evaluated  how  providing  heart  rate  feedback  to  collaborators  influences  their  collaboration  in  three  different  environments  requiring  different  kinds  of  collaboration.  We  have  found  that  when  provided  with  real-time  heart  rate  feedback  participants  felt  the  presence  of  the  collaborator  more  and  felt  that  they  understood  their  collaborator’s  emotional  state  more.  Heart  rate  feedback  also  made  participants  feel  more  dominant  when  performing  the  task.  We  discuss  the  implication  of  this  research  for  collaborative  VR  environments,  provide  design  guidelines,  and  directions  for  future  research.
2	Seamless  interaction  using  a  portable  projector  in  perspective  corrected  multi  display  environments.  In  this  work,  we  study  ways  to  use  a  portable  projector  to  extend  the  workspace  in  a  perspective  corrected  multi  display  environment  (MDE).  This  system  uses  the  relative  position  between  the  user  and  displays  in  order  to  show  the  content  perpendicularly  to  the  user's  point  of  view  in  a  deformation-free  fashion.  We  introduce  the  image  created  by  the  portable  projector  as  a  new,  temporary  and  movable  image  in  the  perspective  corrected  MDE,  creating  a  more  flexible  workspace  to  the  user.  In  our  study,  we  combined  two  ways  of  using  the  projector  (handheld  or  head-mounted)  with  two  ways  of  moving  the  cursor  on  the  screens  (using  a  mouse  or  a  laser-pointing  based  strategy),  proposing  four  techniques  to  be  tried  by  the  users.  Also,  two  exploratory  evaluation  experiments  were  performed  in  order  to  evaluate  our  system.  The  first  experiment  (5  participants)  aimed  to  evaluate  how  using  a  movable  screen  in  order  to  fill  the  gaps  between  displays  affects  the  performance  of  the  user  in  a  cross-display  pointing  task;  while  the  second  (6  participants)  aimed  to  evaluate  how  using  the  projector  to  extend  the  workspace  impacts  the  task  completion  time  in  an  off-screen  content  recognition  task.  Our  results  showed  that  while  no  significant  improvement  of  the  performance  of  the  users  could  be  seen  on  the  pointing  task,  the  users  were  significantly  faster  when  recognizing  off-screen  content.  Also,  the  introduction  of  the  portable  projector  reduced  the  overall  task  load  on  both  tasks.
2	Stakeholders  as  partners  making  aac  work  better.  Abstract  Challenges  associated  with  the  provision  of  AAC  treatment  can  be  daunting,  especially  in  light  of  the  intense  training  requirements  of  many  populations.  Stakeholder  training  can  optimize  t...
2	Demo  on  site  augmented  collaborative  architecture  visualization.  The  early  design  phase  for  a  new  building  is  a  crucial  stage  in  the  design  process  of  architects.  It  has  to  be  ensured  that  the  building  fits  into  the  future  environment.  The  Collaborative  Design  Platform  targets  this  issue  by  integrating  modern  digital  means  with  well  known  traditional  concepts.  Well-used  styrofoam  blocks  are  still  cut  by  hand  but  are  now  tracked,  placed  and  visualized  in  3D  by  use  of  a  tabletop  platform  and  a  TV  screen  showing  an  arbitrary  view  of  the  scenery.  With  this  demonstration,  we  get  one  step  further  and  provide  an  interactive  visualization  at  the  proposed  building  site,  further  enhancing  collaboration  between  different  audiences.  Mobile  phones  and  tablet  devices  are  used  to  visualize  marker-less  registered  virtual  building  structures  and  immediately  show  changes  made  to  the  models  in  the  Collaborative  Design  laboratory.  This  way,  architects  can  get  a  direct  impression  about  how  a  building  will  integrate  within  the  environment  and  residents  can  get  an  early  impression  about  future  plans.
2	Poster  augmented  reality  for  radiation  awareness.  C-arm  fluoroscopes  are  frequently  used  during  surgeries  for  intraoperative  guidance.  Unfortunately,  due  to  X-ray  emission  and  scattering,  increased  radiation  exposure  occurs  in  the  operating  theatre.  The  objective  of  this  work  is  to  sensitize  the  surgeon  to  their  radiation  exposure,  enable  them  to  check  on  their  exposure  over  time,  and  to  help  them  choose  their  best  position  related  to  the  C-arm  gantry  during  surgery.  First,  we  aim  at  simulating  the  amount  of  radiation  that  reaches  the  surgeon  using  the  Geant4  software,  a  toolkit  developed  by  CERN.  Using  a  flexible  setup  in  which  two  RGB-D  cameras  are  mounted  to  the  mobile  C-arm,  the  scene  is  captured  and  modeled  respectively.  After  the  simulation  of  particles  with  specific  energies,  the  dose  at  the  surgeon's  position,  determined  by  the  depth  cameras,  can  be  measured.  The  validation  was  performed  by  comparing  the  simulation  results  to  both  theoretical  values  from  the  C-arms  user  manual  and  real  measurements  made  with  a  QUART  didoSVM  dosimeter.  The  average  error  was  16.46%  and  16.39%,  respectively.  The  proposed  flexible  setup  and  high  simulation  precision  without  a  calibration  with  measured  dosimeter  values,  has  great  potential  to  be  directly  used  and  integrated  intraoperatively  for  dose  measurement.
2	Using  egocentric  vision  to  achieve  robust  inertial  body  tracking  under  magnetic  disturbances.  In  the  context  of  a  smart  user  assistance  system  for  industrial  manipulation  tasks  it  is  necessary  to  capture  motions  of  the  upper  body  and  limbs  of  the  worker  in  order  to  derive  his  or  her  interactions  with  the  task  space.  While  such  capturing  technology  already  exists,  the  novelty  of  the  proposed  work  results  from  the  strong  requirements  of  the  application  context:  The  method  should  be  flexible  and  use  only  on-body  sensors,  work  accurately  in  industrial  environments  that  suffer  from  severe  magnetic  disturbances,  and  enable  consistent  registration  between  the  user  body  frame  and  the  task  space.  Currently  available  systems  cannot  provide  this.  This  paper  suggests  a  novel  egocentric  solution  for  visual-inertial  upper-body  motion  tracking  based  on  recursive  filtering  and  model-based  sensor  fusion.  Visual  detections  of  the  wrists  in  the  images  of  a  chest-mounted  camera  are  used  as  substitute  for  the  commonly  used  magnetometer  measurements.  The  on-body  sensor  network,  the  motion  capturing  system,  and  the  required  calibration  procedure  are  described  and  successful  operation  is  shown  in  a  real  industrial  environment.
2	Real  time  detection  of  simulator  sickness  in  virtual  reality  games  based  on  players  psychophysiological  data  during  gameplay.  Virtual  Reality  (VR)  technology  has  been  proliferating  in  the  last  decade,  especially  in  the  last  few  years.  However,  Simulator  Sickness  (SS)  still  represents  a  significant  problem  for  its  wider  adoption.  Currently,  the  most  common  way  to  detect  SS  is  using  the  Simulator  Sickness  Questionnaire  (SSQ).  SSQ  is  a  subjective  measurement  and  is  inadequate  for  real-time  applications  such  as  VR  games.  This  research  aims  to  investigate  how  to  use  machine  learning  techniques  to  detect  SS  based  on  in-game  characters’  and  users'  physiological  data  during  gameplay  in  VR  games.  To  achieve  this,  we  designed  an  experiment  to  collect  such  data  with  three  types  of  games.  We  trained  a  Long  Short-Term  Memory  neural  network  with  the  dataset  eye-tracking  and  character  movement  data  to  detect  SS  in  real-time.  Our  results  indicate  that,  in  VR  games,  our  model  is  an  accurate  and  efficient  way  to  detect  SS  in  real-time.
2	Mr  in  or  first  analysis  of  ar  vr  visualization  in  100  intra  operative  freehand  spect  acquisitions.  For  the  past  two  decades,  medical  Augmented  Reality  visualization  has  been  researched  and  prototype  systems  have  been  tested  in  laboratory  setups  and  limited  clinical  trials.  Up  to  our  knowledge,  until  now,  no  commercial  system  incorporating  Augmented  Reality  visualization  has  been  developed  and  used  routinely  within  the  real-life  surgical  environment.  In  this  paper,  we  are  reporting  on  observations  and  analysis  concerning  the  usage  of  a  commercially  developed  and  clinically  approved  Freehand  SPECT  system,  which  incorporates  monitor-based  Mixed  Reality  visualization,  during  real-life  surgeries.  The  workflow-based  analysis  we  present  is  focused  on  an  atomic  sub-task  of  sentinel  lymph  node  biopsy.  We  analyzed  the  usage  of  the  Augmented  and  Virtual  Reality  visualization  modes  by  the  surgical  team,  while  leaving  the  staff  completely  uninfluenced  and  unbiased  in  order  to  capture  the  natural  interaction  with  the  system.  We  report  on  our  observations  in  over  100  Freehand  SPECT  acquisitions  within  different  phases  of  52  surgeries.
2	Learning  perceived  emotion  using  affective  and  deep  features  for  mental  health  applications.  Virtual  agents  are  being  increasingly  used  in  the  areas  of  healthcare,  treatments,  and  therapy.  Virtual  agents  with  emotional  intelligence  have  shown  the  potential  to  be  applicable  to  deliver  mental  health  intervention  and  facilitate  therapies  for  children  on  the  autism  spectrum.  To  build  an  emotionally  intelligent  agent,  automatic  recognition  of  emotions  is  an  essential  building  block.  For  mental  health  therapies  and  treatments,  detecting  rapidly  changing  emotions  of  individuals  can  be  useful  to  know  whether  the  patients  are  showing  appropriate  emotional  response  or  not.  To  this  end,  we  present  a  new  data-driven  approach  to  identify  the  perceived  emotions  of  individuals  based  on  their  walking  styles.  We  extract  an  individual’s  walking  gait  in  the  form  of  a  sequence  of  3D  poses  given  an  RGB  video  of  him/her  walking.  We  leverage  the  gait  features  to  classify  the  perceived  emotional  state  of  the  individual  into  one  of  four  categories:  happy,  sad,  angry,  or  neutral.  First,  we  use  an  LSTM  network  to  extract  deep  features  of  the  gait  using  labeled  emotion  datasets.  Next,  we  compute  the  affective  features  of  the  gaits  using  posture  and  movement  cues.  We  combine  these  affective  features  with  the  deep  features  and  classify  them  using  a  Random  Forest  Classifier.  We  observe  that  this  approach  provides  an  accuracy  of  80:07%  in  identifying  the  perceived  emotions.  Additionally,  we  present  a  new  dataset  consisting  of  videos  of  walking  individuals,  their  extracted  gaits,  and  perceived  emotion  labels  associated  with  each  gait.  We  refer  to  this  dataset  as  the  ‘EWalk  (Emotion  Walk)”  dataset.
2	Remnance  of  form  altered  reflection  of  physical  reality.  Remnance  of  Form  is  an  interactive  installation  that  explores  the  dynamic  tension  between  an  object  and  its  shadow.  By  fusing  light,  projection,  and  motion  technologies,  the  shadow  can  now  detach  itself  from  its  former  role.  This  creates  a  new  narrative  that  challenges  our  perception  of  reality,  what's  real  and  what's  not.
2	Foldme  interacting  with  double  sided  foldable  displays.  In  this  paper,  we  present  a  novel  device  concept  that  features  double-sided  displays  which  can  be  folded  using  predefined  hinges.  The  device  concept  enables  users  to  dynamically  alter  both  size  and  shape  of  the  display  and  also  to  access  the  backside  using  fold  gestures.  We  explore  the  design  of  such  devices  by  investigating  different  types  and  forms  of  folding.  Furthermore,  we  propose  a  set  of  interaction  principles  and  techniques.  Following  a  user-centered  design  process,  we  evaluate  our  device  concept  in  two  sessions  with  low-fidelity  and  high-fidelity  prototypes.
2	Human  sensitivity  to  dynamic  rotation  gains  in  head  mounted  displays.  Head-mounted  display  (HMD)  systems  make  it  possible  to  introduce  discrepancies  between  physical  and  virtual  world  rotations.  Small  and  hopefully  unnoticed  discrepancies  can  be  useful  for  redirected  walking  algorithms  which  seek  to  allow  a  user  to  explore  a  large  virtual  space  while  confined  to  a  small  real  space.  Previous  work  has  examined  if  people  can  detect  discrepancies  which  are  fixed  (such  as  when  the  virtual  world  rotation  rate  is  amplified  by  a  fixed  value).  In  this  work,  we  conducted  an  experiment  where  participants  turn  360  degrees  in  the  real  world  and  indicate  if  the  virtual  world  rotation  rate  increased  or  decreased  over  the  course  of  the  turn.  Our  results  show  no  difference  between  rotational  gains  which  instantaneously  jump  from  one  value  to  another  compared  to  gains  which  slowly  change  over  the  course  of  a  360  degree  turn.  We  also  found  that  the  starting  gain  influenced  the  point  of  subjective  equality.  Finally,  our  work  indicates  that  the  range  of  reliably  detectable  gain  changes  is  consistent  for  starting  gains  at  1  and  starting  gains  at  2.
2	Fall  detection  on  the  road.  While  for  an  automatic  and  autonomous  indoor  fall  detection  an  enormous  amount  of  different  sensors  is  conceivable,  for  an  outdoor  fall  detection  only  wearable  sensors  are  applicable.  For  practical  uses  cases,  it  would  be  very  beneficial  if  after  a  detected  fall  a  medical  alert  is  initiated.  In  indoor  environments,  a  base  station  connected  to  a  telephone  line  can  handle  these  calls.  On  the  road,  this  can  be  achieved  by  mobile  phones  or  smartphones.  In  this  paper  we  first  estimate  the  suitability  of  several  systems  for  an  outdoor  fall  detection.  Then,  we  propose  a  system  for  a  mobile  and  stationary  fall  detection  and  alerting  system.  We  implemented  and  evaluated  this  system,  which  is  consisting  of  a  smartphone  and  a  wireless  sensor  node.
2	A  proposal  for  construction  of  telemedicine  information  platform  and  extension  of  health  care  service.  In  this  paper,  we  proposed  a  new  model  of  telemedicine  services  for  the  community  and  family  by  applying  mature  means  of  the  internet  and  telemedicine  technology  for  the  purpose  of  achieving  the  patients'  full  range  of  health  management  with  telemedicine,  health  monitoring,  health  advising,  health  guidance  and  health  care,  etc.  The  implementation  of  the  objectives,  methods,  and  significance  were  also  analyzed  and  discussed  in  the  paper.
2	Using  artificial  neural  nets  to  hemo  metabolites  identification.  Over  the  last  century  there  has  been  a  considerable  increase  in  human  longevity  and  this  made  a  large  number  of  people  to  reach  a  critical  age  for  development  of  several  diseases.  As  a  result  of  this  increase  in  life  expectancy  health  issues  related  to  ageing  appeared,  some  examples  are  hypercholesterolemia,  hyperglycemia  and  increased  levels  of  blood  urea.  This  paper  presents  a  portable  and  low  cost  system  using  Artificial  Neural  Networks  to  Hemo  metabolites  identification.  The  system  developed  is  based  in  amperometric  biosensors  and  is  able  to  perform  the  identification  of  glucose,  cholesterol  and  urea  concentrations  in  the  blood.  The  main  goals  of  this  system  is:  the  identification  of  three  types  of  Hemo  metabolites  with  their  concentrations,  the  low  cost  of  the  entire  system  and  the  reuse  capability  of  the  biosensor.  Cost  of  the  entire  system  and  the  reuse  capability  of  the  biosensor.
2	An  enhanced  threshold  based  technique  for  white  blood  cells  nuclei  automatic  segmentation.  One  of  the  most  important  clinical  examination  tests  is  the  blood  test.  In  a  clinical  laboratory,  counting  different  blood  cells  is  important.  Manual  microscopic  inspection  is  time-consuming  and  requires  technical  knowledge.  Therefore,  automatic  medical  diagnosis  systems  are  required  to  help  physicians  to  diagnose  diseases  in  a  fast  and  yet  efficient  way.  Cell  automatic  classification  has  larger  interest  especially  for  clinics  and  laboratories;  the  most  important  step  in  automatic  classification  success  is  segmentation.  This  paper  shows  an  efficient  technique  for  automatic  blood  cell  nuclei  segmentation.  This  technique  is  relying  on  enhancing  and  filtering  the  gray  scale  image  contrast.  False  objects  are  removed  utilizing  minimum  segment  size.  365  blood  images  were  used  to  examine  this  segmentation  technique.  Quantitative  analysis  of  the  proposed  segmentation  technique  on  the  blood  image  set  gives  80.6%  accuracy.  In  comparison  to  other  techniques  the  proposed  segmentation  technique  performance  was  found  to  be  superior.  The  five  normal  white  blood  cells  types  were  used  for  evaluation  to  compare  isolated  performance.  Eosinophil  was  found  to  have  the  lowest  segmentation  accuracy  which  is  71.0%  and  Monocyte  was  the  highest  one  with  85.9%.  The  blood  images  dataset  and  the  source  code  are  published  on  MATLAB  file  exchange  website  for  comparison  and  re-production.
2	Using  predictive  classifiers  to  prevent  infant  mortality  in  the  brazilian  northeast.  Despite  the  fact  that  infant  mortality  rates  have  been  decreased  in  recent  years,  this  issue  stills  being  considered  alarming  to  Brazilian  health  system  indicators.  In  this  context,  the  GISSA  framework,  an  intelligent  governance  framework  for  Brazilian  health  system,  emerges  as  a  smart  system  for  the  Federal  Government  program,  called  Stork  Network.  Its  main  objective  is  to  improve  the  healthcare  for  pregnant  women  as  well  as  their  newborns.  This  application  aims  to  generate  alerts  focusing  on  the  health  status  verification  of  newborns  and  pregnant  woman  to  support  decision-makers  in  preventive  actions  that  may  mitigate  severe  problems.  Therefore,  this  paper  presents  the  LAIS,  an  Intelligent  health  analysis  system  that  uses  data  mining  (DM)  to  generate  newborns  death  risk  alerts  through  probability-based  methods.  Results  show  that  the  Naive  Bayes  classifier  presents  better  performance  than  the  other  DM  approaches  to  the  used  pregnancy  data  set  analysis  of  this  work.  This  approach  performed  an  accuracy  of  0.982  and  a  Receiver  Operating  Characteristic  (ROC)  Area  of  0.921.  Both  indicators  suggest  the  proposed  model  may  contribute  to  the  reduction  of  maternal  and  fetal  deaths.
2	It  assist  towards  usable  applications  for  elderly  people.  IT-ASSIST  is  a  twenty  months  research  project  which  has  the  goal  to  give  elderly  people  the  opportunity  to  profit  from  digital  media.  Suffering  from  age  related  impairments  concerning  vision,  hearing,  or  dexterity  and  bad  hand-eye  coordination  are  challenges  when  designing  user  interfaces  for  elderly  people.  Common  approaches  are  trying  to  model  systems  for  specific  impairments.  In  this  project,  the  authors  follow  the  approach  to  set  up  interfaces  and  systems  that  can  be  used  independent  from  personal  impairments.  Customization  has  adapted  these  systems  to  be  in  accordance  with  personnel  impairments.  Common  applications  like  photo  editing,  digital  mailing  or  internet  browsing  in  a  redesigned  form  provide  social  communication  accordingly.  In  this  article,  a  prototype  of  a  customized  user  interface,  its  implementation,  and  results  of  user  studies  are  presented  and  discussed.
2	Notes  on  the  concept  of  data  interoperability  cases  from  an  ecology  of  aids  research  infrastructures.  Data  interoperation  functions  with  the  logic  of  a  black  box.  Interoperation  is  achieved  through  front-loaded  work  and  epistemically  charged  negotiation  that  thereafter  become  infrastructural,  that  is,  supporting  downstream  actions  without  fully  revealing  the  underpinnings  that  enable  those  actions.  Drawing  from  ethnographic  and  archival  investigations  of  data  interoperation  within  and  across  an  ecology  of  HIV/AIDS  research  infrastructures,  this  paper  offers  several  sensitizing  concepts  for  the  investigation  of  how  data  are  brought  together  and  thereafter  circulate.  Data  interoperability  is  historical,  infrastructural,  relatively  irreversible,  negotiated,  epistemic,  seamful  and  seamless,  and  is  approaching  the  status  of  a  general  value  rather  than  a  specific  means  to  an  end.
2	Latin  america  as  a  place  for  cscw  research.  Inspired  by  the  ACM  SIGCHI  Across  Borders  Initiative,  this  workshop  focuses  on  ongoing  CSCW  research  in,  or  about,  Latin  America  (LATAM).  We  seek  to  position  LATAM  as  the  common  context  that  unites  students,  academic  and  industry  researchers  who  participate  in  the  workshop.  Our  goals  are:  (1)  to  discuss  the  opportunities  and  challenges  of  doing  CSCW  research  centered  on  LATAM,  (2)  to  collaboratively  mentor  emerging  projects  focused  on  LATAM,  and  (3)  to  make  LATAM  research  projects  more  visible  to  the  international  community.  Senior  CSCW  researchers  from  LATAM  and  other  regions  will  be  invited  to  discuss  the  work  presented  at  the  event  in  order  to  facilitate  a  greater  integration  of  Latin  American  CSCW  into  the  CSCW  community  at  large.
2	The  personality  of  popular  facebook  users.  We  study  the  relationship  between  Facebook  popularity  (number  of  contacts)  and  personality  traits  on  a  large  number  of  subjects.  We  test  to  which  extent  two  prevalent  viewpoints  hold.  That  is,  popular  users  (those  with  many  social  contacts)  are  the  ones  whose  personality  traits  either  predict  many  offline  (real  world)  friends  or  predict  propensity  to  maintain  superficial  relationships.  We  find  that  the  predictor  for  number  of  friends  in  the  real  world  (Extraversion)  is  also  a  predictor  for  number  of  Facebook  contacts.  We  then  test  whether  people  who  have  many  social  contacts  on  Facebook  are  the  ones  who  are  able  to  adapt  themselves  to  new  forms  of  communication,  present  themselves  in  likable  ways,  and  have  propensity  to  maintain  superficial  relationships.  We  show  that  there  is  no  statistical  evidence  to  support  such  a  conjecture.
2	Working  and  sustaining  the  virtual  disaster  desk.  Humanity  Road  is  a  volunteer  organization  working  within  the  domain  of  disaster  response.  The  organization  is  entirely  virtual,  relying  on  ICT  to  both  organize  and  execute  its  work  of  helping  to  inform  the  public  on  how  to  survive  after  disaster  events.  This  paper  follows  the  trajectory  of  Humanity  Road  from  an  emergent  group  to  a  formal  non-profit,  considering  how  its  articulation,  conduct  and  products  of  work  together  express  its  identity  and  purpose,  which  include  aspirations  of  relating  to  and  changing  the  larger  ecosystem  of  emergency  response.  Through  excerpts  of  its  communications,  we  consider  how  the  organization  makes  changes  in  order  to  sustain  itself  in  rapid-response  work  supported  in  large  part  by  episodic  influxes  of  volunteers.  This  case  enlightens  discussion  about  technology-supported  civic  participation,  and  the  means  by  which  dedicated  long-term  commitment  to  the  civic  sphere  is  mobilized.
2	Mutual  assessment  in  the  social  programmer  ecosystem  an  empirical  investigation  of  developer  profile  aggregators.  The  multitude  of  social  media  channels  that  programmers  can  use  to  participate  in  software  development  has  given  rise  to  online  developer  profiles  that  aggregate  activity  across  many  services.  Studying  members  of  such  developer  profile  aggregators,  we  found  an  ecosystem  that  revolves  around  the  social  programmer.  Developers  are  assessing  each  other  to  evaluate  whether  other  developers  are  interesting,  worth  following,  or  worth  collaborating  with.  They  are  self-conscious  about  being  assessed,  and  thus  manage  their  public  images.  They  value  passion  for  software  development,  new  technologies,  and  learning.  Some  recruiters  participate  in  the  ecosystem  and  use  it  to  find  candidates  for  hiring;  other  recruiters  struggle  with  the  interpretation  of  signals  and  issues  of  trust.  This  mutual  assessment  is  changing  how  software  engineers  collaborate  and  how  they  advance  their  skills.
2	The  effects  of  visualizing  activity  history  on  attitudes  and  behaviors  in  a  peer  production  context.  In  a  variety  of  peer  production  settings,  from  Wikipedia  to  open  source  software  development  to  crowdsourcing,  individuals  may  encounter,  edit,  or  review  the  work  of  unknown  others.  Typically  this  is  done  without  much  context  to  the  person's  past  behavior  or  performance.  To  understand  how  exposure  to  an  unknown  individual's  activity  history  influences  attitudes  and  behaviors,  we  conducted  an  online  experiment  on  Mechanical  Turk  varying  the  content,  quality,  and  presentation  of  information  about  another  Turker's  work  history.  Surprisingly,  negative  work  history  did  not  lead  to  negative  outcomes,  but  in  contrast,  a  positive  work  history  led  to  positive  initial  impressions  that  persisted  in  the  face  of  contrary  information.  This  work  provides  insight  into  the  impact  of  activity  history  design  factors  on  psychological  and  behavioral  outcomes  that  can  be  of  use  in  other  related  settings.
2	Social  incentives  in  pervasive  fitness  apps  for  obese  and  diabetic  patients.  Social  incentives  such  as  cooperation  and  competition  are  found  to  motivate  users  in  pervasive  fitness  applications.  This  work  investigates  how  social  incentives  work  for  individuals  with  obesity  and  diabetes.  We  used  a  mobile  fitness  application  called  HealthyTogether  as  an  experimental  platform,  which  allows  dyads  to  achieve  fitness  goals  together  and  compete  in  an  online  community.  We  conducted  a  four-week  study  with  16  obese  and  diabetic  patients  who  used  HealthyTogether  to  exercise  with  a  buddy.  Results  show  that  participants  exercised  more  with  social  incentives  compared  with  their  baseline.  Collaborating  with  buddies  to  compete  in  a  community  was  reported  as  motivating  for  dyads  exercising  with  strong  ties.  Social  interactions  could  be  demotivating  between  dyads  who  did  not  know  each  other  well.  Finally,  it  is  crucial  to  consider  patients'  technical  literacy  when  designing  behavior-changing  technologies.
2	Enticing  casual  nature  preserve  visitors  into  citizen  science  via  photos.  While  scientists  need  the  contributions  of  members  of  the  public  if  they  are  to  document  biological  diversity  across  large  spaces  and  over  long  periods  of  time,  it  is  challenging  to  recruit  enough  volunteers.  Since  many  people  use  their  smartphones  to  take  pictures  when  they  are  in  nature,  it  may  be  beneficial  to  understand  what  they  gravitate  toward  as  a  first  step  in  understanding  how  they  might  be  engaged  in  citizen  science.  We  examined  photographs  taken  by  casual  visitors  to  a  Colorado  nature  preserve  to  look  for  clues  about  what  attracts  them.  A  thematic  analysis  revealed  that  the  majority  of  their  pictures  were  of  plants,  birds,  and  landscapes,  and  three-quarters  chose  to  annotate  some  photos  with  comments  or  questions.  Based  on  these  findings,  we  propose  ways  to  entice  such  visitors  toward  participating  in  biodiversity-oriented  citizen  science  projects.
2	Data  work  in  healthcare  the  new  work  ecologies  of  healthcare  infrastructures.  The  workshop  focuses  on  the  new  work  ecologies  emerging  from  implementation  and  use  of  information  infrastructures  in  healthcare  (IIH).  As  IIH  “grows”  through  organizational  and  regulatory  mechanisms,  CSCW  researchers  grapple  with  the  shifting  nature  of  healthcare  data.  CSCW  has  long  been  concerned  with  coordination,  cooperation,  and  communication  among  interdisciplinary  occupations  in  healthcare.  Yet,  while  medical  record  keeping  is  still  a  primary  function  of  IIH,  second  order  data  usages  are  increasingly  large  foci  of  IIH  design  and  use.  Facilitating  development  of  health  data  practice  and  infrastructure  is  an  area  ripe  for  CSCW  research.  Critical  topics  include  but  are  not  limited  to:  re-use  of  clinical  data  for  second  order  usages;  design  of  artifacts  and  infrastructures;  politics  of  creating  and  using  data;  algorithmic  authority  of  IIH  and  effects  on  the  exercise  of  expertise  and  discretion  of  healthcare  professions;  new  forms  of  healthcare  data  work,  including  new  occupations;  data-driven  accountability  and  management  in  healthcare”
2	Participatory  design  with  older  adults  an  analysis  of  creativity  in  the  design  of  mobile  healthcare  applications.  Researchers  often  use  participatory  design  --  involving  endusers  in  technology  ideation  --  as  this  is  found  to  lead  to  more  useful  and  relevant  products.  Researchers  have  sought  to  involve  older  adults  in  the  design  of  emerging  technologies  like  smartphones,  with  which  older  adults  often  have  little  experience.  Therefore,  their  effectiveness  as  co-designers  could  be  questioned.  We  examine  whether  older  adults  can  create  novel  design  ideas,  and  whether  critiquing  existing  applications  prior  to  ideation  helps  or  hinders  creativity.  Panelists  from  industry  and  academia  evaluated  design  ideas  generated  by  focus  groups  of  older  adults.  Out  of  five  groups,  the  most  creative  design  idea  came  from  one  with  no  smartphone  experience  or  critique  exposure.  We  found  that  while  only  some  designs  scored  high  on  the  novelty  dimension  of  creativity,  participants  were  enthusiastic  about  participating  and  adapted  quickly.  We  found  evidence  that  critiquing  existing  applications  prior  to  ideation  did  more  harm  than  good,  potentially  due  to  design  fixation.  We  recommend  continuing  to  involve  older  adults  in  the  technology  design  ideation  phase.
2	Maggetz  customizable  passive  tangible  controllers  on  and  around  conventional  mobile  devices.  This  paper  proposes  user-customizable  passive  control  widgets,  called  MagGetz,  which  enable  tangible  interaction  on  and  around  mobile  devices  without  requiring  power  or  wireless  connections.  This  is  achieved  by  tracking  and  ana-lyzing  the  magnetic  field  generated  by  controllers  attached  on  and  around  the  device  through  a  single  magnetometer,  which  is  commonly  integrated  in  smartphones  today.  The  proposed  method  provides  users  with  a  broader  interaction  area,  customizable  input  layouts,  richer  physical  clues,  and  higher  input  expressiveness  without  the  need  for  hardware  modifications.  We  have  presented  a  software  toolkit  and  several  applications  using  MagGetz.
2	Holoportation  virtual  3d  teleportation  in  real  time.  We  present  an  end-to-end  system  for  augmented  and  virtual  reality  telepresence,  called  Holoportation.  Our  system  demonstrates  high-quality,  real-time  3D  reconstructions  of  an  entire  space,  including  people,  furniture  and  objects,  using  a  set  of  new  depth  cameras.  These  3D  models  can  also  be  transmitted  in  real-time  to  remote  users.  This  allows  users  wearing  virtual  or  augmented  reality  displays  to  see,  hear  and  interact  with  remote  participants  in  3D,  almost  as  if  they  were  present  in  the  same  physical  space.  From  an  audio-visual  perspective,  communicating  and  interacting  with  remote  users  edges  closer  to  face-to-face  communication.  This  paper  describes  the  Holoportation  technical  system  in  full,  its  key  interactive  capabilities,  the  application  scenarios  it  enables,  and  an  initial  qualitative  study  of  using  this  new  communication  medium.
2	Posetween  pose  driven  tween  animation.  Augmenting  human  action  videos  with  visual  effects  often  requires  professional  tools  and  skills.  To  make  this  more  accessible  by  novice  users,  existing  attempts  have  focused  on  automatically  adding  visual  effects  to  faces  and  hands,  or  let  virtual  objects  strictly  track  certain  body  parts,  resulting  in  rigid-looking  effects.  We  present  PoseTween,  an  interactive  system  that  allows  novice  users  to  easily  add  vivid  virtual  objects  with  their  movement  interacting  with  a  moving  subject  in  an  input  video.  Our  key  idea  is  to  leverage  the  motion  of  the  subject  to  create  pose-driven  tween  animations  of  virtual  objects.  With  our  tool,  a  user  only  needs  to  edit  the  properties  of  a  virtual  object  with  respect  to  the  subject's  movement  at  keyframes,  and  the  object  is  associated  with  certain  body  parts  automatically.  The  properties  of  the  object  at  intermediate  frames  are  then  determined  by  both  the  body  movement  and  the  interpolated  object  keyframe  properties,  producing  natural  object  movements  and  interactions  with  the  subject.  We  design  a  user  interface  to  facilitate  editing  of  keyframes  and  previewing  animation  results.  Our  user  study  shows  that  PoseTween  significantly  requires  less  editing  time  and  fewer  keyframes  than  using  the  traditional  tween  animation  in  making  pose-driven  tween  animations  for  novice  users.
2	Towards  robust  real  time  valence  recognition  from  facial  expressions  for  market  research  applications.  A  novel  system  for  facial  expression  recognition  is  presented.  It  is  designed  to  assess  valence  (positive  or  negative  feeling)  in  the  context  of  market  research  applications.  Other  design  parameters  that  were  met  are  real-time  capability  and  robust  results  even  when  using  off-the-shelf  equipment  (standard  web  cam  and  lighting).  The  paper  further  presents  an  empirical  study  that  attests  the  system  satisfying  predictive  validity  for  market  research  applications  as  long  as  very  small  absolute  valence  values  close  to  zero  are  not  interpreted.
2	The  appraisal  equivalence  hypothesis  verifying  the  domain  independence  of  a  computational  model  of  emotion  dynamics.  Appraisal  theory  is  the  most  influential  theory  within  affective  computing,  and  serves  as  the  basis  for  several  computational  models  of  emotion.  The  theory  makes  strong  claims  of  domain-independence:  seemingly  different  situations,  both  within  and  across  domains  are  claimed  to  produce  the  identical  emotional  responses  if  and  only  if  they  are  appraised  the  same  way.  This  article  tests  this  claim,  and  the  predictions  of  a  computational  model  that  embodies  it,  in  two  very  different  interactive  games.  The  results  extend  prior  empirical  evidence  for  appraisal  theory  to  situations  where  emotions  unfold  and  change  over  time.
2	Pain  detection  with  fnirs  measured  brain  signals  a  personalized  machine  learning  approach  using  the  wavelet  transform  and  bayesian  hierarchical  modeling  with  dirichlet  process  priors.  Currently  self-report  pain  ratings  are  the  gold  standard  in  clinical  pain  assessment.  However,  the  development  of  objective  automatic  measures  of  pain  could  substantially  aid  pain  diagnosis  and  therapy.  Recent  neuroimaging  studies  have  shown  the  potential  of  functional  near-infrared  spectroscopy  (fNIRS)  for  pain  detection.  This  is  a  brain-imaging  technique  that  provides  non-invasive,  long-term  measurements  of  cortical  hemoglobin  concentration  changes.  In  this  study,  we  focused  on  fNIRS  signals  acquired  exclusively  from  the  prefrontal  cortex,  which  can  be  accessed  unobtrusively,  and  derived  an  algorithm  for  the  detection  of  the  presence  of  pain  using  Bayesian  hierarchical  modelling  with  wavelet  features.  This  approach  allows  personalization  of  the  inference  process  by  accounting  for  inter-participant  variability  in  pain  responses.  Our  work  highlights  the  importance  of  adopting  a  personalized  approach  and  supports  the  use  of  fNIRS  for  pain  assessment.
2	Triggering  artwork  swaps  for  live  animation.  Live  animation  of  2D  characters  is  a  new  form  of  storytelling  that  has  started  to  appear  on  streaming  platforms  and  broadcast  TV.  Unlike  traditional  animation,  human  performers  control  characters  in  real  time  so  that  they  can  respond  and  improvise  to  live  events.  Current  live  animation  systems  provide  a  range  of  animation  controls,  such  as  camera  input  to  drive  head  movements,  audio  for  lip  sync,  and  keyboard  shortcuts  to  trigger  discrete  pose  changes  via  artwork  swaps.  However,  managing  all  of  these  controls  during  a  live  performance  is  challenging.  In  this  work,  we  present  a  new  interactive  system  that  specifically  addresses  the  problem  of  triggering  artwork  swaps  in  live  settings.  Our  key  contributions  are  the  design  of  a  multi-touch  triggering  interface  that  overlays  visual  triggers  around  a  live  preview  of  the  character,  and  a  predictive  triggering  model  that  leverages  practice  performances  to  suggest  pose  transitions  during  live  performances.  We  evaluate  our  system  with  quantitative  experiments,  a  user  study  with  novice  participants,  and  interviews  with  professional  animators.
2	Semi  automated  svg  programming  via  direct  manipulation.  Direct  manipulation  interfaces  provide  intuitive  and  interactive  features  to  a  broad  range  of  users,  but  they  often  exhibit  two  limitations:  the  built-in  features  cannot  possibly  cover  all  use  cases,  and  the  internal  representation  of  the  content  is  not  readily  exposed.  We  believe  that  if  direct  manipulation  interfaces  were  to  (a)  use  general-purpose  programs  as  the  representation  format,  and  (b)  expose  those  programs  to  the  user,  then  experts  could  customize  these  systems  in  powerful  new  ways  and  non-experts  could  enjoy  some  of  the  benefits  of  programmable  systems.      In  recent  work,  we  presented  a  prototype  SVG  editor  called  Sketch-n-Sketch  that  offered  a  step  towards  this  vision.  In  that  system,  the  user  wrote  a  program  in  a  general-purpose  lambda-calculus  to  generate  a  graphic  design  and  could  then  directly  manipulate  the  output  to  indirectly  change  design  parameters  (i.e.  constant  literals)  in  the  program  in  real-time  during  the  manipulation.  Unfortunately,  the  burden  of  programming  the  desired  relationships  rested  entirely  on  the  user.      In  this  paper,  we  design  and  implement  new  features  for  Sketch-n-Sketch  that  assist  in  the  programming  process  itself.  Like  typical  direct  manipulation  systems,  our  extended  Sketch-n-Sketch  now  provides  GUI-based  tools  for  drawing  shapes,  relating  shapes  to  each  other,  and  grouping  shapes  together.  Unlike  typical  systems,  however,  each  tool  carries  out  the  user's  intention  by  transforming  their  general-purpose  program.  This  novel,  semi-automated  programming  workflow  allows  the  user  to  rapidly  create  high-level,  reusable  abstractions  in  the  program  while  at  the  same  time  retaining  direct  manipulation  capabilities.  In  future  work,  our  approach  may  be  extended  with  more  graphic  design  features  or  realized  for  other  application  domains.
2	Photochromic  carpet  playful  floor  canvas  with  color  changing  footprints.  Natural  environments  record  their  past  and  reveal  usage  in  subtle  cues  such  as  erosion  and  footprints.  In  modern  society  of  concrete  cities  and  dynamic  touch  screens,  this  richness  is  lost.    We  present  a  large  size  interactive  floor  display  that  captures  visitors'  footsteps  in  playful  prints  to  make  a  modern  environment  into  a  canvas  of  past  activities.  The  implementation  consists  of  a  carpet  coated  with  color  changing  ink  and  shoes  that  activate  color  changes.  Each  step  a  visitor  makes  results  in  a  dynamic  print  that  slowly  fades  away.
2	The  role  of  emotion  in  self  explanations  by  cognitive  agents.  Artificial  Intelligence  (AI)  systems,  including  intelligent  agents,  are  becoming  increasingly  complex.  Explainable  AI  (XAI)  is  the  capability  of  these  systems  to  explain  their  behaviour,  in  a  for  humans  understandable  manner.  Cognitive  agents,  a  type  of  intelligent  agents,  typically  explain  their  actions  with  their  beliefs  and  desires.  However,  humans  also  take  into  account  their  own  and  other's  emotions  in  their  explanations,  and  humans  explain  their  emotions.  We  refer  to  using  emotions  in  XAI  as  Emotion-aware  eXplainable  Artificial  Intelligence  (EXAI).  Although  EXAI  should  also  include  awareness  of  the  other's  emotions,  in  this  work  we  focus  on  how  the  simulation  of  emotions  in  cognitive  agents  can  help  them  self-explain  their  behaviour.  We  argue  that  emotions  simulated  based  on  cognitive  appraisal  theory  enable  (1)  the  explanation  of  these  emotions,  (2)  using  them  as  a  heuristic  to  identify  important  beliefs  and  desires  for  the  explanation,  and  (3)  the  use  of  emotion  words  in  the  explanations  themselves.
2	Chewing  jockey  augmented  food  texture  by  using  sound  based  on  the  cross  modal  effect.  We  focus  on  the  dining  and  show  how  to  improve  dining  experience.  We  use  sound  effects  to  augment  food  texture,  creating  a  cross-modal  illusion.      Our  system  is  composed  of  a  bone-conduction  speaker,  a  microphone,  a  photoreflector  to  measure  the  motion  of  jaw,  and  a  computer  to  design  the  sound  effect  or  filtering.      We  focus  on  the  texture  of  food,  an  important  component  of  deliciousness,  to  enhance  the  eating  experience  without  modifying  the  physical  or  chemical  feature  of  the  food.  We  use  prevailing  technologies  to  detect  chewing  action,  feedback  and  process  the  chewing  sound.  In  addition,  we  design  some  chewing  augmentation  filter  for  each  foods.  These  combinations  create  the  cross-modality  effect  for  food  texture.      We  have  developed  three  elements.  First  is  a  bite-detection  sensor,  utilizing  a  photoreflector,  to  measure  the  movement  of  the  lower  jaw.  Second  is  a  sound  filter  for  each  type  of  food  that  will  be  used  to  control  food  texture.  Third  is  a  self-feedback  system  to  enhance  the  chewing  action  that  records  the  chewing  sound  and  the  jaw  motion,  and  delivers  it  to  the  user  using  bone-conduction  speakers.      Our  aim  is  to  redesign  the  experience  of  eating.  We  believe  this  technology  is  useful  for  following  situations.  For  a  start,  it  is  a  challenge  to  improve  the  eating  QoL  for  dentures  users.  As  they  cannot  bite  strongly,  they  get  a  reduced  sensation  of  food.  Chewing  Jockey  helps  to  restore  that  sensation.  Another  application  is  to  moderate  the  chewing  speed.  Chewing  too  fast  is  not  good  for  digestion  and  also  leads  to  over-eating.  With  our  technology,  we  can  provide  the  most  suitable  chewing  speed  to  alter  such  habits.  Lastly,  chewing  can  be  a  form  of  interaction  for  a  novel  game  design,  in  which  you  could  role-play  a  monster  chewing  on  "living"  things.
2	Dating  deception  gender  online  dating  and  exaggerated  self  presentation.  This  study  examined  how  differences  in  expectations  about  meeting  impacted  the  degree  of  deceptive  self-presentation  individuals  displayed  within  the  context  of  dating.  Participants  filled  out  personality  measures  in  one  of  four  anticipated  meeting  conditions:  face-to-face,  email,  no  meeting,  and  a  control  condition  with  no  pretense  of  dating.  Results  indicated  that,  compared  to  baseline  measures,  male  participants  increased  the  amount  they  self-presented  when  anticipating  a  future  interaction  with  a  prospective  date.  Specifically,  male  participants  emphasized  their  positive  characteristics  more  if  the  potential  date  was  less  salient  (e.g.,  email  meeting)  compared  to  a  more  salient  condition  (e.g.,  face-to-face  meeting)  or  the  control  conditions.  Implications  for  self-presentation  theory,  online  social  interaction,  and  online  dating  research  will  be  discussed.
2	Factors  influencing  the  continuance  intention  to  the  usage  of  web  2  0  an  empirical  study.  New  business  models  and  applications  have  been  continuously  developed  and  popularized  on  the  Internet.  In  recent  years,  a  number  of  applications  including  blogs,  Facebook,  iGoogle,  Plurk,  Twitter,  and  YouTube  known  as  Web  2.0  have  become  very  popular.  These  aforementioned  applications  all  have  a  strong  social  flavor.  However,  what  social  factors  exert  an  influence  onto  their  use  is  still  unclear  and  remains  as  a  research  issue  to  be  further  investigated.  This  research  studies  four  social  factors  and  they  are  subjective  norm,  image,  critical  mass,  and  electronic  word-of-mouth.  A  causal  model  of  the  satisfaction  and  continuance  intention  of  Web  2.0  users  as  a  function  of  these  four  social  factors  is  proposed.  Results  indicate  that  user  satisfaction  with  Web  2.0  applications  significantly  affects  electronic  word-of-mouth,  which  in  turn  significantly  influences  their  continuance  intention.  In  addition,  subjective  norm,  image  and  critical  mass  all  have  a  significant  impact  onto  satisfaction,  which  in  turn  has  an  indirect  significant  influence  on  electronic  word-of-mouth.  Finally,  all  social  factors  have  a  significant  direct  impact  on  continuance  intention.  Finally,  implications  for  service  providers  and  researchers  are  discussed.
2	The  effects  of  the  integration  of  external  and  internal  communication  features  in  digital  magazines  on  consumers  magazine  attitude.  This  study  investigates  the  effects  of  external  and  internal  communication  features  on  consumers'  digital  magazine  attitude,  and  the  processes  (i.e.,  perceived  interactivity  and  social  presence)  underlying  these  effects.  Both  feature  types  enable  communication  between  two  or  more  people.  Though,  in  the  case  of  external  communication  features,  the  interactions  take  place  outside  the  digital  magazine  (e.g.,  on  Facebook),  whereas  in  the  case  of  internal  communication  features,  the  communication  takes  place  inside  the  digital  environment  of  the  magazine.  In  a  two-wave  experiment  with  a  2  (external  communication  features:  present/absent)ź×ź2  (internal  communication  features:  present/absent)  between-subjects  design,  192  participants  were  exposed  to  a  digital  tablet  magazine  in  which  the  presence  of  interactive  features  was  manipulated.  The  results  show  that  digital  magazines  with  either  external  or  internal  communication  features  are  perceived  as  more  interactive,  which  has  a  positive  influence  on  consumers'  digital  magazine  attitude.  The  findings  also  reveal  that  -  in  contrast  to  external  -  internal  communication  features  have  the  ability  to  enhance  feelings  of  social  presence,  another  process  through  which  digital  magazine  attitude  is  positively  affected.  So,  internal  communication  features  improve  consumers'  digital  magazine  attitude  through  two  pathways  (i.e.,  perceived  interactivity  and  social  presence),  and  external  communication  features  only  via  one  (i.e.,  perceived  interactivity).  Adding  ex-/internal  communication  features  increases  interactivity  perceptions.Perceived  interactivity  positively  affects  digital  magazine  attitude.Internal  communication  features  elicit  feelings  of  social  presence.Social  presence  positively  affects  digital  magazine  attitude.Internal  communication  features  enhance  community  feelings  among  readers.
2	Online  collaboration  collaborative  behavior  patterns  and  factors  affecting  globally  distributed  team  performance.  Studying  the  collaborative  behavior  of  online  learning  teams  and  how  this  behavior  is  related  to  communication  mode  and  task  type  is  a  complex  process.  Research  about  small  group  learning  suggests  that  a  higher  percentage  of  social  interactions  occur  in  synchronous  rather  than  asynchronous  mode,  and  that  students  spend  more  time  in  task-oriented  interaction  in  asynchronous  discussions  than  in  synchronous  mode.  This  study  analyzed  the  collaborative  interaction  patterns  of  global  software  development  learning  teams  composed  of  students  from  Turkey,  US,  and  Panama.  Data  collected  from  students'  chat  histories  and  forum  discussions  from  three  global  software  development  projects  were  collected  and  compared.  Both  qualitative  and  quantitative  analysis  methods  were  used  to  determine  the  differences  between  a  group's  communication  patterns  in  asynchronous  versus  synchronous  communication  mode.  K-means  clustering  with  the  Ward  method  was  used  to  investigate  the  patterns  of  behaviors  in  distributed  teams.  The  results  show  that  communication  patterns  are  related  to  communication  mode,  the  nature  of  the  task,  and  the  experience  level  of  the  leader.  The  paper  also  includes  recommendations  for  building  effective  online  collaborative  teams  and  describes  future  research  possibilities.
2	Purpose  of  social  networking  use  and  victimisation.  We  explore  barriers  to  social  media  adoption  for  collaborative  learning.Purpose  of  use  and  victimisation  are  explored  between  students  and  non-students.Students  are  open  to  social  media  use  for  academic  purposes.Students  are  safer  users  of  social  networks  than  those  not  in  HE.  Current  literature  widely  reports  successful  uses  of  social  media  as  a  source  of  information,  collaborative  and  learning  tool  for  students  in  higher  education.  Although  universities  increasingly  promote  the  use  of  Social  Network  Services  (SNS)  little  is  known  about  how  students  use  them.  Also  the  adverse  effects  of  social  media  activity,  such  as  cybercrime  victimisation  in  HE,  are  under  explored.  Concerns  over  informal  leisure  use  of  SNS  by  students  leading  to  cyber  victimisation  may  help  explain  slow  adoption  of  social  media  in  education.  This  paper  shows,  however,  that  students  use  SNS  in  a  similar  way  to  those  users  who  are  not  in  education,  with  more  that  60%  using  SNS  for  both  socialising  and  gathering  information.  We  find  that  students  are  less  likely  to  be  victims  of  cybercrime  than  non-students  suggesting  that  SNS  activity  is  less  risky  within  the  university  lifespan.  The  implications  of  this  study  are  significant  for  policy  and  practice  for  universities  and  educational  authorities.
2	The  impact  of  creativity  and  community  facilitation  on  music  streaming  adoption  and  digital  piracy.  Preventing  unauthorized  downloading  and  other  forms  of  digital  piracy  has  been  a  persistent  challenge.  Strategies  based  on  deterrence,  ethical  arguments,  or  technical  measures  have  been  effective,  but  only  to  a  limited  extent.  The  rising  popularity  of  music  streaming  systems,  however,  affords  the  owners  of  digital  content  opportunities  to  experiment  with  alternative  business  models  and  value  propositions  that  may  also  discourage  digital  piracy.  The  results  of  this  field  study  involving  139  subjects  suggest  that  music  streaming  systems  that  incorporate  features  that  facilitate  individual  creativity  and  community  building  contributes  towards  the  enjoyment  and  usefulness  of  the  system.  This  in  turn  leads  to  music  streaming  adoption.  The  results  of  the  study  also  show  that  the  usefulness  of  a  music  streaming  system  is  associated  with  a  reduction  in  pirating  intention.  Surprisingly,  a  link  between  the  enjoyment  of  a  music  streaming  system  and  a  reduction  in  digital  pirating  intention  was  not  supported.  Display  Omitted  Music  streaming  systems  that  are  useful  and  provide  enjoyment  lead  to  adoption.Systems  that  are  useful  and  provide  enjoyment  reduce  digital  piracy.Usefulness  and  enjoyment  of  a  system  is  a  function  of  creativity  facilitation.Usefulness  and  enjoyment  of  a  system  is  a  function  of  community  facilitation.
2	Investigating  children  s  deep  learning  of  the  tree  life  cycle  using  mobile  technologies.  Abstract  This  study  investigates  children's  problem-solving  activities  during  mobile  learning  in  an  outdoor  summer  camp  setting.  We  designed  a  mobile  application  to  support  children  on  trails  at  a  nature  center  to  apply  strategies  for  decision  making  about  tree  life  cycles.  We  analyzed  video  records  of  10  groups  (9  dyads  and  1  triad)  of  children  (ages  9–12)  using  primarily  a  thematic  qualitative  analysis  of  learning  episodes.  We  analyzed  how  children  used  problem-solving  strategies  to  identify  and  capture  the  tree  cycle  with  the  help  of  mobile  tablets.  We  found  that  our  mobile  learning  experience  and  its  external  representations  supported  the  following:  (1)  engagement  in  deep  learning  in  the  natural  setting  as  evidenced  by  coordinating  decisions  with  photographic  evidence;  (2)  use  of  procedural  or  tactical  strategies  to  approach  the  problem;  and  (3)  use  of  real-time  decision  making  strategies  about  tree  life  cycles.
2	Feeling  alone  among  317  million  others  disclosures  of  loneliness  on  twitter.  Abstract  Increasing  numbers  of  individuals  describe  themselves  as  feeling  lonely,  regardless  of  age,  gender  or  geographic  location.  This  article  investigates  how  social  media  users  self-disclose  feelings  of  loneliness,  and  how  they  seek  and  provide  support  to  each  other.  Motivated  by  related  studies  in  this  area,  a  dataset  of  22,477  Twitter  posts  sent  over  a  one-week  period  was  analyzed  using  both  qualitative  and  quantitative  methods.  Through  a  thematic  analysis,  we  demonstrate  that  self-disclosure  of  perceived  loneliness  takes  a  variety  of  forms,  from  simple  statements  of  “I'm  lonely”,  through  to  detailed  self-reflections  of  the  underlying  causes  of  loneliness.  The  analysis  also  reveals  forms  of  online  support  provided  to  those  who  are  feeling  lonely.  Further,  we  conducted  a  quantitative  linguistic  content  analysis  of  the  dataset  which  revealed  patterns  in  the  data,  including  that  ‘lonely’  tweets  were  significantly  more  negative  than  those  in  a  control  sample,  with  levels  of  negativity  fluctuating  throughout  the  week  and  posts  sent  at  night  being  more  negative  than  those  sent  in  the  daytime.
2	Effect  of  disfluency  on  learning  outcomes  metacognitive  judgments  and  cognitive  load  in  computer  assisted  learning  environments.  Abstract  Cognitive  challenges  that  are  presented  through  the  modification  of  established  design  principles  may  contribute  to  learning.  One  such  challenge  to  the  promotion  of  deeper  processing  is  the  Disfluency  Effect.  Specifically,  disfluency  manipulations  in  learning  materials  interfere  with  the  perceptional  fluency,  which  may  in  turn  lead  to  better  learning  outcomes.  This  likelihood  of  reaching  better  learning  outcomes  through  minor  instructional  modifications  has  led  scholars  to  investigate  the  construct  further.  Accordingly,  the  effect  of  fluency  modifications  on  learning  outcomes,  metacognitive  judgments  and  cognitive  load  is  investigated  in  the  current  study  with  a  true  experiment  conducted  with  292  undergraduate  students  who  were  assigned  randomly  to  one  of  the  six  disfluency  scenarios  in  a  computer-based  learning  environment.  Additional  variables  were  also  considered  such  as  the  working  memory  capacity,  prior  knowledge  and  cognitive  load.  Significant  differences  were  observed  with  regard  to  the  extraneous  cognitive  load,  while  there  was  no  variation  in  the  learning  outcomes  and  metacognitive  judgments  in  the  experimental  groups.  Moreover,  significant  relationships  were  observed  between  cognitive  load,  the  number  of  animations  watched  by  the  students  and  the  learning  outcomes.  The  results  were  interpreted  in  accordance  with  the  approach  in  contemporary  studies  into  the  Disfluency  Effect,  and  theoretical  and  practical  implications  are  discussed.
2	What  comes  with  technological  convenience  exploring  the  behaviors  and  performances  of  learning  with  computer  mediated  dictionaries.  As  technology  develops,  the  prevalence  of  conventional  book  dictionaries  has  slowly  declined  due  to  advancements  in  computer-mediated  aids,  such  as  online  type-in  dictionaries  and  program-installed  pop-up  aids.  The  goal  of  this  study  was  to  examine  how  technology  ''may''  have  changed  the  long-standing  pedagogical  practice  of  book  dictionary  usage  by  identifying  the  learning  processes  associated  with  various  dictionaries  and  verifying  how  these  processes  are  related  to  learning.  Cognitive  load  theory  was  applied  to  generate  predictions  about  learning  performance  and,  therefore,  to  determine  the  nature  of  these  processes.  Information  contained  in  each  dictionary  was  specifically  controlled,  and  thus  we  focused  on  the  effect  of  the  learning  process  alone.  In  the  experiment,  students  first  read  a  simulated  online  text  in  one  of  four  conditions:  pop-up,  type-in,  book  dictionaries  or  no  aid.  They  were  later  tested  for  reading  comprehension  and  vocabulary  learning.  Results  indicated  that  all  dictionaries  enhanced  vocabulary  learning  but  not  comprehension.  Close  examination  revealed  that  vocabulary-learning  efficiency  was  significantly  higher  for  the  pop-up  dictionary  than  the  other  two  aids.  In  addition,  a  complex  relationship  existed  between  reading  comprehension  and  vocabulary  learning.  This  study  has  important  implications  for  future  dictionary  design  and  pedagogical  advice  regarding  dictionary  usage.
2	Internet  specific  parental  self  efficacy  developmental  differences  and  links  to  internet  specific  mediation.  Abstract  Most  children  spend  significant  time  on  the  Internet  every  day,  and  parents  have  an  important  role  in  helping  their  children  to  avoid  negative  online  experiences.  In  this  study,  we  examine  the  potential  role  of  Internet-specific  parental  self-efficacy  (Internet-specific  PSE)  as  an  antecedent  for  Internet-specific  parenting  practices.  A  study  of  1025  parents  of  children  in  grades  6  (approximately  11–12  years)  to  12  (approximately  17–18  years)  allowed  us  to  examine  the  links  among  Internet-specific  PSE,  the  child's  grade  in  school,  and  Internet-specific  parenting  practices.  The  results  showed  developmental  decreases  in  Internet-specific  PSE  and  Internet-specific  parenting  practices:  Parents  of  older  adolescents  felt  less  efficacious  and  used  less  control-based  parenting  practices  than  did  parents  of  younger  adolescents.  Furthermore,  Internet-specific  PSE  was  a  significant  predictor  of  Internet-specific  parenting  practices  (both  communication-based  and  control-based  practices).  These  results  suggest  the  importance  of  both  parental  beliefs  and  children's  grade  in  school  for  parenting  in  the  area  of  children's  online  activities.
2	Alexithymia  impulsivity  disordered  social  media  use  mood  and  alcohol  use  in  relation  to  facebook  self  disclosure.  Abstract  The  tendency  to  disclose  personal  information  on  Facebook  has  been  examined  in  relation  to  the  broad  Big  Five  personality  factors  (extraversion,  openness,  neuroticism,  conscientiousness,  agreeableness),  but  the  potential  roles  of  more  specific  traits  such  as  alexithymia  and  impulsivity  are  not  known.  The  present  study  assessed  the  ability  of  these  two  traits,  along  with  indices  of  disordered  social  media  use,  alcohol  use,  negative  mood,  and  demographic  factors,  to  predict  Facebook  self-disclosure  in  a  hierarchical  regression  model.  The  study  recruited  157  Facebook-using  adults  aged  between  18  and  30  years  (M = 24.31  years),  of  whom  81  (51.6%)  identified  as  female,  from  across  Australia  via  the  online  survey  tool  Qualtrics.  Expected  significant  positive  correlations  of  Facebook  self-disclosure  with  alexithymia,  impulsivity,  disordered  social  media  use,  negative  mood  and  alcohol  use  were  obtained.  In  the  final  regression  model,  alexithymia  and  anxiety  were  the  strongest  predictors,  followed  by  alcohol  and  education;  disordered  social  media  use,  impulsivity,  depression,  stress,  age,  and  gender  were  not  significant.  Subsequent  analysis  revealed  that  of  the  three  facets  of  alexithymia,  only  difficulty  identifying  feelings  explained  variance  in  Facebook  self-disclosure.  Findings  are  interpreted  in  terms  of  the  social  compensation  hypothesis  and  recent  neuroimaging  evidence  of  blunted  brain  response  to  social  rejection  in  alexithymia.
2	Prevalence  and  personality  correlates  of  facebook  bullying  among  university  undergraduates.  The  purpose  of  the  present  study  was  to  examine  the  prevalence  of  cyber-bullying  through  Facebook  in  a  sample  of  226  Greek  university  undergraduates,  and  to  explore  whether  big  five  personality  characteristics,  narcissism,  as  well  as  attitudes  toward  Facebook,  technological  knowledge  and  skills  were  predictive  of  such  behavior.  Participants  completed  a  self-report  questionnaire  measuring  the  above  constructs.  Results  indicated  that  almost  one  third  of  the  sample  reported  Facebook  bullying  engagement  at  least  once  during  the  past  month,  with  male  students  reporting  more  frequent  involvement  than  females.  Bullying  through  Facebook  was  predicted  by  low  Agreeableness  and  more  time  spent  on  Facebook  only  for  males,  whereas  for  females  none  of  the  studied  variables  predicted  engagement  in  Facebook  bullying.  Findings  are  discussed  in  terms  of  prevention  and  intervention  strategies.  Self-reported  FB  bullying  was  examined  among  226  Greek  university  students.One  third  of  the  participants  had  bullied  through  FB  at  least  once.None  of  the  studied  variables  predicted  FB  bullying  for  females.Low  Agreeableness  and  time  spent  on  FB  predicted  males'  FB  bullying.
2	The  role  of  regulation  in  medical  student  learning  in  small  groups.  Computer  supported  collaborative  problem  based  learning  in  medicine  can  lead  to  high  levels  of  metacognition.High  co-regulation  in  problem  based  learning  co-occurs  with  levels  of  Interactive  Social  Presence.Co-regulatory  actions  that  activate  the  discussion  and  metacognitive  acts  of  planning.  This  study  examines  the  role  of  regulatory  processes  in  medical  students  as  they  learn  to  deliver  bad  news  to  patients  in  the  context  of  an  international  web-based  problem  based  learning  environment  (PBL).  In  the  PBL  a  medical  facilitator  and  students  work  together  to  examine  video  cases  on  giving  bad  news  and  share  their  perspectives  on  what  was  done  effectively  and  what  could  be  done  differently.  We  examine  how  regulation  occurs  within  this  collaboration.  A  synchronous  computer-supported  collaborative  learning  environment  (CSCL)  facilitated  peer  discussion  at  a  distance  using  a  combination  of  tools  that  included  video-conferencing,  chat  boxes,  and  a  shared  whiteboard  to  support  collaborative  engagement.  We  examine  regulation  along  a  continuum,  spanning  from  self-  to  co-regulation,  in  situations  where  medical  students  learn  how  to  manage  their  own  emotions  and  adapt  their  responses  to  patient  reactions.  We  examine  the  nature  of  the  discourse  between  medical  students  and  facilitators  to  illustrate  the  conditions  in  which  metacognitive,  co-regulation  and  social  emotional  activities  occur  to  enhance  learning  about  how  to  communicate  bad  news  to  patients.
2	Cyberbullying  the  hidden  side  of  college  students.  The  purpose  of  this  study  was  to  investigate  how  university  students  perceive  their  involvement  in  the  cyberbullying  phenomenon,  and  its  impact  on  their  well-being.  Thus,  this  study  presents  a  preliminary  approach  of  how  college  students'  perceived  involvement  in  acts  of  cyberbullying  can  be  measured.  Firstly,  Exploratory  Factor  Analysis  (N  =  349)  revealed  a  unidimensional  structure  of  the  four  scales  included  in  the  Cyberbullying  Inventory  for  College  Students.  Then,  Item  Response  Theory  (N  =  170)  was  used  to  analyze  the  unidimensionality  of  each  scale  and  the  interactions  between  participants  and  items.  RESULTS  revealed  good  item  reliability  and  Cronbach's  α  for  each  scale.  RESULTS  also  showed  the  potential  of  the  instrument  and  how  college  students  underrated  their  involvement  in  acts  of  cyberbullying.  Additionally,  aggression  types,  coping  strategies  and  sources  of  help  to  deal  with  cyberbullying  were  identified  and  discussed.  Lastly,  age,  gender  and  course-related  issues  were  considered  in  the  analysis.  Implications  for  researchers  and  practitioners  are  discussed.
2	Playing  for  social  comfort.  Online  video  game  (OVG)  play  is  believed  to  lead  to  losses  in  offline  sociability.The  current  study  assessed  the  impact  of  OVG  involvement  on  social  skill  outcomes.To  assess  causality,  the  assessment  was  done  through  the  lens  of  attachment  theory.Attachment  does  not  mediate  the  relationships  between  OVG  play  and  social  outcomes.OVG  spaces  seem  to  hold  the  potential  to  satisfy  needs  for  the  avoidantly  attached.  Internet  connectivity  has  changed  the  way  video  games  are  played  by  allowing  individuals  to  connect  worldwide  in  shared  gaming  spaces.  These  highly  social  environments  allow  players  to  connect,  interact  with,  and  learn  from  each  other.  However,  there  is  a  growing  concern  that  these  social  environments  also  have  the  potential  to  displace  real-world  connections  and  interactions,  contributing  to  a  variety  of  losses  in  'offline'  sociability.  The  current  study  aims  to  elucidate  what  users  may  be  gaining  or  losing  (socially)  as  a  result  of  continued  participation  in  online  video  game  environments,  and  what  potentially  underlies  these  social  changes,  by  examining  the  associations  between  social  skills  and  online  video  game  involvement  through  the  perspective  of  attachment  theory.  The  results  challenge  the  assumption  that  online  video  game  play  is  inexorably  associated  with  negative  social  consequences  for  the  player  and  indicates  the  potential  for  online  gaming  spaces  to  serve  critical  attachment  functions  by  providing  a  social  outlet  that  promotes  a  sense  of  closeness,  belonging,  and  security  that  satisfies  attachment  needs  for  those  high  in  attachment  avoidance.
2	Sector  diversity  in  green  information  technology  practices.  Green  IT  among  IT  Professionals  in  public  and  private  sector  is  investigated.Technology  Acceptance  Model  (TAM)  supports  Green  IT.Subjective  norms,  (SN)  appears  to  be  the  only  factor,  which  indicates  sector  differences.External  factors  have  significant  effect  in  Green  IT  acceptance.  This  paper  examines  the  existence  of  diversity  between  public-and  private-sector  establishments  in  Green  Information  Technology  (GIT)  adoption  using  the  'Technology  Acceptance  Model'  (TAM).  In  this  study,  GIT  simply  refers  to  using  IT  in  ways  that  help  to  reduce  environmental  impacts,  which  include  using  energy  more  efficiently  and  reducing  waste.  The  model  is  extended  to  include  the  external  variables  as  subjective  norm  and  the  level  of  GIT  awareness.  For  this  purpose,  a  survey  was  conducted  among  professionals  from  public-  and  private-sector  establishments.  The  findings  suggest  the  following:  (1)  Diversity  exists  among  establishments  from  public-  and  private-sectors  in  the  influence  of  the  Perceived  Ease-of-Use  (PEU)  on  Perceived  Usefulness  (PU)  and  on  the  Attitude  Towards  Use  (ATU);  (2)  Most  of  the  public-sector  professionals  have  concerns  for  environmental  sustainability  in  using  IT;  (3)  TAM  is  an  important  tool  for  investigating  the  specific  barriers  and  facilitators  of  environmental  behavior  at  work;  (4)  TAM  has  a  have  significant  predictive  power  in  public  -sector  establishments;  and  (5)  TAM  is  significant  for  private-sector  establishments  except  the  relations  between  the  PEU  and  PU,  and  PEU  and  ATU.
2	A  longitudinal  study  of  the  association  between  compulsive  internet  use  and  wellbeing.  Compulsive  Internet  use  predicted  a  decrease  in  happiness  in  adults.Happiness  also  predicted  a  decrease  in  Compulsive  Internet  use  over  time.Compulsive  Internet  use  predicted  increases  in  depression,  loneliness  and  stress.Compulsive  Internet  use  did  not  change  self-esteem  in  adults  over  time.  ObjectiveCompulsive  Internet  Use  (CIU)  has  been  linked  to  lower  wellbeing,  especially  among  adolescents.  Yet,  questions  regarding  the  directionality  of  this  association  remain  unanswered:  CIU  may  influence  wellbeing  and  vice  versa.  Theoretically,  both  directions  are  plausible,  yet  so  far  no  studies  have  examined  the  directionality  of  these  effects  among  adults.  This  article  aims  to  shed  light  on  the  directionality  of  the  relation  between  CIU  and  both  positive  and  negative  wellbeing,  using  a  prospective,  longitudinal  sample  of  adults  (n=398).  MethodsOver  the  course  of  four  years,  participants  completed  five  assessments  of  their  CIU  and  both  positive  and  negative  indicators  of  wellbeing.  Participants  were  married  couples  who  were  recruited  in  the  municipalities  where  they  were  married.  ResultsCIU  predicted  increases  in  depression,  loneliness  and  stress  over  time,  and  a  decrease  in  happiness.  No  effect  of  CIU  on  the  change  in  self-esteem  was  found.  Further,  happiness  predicted  a  decrease  in  CIU  over  time.  ConclusionsThe  results  suggest  CIU  lowers  wellbeing.  This  is  important  given  that  lowered  wellbeing  may  affect  health.  Happiness  is  suggested  to  be  a  buffer  for  developing  CIU.
2	How  college  students  read  and  write  on  the  web  the  role  of  ict  use  in  processing  online  information.  The  diffusion  of  information  and  communication  technology  (ICT)  has  enabled  people  to  process  more  information  than  at  any  time  in  human  history.  Despite  a  growing  body  of  scholarship  in  ICT  use  and  information  processing,  we  still  know  very  little  about  how  people  process  mediated  information  in  an  online  environment.  This  study  contributes  to  the  understanding  of  this  process  by  investigating  the  connection  between  ICT  use  and  processing  of  online  news  information.  Through  an  experiment  (N=114),  several  interesting  relationships  were  detected.  First,  perceived  credibility  of  a  news  article  was  significantly  correlated  with  enjoyment,  knowledge  gain,  and  motivation.  In  addition,  recall  was  significantly  related  to  credibility,  enjoyment,  knowledge,  and  motivation.  Implications  and  avenues  for  future  research  are  discussed.
2	Learning  analytics  messages  impact  of  grade  sender  comparative  information  and  message  style  on  student  affect  and  academic  resilience.  Abstract  Learning  analytics  enable  automated  feedback  to  students  through  dashboards,  reports  and  alerts.  The  underlying  untested  assumption  is  that  providing  analytics  will  be  sufficient  to  improve  self-regulated  learning.  Working  within  a  feedback  recipience  framework,  we  begin  to  test  this  assumption  by  examining  the  impact  of  learning  analytics  messages  on  student  affect  and  academic  resilience.  Three  hundred  and  twenty  undergraduate  students  completed  an  online  survey  and  were  exposed  to  three  randomly  assigned  learning  analytics  alerts  (High  Distinction,  Pass,  and  Fail  grades).  Multivariate  analyses  of  variance  indicated  significant  differences  between  grade  levels  (large  effects),  with  higher  positive  affect  and  lower  resilience  in  response  to  High  Distinction  alerts  than  Pass  or  Fail  alerts.  Within  each  hypothetical  grade  level,  there  were  no  differences  in  student  affect  and  academic  resilience.  Based  upon  systematic  changes  in  feedback  sender,  message  style  or  whether  comparative  peer  achievement  was  included  or  not.  These  findings  indicate  that  grade  level  has  the  largest  impact  on  both  affect  and  academic  resilience.  The  failure  of  message  and  sender  characteristics  to  impact  on  activities  that  promote  self-regulated  learning  suggests  we  need  to  look  beyond  these  characteristics  of  individual  messages  to  identify  drivers  of  engaging  students  in  self-regulated  learning.
2	Creative  identity  re  construction  creative  community  building  and  creative  resistance  a  qualitative  analysis  of  queer  ingroup  members  tweets  after  the  orlando  shooting.  Abstract  With  the  proliferation  of  social  media,  a  growing  body  of  literature  explores  the  significance  of  virtual  grief,  social  support,  and  social  capital  in  the  construction  of  identity  and  community  online.  However,  there  is  a  dearth  of  research  on  how  queer  members  use  social  media,  particularly  Twitter,  as  a  tool  to  claim  and  re-define  their  identity—especially  when  their  identity  is  under  threat.  The  goal  of  this  study  is  to  understand  how  members  of  the  queer  community  linguistically  responded  to  the  threat  to  their  identity  and  safety  that  was  caused  by  the  shooting  that  took  place  on  June  12,  2016,  at  the  Pulse  Nightclub  in  Orlando,  Florida.  To  achieve  this  goal,  we  use  Social  Identity  Theory  (SIT)  as  a  framework  to  examine  how  queer  individuals  use  a  creative  language  strategy  to  redefine  their  shared  identity  when  it  is  threatened.  This  study  used  a  qualitative  iterative  analysis  of  tweets  posted  by  queer  individuals  to  determine  how  this  group  identified  as  ingroup  after  the  Orlando  shooting.  The  study  reveals  that  members  of  the  queer  community  communicatively  responded  to  the  Pulse  shooting  through  social  creativity.  Specifically,  members  of  the  queer  community  responded  to  threats  through  using  creative  language  that  fostered  stronger  ingroup  collaborative  identities  by  provoking  a)  creative  identity  (re)construction,  b)  creative  community  building  and  c)  creative  resistance.  This  study  advances  SIT  by  providing  additional  evidence  that  the  strategy  of  using  creative  language  by  a  minority  ingroup  has  the  ability  to  support  simultaneously  salient  and  threatened  identity.
2	Incivility  on  facebook  and  political  polarization  the  mediating  role  of  seeking  further  comments  and  negative  emotion.  Abstract  This  study  examined  whether  and  how  (in)civility  and  the  presence  of  supporting  evidence  in  disagreeing  comments  influence  individuals'  attitude  polarization.  The  study  used  a  2  (civility  vs.  incivility) × 2  (evidence  vs.  no  evidence)  factorial  design  involving  reading  dissimilar  viewpoints  in  Facebook  comments.  The  results  showed  that  exposure  to  uncivil  opposing  comments,  compared  to  exposure  to  civil  disagreeing  comments,  led  to  lower  levels  of  willingness  to  read  more  comments  and  greater  levels  of  negative  emotions  and  attitude  polarization.  However,  the  presence  or  absence  of  supporting  evidence  in  comments  did  not  have  any  significant  effect  on  the  outcome  variables.  The  findings  suggest  that  it  is  the  civility  or  incivility  of  information  that  influences  whether  exposure  to  dissimilar  perspectives  either  mitigates  or  reinforces  individuals’  attitude  polarization.  This  study  also  suggested  willingness  to  read  more  comments  and  negative  emotions  as  two  mediating  factors  between  exposure  to  uncivil/civil  disagreeing  comments  and  attitude  polarization.
2	The  concurrent  and  longitudinal  relationships  between  adolescents  use  of  social  network  sites  and  their  social  self  esteem.  Abstract      The  first  aim  of  this  study  was  to  investigate  the  concurrent  and  longitudinal  relationships  between  adolescents'  use  of  social  network  sites  (SNSs)  and  their  social  self-esteem.  The  second  aim  was  to  investigate  whether  the  valence  of  the  feedback  that  adolescents  receive  on  SNSs  can  explain  these  relationships.  We  conducted  a  three-wave  panel  study  among  852  pre-  and  early  adolescents  (10–15  years  old).  In  line  with  earlier  research,  we  found  significant  concurrent  correlations  between  adolescents'  SNS  use  and  their  social  self-esteem  in  all  three  data  waves.  The  longitudinal  results  only  partly  confirmed  these  concurrent  findings:  Adolescents'  initial  SNS  use  did  not  significantly  influence  their  social  self-esteem  in  subsequent  years.  In  contrast,  their  initial  social  self-esteem  consistently  influenced  their  SNS  use  in  subsequent  years.  The  valence  of  online  feedback  from  close  friends  and  acquaintances  explained  the  concurrent  relationship  between  SNS  use  and  social  self-esteem,  but  not  the  longitudinal  relationship.  Results  are  discussed  in  terms  of  their  methodological  and  theoretical  implications.
2	Living  in  the  hutt  space  immersive  process  in  the  star  wars  role  play  community  of  second  life.  Immersive  virtual  settings  are  evolving  to  become  new  ''spaces  of  life''.  Humans  inhabit  these  different  virtual  worlds  through  their  avatars,  and  tend  to  gather  into  communities.  However,  the  behavioral  factors  underlying  the  cognitive  process  of  immersion  in  virtual  worlds  are  still  far  to  be  understood.  We  here  investigated  these  factors  using  the  Star  Wars  Role-Play  community  of  the  virtual  setting  of  Second  Life  as  a  model.  More  specifically,  our  studies  focused  on  the  immersion  process  in  the  ''Hutt  Space'',  a  portion  of  the  Star  Wars  Galaxy  ruled  by  the  alien  species  of  the  Hutts,  which  combines  the  trademark  aspects  of  Star  Wars  universe.  Using  both  quantitative  and  qualitative  methods,  we  identified  some  of  the  factors  which  favor  the  immersion  process.  Our  results  suggest  that  the  different  behavioral  factors  contributing  to  the  immersion  process  can  be  organized  in  three  structuring  dimensions:  commitment,  cohesion,  and  coherence.  We  also  unveil  a  compensatory  mechanism  between  appearance  and  behavioral  factors  in  creation  and  maintenance  of  social  groups  in  virtual  worlds.  Finally,  we  point  out  some  of  the  behavioral  aspects  of  the  evolution  from  passive  media  engagement  (spectators),  to  active  media  engagement  (actors),  and  suggest  a  theoretical  framework  to  investigate  how  human  inhabit  immersive  virtual  spaces.
2	Demonstrating  the  validity  of  the  video  game  functional  assessment  revised  vgfa  r.  Problematic  video  play  has  been  well  documented  over  the  course  of  the  last  decade.  So  much  so  the  DSM-5  (APA,  2013)  has  included  problematic  video  gaming  as  disorder  categorized  as  Internet  Gaming  Disorder.  The  field  of  applied  behavior  analysis  has  been  utilizing  functional  assessments  for  the  last  30  years  and  has  showed  evidence  of  effective  results  across  different  populations  and  environments.  Therefore,  the  purpose  of  this  investigation  (comprising  three  studies)  was  to  validate  an  indirect  functional  assessment  entitled  the  Video  Game  Functional  Assessment-Revised  (VGFA-R).  Using  academic  experts  in  the  field  of  video  game  addiction  and  applied  behavioral  analysis  (n?=?6),  the  first  study  examined  the  content  validity  of  the  VGFA-R  and  was  able  to  demonstrate  the  assessment  exceeded  the  criterion  for  an  established  assessment.  A  second  study  comprising  a  survey  of  467  gamers  examined  the  factorability  by  using  a  confirmatory  factor  analysis,  and  found  that  VGFA-R  had  an  overall  variance  above  .60.  Within  the  third  laboratory-based  study  using  gamers  (n?=?11),  the  VGFA-R  was  examined  for  construct  validity  and  found  the  VGFA-R  was  able  to  predict  85%  of  the  appropriate  function  of  behavior.  Implications  of  the  study  are  discussed  along  with  the  strengths  and  limitations  of  the  study  and  future  research  directions.  We  validated  and  standardized  an  indirect  functional  assessment  on  video  gaming.The  assessment  examines  the  motivation  of  individual's  video  game  playing.Attention,  escape,  and  tangible  functions  are  the  ?three  primary  maintaining  functions.The  VGFA-R  can  benefit  therapists  providing  therapy  for  internet  gaming  disorder.
2	Design  and  evaluation  of  hospital  based  business  intelligence  system  hbis.  This  paper  describes  the  development  of  a  hospital-based  business  intelligent  system  (HBIS)  based  on  a  novel  developmental  methodology,  called  the  design  science  research  methodology  (DSRM),  and  implemented  in  a  regional  general  hospital  in  Taiwan.  A  design  science  research  methodology  is  adopted  to  cover  six  activities:  problem  identification  and  motivation,  definition  of  solution  objectives,  design  and  development,  demonstration,  evaluation,  and  communication.  Based  on  the  DSRM  developmental  method,  HBIS  was  successfully  developed  and  deployed  in  the  hospital  case,  and  a  survey  of  users  shows  positive  results.  In  addition,  the  support  and  involvement  of  top  management  in  HBIS  development  is  found  to  be  a  critical  success  factor,  and  system  implementation  allowed  the  hospital  to  significantly  improve  performance  of  managerial  indicators  for  the  three  abovementioned  dimensions.  This  study  contributes  a  novel  developmental  methodology  from  the  Information  Systems  (IS)  field  as  a  reference  model  for  future  HBIS  development,  along  with  the  integration  of  indicators  from  three  major  managerial  dimensions  -  NHI,  hospital  accreditation,  and  healthcare  quality.  A  hospital-based  business  intelligent  system  (HBIS)  is  presented.Reporting  the  development  and  assessment  of  a  HBIS  system.Identifying  the  critical  factors  to  the  development  of  HBIS.Proposing  tools  that  improve  the  decision-making  of  health  system  administrators.
2	A  study  on  the  effects  of  model  based  inquiry  pedagogy  on  students  inquiry  skills  in  a  virtual  physics  lab.  STEM  integrated  education  has  become  the  guiding  principle  of  science  education  in  many  countries  and  a  focus  of  research  efforts.  Developmental  features  of  STEM  education  focus  on  using  technology  as  a  bridge  to  integrate  multiple  subjects.  The  focus  on  new  technologies  and  practical  applications  are  its  major  principles,  and  the  aim  of  STEM  education  is  to  train  a  new  generation  of  multi-skilled  professionals  capable  of  integrating  knowledge  from  different  fields  of  study  to  solve  problems  effectively.  High  school  science  courses  based  upon  technological  science  models  and  science  investigations  have  become  the  major  means  and  methods  for  STEM  education.  For  the  past  one  hundred  years,  efforts  of  elementary  education  reforms  worldwide  have  been  focused  on  scientific  inquiry.  The  development,  utilization,  evaluation,  and  revision  of  various  scientific  models  and  theories  play  a  central  role  in  scientific  inquiry.  Therefore,  model-based  inquiry  would  be  crucial  in  improving  the  learning  of  science  subjects.  This  study  is  based  upon  results  from  past  MBI  pedagogies  research  carried  out  by  renowned  academicians  worldwide  and  incorporated  a  virtual  physics  lab  developed  for  this  study  to  create  the  MBI-VPL  pedagogy  method.  Six  main  learning  modules  were  designed,  namely  (1)  topic  introduction,  (2)  hands-on  experiment,  (3)  virtual  experiment,  (4)  team  work,  (5)  actual  applications,  and  (6)  model  adjustments.  Results  of  experimental  teaching  showed  that  MBI  and  MBI-VPL  pedagogy  were  more  effective  in  developing  student  scientific  inquiry  skills  compared  to  traditional  methods,  with  significant  improvements  in  the  performance  of  process  skills,  comprehensive  skills,  learning  attitude,  communication  skills,  and  reflection  skills.  The  MBI-VPL  pedagogy  was  able  to  introduce  virtual  physics  experiment  design  and  analysis,  allowing  students  to  gain  in-depth  practice  of  process  skills,  comprehensive  skills,  and  reflection  skills  of  scientific  inquiry.  Differences  were  also  observed  in  the  development  of  scientific  inquiry  skills  during  the  experimental  course  between  students  of  different  genders.  Boys  performed  better  in  process  skills  and  comprehensive  skills,  while  girls  performed  better  in  learning  attitude  and  communications.  The  degree  of  student  acceptance  for  the  six  major  learning  modules  in  the  MBI-VPL  model  also  showed  that  students  tend  to  accept  the  use  of  process,  comprehensive,  and  reflective  skills  of  the  virtual  experiment.
2	Knowledge  contribution  behavior  in  online  q  a  communities  an  empirical  investigation.  Abstract  As  social  networks  and  media  technology  combine,  online  Q&A  communities  are  playing  an  important  role  in  satisfying  people’s  knowledge  exchange  needs.  However,  the  combination  of  social  networks  and  media  technology  does  not  mean  that  an  exchange  of  knowledge  will  occur.  Users  are  the  critical  components  of  an  online  Q&A  community,  and  their  active  participation  is  critical  for  its  development.  Thus,  it  is  crucial  to  figure  out  factors  impacting  users’  motivation  to  participate  in  community  activities,  especially  knowledge  contribution.  In  this  study,  we  focus  on  user  behavior  regarding  knowledge  contribution  in  social  Q&A  communities,  and  differentiate  users’  initial  participation  behavior  from  continued  participation  behavior.  We  collect  users’  activity  data  from  a  well-known  Chinese  social  Q&A  community,  and  the  results  show  that  identity-based  trust,  feedback  from  previous  knowledge  contribution,  opportunities  of  social  exposure,  word-of-mouth  marketing,  and  pressure  from  norms  of  reciprocity  have  a  positive  impact  on  users’  continued  knowledge  contribution  behavior.
2	Who  needs  social  networking  an  empirical  enquiry  into  the  capability  of  facebook  to  meet  human  needs  and  satisfaction  with  life.  Abstract  Social  Network  Sites  (SNS)  have  been  the  topic  of  much  scholarly  and  public  debate  for  the  most  part  of  the  21st  century.  A  number  of  studies  have  investigated  the  benefits  and  drawbacks  to  using  SNS,  with  Facebook  the  largest  example  boasting  billions  of  active  monthly  users.  In  recent  months,  media  commentary  has  raised  a  number  of  concerning  cases  surrounding  Facebook's  use  of  data,  its  connection  with  other  organizations  and  its  legitimacy,  making  a  number  of  open  public  calls  to  abandon  the  platform.  However,  active  users  still  number  in  the  billions,  raising  the  question,  “does  Facebook  achieve  something  on  a  fundamental  human  and  social  level  that  users  are  willing  to  overlook  the  potential  drawback  to  its  use?”  Using  Maslow's  needs  hierarchy,  this  study  adopts  a  survey  approach  (n = 316)  and  explores  the  capacity  for  Facebook  to  satisfy  human  needs.  Findings  identify  Facebook  as  a  useful  tool  to  fulfil  human  needs,  which  predict  continued  Facebook  use  intentions  of  participants,  and  further,  satisfaction  with  life.  These  findings  offer  a  broad-based  view  of  use  and  its  resonance  with  key  motivators  of  behavior,  supporting  both  Maslow's  needs  hierarchy  and  highlighting  the  importance  of  need  fulfilment  for  continued  service  use  and  satisfaction  with  life.
2	Effects  of  visual  feedback  on  medical  students  procrastination  within  web  based  planning  and  reflection  protocols.  We  evaluated  the  effects  of  visual  feedback  on  procrastination.We  designed  two  experimental  longitudinal  studies  with  between-subject  design.A  graphical  visualization  of  procrastination  caused  decreased  procrastination.The  visualization  improved  self-regulated  learning  beyond  procrastination.The  main  effect  of  the  visual  feedback  was  raised  metacognitive  awareness.  Procrastination  is  a  very  common  problem  among  students  that  results  from  ineffective  selfregulation.  In  two  field-experimental  studies  (N=18  and  N=49),  we  investigated  whether  visual  feedback  on  students'  previous  procrastination  was  effective  in  provoking  a  decrease  in  students'  future  procrastination  as  well  as  improvements  in  self-regulated  learning.  The  visual  feedback  was  implemented  as  a  dynamic  line  chart  in  a  web-based  planning  and  reflection  protocol  used  once  a  week  by  medical  students  to  record  their  class  preparation  and  homework  once  a  week.  In  the  protocols,  the  students  planned  and  reflected  on  their  personal  learning  processes  and  they  estimated  retrospectively  their  inclination  to  procrastinate.  The  results  of  both  studies  consistently  showed  that  presenting  students  a  line  chart  that  adaptively  visualizes  the  course  and  extent  of  their  self-reported  previous  procrastination  led  to  a  statistically  significant  and  practically  relevant  decrease  in  their  future  procrastination.  Furthermore,  the  visualization  had  positive  effects  on  other  variables  central  to  self-regulated  learning.  The  studies  provide  converging  evidence  that  the  inclination  to  procrastinate  can  successfully  be  counteracted  both  by  a  parsimonious  and  easy-to-implement  method.  They  are  suggestive  of  ways  how  Internet  technology  can  be  used  support  students'  self-regulated  learning.
2	Cyberbullying  in  social  networking  sites.  We  provide  the  views  of  adolescent  victims  of  cyberbullying  in  social  networking  sites.Victims'  definition  of  cyberbullying  is  different  and  more  complex  than  presented  in  previous  research.Victim  impact  was  the  most  important  criterion.  No  participant  referenced  power  imbalance.Emotional,  social  and  behavioural  impacts  were  the  most  reported  by  victims.  Online  social  networking  sites  (SNS)  are  a  ubiquitous  platform  for  communication.  However,  SNS  can  provide  opportunities  for  abuse  and  harassment,  typically  referred  to  as  cyberbullying.  The  current  study  examined  adolescent  victims'  understanding  of  cyberbullying,  the  specific  types  of  cyberbullying  events  experienced  in  SNS  and  the  impact  of  these  events.  Twenty-five  adolescents  (15-24years  old)  who  responded  to  an  invitation  for  participants  with  previous  negative  experiences  in  SNS  took  part  in  individual  semi-structured  interviews.  Results  showed  that  the  basic  criteria  for  the  definition  of  cyberbullying  published  in  previous  research  were  either  not  referenced  by  participants,  or  they  were  more  complex  than  initially  anticipated.  The  most  referenced  criterion  was  the  extent  to  which  the  experience  had  an  impact  on  the  victim,  which  is  not  a  current  definitional  criterion.  It  was  also  found  that  68%  of  victims  reported  experiencing  a  combined  emotional,  social  and  behavioural  impact  for  each  cyberbullying  experience,  and  12%  reported  no  impact  at  all.  These  findings  will  contribute  to  the  measurement  of  cyberbullying  from  the  perspective  of  victims,  and  will  also  aid  the  development  of  intervention  strategies  based  on  the  most  common  impact  areas.
2	Collaborative  argumentation  and  justifications  a  statistical  discourse  analysis  of  online  discussions.  As  justifications  (such  as  evidence  or  explanations)  are  central  to  productive  argumentation,  this  study  examines  how  the  discourse  moves  of  students  engaged  in  collaborative  learning  are  related  to  their  justifications  during  computer  mediated  communication  (CMC).  Twenty-four  students  posted  131  messages  on  Knowledge  Forum,  an  online  collaborative  learning  environment.  These  messages  were  coded  and  analyzed  with  a  multiple  outcome,  multilevel  logit,  vector  autoregression.  When  students  disagreed  or  made  claims,  they  were  more  likely  to  use  evidence.  After  a  student  made  an  alternative  claim,  the  next  student  posting  a  message  was  less  likely  to  use  evidence.  When  students  made  claims,  disagreed,  disagreed  with  other's  justifications,  or  read  more  messages,  they  were  more  likely  to  use  explanations.  Boys  were  more  likely  than  girls  to  make  new  claims.  Together,  these  results  suggest  that  discourse  moves  and  sequences  are  linked  to  justifications  on  online  forums.
2	Usability  guideline  for  banking  software  design.  As  the  diversity  of  services  in  the  financial  market  increases,  it  is  critical  to  design  usable  banking  software  in  order  to  overcome  the  complex  structure  of  the  system.  The  current  study  presents  a  usability  guideline  based  on  heuristics  and  their  corresponding  criteria  that  could  be  used  during  the  early  stages  of  banking  software  design  process.  In  the  design  of  a  usability  guideline,  the  heuristics  and  their  criteria  are  categorized  in  terms  of  their  effectiveness  in  solving  usability  problems  grouped  and  ranging  from  usability  catastrophe  to  cosmetic  problems.  The  current  study  comprises  of  three  main  steps:  First,  actual  usability  problems  from  three  banking  software  development  projects  are  categorized  according  to  their  severity  level.  Secondly,  usability  criteria  are  rated  for  how  well  they  explain  the  usability  problems  encountered.  Finally,  usability  heuristics  are  categorized  according  to  the  severity  level  of  usability  problems  through  two  analytical  models;  corresponding  and  cluster  analyses.  As  the  result,  designers  and  project  managers  may  give  more  importance  to  the  heuristics  related  with  the  following  usability  problem  categories:  Usability  catastrophe  and  then  major  usability  problems.  Furthermore,  the  proposed  guideline  can  be  used  to  understand  which  usability  criteria  would  be  helpful  in  explaining  usability  problems  as  well  as  preventing  banking  system  catastrophes,  by  highlighting  the  critical  parts  in  system  design  of  banking  software.  A  usability  guideline  is  developed  for  the  early  stages  of  banking  software  design.Actual  usability  problems  from  three  banking  software  development  projects  are  used.The  heuristics  are  categorized  in  solving  usability  problems.Designers  may  notice  first  to  the  heuristics  related  with  usability  catastrophe.
2	The  role  of  social  identity  and  online  social  capital  on  psychosocial  outcomes  in  mmo  players.  Previous  literature  has  found  inconsistent  relationships  between  online  gaming  engagement  and  psychosocial  outcomes.  To  add  clarity  to  this  discussion,  we  explored  these  relationships  though  a  multidimensional  lens  of  gaming  engagement.  That  is,  we  examined  the  role  of  gamer  identity  and  online  social  capital  as  mediators  of  online  gaming  engagement  and  psychosocial  outcomes  (i.e.  self-esteem,  loneliness,  social  competence).  We  addressed  this  in  a  sample  of  Massively  Multiplayer  Online  (MMOs)  players  (N  =  708),  via  an  online  questionnaire  to  establish  cross-sectional  associations.  Findings  revealed  positive  relationships  between  MMO  engagement  (measured  by  a  multidimensional  measure),  gamer  identity,  and  online  social  capital.  Additionally,  gamer  identity  related  positively  to  self-esteem  and  social  competence,  and  negatively  with  loneliness.  Differential  outcomes  were  also  found  between  social  capital  and  loneliness.  Specifically,  loneliness  was  negatively  related  to  online  bonding,  but  positively  with  online  bridging  capital,  highlighting  the  importance  of  exploring  the  constitution  of  gaming  communities  to  assess  their  role  in  promoting  varying  dimensions  of  social  capital  and  the  associated  psychological  correlates.  Taken  together,  we  evidence  the  psychosocial  benefits  of  MMO  engagement,  specifically  in  relation  to  the  social  value  of  identifying  and  connecting  with  others  in  MMOs.  Additionally,  we  highlight  the  complexities  surrounding  the  concept  and  measurement  of  gaming  engagement.
2	Cell  phones  during  nonwork  time.  Technology  (e.g.,  cell  phones)  is  increasingly  blurring  the  lines  between  the  work  and  nonwork  domains.  Evidence  suggests  technology  users  experience  both  negative  and  positive  outcomes  associated  with  work-related  technology  use  during  nonwork  hours.  We  extended  the  job  demands-resources  model  (Demerouti,  Bakker,  Nachreiner,  &  Schaufeli,  2001)  to  technology  use  by  conceptualizing  work-related  cell  phone  (WRCP)  use  as  a  job  demand  and  cell  phone  attachment  -valuing  and  being  physically  attached  to  a  cell  phone-  as  a  resource.  We  expected  high  cell  phone  attachment  will  buffer  against  the  negative  effects  of  WRCP  use  on  emotional  exhaustion,  work  engagement,  and  work-family  conflict.  Participants  from  various  occupations  (N?=?313)  responded  to  two  online  surveys  administered  one  week  apart.  Cell  phone  use  and  attachment  were  assessed  at  Time  1;  criteria  were  assessed  at  Time  2.  High  cell  phone  attachment  buffered  against  the  negative  effects  of  WRCP  use  on  emotional  exhaustion  and  work-family  conflict,  and  it  enhanced  the  beneficial  effects  of  WRCP  use  on  work  engagement.  Being  more  engaged  and  attached  to  cell  phones  may  help  employees  deal  with  WRCP  use  during  nonwork  time  more  effectively.  Practical  implications  include  providing  training  for  more  effective  cell  phone  use  during  nonwork  time.  Extends  the  job  demands-resources  model  to  examine  effects  of  cell  phone  use  for  work.We  conceptualize  cell  phone  attachment  as  a  resource.Cell  phone  attachment  minimizes  negative  effects  of  work-related  cell  phone  use.Work  engagement  is  enhanced  by  being  more  attached  to  cell  phones.
2	Is  there  a  link  between  media  multitasking  and  the  executive  functions  of  filtering  and  response  inhibition.  Media-multitasking  refers  to  utilising  at  least  two  forms  of  media  simultaneously.  This  study  examined  the  link  between  media-multitasking  and  the  inhibitory  control  executive  function.  Performance  on  measures  of  filtering  (flanker  task)  and  inhibitory  response  control  (Go/No-Go  task)  were  compared  across  heavy  media  multitaskers  (HMM),  average  media  multitaskers  (AMM)  and  light  media  multitaskers  (LMM).  For  both  tasks  performance  was  better  under  low  than  high  perceptual  load  conditions.  For  the  flanker  task,  there  was  an  effect  of  distractor  congruency  only  for  the  low  perceptual  load  and  no  performance  differences  between  the  media  groups.  For  the  Go/No-Go  task,  there  was  a  distractor  congruency  effect  for  the  LMM  and  HMM  for  the  Go-trials.  For  the  No-Go  trials  there  was  no  difference  between  the  groups  performance  in  the  low  perceptual  load  condition.  The  AMM  were  more  adversely  affected  by  the  higher  perceptual  load  than  the  LMM  or  HMM  particularly  when  the  distractors  were  incongruent  and  neutral  relative  to  the  targets.  These  results  suggest  a  link  between  average  levels  of  media-multitasking  and  the  inhibitory  control  executive  function.  Distractor  filtering  is  not  linked  to  levels  of  media-multitasking.Average  media-multitaskers  have  poorer  inhibitory  response  control.Average  media-multitaskers  are  most  affected  by  increased  perceptual  task  load.
2	Game  transfer  phenomena  and  its  associated  factors.  Previous  research  into  Game  Transfer  Phenomena  (GTP)  has  mainly  been  qualitative.This  is  the  first  survey  to  examine  GTP  and  associated  factors  in  2362  gamers.Most  gamers  (97%)  reported  having  experienced  GTP.Factors  associated  with  GT  included  age,  being  a  student,  having  a  medical  condition,  and  drug  use.Implications  for  understanding  the  effects  of  GTP  among  gamers  are  discussed.  Previous  qualitative  and  quantitative  studies  examining  Game  Transfer  Phenomena  (GTP)  have  demonstrated  that  GTP  experiences  are  common.  These  studies  have  shown  that  many  gamers  report  altered  perceptions,  involuntary  thoughts  and  behaviors  after  playing  video  games  (e.g.,  pseudo-hallucinatory  experiences,  automatic  motor  activations,  etc.).  However,  the  factors  associated  with  GTP  are  unknown.  In  the  present  study,  a  total  of  2362  gamers  were  surveyed  using  an  online  questionnaire  to  examine  the  relationship  between  GTP  and  socio-demographic  factors,  gaming  habits,  individual  characteristics,  and  motivations  for  playing.  Results  showed  that  having  a  pre-existing  medical  condition,  playing  for  3-6h,  and  playing  for  immersion,  exploration,  customization,  mechanics  and  escape  from  the  real  world  were  significantly  associated  with  having  experienced  GTP.  Those  who  were  33-38years  old,  playing  sessions  for  less  than  one  hour,  being  a  professional  player,  being  self-employed,  and  never  recalling  dreams,  were  significantly  more  likely  to  have  not  experienced  GTP.  The  findings  suggest  that  attention  should  be  paid  to  young  adults  and  the  length  of  gaming  sessions,  as  well  as  taking  into  consideration  underlying  factors  such  as  medical  conditions  that  may  make  gamers  more  prone  to  GTP.
2	A  revised  examination  of  the  dual  pathway  model  for  bulimic  symptoms.  ObjectiveTo  replicate  the  Dual  Pathway  Model  (DPM)  of  bulimia  nervosa  (BN)  symptoms  prospectively,  and  to  assess  whether  a  revised  version  of  the  DPM  that  included  the  variables  social  comparisons  made  on  Facebook  and  sociotropy  influenced  the  DPM.  MethodParticipants  were  245  females  who  completed  baseline  measures  (T1)  that  assessed  the  DPM,  as  well  as  the  constructs  social  comparisons  made  on  Facebook,  and  sociotropy,  and  a  follow-up  questionnaire,  which  assessed  symptoms  of  depression,  bulimia,  and  dietary  restraint,  one  month  later  (T2).  ResultsPath  analysis  revealed  that  the  original  and  the  revised  DPMs  had  excellent  fit  once  modifications  to  the  respective  models  were  made.  In  both  DPMs,  T1  pressures  to  be  thin  and  T1  thin  ideal  internalization  were  related  to  T1  body  dissatisfaction.  T1  body  dissatisfaction  prospectively  predicted  T2  depressive  symptoms  and  T2  bulimic  symptoms,  but  not  T2  dietary  restraint.  Furthermore,  T2  dietary  restraint,  but  not  T2  depressive  symptoms,  predicted  T2  BN  symptoms.  Results  also  showed  that  T2  dietary  restraint  was  associated  with  T2  depressive  symptoms.  In  the  revised  DPM,  T1  social  comparisons  made  on  Facebook  were  associated  with  T1  body  dissatisfaction,  T1  pressures  to  be  thin,  and  T2  bulimic  symptoms.  T1  sociotropy  was  related  to  T1  social  comparisons  on  Facebook,  T1  pressures  to  be  thin,  T1  body  dissatisfaction,  and  T2  bulimic  symptoms.  ConclusionsFindings  suggest  the  BN  preventative  efforts  might  benefit  from  addressing  appropriate  forms  of  social  comparisons,  especially  those  made  on  Facebook,  and  the  personality  trait  sociotropy.  Limited  support  found  for  the  dual-mechanism  model  proposed  by  the  DPM.Sociotropy  found  to  be  a  risk  factor  for  BN  symptoms.Appropriate  Facebook  use  should  be  considered  for  bulimia  prevention.
2	How  wechat  can  retain  users.  An  increasing  number  of  users  join  and  immerse  in  Social  Network  Services'  (SNS)  virtual  spaces,  but  many  users  quit  using  these  services  as  well.  SNS  managers  must  enhance  users'  continuance  intention.  Based  on  a  survey  of  WeChat  users,  this  research  examined  the  effects  of  direct  and  indirect  network  externalities  on  users'  perceived  values  (including  social  value,  information  value,  emotional  value,  and  hedonic  value)  and  continuance  intention.  We  conducted  data  analysis  with  structural  equation  modeling  (SEM).  Results  confirmed  that  social  interaction  ties  mediate  the  effect  of  network  externalities  on  the  four  types  of  perceived  values.  Meanwhile,  among  the  four  types  of  perceived  values,  only  social  value  and  hedonic  value  influence  continuance  intention.  Examined  the  impact  of  both  direct  and  indirect  network  externalities  on  users'  four  types  of  perceived  values.Confirmed  that  social  interaction  ties  mediate  the  effect  of  network  externalities  on  four  types  of  perceived  values.Indicated  different  weighted  influences  of  perceived  values  on  users'  continuance  intention.
2	Modelling  continuance  intention  of  citizens  in  government  facebook  page.  The  main  purpose  of  this  paper  is  to  examine  the  continuance  intention  (CI)  of  citizens  in  following  government  Facebook  page.  Applying  theories  of  expectation-confirmation,  and  information  system  success  on  a  sample  of  362  students  in  Malaysia,  and  using  Partial  Least  Squares-Structural  Equation  Modelling  (PLS-SEM),  the  study  finds  that  CI  and  satisfaction  of  government  Facebook  page  is  contingent  upon  information  quality  (IQ)  of  the  Facebook  page  per  se.  IQ  is  found  as  a  second  order  construct  of  five  first  order  factors:  reliability,  completeness,  relevancy,  timeliness,  and  understandability.  Satisfaction  of  government  Facebook  page  is  also  found  as  a  partial  mediator  to  the  relationship  between  IQ  and  CI  of  following  government  Facebook  page.  In  addition,  applying  PLS  multi-group  analysis,  the  results  show  that  different  government  Facebook  pages  moderate  the  relationships  between  IQ  and  satisfaction  of  government  Facebook  page,  IQ  and  CI  of  following  government  Facebook  page  as  well  as  satisfaction  of  government  Facebook  page  and  CI  of  following  government  Facebook  page.  Satisfaction  and  continuance  intention  to  follow  government  Facebook  page  is  contingent  upon  information  quality  of  the  page.Information  quality  of  Facebook  page  is  a  second  order  construct  of  five  first  order  factors.Satisfaction  of  government  Facebook  page  is  a  partial  mediator.Different  government  Facebook  pages  moderate  the  structural  relationships.A  complementary  PLS-SEM  approach  was  applied  to  assess  measurement  and  structural  models.
2	Static  and  interactive  infographics  in  daily  tasks.  Infographics  are  a  common  visual  means  to  inform  users.  This  paper  investigates  how  lay  people  of  different  age,  gender  and  educational  background  perceive  the  use  of  infographics  for  information  visualization  in  daily  tasks.  We  chose  three  topics  of  general  interest:  weather,  study  and  work  and  three  infographics,  one  for  each  topic.  We  administered  a  questionnaire  to  people  randomly  split  in  two  groups:  the  first  group  interacted  with  a  static  version  of  each  infographic,  i.e.,  a  snapshot  of  it;  the  second  group  interacted  with  the  fully  configurable  infographics.  We  aimed  to  assess  information  quality  on  different  dimensions,  to  take  into  account  both  formal  and  substantial  aspects;  interaction  quality  along  dimensions  like  usability  and  ease  of  use;  and  design  quality  on  the  dimensions  of  the  Visualization  Wheel  by  Cairo,  to  assess  the  trade-off  between  information  complexity  and  aesthetics  of  infographics.  The  goal  was  to  measure  whether  the  quality  of  infographics  affects  the  perception  of  information  and  the  users'  interaction.  The  overall  results  suggest  that,  although  interactive  infographics  are  perceived  as  more  complex,  the  experience  with  them  is  better.  From  our  observations,  we  derived  a  model  to  assess  the  overall  quality  of  static  and  interactive  infographics,  based  on  information,  interaction  and  design  quality  dimensions.  A  user  study  on  static  and  interactive  infographics  use  for  daily  tasks.An  information,  interaction  and  design  quality  model  for  infographics.Assessment  of  whether/how  quality  of  infographics  affects  users'  experience.
2	Empathy  and  embodied  experience  in  virtual  environment.  This  study  investigates  the  user  experience  to  clarify  what  it  is  like  to  experience  stories  in  VR  (virtual  reality)  and  how  immersion  influences  story  experiences  in  immersive  storytelling.  This  study  explores  the  immersive  storytelling  context,  developing  and  testing  a  VR  experience  model  that  integrates  presence,  flow,  empathy,  and  embodiment.  The  results  imply  that  users  personal  traits  correlates  immersion  in  VR:  user  experience  in  VR  depend  on  individual  traits,  which  in  turns  influence  how  strongly  users  immerse  in  a  VR.  The  way  users  view  and  accept  VR  stories  derives  from  the  way  they  envisage  and  intend  to  experience  them.  Rather  than  simply  being  influenced  by  technological  features,  users  have  intentional  and  purposeful  control  over  VR  stories.  The  findings  of  this  study  suggest  that  the  cognitive  processes  by  which  users  experience  quality,  presence,  and  flow  determine  how  they  will  empathize  with  and  embody  VR  stories.  User  experience  of  virtual  reality  storytelling.How  immersion  influences  story  experiences  in  immersive  storytelling.Users'  personal  traits  correlates  immersion  in  VR.The  way  users  view  and  accept  VR  stories  derives  from  the  way  they  envisage  and  intend  to  experience  them.
2	Psychological  proximity  as  a  predictor  of  participation  in  a  social  media  issue  campaign.  Abstract  Understanding  how  to  motivate  people's  engagement  in  global  or  social  issues  has  been  challenging.  Using  Construal  Level  Theory,  this  study  explored  the  role  of  psychological  proximity  as  a  predictor  of  individuals'  willingness  to  engage  in  supportive  behaviors  related  to  a  social  media  issue  campaign.  Two  dimensions  of  psychological  proximity  (i.e.,  cognitive  and  emotional  proximity)  were  developed  and  measured  to  determine  whether  they  are  predictors  of  participation  intentions  related  to  a  Facebook  campaign  about  child  welfare  in  Africa.  Structural  Equation  Modeling  tested  the  proposed  model  with  survey  data  collected  from  408  U.S.  adult  residents  via  Amazon's  MTurk.  Results  indicated  that  salience,  knowledge,  and  relevance  were  strong  indicators  of  cognitive  proximity,  whereas  emotional  connectedness  and  empathy  were  determinants  of  emotional  proximity.  Both  cognitive  and  emotional  proximity  were  found  to  have  a  significant  and  positive  effect  on  people's  intentions  to  participate  in  the  social  media  issue  campaign.
2	What  makes  an  ai  device  human  like  the  role  of  interaction  quality  empathy  and  perceived  psychological  anthropomorphic  characteristics  in  the  acceptance  of  artificial  intelligence  in  the  service  industry.  Abstract  Intelligent  AI  devices  have  become  a  common  presence  in  the  business  landscape,  offering  a  wide  range  of  services,  from  the  medical  sector  to  the  hospitality  industry.  From  an  organizational  perspective,  AI  devices  have  several  advantages,  by  performing  certain  tasks  quicker  and  more  accurately  in  comparison  to  humans  while  at  the  same  time  being  more  cost-efficient.  However,  in  order  to  maintain  the  high  standards  of  a  brand,  they  have  to  be  accepted  by  consumers  and  deliver  socially  adequate  performance.  Therefore,  it  is  important  to  determine  the  characteristics  of  AI  devices  which  make  them  accepted  and  trusted  by  consumers.  Based  on  the  Computers  as  Social  Actors  (CASA)  Theory,  we  have  researched  on  the  role  of  psychological  anthropomorphic  characteristics,  perceived  empathy,  and  interaction  quality  in  the  acceptance  of  AI  devices  in  the  service  industry.  The  results  show  that  anthropomorphic  characteristics  alone  do  not  influence  acceptance  and  trust  towards  AI  devices.  However,  both  perceived  empathy  and  interaction  quality  mediate  the  relation  between  anthropomorphic  characteristics  and  acceptance.  A  human-like  AI  device  has  higher  acceptance  when  it  has  the  ability  to  show  empathy  and  interaction  in  relation  to  the  human  consumer.  This  result  reveals  the  importance  of  developing  forms  of  strong  intelligence  and  empathetic  behaviour  in  service  robots  and  AI  devices.
2	User  interface  based  on  natural  interaction  design  for  seniors.  Abstract      With  world  population  ageing,  how  to  help  seniors  to  adapt  to  technology  life  is  an  important  issue.  Technology  is  becoming  life  rather  than  resistance,  because  many  of  the  technology  applications  are  often  accompanied  by  a  lot  of  information  to  process.  This  makes  the  user  interface  to  become  an  important  bridge  between  human  computer  interactions.  Especially  the  inconvenience  caused  by  human  ageing,  these  related  issues  from  the  cognitive  and  operational  of  products  are  derived.  This  study  proposes  a  study  of  user  interface  design  based  on  natural  interaction  to  increase  seniors’  usage  intention.  In  the  proposed  contents,  the  Kinect  sensor  is  used  to  retrieve  seniors’  in-depth  information  in  movements,  thus  the  user  interface  of  system  can  be  operated  by  the  gesture  intuitively.  In  the  framework  of  the  system,  in  the  first  all,  the  morphology  is  applied  to  identify  the  features  of  a  hand  from  depth  values  obtained  from  the  sensor.  Gesture  is  used  to  recognize  operating  behavior  of  users  to  implement  the  interactive  action,  and  collision  detection  is  applied  to  confirm  effectiveness  of  operation.  On  the  other  hand,  through  interpretive  structural  model  (ISM),  each  design  element  of  interactive  interface  can  be  decomposed  and  realized,  and  the  solution  for  target  and  direction  of  design  problem  is  also  proposed.  At  the  meanwhile,  the  concept  of  affordance  is  conducted  to  the  development  of  interface  for  graphic  users  that  proposed  in  this  study,  and  the  design  achievement  contains  operation  and  usability  of  intuition  can  further  be  acquired.  Finally,  based  on  the  proposed  methodology,  an  intuitive  user  interface  of  digital  devices  is  constructed  by  Java  programming  language  that  allows  for  verifying  the  feasibility  of  user  interface  for  seniors.  Besides,  the  proposed  method  can  be  widely  used  to  develop  the  user  interface  for  various  products.
2	Understanding  users  willingness  to  put  their  personal  information  on  the  personal  cloud  based  storage  applications  an  empirical  study.  Abstract  Despite  prevalent  privacy  and  security  threats  on  the  cloud,  users  have  put  tremendous  amounts  of  their  personal  information  on  cloud  storage.  This  present  study  proposes  a  comprehensive  research  framework  to  investigate  cloud  storage  users'  willingness  to  put  personal  information  on  personal  cloud-based  storage  applications.  Our  research  framework  is  theoretically  derived  from  the  Communication  Privacy  Management  Theory  and  Privacy-Trust-Behavioral  Intention  Model.  To  empirically  test  our  research  framework,  we  conducted  an  online  survey  of  786  active  cloud  storage  users  both  in  Indonesia  and  Taiwan.  The  findings  suggest  that  cloud  storage  users'  willingness  to  put  personal  information  is  highly  influenced  by  trust,  perceived  costs,  perceived  benefits,  and  also  the  degree  of  sensitivity  of  the  personal  information.  Some  findings  with  regard  to  cultural  differences  between  the  two  countries  are  also  showed  out.  The  key  findings,  implications,  and  limitations  are  discussed  in  this  paper.
2	The  spreading  impact  of  playing  violent  video  games  on  aggression.  Violent  video  game  exposure  has  been  shown  to  increase  aggression  in  the  player.  The  present  research  examines  the  idea  that  violent  video  game  play  does  not  only  have  an  impact  on  the  player,  but  also  on  the  player's  social  network.  In  fact,  egocentric  social  networking  analyses  showed  that  playing  violent  video  games  is  associated  with  increased  aggression,  which  then  spreads  among  connected  individuals.  Even  participants  that  do  not  play  violent  video  games  themselves  reported  more  aggression  when  their  social  network  consists  of  individuals  who  do  play  violent  video  games.  Psychologists  and  the  public  alike  have  been  concerned  that  violent  video  game  exposure  has  the  potential  to  increase  aggression  on  a  societal  level.  As  the  present  study  shows,  not  only  players  of  violent  video  games  but  also  their  social  network  may  contribute  to  this  phenomenon.  Violent  video  game  exposure  is  related  to  aggressive  behavior  in  the  player.This  effect  in  turn  spreads  across  the  player's  social  network.Non-players  are  more  aggressive  if  they  are  connected  to  players.
2	Self  regulation  of  learning  and  mooc  retention.  Abstract  Background  Due  to  MOOC  high  attrition  rates,  this  study  aims  to  assess  differences  in  self-regulated  learning  strategies  and  other  variables  related  to  MOOC  retention  (perceived  effectiveness,  MOOC  interaction,  motivation  and  socio-demographic  characteristics)  between  course  completers  and  non-completers.  This  work  also  aims  to  translate  into  Spanish  and  validate  an  instrument  for  the  assessment  of  self-regulated  learning  (SRL)  behaviours  in  MOOCs.  Materials  and  methods:  582  participants  answered  the  translated  SRL  questionnaire  and  other  questions  related  to  MOOC  retention.  The  comparison  between  MOOC  completers  and  non-completers  was  carried  out  in  a  subgroup  of  176  undergraduate  students.  Results:  Completer  students  were  more  capable  of  self-regulating  their  learning  and  showed  significantly  higher  levels  of  perceived  effectiveness  and  of  engagement  with  MOOC  contents  than  non-completers.  In  addition,a  logistic  regression  analysis  indicated  that  the  variables  with  greatest  predictive  value  to  discriminate  between  completers  and  non-completers  were  goal-setting,  task  interest  and  the  academic  discipline  of  studies  of  MOOC  participants.  The  percentage  of  cases  correctly  predicted  by  the  model  was  over  84%.  The  Spanish  version  of  the  instrument  replicated  the  original  factor  structure  of  the  SRL  questionnaire  and  showed  high  internal  consistency  (α = 0.948).
2	Potential  funders  motivations  in  reward  based  crowdfunding  the  influence  of  project  attachment  and  business  viability.  Abstract  Although  reward-based  crowdfunding  projects  have  experienced  high  growth  in  recent  years,  it  is  necessary  to  emphasize  that  not  all  the  campaigns  have  success.  Under  these  circumstances,  this  paper  studies  the  influence  of  two  potential  funders'  motivations  (i.e.,  project  attachment  and  business  viability)  on  their  behavioral  intentions  (i.e.,  the  intention  to  fund  a  crowdfunding  project,  and  the  intention  to  spread  positive  physical  and  electronic  word-of-mouth  about  it).  It  also  explores  how  the  effect  of  these  motivations  is  moderated  by  two  campaign  characteristics:  the  percentage  of  target  capital  pledged  and  the  time  remaining  until  the  funding  deadline.  With  this  aim,  this  paper  simulates  a  crowdfunding  project  and  collects  311  survey  responses  about  it.  Subsequently,  a  PLS-SEM  approach  is  applied  to  test  the  model  proposed.  Findings  demonstrate  that  potential  funders’  intentions  are  mainly  influenced  by  their  attachment  to  the  project.  For  its  part,  the  business  viability,  as  perceived  by  potential  funders,  plays  a  secondary  role  mainly  influencing  their  word-of-mouth  intentions.  Finally,  the  campaign  characteristics  moderate  the  effect  of  attachment  and  viability  on  electronic  word-of-mouth  intentions.
2	Effects  of  head  display  lag  on  presence  in  the  oculus  rift.  We  measured  presence  and  perceived  scene  stability  in  a  virtual  environment  viewed  with  different  head-to-display  lag  (i.e.,  system  lag)  on  the  Oculus  Rift  (CV1).  System  lag  was  added  on  top  of  the  measured  benchmark  system  latency  (22.3  ms)  for  our  visual  scene  rendered  in  OpenGL  Shading  Language  (GLSL).  Participants  made  active  head  oscillations  in  pitch  at  1.0Hz  while  viewing  displays.  We  found  that  perceived  scene  instability  increased  and  presence  decreased  when  increasing  system  lag,  which  we  attribute  to  the  effect  of  multisensory  visual-vestibular  interactions  on  the  interpretation  of  the  visual  information  presented.
2	Design  approaches  for  interactive  digital  narrative.  While  authoring  has  long  been  a  concern  for  researchers  engaged  in  interactive  narrative,  generalized  design  approaches  have  been  less  of  a  focus.  At  the  same  time,  the  need  for  design  conventions  to  aid  in  the  creation  of  artifacts  has  long  been  recognized,  starting  with  Murray’s  1997  Hamlet  on  the  Holodeck.  However,  unlike  in  the  related  field  of  game  design,  widely  accepted,  generalized  conventions  are  still  elusive.  In  this  paper  I  investigate  the  state  of  affairs  and  identify  several  broad  trajectories  in  the  scholarly  treatment  of  interactive  narrative  authoring.  I  propose  a  process  and  a  set  of  design  heuristics  developed  in  my  practice  of  teaching  interactive  digital  narrative.
2	Lower  body  control  of  a  semi  autonomous  avatar  in  virtual  reality  balance  and  locomotion  of  a  3d  bipedal  model.  Animated  virtual  humans  may  rely  on  full-body  tracking  system  to  reproduce  user  motions.  In  this  paper,  we  reduce  tracking  to  the  upper-body  and  reconstruct  the  lower  body  to  follow  autonomously  its  upper  counterpart.  Doing  so  reduces  the  number  of  sensors  required,  making  the  application  of  virtual  humans  simpler  and  cheaper.  It  also  enable  deployment  in  cluttered  scenes  where  the  lower  body  is  often  hidden.  The  contribution  here  is  the  inversion  of  the  well-known  capture  problem  for  bipedal  walking.  It  determines  footsteps  rather  than  center-of-mass  motions  and  yet  can  be  solved  with  an  off-the-shelf  capture  problem  solver.  The  quality  of  our  method  is  assessed  in  real-time  tracking  experiments  on  a  wide  variety  of  movements.
2	Designing  for  youth  interpreter  professional  development  a  sociotechnologically  framed  participatory  design  approach.  Informal  science  institutions  (ISIs)  are  beginning  to  adopt  mobile  technology  to  support  interpreters  (docents),  but  not  much  is  known  about  how  to  design  these  supports.  One  approach  to  designing  technology  for  new  scenarios  is  participatory  design  (PD),  where  end-users  are  involved  as  experts  in  the  task  domain  who  can  help  envision  the  application  of  technology.  However,  in  our  context  end-users  are  often  youth  interpreters  who  are  emerging  professionals.  This  poses  a  challenge  because  traditional  PD  methods  trust  that  the  users  can  represent  the  task  domain.  Novice  professionals  may  not  yet  fully  understand  the  task  domain,  but  eliciting  their  needs  and  visions  is  still  important  for  producing  a  tool  they  will  find  useful.  A  design  approach  is  needed  that  captures  the  requirements  for  supporting  expert  task  execution  as  an  underlying  structure  for  the  tool,  while  nonetheless  eliciting  and  respecting  the  special  needs  of  novices.  We  developed  and  applied  two  different  framing  strategies  (one  technological,  one  sociotechnological)  to  traditional  PD  methods  to  help  youth  non-expert  interpreters  generate  task-relevant  design  ideas.  We  report  results  from  using  these  strategies  in  an  exploratory  fashion  and  discuss  opportunities  for  future  research  on  PD  methods  that  can  address  the  needs  of  youth  as  emerging  professionals.
2	Designing  a  relational  social  robot  toolkit  for  preschool  children  to  explore  computational  concepts.  Designing  toolkits  for  teaching  programming  concepts  to  children  using  robots  has  received  growing  attention  in  recent  years.  However,  teaching  preschool  children  computational  concepts,  such  as  non-determinism  and  event-based  programming,  presents  particular  challenges.  We  have  developed  a  programming  toolkit  that  is  embedded  in  an  interpersonal  interaction  context  with  a  social  robot.  The  toolkit  enables  young  children  to  program  social  robots  by  "teaching"  them  to  interact.  In  order  to  "teach",  the  children  show  the  robot  rules  designed  with  reusable  vinyl  stickers.  In  doing  so,  children  can  experiment  with  computational  concepts  while  having  a  playful  interaction  with  the  social  robot.  We  present  the  purpose,  context,  and  design  of  the  social  robot  toolkit  (SoRo  Toolkit),  and  an  evaluation  performed  with  22  preschool  children.  We  show  that  children  have  an  engaging  experience  designing  and  "teaching"  social  interaction  rules  to  the  robot,  orchestrating  give-and-take  exchanges,  and  delighting  in  how  the  robot  engages  with  them  as  they  explore  computational  ideas.
2	Children  asking  questions  speech  interface  reformulations  and  personification  preferences.  The  pervasive  availability  of  voice  assistants  may  support  children  in  finding  answers  to  informational  queries  by  removing  the  literacy  requirements  of  text  search  (e.g.,  typing,  spelling).  However,  most  such  systems  are  not  designed  for  the  specific  needs  and  preferences  of  children  and  may  struggle  with  understanding  the  intent  of  their  questions.  In  our  investigation,  we  observed  87  children  and  27  adults  interacting  with  three  Wizard-of-Oz  speech  interfaces  to  arrive  at  answers  to  questions  that  required  reformulation.  We  found  that  many  children  and  some  adults  required  help  to  reach  an  effective  question  reformulation.  We  report  the  common  types  of  reformulations  (both  effective  and  ineffective  ones).  We  also  compared  three  versions  of  speech  interfaces  with  different  approaches  to  referring  to  itself  (personification)  and  to  the  participant  (naming  personalization).  We  found  that  children  preferred  personified  interfaces,  but  naming  personalization  did  not  affect  preference.  We  connect  our  findings  to  implications  for  design  of  speech  systems  for  families.
2	Marker  based  augmented  reality  instructional  design  to  improve  children  interactions  with  astronomical  concepts.  This  paper  presents  the  instructional-design  of  an  augmented  learning  environment  named  AIBLE-HELIOS®  that  is  targeted  at  teaching  astronomy  to  children.  This  environment  takes  benefit  of  Augmented  Reality  (AR)  and  tangible  interaction  to  stimulate  an  active  and  learner-centered  approach  to  scientific  problem  solving.  This  approach  follows  the  pedagogical  principles  of  the  Inquiry-Based  Sciences  Education  (IBSE).  Technical  specifications  and  the  design  of  the  application  have  been  based  on  didactical  principles.  It  is  intended  to  children  of  8  to  11  years  old  in  formal  education.  HELIOS  was  tested  in-situ,  i.e.,  in  real  teaching  conditions  with  pupils  (grades  4-5)  from  two  primary  schools.  This  user  study  confirms  the  design  assumptions  that  influence  children's  interaction  with  contents  during  sciences  courses.  The  analyses  of  the  children's  interactions  with  the  system  as  well  as  learning  indicate  that  HELIOS  supports  children  in  their  investigations.  Moreover,  it  provides  some  new  information  on  children's  interactions  possibilities  that  will  be  taken  into  account  in  future  versions.  All  these  parameters  contribute  to  the  understanding  of  the  ways  through  which  AR  can  be  used  in  formal  teaching  curricula  in  K-12  schools.
2	Ml  lifecycle  canvas  designing  machine  learning  empowered  ux  with  material  lifecycle  thinking.  As  a  particular  type  of  artificial  intelligence  technology,  machine  learning  (ML)  is  widely  used  to  empower  user  experience  (UX).  However,  designers,  especially  the  novice  designers,  struggle  to  in...
2	Pas  a  pas  a  platform  for  enabling  schools  to  teach  educational  content  using  stop  motion  animation.  Pas  a  Pas  is  a  tool  to  support  education  using  stop  motion  animation.  The  platform  aims  to  bridge  the  gap  between  abstract  concepts  from  educational  content  to  reality  using  the  physicality  and  animated  outcome  of  stop  motion.  This  paper  describes  the  research,  prototype  and  conclusions  of  user  testing,  and  provides  design  ideas  for  further  iterations  of  the  concept.
2	An  owl  in  the  classroom  development  of  an  interactive  storytelling  application  for  preschoolers.  In  research  there  is  a  considerable  interest  in  developing  interactive  educational  systems.  However,  the  typical  classroom  remains  a  rather  lowtech  environment.  Allowing  teachers  to  create,  adapt  and  share  interactive  learning  applications  might  increase  the  uptake  of  technology  in  the  classroom.  In  this  paper  a  study  is  presented  that  explores  the  deployment  of  a  robotstorytelling  application  for  preschoolers,  while  simultaneously  investigating  the  teacher's  requirements  for  a  toolkit  to  create  stories  for  the  robot.  The  results  suggest  that  a  robotstorytelling  application  can  be  a  valuable  addition  to  the  classroom  and  that  indeed  a  toolkit  for  creating  stories  would  increase  its  usefulness  in  the  curriculum.
2	Does  my  smart  device  provider  care  about  my  privacy  investigating  trust  factors  and  user  attitudes  in  iot  systems.  With  the  wide  spread  of  IoT  devices,  smart  systems  gain  more  and  more  control  over  personal  data  and  daily  lives  of  their  users.  This  control,  however,  can  easily  be  misused,  either  by  system  providers  themselves  acting  in  bad  faith,  or  by  external  attackers.  Implementing  proper  measures  towards  security  and  privacy  protection  of  smart  systems,  therefore,  becomes  of  critical  importance.  In  this  paper  we  present  a  study  to  investigate  beliefs  among  end  users,  whether  the  smart  system  providers  are  both  capable  and  motivated  to  implement  such  measures.  For  this  purpose,  we  conduct  an  online  survey  of  98  participants  from  the  UK,  which  we  analyse  using  quantitative  and  qualitative  methods.  Our  results  show  that  users’  trust  in  proper  security  and  privacy  protection  in  smart  systems  is  influenced  by  a  multitude  of  factors  such  as  information  about  concrete  technologies  and  privacy  policies  of  the  systems,  but  also  information  about  the  company  such  as  its  reputation  or  geographical  location.  We  conclude  that  transparency  by  companies,  regarding  both  the  technologies  behind  the  concrete  system  and  the  general  practices  of  the  company  itself,  is  a  crucial  factor  in  ensuring  end  user  confidence.
2	Scale  effects  in  the  steering  time  difference  between  narrowing  and  widening  linear  tunnels.  Steering  time  differs  between  narrowing  and  widening  linear  tunnels;  a  narrowing  tunnel  requires  more  time  to  navigate  than  a  widening  one.  A  prediction  model,  IDGap,  for  the  time  difference  has  recently  been  proposed,  and  it  shows  an  excellent  fit.  However,  the  time  difference  in  movement  and  model  fitness  were  confirmed  on  a  limited  scale.  The  experiment  used  a  13.3-inch  pen  tablet,  which  required  primarily  wrist  movements  with  a  particular  level  of  forearm  extension.  In  this  study,  we  tested  the  scale  effects  in  the  steering  time  difference  between  the  two  tunnel  types.  In  our  experiment,  participants  performed  steering  operations  at  five  scales,  from  the  entire  21.5-inch  tablet  area  to  its  1/12-scale  size.  The  results  always  showed  the  time  difference,  and  the  conventional  steering  law  did  not  show  a  good  fit.  IDGap  improved  the  fitness,  thereby  confirming  the  validity  of  the  model.  The  scale  effects  for  the  other  results,  including  error  rates  and  index  of  performance,  are  also  discussed.
2	Stop  nigmas  experimental  speculative  design  through  pragmatic  aesthetics  and  public  art.  This  paper  describes  a  project  titled  Stop  Nigmas  which  explores  the  future  of  privacy  and  surveillance.  Guided  by  pragmatic  philosophy  and  approaches  from  futures  studies,  speculative  design,  the  project  seeks  to  demonstrate  how  interactive  objects  can  be  used  to  engage  the  audience  in  creating  alternative  narratives  about  the  future.  In  the  first  section,  the  paper  outlines  a  narrative  in  a  form  of  a  timeline  of  events  that  leads  to  a  future  with  explicitly  restricted  privacy  in  public  spaces.  Following  the  timeline  of  events,  the  paper  describes  the  process  of  scenario  development,  object  design,  interaction  design  and  audience  engagement.  The  author  outlines  how  engagement  through  public  art  and  social  media  allows  the  interactive  object  to  serve  as  means  of  speculation  through  John  Dewey's  notion  of  consummatory  experience,  allowing  both  the  designer  and  the  audience  to  act  as  agents  of  speculation.  The  paper  concludes  that  pragmatic  aesthetics  and  futures  studies  can  provide  useful  guidance  in  designing  speculative  objects  and  interactions  that  are  open  to  dialogue  and  participation.  It  suggests  new  research  avenues  for  speculative  design  research  in  human-computer  interaction  (HCI).
2	Physical  digital  and  hybrid  setups  supporting  card  based  collaborative  design  ideation.  Physical  tools  and  materials  like  pen,  paper,  sticky-notes,  and  whiteboards  are  commonly  used  in  collaborative  creative  design  processes,  whereas  digital  tools  play  a  more  marginal  role.  But  what  are  the  benefits  and  drawbacks  of  physical,  digital,  and  hybrid  physical-digital  setups  when  it  comes  to  supporting  collaborative  ideation?  To  answer  this  question,  we  present  a  study  and  analysis  of  three  different  implementations  of  a  well-established  collaborative  ideation  technique  called  Inspiration  Card  Workshop,  with  physical,  digital,  and  hybrid  setups.  Each  setup  is  a  controlled  experiment  with  three  different  groups  of  designers.  We  analyse  the  setups  in  terms  of  how  they  support  five  key  aspects  of  collaborative  design.  Based  on  our  insights,  we  present  implications  for  future  use  of  digital  tools  to  support  card-based  collaborative  design  ideation,  in  which  we  argue  for  a  technically  lightweight  hybrid  workflow  setup  that  builds  on  well-proven  physical  and  digital  components.
2	Interrupting  or  not  exploring  the  effect  of  social  context  on  interrupters  decision  making.  In  recent  decades  technology-induced  interruptions  emerged  as  a  key  object  of  study  in  HCI  and  CSCW  research,  but  until  recently  the  social  dimension  of  interruptions  has  been  relatively  neglected.  The  focus  of  existing  research  on  interruptions  has  been  mostly  on  their  direct  effects  on  the  persons  whose  activities  are  interrupted.  Arguably,  however,  it  is  also  necessary  to  take  into  account  the  "ripple  effect"  of  interruptions,  that  is,  indirect  consequences  of  interruptions  within  the  social  context  of  an  activity,  to  properly  understand  interrupting  behavior  and  provide  advanced  technological  support  for  handling  interruptions.  This  paper  reports  an  empirical  study,  in  which  we  examine  a  set  of  facets  of  the  social  context  of  interruptions,  which  we  identified  in  a  previous  conceptual  analysis.  The  results  suggest  that  people  do  take  into  account  various  facets  of  the  social  context  when  making  decisions  about  whether  or  not  it  is  appropriate  to  interrupt  another  person.
2	A  networked  suite  of  mixed  technology  robotic  artifacts  for  advancing  literacy  in  children.  Illiteracy  is  a  global  problem  that  impacts  societal  and  economic  growth  and  development,  and  is  directly  correlated  with  the  financial  success,  health  and  overall  well-being  of  individuals.  Studies  indicate  that  picture-book  reading  within  a  facilitated  story-time  setting  is  an  important  tool  for  language  acquisition  in  children.  We  hypothesize  that  in  an  increasingly  digital  society,  literacy  can  be  cultivated  in  a  robot-embedded  environment  that  is,  at  once,  physical,  digital  and  evocative  of  the  picture-book  being  read.  Inspired  by  concepts  of  embodied  interaction,  our  developing  LIT  ROOM  is  an  intelligent,  fine-tunable  suite  of  architectural-robotic  artifacts  distributed  at  room-scale  in  a  public  library  setting.  Presented  here  are  motivations  for  and  design  overview  of  this  developing  interactive  artifact.  Through  a  reconfigurable,  co-adaptive  learning  environment,  the  LIT  ROOM  aims  to  augment  the  dialogical  reading  of  picture-books  within  an  engaging  and  exploratory  space  for  the  advancement  of  literacy  and  learning.
2	Looking  behind  bezels  french  windows  for  wall  displays.  Using  tiled  monitors  to  build  wall-sized  displays  has  multiple  advantages:  higher  pixel  density,  simpler  setup  and  easier  calibration.  However,  the  resulting  display  walls  suffer  from  the  visual  discontinuity  caused  by  the  bezels  that  frame  each  monitor.  To  avoid  introducing  distortion,  the  image  has  to  be  rendered  as  if  some  pixels  were  drawn  behind  the  bezels.  In  turn,  this  raises  the  issue  that  a  non-negligible  part  of  the  rendered  image,  that  might  contain  important  information,  is  visually  occluded.  We  propose  to  draw  upon  the  analogy  to  french  windows  that  is  often  used  to  describe  this  approach,  and  make  the  display  really  behave  as  if  the  visualization  were  observed  through  a  french  window.  We  present  and  evaluate  two  interaction  techniques  that  let  users  reveal  content  hidden  behind  bezels.  ePan  enables  users  to  offset  the  entire  image  through  explicit  touch  gestures.  GridScape  adopts  a  more  implicit  approach:  it  makes  the  grid  formed  by  bezels  act  like  a  true  french  window  using  head  tracking  to  simulate  motion  parallax,  adapting  to  users'  physical  movements  in  front  of  the  display.  The  two  techniques  work  for  both  single-  and  multiple-user  contexts.
2	Spalendar  visualizing  a  group  s  calendar  events  over  a  geographic  space  on  a  public  display.  Portable  paper  calendars  (i.  e.,  day  planners  and  organizers)  have  greatly  influenced  the  design  of  group  electronic  calendars.  Both  use  time  units  (hours/days/weeks/etc.)  to  organize  visuals,  with  useful  information  (e.g.,  event  types,  locations,  attendees)  usually  presented  as  -  perhaps  abbreviated  or  even  hidden  -  text  fields  within  those  time  units.  The  problem  is  that,  for  a  group,  this  visual  sorting  of  individual  events  into  time  buckets  conveys  only  limited  information  about  the  social  network  of  people.  For  example,  people's  whereabouts  cannot  be  read  'at  a  glance'  but  require  examining  the  text.  Our  goal  is  to  explore  an  alternate  visualization  that  can  reflect  and  illustrate  group  members'  calendar  events.  Our  main  idea  is  to  display  the  group's  calendar  events  as  spatiotemporal  activities  occurring  over  a  geographic  space  animated  over  time,  all  presented  on  a  highly  interactive  public  display.  In  particular,  our  Spalendar  (Spatial  Calendar)  design  animates  people's  past,  present  and  forthcoming  movements  between  event  locations  as  well  as  their  static  locations.  Detail  of  people's  events,  their  movements  and  their  locations  is  progressively  revealed  and  controlled  by  the  viewer's  proximity  to  the  display,  their  identity,  and  their  gestural  interactions  with  it,  all  of  which  are  tracked  by  the  public  display.
2	Temoco  doc  a  visualization  for  supporting  temporal  and  contextual  analysis  of  dialogues  and  associated  documents.  A  common  task  in  a  number  of  application  areas  is  to  create  textual  documents  based  on  recorded  audio  data.  Visualizations  designed  to  support  such  tasks  require  linking  temporal  audio  data  with  contextual  data  contained  in  the  resulting  documents.  In  this  paper,  we  present  a  tool  for  the  visualization  of  temporal  and  contextual  links  between  recorded  dialogues  and  their  summary  documents.
2	Eyegaze  enabling  eye  contact  over  video.  Traditional  video  communication  systems  offer  a  very  limited  experience  of  eye  contact  due  to  the  offset  between  cameras  and  the  screen.  In  response,  we  present  EyeGaze,  which  uses  multiple  Kinect  cameras  to  generate  a  3D  model  of  the  user,  and  then  renders  a  virtual  camera  angle  giving  the  user  an  experience  of  eye  contact.  As  a  novel  approach,  we  use  concepts  from  KinectFusion,  such  as  a  volumetric  voxel  data  representation  and  GPU  accelerated  ray  tracing  for  viewpoint  rendering.  This  achieves  detail  from  a  noisy  source,  and  allows  the  real-time  video  output  to  be  a  composite  of  old  and  new  data.  We  frame  our  work  in  literature  on  eye  contact  and  previous  approaches  to  supporting  it  over  video.  We  then  describe  EyeGaze,  and  an  empirical  study  comparing  it  with  communication  face-to-face  or  over  traditional  video.  The  study  shows  that  while  face-to-face  is  still  superior,  EyeGaze  has  added  value  over  traditional  video  in  terms  of  eye  contact,  involvement,  turn-taking  and  co-presence.
2	Deformation  parameterization  and  analysis  of  a  single  locomotion  cycle.  We  present  preliminary  results  of  a  framework  that  can  synthesize  parameterized  locomotion  with  controllable  quality  from  simple  deformations  over  a  single  step  cycle.  Our  approach  enforces  feet  constraints  per  phase  in  order  to  appropriately  perform  motion  deformation  operations,  resulting  in  a  generative  and  controllable  model  that  maintains  the  style  of  the  input  motion.  The  method  is  lightweight  and  has  quantifiable  motion  quality  related  to  the  amount  of  deformation  used.  It  only  requires  a  single  cycle  of  locomotion.  An  analysis  of  the  deformation  is  presented  with  the  quantification  of  the  valid  portion  of  the  deformed  motion  space,  informing  on  the  parameterization  coverage  of  the  deformable  motion  cycle.
2	An  adaptive  parameter  space  filling  algorithm  for  highly  interactive  cluster  exploration.  For  a  user  to  perceive  continuous  interactive  response  time  in  a  visualization  tool,  the  rule  of  thumb  is  that  it  must  process,  deliver,  and  display  rendered  results  for  any  given  interaction  in  under  100  milliseconds.  In  many  visualization  systems,  successive  interactions  trigger  independent  queries  and  caching  of  results.  Consequently,  computationally  expensive  queries  like  multidimensional  clustering  cannot  keep  up  with  rapid  sequences  of  interactions,  precluding  visual  benefits  such  as  motion  parallax.  In  this  paper,  we  describe  a  heuristic  prefetching  technique  to  improve  the  interactive  response  time  of  KMeans  clustering  in  dynamic  query  visualizations  of  multidimensional  data.  We  address  the  tradeoff  between  high  interaction  and  intense  query  computation  by  observing  how  related  interactions  on  overlapping  data  subsets  produce  similar  clustering  results,  and  characterizing  these  similarities  within  a  parameter  space  of  interaction.  We  focus  on  the  two-dimensional  parameter  space  defined  by  the  minimum  and  maximum  values  of  a  time  range  manipulated  by  dragging  and  stretching  a  one-dimensional  filtering  lens  over  a  plot  of  time  series  data.  Using  calculation  of  nearest  neighbors  of  interaction  points  in  parameter  space,  we  reuse  partial  query  results  from  prior  interaction  sequences  to  calculate  both  an  immediate  best-effort  clustering  result  and  to  schedule  calculation  of  an  exact  result.  The  method  adapts  to  user  interaction  patterns  in  the  parameter  space  by  reprioritizing  the  interaction  neighbors  of  visited  points  in  the  parameter  space.  A  performance  study  on  Mesonet  meteorological  data  demonstrates  that  the  method  is  a  significant  improvement  over  the  baseline  scheme  in  which  interaction  triggers  on-demand,  exact-range  clustering  with  LRU  caching.  We  also  present  initial  evidence  that  approximate,  temporary  clustering  results  are  sufficiently  accurate  (compared  to  exact  results)  to  convey  useful  cluster  structure  during  rapid  and  protracted  interaction.
2	Watch  this  a  taxonomy  for  dynamic  data  visualization.  Visualizations  embody  design  choices  about  data  access,  data  transformation,  visual  representation,  and  interaction.  To  interpret  a  static  visualization,  a  person  must  identify  the  correspondences  between  the  visual  representation  and  the  underlying  data.  These  correspondences  become  moving  targets  when  a  visualization  is  dynamic.  Dynamics  may  be  introduced  in  a  visualization  at  any  point  in  the  analysis  and  visualization  process.  For  example,  the  data  itself  may  be  streaming,  shifting  subsets  may  be  selected,  visual  representations  may  be  animated,  and  interaction  may  modify  presentation.  In  this  paper,  we  focus  on  the  impact  of  dynamic  data.  We  present  a  taxonomy  and  conceptual  framework  for  understanding  how  data  changes  influence  the  interpretability  of  visual  representations.  Visualization  techniques  are  organized  into  categories  at  various  levels  of  abstraction.  The  salient  characteristics  of  each  category  and  task  suitability  are  discussed  through  examples  from  the  scientific  literature  and  popular  practices.  Examining  the  implications  of  dynamically  updating  visualizations  warrants  attention  because  it  directly  impacts  the  interpretability  (and  thus  utility)  of  visualizations.  The  taxonomy  presented  provides  a  reference  point  for  further  exploration  of  dynamic  data  visualization  techniques.
2	Ilamp  exploring  high  dimensional  spacing  through  backward  multidimensional  projection.  Ever  improving  computing  power  and  technological  advances  are  greatly  augmenting  data  collection  and  scientific  observation.  This  has  directly  contributed  to  increased  data  complexity  and  dimensionality,  motivating  research  of  exploration  techniques  for  multidimensional  data.  Consequently,  a  recent  influx  of  work  dedicated  to  techniques  and  tools  that  aid  in  understanding  multidimensional  datasets  can  be  observed  in  many  research  fields,  including  biology,  engineering,  physics  and  scientific  computing.  While  the  effectiveness  of  existing  techniques  to  analyze  the  structure  and  relationships  of  multidimensional  data  varies  greatly,  few  techniques  provide  flexible  mechanisms  to  simultaneously  visualize  and  actively  explore  high-dimensional  spaces.  In  this  paper,  we  present  an  inverse  linear  affine  multidimensional  projection,  coined  iLAMP,  that  enables  a  novel  interactive  exploration  technique  for  multidimensional  data.  iLAMP  operates  in  reverse  to  traditional  projection  methods  by  mapping  low-dimensional  information  into  a  high-dimensional  space.  This  allows  users  to  extrapolate  instances  of  a  multidimensional  dataset  while  exploring  a  projection  of  the  data  to  the  planar  domain.  We  present  experimental  results  that  validate  iLAMP,  measuring  the  quality  and  coherence  of  the  extrapolated  data;  as  well  as  demonstrate  the  utility  of  iLAMP  to  hypothesize  the  unexplored  regions  of  a  high-dimensional  space.
2	Application  discoverability  on  multipurpose  public  displays  popularity  comes  at  a  price.  An  important  step  in  developing  multipurpose  public  displays  is  understanding  application  discoverability:  the  effort  required  to  locate  or  "discover"  an  application  amongst  others.  Discoverability  can  affect  the  adoption  and  potential  success  applications.  Here  we  investigate  the  effects  of  application  discoverability  on  two  aspects  of  application  use:  relative  utility  and  conversion  rate.  We  do  so  by  testing  three  conditions  that  provide  incremental  discoverability  to  an  application.  Our  results  indicate  that  increased  discoverability  leads  to  higher  relative  utility  but  lower  conversion  rates.  We  discuss  the  implications  our  findings  have  on  evaluating  applications  on  multipurpose  displays,  and  finally  we  show  how  our  results  contribute  to  understanding  the  economics  of  discoverability  mechanisms.
2	Just  in  time  annotation  of  clusters  outliers  and  trends  in  point  based  data  visualizations.  We  introduce  the  concept  of  just-in-time  descriptive  analytics  as  a  novel  application  of  computational  and  statistical  techniques  performed  at  interaction-time  to  help  users  easily  understand  the  structure  of  data  as  seen  in  visualizations.  Fundamental  to  just-intime  descriptive  analytics  is  (a)  identifying  visual  features,  such  as  clusters,  outliers,  and  trends,  user  might  observe  in  visualizations  automatically,  (b)  determining  the  semantics  of  such  features  by  performing  statistical  analysis  as  the  user  is  interacting,  and  (c)  enriching  visualizations  with  annotations  that  not  only  describe  semantics  of  visual  features  but  also  facilitate  interaction  to  support  high-level  understanding  of  data.  In  this  paper,  we  demonstrate  just-in-time  descriptive  analytics  applied  to  a  point-based  multi-dimensional  visualization  technique  to  identify  and  describe  clusters,  outliers,  and  trends.  We  argue  that  it  provides  a  novel  user  experience  of  computational  techniques  working  alongside  of  users  allowing  them  to  build  faster  qualitative  mental  models  of  data  by  demonstrating  its  application  on  a  few  use-cases.  Techniques  used  to  facilitate  just-in-time  descriptive  analytics  are  described  in  detail  along  with  their  runtime  performance  characteristics.  We  believe  this  is  just  a  starting  point  and  much  remains  to  be  researched,  as  we  discuss  open  issues  and  opportunities  in  improving  accessibility  and  collaboration.
2	Towards  design  for  renegotiating  the  parent  adult  child  relationship  after  children  leave  home.  This  study  explores  how  to  move  towards  designing  technologies  to  enrich  the  parent-adult  child  relationship  after  adult  children  leave  home.  This  time  is  a  turning  point  for  adult  children  as  they  establish  an  independent  life,  while  it  marks  a  change  in  responsibilities  and  freedoms  enjoyed  by  parents.  We  conducted  interviews  with  7  parents  and  6  adult  children  to  understand  how  they  currently  use  technologies  like  Skype  and  WhatsApp  to  maintain  their  relationship.  The  findings  show  how  parents  and  adult  children's  positions  raise  tensions  when  balancing  independence  and  closeness,  how  these  tensions  affect  technology-mediated  communication,  and  that  there  is  limited  dialogue  about  the  differences  between  their  positions.  Inspired  by  Position  Exchange  Theory,  we  discuss  how  design  methods  and  technology  design  can  enable  parents  and  adult  children  to  see  the  world  through  each  other's  eyes.  This  process  can  contribute  to  developing  a  better  understanding  of  how  they  can  renegotiate  their  new  social  positions  and  thus  enrich  their  relationship.
2	Shrug  stroke  haptic  rehabilitation  using  gaming.  In  this  paper  we  present  SHRUG,  an  interactive  shoulder  rehabilitation  exerciser.  With  this  work-in-progress  system,  we  intend  to  (1)  explore  the  effectiveness  of  providing  interactive  and  just-in-time  feedback  to  the  patients  and  therapists;  (2)  explore  the  effect  of  adding  a  gaming  element  on  the  motivation  of  the  patients.  The  SHRUG  prototype  was  developed  in  collaboration  with  the  rehabilitation  therapists  by  augmenting  their  existing  exercising  system.  We  present  the  implementation  details  of  the  system  and  some  of  the  initial  reactions  from  the  therapists  on  various  aspects  of  the  SHRUG  prototypes.
2	Handheld  versus  wearable  interaction  design  for  professionals  a  case  study  of  hospital  service  work.  With  the  blooming  of  new  available  wrist-worn  devices  there  are  potentials  for  these  to  support  the  work  done  in  many  professional  domains.  One  such  domain  is  hospital  service  work.  This  paper  explores  two  wearable  prototypes'  challenges  and  opportunities  to  support  future  hospital  service  work.  This  explorative  study  was  conducted  with  4  experienced  hospital  orderlies  who  interacted  with  an  application  across  two  wearable  concepts,  and  one  handheld  smartphone  in  five  scenarios,  not  involving  patients,  in  a  hospital  environment.  The  interactions  were  recorded  with  a  chest-mounted  camera  afterwards  semi-structured  interviews  with  each  participant  were  conducted.  This  study  shows  that  wearable  computers  can  effectively  support  the  maintenance  work  of  the  orderlies  and  has  domain-specific  advantages  over  the  handheld  smartphone,  e.g.,  the  former  support  glancing  at  the  task  information.  Furthermore,  we  outline  aspects  to  aid  designers  of  next  generation  wearable  designs  for  hospital  service  work.
2	Call  to  interact  communicating  interactivity  and  affordances  for  contactless  gesture  controlled  public  displays.  Interactive  public  displays  are  becoming  increasingly  commonplace.  Two  important  challenges  for  these  displays  exist:  1)  how  interactivity  is  communicated  to  the  passer-by  and  2)  once  the  audience  have  engaged,  how  the  interactive  affordances  are  communicated.  We  explore  these  challenges  through  the  lens  of  a  new  emerging  interaction  modality  -  contactless  mid-air  gesture  controlled  public  displays.  This  paper  presents  several  solutions  to  the  problems  above,  inspired  by  the  findings  existing  within  the  related  research  on  public  displays.  These  solutions  are  deployed  in  an  in-the-wild  setting  -  a  cinema  for  an  8  week  duration  -  and  evaluated  through  observational  and  quantitative  research.  The  data  forms  a  set  of  design  guidelines  for  successfully  attracting  the  passer-by  and  conveying  the  affordances  of  the  contactless  gestural  public  displays.
2	The  combination  of  cognitive  behavioural  therapy  with  virtual  reality  for  the  treatment  of  post  natal  depression.  Cognitive-behavioural  therapy  (CBT)  is  an  effective  treatment  for  post-natal  depression  (PND),  a  depressive  disorder  experienced  in  the  post-partum  period.  Virtual  reality  (VR)  has  never  been  used  for  the  treatment  of  PND.  This  pilot  study  investigated  the  feasibility  of  combining  CBT  and  VR  for  the  treatment  of  PND.  It  tests  the  entire  treatment  protocol,  including  the  VR  system.  The  results  show  that  the  combination  of  CBT  and  VR  is  feasible.  The  study  identifies  limitations  and  technical  difficulties.  It  provides  recommendations  for  the  better  implementation  of  VR  within  the  treatment  protocol  based  on  the  participants'  feedback.
2	A  wearable  device  for  recording  of  biopotentials  and  body  movements.  Long  term  recording  of  biomedical  signals  such  as  ECG,  EMG,  respiration  and  other  information  (e.g.  body  motion)  can  improve  diagnosis  and  potentially  monitor  the  evolution  of  many  widespread  diseases.  However,  long  term  monitoring  requires  specific  solutions,  portable  and  wearable  equipment  that  should  be  particularly  comfortable  for  patients.  The  key-issues  of  portable  biomedical  instrumentation  are:  power  consumption,  long-term  sensor  stability,  comfortable  wearing  and  wireless  connectivity.  In  this  scenario,  it  would  be  valuable  to  realize  prototypes  using  available  technologies  to  assess  long-term  personal  monitoring  and  foster  new  ways  to  provide  healthcare  services.  The  aim  of  this  work  is  to  discuss  the  advantages  and  the  drawbacks  in  long  term  monitoring  of  biopotentials  and  body  movements  using  textile  electrodes  embedded  in  clothes.  The  textile  electrodes  were  embedded  into  garments;  tiny  shirt  and  short  were  used  to  acquire  electrocardiographic  and  electromyographic  signals.  The  garment  was  equipped  with  low  power  electronics  for  signal  acquisition  and  data  wireless  transmission  via  Bluetooth.  A  small,  battery  powered,  biopotential  amplifier  and  three-axes  acceleration  body  monitor  was  realized.  Patient  monitor  incorporates  a  microcontroller,  analog-to-digital  signal  conversion  at  programmable  sampling  frequencies.  The  system  was  able  to  acquire  and  to  transmit  real-time  signals,  within  10  m  range,  to  any  Bluetooth  device  (including  PDA  or  cellular  phone).  The  electronics  were  embedded  in  the  shirt  resulting  comfortable  to  wear  for  patients.  Small  size  MEMS  3-axes  accelerometers  were  also  integrated.
2	Influence  of  motion  artifacts  on  a  smart  garment  for  monitoring  respiratory  rate.  Wearable  devices  are  gaining  large  acceptance  in  the  continuous  monitoring  of  vital  signs.  Among  others,  respiratory  rate  (f  R  )  can  be  used  to  detect  physiological  abnormalities  and  health  status  changes.The  purpose  of  this  work  was  to  investigate  how  the  output  of  a  smart  garment  used  for  respiratory  monitoring  is  influenced  by  walking  and  running.  This  garment  consists  of  three  bands,  each  one  embeds  two  piezoresistive  elements  sensitive  to  strain.Experimental  trials  were  carried  out  on  a  volunteer  who  worn  the  three  bands  at  the  level  of  upper  thorax,  inferior  thorax  and  abdomen  during  three  different  activities  (i.e.,  static  pose,  walking  and  running).  A  treadmill  was  used  to  set  specific  speeds  (i.e.,  from  1.6  km•h−1  to  8.8  km•h−1).  The  f  R  values  estimated  by  the  proposed  garment  were  compared  to  the  ones  monitored  by  a  reference  system  (i.e.,  a  flowmeter).The  analysis  in  the  frequency-domain  demonstrated  differences  up  to  3  bpm  between  the  average  f  R  estimated  by  the  two  systems.  The  mean  absolute  error  (MAE)  was  used  to  investigate  the  performances  of  the  garment  against  the  reference  device  in  estimating  the  instantaneous  f  R  .  MAE  increased  with  speed  (it  reached  1.8  bpm  during  running).  Bland-Altman  analysis  showed  a  bias  of  -0.02±2.02  bpm  when  all  the  data  of  walking  and  running  were  considered.The  garment  based  on  6  sensing  elements  provides  good  performances  for  estimating  both  average  and  instantaneous  f  R  values  during  activities  of  walking  and  running.
2	Eyes  understand  the  sketch  gaze  aided  stroke  grouping  of  hand  drawn  flowcharts.  Stroke  grouping  in  sketch  recognition  is  both  difficult  and  time-consuming.  Our  preliminary  experiment  indicates  that,  when  people  drawing  flowcharts,  their  gaze  focused  on  non-arrow  areas,  which  providing  a  spatial  cue  for  stroke  grouping.  Therefore,  we  present  a  novel  stroke  grouping  method  aided  by  gaze  information.  Based  on  gaze  data  that  is  collected  simultaneously  during  natural  drawing  process,  we  generate  hotspot  areas  serving  as  the  position  reference  of  semantic  symbols.  Strokes  are  first  roughly  grouped  by  the  hotspot  areas,  so  as  to  efficiently  decrease  the  searching  space.  Experiment  on  a  dataset  of  54  flowcharts  shows  that  time  efficiency  of  stroke  grouping  can  be  greatly  improved  in  our  method  and  there  is  much  potential  for  introducing  eye-gaze  data  in  sketch  recognition.
2	Users  eye  gaze  pattern  in  organization  based  recommender  interfaces.  In  this  paper,  we  report  the  hotspot  and  gaze  path  of  users'  eye-movements  on  three  different  layouts  for  recommender  interfaces.  One  is  the  standard  list  layout,  as  appearing  in  most  of  current  recommender  systems.  The  other  two  are  variations  of  organization  interfaces  where  recommended  items  are  organized  into  categories  and  each  category  is  annotated  by  a  title.  Gaze  plots  infer  that  the  organization  interfaces,  especially  the  quadrant  layout,  are  likely  to  arouse  users'  attentions  to  more  recommendations.  In  addition,  more  users  chose  products  from  the  organization  layouts.  Combining  the  results  with  our  prior  works,  we  suggest  a  set  of  design  guidelines  and  practical  implications  to  our  future  work.
2	Exploring  head  tracked  head  mounted  displays  for  first  person  robot  teleoperation.  We  explore  the  capabilities  of  head  tracking  combined  with  head  mounted  displays  (HMD)  as  an  input  modality  for  robot  navigation.  We  use  a  Parrot  AR  Drone  to  test  five  techniques  which  include  metaphors  for  plane-like  banking  control,  car-like  turning  control  and  virtual  reality-inspired  translation  and  rotation  schemes  which  we  compare  with  a  more  traditional  game  controller  interface.  We  conducted  a  user  study  to  observe  the  effectiveness  of  each  of  the  interfaces  we  developed  in  navigating  through  a  number  of  archways  in  an  indoor  course.  We  examine  a  number  of  qualitative  and  quantitative  metrics  to  determine  performance  and  preference  among  each  metaphor.  Our  results  show  an  appreciation  for  head  rotation  based  controls  over  other  head  gesture  techniques,  with  the  classic  controller  being  preferred  overall.  We  discuss  possible  shortcomings  with  head  tracked  HMDs  as  a  primary  input  method  as  well  as  propose  improved  metaphors  that  alleviate  some  of  these  drawbacks.
2	A  taxonomy  of  deviant  encodings.  The  main  objective  of  this  paper  is  to  design  a  common  background  for  various  philosophical  discussions  about  adequate  conceptual  analysis  of  “computation”.
2	On  maximal  block  functions  of  computable  η  like  linear  orderings.  We  prove  the  existence  of  a  computable  η-like  linear  ordering  Open  image  in  new  window  such  that,  for  any  \(\Pi^0_2\)  function  G  :  ℚ  →  ℕ  ∖  {0}  and  linear  ordering  Open  image  in  new  window  ,  Open  image  in  new  window  does  not  have  order  type  τ  =  ∑  {  G(q)  |  q  ∈  ℚ  }.
2	Towards  computable  analysis  on  the  generalised  real  line.  In  this  paper  we  use  infinitary  Turing  machines  with  tapes  of  length  \(\kappa  \)  and  which  run  for  time  \(  \kappa  \)  as  presented,  e.g.,  by  Koepke  &  Seyfferth,  to  generalise  the  notion  of  type  two  computability  to  \({2}^{\kappa  }\),  where  \(\kappa  \)  is  an  uncountable  cardinal  with  \(\kappa  ^{<\kappa  }=\kappa  \).  Then  we  start  the  study  of  the  computational  properties  of  \({{\mathrm{\mathbb  {R}}}}_\kappa  \),  a  real  closed  field  extension  of  \({{\mathrm{\mathbb  {R}}}}\)  of  cardinality  \({2}^{\kappa  }\),  defined  by  the  first  author  using  surreal  numbers  and  proposed  as  the  candidate  for  generalising  real  analysis.  In  particular  we  introduce  representations  of  \({{\mathrm{\mathbb  {R}}}}_\kappa  \)  under  which  the  field  operations  are  computable.  Finally  we  show  that  this  framework  is  suitable  for  generalising  the  classical  Weihrauch  hierarchy.  In  particular  we  start  the  study  of  the  computational  strength  of  the  generalised  version  of  the  Intermediate  Value  Theorem.
2	Consecutive  ones  property  testing  cut  or  swap.  Let  C  be  a  finite  set  of  n  elements  and  R  =  {r1,  r2,  ...,  rm}  a  family  of  m  subsets  of  C.  The  family  R  verifies  the  consecutive  ones  property  if  there  exists  a  permutation  P  of  C  such  that  each  ri  in  R  is  an  interval  of  P.  Several  algorithms  have  been  proposed  to  test  this  property  in  time  O(Σi=1m|ri|),  all  being  involved.  We  present  a  simpler  algorithm,  based  on  a  new  partitioning  scheme.
2	Rehabilitation  outcome  in  patients  undergone  hip  or  knee  replacement  surgery  using  inertial  technology  for  gait  analysis.  Modern  biomedical  technologies  are  increasingly  spreading  to  overcome  the  limitation  based  on  the  use  of  qualitative  methods  such  as  clinical  scales,  in  fact  in  the  clinical  practice  wearable  inertial  systems  for  gait  analysis  are  spreading  to  support  the  clinical-therapeutic  decision  and  to  control  the  follow-up  of  patients.  The  purpose  of  this  study  is  to  investigate  the  rehabilitation  outcome  of  patients  under  examination  in  terms  of  spatiotemporal  parameters  using  the  Stand  and  Walk  test.  A  population-based  sample  of  30  post-surgical  patients  undergone  hip  or  knee  replacement  surgery  was  studied;  gender,  age,  weight,  drug  therapy  and  physiotherapy  treatment  were  recorded.  Data  were  calculated  using  a  wearable  inertial  system  for  gait  analysis:  Opal  System  by  APDM  and  analyzed  using  ANOVA  test.  Overall,  ANOVA  test  between  admission  phase  and  discharge  one,  it  emerges  that  there  is  a  significant  statistical  difference  on  six  spatiotemporal  parameters  related  to  gait,  turning  and  anticipatory  postural  adjustments  (APA).  Study  results  suggest  that  there  is  an  improvement  in  mobility  of  patients  hospitalized  at  the  Operating  Rehabilitation  and  Functional  Recovery  Unit  of  ICS  Maugeri  Institute  of  Care  and  Scientific  Research  of  Bari  (Italy)  after  only  a  month  of  rehabilitation  treatment.  This  result  suggest  the  importance  of  rehabilitation  and  of  the  rehabilitation  institutes  in  the  follow-up  of  post-surgical  patients.
2	Musical  and  conversational  artificial  intelligence.  Music  production  software  often  has  complex  interfaces  and  needs  the  user  to  know  the  basic  musical  know-how.  In  this  paper,  we  present  a  conversational  agent  that  allows  creating  music  in  a  simplified  way  through  voice-based  interaction.  Indeed,  our  agent  can  be  configured  and  customized  with  simple  and  natural  voice  commands.  In  addition,  it  has  some  typically  human  cognitive  skills  to  produce  music:  it  listens  to  the  user  while  singing  a  song  and  generates  a  melody  by  discovering  and  copying  the  patterns  of  her/his  human  voice.  Technologically,  the  system  is  empowered  by  Google  Dialogflow  for  conversation  management  and  uses  an  advanced  technique  called  abstract  melody  for  music  production.  This  Musical  and  Conversational  Artificial  Intelligence  is  an  actual  innovation  since  it  does  not  require  any  preliminary  knowledge  about  music  and,  consequently,  includes  professionals,  but  also  children,  beginners,  and  people  with  physical  disease.
2	Measurements  of  rat  brain  activity  originating  from  ultrasound  waves  in  air.  Mammals  such  as  rats  and  bats  process  high  frequency  ultrasound  waves,  through  their  acoustic  nerves,  by  codifying  acoustic  signals  by  means  of  a  complex  biological  neural  network.  Similarly,  sonar  systems  based  on  piezoelectric  polymers  process  ultrasounds  in  air  which,  properly  elaborated  and  codified  by  an  artificial  neural  network,  are  suitable  for  stimulating  the  central  nervous  system,  exactly  the  same  as  the  ultrasonic  information  carried  along  the  acoustic  nerve  in  mammals.  Our  experiments  aim  at  investigating,  measuring  and  characterizing,  the  reaction  of  mammals  to  an  external  stimulus  perceivable  by  artificial  sense  organs  and  not  forced  by  a  standard  electrical  instrumentation.  Electroencephalography  recordings  were  performed  in  order  to  analyze  the  cortical  effects  of  the  received  ultrasonic  signal.  Preliminary  measurements  of  brain  activity  demonstrated  that  electrical  information  elaborated  from  ultrasound  can  be  transferred  to  the  brain  in  order  to  obtain  a  response  related  to  both  signal  characteristics  and  the  stimulated  area.
2	Understanding  the  role  of  the  modality  principle  in  multimedia  learning  environments.  The  modality  principle  states  that  low-experience  learners  more  successfully  understand  information  that  uses  narration  rather  than  on-screen  text.  This  is  due  to  the  idea  that  on-screen  text  may  produce  a  cognitive  overload  if  it  is  accompanied  by  other  visual  elements.  Other  studies  provided  additional  data  and  support  for  the  modality  principle  in  multimedia  learning  environments.  However,  some  recent  studies  began  to  show  the  modality  principle's  impact  had  certain  parameters,  and  it  was  impacting  various  groups,  conditions  and  environments  differently.  This  study  replicated  Mayer  and  Moreno's  studies  on  the  modality  principle  (1998;  2006).  Seventy-nine  college  students  attending  a  Midwestern  University  in  the  USA  participated  in  this  study  in  2015.    The  results  of  the  study  showed  that  the  modality  principle  was  not  an  effective  strategy  for  the  group  of  low-experience  content  users.  The  results  showed  the  retention  and  transfer  of  knowledge  was  not  as  effective  for  students  who  viewed  the  narrated  PowerPoint  presentation.  In  fact,  students  who  viewed  the  PowerPoint  presentation  that  only  included  the  on-screen  text,  had  more  effective  retention  and  transfer  of  knowledge.    Lay  descriptionWhat  is  currently  known  about  the  modality  principle:  Learners  are  more  successful  with  understanding  information  that  uses  narration  than  on-screen  text  specifically  low-experience  learners.  This  is  due  to  the  idea  that  on-screen  text  may  produce  a  cognitive  overload  if  it  is  accompanied  by  other  visual  elements.      What  this  study  adds  to  this  subject  matter:  Carefully  replicated  Mayer  and  Moreno's  studies  on  the  modality  principle  with  the  intent  to  verify  if  the  modality  principle  held  with  digital-aged  students.  Data  collection  took  place  in  the  participants'  actual  classroom  (natural  setting)  in  opposition  to  a  lab-like  setting.      Implications  of  study  findings  for  practitioners:  Results  counter  the  original  study's  findings  specifically  for  low-experience  learners,  meaning  on-screen  text  was  more  effective  for  transfer  and  retention  of  knowledge  when  compared  with  printed  text  and  narration.  When  designing  multimedia  learning,  instructional  designers  need  to  consider  other  elements  (e.?g.,  presentation  speed  and  learning  times)  that  may  be  causing  a  change  in  the  modality  effect.
2	Facilitating  collaboration  in  lecture  based  learning  through  shared  notes  using  wireless  technologies.  This  paper  reports  a  case  study  for  developing  lecture  teaching  in  higher  education  by  connecting  simultaneously  the  benefits  of  face-to-face  teaching  and  social  software  for  capturing  and  sharing  students'  lecture  notes.  The  study  was  conducted  with  12  university  students  taking  a  degree  course  on  pre-primary  education.  Data  were  collected  on  (1)  the  nature  of  the  shared  lecture  notes  produced  by  the  students;  and  (2)  their  experiences  in  creating  and  sharing  lecture  notes.  Students  wrote  367  notes  in  eight  lecture  sessions.  Discourse  analysis  revealed  five  types  of  notes:  reproducing  lecture  content;  summarizing  lecture  content;  connecting  key  concepts;  developing  lecture  content;  questions  arising  from  lecture  content.  Content  analysis  revealed  those  aspects  of  the  lectures  developed  through  the  shared  notes.  Discussions  with  four  students  at  the  end  of  the  course  explored  their  experiences  of  using  the  shared  notes.  The  results  are  discussed  in  the  context  of  changes  to  the  cultural  ecology  of  learning.
2	Using  360  degree  videos  in  teacher  education  to  improve  preservice  teachers  professional  interpersonal  vision.  Noticing  and  interpreting  classroom  events  (professional  vision)  is  an  important  element  of  preservice  teachers'  (PSTs)  interpersonal  competence.  This  paper  presents  a  mixed-method  study  about  a  classroom  simulation  using  360-degree  videos  combined  with  theoretical  lectures  in  teacher  education,  intended  to  improve  PSTs'  interpretations  of  noticed  events.  Furthermore,  this  study  examined  how  PSTs  evaluate  technological  and  educational  affordances  of  360-degree  videos.  Results  indicate  that  participating  PSTs  improved  in  noticing  classroom  events  and  in  applying  a  more  theory-based  terminology  to  describe  these  events.  PSTs  perceived  observing  other  teachers  teach  as  an  educational  affordance  for  mastering  theory  and  for  developing  insights  about  interpersonal  teacher  behaviour.  Concerning  technological  affordances,  mainly  physical  discomforts  and  technical  hindrances,  was  reported  by  PSTs.  The  results  of  this  study  imply  that  360-degree  videos  can  be  useful  for  teacher  education  to  improve  PSTs'  interpretation  of  noticed  events.
2	Could  a  mobile  assisted  learning  system  support  flipped  classrooms  for  classical  chinese  learning.  In  this  study,  the  researcher  aimed  to  develop  a  mobile-assisted  learning  system  and  to  investigate  whether  it  could  promote  teenage  learners'  classical  Chinese  learning  through  the  flipped  classroom  approach.  The  researcher  first  proposed  the  structure  of  the  Cross-device  Mobile-Assisted  Classical  Chinese  CMACC  system  according  to  the  pilot  survey  and  reviewed  literature,  and  then  adopted  a  quasi-experimental  design  to  understand  whether  the  developed  system  could  promote  and  support  flipped  classroom  learning  for  classical  Chinese.  A  total  of  56  eleventh  graders  from  two  classes  participated  in  the  experiment.  The  learners  in  the  experimental  group  learned  classical  Chinese  with  the  flipped  classroom  learning  strategy  with  the  assistance  of  the  CMACC  system,  while  the  control  group  adopted  the  flipped  classroom  learning  strategy  without  using  the  CMACC  system.  The  results  reveal  that  all  of  the  students  improved  their  Chinese  performance,  but  it  was  noticeable  that  the  learners  who  used  the  CMACC  system  showed  better  motivation  in  terms  of  self-directed  preview  learning,  while  those  who  only  learned  with  the  traditional  textbooks  tended  to  be  more  passive.  In  sum,  the  mobile-assisted  learning  system  added  value  in  providing  learners  with  opportunities  to  achieve  anytime  and  anywhere  flipped  classroom  learning.  The  integration  of  ubiquitous  mobile  learning  technology  and  the  flipped  classroom  strategy  can  be  viewed  as  a  critical  factor  leading  to  students  achieving  self-regulated  learning.  It  is  also  suggested  that  instructors  should  carefully  take  the  targeted  learners'  cultural  background  and  the  availability  of  supporting  learning  devices  into  consideration  so  as  to  prevent  the  flipped  classroom  from  exacerbating  the  digital  divide.  Other  suggestions  for  educators  and  instructional  designers  are  also  proposed.
2	Elvis  extensible  log  visualization.  In  this  article,  we  propose  ELVIS,  a  security-oriented  log  visualization  tool  that  allows  security  experts  to  visually  explore  numerous  types  of  log  files  through  relevant  representations.  When  a  log  file  is  loaded  into  ELVIS,  a  summary  view  is  displayed.  This  view  is  the  starting  point  for  exploring  the  log.  The  analyst  can  then  choose  to  explore  certain  fields  or  sets  of  fields  from  the  dataset.  To  that  end,  ELVIS  selects  relevant  representations  according  to  the  fields  chosen  by  the  analyst  for  display.
2	Using  wikis  and  collaborative  learning  for  science  teachers  professional  development.  Wiki  bears  great  potential  to  transform  learning  and  instruction  by  scaffolding  personal  and  social  constructivism.  Past  studies  have  shown  that  proper  application  of  wiki  benefits  both  students  and  teachers;  however,  few  studies  have  integrated  wiki  and  collaborative  learning  to  examine  the  growth  of  science  teachers'  Technological,i¾?Pedagogical  andi¾?Contenti¾?KnowledgeTPACK.  This  study  introduced  a  wiki-based  TPACK  growth  model  and  examined  nine  elementary  and  middle  science  teachers'  knowledge  growth  in  a  graduate-level  course.  Data  sources  included  reflective  journals,  wiki  data  and  interviews.  Results  showed  that  with  wiki,  science  teachers  learned  to  design  more  understandable  and  lively  science  teaching  content,  and  they  collaboratively  generated  creative  instructional  strategies.  Furthermore,  wiki  and  collaborative  learning  helped  in-service  teachers  exchange  and  elaborate  ideas  related  to  the  development  of  TPACK.  Implications,  suggestions  and  future  research  directions  were  put  forward  regarding  wiki,  TPACK  and  in-service  teachers'  professional  development.
2	Virca  net  a  case  study  for  collaboration  in  shared  virtual  space.  To  take  up  the  challenge  of  global  distributed  companies,  Manufacturing  Engineering  has  to  be  accomplished  in  a  worldwide  collaborative  way.  Design  and  development  of  production  processes  and  their  execution  are  nowadays  spread  all  over  the  world.  To  ensure  quality  standards  and  to  create  synergies  between  spatially  distributed  entities,  distance  collaboration  tools  must  be  provided  to  allow  cooperation  even  over  large  distances.  The  Virtual  Reality  (VR)  is  thereby  offering  beneficial  capabilities  to  exchange  current  planning  stages,  identify  problems  and  solve  them  cooperatively.  This  paper  introduces  a  new  approach  for  distance  collaboration,  which  enables  users  to  work  in  a  joint  virtual  space,  nearly  as  if  they  were  working  together  in  the  same  place.  Therefore  two  full-immersive  CAVE-like  systems  are  interconnected  using  the  VirCA  (Virtual  Collaboration  Arena)  platform.  The  paper  describes  the  requirements  of  Mechanical  Engineers  towards  distance  collaboration  tools  and  the  technical  challenges  solved  with  the  enhanced  VirCA  framework.  This  working  prototype  is  one  premier  application  in  the  field  of  VR-enhanced  spatially  distributed  collaboration.  A  typical  use  case  scenario  is  provided  to  highlight  the  interaction  and  cooperation  capabilities  offered  by  VirCA  NET.
2	Inquiry  based  learning  and  retrospective  action  problematizing  student  work  in  a  computer  supported  learning  environment.  We  examined  student  performance  in  a  computer-supported  learning  environment  after  students  undertook,  among  others,  a  graphing  task  within  an  inquiry  context.  Students  were  assigned  in  two  conditions:  (a)  Students  were  given  one  variable,  and  they  had  to  select  the  second  one  to  construct  their  graph;  (b)  students  were  given  four  variables,  and  they  had  to  select  two  to  construct  their  graph.  Both  conditions  problematized  student  work  by  triggering  retrospective  action,  where  students  returned  to  previous  stages  of  the  learning  activity  sequence.  Retrospective  action  correlated  positively  to  knowledge  gains  in  Condition  2,  where  students  were  more  likely  to  revisit  earlier  stages  of  their  inquiry.  Time-on-task,  when  students  passed  through  learning  tasks  for  the  first  time,  correlated  negatively  with  retrospective  action  (second  pass),  which  indicated  that  there  was  a  minimum  amount  of  time  needed  to  effectively  execute  tasks.  Trade-offs  between  time-on-task  (first  pass)  and  retrospective  action  demarcate  a  novel  field  of  research.
2	Lane  marking  detection  using  image  features  and  line  fitting  model.  The  lane  marking  detection  task  is  an  essential  process  in  the  field  of  semi-autonomous  and  autonomous  navigation.  This  paper  proposes  a  method  that  combines  the  color  and  edge  information  to  robustly  detect  the  lane  marking  within  the  image  either  located  far  on  near  to  the  vehicle.  Firstly,  the  region  of  interest  is  extracted  from  the  image.  Secondly,  the  set  of  lane  marking  features  are  extracted.  To  do  that,  the  change  in  color  between  road  and  marking  surface  is  used  along  a  probability  density  function  to  extract  the  set  of  candidates.  Finally,  a  clustering  method  along  a  line  fitting  model  is  implemented.  Preliminary  results  were  performed  and  tested  on  a  group  of  consecutive  frames  to  prove  the  effectiveness  of  the  proposed  method.
2	Classifying  document  categories  based  on  physiological  measures  of  analyst  responses.  Improvements  in  the  collection  and  analysis  of  physiological  signals  has  increased  the  potential  for  computer  systems  to  assist  human  analysts  in  various  workplace  tasks.  We  have  constructed  a  data  set  of  documents  with  three  main  categories  of  documents,  being  related  to  national  security,  natural  disasters  and  computer  science,  ranging  from  stressful  to  non-stressful.  We  include  some  documents  which  contain  more  than  one  of  these  categories  and  some  which  contain  none  of  these  categories.  The  document  collection  is  designed  to  mimic  the  range  of  documents  an  intelligence  analyst  would  need  to  read  quickly  and  categorize  in  the  few  days  after  the  seizure  of  computers  from  suspects  in  a  national  security  investigation.  Our  participants  were  university  students,  primarily  our  own  computer  science  students,  hence  the  inclusion  of  the  computer  science  category.  We  found  that  on  our  dataset  our  participants  were  79%  correct  on  average,  which  we  could  replicate  with  88%  accuracy,  that  is,  by  a  70%  correctness  on  the  underlying  task.  The  worst  results  by  our  participants  was  on  the  computer  science  task  which  was  surprising,  but  this  did  not  reduce  the  performance  of  our  replicating  the  results  using  AI  techniques.
2	Industry  oriented  enhancement  of  information  management  systems  at  audi  hungaria  using  maxwhere  s  3d  digital  environments.  This  paper  investigates  how  3D  digital  environments  may  be  used  to  replace  2D  window-like  user  interfaces  on  everyday  computers,  potentially  leading  to  important  benefits  in  the  management  of  large-sized  industrial  enterprises.  The  paper  briefly  reviews  the  evolution  from  DOS  to  Windows,  and  more  recently  from  Windows  to  3D  spaces.  The  paper  also  discusses  how  the  latest  3D  digital  environments  improve  user  effectiveness.  This  is  followed  by  the  presentation  of  a  use  case  scenario  based  on  an  implementation  of  a  management  system,  that  is  used  by  both  employees  and  management,  in  a  3D  digital  environment  at  Audi  Hungaria.  The  paper  concludes  that  considerable  improvements  can  be  made  to  information  management  systems  using  3D  technologies  in  the  near  future.
2	Comparison  of  perceptual  features  efficiency  for  automatic  identification  of  emotional  states  from  speech.  The  following  paper  presents  parameterization  of  emotional  speech  using  perceptual  coefficients  as  well  as  a  comparison  of  Mel  Frequency  Cepstral  Coefficients  (MFCC),  Bark  Frequency  Cepstral  Coefficients  (BFCC),  Perceptual  Linear  Prediction  Coefficients  (PLP)  and  Revised  Perceptual  Linear  Prediction  Coefficients  (RPLP).  Analysis  was  performed  on  two  different  databases:  Database  of  Polish  Emotional  Speech  and  the  most  commonly  used  for  emotion  recognition  -  Berlin  Database  of  Emotional  Speech.  Both  consist  of  acted  emotional  speech  grouped  into  six  classes  of  primary  emotions.  Emotion  classification  was  performed  using  k-NN  algorithm.
2	Analog  circuits  testing  by  means  of  walsh  hadamard  spectrum  of  supply  current  transient  state  monitoring.  The  paper  presents  a  practical  technique  of  the  analog  circuits  testing  based  on  its  supply  current  waveform  evaluation.  The  decision  about  an  actual  circuit  under  test  state  is  made  during  the  circuit  power  turn  on  stage.  The  step  of  the  supply  voltage  with  an  assumed  rise  time  stimulates  the  circuit  and  the  differential  of  its  current  response  is  decomposed  by  means  of  Walsh-Hadamard  transform.  The  analysis  of  the  spectra  in  the  sequences  domain  allows  to  detect  faults  presence  effectively.  Finally,  after  the  circuit  testing  on  launching  stage,  only  in  case  of  its  healthy  state  confirmation,  it  is  admitted  for  use.  Thanks  to  the  minimal  computational  effort  necessary  for  the  applied  kind  of  transformation,  the  proposed  method  is  suitable  to  build  in  reliable  systems  with  the  use  of  a  cheap  microcontroller.  Besides,  the  numerical  complexity  of  the  method  can  be  easily  reduced  additionally  by  the  set  of  the  observed  spectra  points  minimization  by  means  of  genetic  algorithm.
2	Contextual  consent  ethical  mining  of  social  media  for  health  research.  Social  media  are  a  rich  source  of  insight  for  data  mining  and  user-centred  research,  but  the  question  of  consent  arises  when  studying  such  data  without  the  express  knowledge  of  the  creator.  Case  studies  that  mine  social  data  from  users  of  online  services  such  as  Facebook  and  Twitter  are  becoming  increasingly  common.  This  has  led  to  calls  for  an  open  discussion  into  how  researchers  can  best  use  these  vast  resources  to  make  innovative  findings  while  still  respecting  fundamental  ethical  principles.  In  this  position  paper  we  highlight  some  key  considerations  for  this  topic  and  argue  that  the  conditions  of  informed  consent  are  often  not  being  met,  and  that  using  social  media  data  that  some  deem  free  to  access  and  analyse  may  result  in  undesirable  consequences,  particularly  within  the  domain  of  health  research  and  other  sensitive  topics.  We  posit  that  successful  exploitation  of  online  personal  data,  particularly  for  health  and  other  sensitive  research,  requires  new  and  usable  methods  of  obtaining  consent  from  the  user.
2	Vonda  a  framework  for  ontology  based  dialogue  management.  We  present  VOnDA,  a  framework  to  implement  the  dialogue  management  functionality  in  dialogue  systems.  Although  domain-independent,  VOnDA  is  tailored  towards  dialogue  systems  with  a  focus  on  social  communication,  which  implies  the  need  of  long-term  memory  and  high  user  adaptivity.  For  these  systems,  which  are  used  in  health  environments  or  elderly  care,  margin  of  error  is  very  low  and  control  over  the  dialogue  process  is  of  topmost  importance.  The  same  holds  for  commercial  applications,  where  customer  trust  is  at  risk.  VOnDA's  specification  and  memory  layer  relies  upon  (extended)  RDF/OWL,  which  provides  a  universal  and  uniform  representation,  and  facilitates  interoperability  with  external  data  sources,  e.g.,  from  physical  sensors.
2	Measuring  bot  and  human  behavioral  dynamics.  Bots,  social  media  accounts  controlled  by  software  rather  than  by  humans,  have  recently  been  under  the  spotlight  for  their  association  with  various  forms  of  online  manipulation.  To  date,  much  work  has  focused  on  social  bot  detection,  but  little  attention  has  been  devoted  to  the  characterization  and  measurement  of  the  behavior  and  activity  of  bots,  as  opposed  to  humans'.  Over  the  course  of  the  years,  bots  have  become  more  sophisticated,  and  capable  to  reflect  some  short-term  behavior,  emulating  that  of  human  users.  The  goal  of  this  paper  is  to  study  the  behavioral  dynamics  that  bots  exhibit  over  the  course  of  one  activity  session,  and  highlight  if  and  how  these  differ  from  human  activity  signatures.  By  using  a  large  Twitter  dataset  associated  with  recent  political  events,  we  first  separate  bots  and  humans,  then  isolate  their  activity  sessions.  We  compile  a  list  of  quantities  to  be  measured,  like  the  propensity  of  users  to  engage  in  social  interactions  or  to  produce  content.  Our  analysis  highlights  the  presence  of  short-term  behavioral  trends  in  humans,  which  can  be  associated  with  a  cognitive  origin,  that  are  absent  in  bots,  intuitively  due  to  their  automated  activity.  These  findings  are  finally  codified  to  create  and  evaluate  a  machine  learning  algorithm  to  detect  activity  sessions  produced  by  bots  and  humans,  to  allow  for  more  nuanced  bot  detection  strategies.
2	Swar  the  voice  operated  pc.  Keyboard,  although  a  popular  medium,  is  not  very  convenient  as  it  requires  a  certain  amount  of  skill  for  effective  usage.  A  mouse  on  the  other  hand  requires  a  good  hand-eye  co-ordination.  Also  current  computer  interfaces  also  assume  a  certain  level  of  literacy  from  the  user.  It  also  expect  the  user  to  have  certain  level  of  proficiency  in  English.  In  our  country  where  the  literacy  level  is  as  low  as  50%  in  some  states,  if  information  technology  has  to  reach  the  grass  root  level;  these  constraints  have  to  be  eliminated.  As  a  solution  for  these,  Speech  Recognition  and  hence  the  concept  of  Voice  operated  computer  system  comes  into  picture.  In  this  paper  we  propose  a  technique  to  develop  a  voice  recognition  system  which  will  be  used  for  controlling  computer  via  speech  input  from  any  user  i.e.  without  the  use  of  mouse  and  /  or  keyboard.  Once  developed  this  system  would  be  of  great  benefit  to  physically  handicapped  people  as  Instead  of  scrolling  through  written  procedures  on  a  laptop  or  handheld  computer,  they  can  wear  a  headset  and  have  their  hands  and  eyes  free.
2	Intelligent  tutoring  systems  a  comprehensive  historical  survey  with  recent  developments.  This  paper  provides  interested  beginners  with  an  updated  and  detailed  introduction  to  the  field  of  Intelligent  Tutoring  Systems  (ITS).  ITSs  are  computer  programs  that  use  artificial  intelligence  techniques  to  enhance  and  personalize  automation  in  teaching.  This  paper  is  a  literature  review  that  provides  the  following:  First,  a  review  of  the  history  of  ITS  along  with  a  discussion  on  the  interface  between  human  learning  and  computer  tutors  and  how  effective  ITSs  are  in  contemporary  education.  Second,  the  traditional  architectural  components  of  an  ITS  and  their  functions  are  discussed  along  with  approaches  taken  by  various  ITSs.  Finally,  recent  innovative  ideas  in  ITS  systems  are  presented.  This  paper  concludes  with  some  of  the  author's  views  regarding  future  work  in  the  field  of  intelligent  tutoring  systems.
2	Interactive  graph  query  language  for  multidimensional  data  in  collaboration  spotting  visual  analytics  framework.  Human  reasoning  in  visual  analytics  of  data  networks  relies  mainly  on  the  quality  of  visual  perception  and  the  capability  of  interactively  exploring  the  data  from  different  facets.  Visual  quality  strongly  depends  on  networks'  size  and  dimensional  complexity  while  network  exploration  capability  on  the  intuitiveness  and  expressiveness  of  user  frontends.  The  approach  taken  in  this  paper  aims  at  addressing  the  above  by  decomposing  data  networks  into  multiple  networks  of  smaller  dimensions  and  building  an  interactive  graph  query  language  that  supports  full  navigation  across  the  sub-networks.  Within  sub-networks  of  reduced  dimensionality,  structural  abstraction  and  semantic  techniques  can  then  be  used  to  enhance  visual  perception  further.
2	Carina  interactive  million  node  graph  visualization  using  web  browser  technologies.  We  are  working  on  a  scalable,  interactive  visualization  system,  called  Carina,  for  people  to  explore  million-node  graphs.  By  using  latest  web  browser  technologies,  Carina  offers  fast  graph  rendering  via  WebGL,  and  works  across  desktop  (via  Electron)  and  mobile  platforms.  Different  from  most  existing  graph  visualization  tools,  Carina  does  not  store  the  full  graph  in  RAM,  enabling  it  to  work  with  graphs  with  up  to  69M  edges.  We  are  working  to  improve  and  open-source  Carina,  to  offer  researchers  and  practitioners  a  new,  scalable  way  to  explore  and  visualize  large  graph  datasets.
2	Liger  combining  interaction  paradigms  for  visual  analysis.  Visualization  tools  usually  leverage  a  single  interaction  paradigm  (e.g.,  manual  view  specification,  visualization  by  demonstration,  etc.),  which  fosters  the  process  of  visualization  construction.  A  large  body  of  work  has  investigated  the  effectiveness  of  individual  interaction  paradigms,  building  an  understanding  of  advantages  and  disadvantages  of  each  in  isolation.  However,  how  can  we  leverage  the  benefits  of  multiple  interaction  paradigms  by  combining  them  into  a  single  tool?  We  currently  lack  a  holistic  view  of  how  interaction  paradigms  that  use  the  same  input  modality  (e.g.,  mouse)  can  be  combined  into  a  single  tool  and  how  people  use  such  tools.  To  investigate  opportunities  and  challenges  in  combining  paradigms,  we  first  created  a  multi-paradigm  prototype  (Liger)  that  combines  two  mouse-based  interaction  paradigms  (manual  view  specification  and  visualization  by  demonstration)  in  a  unified  tool.  We  then  conducted  an  exploratory  study  with  Liger,  providing  initial  evidence  that  people  1)  use  both  paradigms  interchangeably,  2)  seamlessly  switch  between  paradigms  based  on  the  operation  at  hand,  and  3)  choose  to  successfully  complete  a  single  operation  using  a  combination  of  both  paradigms.
2	Towards  grad  cam  based  explainability  in  a  legal  text  processing  pipeline.  Explainable  AI(XAI)is  a  domain  focused  on  providing  interpretability  and  explainability  of  a  decision-making  process.  In  the  domain  of  law,  in  addition  to  system  and  data  transparency,  it  also  requires  the  (legal-)  decision-model  transparency  and  the  ability  to  understand  the  models  inner  working  when  arriving  at  the  decision.  This  paper  provides  the  first  approaches  to  using  a  popular  image  processing  technique,  Grad-CAM,  to  showcase  the  explainability  concept  for  legal  texts.  With  the  help  of  adapted  Grad-CAM  metrics,  we  show  the  interplay  between  the  choice  of  embeddings,  its  consideration  of  contextual  information,  and  their  effect  on  downstream  processing.
2	Using  embodied  audio  visual  interaction  to  promote  social  encounters  around  large  media  facades.  In  this  paper  we  describe  the  design  of  a  large-scale  interactive  light  and  music  intervention  on  a  corporate  high  rise  building  and  its  surrounding  urban  area.  Designing  for  interaction  with  media  facades  has  traditionally  posed  challenges  regarding  proxemics,  scale  of  the  augmented  architecture  and  placement  of  interactive  spaces.  With  the  increasing  availability  and  affordability  of  interactive  technologies,  factors  such  as  playability  and  tangibility  are  assumed  not  only  to  be  present  but  also  to  enable  richer  collective  experiences.  We  propose  a  new  approach  for  interaction  with  large  media  facades  employing  embodied  audio-visual  interaction  at  the  floor  level.  That  way,  the  floor  level  serves  as  proxy  for  interacting  with  the  media  facade  whilst  facilitating  social  encounters.  We  discuss  aspects  considered  during  different  phases  of  the  project  development  and  derive  principles  for  connecting  zones  of  proxemics,  promoting  encounters  by  distributing  the  performance,  designing  for  urban  activation  and  isolating  implementation  concerns.
2	Liveness  localization  and  lookahead  interaction  elements  for  parametric  design.  Scripting  has  become  an  integral  part  of  design  work  in  Computer-Aided  Design  (CAD),  especially  with  parametric  systems.  Designers  who  script  face  a  very  steep  learning  and  use  curve  due  to  the  new  (to  them)  script  notation  and  the  loss  of  direct  manipulation  of  the  model.  Programming  In  the  Model  (PIM)  is  a  prototype  parametric  CAD  system  with  a  live  interface  with  side-by-side  model  and  script  windows;  real-time  updating  of  the  script  and  the  model;  on-demand  dependency,  object  and  script  representations  in  the  model;  and  operation  preview  (lookahead).  These  features  aim  to  break  the  steep  learning  and  use  curve  of  scripting  into  small  steps  and  to  bring  programming  and  modeling  tasks  `closer  together.'  A  qualitative  user  study  with  domain  experts  shows  the  importance  of  multi-directional  live  scripting  and  script  localization  within  the  model.  Other  PIM  features  show  promise  but  require  additional  design  work  to  create  a  better  user  experience.
2	Do  you  care  if  a  computer  says  sorry  user  experience  design  through  affective  messages.  While  traditional  HCI  research  emphasizes  usability  based  on  models  of  cognition,  user  experience  (UX)  focuses  on  affect  and  emotion  through  the  provision  of  positive  interactive  experiences.  Providing  affective  cues,  such  as  apologetic  on-screen  display  messages,  appears  to  be  a  way  to  influence  users'  affective  states  as  well  as  their  perceptions  toward  an  information  retrieval  system.  A  study  was  designed  to  determine  whether  users'  affect  and  perceptions  differ  between  three  types  of  systems:  neutral,  apologetic,  and  non-apologetic.  Our  results  revealed  that  the  users  perceived  the  apologetic  system  as  more  aesthetically  appealing  and  usable  than  the  neutral  or  non-apologetic  system.  The  result  also  showed  that  users'  frustration  was  the  lowest  when  using  the  apologetic  system.  We  discuss  the  implications  of  these  results  in  designing  a  more  experience-centered  system.
2	A  qualitative  study  of  workplace  intercultural  communication  tensions  in  dyadic  face  to  face  and  computer  mediated  interactions.  We  present  findings  from  a  qualitative  study  with  28  participants  of  the  dyadic  intercultural  communication  tensions  professionals  experience  in  Face-to-Face  (FTF)  and  Computer-Mediated  Communication  (CMC)  workplace  interactions.  We  identify  four  categories  of  intercultural  communication  tensions  that  emerged  most  frequently  in  our  dataset  including  range  of  emotional  expression,  level  of  formality,  "fixed"  versus  flexible  appointments  and  task  versus  social-orientation.  We  discuss  how  these  tensions  manifested  in  FTF  and  CMC  media  and  unravel  the  ways  media  supports  or  hinders  intercultural  communication.  We  present  the  adaptations  participants  made  to  mitigate  such  tensions  and  offer  implications  for  design.  Our  findings  demonstrate  that  the  most  frequently  occurring  intercultural  communication  tensions  manifested  in  both  FTF  and  CMC,  regardless  of  the  medium  used.  This  indicates  that  cultural  communication  challenges  will  persist  no  matter  the  medium,  highlighting  the  opportunity  for  technologies  to  better  support  workplace  intercultural  communication.
2	Iot  designing  for  human  values.  This  one-day,  embedded  workshop  will  explore  the  design  intersections  of  human  values  and  internet  of  things  (IoT)  applications.  In  a  day-long  session  we  will  configure  and  build  small  IoT  devices  (using  the  Particle  platform),  then  deploy  them  to  collect,  share  and  publish  the  data  they  harvest  throughout  the  conference.  During  the  conference  program  we  will  reconvene  to  debate  how  a  world  of  connected  devices  intersects  with  human  values  (such  as  privacy  and  transparency)  and  to  articulate  the  specific  challenges  for  designing  a  value-conscious  IoT.
2	Product  versus  process  representing  and  appropriating  diy  projects  online.  Despite  the  growth  of  online  communities  for  sharing  DIY  projects,  little  research  has  focused  on  the  methods  by  which  project  documentation  is  created  and  utilized  --  that  is,  what  techniques  do  designers  use  to  document  their  work,  how  do  they  describe  their  work  to  others,  and  how  do  readers  utilize  design  documentation  in  the  context  of  their  own  projects?  Through  interviews  and  surveys  with  authors  and  readers  of  Instructables,  we  describe  differences  found  in  the  practices  of  these  two  types  of  users  in  creating  and  applying  design  documentation.  Based  on  the  results,  we  identify  design  opportunities  for  members  of  the  HCI  community  developing  tools  to  better  support  people  sharing  creative  work  online.
2	Human  actions  made  tangible  analysing  the  temporal  organization  of  activities.  With  designers  increasingly  moving  beyond  button  pushing  and  flat-screen  interaction  towards  tangible  and  embodied  interaction,  techniques  for  user  studies  need  to  develop  as  well.  While  ethnographic  video  studies  and  ethnomethod-ological  analyses  are  becoming  standard  in  many  interaction  design  projects,  it  remains  a  challenge  to  investigate  in  detail  how  people  interact  with  all  of  their  body.  Analysis  of  full-body  movement  is  time  consuming,  notation  techniques  are  rare,  and  findings  are  difficult  to  share  between  members  of  a  design  team.  In  this  paper  we  propose  tangible  video  analysis,  a  method  developed  to  engage  people  from  different  backgrounds  in  collaboratively  analysing  videos  with  the  help  of  physical  objects.  We  will  present  one  of  these  tools,  Action  Scrabble,  for  analysing  temporal  organisation  of  human  actions.  We  work  with  a  case  of  skilled  forklift  truck  driving.  By  backtracking  our  design  research  experiments,  we  will  unfold  how  and  why  the  tangible  tool  succeeds  in  engaging  designers  with  varied  analysis  experience  to  collaboratively  focus  on  human  action  structures  --  and  even  find  video  analysis  fun!
2	Implications  of  location  and  touch  for  on  body  projected  interfaces.  Very  recently,  there  has  been  a  perfect  storm  of  technical  advances  that  has  culminated  in  the  emergence  of  a  new  interaction  modality:  on-body  interfaces.  Such  systems  enable  the  wearer  to  use  their  body  as  an  input  and  output  platform  with  interactive  graphics.  Projects  such  as  PALMbit  and  Skinput  sought  to  answer  the  initial  and  fundamental  question:  whether  or  not  on-body  interfaces  were  technologically  possible.  Although  considerable  technical  work  remains,  we  believe  it  is  important  to  begin  shifting  the  question  away  from  how  and  what,  and  towards  where,  and  ultimately  why.  These  are  the  class  of  questions  that  inform  the  design  of  next  generation  systems.  To  better  understand  and  explore  this  expansive  space,  we  employed  a  mixed-methods  research  process  involving  more  than  two  thousand  individuals.  This  started  with  high-resolution,  but  low-detail  crowdsourced  data.  We  then  combined  this  with  rich,  expert  interviews,  exploring  aspects  ranging  from  aesthetics  to  kinesthetics.  The  results  of  this  complimentary,  structured  exploration,  point  the  way  towards  more  comfortable,  efficacious,  and  enjoyable  on-body  user  experiences.
2	Manipulating  reality  designing  and  deploying  virtual  reality  in  sensitive  settings.  Virtual  reality  (VR)  is  now  being  designed  and  deployed  in  diverse  sensitive  settings,  especially  for  therapeutic  purposes.  For  example,  VR  experiences  are  used  for  diversional  therapy  in  aged  care  and  as  therapy  for  people  living  with  conditions  such  as  phobias  and  post-traumatic  stress.  While  these  uses  of  VR  offer  great  promise,  they  also  present  significant  challenges.  Given  the  novelty  of  VR,  its  immersive  nature,  and  its  impact  on  the  user's  sense  of  reality,  it  can  be  particularly  challenging  to  engage  participants  in  co-design  and  predict  what  might  go  wrong  when  implementing  these  technologies  in  sensitive  settings.  This  workshop  provides  a  forum  for  researchers  working  in  this  emerging  space  to  share  stories  about  their  experiences  of  designing  and  evaluating  VR  applications  in  settings  such  as  aged  care  or  mental  health  therapy.  The  workshop  will  develop  a  manifesto  for  good  practice,  outlining  co-design  strategies  and  ethical  issues  to  consider  when  designing  and  deploying  VR  in  sensitive  settings.
2	From  the  lab  to  the  world  studying  real  time  second  screen  interaction  with  live  sports.  This  paper  describes  the  design  and  evaluation  of  a  new  feature  included  on  a  second  screen  application  for  real-time  interaction  during  live  sports  TV  broadcasts.  This  feature  allows  remote  users  to  bet  if  a  goal  is  about  to  happen  during  a  soccer  match,  increasing  their  interest,  engagement  and  excitement  towards  the  broadcasted  event.  To  achieve  these  objectives,  we  used  an  eyes-free  interaction  method  so  users  would  not  need  to  constantly  shift  their  attention  from  the  TV  screen  to  the  mobile  device  every  time  a  goal  might  happen.  We  conducted  an  evaluation  study,  with  the  real  users  of  the  application,  which  is  available  on  the  market,  to  gather  the  users'  feedback  regarding  their  experience  and  the  use  of  the  interaction  method.  Results  may  give  a  considerable  contribution  to  future  researchers  in  this  field,  regarding  the  challenges  involved  when  deploying  real-time  interactions  during  live  broadcasted  events.
2	Predicting  smartphone  battery  life  based  on  comprehensive  and  real  time  usage  data.  Smartphones  and  smartphone  apps  have  undergone  an  explosive  growth  in  the  past  decade.  However,  smartphone  battery  technology  hasn't  been  able  to  keep  pace  with  the  rapid  growth  of  the  capacity  and  the  functionality  of  smartphones  and  apps.  As  a  result,  battery  has  always  been  a  bottleneck  of  a  user's  daily  experience  of  smartphones.  An  accurate  estimation  of  the  remaining  battery  life  could  tremendously  help  the  user  to  schedule  their  activities  and  use  their  smartphones  more  efficiently.  Existing  studies  on  battery  life  prediction  have  been  primitive  due  to  the  lack  of  real-world  smartphone  usage  data  at  scale.  This  paper  presents  a  novel  method  that  uses  the  state-of-the-art  machine  learning  models  for  battery  life  prediction,  based  on  comprehensive  and  real-time  usage  traces  collected  from  smartphones.  The  proposed  method  is  the  first  that  identifies  and  addresses  the  severe  data  missing  problem  in  this  context,  using  a  principled  statistical  metric  called  the  concordance  index.  The  method  is  evaluated  using  a  dataset  collected  from  51  users  for  21  months,  which  covers  comprehensive  and  fine-grained  smartphone  usage  traces  including  system  status,  sensor  indicators,  system  events,  and  app  status.  We  find  that  the  remaining  battery  life  of  a  smartphone  can  be  accurately  predicted  based  on  how  the  user  uses  the  device  at  the  real-time,  in  the  current  session,  and  in  history.  The  machine  learning  models  successfully  identify  predictive  features  for  battery  life  and  their  applicable  scenarios.
2	Confronting  people  s  fears  about  bats  combining  multi  modal  and  environmentally  sensed  data  to  promote  curiosity  and  discovery.  Bats  are  often  disliked  and  feared  by  people.  How  might  we  enable  the  general  public  to  learn  more  about  the  true  nature  of  these  creatures,  and  even  to  like  them?  In  this  paper,  we  introduce  PlayBat,  a  physical  public  display,  which  combines  a  multi-modal  interface,  a  constrained  narrative  structure  and  real-time  IoT  environmentally  sensed  bat  call  data.  The  aim  of  our  research  is  to  investigate  whether  promoting  curiosity  and  discovery  through  enabling  people  to  explore  real-life  data,  answer  quiz-like  questions  and  engage  with  a  multi-modal  interface,  is  effective  at  engaging  people  and  confronting  their  fears.  We  report  on  the  design  process  and  implementation  of  PlayBat,  and  the  findings  from  an  in-the-wild  study.  We  discuss  how  tapping  into  multiple  senses  can  draw  people  in,  evoke  curiosity  and  even  change  their  views.
2	Characterizing  the  tool  notation  people  triplet  in  software  modeling  tasks.  The  existence  of  some  kind  of  relationship  between  the  usability  of  software  development  tools  and  the  quality  of  use  of  the  development  process  end  product  would  not  be  surprising.  Yet,  this  topic  hasn't  attracted  a  substantial  share  of  interest  among  HCI  or  Software  Engineering  researchers,  possibly  because  we  lack  the  appropriate  conceptual  and  technical  tools  to  address  the  problem.  In  this  paper,  we  articulate  a  tool-notation-people  triplet  and  suggest  that  it  can  be  used  in  interaction  design  evaluation  of  modeling  tools  used  by  many  software  developers.  The  evaluation  is  carried  out  with  a  specific  method  that  combines  cognitive  and  communicative  dimensions  of  such  tools  and  characterizes  how  the  tool-notation-people  triplet  is  instantiated  for  the  case  under  examination.  We  demonstrate  the  value  of  our  proposal  with  a  study  of  IBM  RSA,  a  popular  software  modeling  tool.  The  interest  of  this  work  for  the  HCI  community  is  to  provide  a  set  of  resourceful  tools  --  combined  method  and  triplet  --  to  identify  and  collect  interaction  issues  that  could  be  used  to  improve  the  design  of  modeling  support  tools.
2	Hajukone  developing  an  open  source  olfactory  device.  In  comparison  to  our  other  senses,  there  has  been  relatively  little  work  on  how  our  sense  of  smell  can  be  effectively  utilised  in  a  Human-Computer  Interface.  We  argue  that  the  lack  of  easy  access  to  'off-the-shelf'  computer  controlled  scent  delivery  devices  restricts  research  in  this  area,  and  that  without  understanding  what  smell  can  be  used  for,  there  is  little  commercial  case  to  make  such  devices  available.  In  considering  these  issues,  we  have  developed  Hajukone:  a  smell  delivery  device  that  is  both  open  source  and  can  be  built  with  low  technical  skills,  yet  provides  high  quality  olfactory  capabilities.  We  outline  the  design  of  Hajukone,  showing  how  it  overcomes  critical  design  requirements  that  have  restricted  prior  research,  before  outlining  our  future  plans  for  its  development  and  use.
2	Supporting  children  to  engage  in  play  for  wellbeing.  Engaging  children  in  play  for  wellbeing  is  coming  to  prominence  in  HCI  as  the  community  increasingly  engages  with  issues  to  do  with  health,  care  and  therapy.  To  date,  research  in  this  area  has  primarily  focused  on  engaging  children  with  special  needs  or  who  suffer  from  mental  health  problems  in  organized  play  to  improve  their  self-efficacy  or  support  therapeutic  treatment.  However,  play  could  be  more  broadly  beneficial  for  all  children  and  their  wellbeing.  This  workshop  aims  to  build  an  interdisciplinary  community  of  researchers,  designers,  and  practitioners  to  share  and  discuss  their  work.  Additionally,  it  seeks  to  explore  the  interaction  between  play  and  wellbeing  in  the  context  of  interaction  design  and  children  and  to  identify  directions  for  future  research.
2	Waveform  remote  video  blending  for  vjs  using  in  air  multitouch  gestures.  We  present  WaveForm,  a  system  that  enables  a  Video  Jockey  (VJ)  to  directly  manipulate  video  content  on  a  large  display  on  a  stage,  from  a  distance.  WaveForm  implements  an  in-air  multitouch  gesture  set  to  layer,  blend,  scale,  rotate,  and  position  video  content  on  the  large  display.  We  believe  this  leads  to  a  more  immersive  experience  for  the  VJ  user,  as  well  as  for  the  audience  witnessing  the  VJ's  performance  during  a  live  event.
2	Communiplay  a  field  study  of  a  public  display  mediaspace.  We  present  Communiplay,  a  public  display  media  space.  People  passing  by  see  their  own  contour  mirrored  on  a  public  display  and  can  start  to  play  with  virtual  objects.  At  the  same  time,  they  see  others  playing  at  remote  displays  within  the  same  virtual  space.  We  are  interested  whether  people  would  use  such  a  public  display  media  space,  and  if  so,  how  and  why.  We  evaluate  Communiplay  in  a  field  study  in  six  connected  locations  and  find  a  remote  honey-pot  effect,  i.e.  people  interacting  at  one  location  attract  people  at  other  locations.  The  conversion  rate  (percentage  of  passers-by  starting  to  interact)  rose  by  +136%  when  people  saw  others  playing  at  remote  locations.  We  also  provide  the  first  quantification  of  the  (local)  honey-pot  effect  (in  our  case  it  raised  the  conversion  rate  by  +604%  when  people  saw  others  playing  at  the  same  location).  We  conclude  that  the  integration  of  multiple  public  displays  into  a  media  space  is  a  promising  direction  for  public  displays  and  can  make  them  more  attractive  and  valuable.
2	Darls  differencing  and  merging  diagrams  using  dual  view  animation  re  layout  layers  and  a  storyboard.  We  present  a  new  system  for  visualizing  and  merging  differences  in  diagrams.  It  uses  animation,  dual  views,  a  storyboard,  relative  re-layout,  and  layering  to  visualize  differences.  The  system  is  also  capable  of  differencing  UML  class  diagrams.  An  evaluation  produced  positive  results  for  animation  and  dual  views  with  difference  layer.
2	Assessing  user  engagement  in  information  visualization.  Engagement  is  an  important  aspect  of  user  experience.  While  some  researchers  have  investigated  user  engagement  in  the  context  of  information  visualization,  there  is  still  a  lack  of  scholarship  on  the  topic.  In  this  paper  we  briefly  explore  the  role  and  significance  of  user  engagement  in  information  visualization,  and  discuss  challenges  in  its  characterization  and  assessment.  We  present  VisEngage,  a  self-assessment  questionnaire  that  provides  insight  into  11  different  characteristics  of  user  engagement.  We  report  the  results  of  an  online  pilot  study  that  was  conducted  using  VisEngage,  and  reflect  on  its  potential  utility  for  visualization  researchers  and  designers.
2	From  third  to  surveilled  place  the  mobile  in  irish  pubs.  A  home  away  from  home,  the  pub  is  synonymous  with  good  conversation.  Yet,  the  art  of  conversation  in  pubs  is  changing  with  the  ubiquity  of  mobile  phones.  We  present  a  qualitative  study  spanning  over  three  years  describing  experiences  and  rhetoric  surrounding  the  relationship  that  mobiles  have  and  should  have  with  our  conversation  in  the  pub.  We  found  that  mobile  phones  are  able  to  enhance  conversation  but  can  also  cause  a  disruption  to  the  informal  and  adhoc  nature  of  pubs.  The  use  of  Facebook  on  mobile  phones  has  also  changed  pubs  from  what  Oldenburg  terms  a  third  space  to  a  space  that  is  potentially  being  surveilled.  We  suggest  future  designs  should  not  necessarily  discourage  or  encourage  mobile  use  in  pubs,  but  rather  provoke  us  into  reflecting  on  how  intertwined  modern  conversation  is  with  mobile  technology  in  the  context  of  the  pub  space.
2	Mind  the  gap  a  sig  on  bridging  the  gap  in  research  on  body  sensing  body  perception  and  multisensory  feedback.  People's  perceptions  of  their  own  body's  appearance,  capabilities  and  position  are  constantly  updated  through  sensory  cues  [10,14]  that  are  naturally  produced  by  their  actions.  Increasingly  cheap  and  ubiquitous  sensing  technology  is  being  used  with  multisensory  feedback  in  multiple  HCI  areas  of  sports,  health,  rehabilitation,  psychology,  neuroscience,  arts  and  games  to  alter  or  en-hance  sensory  cues  to  achieve  many  ends  such  as  enhanced  body  perception  and  body  awareness.  However,  the  focus  and  aims  differ  between  areas.  Designing  more  effective  and  efficient  multisensory  feedback  re-quires  an  attempt  to  bridge  the  gap  between  these  worlds.  This  interactive  SIG  with  minute  madness  technology  presentations,  expert  sessions,  and  multidisciplinary  discussions  will:  (i)  bring  together  HCI  researchers  from  different  areas,  (ii)  discuss  tools,  methods  and  frameworks,  and  (iii)  form  a  multidisciplinary  community  to  build  synergies  for  further  collaboration.
2	Examining  game  world  topology  personalization.  We  present  an  exploratory  analysis  of  the  effects  of  game  world  topologies  on  self-reported  player  experience  in  Computer  Role  Playing  Games  (CRPGs).  We  find  that  (a)  players  are  more  engaged  in  game  worlds  that  better  match  their  self-reported  preferences;  and  (b)  player  preferences  for  game  topology  can  be  predicted  based  on  their  in-game  behavior.  We  further  describe  how  in-game  behavioral  features  that  correlate  to  preferences  can  be  used  to  control  procedural  content  generation  algorithms.
2	Analyzing  value  discovery  in  design  decisions  through  ethicography.  HCI  scholarship  is  increasingly  concerned  with  the  ethical  impact  of  socio-technical  systems.  Current  theoretically  driven  approaches  that  engage  with  ethics  generally  prescribe  only  abstract  approaches  by  which  designers  might  consider  values  in  the  design  process.  However,  there  is  little  guidance  on  methods  that  promote  value  discovery,  which  might  lead  to  more  specific  examples  of  relevant  values  in  specific  design  contexts.  In  this  paper,  we  elaborate  a  method  for  value  discovery,  identifying  how  values  impact  the  designer's  decision  making.  We  demonstrate  the  use  of  this  method,  called  Ethicography,  in  describing  value  discovery  and  use  throughout  the  design  process.  We  present  analysis  of  design  activity  by  user  experience  (UX)  design  students  in  two  lab  protocol  conditions,  describing  specific  human  values  that  designers  considered  for  each  task,  and  visualizing  the  interplay  of  these  values.  We  identify  opportunities  for  further  research,  using  the  Ethicograph  method  to  illustrate  value  discovery  and  translation  into  design  solutions.
2	Bridging  social  critique  and  design  building  a  health  informatics  tool  for  transgender  voice.  This  project  aims  to  develop  a  voice  training  application  for  transgender  people.  Voice  training  is  typically  conducted  by  a  speech  therapist,  and  consists  of  personalized  sessions  that  support  individuals  in  changing  their  voices  (such  as  modifying  pitch,  resonance,  or  speech  patterns).  The  reasons  why  people  may  pursue  voice  training  are  varied,  but  often  includes  discomfort  with  voice  being  misaligned  with  gender  identity.  Training  with  a  speech  therapist  may  be  inaccessible  due  to  health  disparities;  thus,  a  technological  solution,  as  I  propose  in  my  research,  is  necessary.  This  project  will  address  existing  constraints  to  design  a  novel  voice  training  application  in  partnership  with  community  members,  using  a  participatory  research  methodology  and  combining  the  fields  of  speech  science,  feminist  and  queer  theory,  and  HCI.
2	Recipes  for  programmable  money.  This  paper  presents  a  qualitative  study  of  the  recent  integration  of  a  UK-based,  digital-first  mobile  banking  app  -  Monzo  -  with  the  web  automation  service  IFTTT  (If  This  Then  That).  Through  analysis  of  113  unique  IFTTT  'recipes'  shared  by  Monzo  users  on  public  community  forums,  we  illustrate  the  potentially  diverse  functions  of  these  recipes,  and  how  they  are  achieved  through  different  kinds  of  automation.  Beyond  achieving  more  convenient  and  efficient  financial  management,  we  note  many  playful  and  expressive  applications  of  conditionality  and  automation  that  far  extend  traditional  functions  of  banking  applications  and  infrastructure.  We  use  these  findings  to  map  opportunities,  challenges  and  areas  of  future  research  in  the  development  of  'programmable  money'  and  related  financial  technologies.  Specifically,  we  present  design  implications  for  the  extension  of  native  digital  banking  applications;  novel  uses  of  banking  data;  the  applicability  of  blockchains  and  smart  contracts;  and  future  forms  of  financial  autonomy.
2	Exploring  and  designing  for  memory  impairments  in  depression.  Depression  is  an  affective  disorder  with  distinctive  autobiographical  memory  impairments,  including  negative  bias,  overgeneralization  and  reduced  positivity.  Several  clinical  therapies  address  these  impairments,  and  there  is  an  opportunity  to  develop  new  supports  for  treatment  by  considering  depression-associated  memory  impairments  within  design.  We  report  on  interviews  with  ten  experts  in  treating  depression,  with  expertise  in  both  neuropsychology  and  cognitive  behavioral  therapies.  The  interviews  explore  approaches  for  addressing  each  of  these  memory  impairments.  We  found  consistent  use  of  positive  memories  for  treating  all  memory  impairments,  the  challenge  of  direct  retrieval,  and  the  need  to  support  the  experience  of  positive  memories.  We  aim  to  sensitize  HCI  researchers  to  the  limitations  of  memory  technologies,  broaden  their  awareness  of  memory  impairments  beyond  episodic  memory  recall,  and  inspire  them  to  engage  with  this  less  explored  design  space.  Our  findings  open  up  new  design  opportunities  for  memory  technologies  for  depression,  including  positive  memory  banks  for  active  encoding  and  selective  retrieval,  novel  cues  for  supporting  generative  retrieval,  and  novel  interfaces  to  strengthen  the  reliving  of  positive  memories.
2	Developing  a  conversational  recommendation  systemfor  navigating  limited  options.  We  have  developed  a  conversational  recommendation  system  designed  to  help  users  navigate  through  a  set  of  limited  options  to  find  the  best  choice.  Unlike  many  internet  scale  systems  that  use  a  singular  set  of  search  terms  and  return  a  ranked  list  of  options  from  amongst  thousands,  our  system  uses  multi-turn  user  dialog  to  deeply  understand  the  user’s  preferences.  The  system  responds  in  context  to  the  user’s  specific  and  immediate  feedback  to  make  sequential  recommendations.  We  envision  our  system  would  be  highly  useful  in  situations  with  intrinsic  constraints,  such  as  finding  the  right  restaurant  within  walking  distance  or  the  right  retail  item  within  a  limited  inventory.  Our  research  prototype  instantiates  the  former  use  case,  leveraging  real  data  from  Google  Places,  Yelp,  and  Zomato.  We  evaluated  our  system  against  a  similar  system  that  did  not  incorporate  user  feedback  in  a  16  person  remote  study,  generating  64  scenario-based  search  journeys.  When  our  recommendation  system  was  successfully  triggered,  we  saw  both  an  increase  in  efficiency  and  a  higher  confidence  rating  with  respect  to  final  user  choice.  We  also  found  that  users  preferred  our  system  (75%)  compared  with  the  baseline.
2	What  is  this  url  s  destination  empirical  evaluation  of  users  url  reading.  Common  anti-phishing  advice  tells  users  to  mouse  over  links,  look  at  the  URL,  and  compare  to  the  expected  destination,  implicitly  assuming  that  they  are  able  to  read  the  URL.  To  test  this  assumption,  we  conducted  a  survey  with  1929  participants  recruited  from  the  Amazon  Mechanical  Turk  and  Prolific  Academic  platforms.  Participants  were  shown  23  URLs  with  various  URL  structures.  For  each  URL,  participants  were  asked  via  a  multiple  choice  question  where  the  URL  would  lead  and  how  safe  they  feel  clicking  on  it  would  be.  Using  latent  class  analysis,  participants  were  stratified  by  self-reported  technology  use.  Participants  were  strongly  biased  towards  answering  that  the  URL  would  lead  to  the  website  of  the  organization  whose  name  appeared  in  the  URL,  regardless  of  its  position  in  the  URL  structure.  The  group  with  the  highest  technology  use  was  only  minorly  better  at  URL  reading.
2	Chameleon  devices  investigating  more  secure  and  discreet  mobile  interactions  via  active  camouflaging.  Many  users  value  the  ability  to  have  quick  and  frequent  sight  of  their  mobiles  when  in  public  settings.  However,  in  doing  so,  they  expose  themselves  to  potential  risks,  ranging  from  being  targets  of  robbery  to  the  more  subtle  social  losses  through  being  seen  to  be  rude  or  inattentive  to  those  around  them.  In  nature,  some  animals  can  blend  into  their  environments  to  avoid  being  eaten  or  to  reduce  their  impact  on  the  ecosystem  around  them.  Taking  inspiration  from  these  evolved  systems  we  investigate  the  notion  of  chameleon  approaches  for  mobile  interaction  design.  Our  probes  were  motivated,  inspired  and  refined  through  extended  interactions  with  people  drawn  from  contexts  with  differing  ranges  of  security  and  privacy  concerns.  Through  deployments  on  users'  own  devices,  our  prototypes  show  the  value  of  the  concept.  The  encouraging  results  motivate  further  research  in  materials  and  form  factors  that  can  provide  more  effective  automatic  plain-sight  hiding.
2	Chi  2013  human  work  interaction  design  hwid  sig  past  history  and  future  challenges.  In  this  SIG  we  aim  to  introduce  the  IFIP  13.6  Human  Work  Interaction  Design  (HWID)  approach  to  the  CHI  audience.  The  HWID  working  group  aims  at  establishing  relationships  between  extensive  empirical  work-domain  studies  and  HCI  design.  We  invite  participants  from  industry  and  academia  with  an  interest  on  empirical  work  analysis,  HCI,  interaction  design  and  usability  and  user  experience  in  work  situations  and  in  the  workplace.  This  SIG  is  a  vital  step  towards  creating  a  CHI2014  workshop  on  this  topic.
2	Everyday  telepresence  emerging  practices  and  future  research  directions.  As  network  availability  becomes  ubiquitous,  users  are  leveraging  this  access  to  establish  their  presence  in  remote  locations  through  the  use  of  commercially  available  telepresence  technologies.  With  the  increasing  adoption  of  systems,  new  questions  are  emerging  about  how  these  technologies  affect  user  interactions  and  relationships.  Our  goal  for  this  workshop  is  to  bring  an  interdisciplinary  group  of  telepresence  researchers  together  to  trade  perspectives,  fostering  new  opportunities  for  collaboration  and  to  facilitate  discussion  on  how  to  advance  the  field.
2	Can  i  think  of  something  else  when  using  a  bci  cognitive  demand  of  an  ssvep  based  bci.  BCIs  are  presumably  supposed  to  require  the  full  attention  of  their  users  and  to  lose  accuracy  if  they  pay  attention  to  another  task.  This  assertion  has  been  verified  with  several  BCI  paradigms  (e.g.  P300).  But  the  cognitive  demand  of  the  promising  SSVEP  paradigm  had  never  been  specifically  assessed  yet.  We  measured  the  accuracy  of  an  SSVEP-based  BCI  used  by  26  participants  in  various  conditions  of  mental  workload.  Our  analysis  revealed  that  surprisingly,  for  this  type  of  BCI,  little  attention  is  actually  needed  from  participants  to  reach  optimal  accuracy:  participants  were  able  to  successfully  perform  a  complex  secondary  task  (N-back)  without  degrading  the  BCI  accuracy.  The  same  observation  was  made  whether  visual  or  auditive  attention  was  solicited.  These  results  indicate  that  SSVEP  is  a  low-demanding  paradigm  in  terms  of  cognitive  resources,  and  are  encouraging  for  its  use  in  complex  interaction  settings.
2	Designlibs  a  scenario  based  design  method  for  ideation.  Generating  potential  design  ideas  through  ideation  often  benefits  from  the  spontaneity  of  random  ideas.  Having  potential  users  participate  in  this  process  can  be  beneficial,  but  is  often  difficult  to  implement.  We  present  a  new  method  for  generating  design  ideas  with  potential  users.  The  method  uses  scenarios  with  missing  words,  which  potential  users  fill  in  to  generate  ideas  for  features  and  attributes  of  new  technology  designs,  similar  to  the  children's  game  of  Mad  Libs.  We  developed  three  different  formats  of  DesignLibs,  including  1)  "Mad  Libs-style":  blanks  presented  before  seeing  the  scenario,  2)  "Fill-in-the-Blanks":  blanks  presented  within  the  context  of  the  scenario,  and  3)  "Q&A":  blanks  presented  as  questions  and  answers.  We  found  that  Design-Libs  generated  a  number  of  new  ideas,  with  the  Fill-in-the-Blanks  method  providing  the  highest  ratings  for  usefulness,  feasibility,  and  diversity  of  answers.  All  three  formats  provided  equal  ratings  for  creativity.
2	Leveraging  motor  learning  for  a  tangible  password  system.  Tangible  user  interfaces  (TUIs)  may  allow  users  to  have  more  direct  interaction  with  systems  when  compared  to  traditional  graphical  user  interfaces  (GUIs).  However,  the  full  range  of  applications  where  TUIs  can  be  utilized  in  practice  is  unclear.  To  resolve  this  problem,  the  benefits  of  TUIs  must  be  analyzed  and  matched  to  an  application  domain  where  they  hold  advantages  over  more  traditional  systems.  Since  TUIs  require  users  to  use  their  hands  in  order  to  interact  with  the  system,  there  is  the  possibility  for  these  systems  to  leverage  motor  learning  to  help  users  perform  specific  tasks.  In  this  paper  we  will  describe  an  early  attempt  to  understand  how  motor  learning  can  be  used  to  create  a  tangible  password  system.  A  novel  tangible  password  system  was  created  and  a  small  study  conducted  in  order  to  identify  future  research  objectives.
2	Evaluating  information  visualization  via  the  interplay  of  heuristic  evaluation  and  question  based  scoring.  In  an  instructional  setting  it  can  be  difficult  to  accurately  assess  the  quality  of  information  visualizations  of  several  variables.  Instead  of  a  standard  design  critique,  an  alternative  is  to  ask  potential  readers  of  the  chart  to  answer  questions  about  it.  A  controlled  study  with  47  participants  shows  a  good  correlation  between  aggregated  novice  heuristic  evaluation  scores  and  results  of  answering  questions  about  the  data,  suggesting  that  the  two  forms  of  assessment  can  be  complementary.  Using  both  metrics  in  parallel  can  yield  further  benefits;  discrepancies  between  them  may  reveal  incorrect  application  of  heuristics  or  other  issues.
2	Enhancing  personal  informatics  through  social  sensemaking.  Personal  informatics  practices  are  increasingly  common,  with  a  range  of  consumer  technologies  available  to  support,  largely  individual,  interactions  with  data  (e.g.,  performance  measurement  and  activity/health  monitoring).  In  this  paper,  we  explore  the  concept  of  social  sensemaking.  In  contrast  to  high-level  statistics,  we  posit  that  social  networking  and  reciprocal  sharing  of  fine-grained  self-tracker  data  can  provide  valuable  context  for  individuals  in  making  sense  of  their  data.  We  present  the  design  of  an  online  platform  called  Citizense  Makers  (CM),  which  facilitates  group  sharing,  annotating  and  discussion  of  self-tracker  data.  In  a  field  trial  of  CM,  we  explore  design  issues  around  willingness  to  share  data  reciprocally;  the  importance  of  familiarity  between  individuals;  and  understandings  of  common  activities  in  contextualising  one's  own  data.
2	Multinet  reducing  interaction  overhead  in  domestic  wireless  networks.  We  present  MultiNet,  a  novel  method  for  securely  associating  devices  with  a  domestic  wireless  network.  We  show  that  MultiNet  has  usability  benefits  over  currently  deployed  commercial  solutions  while  being  backwards  compatible  with  existing  devices.  MultiNet  reduces  the  interaction  overhead  of  secure  association  by  focusing  on  users'  interactions  rather  than  the  network's  requirements.  This  leads  to  a  novel  architectural  arrangement  of  the  home  network  infrastructure:  the  network  is  dynamically  re-configured  to  accept  each  pre-configured  device,  rather  than  the  current  norm  where  each  device  is  configured  to  be  acceptable  to  the  pre-configured  network.  Assuming  devices  are  pre-configured  for  a  unique,  device-specific  network  name  and  passphrase,  MultiNet  constructs  an  out-of-band  visual  channel  via  an  intermediary  network  controller  device  to  convey  the  device's  configuration  to  the  network.  This  makes  the  interaction  to  join  a  device  to  the  wireless  network  lightweight  and  identical  across  all  devices,  considerably  reducing  the  interaction  overheads  for  users.
2	The  representation  of  self  in  mediated  interaction  with  computers.  According  to  the  computers  as  media  perspective,  all  kinds  of  software,  whether  created  by  expert  programmers  or  end  users,  carry  an  implicit  or  explicit  representation  of  their  creators.  This  paper  discusses  the  representation  of  the  user's  self  in  a  study  with  the  Web  Navigation  Helper  (WNH),  a  user  agent  designed  to  support  Web  navigation  through  mediation  dialogs.  In  our  study,  school  teachers  built  mediation  dialogs  to  orient  their  students  while  navigating  proposed  Websites.  Our  findings  show  how  this  specific  kind  of  mediation  technology  communicated  (intentional  or  unintentional)  self-representation  and  supported  self-expression.  They  also  provide  insights  into  how  similar  mediation  technologies  can  be  used  in  the  broader  context  of  computer-supported  social  participation.
2	The  ethics  of  unaware  participation  in  public  interventions.  Interaction  design  is  increasingly  merging  with  designing  our  everyday  environment.  Trialing  and  evaluating  such  designs  in  an  ecologically  valid  way  often  requires  that  they  be  installed  in  public  space  without  clearly  communicating  their  nature  as  trials.  This  leads  to  unaware  participation  in  what,  in  fact,  is  an  experimental  intervention.      This  article  focuses  on  the  ethical  considerations  that  arise  from  doing,  and  studying,  interventions  in  public  space,  including  but  not  restricted  to  interactive  installations.  It  argues  that  under  certain  circumstances,  such  as  when  the  known  risks  are  low  and  the  intervention  presents  sufficient  support  for  avoiding  involvement,  active  participation  can  be  considered  implicit  consent.  We  revisit  some  example  interventions  from  literature  and  press  to  scrutinize  the  potential  risks  and  pitfalls  associated  to  unaware  participation.
2	Chatbots  humbots  and  the  quest  for  artificial  general  intelligence.  What  began  as  a  quest  for  artificial  general  intelligence  branched  into  several  pursuits,  including  intelligent  assistants  developed  by  tech  companies  and  task-oriented  chatbots  that  deliver  more  information  or  services  in  specific  domains.  Progress  quickened  with  the  spread  of  low-latency  networking,  then  accelerated  dramatically  a  few  years  ago.  In  2016,  task-focused  chatbots  became  a  centerpiece  of  machine  intelligence,  promising  interfaces  that  are  more  engaging  than  robotic  answering  systems  and  that  can  accommodate  our  increasingly  phone-based  information  needs.  Hundreds  of  thousands  were  built.  Creating  successful  non-trivial  chatbots  proved  more  difficult  than  anticipated.  Some  developers  now  design  for  human-chatbot  (humbot)  teams,  with  people  handling  difficult  queries.  This  paper  describes  the  conversational  agent  space,  difficulties  in  meeting  user  expectations,  potential  new  design  approaches,  uses  of  human-bot  hybrids,  and  implications  for  the  ultimate  goal  of  creating  software  with  general  intelligence.
2	Evaluating  the  user  experience  in  interactive  installations  a  case  study.  Currently,  digital  technologies  are  present  in  many  areas  of  our  lives  and  are  used  for  various  purposes.  The  ubiquitous  technology  landscape  through  ubiquitous  and  pervasive  technologies  has  brought  new  forms  of  interaction.  In  this  context,  Human-Computer  Interaction  (HCI)  community  has  been  directing  efforts  towards  its  methods.  Among  the  challenges  are  the  types  of  evaluations  commonly  performed  to  improve  the  quality  of  systems.  In  this  work,  we  explored  different  methods  to  evaluate  the  user  experience  with  nine  interactive  installations  developed  in  a  Human-Computer  Interaction  discipline  project.  The  User  Experience  evaluation  was  made  using  AttrakDiff,  which  seeks  to  measure  hedonic,  pragmatic,  and  attractiveness  qualities  in  an  interaction.  The  second  method  was  a  combination  of  the  Pleasure  Framework  (its  thirteen  categories  of  pleasure)  and  the  Self-Assessment  Manikin  (SAM)  in  its  Pleasure  dimension.  The  choice  of  methods  was  given  by  the  nature  of  the  interactive  installations,  equally  valuating  pragmatic  qualities  as  well  as  hedonic  ones.  The  results  show  that  triangulation  approaches,  such  as  the  used  in  this  work,  were  effective  because  they  brought  qualitative  aspects  of  the  experience,  considering  both  pragmatic  and  hedonic  aspects  of  the  interaction.  Particularly  in  our  case  study,  artifacts  with  high  hedonic  qualities  obtained  good  ratings  in  the  pleasure  categories.
2	Time  to  scale  generalizable  affect  detection  for  tens  of  thousands  of  students  across  an  entire  school  year.  We  developed  generalizable  affect  detectors  using  133,966  instances  of  18  affective  states  collected  from  69,174  students  who  interacted  with  an  online  math  learning  platform  called  Algebra  Nation  over  the  entire  school  year.  To  enable  scalability  and  generalizability,  we  used  generic  interaction  features  (e.g.,  viewing  a  video,  taking  a  quiz),  which  do  not  require  specialized  sensors  and  are  domain-  and  (to  a  certain  extent)  system-independent.  We  experimented  with  standard  classifiers,  recurrent  neural  networks,  and  genetically  evolved  neural  networks  for  affect  modeling.  Prediction  accuracies,  quantified  with  Spearman's  rho,  were  modest  and  ranged  from  .08  (for  surprise)  to  .34  (for  happiness)  with  a  mean  of  .25.  Our  model  trained  on  Algebra  students  generalized  to  a  different  set  of  Geometry  students  (n  =  28,458)  on  the  same  platform.  We  discuss  implications  for  scaling  up  affect  detection  for  affect-sensitive  online  learning  environments  which  aim  to  improve  engagement  and  learning  by  detecting  and  responding  to  student  affect.
2	You  cannot  offer  such  a  suggestion  designing  for  family  caregiver  input  in  home  care  systems.  Previous  work  has  looked  closely  at  the  challenges  of  using  patient-generated  data  to  enable  remote  assessment  and  monitoring  by  healthcare  professionals.  In  this  paper,  we  examine  family  caregivers  who  act  as  proxies  for  patients  who  may  not  have  the  capacity  of  capturing  the  necessary  data.  We  worked  with  occupational  therapists  to  develop  an  application  for  remote  assessment  of  the  safety  of  patients'  homes  by  occupational  therapists  with  the  assistance  of  family  caregivers.  We  evaluated  the  application  with  family  caregivers  and  found  two  features  unique  to  communication  between  family  caregivers  and  healthcare  professionals:  Caregivers  want  to  be  able  to  direct  healthcare  professionals'  attention  to  support  problem-solving  at  home,  and  they  include  their  perspective  on  how  to  best  meet  the  patient's  health  needs.  We  discuss  the  importance  of  these  findings  for  home  systems  in  the  domain  of  long-term  chronic  care.
2	The  government  s  dividend  complex  perceptions  of  social  media  misinformation  in  china.  The  social  media  environment  in  China  has  become  the  dominant  source  of  information  and  news  over  the  past  decade.  This  news  environment  has  naturally  suffered  from  challenges  related  to  mis-  and  dis-information,  encumbered  by  an  increasingly  complex  landscape  of  factors  and  players  including  social  media  services,  fact-checkers,  censorship  policies,  and  astroturfing.  Interviews  with  44  Chinese  WeChat  users  were  conducted  to  understand  how  individuals  perceive  misinformation  and  how  it  impacts  their  news  consumption  practices.  Overall,  this  work  exposes  the  diverse  attitudes  and  coping  strategies  that  Chinese  users  employ  in  complex  social  media  environments.  Due  to  the  complex  nature  of  censorship  in  China  and  participants'  lack  of  understanding  of  censor-ship,  they  expressed  varied  opinions  about  its  influence  on  the  credibility  of  online  information  sources.  Further,  although  most  participants  claimed  that  their  opinions  would  not  be  easily  swayed  by  astroturfers,  many  admitted  that  they  could  not  effectively  distinguish  astroturfers  from  ordinary  Internet  users.  Participants'  inability  to  make  sense  of  comments  found  online  lead  many  participants  to  hold  pro-censorship  attitudes:  the  Government's  Dividend.
2	Alexa  as  coach  leveraging  smart  speakers  to  build  social  agents  that  reduce  public  speaking  anxiety.  Public  speaking  anxiety  is  one  of  the  most  common  social  phobias.  We  explore  the  feasibility  of  using  a  conversational  agent  to  reduce  this  anxiety.  We  developed  a  public-speaking  tutor  on  the  Amazon  Alexa  platform  that  enables  users  to  engage  in  cognitive  reconstruction  exercises.  We  also  investigated  how  the  sociability  of  the  agent  might  affect  its  performance  as  a  tutor.  A  user  study  of  53  college  students  with  fear  of  public  speaking  showed  that  the  interaction  with  the  agent  served  to  assuage  pre-speech  state  anxiety.  Agent  sociability  improved  the  sense  of  interpersonal  closeness,  which  was  associated  with  lower  pre-speech  anxiety.  Moreover,  sociability  of  the  agent  increased  participants'  satisfaction  and  their  willingness  to  continue  engagement.  Our  findings,  thus,  have  implications  not  only  for  addressing  public  speaking  anxiety  in  a  scalable  way  but  also  for  the  design  of  future  conversational  agents  using  smart  speaker  platforms.
2	Modeling  task  deviations  as  eccentricity  distribution  peaks.  Detailed  usage  data  is  becoming  available  through  different  devices  (e.g.,  personal  computer,  cell  phones,  tablets,  watches,  glasses,  wrist  bands),  in  huge  volumes,  and  in  a  speed  that  requires  new  models  and  visualizations  to  support  the  understanding  of  detailed  user  actions  at  scale.  Without  appropriate  methods  that  summarize  or  provide  means  of  analyzing  large  usage  data  sets,  a  semantic  gap  between  the  event-by-event  data  and  the  tasks  profile  remains.  In  this  context,  this  work  proposes  a  technique  to  support  the  analysis  of  task  deviation  from  the  examination  of  detailed  user  interface  events  streams.  From  the  analysis  of  427  event-by-event  logged  sessions  (captured  under  user  consent)  of  a  technical  reference  website,  this  work  presents  how  to  identify  task  deviations  by  using  eccentricity  distribution.  The  proposed  technique  is  a  promising  way  of  identifying  task  deviations  in  large  log  data  sets  containing  information  about  how  users  performed  real  tasks.
2	Design  frictions  for  mindful  interactions  the  case  for  microboundaries.  Design  frictions,  a  term  found  in  popular  media  articles  about  user  experience  design,  refer  to  points  of  difficulty  occurring  during  interaction  with  technology.  Such  articles  often  argue  that  these  frictions  should  be  removed  from  interaction  flows  in  order  to  reduce  the  risk  of  user  frustration  and  disengagement.  In  this  paper  we  argue  that,  in  many  scenarios,  designing  friction  into  interactions  through  the  introduction  of  microboundaries,  can,  in  fact,  have  positive  effects.  Design  frictions  can  disrupt  "mindless"  automatic  interactions,  prompting  moments  of  reflection  and  more  "mindful"  interaction.  The  potential  advantages  of  intentionally  introduced  frictions  are  numerous:  from  reducing  the  likelihood  of  errors  in  data-entry  tasks,  to  supporting  health-behaviour  change.
2	Active  collaboration  in  healthcare  design  participatory  design  to  develop  a  dementia  care  app.  This  paper  describes  a  research  project  aimed  at  developing  a  mealtime  data  registration  tool  for  people  with  dementia.  As  to  actively  involve  all  stakeholders  in  this  healthcare  design  project  and  to  generate  empathy  and  involvement,  methods  from  participatory  design  were  used.  For  each  of  the  three  research  phases  (ethnography,  ideation  &  conceptualization  and  prototyping)  we  describe  our  approach  towards  stakeholder  involvement  and  active  collaboration.  We  discuss  lessons  learned  in  terms  of  good  practices  and  the  issues  we  struggle  with.
2	Looking  glass  a  field  study  on  noticing  interactivity  of  a  shop  window.  In  this  paper  we  present  our  findings  from  a  lab  and  a  field  study  investigating  how  passers-by  notice  the  interactivity  of  public  displays.  We  designed  an  interactive  installation  that  uses  visual  feedback  to  the  incidental  movements  of  passers-by  to  communicate  its  interactivity.  The  lab  study  reveals:  (1)  Mirrored  user  silhouettes  and  images  are  more  effective  than  avatar-like  representations.  (2)  It  takes  time  to  notice  the  interactivity  (approx.  1.2s).  In  the  field  study,  three  displays  were  installed  during  three  weeks  in  shop  windows,  and  data  about  502  interaction  sessions  were  collected.  Our  observations  show:  (1)  Significantly  more  passers-by  interact  when  immediately  showing  the  mirrored  user  image  (+90%)  or  silhouette  (+47%)  compared  to  a  traditional  attract  sequence  with  call-to-action.  (2)  Passers-by  often  notice  interactivity  late  and  have  to  walk  back  to  interact  (the  landing  effect).  (3)  If  somebody  is  already  interacting,  others  begin  interaction  behind  the  ones  already  interacting,  forming  multiple  rows  (the  honeypot  effect).  Our  findings  can  be  used  to  design  public  display  applications  and  shop  windows  that  more  effectively  communicate  interactivity  to  passers-by.
2	Art  chi.  At  CHI2014  our  two  day  workshop  "Curating  the  Digital:  Space  for  Art  and  Interaction",  led  to  a  set  of  recommendations  to  the  SIGCHI  Executive  for  future  two  day  workshops  at  CHI2015  and  CHI2016,  in  which  interactive  artworks  would  be  the  focus  of  presentation  and  discussion.  The  Executive  and  the  chairs  of  the  upcoming  conferences  accepted  these  recommendations.  In  this  proposal  we  set  out  how  we  will  attract  and  select  appropriate  artworks  for  the  CH2015  workshop,  and  how  we  will  run  the  workshop  to  explore  the  themes  that  the  art  works  raise.  Additionally  we  will  discuss  how  we  will  involve  South  Korean  partners  to  highlight  local  culture  and  impacts  of  South  Korean  interactive  artists  and  provide  opportunities  for  deep  cross-cultural  dialogs.
2	Powersocket  towards  on  outlet  power  consumption  visualization.  Power  consumption  is  measured  in  W  and  Wh,  but  what  do  these  units  mean?  Water  consumption  can  easily  be  understood,  as  we  all  know  what  a  liter  of  water  looks  like.  Common  power  meters,  however,  rely  on  the  physical  units  or  their  translation  to  costs  as  display.  We  classified  existing  displays  and  ambient  visualizations  in  a  taxonomy  that  focuses  on  the  characteristics  of  power  consumption  displays.  We  adapted  representatives  of  the  different  categories  of  displays  to  an  on-outlet  display  and  compared  these  using  a  combination  of  soft-  and  hardware  prototyping.  Results  indicate  that  ambient  visualizations  make  it  easier  to  understand  power  consumption.
2	Perceived  distance  from  hitting  with  a  stick  is  altered  by  overlapping  vibration  to  holding  hand.  Distance  perception  by  hitting  with  a  holding  stick  is  quite  important  for  the  people  with  visual  impairments  who  daily  use  white  cane.  If  the  mechanism  of  this  perception  is  well  understood,  it  can  be  applied  for  the  development  of  more  intuitive  and  simple  electric  white  cane  consisting  of  a  range  sensor  and  a  haptic  display.  A  hypothetical  mechanical  model  of  a  stick  and  a  holding  palm  told  us  that  hitting  at  a  closer  point  should  induce  a  stronger  vibration  at  thumb  side  of  the  palm,  and  percussing  a  farther  point  should  induce  equally  distributed  vibrations  in  the  palm.  To  verify  if  this  vibration  distribution  plays  role  in  the  distance  perception,  we  conducted  an  experiment  that  superimpose  vibration  to  the  real  vibration  while  percussing,  to  change  the  center  of  gravity  of  vibration.  The  experimental  results  showed  that  adding  vibration  to  the  thumb  side  shortened  the  perceived  collision  distance  than  adding  vibration  to  the  little-finger  side,  which  partly  agrees  with  our  hypothetical  model.
2	The  influence  of  emotion  on  number  entry  errors.  Given  the  proliferation  of  devices  like  infusion  pumps  in  hospitals,  number  entry  and  in  particular  number  entry  error  is  an  emerging  important  concern  in  HCI.  There  are  clearly  design  features  that  could  greatly  improve  accuracy  in  entering  numbers  but  the  context  of  the  task  could  also  play  an  important  role.  In  particular,  the  emotional  state  of  a  person  is  known  to  strongly  influence  their  response  to  a  difficult  situation  and  hence  the  errors  that  they  make.  In  this  paper,  we  consider  the  impact  of  the  emotional  state  of  the  user  on  the  accuracy  with  which  people  enter  numbers.  Our  experiment  shows  that  participants  who  are  in  a  more  positive  emotional  state  are  more  accurate.  The  effect  is  small  but  could  be  very  important  when  considering  the  potentially  highly-charged  emotional  contexts  where  many  healthcare  devices  are  used.
2	Scalable  webcam  eye  tracking  by  learning  from  user  interactions.  Eye  tracking  systems  are  commonly  used  in  a  variety  of  research  domains,  but  cost  thousands  of  dollars.  In  my  thesis  I  investigate  a  new  approach  to  enable  eye  tracking  for  common  webcams.  The  aim  is  to  provide  a  natural  experience  to  everyday  users  that  are  not  restricted  to  laboratories  and  highly  controlled  studies.  The  accuracy  of  eye  tracking  webcams  will  be  improved  by  user  interactions  which  continuously  calibrate  the  eye  tracker  during  regular  usage.  Eye  tracking  can  become  a  reality  for  many  potential  applications  such  as  large-scale  naturalistic  user  studies,  online  gaming,  or  enabling  people  to  perform  hands-free  navigation  of  websites.
2	Crowdfunding  an  emerging  field  of  research.  Crowdfunding,  the  request  of  resources  through  social  media,  has  generated  much  discussion  in  the  popular  press;  however,  there  have  been  few  systematic  empirical  studies  of  this  growing  phenomenon.  We  bring  together  the  leading  HCI  researchers  in  crowdfunding  and  crowdsourcing  to  discuss  this  potentially  transformative  socio-technical  innovation  that  may  advance  (or  harm)  human  capabilities  to  innovate  and  collaborate.  We  will  discuss  current  empirical  research  on  crowdfunding  and  the  future  of  research  in  this  field  from  diverse  perspectives  including  computer  science,  social  science,  communications,  and  design,  using  both  qualitative  and  quantitative  research  methods.  To  make  real  progress  towards  realizing  future  research,  we  will  lead  a  discussion  with  the  audience  of  new  research  agendas  in  crowdfunding.
2	Tactile  perceptions  of  digital  textiles  a  design  research  approach.  Current  interactive  media  presentations  of  textiles  provide  an  impoverished  communication  of  their  'textile  hand',  that  is  their  weight,  drape,  how  they  feel  to  touch.  These  are  complex  properties  experienced  through  the  visual,  tactile,  auditory  and  proprioceptive  senses  and  are  currently  lost  when  textile  materials  are  presented  in  interactive  video.  This  paper  offers  a  new  perspective  from  which  the  production  of  multi-touch  interactive  video  representations  of  the  tactile  qualities  of  materials  is  considered.  Through  an  understanding  of  hand  properties  of  textiles  and  how  people  inherently  touch  and  handle  them,  we  are  able  to  develop  methods  to  animate  and  bring  these  properties  alive  using  design  methods.  Observational  studies  were  conducted,  noting  gestures  consumers  used  to  evaluate  textile  hand.  Replicating  the  appropriate  textile  deformations  for  these  gestures  in  interactive  video  was  explored  as  a  design  problem.  The  resulting  digital  textile  swatches  and  their  interactive  behavior  were  then  evaluated  for  their  ability  to  communicate  tactile  qualities  similar  to  those  of  the  real  textiles.
2	Ibeacon  and  hci  in  special  education  micro  location  based  augmentative  and  alternative  communication  for  children  with  intellectual  disabilities.  Daily  communication  is  essential  for  everyone,  including  people  with  communication  difficulties.  While  a  number  of  augmentative  and  alternative  communication  (AAC)  technologies  have  been  developed  for  users  with  complex  communication  needs,  their  usage  still  remains  a  challenge  for  those  with  cognitive  impairments.  This  extended  abstract  presents  our  work  in  AAC  for  non-verbal  students  with  intellectual  disabilities.  We  enhance  existing  AAC  solutions  with  iBeacon-based  ranging  and  micro-location  detection  capabilities  to  reduce  user's  cognitive  load  when  interacting  with  the  user  interface.  Our  system  has  been  piloted  in  a  special  education  school  for  students  with  moderate  intellectual  disability.  We  report  our  contextual  field  studies,  system  design,  and  implementation  experience;  and  hope  to  engender  further  discussions  on  HCI  in  special  education.
2	Is  more  better  impact  of  multiple  photos  on  perception  of  persona  profiles.  In  this  research,  we  investigate  if  and  how  more  photos  than  a  single  headshot  can  heighten  the  level  of  information  provided  by  persona  profiles.  We  conduct  eye-tracking  experiments  and  qualitative  interviews  with  variations  in  the  photos:  a  single  headshot,  a  headshot  and  images  of  the  persona  in  different  contexts,  and  a  headshot  with  pictures  of  different  people  representing  key  persona  attributes.  The  results  show  that  more  contextual  photos  significantly  improve  the  information  end  users  derive  from  a  persona  profile;  however,  showing  images  of  different  people  creates  confusion  and  lowers  the  informativeness.  Moreover,  we  discover  that  choice  of  pictures  results  in  various  interpretations  of  the  persona  that  are  biased  by  the  end  users'  experiences  and  preconceptions.  The  results  imply  that  persona  creators  should  consider  the  design  power  of  photos  when  creating  persona  profiles.
2	Attending  to  slowness  and  temporality  with  olly  and  slow  game  a  design  inquiry  into  supporting  longer  term  relations  with  everyday  computational  objects.  Slowness  has  emerged  as  a  rich  lens  to  frame  HCI  investigations  into  supporting  longer-term  human-technology  relations.  Yet,  there  is  a  need  to  further  address  how  we  design  for  slowness  on  conceptual  and  practical  levels.  Drawing  on  the  concepts  of  unawareness,  intersections,  and  ensembles,  we  contribute  an  investigation  into  designing  for  slowness  and  temporality  grounded  in  design  practice  through  two  cases:  Olly  and  Slow  Game.  We  designed  these  artifacts  over  two  and  a  half  years  with  careful  attention  to  how  the  set  of  concepts  influenced  key  design  decisions  in  terms  of  their  form,  materials,  and  computational  qualities.  Our  designer-researcher  approach  revealed  that,  when  put  into  practice,  the  concepts  helped  generatively  grapple  with  slowness  and  temporality,  but  are  in  need  of  further  development  to  be  mobilized  for  design.  We  critically  reflect  on  insights  emerging  across  our  practice-based  research  to  reflexively  refine  the  concepts  and  better  support  future  HCI  research  and  practice.
2	Designing  ambient  narrative  based  interfaces  to  reflect  and  motivate  physical  activity.  Numerous  technologies  now  exist  for  promoting  more  active  lifestyles.  However,  while  quantitative  data  representations  (e.g.,  charts,  graphs,  and  statistical  reports)  typify  most  health  tools,  growing  evidence  suggests  such  feedback  can  not  only  fail  to  motivate  behavior  but  may  also  harm  self-integrity  and  fuel  negative  mindsets  about  exercise.  Our  research  seeks  to  devise  alternative,  more  qualitative  schemes  for  encoding  personal  information.  In  particular,  this  paper  explores  the  design  of  data-driven  narratives,  given  the  intuitive  and  persuasive  power  of  stories.  We  present  WhoIsZuki,  a  smartphone  application  that  visualizes  physical  activities  and  goals  as  components  of  a  multi-chapter  quest,  where  the  main  character's  progress  is  tied  to  the  user's.  We  report  on  our  design  process  involving  online  surveys,  in-lab  studies,  and  in-the-wild  deployments,  aimed  at  refining  the  interface  and  the  narrative  and  gaining  a  deep  understanding  of  people's  experiences  with  this  type  of  feedback.  From  these  insights,  we  contribute  recommendations  to  guide  future  development  of  narrative-based  applications  for  motivating  healthy  behavior.
2	Omniglobevr  a  collaborative  360  communication  system  for  vr.  In  this  paper,  we  propose  OmniGlobeVR,  a  novel  collaboration  tool  based  on  an  asymmetric  cooperation  system  that  supports  communication  and  cooperation  between  a  VR  user  (occupant)  and  multiple  non-VR  users  (designers)  across  the  virtual  and  physical  platform.  The  OmniGlobeVR  allows  designer(s)  to  access  the  content  of  a  VR  space  from  any  point  of  view  using  two  view  modes:  360°  first-person  mode  and  third-person  mode.  Furthermore,  a  proper  interface  of  a  shared  gaze  awareness  cue  is  designed  to  enhance  communication  between  the  occupant  and  the  designer(s).  The  system  also  has  a  face  window  feature  that  allows  designer(s)  to  share  their  facial  expressions  and  upper  body  gesture  with  the  occupant  in  order  to  exchange  and  express  information  in  a  nonverbal  context.  Combined  together,  the  OmniGlobeVR  allows  collaborators  between  the  VR  and  non-VR  platforms  to  cooperate  while  allowing  designer(s)  to  easily  access  physical  assets  while  working  synchronously  with  the  occupant  in  the  VR  space.
2	Managing  user  experience  teams  lessons  from  case  studies  and  establishing  best  practices.  This  workshop  focuses  on  managing  cross-disciplinary  user  experience  teams  to  achieve  product  and  corporate  success.  The  workshop  brings  together  a  diverse  group  of  leaders  in  order  to  create  a  set  of  case  studies  to  illuminate  challenges  and  success  factors.  Emphasis  is  placed  on  cross-disciplinary  teams,  corporate  culture  and  environment,  organizational  structure,  and  international  considerations.  The  goal  of  the  workshop  is  to  develop  a  set  of  contingent,  specific,  and  applicable  guidelines  for  managing  user  experience  teams  in  a  variety  of  circumstances  based  on  case  studies.
2	Personal  informatics  self  insight  and  behavior  change  a  critical  review  of  current  literature.  Personal  informatics  (PI)  systems  allow  users  to  collect  and  review  personally  relevant  information.  The  purpose  commonly  envisioned  for  these  systems  is  that  they  provide  users  with  actionable,  data-driven  self-insight  to  help  them  change  their  behavioral  patterns  for  the  better.  Here,  we  review  relevant  theory  as  well  as  empirical  evidence  for  this  self-improvement  hypothesis.  From  a  corpus  of  6,568,  only  24  studies  met  the  selection  criteria  of  being  a  peer-reviewed  empirical  study  reporting  on  actionable,  data-driven  insights  from  PI  data,  using  a  “clean”  PI  system  with  no  other  intervention  techniques  (e.g.,  additional  coaching)  on  a  nonclinical  population.  First  results  are  promising  —  many  of  the  selected  articles  report  users  gaining  actionable  insights  —  but  we  do  note  a  number  of  methodological  issues  that  make  these  results  difficult  to  interpret.  We  conclude  that  more  work  is  needed  to  investigate  the  self-improvement  hypothesis  and  provide  a  set  of  recom-mendations  for  future  work.
2	Skin  drag  displays  dragging  a  physical  tactor  across  the  user  s  skin  produces  a  stronger  tactile  stimulus  than  vibrotactile.  We  propose  a  new  type  of  tactile  displays  that  drag  a  physical  tactor  across  the  skin  in  2D.  We  call  this  skin  drag.  We  demonstrate  how  this  allows  us  to  communicate  geometric  shapes  or  characters  to  users.  The  main  benefit  of  our  approach  is  that  it  simultaneously  produces  two  types  of  stimuli,  i.e.,  (1)  it  moves  a  tactile  stimulus  across  skin  locations  and  (2)  it  stretches  the  user's  skin.  Skin  drag  thereby  combines  the  essential  stimuli  produced  by  vibrotactile  and  skin  stretch.  In  our  study,  skin  drag  allowed  participants  to  recognize  tactile  shapes  significantly  better  than  a  vibrotactile  array  of  comparable  size.  We  present  two  arm-worn  prototype  devices  that  implement  our  concept.
2	Serendipity  finger  gesture  recognition  using  an  off  the  shelf  smartwatch.  Previous  work  on  muscle  activity  sensing  has  leveraged  specialized  sensors  such  as  electromyography  and  force  sensitive  resistors.  While  these  sensors  show  great  potential  for  detecting  finger/hand  gestures,  they  require  additional  hardware  that  adds  to  the  cost  and  user  discomfort.  Past  research  has  utilized  sensors  on  commercial  devices,  focusing  on  recognizing  gross  hand  gestures.  In  this  work  we  present  Serendipity,  a  new  technique  for  recognizing  unremarkable  and  fine-motor  finger  gestures  using  integrated  motion  sensors  (accelerometer  and  gyroscope)  in  off-the-shelf  smartwatches.  Our  system  demonstrates  the  potential  to  distinguish  5  fine-motor  gestures  like  pinching,  tapping  and  rubbing  fingers  with  an  average  f1-score  of  87%.  Our  work  is  the  first  to  explore  the  feasibility  of  using  solely  motion  sensors  on  everyday  wearable  devices  to  detect  fine-grained  gestures.  This  promising  technology  can  be  deployed  today  on  current  smartwatches  and  has  the  potential  to  be  applied  to  cross-device  interactions,  or  as  a  tool  for  research  in  fields  involving  finger  and  hand  motion.
2	Culture  or  fluency  unpacking  interactions  between  culture  and  communication  medium.  In  this  paper  we  describe  two  studies  intended  to  replicate  earlier  work  comparing  American  and  Chinese  communication  in  a  negotiation  task  using  several  different  media.  In  the  earlier  studies,  the  participants  all  spoke  in  English,  raising  the  question  of  whether  differences  in  fluency  rather  than  differences  in  cultural  background  explained  the  results.  We  replicated  the  earlier  studies  using  materials  translated  into  Chinese,  a  native  Chinese-speaking  experimenter,  and  native  Chinese  participants.  Counts  of  Chinese  characters  in  each  media  show  nearly  the  identical  pattern  found  in  the  earlier  studies,  suggesting  that  cultural  differences  in  communication  styles,  rather  than  fluency,  account  for  the  earlier  findings.  We  describe  implications  of  this  work  for  tools  to  support  intercultural  communication.
2	Nlg  based  moderator  response  generator  to  support  mental  health.  The  global  need  to  effectively  address  mental  health  problems  and  wellbeing  is  well  recognised.  Today,  online  systems  are  increasingly  being  viewed  as  an  effective  solution  for  their  ability  to  reach  broad  populations.  As  online  support  groups  become  popular  the  workload  for  human  moderators  increases.  Maintaining  quality  feedback  becomes  increasingly  challenging  as  the  community  grows.  Tools  that  can  automatically  detect  mental  health  problems  from  social  media  posts  and  then  generate  smart  feedback  can  greatly  reduce  human  overload.  In  this  paper,  we  present  a  system  for  the  automation  of  interventions  using  Natural  Language  Generation  (NLG)  techniques.  In  particular,  we  focus  on  'depression'  and  'anxiety'  related  interventions.  Psychologists  evaluated  the  quality  of  the  systems'  interventions  and  results  were  compared  against  human  (i.e.  moderator)  interventions.  Results  indicate  our  intervention  system  still  has  a  long  way  to  go,  but  is  a  step  in  the  right  direction  as  a  tool  to  assist  human  moderators  with  their  service.
2	Fingerio  using  active  sonar  for  fine  grained  finger  tracking.  We  present  fingerIO,  a  novel  fine-grained  finger  tracking  solution  for  around-device  interaction.  FingerIO  does  not  require  instrumenting  the  finger  with  sensors  and  works  even  in  the  presence  of  occlusions  between  the  finger  and  the  device.  We  achieve  this  by  transforming  the  device  into  an  active  sonar  system  that  transmits  inaudible  sound  signals  and  tracks  the  echoes  of  the  finger  at  its  microphones.  To  achieve  sub-centimeter  level  tracking  accuracies,  we  present  an  innovative  approach  that  use  a  modulation  technique  commonly  used  in  wireless  communication  called  Orthogonal  Frequency  Division  Multiplexing  (OFDM).  Our  evaluation  shows  that  fingerIO  can  achieve  2-D  finger  tracking  with  an  average  accuracy  of  8  mm  using  the  in-built  microphones  and  speaker  of  a  Samsung  Galaxy  S4.  It  also  tracks  subtle  finger  motion  around  the  device,  even  when  the  phone  is  in  the  pocket.  Finally,  we  prototype  a  smart  watch  form-factor  fingerIO  device  and  show  that  it  can  extend  the  interaction  space  to  a  0.5×0.25  m2  region  on  either  side  of  the  device  and  work  even  when  it  is  fully  occluded  from  the  finger.
2	Using  socio  ecological  model  to  inform  the  design  of  persuasive  applications.  Diverse  persuasive  applications  that  aim  for  behavioural  changes  have  been  developed.  However,  the  method  in  which  particular  persuasive  design  principles  are  chosen  over  others  remains  unclear.  Meanwhile,  the  use  of  socio-ecological  model  has  been  widely  utilized  in  clinical  research,  as  a  basis  to  understand  the  factors  in  the  entire  ecological  system  that  influences  behavioural  patterns.  Because  persuasive  technology  aims  to  change  the  behaviour  and  attitudes  of  users,  we  believe  that  the  use  of  socio-ecological  model  would  be  beneficial  to  inform  the  design  of  persuasive  applications.  Accordingly,  in  this  paper,  we  attempt  to  demonstrate  the  mapping  of  the  socio-ecological  factors  and  persuasive  design  principles  by  conducting  interviews  and  expert  reviews.  Based  on  our  approach,  12  socio-ecological  factors  that  influence  physical  activity  behaviour,  and  corresponding  relevant  persuasive  design  principles  to  deal  with  these  factors,  are  identified.
2	How  carat  affects  user  behavior  implications  for  mobile  battery  awareness  applications.  Mobile  devices  have  limited  battery  life,  and  numerous  battery  management  applications  are  available  that  aim  to  improve  it.  This  paper  examines  a  large-scale  mobile  battery  awareness  application,  called  Carat,  to  see  how  it  changes  user  behavior  with  long-term  use.  We  conducted  a  survey  of  current  Carat  Android  users  and  analyzed  their  interaction  logs.  The  results  show  that  long-term  Carat  users  save  more  battery,  charge  their  devices  less  often,  learn  to  manage  their  battery  with  less  help  from  Carat,  have  a  better  understanding  of  how  Carat  works,  and  may  enjoy  competing  against  other  users.  Based  on  these  findings,  we  propose  a  set  of  guidelines  for  mobile  battery  awareness  applications:  battery  awareness  applications  should  make  the  reasoning  behind  their  recommendations  understandable  to  the  user,  be  tailored  to  retain  long-term  users,  take  the  audience  into  account  when  formulating  feedback,  and  distinguish  third-party  and  system  applications.
2	Legitimacy  work  invisible  work  in  philanthropic  crowdfunding.  Crowdfunding,  the  practice  of  funding  a  project  by  soliciting  donations  via  the  internet,  allows  organizations  and  individuals  alike  to  raise  funds  for  a  variety  of  causes.  In  this  paper,  we  present  the  results  of  a  study  of  philanthropic  crowdfunding,  aimed  at  understanding  some  of  the  practices  and  needs  associated  with  raising  money  for  charitable  causes.  Our  analysis  highlights  the  diversity  of  stakeholders  and  roles  in  philanthropic  crowdfunding  and  the  immense  amount  of  work  associated  with  legitimizing  many  of  these  roles,  including  the  fundraiser,  organization,  platform,  and  project.  We  introduce  the  construct  of  legitimacy  work  and  discuss  ways  in  which  current  crowdfunding  systems  both  support  and  thwart  this  work.
2	Virtual  projection  exploring  optical  projection  as  a  metaphor  for  multi  device  interaction.  Handheld  optical  projectors  provide  a  simple  way  to  overcome  the  limited  screen  real-estate  on  mobile  devices.  We  present  virtual  projection  (VP),  an  interaction  metaphor  inspired  by  how  we  intuitively  control  the  position,  size,  and  orientation  of  a  handheld  optical  projector's  image.  VP  is  based  on  tracking  a  handheld  device  without  an  optical  projector  and  allows  selecting  a  target  display  on  which  to  position,  scale,  and  orient  an  item  in  a  single  gesture.  By  relaxing  the  optical  projection  metaphor,  we  can  deviate  from  modeling  perspective  projection,  for  example,  to  constrain  scale  or  orientation,  create  multiple  copies,  or  offset  the  image.  VP  also  supports  dynamic  filtering  based  on  the  projection  frustum,  creating  overview  and  detail  applications,  and  selecting  portions  of  a  larger  display  for  zooming  and  panning.  We  show  exemplary  use  cases  implemented  using  our  optical  feature-tracking  framework  and  present  the  results  of  a  user  study  demonstrating  the  effectiveness  of  VP  in  complex  interactions  with  large  displays.
2	Designing  sports  a  framework  for  exertion  games.  Exertion  games  require  investing  physical  effort.  The  fact  that  such  games  can  support  physical  health  is  tempered  by  our  limited  understanding  of  how  to  design  for  engaging  exertion  experiences.  This  paper  introduces  the  Exertion  Framework  as  a  way  to  think  and  talk  about  Exertion  Games,  both  for  their  formative  design  and  summative  analysis.  Our  Exertion  Framework  is  based  on  the  ways  in  which  we  can  conceive  of  the  body  investing  in  game-directed  exertion,  supported  by  four  perspectives  on  the  body  (the  Responding  Body,  Moving  Body,  Sensing  Body  and  Relating  Body)  and  three  perspectives  on  gaming  (rules,  play  and  context).  The  paper  illustrates  how  this  framework  was  derived  from  prior  systems  and  theory,  and  presents  a  case  study  of  how  it  has  been  used  to  inspire  novel  exertion  interactions.
2	Steering  performance  with  error  accepting  delays.  In  steering  law  tasks,  deviating  from  the  path  is  immediately  considered  an  error  operation.  However,  in  navigating  a  hierarchical  menu  item,  which  is  a  representative  application  of  the  law,  a  deviation  within  a  short  duration  is  sometimes  permitted.  We  tested  the  validity  of  the  steering  law  model  with  various  durations  of  such  error-accepting  delays  and  found  that  it  showed  high  fits  for  each  delay  condition  (R2  >  0.96)  but  poor  fits  if  the  delay  values  were  not  separated  (R2  =  0.58).  Because  the  average  movement  speed  linearly  increased  as  the  delay  increased,  we  refined  the  model  by  taking  the  delay  into  account,  and  the  fitness  was  significantly  improved  (R2  =  0.97).  Our  model  will  help  GUI  designers  estimate  the  average  operational  time  on  the  basis  of  the  menu  item  length,  width,  and  error-accepting  delay.
2	Hybrid  brailler  combining  physical  and  gestural  interaction  for  mobile  braille  input  and  editing.  Braille  input  enables  fast  nonvisual  entry  speeds  on  mobile  touchscreen  devices.  Yet,  the  lack  of  tactile  cues  commonly  results  in  typing  errors,  which  are  hard  to  correct.  We  propose  Hybrid-Brailler,  an  input  solution  that  combines  physical  and  gestural  interaction  to  provide  fast  and  accurate  Braille  input.  We  use  the  back  of  the  device  for  physical  chorded  input  while  freeing  the  touchscreen  for  gestural  interaction.  Gestures  are  used  in  editing  operations,  such  as  caret  movement,  text  selection,  and  clipboard  control,  enhancing  the  overall  text  entry  experience.  We  conducted  two  user  studies  to  assess  both  input  and  editing  performance.  Results  show  that  Hybrid-Brailler  supports  fast  entry  rates  as  its  virtual  counterpart,  while  significantly  increasing  input  accuracy.  Regarding  editing  performance,  when  compared  with  the  mainstream  technique,  Hybrid-Brailler  shows  performance  benefits  of  21%  in  speed  and  increased  editing  accuracy.  We  finish  with  lessons  learned  for  designing  future  nonvisual  input  and  editing  techniques.
2	Feel  my  pain  design  and  evaluation  of  painpad  a  tangible  device  for  supporting  inpatient  self  logging  of  pain.  Monitoring  patients'  pain  is  a  critical  issue  for  clinical  caregivers,  particularly  among  staff  responsible  for  providing  analgesic  relief.  However,  collecting  regularly  scheduled  pain  readings  from  patients  can  be  difficult  and  time-consuming  for  clinicians.  In  this  paper  we  present  Painpad,  a  tangible  device  that  was  developed  to  allow  patients  to  engage  in  self-logging  of  their  pain.  We  report  findings  from  two  hospital-based  field  studies  in  which  Painpad  was  deployed  to  a  total  of  78  inpatients  recovering  from  ambulatory  surgery.  We  find  that  Painpad  results  in  improved  frequency  and  compliance  with  pain  logging,  and  that  self-logged  scores  may  be  more  faithful  to  patients'  experienced  pain  than  corresponding  scores  reported  to  nurses.  We  also  show  that  older  adults  may  prefer  tangible  interfaces  over  tablet-based  alternatives  for  reporting  their  pain,  and  we  contribute  design  lessons  for  pain  logging  devices  intended  for  use  in  hospital  settings.
2	A  bermuda  triangle  a  review  of  method  application  and  triangulation  in  user  experience  evaluation.  User  experience  (UX)  evaluation  is  a  growing  field  with  diverse  approaches.  To  understand  the  development  since  previous  meta-review  efforts,  we  conducted  a  state-of-the-art  review  of  UX  evaluation  techniques  with  special  attention  to  the  triangulation  between  methods.  We  systematically  selected  and  analyzed  100  papers  from  recent  years  and  while  we  found  an  increase  of  relevant  UX  studies,  we  also  saw  a  remaining  overlap  with  pure  usability  evaluations.  Positive  trends  include  an  increasing  percentage  of  field  rather  than  lab  studies  and  a  tendency  to  combine  several  methods  in  UX  studies.  Triangulation  was  applied  in  more  than  two  thirds  of  the  studies,  and  the  most  common  method  combination  was  questionnaires  and  interviews.  Based  on  our  analysis,  we  derive  common  patterns  for  triangulation  in  UX  evaluation  efforts.  A  critical  discussion  about  existing  approaches  should  help  to  obtain  stronger  results,  especially  when  evaluating  new  technologies.
2	Embodied  learning  in  immersive  smart  spaces.  This  paper  presents  the  design  and  evaluation  of  IMAGINE,  a  novel  interactive  immersive  smart  space  for  embodied  learning.  In  IMAGINE  children  use  full-body  movements  and  gestures  to  interact  with  multimedia  educational  contents  projected  on  the  wall  and  on  the  floor,  while  synchronized  light  effects  enhance  immersivity.  A  controlled  study  performed  at  a  primary  school  with  48  children  aged  6-8  highlights  the  educational  potential  of  an  immersive  embodied  solution,  also  compared  to  traditional  teaching  methods,  and  draws  some  implications  for  smart-space  technology  adoption  in  educational  contexts.
2	Technology  for  situated  and  emergent  play  a  bridging  concept  and  design  agenda.  Despite  the  capacity  of  play  to  spontaneously  emerge  in  our  daily  life,  the  scope  of  application  of  play  design  in  HCI  is  generally  narrower,  specifically  targeting  areas  of  pure  leisure,  or  wholly  utilitarian  and  productive  play.  Here  we  focus  on  the  value  of  play  design  to  respond  to  and  support  our  natural  gravitation  towards  emergent  play  that  helps  to  meet  our  social  and  emotional  needs.  We  present  a  bridging  concept:  Technology  for  Situated  and  Emergent  Play,  i.e.  technology  design  that  supports  playful  engagement  that  emerges  interwoven  with  our  everyday  activities  outside  leisure,  and  that  enriches  these  activities  with  socio-emotional  value.  Our  intermediate-level  contribution  has  value  as  a  synthesis  piece:  it  weaves  together  theories  of  play  and  play  design  and  bridges  them  with  concrete  design  examples.  As  a  bridging  concept,  it  contributes:  i)  theoretical  grounding;  ii)  inspiring  design  exemplars  that  illustrate  the  theory  and  foreground  its  value;  and  iii)  design  articulations  in  the  form  of  valuable  experiential  qualities  and  design  features.  Our  work  can  help  to  focus  design  agendas  for  playful  technology  and  inspire  future  designs  in  this  space.
2	Measuring  situational  awareness  aptitude  using  functional  near  infrared  spectroscopy.  Attempts  have  been  made  to  evaluate  people’s  situational  awareness  (SA)  in  military  and  civilian  contexts  through  subjective  surveys,  speed,  and  accuracy  data  acquired  during  SA  target  tasks.  However,  it  is  recognized  in  the  SA  domain  that  more  systematic  measurement  is  necessary  to  assess  SA  theories  and  applications.  Recent  advances  in  biomedical  engineering  have  enabled  relatively  new  ways  to  measure  cognitive  and  physiological  state  changes,  such  as  with  functional  near-infrared  spectroscopy  (fNIRS).  In  this  paper,  we  provide  a  literature  review  relating  to  SA  and  fNIRS  and  present  an  experiment  conducted  with  an  fNIRS  device  comparing  differences  in  the  brains  between  people  with  high  and  low  SA  aptitude.  Our  results  suggest  statistically  significant  differences  in  brain  activity  between  the  high  SA  group  and  low  SA  group.
2	Kolibri  tiny  and  fast  gestures  for  large  pen  based  surfaces.  Triggering  commands  on  large  interactive  surfaces  is  less  efficient  than  on  desktop  PCs.  It  requires  either  large  physical  movements  to  reach  an  interaction  area  (e.g.,  buttons)  or  additional  operations  to  call  context  menus  (e.g.,  dwell).  There  is  a  lack  of  efficient  ways  to  trigger  shortcuts.  We  introduce  Kolibri  -  a  pen-based  gesture  system  that  allows  fast  access  of  commands  on  interactive  whiteboards.  Users  can  draw  tiny  gestures  (approx.  3  mm)  anywhere  on  the  surface  to  trigger  commands  without  interfering  with  normal  inking.  This  approach  does  neither  require  entering  a  gesture  mode,  nor  dedicated  gesture  areas.  The  implementation  relies  on  off-the-shelf  hardware  only.  We  tested  the  feasibility  and  explored  the  properties  of  this  technique  with  several  studies.  The  results  from  a  controlled  experiment  show  significant  benefits  of  Kolibri  comparing  to  an  existing  approach.
2	Ethics  logs  and  videotape  ethics  in  large  scale  user  trials  and  user  generated  content.  As  new  technologies  are  appropriated  by  researchers,  the  community  must  come  to  terms  with  the  evolving  ethical  responsibilities  we  have  towards  participants.  This  workshop  brings  together  researchers  to  discuss  the  ethical  issues  of  running  large-scale  user  trials,  and  to  provide  guidance  for  future  research.  Trials  of  the  scale  of  10s  or  100s  of  thousands  of  participants  offer  great  potential  benefits  in  terms  of  attracting  users  from  vastly  different  geographical  and  social  contexts,  but  raise  significant  ethical  challenges.  The  inability  to  ensure  user  understanding  of  the  information  required  to  provide  informed  consent  and  problems  involved  in  making  users  aware  of  the  implications  of  the  information  being  collected  all  beg  the  question:  how  can  researchers  ethically  take  advantage  of  the  opportunities  these  new  technologies  afford?
2	The  secret  life  of  a  persona  when  the  personal  becomes  private.  Some  organizations  fail  to  involve  users  in  systems  development  due  to  a  widespread  organization,  high  workload  or  secrecy  issues.  A  remedy  against  this  situation  could  be  the  persona  method  in  which  users  and  main  stakeholders  as  represented  as  fictitious  characters.  Personas  help  eliciting  user  needs  and  requirements,  facilitate  design  choices  and  are  an  overall  communication  aid  where  users  cannot  be  present.  An  important  part  of  the  persona  method,  as  portrayed  in  literature,  is  the  personal  details  that  make  the  personas  trustworthy  and  alive.  In  this  paper  we  present  two  cases  in  which  personas  have  been  developed  and  used,  but  where  the  personal  is  scarce  or  even  non-existent  because  of  a  dispersed  organisation,  the  organisational  culture  and  secrecy  issues.  The  paper  describes  how  the  personas  were  developed,  used  and  received  and  how  the  method  was  altered  in  order  to  work  in  these  special  circumstances.
2	I  feel  for  my  avatar  embodied  perception  in  ves.  Visual  perception  is  dependent  upon  one's  physical  state.  The  apparent  inclination  of  a  hill  is  overestimated  when  the  observer  is  carrying  a  heavy  backpack.  But,  what  if  the  hill  is  a  virtual  one  and  the  user  is  about  to  navigate  the  virtual  environment  through  an  avatar?  In  a  2  (user  with  a  backpack  vs.  user  without  the  backpack)  ×  2  (avatar  with  a  virtual  backpack  vs.  avatar  without  a  virtual  backpack)  ×  2  (customized  avatar  vs.  assigned  avatar)  between-subjects  experiment  (N  =  121),  participants  estimated  the  hill  as  being  steeper  when  using  a  customized  avatar  rather  than  an  assigned  one.  When  the  avatar  is  encumbered  by  a  heavy  virtual  backpack,  those  with  a  customized  avatar  perceived  the  virtual  hill  as  being  more  difficult  to  climb.  Avatar  customization  and  the  physical  resources  of  the  avatar  (operationalized  here  in  the  form  of  a  'virtual'  backpack)  were  found  to  be  key  predictors  of  embodied  perception  in  virtual  environments  (VE).  This  has  implications  for  the  design  of  games  and  interventions  that  make  use  of  VEs.
2	Provenance  for  the  people  an  hci  perspective  on  the  w3c  prov  standard  through  an  online  game.  In  the  information  age,  tools  for  examining  the  validity  of  data  are  invaluable.  Provenance  is  one  such  tool,  and  the  PROV  model  proposed  by  the  World  Wide  Web  Consortium  in  2013  offers  a  means  of  expressing  provenance  in  a  machine  readable  format.  In  this  paper,  we  examine  from  a  user's  standpoint  notions  of  provenance,  the  accessibility  of  the  PROV  model,  and  the  general  attitudes  towards  history  and  the  verifiability  of  information  in  modern  data  society.  We  do  this  through  the  medium  of  an  online-game  designed  to  explore  these  issues  and  present  the  findings  of  the  study  along  with  a  discussion  of  some  of  its  implications.
2	Beyond  geofencing  specifying  location  in  location  based  reminder  applications.  Location-based  reminders  (LBRs)  use  people's  physical  location  to  trigger  reminders.  Today's  advanced  mobile  technology  has  made  LBRs  commonplace.In  this  paper  we  report  of  a  short  study  to  evaluate  the  specification  of  location  in  a  commercially  available  LBR.  The  study  was  followed  by  a  survey  to  collect  typical  to-do  tasks  that  include  location  information.Our  findings  suggest  that  location-based  reminders  are  more  complicated  than  what  current  LBR  software  supports.  Based  on  our  results,  we  propose  a  classification  of  different  uses  of  location  in  to-do  tasks.  The  different  uses  of  locations,  as  per  our  classification,  has  implications  for  the  design  of  future  LBRs.
2	Robotic  wheelchair  easy  to  move  and  communicate  with  companions.  Although  it  is  desirable  for  wheelchair  users  to  go  out  alone  by  operating  wheelchairs  on  their  own,  they  are  often  accompanied  by  caregivers  or  companions.  In  designing  robotic  wheelchairs,  therefore,  it  is  important  to  consider  not  only  how  to  assist  the  wheelchair  user  but  also  how  to  reduce  companions'  load  and  support  their  activities.  We  specially  focus  on  the  communications  among  wheelchair  users  and  companions  because  the  face-to-face  communication  is  known  to  be  effective  to  ameliorate  elderly  mental  health.  Hence,  we  proposed  a  robotic  wheelchair  able  to  move  alongside  a  companion.  We  demonstrate  our  robotic  wheelchair.  All  attendees  can  try  to  ride  and  control  our  robotic  wheelchair.
2	Reflexive  loopers  for  solo  musical  improvisation.  Loop  pedals  are  real-time  samplers  that  playback  audio  played  previously  by  a  musician.  Such  pedals  are  routinely  used  for  music  practice  or  outdoor  "busking".  However,  loop  pedals  always  playback  the  same  material,  which  can  make  performances  monotonous  and  boring  both  to  the  musician  and  the  audience,  preventing  their  widespread  uptake  in  professional  concerts.  In  response,  we  propose  a  new  approach  to  loop  pedals  that  addresses  this  issue,  which  is  based  on  an  analytical  multi-modal  representation  of  the  audio  input.  Instead  of  simply  playing  back  prerecorded  audio,  our  system  enables  real-time  generation  of  an  audio  accompaniment  reacting  to  what  is  currently  being  performed  by  the  musician.  By  combining  different  modes  of  performance  -  e.g.  bass  line,  chords,  solo  -  from  the  musician  and  system  automatically,  solo  musicians  can  perform  duets  or  trios  with  themselves,  without  engendering  the  so-called  canned  (boringly  repetitive  and  unresponsive)  music  effect  of  loop  pedals.  We  describe  the  technology,  based  on  supervised  classification  and  concatenative  synthesis,  and  then  illustrate  our  approach  on  solo  performances  of  jazz  standards  by  guitar.  We  claim  this  approach  opens  up  new  avenues  for  concert  performance.
2	Supporting  the  mobile  notification  process  through  tactile  cues  selected  using  a  paired  comparison  task.  The  process  of  checking  mobile  notifications  can  be  challenging  when  the  user  is  engaged  with  another  task  that  requires  him/her  to  monitor  the  path  ahead  (e.g.  running,  driving).  Developing  expressive  tactile  feedback  to  communicate  key  components  of  the  message  would  enable  users  to  decide  whether  to  attend  to  the  notification,  or  to  continue  with  the  on-going  activity.  We  describe  the  design  of  a  paired-comparison  task  to  determine  how  to  map  tactile  parameters  to  characteristics  of  incoming  messages.  Early  findings  from  a  field  study  highlight  the  promise  offered  by  multi-parameter  tactile  cues  designed  using  mappings  identified  from  the  paired-comparison  task,  even  when  distracters  are  present.
2	Augkey  increasing  foveal  throughput  in  eye  typing  with  augmented  keys.  Eye-typing  is  an  important  tool  for  people  with  physical  disabilities  and,  for  some,  it  is  their  main  form  of  communication.  By  observing  expert  typists  using  physical  keyboards,  we  notice  that  visual  throughput  is  considerably  reduced  in  current  eye-typing  solutions.  We  propose  AugKey  to  improve  throughput  by  augmenting  keys  with  a  prefix,  to  allow  continuous  text  inspection,  and  suffixes  to  speed  up  typing  with  word  prediction.  AugKey  limits  the  visual  information  to  the  foveal  region  to  minimize  eye  movements  (i.e.,  reduce  eye  work).  We  have  applied  AugKey  to  a  dwell-time  keyboard  and  compared  its  performance  with  two  conditions  with  no  augmented  feedback:  a  keyboard  with  and  one  without  word  prediction.  Results  show  that  AugKey  can  be  about  28%  faster  than  no  word  prediction  and  20%  faster  than  traditional  word  prediction,  with  a  smaller  workload  index.
2	Ux  strategy  as  a  kick  starter  for  design  transformation  in  an  engineering  company.  Digitalization  is  driving  established  companies  to  become  software  providers.  To  succeed  in  software,  these  companies  must  excel  beyond  traditional  engineering  and  take  user  experience  seriously.  How  do  you  make  this  kind  of  a  change?  In  this  case  study,  we  describe  a  start  of  a  UX  improvement  program  at  a  global  company  with  a  number  external  and  internal  software  offerings,  but  no  culture  of  utilizing  design  or  dealing  with  user  experience.  The  story  starts  from  the  design  research,  leads  to  a  UX  strategy  framework  and  an  eventual  launch  of  a  UX  transformation  project.  We  explain  the  elements  of  UX  strategy  developed  in  the  project  and  provide  insights  on  what  are  the  key  actions  and  enablers,  such  as  the  executive  buy  in,  for  a  such  project.  Our  goal  with  this  report  is  to  inform  the  UX  researchers  of  the  need  for  strategic  UX  work  as  an  enabler  of  practical  UX.
2	Learnersourcing  thematic  and  inter  contextual  annotations  from  islamic  texts.  In  this  paper,  I  introduce  an  approach  for  obtaining  semantic  annotations  in  specialized  and  knowledge  intensive  domains.  In  particular,  I  consider  the  case  of  classical  and  historic  Islamic  texts,  primarily  the  Qur'an  and  the  books  of  Prophetic  narrations  called  the  Hadith.  I  propose  formal  and  scalable  methods  towards  thematic  classification,  annotion  and  interlinking  for  these  texts;  this  is  done  at  various  levels  of  granulartiy  that  existing  research  fails  to  address.  I  apply  a  'semantics  driven  learnersourcing'  methodology,  which  leverages  primarily  upon  students  engaging  in  typical  knowledge  seeking  and  learning  scenarios,  and  embeds  within  them,  semantic  annotation  tasks.  The  chosen  method  also  ensures  annotation  reliability  by  introducing  an  'expert  sourcing'  workflow  tightly  integrated  within  the  system.  Therefore,  quantitative  measures  of  ensuring  annotation  quality  are  woven  into  the  very  fabric  of  learnersourcing.
2	Hustling  online  understanding  consolidated  facebook  use  in  an  informal  settlement  in  nairobi.  Facebook  is  a  global  phenomenon,  yet  little  is  known  about  use  of  the  site  in  urban  parts  of  the  developing  world  where  the  social  network's  users  are  increasingly  located.  We  qualitatively  studied  Facebook  use  among  28  young  adults  living  in  Viwandani,  an  informal  settlement,  or  slum,  in  Nairobi,  Kenya.  We  find  that  to  overcome  the  costs  associated  with  Internet  use,  participants  consolidated  diverse  online  activities  onto  Facebook;  here  we  focus  on  the  most  common  practice--using  Facebook  to  support  income  generation.  Viwandani  residents  used  the  site  to  look  for  employment  opportunities,  market  themselves,  and  seek  remittances  from  friends  and  family  abroad.  We  use  our  findings  to  motivate  a  design  agenda  for  the  urban  poor  built  on  an  understanding  that  Facebook  is  used,  with  mixed-success,  to  support  income  generation.  A  key  part  of  this  agenda  calls  for  developing  ICT  interventions  grounded  in  users'  existing  practices  rather  than  introducing  new  and  unfamiliar  ones.
2	Assessing  the  vulnerability  of  magnetic  gestural  authentication  to  video  based  shoulder  surfing  attacks.  Secure  user  authentication  on  mobile  phones  is  crucial,  as  they  store  highly  sensitive  information.  Common  approaches  to  authenticate  a  user  on  a  mobile  phone  are  based  either  on  entering  a  PIN,  a  password,  or  drawing  a  pattern.  However,  these  authentication  methods  are  vulnerable  to  the  shoulder  surfing  attack.  The  risk  of  this  attack  has  increased  since  means  for  recording  high-resolution  videos  are  cheaply  and  widely  accessible.  If  the  attacker  can  videotape  the  authentication  process,  PINs,  passwords,  and  patterns  do  not  even  provide  the  most  basic  level  of  security.  In  this  project,  we  assessed  the  vulnerability  of  a  magnetic  gestural  authentication  method  to  the  video-based  shoulder  surfing  attack.  We  chose  a  scenario  that  is  favourable  to  the  attack-er.  In  a  real  world  environment,  we  videotaped  the  interactions  of  four  users  performing  magnetic  signatures  on  a  phone,  in  the  presence  of  HD  cameras  from  four  different  angles.  We  then  recruited  22  participants  and  asked  them  to  watch  the  videos  and  try  to  forge  the  signatures.  The  results  revealed  that  with  a  certain  threshold,  i.e,  th=1.67,  none  of  the  forging  attacks  was  successful,  whereas  at  this  level  all  eligible  login  attempts  were  successfully  recognized.  The  qualitative  feedback  also  indicated  that  users  found  the  magnetic  gestural  signature  authentication  method  to  be  more  secure  than  PIN-based  and  2D  signature  methods.
2	Hapthimble  a  wearable  haptic  device  towards  usable  virtual  touch  screen.  A  virtual  touch  screen  concept  using  an  optical  see-through  head-mounted  display  has  been  suggested.  With  a  virtual  touch  screen,  the  user's  direct-touch  interactions  are  allowed  in  much  the  same  way  as  a  conventional  touch  screen,  but  the  absence  of  haptic  feedback  and  physical  constraint  leads  to  poor  user  performance.  To  overcome  this  issue,  we  developed  a  wearable  haptic  device,  called  HapThimble.  It  provides  various  types  of  haptic  feedback  (tactile,  pseudo-force,  and  vibrotactile)  to  the  user's  fingertip  and  mimics  physical  buttons  based  on  force-penetration  depth  curves.  We  conducted  three  experiments  with  HapThimble.  The  first  experiment  confirmed  that  HapThimble  could  increase  a  users'  performance  when  conducting  clicking  and  dragging  tasks.  The  second  experiment  revealed  that  users  could  differentiate  between  six  types  of  haptic  feedback,  rendered  based  on  different  force-penetration  depth  curves  obtained  using  HapThimble.  Last,  we  conducted  a  test  to  investigate  the  similarity  between  the  physical  buttons  and  the  mimicked  haptic  buttons  and  obtained  a  90.3%  success  rate.
2	Assessment  design  for  emergent  game  based  learning.  Educational  games  may  lend  themselves  to  innovative  forms  of  learning  assessment.  This  paper  reports  on  game-based  science  learning  assessments  that  explore  how  to  measure  the  emergent  learning  that  takes  place  in  games  by  revealing  tacit  knowledge  development.  This  research  combines  video  analysis  and  educational  data  mining  to  identify  cognitive  strategies  that  emerge  through  gameplay.  By  studying  the  video  and  click  data  from  high  school  learners  playtesting  the  game,  Impulse,  we  identify  systematic  ways  of  predicting  the  observed  strategies  and  making  possible  connections  to  formal  science  learning.
2	Social  reflections  on  fitness  tracking  data  a  study  with  families  in  low  ses  neighborhoods.  Wearable  activity  trackers  can  encourage  physical  activity  (PA)-a  behavior  critical  for  preventing  obesity  and  reducing  the  risks  of  chronic  diseases.  However,  prior  work  has  rarely  explored  how  these  tools  can  leverage  family  support  or  help  people  think  about  strategies  for  being  active-wo  factors  necessary  for  achieving  regular  PA.  In  this  2-month  qualitative  study,  we  investigated  PA  tracking  practices  amongst  14  families  living  in  low-income  neighborhoods,  where  obesity  is  prevalent.  We  characterize  how  social  discussions  of  PA  data  rarely  extended  beyond  the  early  stages  of  experiential  learning,  thus  limiting  the  utility  of  PA  trackers.  Caregivers  and  children  rarely  analyzed  their  experiences  to  derive  insights  about  the  meaning  of  their  PA  data  for  their  wellbeing.  Those  who  engaged  in  these  higher-order  learning  processes  were  often  influenced  by  parenting  beliefs  shaped  by  personal  health  experiences.  We  contribute  recommendations  for  how  technology  can  more  effectively  support  family  experiential  learning  using  PA  tracking  data.
2	Social  media  testdrive  real  world  social  media  education  for  the  next  generation.  Social  media  sites  are  where  life  happens  for  many  of  today's  young  people,  so  it  is  important  to  teach  them  to  use  these  sites  safely  and  effectively.  Many  youth  receive  classroom  education  on  digital  literacy  topics,  but  have  few  chances  to  build  actual  skills.  Social  Media  TestDrive,  an  interactive  social  media  simulation,  fills  a  gap  in  digital  literacy  education  by  combining  experiential  learning  in  a  realistic  and  safe  social  media  environment  with  educator-facilitated  classroom  lessons.  The  tool  was  piloted  with  12  educators  and  over  200  students,  and  formative  evaluation  data  suggest  that  TestDrive  achieved  high  levels  of  engagement  with  both  groups.  Students  reported  the  modules  enhanced  their  understanding  of  digital  citizenship  issues,  and  educators  noted  that  students  were  engaging  in  meaningful  classroom  conversations.  Finally,  we  discuss  the  importance  of  involving  multiple  stakeholder  groups  (e.g.,  researchers,  youth,  educators,  curriculum  developers)  in  designing  educational  technology.
2	Effects  of  local  latency  on  game  pointing  devices  and  game  pointing  tasks.  Studies  have  shown  certain  game  tasks  such  as  targeting  to  be  negatively  and  significantly  affected  by  latencies  as  low  as  41ms.  Therefore  it  is  important  to  understand  the  relationship  between  local  latency  -  delays  between  an  input  action  and  resulting  change  in  the  display  -  and  common  gaming  tasks  such  as  targeting  and  tracking.  In  addition,  games  now  use  a  variety  of  input  devices,  including  touchscreens,  mice,  tablets  and  controllers.  These  devices  provide  very  different  combinations  of  direct/indirect  input,  absolute/relative  movement,  and  position/rate  control,  and  are  likely  to  be  affected  by  latency  in  different  ways.  We  performed  a  study  evaluating  and  comparing  the  effects  of  latency  across  four  devices  (touchscreen,  mouse,  controller  and  drawing  tablet)  on  targeting  and  interception  tasks.  We  analyze  both  throughput  and  path  characteristics,  identify  differences  between  devices,  and  provide  design  considerations  for  game  designers.
2	Geocoin  supporting  ideation  and  collaborative  design  with  smart  contracts.  Design  and  HCI  researchers  are  increasingly  working  with  complex  digital  infrastructures,  such  as  cryptocurrencies,  distributed  ledgers  and  smart  contracts.  These  technologies  will  have  a  profound  impact  on  digital  systems  and  their  audiences.  However,  given  their  emergent  nature  and  technical  complexity,  involving  non-specialists  in  the  design  of  applications  that  employ  these  technologies  is  challenging.  In  this  paper,  we  discuss  these  challenges  and  present  GeoCoin,  a  location-based  platform  for  embodied  learning  and  speculative  ideating  with  smart  contracts.  In  collaborative  workshops  with  GeoCoin,  participants  engaged  with  location-based  smart  contracts,  using  the  platform  to  explore  digital  'debit'  and  'credit'  zones  in  the  city.  These  exercises  led  to  the  design  of  diverse  distributed-ledger  applications,  for  time-limited  financial  unions,  participatory  budgeting,  and  humanitarian  aid.  These  results  contribute  to  the  HCI  community  by  demonstrating  how  an  experiential  prototype  can  support  understanding  of  the  complexities  behind  new  digital  infrastructures  and  facilitate  participant  engagement  in  ideation  and  design  processes.
2	Trends  and  trajectories  for  explainable  accountable  and  intelligible  systems  an  hci  research  agenda.  Advances  in  artificial  intelligence,  sensors  and  big  data  management  have  far-reaching  societal  impacts.  As  these  systems  augment  our  everyday  lives,  it  becomes  increasing-ly  important  for  people  to  understand  them  and  remain  in  control.  We  investigate  how  HCI  researchers  can  help  to  develop  accountable  systems  by  performing  a  literature  analysis  of  289  core  papers  on  explanations  and  explaina-ble  systems,  as  well  as  12,412  citing  papers.  Using  topic  modeling,  co-occurrence  and  network  analysis,  we  mapped  the  research  space  from  diverse  domains,  such  as  algorith-mic  accountability,  interpretable  machine  learning,  context-awareness,  cognitive  psychology,  and  software  learnability.  We  reveal  fading  and  burgeoning  trends  in  explainable  systems,  and  identify  domains  that  are  closely  connected  or  mostly  isolated.  The  time  is  ripe  for  the  HCI  community  to  ensure  that  the  powerful  new  autonomous  systems  have  intelligible  interfaces  built-in.  From  our  results,  we  propose  several  implications  and  directions  for  future  research  to-wards  this  goal.
2	Sig  chatbots  for  social  good.  Chatbots  are  emerging  as  an  increasingly  important  area  for  the  HCI  community,  as  they  provide  a  novel  means  for  users  to  interact  with  service  providers.  Due  to  their  conversational  character,  chatbots  are  potentially  effective  tools  for  engaging  with  customers,  and  are  often  developed  with  commercial  interests  at  the  core.  However,  chatbots  also  represent  opportunities  for  positive  social  impact.  Chatbots  can  make  needed  services  more  accessible,  available,  and  affordable.  They  can  strengthen  users'  autonomy,  competence,  and  (possibly  counter-intuitively)  social  relatedness.  In  this  SIG  we  address  the  possible  social  benefits  of  chatbots  and  conversational  user  interfaces.  We  will  bring  together  the  existing,  but  disparate,  community  of  researchers  and  practitioners  within  the  CHI  community  and  broader  fields  who  have  an  interest  in  chatbots.  We  aim  to  discuss  the  potential  for  chatbots  to  move  beyond  their  assumed  role  as  channels  for  commercial  service  providers,  explore  how  they  may  be  used  for  social  good,  and  how  the  HCI  community  may  contribute  to  realize  this.
2	Ijqwerty  what  difference  does  one  key  change  make  gesture  typing  keyboard  optimization  bounded  by  one  key  position  change  from  qwerty.  Despite  of  a  significant  body  of  research  in  optimizing  the  virtual  keyboard  layout,  none  of  them  has  gained  large  adoption,  primarily  due  to  the  steep  learning  curve.  To  address  this  learning  problem,  we  introduced  three  types  of  Qwerty  constraints,  Qwerty1,  QwertyH1,  and  One-Swap  bounds  in  layout  optimization,  and  investigated  their  effects  on  layout  learnability  and  performance.  This  bounded  optimization  process  leads  to  IJQwerty,  which  has  only  one  pair  of  keys  different  from  Qwerty.  Our  theoretical  analysis  and  user  study  show  that  IJQwerty  improves  the  accuracy  and  input  speed  of  gesture  typing  over  Qwerty  once  a  user  reaches  the  expert  mode.  IJQwerty  is  also  extremely  easy  to  learn.  The  initial  upon-use  text  entry  speed  is  the  same  with  Qwerty.  Given  the  high  performance  and  learnability,  such  a  layout  will  more  likely  gain  large  adoption  than  any  of  previously  obtained  layouts.  Our  research  also  shows  the  disparity  from  Qwerty  substantially  affects  layout  learning.  To  minimize  the  learning  effort,  a  new  layout  needs  to  hold  a  strong  resemblance  to  Qwerty.
2	Fake  news  on  facebook  and  twitter  investigating  how  people  don  t  investigate.  With  misinformation  proliferating  online  and  more  people  getting  news  from  social  media,  it  is  crucial  to  understand  how  people  assess  and  interact  with  low-credibility  posts.  This  study  explores  how  users  react  to  fake  news  posts  on  their  Facebook  or  Twitter  feeds,  as  if  posted  by  someone  they  follow.  We  conducted  semi-structured  interviews  with  25  participants  who  use  social  media  regularly  for  news,  temporarily  caused  fake  news  to  appear  in  their  feeds  with  a  browser  extension  unbeknownst  to  them,  and  observed  as  they  walked  us  through  their  feeds.  We  found  various  reasons  why  people  do  not  investigate  low-credibility  posts,  including  taking  trusted  posters'  content  at  face  value,  as  well  as  not  wanting  to  spend  the  extra  time.  We  also  document  people's  investigative  methods  for  determining  credibility  using  both  platform  affordances  and  their  own  ad-hoc  strategies.  Based  on  our  findings,  we  present  design  recommendations  for  supporting  users  when  investigating  low-credibility  posts.
2	M  i  n  d  brain  sensor  caps  coupling  precise  brain  imaging  to  virtual  reality  head  mounted  displays.  Today,  Virtual  Reality  (VR)  and  Augmented  Reality  (AR)  are  the  new  communication  tools  readily  available  to  consumers.  Because  of  the  increasing  availability  of  AR  and  VR,  communication  and  neuroscience  researchers  are  showing  increasing  interest  in  the  use  of  VR  systems  for  studies  in  collaboration,  communication,  and  basic  neuroscience.  Beyond  relying  on  self-reported  or  behavioral  measures,  psychophysiological  or  functional  neuroimaging  measurements  sensing  brain  waves  (e.g.  EEG)  or  brain  hemodynamics  (e.g.  fNIRS)  are  powerful  techniques  for  measuring  brain  activity  while  interacting  with  virtual  reality  stimuli  or  environments.  However,  using  these  measures  with  virtual  reality  systems  can  be  difficult  due  to  physical  and  technical  constraints.  Both  Functional  Near-Infrared  Spectroscopy  (fNIRS)  and  Electroencephalography  (EEG)  need  multiple  channels  to  measure  brain  activity,  a  combination  of  cables  and  probes  must  be  attached  to  a  head  cap.  However,  this  setup  obstructs  wearing  head-mounted  display  (HMD)  in  a  VR  environment  and  the  challenge  varies  with  the  design  of  the  HMD.  To  overcome  these  limitations,  we  introduce  the  design  and  development  of  the  M.I.N.D.  brain  measurement  cap  specifically  adapted  for  research  with  virtual  reality  system.  We  discuss  the  design  process  as  well  as  the  advantages  and  limitations  of  the  current  iterative  design  of  the  cap.  Generally,  we  anticipate  that  this  measurement  system  will  expand  the  potential  of  influence  of  cognitive  neuroscience  contribute  on  VR  research  by  making  it  easier  for  researchers  to  use  a  breadth  of  tools.
2	Next  generation  image  based  lighting  using  hdr  video.  We  present  an  overview  of  our  recently  developed  systems  pipeline  for  capture,  reconstruction,  modeling  and  rendering  of  real  world  scenes  based  on  state-of-the-art  high  dynamic  range  video  (HDRV).  The  reconstructed  scene  representation  allows  for  photo-realistic  Image  Based  Lighting  (IBL)  in  complex  environments  with  strong  spatial  variations  in  the  illumination.  The  pipeline  comprises  the  following  essential  steps:      1.)  Capture  -  The  scene  capture  is  based  on  a  4MPixel  global  shutter  HDRV  camera  with  a  dynamic  range  of  more  than  24  f-stops  at  30  fps.  The  HDR  output  stream  is  stored  as  individual  un-compressed  frames  for  maximum  flexibility.  A  scene  is  usually  captured  using  a  combination  of  panoramic  light  probe  sequences  [1],  and  sequences  with  a  smaller  field  of  view  to  maximize  the  resolution  at  regions  of  special  interest  in  the  scene.  The  panoramic  sequences  ensure  full  angular  coverage  at  each  position  and  guarantee  that  the  information  required  for  IBL  is  captured.  The  position  and  orientation  of  the  camera  is  tracked  during  capture.      2.)  Scene  recovery  -  Taking  one  or  more  HDRV  sequences  as  input,  a  geometric  proxy  model  of  the  scene  is  built  using  a  semi-automatic  approach.  First,  traditional  computer  vision  algorithms  such  as  structure  from  motion  [2]  and  Manhattan  world  stereo  [3]  are  used.  If  necessary,  the  recovered  model  is  then  modified  using  an  interaction  scheme  based  on  visualizations  of  a  volumetric  representation  of  the  scene  radiance  computed  from  the  input  HDRV  sequence.  The  HDR  nature  of  this  volume  also  enables  robust  extraction  of  direct  light  sources  and  other  high  intensity  regions  in  the  scene.      3.)  Radiance  processing  -  When  the  scene  proxy  geometry  has  been  recovered,  the  radiance  data  captured  in  the  HDRV  sequences  are  re-projected  onto  the  surfaces  and  the  recovered  light  sources.  Since  most  surface  points  have  been  imaged  from  a  large  number  of  directions,  it  is  possible  to  reconstruct  view  dependent  texture  maps  at  the  proxy  geometries.  These  4D  data  sets  describe  a  combination  of  detailed  geometry  that  has  not  been  recovered  and  the  radiance  reflected  from  the  underlying  real  surfaces.  The  view  dependent  textures  are  then  processed  and  compactly  stored  in  an  adaptive  data  structure.      4.)  Rendering  -  Once  the  geometric  and  radiometric  scene  information  has  been  recovered,  it  is  possible  to  place  virtual  objects  into  the  real  scene  and  create  photo-realistic  renderings  as  illustrated  above.  The  extracted  light  sources  enable  efficient  sampling  and  rendering  times  that  are  fully  comparable  to  that  of  traditional  virtual  computer  graphics  light  sources.  No  previously  described  method  is  capable  of  capturing  and  reproducing  the  angular  and  spatial  variation  in  the  scene  illumination  in  comparable  detail.      We  believe  that  the  rapid  development  of  high  quality  HDRV  systems  will  soon  have  a  large  impact  on  both  computer  vision  and  graphics.  Following  this  trend,  we  are  developing  theory  and  algorithms  for  efficient  processing  HDRV  sequences  and  using  the  abundance  of  radiance  data  that  is  going  to  be  available.
2	Style  compatibility  for  3d  furniture  models.  This  paper  presents  a  method  for  learning  to  predict  the  stylistic  compatibility  between  3D  furniture  models  from  different  object  classes:  e.g.,  how  well  does  this  chair  go  with  that  table?  To  do  this,  we  collect  relative  assessments  of  style  compatibility  using  crowdsourcing.  We  then  compute  geometric  features  for  each  3D  model  and  learn  a  mapping  of  them  into  a  space  where  Euclidean  distances  represent  style  incompatibility.  Motivated  by  the  geometric  subtleties  of  style,  we  introduce  part-aware  geometric  feature  vectors  that  characterize  the  shapes  of  different  parts  of  an  object  separately.  Motivated  by  the  need  to  compute  style  compatibility  between  different  object  classes,  we  introduce  a  method  to  learn  object  class-specific  mappings  from  geometric  features  to  a  shared  feature  space.  During  experiments  with  these  methods,  we  find  that  they  are  effective  at  predicting  style  compatibility  agreed  upon  by  people.  We  find  in  user  studies  that  the  learned  compatibility  metric  is  useful  for  novel  interactive  tools  that:  1)  retrieve  stylistically  compatible  models  for  a  query,  2)  suggest  a  piece  of  furniture  for  an  existing  scene,  and  3)  help  guide  an  interactive  3D  modeler  towards  scenes  with  compatible  furniture.
2	Compressive  light  field  photography  using  overcomplete  dictionaries  and  optimized  projections.  Light  field  photography  has  gained  a  significant  research  interest  in  the  last  two  decades;  today,  commercial  light  field  cameras  are  widely  available.  Nevertheless,  most  existing  acquisition  approaches  either  multiplex  a  low-resolution  light  field  into  a  single  2D  sensor  image  or  require  multiple  photographs  to  be  taken  for  acquiring  a  high-resolution  light  field.  We  propose  a  compressive  light  field  camera  architecture  that  allows  for  higher-resolution  light  fields  to  be  recovered  than  previously  possible  from  a  single  image.  The  proposed  architecture  comprises  three  key  components:  light  field  atoms  as  a  sparse  representation  of  natural  light  fields,  an  optical  design  that  allows  for  capturing  optimized  2D  light  field  projections,  and  robust  sparse  reconstruction  methods  to  recover  a  4D  light  field  from  a  single  coded  2D  projection.  In  addition,  we  demonstrate  a  variety  of  other  applications  for  light  field  atoms  and  sparse  coding,  including  4D  light  field  compression  and  denoising.
2	Shading  atlas  streaming.  Streaming  high  quality  rendering  for  virtual  reality  applications  requires  minimizing  perceived  latency.  We  introduce  Shading  Atlas  Streaming  (SAS),  a  novel  object-space  rendering  framework  suitable  for  streaming  virtual  reality  content.  SAS  decouples  server-side  shading  from  client-side  rendering,  allowing  the  client  to  perform  framerate  upsampling  and  latency  compensation  autonomously  for  short  periods  of  time.  The  shading  information  created  by  the  server  in  object  space  is  temporally  coherent  and  can  be  efficiently  compressed  using  standard  MPEG  encoding.  Our  results  show  that  SAS  compares  favorably  to  previous  methods  for  remote  image-based  rendering  in  terms  of  image  quality  and  network  bandwidth  efficiency.  SAS  allows  highly  efficient  parallel  allocation  in  a  virtualized-texture-like  memory  hierarchy,  solving  a  common  efficiency  problem  of  object-space  shading.  With  SAS,  untethered  virtual  reality  headsets  can  benefit  from  high  quality  rendering  without  paying  in  increased  latency.
2	Super  space  clothoids.  Thin  elastic  filaments  in  real  world  such  as  vine  tendrils,  hair  ringlets  or  curled  ribbons  often  depict  a  very  smooth,  curved  shape  that  low-order  rod  models  ---  e.g.,  segment-based  rods  ---  fail  to  reproduce  accurately  and  compactly.  In  this  paper,  we  push  forward  the  investigation  of  high-order  models  for  thin,  inextensible  elastic  rods  by  building  the  dynamics  of  a  G2-continuous  piecewise  3D  clothoid:  a  smooth  space  curve  with  piecewise  affine  curvature.  With  the  aim  of  precisely  integrating  the  rod  kinematic  problem,  for  which  no  closed-form  solution  exists,  we  introduce  a  dedicated  integration  scheme  based  on  power  series  expansions.  It  turns  out  that  our  algorithm  reaches  machine  precision  orders  of  magnitude  faster  compared  to  classical  numerical  integrators.  This  property,  nicely  preserved  under  simple  algebraic  and  differential  operations,  allows  us  to  compute  all  spatial  terms  of  the  rod  kinematics  and  dynamics  in  both  an  efficient  and  accurate  way.  Combined  with  a  semi-implicit  time-stepping  scheme,  our  method  leads  to  the  efficient  and  robust  simulation  of  arbitrary  curly  filaments  that  exhibit  rich,  visually  pleasing  configurations  and  motion.  Our  approach  was  successfully  applied  to  generate  various  scenarios  such  as  the  unwinding  of  a  curled  ribbon  as  well  as  the  aesthetic  animation  of  spiral-like  hair  or  the  fascinating  growth  of  twining  plants.
2	Staggered  meshless  solid  fluid  coupling.  Simulating  solid-fluid  coupling  with  the  classical  meshless  methods  is  an  difficult  issue  due  to  the  lack  of  the  Kronecker  delta  property  of  the  shape  functions  when  enforcing  the  essential  boundary  conditions.  In  this  work,  we  present  a  novel  staggered  meshless  method  to  overcome  this  problem.  We  create  a  set  of  staggered  particles  from  the  original  particles  in  each  time  step  by  mapping  the  mass  and  momentum  onto  these  staggered  particles,  aiming  to  stagger  the  velocity  field  from  the  pressure  field.  Based  on  this  arrangement,  an  new  approximate  projection  method  is  proposed  to  enforce  divergence-free  on  the  fluid  velocity  with  compatible  boundary  conditions.  In  the  simulations,  the  method  handles  the  fluid  and  solid  in  a  unified  meshless  manner  and  generalizes  the  formulations  for  computing  the  viscous  and  pressure  forces.  To  enhance  the  robustness  of  the  algorithm,  we  further  propose  a  new  framework  to  handle  the  degeneration  case  in  the  solid-fluid  coupling,  which  guarantees  stability  of  the  simulation.  The  proposed  method  offers  the  benefit  that  various  slip  boundary  conditions  can  be  easily  implemented.  Besides,  explicit  collision  handling  for  the  fluid  and  solid  is  avoided.  The  method  is  easy  to  implement  and  can  be  extended  from  the  standard  SPH  algorithm  in  a  straightforward  manner.  The  paper  also  illustrates  both  one-way  and  two-way  couplings  of  the  fluids  and  rigid  bodies  using  several  test  cases  in  two  and  three  dimensions.
2	Temporally  coherent  completion  of  dynamic  video.  We  present  an  automatic  video  completion  algorithm  that  synthesizes  missing  regions  in  videos  in  a  temporally  coherent  fashion.  Our  algorithm  can  handle  dynamic  scenes  captured  using  a  moving  camera.  State-of-the-art  approaches  have  difficulties  handling  such  videos  because  viewpoint  changes  cause  image-space  motion  vectors  in  the  missing  and  known  regions  to  be  inconsistent.  We  address  this  problem  by  jointly  estimating  optical  flow  and  color  in  the  missing  regions.  Using  pixel-wise  forward/backward  flow  fields  enables  us  to  synthesize  temporally  coherent  colors.  We  formulate  the  problem  as  a  non-parametric  patch-based  optimization.  We  demonstrate  our  technique  on  numerous  challenging  videos.
2	Isotopic  approximation  within  a  tolerance  volume.  We  introduce  in  this  paper  an  algorithm  that  generates  from  an  input  tolerance  volume  a  surface  triangle  mesh  guaranteed  to  be  within  the  tolerance,  intersection  free  and  topologically  correct.  A  pliant  meshing  algorithm  is  used  to  capture  the  topology  and  discover  the  anisotropy  in  the  input  tolerance  volume  in  order  to  generate  a  concise  output.  We  first  refine  a  3D  Delaunay  triangulation  over  the  tolerance  volume  while  maintaining  a  piecewise-linear  function  on  this  triangulation,  until  an  isosurface  of  this  function  matches  the  topology  sought  after.  We  then  embed  the  isosurface  into  the  3D  triangulation  via  mutual  tessellation,  and  simplify  it  while  preserving  the  topology.  Our  approach  extends  to  surfaces  with  boundaries  and  to  non-manifold  surfaces.  We  demonstrate  the  versatility  and  efficacy  of  our  approach  on  a  variety  of  data  sets  and  tolerance  volumes.
2	Roto  accelerating  professional  rotoscoping  using  shape  manifolds.  Rotoscoping  (cutting  out  different  characters/objects/layers  in  raw  video  footage)  is  a  ubiquitous  task  in  modern  post-production  and  represents  a  significant  investment  in  person-hours.  In  this  work,  we  study  the  particular  task  of  professional  rotoscoping  for  high-end,  live  action  movies  and  propose  a  new  framework  that  works  with  roto-artists  to  accelerate  the  workflow  and  improve  their  productivity.  Working  with  the  existing  keyframing  paradigm,  our  first  contribution  is  the  development  of  a  shape  model  that  is  updated  as  artists  add  successive  keyframes.  This  model  is  used  to  improve  the  output  of  traditional  interpolation  and  tracking  techniques,  reducing  the  number  of  keyframes  that  need  to  be  specified  by  the  artist.  Our  second  contribution  is  to  use  the  same  shape  model  to  provide  a  new  interactive  tool  that  allows  an  artist  to  reduce  the  time  spent  editing  each  keyframe.  The  more  keyframes  that  are  edited,  the  better  the  interactive  tool  becomes,  accelerating  the  process  and  making  the  artist  more  efficient  without  compromising  their  control.  Finally,  we  also  provide  a  new,  professionally  rotoscoped  dataset  that  enables  truly  representative,  real-world  evaluation  of  rotoscoping  methods.  We  used  this  dataset  to  perform  a  number  of  experiments,  including  an  expert  study  with  professional  roto-artists,  to  show,  quantitatively,  the  advantages  of  our  approach.
2	T  i  engine  traversal  and  intersection  engine  for  hardware  accelerated  ray  tracing.  Ray  tracing  naturally  supports  high-quality  global  illumination  effects,  but  it  is  computationally  costly.  Traversal  and  intersection  operations  dominate  the  computation  of  ray  tracing.  To  accelerate  these  two  operations,  we  propose  a  hardware  architecture  integrating  three  novel  approaches.  First,  we  present  an  ordered  depth-first  layout  and  a  traversal  architecture  using  this  layout  to  reduce  the  required  memory  bandwidth.  Second,  we  propose  a  three-phase  ray-triangle  intersection  architecture  that  takes  advantage  of  early  exit.  Third,  we  propose  a  latency  hiding  architecture  defined  as  the  ray  accumulation  unit.  Cycle-accurate  simulation  results  indicate  our  architecture  can  achieve  interactive  distributed  ray  tracing.
2	Ssvep  based  bci  control  of  the  dasher  writing  system.  Steady  state  visual  evoked  potentials  represent  the  least  training-dependent  brain  computer  interface  paradigm,  a  condition  that  significantly  improves  the  user  experience  with  this  kind  of  interfaces.  On  the  other  hand,  the  open-source  DASHER  writing  system  is  an  unconventional  machine  interface  paradigm  on  its  own,  for  which  their  developers  encourage  the  search  of  new  controlling  mechanisms.  In  this  paper,  we  report  the  use  of  a  single  steady-state  visual  stimulus  approach  to  control  the  navigation  vector  of  the  DASHER  writing  system.  Fourier-based  power  estimation  was  used  to  compute  the  power  around  the  fundamental  band  of  the  evoked  response  detected  in  occipital  electrodes,  and  user-dependent  threshold  was  used  as  the  directional  control.  In  an  online  trial,  a  short  word  was  successfully  -albeit  inefficiently-  written  by  a  naive  user,  which  suggests  the  feasibility  of  the  application  of  such  a  paradigm  for  performing  this  task.
2	How  do  humans  sketch  objects.  Humans  have  used  sketching  to  depict  our  visual  world  since  prehistoric  times.  Even  today,  sketching  is  possibly  the  only  rendering  technique  readily  available  to  all  humans.  This  paper  is  the  first  large  scale  exploration  of  human  sketches.  We  analyze  the  distribution  of  non-expert  sketches  of  everyday  objects  such  as  'teapot'  or  'car'.  We  ask  humans  to  sketch  objects  of  a  given  category  and  gather  20,000  unique  sketches  evenly  distributed  over  250  object  categories.  With  this  dataset  we  perform  a  perceptual  study  and  find  that  humans  can  correctly  identify  the  object  category  of  a  sketch  73%  of  the  time.  We  compare  human  performance  against  computational  recognition  methods.  We  develop  a  bag-of-features  sketch  representation  and  use  multi-class  support  vector  machines,  trained  on  our  sketch  dataset,  to  classify  sketches.  The  resulting  recognition  method  is  able  to  identify  unknown  sketches  with  56%  accuracy  (chance  is  0.4%).  Based  on  the  computational  model,  we  demonstrate  an  interactive  sketch  recognition  system.  We  release  the  complete  crowd-sourced  dataset  of  sketches  to  the  community.
2	Appearance  modeling  via  proxy  to  image  alignment.  Endowing  3D  objects  with  realistic  surface  appearance  is  a  challenging  and  time-demanding  task,  as  real-world  surfaces  typically  exhibit  a  plethora  of  spatially  variant  geometric  and  photometric  detail.  Not  surprisingly,  computer  artists  commonly  use  images  of  real-world  objects  as  an  inspiration  and  a  reference  for  their  digital  creations.  However,  despite  two  decades  of  research  on  image-based  modeling,  there  are  still  no  tools  available  for  automatically  extracting  the  detailed  appearance  (microgeometry  and  texture)  of  a  3D  surface  from  a  single  image.  In  this  article,  we  present  a  novel  user-assisted  approach  for  quickly  and  easily  extracting  a  nonparametric  appearance  model  from  a  single  photograph  of  a  reference  object.  The  extraction  process  requires  a  user-provided  proxy,  whose  geometry  roughly  approximates  that  of  the  object  in  the  image.  Since  the  proxy  is  just  a  rough  approximation,  it  is  necessary  to  align  and  deform  it  so  as  to  match  the  reference  object.  The  main  contribution  of  this  work  is  a  novel  technique  to  perform  such  an  alignment,  which  enables  accurate  joint  recovery  of  geometric  detail  and  reflectance.  The  correlations  between  the  recovered  geometry  at  various  scales  and  the  spatially  varying  reflectance  constitute  a  nonparametric  appearance  model.  Once  extracted,  the  appearance  model  may  then  be  applied  to  various  3D  shapes,  whose  large-scale  geometry  may  differ  considerably  from  that  of  the  original  reference  object.  Thus,  our  approach  makes  it  possible  to  construct  an  appearance  library,  allowing  users  to  easily  enrich  detail-less  3D  shapes  with  realistic  geometric  detail  and  surface  texture.
2	Neural  drive  estimation  using  the  hypothesis  of  muscle  synergies  and  the  state  constrained  kalman  filter.  We  explore  the  hypothesis  of  muscle  synergies  to  estimate  the  neural  drive  (movement  intent)  for  upper  extremity  myoelectric  prosthesis  using  the  surface  myoelectric  signals.  Commonly  employed  pattern  classification  systems  have  certain  limitations,  like  inherent  discrete  nature,  finite  movement  classes  and  limited  degrees-of-freedom.  We  propose  a  novel  framework  based  on  the  state  space  modeling  and  the  hypothesis  of  muscle  synergies.  The  problem  is  formulated  in  the  state  space  framework  in  a  novel  way,  where  the  movement  intent  is  modeled  as  the  hidden  state  of  the  system.  A  continuous  stream  of  the  movement  intent  (the  hidden  state)  is  estimated  using  the  state-constrained  Kalman  filter.  Preliminary  experimental  results  also  confirm  the  applicability  of  the  proposed  framework  for  estimation  of  movement  intent.
2	A  signal  based  approach  for  assessing  the  accuracy  of  high  density  surface  emg  decomposition.  This  study  introduces  a  novel  and  computationally  efficient  metric  for  assessment  of  accuracy  of  decomposition  of  high-density  surface  EMG  signals.  The  metric,  so  called  Pulse-to-Noise  Ratio  (PNR),  builds  on  the  results  of  the  previously  published  Convolution  Kernel  Compensation  (CKC)  decomposition  technique  and  is  applied  to  every  identified  motor  unit  (MU),  without  any  significant  computational  or  experimental  cost.  As  validated  on  both  synthetic  and  experimental  signals  with  different  spatial  supports,  the  proposed  PNR  metrics  correlates  significantly  with  both  the  sensitivity  and  the  false  alarm  rate  of  the  identified  MU  discharges.  In  our  study,  all  the  MUs  identified  with  PNR  larger  than  30  dB  exhibited  sensitivity  larger  than  90  %  and  false  alarm  rate  below  1  %,  so  that  30  dB  can  be  used  as  a  practical  threshold  in  PNR  for  assuring  highly  accurate  MU  spike  train  identification.
2	An  intrafascicular  electrode  with  integrated  amplifiers  for  peripheral  nerve  recording.  Thin-film  longitudinal  intrafascicular  electrodes  (tf-LIFE)  are  widely  used  for  peripheral  nerve  recordings.  tf-LIFEs  are  also  promising  electrodes  for  neural  signal  acquisition  in  future  peripheral  nerve  prostheses.  However,  common  mode  signal  interference,  and  electrical  artifacts  originating  from  long  wire  leads  and  wire  movement  are  known  problems  encountered  when  using  such  electrodes,  which  lead  to  degradation  in  the  recording  quality.  Here,  we  report  an  active  tf-LIFE  electrode  implemented  by  integrating  a  neural  amplifier  chip  die  in  close  proximity  to  a  tf-LIFE  electrode.  Consuming  only  1mW  and  measuring  37  mm×7.2  mm×2.4  mm,  this  active  tf-LIFE  electrode  creates  a  reliable  connection  and  considerably  shortens  the  distance  between  the  electrode  site  and  neural  amplifier.  This  active  electrode  has  demonstrated  repeatable  in-vivo  recordings  of  compound  action  potentials  from  the  rat  sciatic  nerve.  Our  results  show  that  this  electrode  is  suitable  for  repeated  in-vivo  recordings  of  compound  action  potentials  from  nerves  in  applications  such  as  peripheral  and  visceral  nerve  interfaces  that  require  low-noise  stable  nerve  recordings.
2	Dual  loops  meshing  quality  quad  layouts  on  manifolds.  We  present  a  theoretical  framework  and  practical  method  for  the  automatic  construction  of  simple,  all-quadrilateral  patch  layouts  on  manifold  surfaces.  The  resulting  layouts  are  coarse,  surface-embedded  cell  complexes  well  adapted  to  the  geometric  structure,  hence  they  are  ideally  suited  as  domains  and  base  complexes  for  surface  parameterization,  spline  fitting,  or  subdivision  surfaces  and  can  be  used  to  generate  quad  meshes  with  a  high-level  patch  structure  that  are  advantageous  in  many  application  scenarios.  Our  approach  is  based  on  the  careful  construction  of  the  layout  graph's  combinatorial  dual.  In  contrast  to  the  primal  this  dual  perspective  provides  direct  control  over  the  globally  interdependent  structural  constraints  inherent  to  quad  layouts.  The  dual  layout  is  built  from  curvature-guided,  crossing  loops  on  the  surface.  A  novel  method  to  construct  these  efficiently  in  a  geometry-  and  structure-aware  manner  constitutes  the  core  of  our  approach.
2	A  colloidal  display  membrane  screen  that  combines  transparency  brdf  and  3d  volume.  It  is  a  common  knowledge  that  the  surface  of  soap  bubble  is  a  micro  membrane.  It  allows  light  to  pass  through  and  displays  the  color  on  its  structure.  We  developed  an  ultra  thin  and  flexible  BRDF  screen  using  the  mixture  of  two  colloidal  liquids.  There  have  been  several  researches  on  dynamic  BRDF  display[1]  in  the  past.  However,  our  work  is  different  in  several  points.  Our  membrane  screen  can  be  controlled  using  ultrasonic  vibrations.  Membrane  can  change  its  transparency  and  surface  states  depending  on  the  scales  of  ultrasonic  waves.  Based  on  these  facts,  we  developed  several  applications  of  the  membranes  such  as  3D  volume  screen.
2	Stair  blue  noise  sampling.  A  common  solution  to  reducing  visible  aliasing  artifacts  in  image  reconstruction  is  to  employ  sampling  patterns  with  a  blue  noise  power  spectrum.  These  sampling  patterns  can  prevent  discernible  artifacts  by  replacing  them  with  incoherent  noise.  Here,  we  propose  a  new  family  of  blue  noise  distributions,  Stair  blue  noise,  which  is  mathematically  tractable  and  enables  parameter  optimization  to  obtain  the  optimal  sampling  distribution.  Furthermore,  for  a  given  sample  budget,  the  proposed  blue  noise  distribution  achieves  a  significantly  larger  alias-free  low-frequency  region  compared  to  existing  approaches,  without  introducing  visible  artifacts  in  the  mid-frequencies.  We  also  develop  a  new  sample  synthesis  algorithm  that  benefits  from  the  use  of  an  unbiased  spatial  statistics  estimator  and  efficient  optimization  strategies.
2	Sweet  home.  Art  is  not  what  you  see,  but  is  what  you  interact  with.  Good  paintings  make  their  viewers  imaginative.  As  every  artist  has  been  expressing  love  in  many  ways,  the  authors  introduce  a  novel  way  to  express  the  shape  of  love  in  an  interactive  way  using  state-of-the-art  computing  technology.  The  viewers  of  the  art  will  not  see  the  picture,  but  will  interact  with  the  picture  literally.  The  concept  of  the  interactive  art  that  the  authors  propose  in  this  paper  is  that  the  viewer  of  the  art  feel  as  if  he/she  is  inside  the  painting.  You  first  find  yourself  moving  in  the  acrylic  painting  as  if  it  reflects  you  like  a  mirror.  In  the  picture  you  interact  with  a  family  of  Alpaca,  who  will  present  you  with  a  gift.  If  you  stare  at  yourself  in  the  picture  carefully  you  will  see  your  face  is  slightly  morphed  to  that  of  your  child  age.  You  are  to  bring  a  candle  with  you  to  lighten  the  picture.  As  you  move  the  candle,  the  lighting  in  the  picture  changes.  You  can  even  blow  out  the  candle,  and  every  animated  character  disappear  from  the  picture,  though  the  gift  from  Alpaca  family  remains  inside  you.
2	Adaptive  maximal  poisson  disk  sampling  on  surfaces.  In  this  paper,  we  study  the  generation  of  maximal  Poisson-disk  sets  with  varying  radii  on  surfaces.  Based  on  the  concepts  of  power  diagram  and  regular  triangulation,  we  present  a  geometric  analysis  of  gaps  in  such  disk  sets  on  surfaces,  which  is  the  key  ingredient  of  the  adaptive  maximal  Poisson-disk  sampling  framework.  Moreover,  we  adapt  the  presented  sampling  framework  for  remeshing  applications.  Several  novel  and  efficient  operators  are  developed  for  improving  the  sampling/meshing  quality  over  the  state-of-the-art.
2	A  movable  immaterial  volumetric  display.  We  have  created  a  movable,  limitedly  volumetric  "immaterial"  display.  Our  prototype  is  the  first  mobile,  hand-held  fogscreen.  It  can  show  e.g.,  slices  of  volumetric  objects  when  swept  across  mid-air.  It  is  based  on  the  patented  FogScreen  [Fogio  2013]  technology.  The  previous  FogScreen  installations  have  been  fixed  set-ups,  where  the  screen  device  and  a  projector  are  typically  rigged  up,  leaving  space  for  the  viewers  to  walk  through  the  mid-air  display.  Also  mid-air  virtual  reality  and  mid-air  user  interfaces  have  been  implemented  [DiVerdi  et  al.  2006,  Rakkolainen  et  al.  2009].
2	Photorealistic  inner  mouth  expression  in  speech  animation.  We  often  see  close-ups  of  CG  characters'  faces  in  movies  or  video  games.  In  such  situations,  the  quality  of  a  character's  face  (mainly  in  dialogue  scenes)  primarily  determines  that  of  the  entire  movie.  Creating  highly  realistic  speech  animation  is  essential  because  viewers  watch  these  scenes  carefully.  In  general,  such  speech  animations  are  created  manually  by  skilled  artists.  However,  creating  them  requires  a  considerable  effort  and  time.
2	Co  hierarchical  analysis  of  shape  structures.  We  introduce  an  unsupervised  co-hierarchical  analysis  of  a  set  of  shapes,  aimed  at  discovering  their  hierarchical  part  structures  and  revealing  relations  between  geometrically  dissimilar  yet  functionally  equivalent  shape  parts  across  the  set.  The  core  problem  is  that  of  representative  co-selection.  For  each  shape  in  the  set,  one  representative  hierarchy  (tree)  is  selected  from  among  many  possible  interpretations  of  the  hierarchical  structure  of  the  shape.  Collectively,  the  selected  tree  representatives  maximize  the  within-cluster  structural  similarity  among  them.  We  develop  an  iterative  algorithm  for  representative  co-selection.  At  each  step,  a  novel  cluster-and-select  scheme  is  applied  to  a  set  of  candidate  trees  for  all  the  shapes.  The  tree-to-tree  distance  for  clustering  caters  to  structural  shape  analysis  by  focusing  on  spatial  arrangement  of  shape  parts,  rather  than  their  geometric  details.  The  final  set  of  representative  trees  are  unified  to  form  a  structural  co-hierarchy.  We  demonstrate  co-hierarchical  analysis  on  families  of  man-made  shapes  exhibiting  high  degrees  of  geometric  and  finer-scale  structural  variabilities.
2	A  similarity  measure  for  illustration  style.  This  paper  presents  a  method  for  measuring  the  similarity  in  style  between  two  pieces  of  vector  art,  independent  of  content.  Similarity  is  measured  by  the  differences  between  four  types  of  features:  color,  shading,  texture,  and  stroke.  Feature  weightings  are  learned  from  crowdsourced  experiments.  This  perceptual  similarity  enables  style-based  search.  Using  our  style-based  search  feature,  we  demonstrate  an  application  that  allows  users  to  create  stylistically-coherent  clip  art  mash-ups.
2	Remote  collaboration  in  ar  and  vr  using  virtual  replicas.  In  many  complex  tasks,  a  remote  subject-matter  expert  may  need  to  assist  a  local  user,  to  guide  their  actions  on  objects  in  the  local  user's  environment.  However,  effective  spatial  referencing  and  action  demonstration  in  a  remote  physical  environment  can  be  challenging.  We  demonstrate  an  approach  that  uses  Virtual  Reality  (VR)  or  Augmented  Reality  (AR)  for  the  remote  expert,  and  AR  for  the  local  user,  each  wearing  a  stereo  head-worn  display  (HWD).  Our  approach  allows  the  remote  expert  to  create  and  manipulate  virtual  replicas  of  physical  objects  in  the  local  environment  to  refer  to  parts  of  those  physical  objects  and  to  indicate  actions  on  them.  This  can  be  especially  useful  for  parts  that  are  occluded  or  difficult  to  access.  The  remote  expert  can  demonstrate  actions  in  3D  by  manipulating  virtual  replicas,  supported  by  constraints  and  annotations,  and  point  in  3D  to  portions  of  virtual  replicas  to  annotate  them.
2	Mages  3  0  tying  the  knot  of  medical  vr.  In  this  work,  we  present  MAGES  3.0,  a  novel  Virtual  Reality  (VR)-based  authoring  SDK  platform  for  accelerated  surgical  training  and  assessment.  The  MAGES  Software  Development  Kit  (SDK)  allows  code-free  prototyping  of  any  VR  psychomotor  simulation  of  medical  operations  by  medical  professionals,  who  urgently  need  a  tool  to  solve  the  issue  of  outdated  medical  training.  Our  platform  encapsulates  the  following  novel  algorithmic  techniques:  a)  collaborative  networking  layer  with  Geometric  Algebra  (GA)  interpolation  engine  b)  supervised  machine  learning  analytics  module  for  real-time  recommendations  and  user  profiling  c)  GA  deformable  cutting  and  tearing  algorithm  d)  on-the-go  configurable  soft  body  simulation  for  deformable  surfaces.
2	Left  arm  up  interactive  yoga  training  in  virtual  environment.  The  paper  describes  a  Yoga  training  system  that  is  built  based  on  motion  replication  technique  (MoRep),  including  hardware,  virtual  scenario  and  feedback  design.  The  motion  replication  technique  proposed  here  can  determine  the  similarity  between  Yoga  master  and  student's  postures  and  then  provide  feedback  on  the  incorrect  body  posture  of  the  student  through  multimodal  channels.  The  key  innovations  of  this  project  are  also  discussed.
2	Augmentation  of  road  surfaces  with  subsurface  utility  model  projections.  Subsurface  utility  work  planning  would  benefit  from  augmented  reality.  Unfortunately,  the  exact  pipe  location  is  rarely  known,  which  produces  unreliable  augmentations.  We  proposed  an  augmentation  technique  that  drapes  2D  pipe  maps  onto  the  road  surface  and  aligns  them  with  corresponding  features  in  the  physical  world  using  a  pre-captured  3D  mesh.  Resulting  augmentations  are  more  likely  to  be  displayed  at  the  true  pipe  locations.
2	Role  of  haptic  cues  in  motor  learning.  We  introduced  haptic  cues  to  the  serial  reaction  time  (SRT)  task  alongside  the  standard  visual  cues  to  explore  the  relative  contributions  of  haptic  and  visual  cues  to  motor  memory  and  perceptual  memory.  Motor  learning  is  a  complex  process  that  likely  depends  on  the  availability  of  haptic  sensory  feedback.  Motor  learning  is  also  often  an  implicit  process,  wherein  the  learner  is  not  even  aware  that  learning  is  taking  place.  The  SRT  task  was  devised  to  study  implicit  learning.  Reaction  times  (RTs)  are  reduced  with  practice  even  in  the  absence  of  explicit  learning  about  a  sequence  embedded  in  a  train  of  cues.  We  adopted  an  SRT  protocol  developed  by  Willingham  in  1999  to  determine  whether  haptic  cues  contribute  differently  than  visual  cues  to  the  balance  of  motor  and  perceptual  learning.  Experimental  results  involving  32  participants  showed  that  sequence  learning  occurs  implicitly  with  haptic  stimuli  to  much  the  same  extent  as  visual  stimuli.  Also,  it  was  revealed  that  the  dependence  on  motor  memory  (as  opposed  to  perceptual  memory)  was  greater  in  the  group  responding  to  haptic  cues  than  the  group  responding  to  visual  cues.  Comparing  the  dependence  on  motor  versus  perceptual  memory,  the  haptic  group  reached  marginal  significance  (p=.09),  whereas  the  visual  group  difference  was  not  significant  (p=.23).
2	Performance  evaluation  of  haptic  data  compression  methods  in  teleoperation  systems.  In  this  paper,  we  present  the  performance  evaluation  of  haptic  compression  methods  for  networked  teleoperation  systems  or  haptic  interfaces  in  virtual  environments.  Haptic  data,  which  include  position,  velocity,  and  force  data  exchanged  through  the  communication  channel,  are  considered  by  various  compression  methods  based  on  down-sampling.  We  introduce  the  operational  rate-distortion  performance  measure  that  evaluates  not  only  the  compression  ratio,  but  also  the  quality  of  reconstructed  haptic  data.  The  deadband  method,  also  known  as  a  perception-based  haptic  compression  method,  is  categorized  as  an  adaptive  down-sampling  method  and  compared  with  a  fixed  rate  down-sampling  method.  In  the  case  of  force  data  compression,  we  propose  the  modified  deadband  method,  which  adopts  a  force  predictor,  a  quantizer,  and  a  contact  force  detector.  Experiments  are  performed  by  using  a  haptic  device  incorporated  with  a  virtual  teleoperator.  The  performance  evaluation  and  comparison  of  the  fixed  rate  down-sampling,  deadband,  prediction-based  deadband,  and  modified  deadband  methods  are  provided.  The  results  show  that  the  proposed  method  achieves  improvement  in  the  compression  ratio  and  quality  measurement  compared  to  other  methods.
2	Fatigue  related  alterations  to  intra  muscular  coherence.  Oscillations  in  the  alpha  (8–12  Hz),  beta  (15–35  Hz)  and  gamma  (35–60  Hz)  frequency  bands  are  commonly  observed  in  recordings  from  the  primary  motor  cortex.  Coherence  analysis  based  on  motor  unit  spike  trains  is  commonly  used  to  quantify  the  degree  of  shared  cortical  input  and  the  common  modulation  of  motor  unit  discharge  rates  between  muscles.  In  this  study,  intra-muscular  coherence  is  used  to  investigate  the  alterations  in  the  neural  drive  to  the  First  Dorsal  Interosseous  muscle  directly  after  a  fatiguing  contraction  and  following  a  rest  period.  An  increase  in  coherence  was  observed  for  all  frequency  bands  examined,  which  was  statistically  significant  within  the  alpha  and  beta  frequency  ranges.  There  was  no  consistent  difference  between  the  coherence  estimates  obtained  pre-fatigue  and  those  reported  after  the  recovery  period.  The  increase  in  beta  band  coherence  post-fatigue  may  indicate  increased  levels  of  cortical  drive  to  the  motor  unit  pool.  Although  the  functional  significance  behind  the  increase  in  beta  frequency  coherence  is  unclear,  it  may  aid  in  the  coordination  of  muscle  activity  to  compensate  for  the  decline  in  the  force  generating  capacity  after  fatigue.
2	Dent  softness  illusion  in  mixed  reality  space  further  experiments  and  considerations.  When  humans  sense  the  hardness  of  real  objects,  their  perception  is  known  to  be  influenced  by  not  only  tactile  information  but  also  visual  information.  In  a  mixed-reality  (MR)  environment,  the  appearance  of  touchable  objects  can  be  changed  by  superimposing  a  computer-generated  image  onto  them  (MR  visual  stimulation).  In  our  previous  study,  we  first  superimposed  computer-generated  images  (CGI)  on  real  objects,  and  then,  applied  extreme  deformation  to  the  superimposed  images  while  the  participants  pressed  the  real  objects  with  their  fingers.  The  results  showed  that  humans  sense  hardness  differently  while  receiving  MR  visual  stimulation;  we  named  this  psychophysical  effect  the  dent-softness  illusion.  As  the  next  step,  in  this  study,  we  verify  whether  humans  perceive  hardness  differently  when  pressing  a  real  object  with  and  without  superimposed  animation  and  whether  the  dent-softness  illusion  occurs  in  different  parts  of  the  body  such  as  the  finger,  palm,  and  arm.
2	Ethical  concerns  of  the  use  of  virtual  avatars  in  consumer  entertainment.  Many  questions  still  remain  of  the  uses  both  positive  and  negative  of  virtual  avatars.  With  virtual  avatars  primed  for  consumer  entertainment,  the  effects  they  can  elicit  must  be  further  investigated  before  mainstream  adoption.  In  this  paper  we  present  an  overview  of  the  potential  risks  posed  by  virtual  avatars.  Followed  by  a  study  designed  to  investigate  these  risks,  and  finally  ethical  research  considerations  for  researchers  interested  in  conducting  research  with  virtual  avatars.
2	Remapped  physical  virtual  interfaces  with  bimanual  haptic  retargeting.  This  paper  proposes  a  novel  interface  for  virtual  reality  in  which  physical  interface  components  are  mapped  to  multiple  virtual  counterparts  using  haptic  retargeting  illusions.  This  gives  virtual  reality  interfaces  the  ability  to  have  correct  haptic  sensations  for  many  virtual  buttons  although  in  the  physical  space  there  is  only  one.  This  is  a  generic  system  that  can  be  applied  to  areas  including  design,  interaction  tasks,  product  prototype  development  and  interactive  games  in  virtual  reality.  The  system  presented  extends  existing  retargeting  algorithms  to  support  asymmetric  bimanual  interactions.  A  new  warp  technique,  called  interface  warp,  was  developed  to  support  remapped  virtual  reality  user  interfaces.  Through  an  experimental  user  study,  we  explore  the  effects  of  bimanual  retargeting  and  the  interface  warp  technique  on  task  response  time,  errors,  presence,  perceived  manipulation  compared  to  unimanual  (single  handed)  retargeting  and  other  existing  warp  techniques.  The  results  demonstrated  faster  task  response  time  and  less  errors  for  the  interface  warp  technique  and  shows  no  significant  effect  of  bimanual  interactions.
2	Vestibulohaptic  passive  stimulation  for  a  walking  sensation.  This  paper  describes  a  passive  stimulation  of  a  body  to  evoke  a  walking  sensation  using  a  vestibular  and  haptic  device  while  the  real  body  of  the  user  is  sitting.  It  imparts  a  pseudo  body  image  to  the  user  through  the  real  (physical)  body  of  the  user  as  a  part  of  the  virtual  reality  (VR)  display  system.  The  created  walking  sensation  was  evaluated  by  nine  factors  to  analyze  the  complex  nature  of  the  walking  sensation.
2	Image  complex  situation  understanding  an  immersive  concept  development.  This  paper  presents  an  immersive  Human-centric/built  virtual  work  cell  for  analyzing  complex  situations  dynamically.  This  environment  is  supported  by  a  custom  open  architecture,  is  composed  of  objects  of  complementary  nature  reflecting  the  level  of  Human  understanding.  Furthermore,  it  is  controlled  by  an  intuitive  3D  bimanual  gestural  interface  using  data  gloves.
2	The  effect  of  pitch  in  auditory  error  feedback  for  fitts  tasks  in  virtual  reality  training  systems.  Fitts'  law  and  the  associated  throughput  measure  characterize  user  pointing  performance  in  virtual  reality  (VR)  training  systems  and  simulators  well.  Yet,  pointing  performance  can  be  affected  by  the  feedback  users  receive  from  a  VR  application.  This  work  examines  the  effect  of  the  pitch  of  auditory  error  feedback  on  user  performance  in  a  Fitts'  task  through  a  distributed  experiment.  In  our  first  study,  we  used  middle-  and  high-frequency  sound  feedback  and  demonstrated  that  high-pitch  error  feedback  significantly  decreases  user  performance  in  terms  of  time  and  throughput.  In  the  second  study,  we  used  adaptive  sound  feedback,  where  we  increased  the  frequency  with  the  error  rate,  while  asking  subjects  to  execute  the  task  “as  fast/as  precise/as  fast  and  precise  as  possible”.  Results  showed  that  adaptive  sound  feedback  decreases  the  error  rate  for  “as  fast  as  possible”  task  execution  without  affecting  the  time.  The  results  can  be  used  to  enhance  and  design  various  VR  systems.
2	Synthesizing  personalized  training  programs  for  improving  driving  habits  via  virtual  reality.  The  recent  popularity  of  consumer-grade  virtual  reality  devices,  such  as  Oculus  Rift,  HTC  Vive,  and  Fove  virtual  reality  headset,  has  enabled  household  users  to  experience  highly  immersive  virtual  environments.  We  take  advantage  of  the  commercial  availability  of  these  devices  to  provide  a  novel  virtual  reality-based  driving  training  approach  designed  to  help  individuals  improve  their  driving  habits  in  common  scenarios.  Our  approach  first  identifies  improper  driving  habits  of  a  user  when  he  drives  in  a  virtual  city.  Then  it  synthesizes  a  pertinent  training  program  to  help  improve  the  users  driving  skills  based  on  the  discovered  improper  habits  of  the  user.  To  apply  our  approach,  a  user  first  goes  through  a  pre-evaluation  test  from  which  his  driving  habits  are  analyzed.  The  analysis  results  are  used  to  drive  optimization  for  synthesizing  a  training  program.  This  training  program  is  a  personalized  route  which  includes  different  traffic  events.  When  the  user  drives  along  this  route  via  a  driving  controller  and  an  eye-tracking  virtual  reality  headset,  the  traffic  events  he  encounters  will  help  him  to  improve  his  driving  habits.  To  validate  the  effectiveness  of  our  approach,  we  conducted  a  user  study  to  compare  our  virtual  reality-based  driving  training  with  other  training  methods.  The  user  study  results  show  that  the  participants  trained  by  our  approach  perform  better  on  average  than  those  trained  by  other  methods  in  terms  of  evaluation  score  and  response  time  and  their  improvement  is  more  persistent.
2	Test  case  generation  from  mutated  task  models.  This  paper  describes  an  approach  to  the  model-based  testing  of  graphical  user  interfaces  from  task  models.  Starting  from  a  task  model  of  the  system  under  test,  oracles  are  generated  whose  behaviour  is  compared  with  the  execution  of  the  running  system.  The  use  of  task  models  means  that  the  effort  of  producing  the  test  oracles  is  reduced.  It  does  also  mean,  however,  that  the  oracles  are  confined  to  the  set  of  expected  user  behaviours  for  the  system.  The  paper  focuses  on  solving  this  problem.  It  shows  how  task  mutations  can  be  generated  automatically,  enabling  a  broader  range  of  user  behaviours  to  be  considered.  A  tool,  based  on  a  classification  of  user  errors,  generates  these  mutations.  A  number  of  examples  illustrate  the  approach.
2	Myui  generating  accessible  user  interfaces  from  multimodal  design  patterns.  Adaptive  user  interfaces  can  make  technology  more  accessible.  Quite  a  number  of  conceptual  and  technical  approaches  have  been  proposed  for  adaptations  to  diverse  user  needs,  multiple  devices  or  multiple  environments.  Little  work,  however,  has  been  directed  at  integrating  all  the  essential  aspects  of  adaptive  user  interfaces  for  accessibility  in  one  system.  In  this  paper,  we  present  our  generic  MyUI  infrastructure  for  increased  accessibility  through  automatically  generated  adaptive  user  interfaces.  The  multimodal  design  patterns  repository  serves  as  the  basis  for  a  modular  approach  to  individualized  user  interfaces.  This  open  and  extensible  pattern  repository  makes  the  adaptation  rules  transparent  for  designers  and  developers  who  can  contribute  to  the  repository  by  sharing  their  knowledge  about  accessible  design.  The  adaptation  architecture  and  procedures  enable  user  interface  generation  and  dynamic  adaptations  during  run-time.  For  the  specification  of  an  abstract  user  interface  model,  a  novel  statecharts-based  notation  has  been  developed.  A  development  tool  supports  the  interactive  creation  of  the  graphical  user  interface  model.
2	Exploring  design  principles  of  task  elicitation  systems  for  unrestricted  natural  language  documents.  During  the  design  of  interactive  systems,  user  tasks  need  to  be  identified  within  natural  language  documents  (like  interview  transcripts,  support  messages  or  workshop  memos)  and  be  transformed  into  task  models.  This  time-consuming  and  error-prone  analysis  process  demands  for  automation,  however  corresponding  software  support  is  still  sparse.  This  paper  describes  a  Design  Science  Research  project,  which  explores  design  principles  for  a  system  aiming  to  close  this  gap.  To  evaluate  the  principles,  they  are  instantiated  in  an  innovative  artifact  called  REMINER  which  combines  Information  Retrieval,  Natural  Language  Processing  and  Annotation  technology.  The  artifact  can  be  used  to  semi-automatically  identify  user  tasks  from  unrestricted  natural  language  documents  and  to  organize  them  into  task  models.  Results  of  two  extensive  evaluations  of  the  artifact  show,  that  it  considerably  addresses  the  underlying  problem  areas  of  this  process.
2	Magic  ring  a  self  contained  gesture  input  device  on  finger.  Control  and  Communication  in  the  computing  environment  with  diverse  equipment  could  be  clumsy,  obtrusive,  and  frustrating  even  just  for  finding  the  right  input  device  or  getting  familiar  with  the  input  interface.  In  this  paper,  we  present  Magic  Ring  (MR),  a  finger  ring  shape  input  device  using  inertial  sensor  to  detect  the  subtle  finger  gestures  and  routine  daily  activities.  As  a  self-contained,  always-available,  and  hands-free  input  device,  we  believe  that  MR  will  enable  diverse  applications  in  the  intelligent  computing  environment.  In  this  demonstration,  we  will  show  a  prototype  design  of  MR  and  three  proof-of-concept  application  systems:  a  remote  controller  to  control  the  electrical  appliance  like  TV,  radio,  and  lamp  using  simple  finger  gestures;  a  natural  communication  tools  to  chat  using  the  simplified  sign  languages;  a  daily  activity  tracker  to  record  daily  activities  such  as  room  cleaning,  eating,  cooking,  writing  with  only  one  MR  on  the  index  finger.
2	Touching  the  void  introducing  cost  corpus  of  social  touch.  Touch  behavior  is  of  great  importance  during  social  interaction.  To  transfer  the  tactile  modality  from  interpersonal  interaction  to  other  areas  such  as  Human-Robot  Interaction  (HRI)  and  remote  communication  automatic  recognition  of  social  touch  is  necessary.  This  paper  introduces  CoST:  Corpus  of  Social  Touch,  a  collection  containing  7805  instances  of  14  different  social  touch  gestures.  The  gestures  were  performed  in  three  variations:  gentle,  normal  and  rough,  on  a  sensor  grid  wrapped  around  a  mannequin  arm.  Recognition  of  the  rough  variations  of  these  14  gesture  classes  using  Bayesian  classifiers  and  Support  Vector  Machines  (SVMs)  resulted  in  an  overall  accuracy  of  54%  and  53%,  respectively.  Furthermore,  this  paper  provides  more  insight  into  the  challenges  of  automatic  recognition  of  social  touch  gestures,  including  which  gestures  can  be  recognized  more  easily  and  which  are  more  difficult  to  recognize.
2	Facetube  predicting  personality  from  facial  expressions  of  emotion  in  online  conversational  video.  The  advances  in  automatic  facial  expression  recognition  make  possible  to  mine  and  characterize  large  amounts  of  data,  opening  a  wide  research  domain  on  behavioral  understanding.  In  this  paper,  we  leverage  the  use  of  a  state-of-the-art  facial  expression  recognition  technology  to  characterize  users  of  a  popular  type  of  online  social  video,  conversational  vlogs.  First,  we  propose  the  use  of  several  activity  cues  to  characterize  vloggers  based  on  frame-by-frame  estimates  of  facial  expressions  of  emotion.  Then,  we  present  results  for  the  task  of  automatically  predicting  vloggers'  personality  impressions  using  facial  expressions  and  the  Big-Five  traits.  Our  results  are  promising,  specially  for  the  case  of  the  Extraversion  impression,  and  in  addition  our  work  poses  interesting  questions  regarding  the  representation  of  multiple  natural  facial  expressions  occurring  in  conversational  video.
2	Multimodal  interaction  in  classrooms  implementation  of  tangibles  in  integrated  music  and  math  lessons.  This  demo  presents  the  multidisciplinary  development  of  the  enrichment  program  “Listening  to  Math:  Kids  compose  with  LEGO”,  that  aims  at  fostering  the  math  and  music  skills  of  gifted  primary  school  students.  In  this  program  LEGO  bricks  are  used  as  tangible  representation  of  music  notes.  The  so  called  LEGO-Table,  a  tabletop  computer,  translates  patterns  made  out  of  bricks  into  piano  melodies.  We  will  present  the  finished  program,  address  our  experiences  made  during  the  project,  and  we  will  also  give  some  insights  on  the  implementation  and  evaluation  of  the  program.  Functions  of  the  LEGO-Table  will  be  demonstrated  interactively  on  a  tablet.
2	Exploring  local  history  and  cultural  heritage  through  a  mobile  game.  We  describe  our  work  on  developing  a  mobile  game  that  utilizes  local  history  and  cultural  heritage  in  its  storyline  and  content.  The  game  is  depicted  in  the  town  of  Kemijarvi,  Northern  Finland,  in  the  1920's,  and  its  aim  is  to  encourage  visitors  and  locals  to  get  to  know  the  town's  history.  The  game  was  designed  with  input  from  history  experts  on  the  topic,  and  it  introduces  local  history  in  the  form  of  a  narrated  story,  where  the  user  has  to  visit  the  town's  historical  places  and  characters.  We  present  the  design  process,  the  game  concept  and  a  user  evaluation.  As  the  lessons  learnt  and  findings  from  our  design  process,  we  highlight  the  challenge  of  finding  the  balance  between  historical  accuracy  and  engaging  game  narrative,  and  the  importance  of  selecting  the  target  user  group  and  involving  them  when  refining  the  concept  and  user  interface  design.
2	Differences  between  smart  speakers  and  graphical  user  interfaces  for  music  search  considering  gender  effects.  The  ubiquitous  availability  of  smart  speakers  allows  hands-  and  eyes-free  interaction  through  Voice  User  Interfaces  (VUIs).  Control-ling  music  playback  is  the  most  commonly  used  feature  of  VUIs.  Previous  work  investigated  how  users  naturally  interact  with  smart  speakers  and  suggested  that  users'  gender  could  affect  the  devices'  usability.  The  usability  of  commercial  devices  compared  to  other  interactive  systems  and  the  effects  of  users'  gender  is,  however,  unclear.  Therefore,  we  conducted  a  study  with  20  participants  using  an  Amazon  Echo  Dot  and  a  laptop  device.  Participants  searched  for  artists  and  titles  using  a  Graphical  User  Interface  (GUI)  and  a  VUI.  In  addition,  they  performed  different  tasks  such  as  saving  a  song  in  a  playlist  or  adding  songs  into  a  queue.  The  analysis  revealed  that  the  VUI  provides  significantly  lower  usability  because  it  lacks  features,  requires  higher  mental  effort,  and  provides  confusing  answers.  In  contrast  to  previous  concerns,  the  analysis  did  not  reveal  significant  device×gender  effects.
2	Deep  learning  for  emotion  recognition  on  small  datasets  using  transfer  learning.  This  paper  presents  the  techniques  employed  in  our  team's  submissions  to  the  2015  Emotion  Recognition  in  the  Wild  contest,  for  the  sub-challenge  of  Static  Facial  Expression  Recognition  in  the  Wild.  The  objective  of  this  sub-challenge  is  to  classify  the  emotions  expressed  by  the  primary  human  subject  in  static  images  extracted  from  movies.  We  follow  a  transfer  learning  approach  for  deep  Convolutional  Neural  Network  (CNN)  architectures.  Starting  from  a  network  pre-trained  on  the  generic  ImageNet  dataset,  we  perform  supervised  fine-tuning  on  the  network  in  a  two-stage  process,  first  on  datasets  relevant  to  facial  expressions,  followed  by  the  contest's  dataset.  Experimental  results  show  that  this  cascading  fine-tuning  approach  achieves  better  results,  compared  to  a  single  stage  fine-tuning  with  the  combined  datasets.  Our  best  submission  exhibited  an  overall  accuracy  of  48.5%  in  the  validation  set  and  55.6%  in  the  test  set,  which  compares  favorably  to  the  respective  35.96%  and  39.13%  of  the  challenge  baseline.
2	Humans  and  smart  environments  a  novel  multimodal  interaction  approach.  In  this  paper,  we  describe  a  multimodal  approach  for  human-smart  environment  interaction.  The  input  interaction  is  based  on  three  modalities:  deictic  gestures,  symbolic  gestures  and  isolated-words.  The  deictic  gesture  is  interpreted  using  the  PTAMM  (Parallel  Tracking  and  Multiple  Mapping)  method  exploiting  a  camera  handheld  or  worn  on  the  user  arm.  The  PTAMM  algorithm  tracks  in  real-time  the  position  and  orientation  of  the  hand  in  the  environment.  This  information  is  used  to  point  real  or  virtual  objects,  previously  added  to  the  environment,  using  the  optical  camera  axis.  Symbolic  hand-gestures  and  isolated  voice  commands  are  recognized  and  used  to  interact  with  the  pointed  target.  Haptic  and  acoustic  feedbacks  are  provided  to  the  user  in  order  to  improve  the  quality  of  the  interaction.  A  complete  prototype  has  been  realized  and  a  first  usability  evaluation,  assessed  with  the  help  of  10  users  has  shown  positive  results.
2	What  have  we  learnt  about  mediterranean  catchment  hydrology  30  years  observing  hydrological  processes  in  the  vallcebre  research  catchments.  This  paper  presents  the  main  results  obtained  from  the  study  of  hydrological  processes  in  the  Vallcebre  Research  Catchments  since  1988.  Distributed  hydrometric  measurements,  environmental  tracers  and  hydrological  modelling  were  used  to  understand  Mediterranean  catchment  behaviour  and  to  provide  new  data  to  help  assess  the  global  change  effects  on  these  catchments'  water  resources.  Thirty  years  of  hydrological  processes  observation  in  the  Vallcebre  Research  Catchments  have  increased  understanding  not  only  of  their  hydrological  response,  but  also  of  the  main  hydrological  and  erosion  processes  characteristic  of  Mediterranean  mountain  catchments.  This  paper  briefly  summarises  the  main  results  obtained  since  1988  on  ecohydrological  processes,  hydrological  response,  runoff  generation  processes,  erosion  and  sediment  transport.  Some  of  the  main  findings  from  this  research  are  (i)  the  importance  of  temporal  variability  in  precipitation  to  determine  the  hydrological  processes;  (ii)  the  paramount  role  played  by  forest  cover  in  reducing  soil  water  content;  (iii)  the  marked  influence  of  antecedent  wetness  conditions  on  runoff  generation  that  determine  different  runoff  responses;  (v)  the  dominant  contribution  of  pre-existing  water  during  floods;  (vi)  the  importance  of  freezing-thawing  processes  in  badland  areas  on  erosion  and  the  role  of  summer  convective  storms  in  controlling  sediment  transport.
2	Using  interlocutor  modulated  attention  blstm  to  predict  personality  traits  in  small  group  interaction.  Small  group  interaction  occurs  often  in  workplace  and  education  settings.  Its  dynamic  progression  is  an  essential  factor  in  dictating  the  final  group  performance  outcomes.  The  personality  of  each  individual  within  the  group  is  reflected  in  his/her  interpersonal  behaviors  with  other  members  of  the  group  as  they  engage  in  these  task-oriented  interactions.  In  this  work,  we  propose  an  interlocutor-modulated  attention  BSLTM  (IM-aBLSTM)  architecture  that  models  an  individual's  vocal  behaviors  during  small  group  interactions  in  order  to  automatically  infer  his/her  personality  traits.  The  interlocutor-modulated  attention  mechanism  jointly  optimize  the  relevant  interpersonal  vocal  behaviors  of  other  members  of  group  during  interactions.  In  specifics,  we  evaluate  our  proposed  IM-aBLSTM  in  one  of  the  largest  small  group  interaction  database,  the  ELEA  corpus.  Our  framework  achieves  a  promising  unweighted  recall  accuracy  of  87.9%  in  ten  different  binary  personality  trait  prediction  tasks,  which  outperforms  the  best  results  previously  reported  on  the  same  database  by  10.4%  absolute.  Finally,  by  analyzing  the  interpersonal  vocal  behaviors  in  the  region  of  high  attention  weights,  we  observe  several  distinct  intra-  and  inter-personal  vocal  behavior  patterns  that  vary  as  a  function  of  personality  traits.
2	Starcraft  brood  war  strategy  powered  by  the  soma  swarm  algorithm.  This  participation  is  focused  on  artificial  intelligence  techniques  and  their  practical  use  in  computer  game.  The  aim  is  to  show  how  program  (based  on  evolutionary  algorithms)  can  replace  a  man  in  the  strategy  game  StarCraft:  Brood  War.  Implementation  used  in  our  experiments  use  classic  techniques  of  artificial  intelligence  environments,  as  well  as  unconventional  techniques,  such  as  evolutionary  computation.  An  artificial  player,  proposed  in  this  paper,  is  the  combination  of  the  decision  tree  and  evolutionary  algorithm  SOMA.  Whole  code  for  experiments  was  written  in  the  Java  programming  language.  The  proposed  code  provides  a  simple  implementation  of  the  artificial  computer  player  in  combination  with  slightly  modified  algorithm  SOMA.  This  provides  an  opportunity  for  effective,  coordinated  movement  of  combat  units  around  the  combat  landscape.  Research  reported  here  has  shown  potential  benefit  of  evolutionary  computation  in  the  field  of  strategy  games.
2	Spatio  temporal  patterns  of  meteorological  droughts  in  the  balearic  islands  spain.  The  Balearic  Islands  is  a  region  highly  prone  to  suffer  meteorological  and  hydrological  droughts,  mainly  due  to  the  high  intra-  and  inter-annual  variability  of  precipitation,  and  the  high  water  consumption  associated  to  summer  touristic  pressure.  In  this  work  we  aim  to  characterize  the  spatial  and  temporal  characteristics  of  meteorological  droughts  in  the  region  using  a  high-density  precipitation  database  for  the  three  main  islands.  The  “Standardized  Precipitation  Index  –  SPI”  was  calculated  for  each  of  the  50  precipitation  series  at  the  temporal  scale  of  12  months,  which  enabled  pinpointing  in  time  and  space  the  main  drought  episodes  from  1974  to  2014.  Moreover,  a  Principal  Component  Analysis  performed  over  the  50  12-month  SPI  series  allowed  us  to  identify  two  main  patterns  of  drought  variability,  with  a  clear  spatial  distribution.  The  occurrence  of  droughts  in  the  northern  sector  –  including  the  Tramuntana  mountain  range  (Majorca)  and  Minorca  Island  –  was  contrasted  and  independent  to  the  occurrence  of  droughts  in  the  southern  areas.  Similarly,  the  duration  of  droughts  shows  a  decreasing  trend  for  the  northern  region,  and  a  slight  increase  for  the  southern  stations.  Two  great  drought  episodes  were  identified,  with  a  contrasted  spatial  propagation:  The  one  in  1988-1991  affected  mainly  the  northern  region  whereas  in  the  south  moist  conditions  prevailed;  and  the  one  occurred  in  1999-2001,  which  affected  the  whole  region  but  started  its  propagation  from  the  south,  and  ended  with  extreme  drought  conditions  in  the  eastern  part  of  the  territory.  Results  obtained  highlight  the  need  for  studies  of  high  spatial  resolution  in  order  to  accurately  assess  the  spatiotemporal  patterns  of  meteorological  droughts.
2	Collaborative  exploration  with  a  micro  aerial  vehicle  a  novel  interaction  method  for  controlling  a  mav  with  a  hand  held  device.  In  order  to  collaboratively  explore  an  environment  with  a  Micro  Aerial  Vehicle  (MAV),  an  operator  needs  a  mobile  interface,  which  can  support  the  operator's  divided  attention.  To  this  end,  we  developed  the  Micro  Aerial  Vehicle  Exploration  of  an  Unknown  Environment  (MAV-VUE)  interface,  which  allows  operators  with  minimal  training  the  ability  to  remotely  explore  their  environment  with  a  MAV.  MAV-VUE  employs  a  concept  we  term  Perceived  First-Order  (PFO)  control,  which  allows  an  operator  to  effectively  "fly"  a  MAV  with  no  risk  to  the  vehicle.  PFO  control  utilizes  a  position  feedback  control  loop  to  fly  the  MAV  while  presenting  rate  feedback  to  the  operator.  A  usability  study  was  conducted  to  evaluate  MAV-VUE.  This  interface  was  connected  remotely  to  an  actual  MAV  to  explore  a  GPS-simulated  urban  environment.
2	Analytics  driven  dynamic  game  adaption  for  player  retention  in  scrabble.  This  paper  shows  how  game  analytics  can  be  used  in  conjunction  with  an  adaptive  system  in  order  to  increase  player  retention  at  the  level  of  individual  game  sessions  in  Scrabblesque,  a  Flash  game  based  on  the  popular  board  game  Scrabble.  In  this  paper,  we  use  game  analytic  knowledge  to  create  a  simplified  search  space  (called  the  game  analytic  space)  of  board  states.  We  then  target  a  distribution  of  game  analytic  states  that  are  predictive  of  players  playing  a  complete  game  session  of  Scrabblesque  in  order  to  increase  player  retention.  Our  adaptive  system  then  has  a  computer-controlled  AI  opponent  take  moves  that  will  help  realize  this  distribution  of  game  analytic  states  with  the  ultimate  goal  of  reducing  the  quitting  rate.  We  test  this  system  by  performing  a  user  study  in  which  we  compare  how  many  people  quit  playing  the  adaptive  version  of  Scrabblesque  early  and  how  many  people  quit  playing  a  nonadaptive  version  of  Scrabblesque  early.  We  also  compare  how  well  the  adaptive  version  of  Scrabblesque  was  able  to  influence  player  behavior  as  described  by  game  analytics.  Our  results  show  that  our  adaptive  system  is  able  to  produce  a  significant  reduction  in  the  quitting  rate  (p  =  0.03)  when  compared  to  the  non-adaptive  version.  In  addition,  the  adaptive  version  of  Scrabblesque  is  able  to  better  fit  a  target  distribution  of  game  analytic  states  when  compared  to  the  non-adaptive  version.
2	Influence  map  based  controllers  for  ms  pacman  and  the  ghosts.  Ms.  Pac-Man,  one  of  the  classic  arcade  games  has  recently  gained  attention  in  the  field  of  game  AI  through  the  yearly  competitions  of  various  kinds  held  at  e.g.  CIG.  We  have  implemented  an  Influence  Map-based  controller  for  Ms.  Pac-Man  as  well  as  for  the  ghosts  within  the  game.  We  show  that  it  is  able  to  handle  a  number  of  various  situations  through  the  interesting  behaviors  emerging  through  the  interplay  of  the  different  maps.  It  is  also  significantly  better  than  the  previous  implementations  based  on  similar  techniques,  such  as  potential  fields.
2	Applying  commitment  to  churn  and  remaining  players  lifetime  prediction.  In  this  paper,  we  present  a  Machine  Learning  approach  based  on  commitment  to  deal  with  two  risky  situations  over  game  usage  lifecycle:  the  prediction  of  churn  and  players  remaining  lifetime.  These  risky  situations  are  gauged  by  game  producers  to  try  to  maintain  the  players  motivated,  intervening  when  players  tend  to  leave.  The  problem  is  that  this  information  is  not  trivial  to  obtain  due  to  the  players‘  motivational  usage  linked  to  the  many  activities  available  in  the  game  content  (e.g.,  all  the  possible  actions  in  an  MMORPG).  To  deal  with  that,  we  proposed  the  use  of  the  commitment  concept  to  assign  the  engagement  of  a  player  to  a  game  based  on  the  usage  data.  The  main  aspect  of  the  proposed  approach  is  related  to  the  preprocessing  step,  where  measures  for  the  actions  related  to  commitment  were  computed,  generating  a  tendency  degree  for  each  one.  We  used  a  dataset  from  the  CIG  2017  Game  Data  Mining  competition  which  focused  on  the  same  risky  situations  as  a  comparison  baseline.  This  paper  approach  overcomes  the  competition’s  results  for  both  challenges.  All  experiments  followed  the  competition  rules.  These  experiments  show  that  looking  at  tendencies  on  commitment  is  an  option  to  gauge  players’  engagement  over  time.
2	Ladec  the  large  database  of  english  compounds.  The  Large  Database  of  English  Compounds  (LADEC)  consists  of  over  8,000  English  words  that  can  be  parsed  into  two  constituents  that  are  free  morphemes,  making  it  the  largest  existing  database  specifically  for  use  in  research  on  compound  words.  Both  monomorphemic  (e.g.,  wheel)  and  multimorphemic  (e.g.,  teacher)  constituents  were  used.  The  items  were  selected  from  a  range  of  sources,  including  CELEX,  the  English  Lexicon  Project,  the  British  Lexicon  Project,  the  British  National  Corpus,  and  Wordnet,  and  were  hand-coded  as  compounds  (e.g.,  snowball).  Participants  rated  each  compound  in  terms  of  how  predictable  its  meaning  is  from  its  parts,  as  well  as  the  extent  to  which  each  constituent  retains  its  meaning  in  the  compound.  In  addition,  we  obtained  linguistic  characteristics  that  might  influence  compound  processing  (e.g.,  frequency,  family  size,  and  bigram  frequency).  To  show  the  usefulness  of  the  database  in  investigating  compound  processing,  we  conducted  a  number  of  analyses  that  showed  that  compound  processing  is  consistently  affected  by  semantic  transparency,  as  well  as  by  many  of  the  other  variables  included  in  LADEC.  We  also  showed  that  the  effects  of  the  variables  associated  with  the  two  constituents  are  not  symmetric.  In  short,  LADEC  provides  the  opportunity  for  researchers  to  investigate  a  number  of  questions  about  compounds  that  have  not  been  possible  to  investigate  in  the  past,  due  to  the  lack  of  sufficiently  large  and  robust  datasets.  In  addition  to  directly  allowing  researchers  to  test  hypotheses  using  the  information  included  in  LADEC,  the  database  will  contribute  to  future  compound  research  by  allowing  better  stimulus  selection  and  matching.
2	Monte  carlo  based  statistical  power  analysis  for  mediation  models  methods  and  software.  The  existing  literature  on  statistical  power  analysis  for  mediation  models  often  assumes  data  normality  and  is  based  on  a  less  powerful  Sobel  test  instead  of  the  more  powerful  bootstrap  test.  This  study  proposes  to  estimate  statistical  power  to  detect  mediation  effects  on  the  basis  of  the  bootstrap  method  through  Monte  Carlo  simulation.  Nonnormal  data  with  excessive  skewness  and  kurtosis  are  allowed  in  the  proposed  method.  A  free  R  package  called  bmem  is  developed  to  conduct  the  power  analysis  discussed  in  this  study.  Four  examples,  including  a  simple  mediation  model,  a  multiple-mediator  model  with  a  latent  mediator,  a  multiple-group  mediation  model,  and  a  longitudinal  mediation  model,  are  provided  to  illustrate  the  proposed  method.
2	The  effectiveness  of  argumentation  in  tutorial  dialogues  with  an  intelligent  tutoring  system  for  genetic  risk  of  breast  cancer.  BRCA  Gist  is  an  Intelligent  Tutoring  System  that  helps  women  understand  issues  related  to  genetic  testing  and  breast  cancer  risk.  In  two  laboratory  experiments  and  a  field  experiment  with  community  and  web-based  samples,  an  avatar  asked  120  participants  to  produce  arguments  for  and  against  genetic  testing  for  breast  cancer  risk.  Two  raters  assessed  the  number  of  argumentation  elements  (claim,  reason,  backing,  etc.)  found  in  response  to  prompts  soliciting  arguments  for  and  against  genetic  testing  for  breast  cancer  risk  (IRR=.85).  When  asked  to  argue  for  genetic  testing,  53.3  %  failed  to  meet  the  minimum  operational  definition  of  making  an  argument,  a  claim  supported  by  one  or  more  reasons.  When  asked  to  argue  against  genetic  testing,  59.3  %  failed  to  do  so.  Of  those  who  failed  to  generate  arguments  most  simply  listed  disconnected  reasons.  However,  participants  who  provided  arguments  against  testing  (40.7  %)  performed  significantly  higher  on  a  posttest  of  declarative  knowledge.  In  each  study  we  found  positive  correlations  between  the  quality  of  arguments  against  genetic  testing  (i.e.,  number  of  argumentation  elements)  and  genetic  risk  categorization  scores.  Although  most  interactions  did  not  contain  two  or  more  argument  elements,  when  more  elements  of  arguments  were  included  in  the  argument  against  genetic  testing  interaction,  participants  had  greater  learning  outcomes.  Apparently,  many  participants  lack  skills  in  making  coherent  arguments.  These  results  suggest  an  association  between  argumentation  ability  (knowing  how  to  make  complex  arguments)  and  subsequent  learning.  Better  education  in  developing  arguments  may  be  necessary  for  people  to  learn  from  generating  arguments  within  Intelligent  Tutoring  Systems  and  other  settings.
2	A  generalized  longitudinal  mixture  irt  model  for  measuring  differential  growth  in  learning  environments.  This  article  describes  a  generalized  longitudinal  mixture  item  response  theory  (IRT)  model  that  allows  for  detecting  latent  group  differences  in  item  response  data  obtained  from  electronic  learning  (e-learning)  environments  or  other  learning  environments  that  result  in  large  numbers  of  items.  The  described  model  can  be  viewed  as  a  combination  of  a  longitudinal  Rasch  model,  a  mixture  Rasch  model,  and  a  random-item  IRT  model,  and  it  includes  some  features  of  the  explanatory  IRT  modeling  framework.  The  model  assumes  the  possible  presence  of  latent  classes  in  item  response  patterns,  due  to  initial  person-level  differences  before  learning  takes  place,  to  latent  class-specific  learning  trajectories,  or  to  a  combination  of  both.  Moreover,  it  allows  for  differential  item  functioning  over  the  classes.  A  Bayesian  model  estimation  procedure  is  described,  and  the  results  of  a  simulation  study  are  presented  that  indicate  that  the  parameters  are  recovered  well,  particularly  for  conditions  with  large  item  sample  sizes.  The  model  is  also  illustrated  with  an  empirical  sample  data  set  from  a  Web-based  e-learning  environment.
2	Comparing  patterns  of  component  loadings  principal  component  analysis  pca  versus  independent  component  analysis  ica  in  analyzing  multivariate  non  normal  data.  Principal  component  analysis  identifies  uncorrelated  components  from  correlated  variables,  and  a  few  of  these  uncorrelated  components  usually  account  for  most  of  the  information  in  the  input  variables.  Researchers  interpret  each  component  as  a  separate  entity  representing  a  latent  trait  or  profile  in  a  population.  However,  the  components  are  guaranteed  to  be  independent  and  uncorrelated  only  when  the  multivariate  normality  of  the  variables  is  assumed.  If  the  normality  assumption  does  not  hold,  components  are  guaranteed  to  be  uncorrelated,  but  not  independent.  If  the  independence  assumption  is  violated,  each  component  cannot  be  uniquely  interpreted  because  of  contamination  by  other  components.  Therefore,  in  the  present  study,  we  introduced  independent  component  analysis,  whose  components  are  uncorrelated  and  independent  even  when  the  multivariate  normality  assumption  is  violated,  and  each  component  carries  unique  information.
2	Using  task  effort  and  pupil  size  to  track  covert  shifts  of  visual  attention  independently  of  a  pupillary  light  reflex.  We  tested  the  link  between  pupil  size  and  the  task  effort  involved  in  covert  shifts  of  visual  attention.  The  goal  of  this  study  was  to  establish  pupil  size  as  a  marker  of  attentional  shifting  in  the  absence  of  luminance  manipulations.  In  three  experiments,  participants  evaluated  two  stimuli  that  were  presented  peripherally,  appearing  equidistant  from  and  on  opposite  sides  of  eye  fixation.  The  angle  between  eye  fixation  and  the  peripherally  presented  target  stimuli  varied  from  12.5°  to  42.5°.  The  evaluation  of  more  distant  stimuli  led  to  poorer  performance  than  did  the  evaluation  of  more  proximal  stimuli  throughout  our  study,  confirming  that  the  former  required  more  effort  than  the  latter.  In  addition,  in  Experiment  1  we  found  that  pupil  size  increased  with  increasing  angle  and  that  this  effect  could  not  be  reduced  to  the  operation  of  low-level  visual  processes  in  the  task.  In  Experiment  2  the  pupil  dilated  more  strongly  overall  when  participants  evaluated  the  target  stimuli,  which  required  shifts  of  attention,  than  when  they  merely  reported  on  the  target’s  presence  versus  absence.  Both  conditions  yielded  larger  pupils  for  more  distant  than  for  more  proximal  stimuli,  however.  In  Experiment  3,  we  manipulated  task  difficulty  more  directly,  by  changing  the  contrast  at  which  the  target  stimuli  were  presented.  We  replicated  the  results  from  Experiment  1  only  with  the  high-contrast  stimuli.  With  stimuli  of  low  contrast,  ceiling  effects  in  pupil  size  were  observed.  Our  data  show  that  the  link  between  task  effort  and  pupil  size  can  be  used  to  track  the  degree  to  which  an  observer  covertly  shifts  attention  to  or  detects  stimuli  in  peripheral  vision.
2	Mixwild  a  program  for  examining  the  effects  of  variance  and  slope  of  time  varying  variables  in  intensive  longitudinal  data.  The  use  of  intensive  sampling  methods,  such  as  ecological  momentary  assessment  (EMA),  is  increasingly  prominent  in  medical  research.  However,  inferences  from  such  data  are  often  limited  to  the  subject-specific  mean  of  the  outcome  and  between-subject  variance  (i.e.,  random  intercept),  despite  the  capability  to  examine  within-subject  variance  (i.e.,  random  scale)  and  associations  between  covariates  and  subject-specific  mean  (i.e.,  random  slope).  MixWILD  (Mixed  model  analysis  With  Intensive  Longitudinal  Data)  is  statistical  software  that  tests  the  effects  of  subject-level  parameters  (variance  and  slope)  of  time-varying  variables,  specifically  in  the  context  of  studies  using  intensive  sampling  methods,  such  as  ecological  momentary  assessment.  MixWILD  combines  estimation  of  a  stage  1  mixed-effects  location-scale  (MELS)  model,  including  estimation  of  the  subject-specific  random  effects,  with  a  subsequent  stage  2  linear  or  binary/ordinal  logistic  regression  in  which  values  sampled  from  each  subject's  random  effect  distributions  can  be  used  as  regressors  (and  then  the  results  are  aggregated  across  replications).  Computations  within  MixWILD  were  written  in  FORTRAN  and  use  maximum  likelihood  estimation,  utilizing  both  the  expectation-maximization  (EM)  algorithm  and  a  Newton-Raphson  solution.  The  mean  and  variance  of  each  individual's  random  effects  used  in  the  sampling  are  estimated  using  empirical  Bayes  equations.  This  manuscript  details  the  underlying  procedures  and  provides  examples  illustrating  standalone  usage  and  features  of  MixWILD  and  its  GUI.  MixWILD  is  generalizable  to  a  variety  of  data  collection  strategies  (i.e.,  EMA,  sensors)  as  a  robust  and  reproducible  method  to  test  predictors  of  variability  in  level  1  outcomes  and  the  associations  between  subject-level  parameters  (variances  and  slopes)  and  level  2  outcomes.
2	Design  and  control  of  an  air  jet  lump  display.  A  common  surgical  task  is  identifying  hard  lumps  embedded  in  soft  tissue.  During  open  procedures,  surgeons  can  localize  lumps  using  the  distributed  tactile  feedback  provided  through  manual  palpation  with  the  fingers.  Tactile  displays  developed  to  restore  tactile  feedback  for  both  traditional  and  robot-assisted  minimally  invasive  surgery  (RMIS)  are  designed  generically  to  provide  a  wide  range  of  tactile  sensations  to  the  finger,  and  as  such,  are  often  bulky  and  electro-mechanically  complex.  We  developed  a  novel  adjustable  aperture  air-jet  pneumatic  lump  display  that  directs  a  thin  stream  of  pressurized  air  through  an  aperture  onto  the  fingerpad.  The  display  is  designed  to  produce  the  sensation  of  a  lump  to  the  finger,  with  minimal  hardware  requirements.  It  has  two  degrees  of  freedom,  enabling  independent  control  of  pressure  and  aperture  size.  We  describe  the  design  of  the  display  and  demonstrate  the  process  through  which  the  output  of  the  display  can  be  controlled,  using  two  different  methods  for  adjusting  aperture  size.  The  output  of  the  pneumatic  air-jet  lump  display  is  quantitatively  measured  with  capacitive  tactile  sensor  arrays,  and  results  show  that  the  display  is  capable  of  changing  both  the  size  and  pressure  of  the  output.
2	Interactive  forces  caused  by  scanning  wavy  surfaces.  Well-designed  force  fields  can  cause  the  perception  of  surface  shapes  such  as  holes  or  bumps  on  a  flat  surface  without  involving  actual  geometric  displacements  during  exploratory  hand  motions.  To  establish  a  better  force  field  model,  we  observed  the  interactive  forces  caused  by  scanning  a  sinusoidal  wavy  surface  under  point  and  surface  contact  conditions.  Under  the  point  contact  condition,  the  acquired  forces  fit  well  into  the  model  of  earlier  studies  with  the  exception  of  kinetic  frictions.  When  the  surface  was  scanned  with  a  bare  finger  to  obtain  a  surface  contact  area,  the  observed  force  profiles  significantly  deformed  compared  with  those  of  the  earlier  static  point-contact  model.  Such  deformation  of  the  force  profile  can  be  explained  by  combining  Hertz  contact  theory  and  the  static  point-contact  model.  The  results  of  this  study  will  lead  to  the  development  of  an  interactive  force  model  for  surface  exploration,  that  is  expected  to  improve  force  rendering  algorithms.
2	Reducing  audio  stimulus  presentation  latencies  across  studies  laboratories  and  hardware  and  operating  system  configurations.  Using  differing  computer  platforms  and  audio  output  devices  to  deliver  audio  stimuli  often  introduces  (1)  substantial  variability  across  labs  and  (2)  variable  time  between  the  intended  and  actual  sound  delivery  (the  sound  onset  latency).  Fast,  accurate  audio  onset  latencies  are  particularly  important  when  audio  stimuli  need  to  be  delivered  precisely  as  part  of  studies  that  depend  on  accurate  timing  (e.g.,  electroencephalographic,  event-related  potential,  or  multimodal  studies),  or  in  multisite  studies  in  which  standardization  and  strict  control  over  the  computer  platforms  used  is  not  feasible.  This  research  describes  the  variability  introduced  by  using  differing  configurations  and  introduces  a  novel  approach  to  minimizing  audio  sound  latency  and  variability.  A  stimulus  presentation  and  latency  assessment  approach  is  presented  using  E-Prime  and  Chronos  (a  new  multifunction,  USB-based  data  presentation  and  collection  device).  The  present  approach  reliably  delivers  audio  stimuli  with  low  latencies  that  vary  by  ≤1  ms,  independent  of  hardware  and  Windows  operating  system  (OS)/driver  combinations.  The  Chronos  audio  subsystem  adopts  a  buffering,  aborting,  querying,  and  remixing  approach  to  the  delivery  of  audio,  to  achieve  a  consistent  1-ms  sound  onset  latency  for  single-sound  delivery,  and  precise  delivery  of  multiple  sounds  that  achieves  standard  deviations  of  1/10th  of  a  millisecond  without  the  use  of  advanced  scripting.  Chronos’s  sound  onset  latencies  are  small,  reliable,  and  consistent  across  systems.  Testing  of  standard  audio  delivery  devices  and  configurations  highlights  the  need  for  careful  attention  to  consistency  between  labs,  experiments,  and  multiple  study  sites  in  their  hardware  choices,  OS  selections,  and  adoption  of  audio  delivery  systems  designed  to  sidestep  the  audio  latency  variability  issue.
2	A  randomization  test  wrapper  for  synthesizing  single  case  experiments  using  multilevel  models  a  monte  carlo  simulation  study.  Multilevel  models  (MLMs)  have  been  proposed  in  single-case  research,  to  synthesize  data  from  a  group  of  cases  in  a  multiple-baseline  design  (MBD).  A  limitation  of  this  approach  is  that  MLMs  require  several  statistical  assumptions  that  are  often  violated  in  single-case  research.  In  this  article  we  propose  a  solution  to  this  limitation  by  presenting  a  randomization  test  (RT)  wrapper  for  MLMs  that  offers  a  nonparametric  way  to  evaluate  treatment  effects,  without  making  distributional  assumptions  or  an  assumption  of  random  sampling.  We  present  the  rationale  underlying  the  proposed  technique  and  validate  its  performance  (with  respect  to  Type  I  error  rate  and  power)  as  compared  to  parametric  statistical  inference  in  MLMs,  in  the  context  of  evaluating  the  average  treatment  effect  across  cases  in  an  MBD.  We  performed  a  simulation  study  that  manipulated  the  numbers  of  cases  and  of  observations  per  case  in  a  dataset,  the  data  variability  between  cases,  the  distributional  characteristics  of  the  data,  the  level  of  autocorrelation,  and  the  size  of  the  treatment  effect  in  the  data.  The  results  showed  that  the  power  of  the  RT  wrapper  is  superior  to  the  power  of  parametric  tests  based  on  F  distributions  for  MBDs  with  fewer  than  five  cases,  and  that  the  Type  I  error  rate  of  the  RT  wrapper  is  controlled  for  bimodal  data,  whereas  this  is  not  the  case  for  traditional  MLMs.
2	Decision  moving  window  using  interactive  eye  tracking  to  examine  decision  processes.  It  has  become  increasingly  more  important  for  researchers  to  better  capture  the  complexities  of  making  a  decision.  To  better  measure  cognitive  processes  such  as  attention  during  decision  making,  we  introduce  a  new  methodology:  the  decision  moving  window,  which  capitalizes  on  both  mouse-tracing  and  eye-tracking  methods.  We  demonstrate  the  effectiveness  of  this  methodology  in  a  probabilistic  inferential  decision  task  where  we  reliably  measure  attentional  processing  during  decision  making  while  allowing  the  person  to  determine  how  information  is  acquired.  We  outline  the  advantages  of  this  methodological  paradigm  and  how  it  can  advance  both  decision-making  research  and  the  development  of  new  metrics  to  capture  cognitive  processes  in  complex  tasks.
2	Conceptualizing  syntactic  categories  as  semantic  categories  unifying  part  of  speech  identification  and  semantics  using  co  occurrence  vector  averaging.  Co-occurrence  models  have  been  of  considerable  interest  to  psychologists  because  they  are  built  on  very  simple  functionality.  This  is  particularly  clear  in  the  case  of  prediction  models,  such  as  the  continuous  skip-gram  model  introduced  in  Mikolov,  Chen,  Corrado,  and  Dean  (2013),  because  these  models  depend  on  functionality  closely  related  to  the  simple  Rescorla–Wagner  model  of  discriminant  learning  in  nonhuman  animals  (Rescorla  &  Wagner,  1972),  which  has  a  rich  history  within  psychology  as  a  model  of  many  animal  learning  processes.  We  replicate  and  extend  earlier  work  showing  that  it  is  possible  to  extract  accurate  information  about  syntactic  category  and  morphological  family  membership  directly  from  patterns  of  word  co-occurrence,  and  provide  evidence  from  four  experiments  showing  that  this  information  predicts  human  reaction  times  and  accuracy  for  class  membership  decisions.
2	Communicative  and  noncommunicative  point  light  actions  featuring  high  resolution  representation  of  the  hands  and  fingers.  We  describe  the  creation  of  a  set  of  point-light  movies  depicting  43  communicative  gestures  and  43  noncommunicative,  pantomimed  actions.  These  actions  were  recorded  using  a  motion  capture  system  that  is  worn  on  the  body  and  provides  accurate  capture  of  the  positions  and  movements  of  individual  fingers.  The  movies  created  thus  include  point-lights  on  the  fingers,  allowing  for  representation  of  actions  and  gestures  that  would  not  be  possible  with  a  conventional,  line-of-sight-based  motion  capture  system.  These  videos  would  be  suitable  for  use  in  cognitive  and  cognitive  neuroscientific  studies  of  biological  motion  and  gesture  perception.  Each  video  is  described,  along  with  an  H  statistic  indicating  the  consistency  of  the  descriptive  labels  that  20  observers  gave  to  the  actions.  We  also  produced  a  scrambled  version  of  each  movie,  in  which  the  starting  position  of  each  point  was  randomized  but  its  local  motion  vector  was  preserved.  These  scrambled  movies  would  be  suitable  for  use  as  control  stimuli  in  experimental  studies.  As  supplementary  materials,  we  provide  QuickTime  movie  files  of  each  action,  along  with  text  files  specifying  the  three-dimensional  coordinates  of  each  point-light  in  each  frame  of  each  movie.
2	How  honest  are  the  signals  a  protocol  for  validating  wearable  sensors.  There  is  growing  interest  among  organizational  researchers  in  tapping  into  alternative  sources  of  data  beyond  self-reports  to  provide  a  new  avenue  for  measuring  behavioral  constructs.  Use  of  alternative  data  sources  such  as  wearable  sensors  is  necessary  for  developing  theory  and  enhancing  organizational  practice.  Although  wearable  sensors  are  now  commercially  available,  the  veracity  of  the  data  they  capture  is  largely  unknown  and  mostly  based  on  manufacturers’  claims.  The  goal  of  this  research  is  to  test  the  validity  and  reliability  of  data  captured  by  one  such  wearable  badge  (by  Humanyze)  in  the  context  of  structured  meetings  where  all  individuals  wear  a  badge  for  the  duration  of  the  encounter.  We  developed  a  series  of  studies,  each  targeting  a  specific  sensor  of  this  badge  that  is  relevant  for  structured  meetings,  and  we  make  specific  recommendations  for  badge  data  usage  based  on  our  validation  results.  We  have  incorporated  the  insights  from  our  studies  on  a  website  that  researchers  can  use  to  conduct  validation  tests  for  their  badges,  upload  their  data,  and  assess  the  validity  of  the  data.  We  discuss  this  website  in  the  corresponding  studies.
2	An  adaptive  algorithm  for  fast  and  reliable  online  saccade  detection.  To  investigate  visual  perception  around  the  time  of  eye  movements,  vision  scientists  manipulate  stimuli  contingent  upon  the  onset  of  a  saccade.  For  these  experimental  paradigms,  timing  is  especially  crucial,  because  saccade  offset  imposes  a  deadline  on  the  display  change.  Although  efficient  online  saccade  detection  can  greatly  improve  timing,  most  algorithms  rely  on  spatial-boundary  techniques  or  absolute-velocity  thresholds,  which  both  suffer  from  weaknesses:  late  detections  and  false  alarms,  respectively.  We  propose  an  adaptive,  velocity-based  algorithm  for  online  saccade  detection  that  surpasses  both  standard  techniques  in  speed  and  accuracy  and  allows  the  user  to  freely  define  the  detection  criteria.  Inspired  by  the  Engbert–Kliegl  algorithm  for  microsaccade  detection,  our  algorithm  computes  two-dimensional  velocity  thresholds  from  variance  in  the  preceding  fixation  samples,  while  compensating  for  noisy  or  missing  data  samples.  An  optional  direction  criterion  limits  detection  to  the  instructed  saccade  direction,  further  increasing  robustness.  We  validated  the  algorithm  by  simulating  its  performance  on  a  large  saccade  dataset  and  found  that  high  detection  accuracy  (false-alarm  rates  of  <  1%)  could  be  achieved  with  detection  latencies  of  only  3  ms.  High  accuracy  was  maintained  even  under  simulated  high-noise  conditions.  To  demonstrate  that  purely  intrasaccadic  presentations  are  technically  feasible,  we  devised  an  experimental  test  in  which  a  Gabor  patch  drifted  at  saccadic  peak  velocities.  Whereas  this  stimulus  was  invisible  when  presented  during  fixation,  observers  reliably  detected  it  during  saccades.  Photodiode  measurements  verified  that—including  all  system  delays—the  stimuli  were  physically  displayed  on  average  20  ms  after  saccade  onset.  Thus,  the  proposed  algorithm  provides  a  valuable  tool  for  gaze-contingent  paradigms.
2	Server  based  rendering  of  large  3d  scenes  for  mobile  devices  using  g  buffer  cube  maps.  Large  virtual  3D  scenes  play  a  major  role  in  growing  number  of  applications,  systems,  and  technologies  to  effectively  communicate  complex  spatial  information.  Their  web-based  provision,  in  particular  on  mobile  devices,  represents  a  key  challenge  for  system  and  application  development.  In  contrast  to  approaches  based  on  streaming  3D  scene  data  to  clients,  our  approach  splits  3D  rendering  into  two  processes:  A  server  process  is  responsible  for  real-time  rendering  of  virtual  panoramas,  represented  by  G-buffer  cube  maps,  for  a  requested  camera  setting.  The  client  reconstruction  process  uses  these  cube  maps  to  reconstruct  the  3D  scene  and  allows  users  to  operate  on  and  interact  with  that  representation.  The  key  properties  of  this  approach  include  that  (a)  the  complexity  of  transmitted  data  not  depend  on  the  3D  scene's  complexity;  (b)  3D  rendering  can  take  place  within  a  controlled  and  a-priori  known  server  environment;  (c)  crucial  3D  model  data  never  leaves  the  server  environment;  and  (d)  the  clients  can  flexibly  extend  the  3D  cube  map  viewer  by  adding  both  local  3D  models  and  specialized  3D  operations.
2	G  sparks  glanceable  sparklines  on  smartwatches.  Optimizing  the  use  of  a  small  display  while  presenting  graphic  data  such  as  line  charts  is  challenging.  To  tackle  this,  we  propose  GSparks,  a  compact  visual  representation  of  glanceable  line  graphs  for  smartwatches.  Our  exploration  primarily  considered  the  suitable  compression  axes  for  time-series  charts.  In  a  first  study  we  examine  the  optimal  line-graph  compression  approach  without  compromising  perceptual  metrics,  such  as  slope  or  height  detections.  We  evaluated  compressions  of  line  segments,  the  elementary  unit  of  a  line  graph,  along  the  x-axis,  y-axis,  and  xyaxes.  Contrary  to  intuition,  we  find  that  condensing  graphs  yield  more  accurate  reading  of  height  estimations  than  non-compressed  graphs,  but  only  when  these  are  compressed  along  the  x-axis.  Building  from  this  result,  we  study  the  effect  of  an  x-axis  compression  on  users'  ability  to  perform  "glanceable"  analytic  tasks  with  actual  data.  Glanceable  tasks  include  quick  perceptual  judgements  of  graph  properties.  Using  bio-metric  data  (heart  rate),  we  find  that  shrinking  a  line  graph  to  the  point  of  representing  one  data  sample  per  pixel  does  not  compromise  legibility.  As  expected,  such  type  of  compression  also  has  the  effect  of  minimizing  the  needed  amount  of  flicking  to  interact  with  such  graphs.  From  our  results,  we  offer  guidelines  to  application  designers  needing  to  integrate  line  charts  into  smartwatch  apps.
2	Selecting  and  sliding  hidden  objects  in  3d  desktop  environments.  Selecting  and  positioning  objects  in  3D  space  are  fundamental  tasks  in  3D  user  interfaces.  We  present  two  new  techniques  to  improve  3D  selection  and  positioning.  We  first  augment  3D  user  interfaces  with  a  new  technique  that  enables  users  to  select  objects  that  are  hidden  from  the  current  viewpoint.  This  layer-based  technique  for  selecting  hidden  objects  works  for  arbitrary  objects  and  scenes.  We  then  also  extend  a  mouse-based  sliding  technique  to  work  even  if  the  manipulated  object  is  hidden  behind  other  objects,  by  making  the  manipulated  object  always  fully  visible  through  a  transparency  mask  during  drag-and-drop  positioning.  Our  user  study  shows  that  with  the  new  techniques,  users  can  easily  select  hidden  objects  and  that  sliding  with  transparency  performs  faster  than  the  common  3D  widgets  technique.
2	Geodesic  voxel  binding  for  production  character  meshes.  We  propose  a  fully  automatic  method  for  specifying  influence  weights  for  closed-form  skinning  methods,  such  as  linear  blend  skinning.  Our  method  is  designed  to  work  with  production  meshes  that  may  contain  non-manifold  geometry,  be  non-watertight,  have  intersecting  triangles,  or  be  comprise  of  multiple  connected  components.  Starting  from  a  character  rest  pose  mesh  and  skeleton  hierarchy,  we  first  voxelize  the  input  geometry.  The  resulting  voxelization  is  then  used  to  calculate  binding  weights,  based  on  the  geodesic  distance  between  each  voxel  lying  on  a  skeleton  "bone"  and  all  non-exterior  voxels.  This  yields  smooth  weights  at  interactive  rates,  without  time-constants,  iteration  parameters,  or  costly  optimization  at  bind  or  pose  time.  By  decoupling  weight  assignment  from  distance  computation  we  make  it  possible  to  modify  weights  interactively,  at  pose  time,  without  additional  pre-processing  or  computation.  This  allows  artists  to  assess  impact  of  weight  selection  in  the  context  in  which  they  are  used.
2	The  design  space  of  nonvisual  word  completion.  Word  completion  interfaces  are  ubiquitously  available  in  mobile  virtual  keyboards;  however,  there  is  no  prior  research  on  how  to  design  these  interfaces  for  screen  reader  users.  In  addressing  this,  we  propose  a  design  space  for  nonvisual  representation  of  word  completions.  The  design  space  covers  seven  categories  aiming  to  identify  challenges  and  opportunities  for  interaction  design  in  an  unexplored  research  topic.  It  is  intended  to  guide  the  design  of  novel  interaction  techniques,  serving  as  a  framework  for  researchers  and  practitioners  working  on  nonvisual  word  completion.  To  demonstrate  its  potential,  we  engaged  blind  users  in  an  exploration  of  the  design  space,  to  create  their  own  bespoke  word  completion  solutions.  Through  this  study  we  found  that  users  create  alternative  interfaces  that  extended  current  screen  readers'  capabilities.  Resulting  interfaces  are  less  conservative  than  mainstream  solutions  on  notification  frequency  and  cardinality.  Customization  decisions  were  based  on  perceived  benefits/costs  and  varied  depending  on  multiple  factors  such  as  users'  perceived  prediction  accuracy,  potential  keystroke  gains,  and  situational  restrictions.
2	Chimelight  augmenting  instruments  in  interactive  music  therapy  for  children  with  neurodevelopmental  disorders.  In  this  paper,  we  propose  a  new  mobile  system  to  support  therapists  for  teaching  and  tracking  socio-communicative  behaviors  in  children  with  neurodevelopmental  disorders  during  music  therapy  sessions.  The  CHIMELIGHT  system  was  designed  to  deal  with  the  current  issues  in  conventional  therapies,  such  as  the  difficulty  in  both  evaluating  the  performance  and  maintaining  engagement  of  these  children  during  therapeutic  activities.  The  system  evaluated  movements  made  by  a  child  with  neurodevelopmental  disorders  playing  a  musical  instrument  while  delivering  contingent  visual  feedback  based  on  real-time  motion  analysis.  A  set  of  metrics  was  implemented  to  evaluate  the  performance  during  the  therapy  activity  and  quantify  specific  target  behaviors.  An  evaluation  study  performed  during  music  therapy  group  sessions  showed  that  the  CHIMELIGHT-delivered  visual  feedback  increased  the  engagement  of  children  in  the  activity  and  decreased  targeted  negative  behaviors.  In  some  participants,  we  observed  potential  changes  in  their  positive  behaviors.  Interviews  and  questionnaires  provided  to  therapists  showed  that  the  developed  system  was  effective  for  supporting  evidence-based  music  therapy.  Accordingly,  our  research  enables  new  methods  for  both  interactive  therapy  and  mediation  of  the  interaction  between  therapists  and  children  with  neurodevelopmental  disorders.
2	Visual  complexity  player  experience  performance  and  physical  exertion  in  motion  based  games  for  older  adults.  Motion-based  video  games  can  have  a  variety  of  benefits  for  the  players  and  are  increasingly  applied  in  physical  therapy,  rehabilitation  and  prevention  for  older  adults.  However,  little  is  known  about  how  this  audience  experiences  playing  such  games,  how  the  player  experience  affects  the  way  older  adults  interact  with  motion-based  games,  and  how  this  can  relate  to  therapy  goals.  In  our  work,  we  decompose  the  player  experience  of  older  adults  engaging  with  motion-based  games,  focusing  on  the  effects  of  manipulations  of  the  game  representation  through  the  visual  channel  (visual  complexity),  since  it  is  the  primary  interaction  modality  of  most  games  and  since  vision  impairments  are  common  amongst  older  adults.  We  examine  the  effects  of  different  levels  of  visual  complexity  on  player  experience,  performance,  and  exertion  in  a  study  with  fifteen  participants.  Our  results  show  that  visual  complexity  affects  the  way  games  are  perceived  in  two  ways:  First,  while  older  adults  do  have  preferences  in  terms  of  visual  complexity  of  video  games,  notable  effects  were  only  measurable  following  drastic  variations.  Second,  perceived  exertion  shifts  depending  on  the  degree  of  visual  complexity.  These  findings  can  help  inform  the  design  of  motion-based  games  for  therapy  and  rehabilitation  for  older  adults.
2	But  i  don  t  want  need  a  power  wheelchair  toward  accessible  power  assistance  for  manual  wheelchairs.  Power  assist  devices  help  manual  wheelchair  users  to  propel  their  wheelchair  thus  increasing  their  independence  and  reducing  the  risk  of  upper  limb  injuries  due  to  excessive  use.  These  benefits  can  be  invaluable  for  people  that  already  have  upper  limb  joint  pain  and  reduced  muscular  strength.  However,  it  is  not  clear  if  the  way  that  assistance  is  provided  by  such  devices  is  what  manual  wheelchair  users  need  and  expect.  12  manual  wheelchair  users  were  interviewed  to  understand:  the  situations  in  which  they  find  it  difficult  to  propel  their  wheelchairs;  situations  they  considered  paramount  to  have  power  assistance;  their  experience  or  knowledge  of  power  assist  devices;  and  likes  and  dislikes  of  commercially  available  power  assist  devices.  Finally,  they  were  asked  to  comment  on  their  ideal  form  factor  of  a  power  assist  device.  Users  have  suggested  improvements  of  the  devices'  accessibility  and  visualized  new  ways  in  which  they  could  interact  with  the  technology.  These  interactions  involve  "chairable"  devices  independent  from,  but  not  excluding,  wearable  devices  and  mobile  applications.  We  have  identified  the  need  of  monitoring  emotions  and  the  need  for  designing  an  open  source  do-it-yourself  wheelchair  propelling  assistance  device  which  we  believe  is  required  equally  in  developed  and  in  developing  countries.
2	Virtual  reality  for  skin  exploration.  Background:  We  have  developed  and  set  up  the  SkinExplorer™  platform,  a  new  tool  to  exploit  and  rebuild  serial  confocal  images  into  3D  numerical  models  [1,  2].  The  acquisitions  using  confocal  microscopy  allow  visualizing  cutaneous  components  as  elastic  fibers,  melanocytes  and  keratinocytes  etc...  These  diversified  sources  of  data  participate  to  create  numerical  3D  volume  models  with  high  quality  of  visualization.      Objective:  To  create  a  Virtual  Reality  (VR)  experience,  to  communicate  and  change  the  perception  of  skin  structures  by  virtualization  mode.      Methods:  The  use  of  ART  TRACKPACK  system  and  ART  SMARTTRACK  device  allow  us  to  valorize  new  sensory  images  for  the  volumetric  rendering  of  the  3D  skin  models.      Results:  We  increase  the  perception  and  the  understanding  of  skin  components  organization.      Conclusion:  The  SkinExplorer™  platform  seems  to  be  a  promising  system  for  exploring  the  skin.
2	Anthropometry  of  external  auditory  canal  by  non  contactable  measurement.  Abstract      Human  ear  canals  cannot  be  measured  directly  with  existing  general  measurement  tools.  Furthermore,  general  non-contact  optical  methods  can  only  conduct  simple  peripheral  measurements  of  the  auricle  and  cannot  obtain  the  internal  ear  canal  shape-related  measurement  data.  Therefore,  this  study  uses  the  computed  tomography  (CT)  technology  to  measure  the  geometric  shape  of  the  ear  canal  and  the  shape  of  the  ear  canal  using  a  non-invasive  method,  and  to  complete  the  anthropometry  of  external  auditory  canal.  The  results  of  the  study  show  that  the  average  height  and  width  of  ear  canal  openings,  and  the  average  depth  of  the  first  bend  for  men  are  generally  longer,  wider  and  deeper  than  those  for  women.  In  addition,  the  difference  between  the  height  and  width  of  the  ear  canal  opening  is  about  40%  (p
2	An  experimental  study  on  the  ergonomics  indices  of  partial  pressure  suits.  Abstract      Partial  pressure  suits  (PPSs)  are  used  under  high  altitude,  low-pressure  conditions  to  protect  the  pilots.  However,  the  suit  often  limits  pilot’s  mobility  and  work  efficiency.  The  lack  of  ergonomic  data  on  the  effects  of  PPSs  on  mobility  and  performance  creates  difficulties  for  human  factor  engineers  and  cockpit  layout  specialists.  This  study  investigated  the  effects  of  PPSs  on  different  ergonomic  mobility  and  performance  indices  in  order  to  evaluate  the  suit’s  impact  on  pilot’s  body  mobility  and  work  efficiency.  Three  types  of  ergonomics  indices  were  studied:  the  manipulative  mission,  operational  reach  and  operational  strength.  Research  results  indicated  that  a  PPS  significantly  affects  the  mobility  and  operational  performance  of  the  wearers.  The  results  may  provide  mission  planners  and  human  factors  engineers  with  better  insight  into  the  understanding  of  pilots’  operational  function,  mobility  and  strength  capabilities  when  wearing  PPS.
2	Causes  consequences  and  countermeasures  to  driver  fatigue  in  the  rail  industry  the  train  driver  perspective.  Fatigue  is  an  important  workplace  risk  management  issue.  Within  the  rail  industry,  the  passing  of  a  stop  signal  (signal  passed  at  danger;  SPAD)  is  considered  to  be  one  of  the  most  major  safety  breaches  which  can  occur.  Train  drivers  are  very  aware  of  the  negative  consequences  associated  with  a  SPAD.  Therefore,  SPADs  provide  a  practical  and  applied  safety  relevant  context  within  which  to  structure  a  discussion  on  fatigue.  Focus  groups  discussing  contributing  factors  to  SPADs  were  undertaken  at  eight  passenger  rail  organisations  across  Australia  and  New  Zealand  (n  =  28  drivers).  Data  relating  to  fatigue  was  extracted  and  inductively  analysed  identifying  three  themes:  causes,  consequences,  and  countermeasures  (to  fatigue).  Drivers  experienced  negative  consequences  of  fatigue,  despite  existing  countermeasures  to  mitigate  it.  Organisational  culture  was  a  barrier  to  effective  fatigue  management.  A  fatigue  assessment  tool  consistently  informed  rostering,  however,  shift  swapping  was  commonplace  and  often  unregulated,  reducing  any  potential  positive  impact.  In  discussing  fatigue  countermeasure  strategies,  drivers  talked  interchangeably  about  mitigating  task  related  fatigue  (e.g.  increasing  cognitive  load)  and  sleepiness  (e.g.  caffeine).  Ensuring  the  concepts  of  fatigue  and  sleepiness  are  properly  understood  has  the  potential  to  maximise  safety.
2	A  practical  guidance  for  assessments  of  sedentary  behavior  at  work  a  perosh  initiative.  Abstract      Sedentary  behavior  is  defined  as  sitting  or  lying  with  low  energy  expenditure.  Humans  in  industrialized  societies  spend  an  increasing  amount  of  time  in  sedentary  behaviors  every  day.  This  has  been  associated  with  detrimental  health  outcomes.  Despite  a  growing  interest  in  the  health  effects  of  sedentary  behavior  at  work,  associations  remain  unclear,  plausibly  due  to  poor  and  diverse  methods  for  assessing  sedentary  behavior.  Thus,  good  practice  guidance  for  researchers  and  practitioners  on  how  to  assess  occupational  sedentary  behavior  are  needed.    The  aim  of  this  paper  is  to  provide  a  practical  guidance  for  practitioners  and  researchers  on  how  to  assess  occupational  sedentary  behavior.    Ambulatory  systems  for  use  in  field  applications  (wearables)  are  a  promising  approach  for  sedentary  behavior  assessment.  Many  different  small-size  consumer  wearables,  with  long  battery  life  and  high  data  storage  capacity  are  commercially  available  today.  However,  no  stand-alone  commercial  system  is  able  to  assess  sedentary  behavior  in  accordance  with  its  definition.  The  present  paper  offers  decision  support  for  practitioners  and  researchers  in  selecting  wearables  and  data  collection  strategies  for  their  purpose  of  study  on  sedentary  behavior.    Valid  and  reliable  assessment  of  occupational  sedentary  behavior  is  currently  not  easy.  Several  aspects  need  to  be  considered  in  the  decision  process  on  how  to  assess  sedentary  behavior.  There  is  a  need  for  development  of  a  cheap  and  easily  useable  wearable  for  assessment  of  occupational  sedentary  behavior  by  researchers  and  practitioners.
2	The  effects  of  protective  footwear  on  spine  control  and  lifting  mechanics.  Abstract  Manual  materials  handling  is  often  performed  in  hazardous  environments  where  protective  footwear  must  be  worn;  however,  workers  can  wear  different  types  of  footwear  depending  on  the  hazards  present.  Therefore,  the  goal  of  this  study  was  to  investigate  how  three-dimensional  lifting  mechanics  and  trunk  local  dynamic  stability  are  affected  by  different  types  of  protective  footwear  (i.e.  steel-toed  shoes  (unlaced  boot),  steel-toed  boots  (work  boot),  and  steel-toed  boots  with  a  metatarsal  guard  (MET)).  Twelve  males  and  twelve  females  performed  a  repetitive  lifting  task  at  10%  of  their  maximum  lifting  effort,  under  three  randomized  footwear  conditions.  Footwear  type  influenced  ankle  range  of  motion  (ROM).  The  work  boot  condition  reduced  ankle  sagittal  ROM  (p = 0.007)  and  the  MET  condition  reduced  ankle  ROM  in  the  sagittal  (p = 0.004),  frontal  (p = 0.001)  and  transverse  (p = 0.003)  planes.  Despite  these  differences  at  the  ankle,  no  other  changes  in  participant  lifting  mechanics  were  observed.
2	Reliability  of  a  battery  of  tests  for  functional  evaluation  of  trunk  exoskeletons.  Abstract  Recently,  several  spinal  exoskeletons  were  developed  with  the  aim  to  assist  occupational  tasks  such  as  load-handling  and  work  in  prolonged  static  postures.  While  the  biomechanical  effects  of  such  devices  has  been  well  investigated,  only  limited  feedback  to  the  developers  is  usually  provided  regarding  the  subjective  perceptions  of  the  end-users.  The  aim  of  this  study  was  to  present  a  novel  battery  of  tests,  designed  to  assess  functional  performance  and  subjective  outcomes  during  the  use  of  assistive  trunk  exoskeletons,  and  to  assess  its  test-retest  reliability.  The  battery  of  tests  consists  of  12  different  simple  functional  tasks.  Twenty  participants  were  included  in  an  intra-session  reliability  test  and  repeated  the  tests  within  7–10  days  to  assess  inter-session  reliability.  They  were  wearing  a  novel  passive  spinal  exoskeleton  during  all  trials.  The  outcomes  included  quantitative  and  subjective  measures,  such  as  performance  time  and  rating  of  discomfort  and  perceived  task  difficulty.  The  majority  of  the  outcome  measures  were  reliable  within  session  and  between  sessions  (ICC  or  α > 0.80).  Systematic  effects  were  observed  in  a  few  tasks,  suggesting  that  familiarization  trials  will  be  needed  to  minimize  the  learning  effects.  The  novel  battery  of  tests  could  become  an  important  easy-to-use  tool  for  functional  testing  of  the  spinal  exoskeletons  in  addition  to  more  specific  biomechanical  and  physiological  testing.  Further  studies  should  address  the  reliability  of  the  present  battery  of  tests  for  assessing  specific  populations,  such  as  low  back  pain  patients  and  explore  how  to  minimize  systematic  effects  that  were  observed  in  this  study.
2	The  impact  of  anticipating  a  stressful  task  on  sleep  inertia  when  on  call.  Abstract  Sleep  inertia,  the  state  of  reduced  alertness  upon  waking,  can  negatively  impact  on-call  workers.  Anticipation  of  a  stressful  task  on  sleep  inertia,  while  on-call  was  investigated.  Young,  healthy  males  (n = 23)  spent  an  adaptation,  control  and  two  counterbalanced  on-call  nights  in  the  laboratory.  When  on-call,  participants  were  told  they  would  be  woken  to  a  high  or  low  stress  task.  Participants  were  not  woken  during  the  night,  instead  were  given  a  2300-0700  sleep  opportunity.  Participants  slept  ∼7.5-h  in  all  conditions.  Upon  waking,  sleep  inertia  was  quantified  using  the  Karolinska  Sleepiness  Scale  and  Psychomotor  Vigilance  and  Spatial  Configuration  Tasks,  administered  at  15-min  intervals.  Compared  to  control,  participants  felt  sleepier  post  waking  when  on-call  and  sleepiest  in  the  low  stress  compared  to  the  high  stress  condition  (p
2	Measuring  driver  distraction  evaluation  of  the  box  task  method  as  a  tool  for  assessing  in  vehicle  system  demand.  Abstract  Several  tools  have  been  developed  over  the  past  twenty  years  to  assess  the  degree  of  driver  distraction  caused  by  secondary  task  engagement.  A  relatively  new  and  promising  method  in  this  area  is  the  box  task  combined  with  a  detection  response  task  (BT + DRT).  However,  no  evaluation  regarding  the  BT's  sensitivity  currently  exists.  Thus,  the  aim  of  the  present  study  was  to  evaluate  the  BT + DRT  by  comparing  its  sensitivity  to  the  sensitivity  of  already  established  methods.  Twenty-nine  participants  engaged  in  several  artificial  and  realistic  secondary  tasks  while  either  performing  the  BT + DRT,  the  Lane  Change  Test  (LCT),  or  driving  through  a  simple  course  in  a  simulator.  The  results  showed  that  the  BT  parameters  (especially  the  standard  deviation  of  box  position  and  size)  were  sensitive  to  differences  in  demand  across  the  visual-manual  secondary  tasks.  This  was  comparable  to  what  was  found  with  the  LCT.  Surprisingly,  the  BT  performance  measures  were  more  sensitive  than  those  of  the  driving  simulation  task.  The  BT + DRT  also  captured  cognitive  distraction  effects  with  the  integration  of  the  DRT.  Hence,  the  BT + DRT  could  be  a  cost-effective  method  to  assess  in-vehicle  system  demand.  However,  further  investigations  are  necessary  to  better  understand  the  potential  of  the  BT  method.
2	The  quick  exposure  check  qec  inter  rater  reliability  in  total  score  and  individual  items.  The  development  of  musculoskeletal  disorders  has  been  linked  to  various  risk  factors  in  the  work  environment  including  lifting  heavy  loads,  machine  and  materials  handling,  work  postures,  repetitive  work,  work  with  handheld  vibrating  tools,  and  work  stress.  The  Quick  Exposure  Check  (QEC)  was  designed  to  assess  exposure  to  work-related  musculoskeletal  risk  factors  affecting  the  back,  shoulder/arm,  wrist/hand,  and  neck.  We  investigated  the  inter-rater  reliability  of  the  summary  scores  and  individual  items  of  the  QEC  by  comparing  two  simultaneous  assessments  of  51  work  tasks,  performed  by  14  different  workers.  The  work  tasks  were  mainly  "light"  to  "moderately  heavy".  For  total  scores,  the  level  of  disagreement  for  shoulder/arm  had  a  Relative  Position  of  0.13  (95%  CI:  0.02;  0.23)  and  no  statistically  significant  random  disagreement.  Percentage  agreement  was  63-100%  for  individual  items  and  71-88%  for  total  score.  Weighted  Kappa  of  agreement  for  the  individual  items  rated  by  the  assessors  were  -0.94-0.77;  highest  for  back  motion,  and  lowest  for  wrist/hand  position.  The  Swedish  translation  of  the  Quick  Exposure  Check  has  moderate  to  very  good  inter-rater  reliability  with  fair  to  slight  levels  of  systematic  disagreement.  There  was  no  statistically  significant  random  disagreement.
2	Evaluating  advanced  driver  assistance  system  trainings  using  driver  performance  attention  allocation  and  neural  efficiency  measures.  There  are  about  44  million  licensed  older  drivers  in  the  U.S.  Older  adults  have  higher  crash  rates  and  fatalities  as  compared  to  middle-aged  and  young  drivers,  which  might  be  associated  with  degradations  in  sensory,  cognitive,  and  physical  capabilities.  Advanced  driver-assistance  systems  (ADAS)  have  the  potential  to  substantially  improve  safety  by  removing  some  of  driver  vehicle  control  responsibilities.  However,  a  critical  aspect  of  providing  ADAS  is  educating  drivers  on  their  operational  characteristics  and  continued  use.  Twenty  older  adults  participated  in  a  driving  simulation  study  assessing  the  effectiveness  of  video-based  and  demonstration-based  training  protocols  in  learning  ADAS  considering  gender  differences.  The  findings  revealed  video-based  training  to  be  more  effective  than  demonstration-based  training  in  improving  driver  performance  and  reducing  off-road  visual  attention  allocation  and  mental  workload.  In  addition,  female  drivers  required  lower  investment  of  mental  effort  (higher  neural  efficiency)  to  maintain  the  performance  relative  to  males  and  they  were  less  distracted  by  ADAS.  However,  male  drivers  were  faster  in  activating  ADAS  as  compared  to  females  since  they  were  monitoring  the  status  of  ADAS  features  more  frequently  while  driving.  The  findings  of  this  study  provided  an  empirical  support  for  using  video-based  approach  for  learning  ADAS  in  older  adults  to  improve  driver  safety  and  supported  previous  findings  on  older  adults'  learning  that  as  age  increases  there  is  a  tendency  to  prefer  more  passive  and  observational  learning  methods.
2	Novel  ventilation  design  of  combining  spacer  and  mesh  structure  in  sports  t  shirt  significantly  improves  thermal  comfort.  Abstract      This  paper  reports  on  novel  ventilation  design  in  sports  T-shirt,  which  combines  spacer  and  mesh  structure,  and  experimental  evidence  on  the  advantages  of  design  in  improving  thermal  comfort.  Evaporative  resistance  (Re)  and  thermal  insulation  (Rc)  of  T-shirts  were  measured  using  a  sweating  thermal  manikin  under  three  different  air  velocities.  Moisture  permeability  index  (im)  was  calculated  to  compare  the  different  designed  T-shirts.  The  T-shirts  of  new  and  conventional  designs  were  also  compared  by  wearer  trials,  which  were  comprised  of  30 min  treadmill  running  followed  by  10 min  rest.  Skin  temperature,  skin  relative  humidity,  heart  rate,  oxygen  inhalation  and  energy  expenditure  were  monitored,  and  subjective  sensations  were  asked.  Results  demonstrated  that  novel  T-shirt  has  11.1%  significant  lower  im  than  control  sample  under  windy  condition.  The  novel  T-shirt  contributes  to  reduce  the  variation  of  skin  temperature  and  relative  humidity  up  to  37%  and  32%,  as  well  as  decrease  3.3%  energy  consumption  during  exercise.
2	Seat  cushions  made  of  warp  knitted  spacer  fabrics  influence  seat  transmissibility.  Abstract  The  aim  of  this  study  was  to  compare  the  application  of  different  warp  knitted  spacer  fabrics  on  a  car  seat  shell  to  a  standard  seat  in  terms  of  vertical  seat  transmissibility.  Furthermore,  the  results  obtained  by  human  subject  tests  were  compared  to  results  of  an  anthropodynamic  dummy  test.  Experiments  were  conducted  on  a  vertically  actuated  platform  under  laboratory  conditions  with  16  human  subjects  and  an  anthropodynamic  dummy.  Seat  Effective  Amplitude  Transmissibility  of  the  seat  pan  and  the  seat  backrest  were  calculated  to  evaluate  ride  quality.  Seat  transmissibility  ranged  between  73.6%  for  backrest  and  177.7%  for  pan.  Based  on  the  results  of  statistical  tests,  the  hypothesis  that  the  transmissibility  of  a  seat  would  be  influenced  by  seat  cushion  conditions  was  accepted  (p
2	Postural  stability  when  walking  effect  of  the  frequency  and  magnitude  of  lateral  oscillatory  motion.  Abstract      While  walking  on  an  instrumented  treadmill,  20  subjects  were  perturbed  by  lateral  sinusoidal  oscillations  representative  of  those  encountered  in  transport:  frequencies  in  the  range  0.5–2 Hz  and  accelerations  in  the  range  0.1–2.0 ms−2  r.m.s.,  corresponding  to  velocities  in  the  range  0.032–0.16 ms−1  r.m.s.  Postural  stability  was  assessed  from  the  self-reported  probability  of  losing  balance  (i.e.,  perceived  risk  of  falling)  and  the  movements  of  the  centre  of  pressure  beneath  the  feet.  With  the  same  acceleration  at  all  frequencies,  the  velocities  and  displacements  of  the  oscillatory  perturbations  were  greater  with  the  lower  frequency  oscillations,  and  these  caused  greater  postural  instability.  With  the  same  velocity  at  all  frequencies,  postural  instability  was  almost  independent  of  the  frequency  of  oscillation.  Movements  of  the  centre  of  pressure  show  that  subjects  attempted  to  compensate  for  the  perturbations  by  increasing  their  step  width  and  increasing  their  step  rate.
2	Age  familiarity  and  intuitive  use  an  empirical  investigation.  Research  has  shown  that  older  adults  interact  with  products  less  intuitively  than  younger  adults,  and  that  familiarity  is  an  essential  element  of  intuitive  interaction.  This  paper  reports  on  the  findings  of  two  empirical  studies  that  examined  familiarity  in  younger  and  older  adults.  Each  study  comprised  32  participants  over  four  age  groups.  The  first  study  required  participants  to  use  their  own  contemporary  products  in  their  homes  in  order  to  investigate  older  adults’  familiarity  with  them,  and  how  this  familiarity  differed  from  that  of  younger  adults.  Older  people  were  less  familiar  with  their  own  contemporary  products  that  younger  people.  The  second  study  aimed  to  investigate  differences  in  familiarity  between  younger  and  older  adults  while  using  products  that  they  did  not  own  and  were  likely  to  be  less  familiar  with.  When  using  products  not  already  familiar  to  them,  both  middle  aged  and  older  adults  showed  significantly  lower  familiarity  than  younger  people.  The  significance  of  this  research  is  in  its  empirical  findings  about  familiarity  differences  between  age  groups.  It  has  been  recognised  that  the  identification  and  understanding  of  differences  in  familiarity  will  enable  designers  to  design  more  intuitive  interfaces  and  systems  for  both  younger  and  older  cohorts.  The  implications  of  the  findings  from  the  two  studies  reported  here  are  discussed  in  light  of  this  recognition.
2	Microwave  photonic  frequency  down  conversion  link  based  on  intensity  and  phase  paralleled  modulation.  A  photonic  microwave  down-conversion  approach  is  proposed  and  experimentally  demonstrated  based  on  a  Mach-Zehnder  modulator  paralleled  with  a  phase  modulator.  The  incident  radio  frequency  signal  and  the  local  oscillator  signal  are  feed  to  the  MZM  and  PM,  respectively,  and  these  two  modulated  optical  signals  interfere  in  the  coupler.  The  useless  higher-order  sidebands  are  removed  by  a  tunable  optical  band-pass  filter.  The  principle  of  microwave  frequency  down-conversion  is  analyzed  theoretically,  the  MZM  and  PM  paralleled  frequency  down-conversion  system  is  built.  Then  the  performance  of  system  is  tested,  and  the  experimental  results  show  that  the  spurious-free  dynamic  range  achieves  104.8  dB:Hz2/3.  Compared  to  the  conventional  MZM-MZM  cascaded  system,  the  SFDR  has  been  improved  by  16  dB.  The  MZM  and  PM  paralleled  frequency  down-conversion  system  can  balance  the  intensity  of  the  two  coherent  beams  easily,  and  only  single  DC  bias  is  needed.  The  proposed  method  possesses  simple  structure  and  high  dynamic  range.
2	Recreating  little  manila  through  a  virtual  reality  serious  game.  This  paper  describes  the  authors’  efforts  to  develop  a  virtual  reality  (VR)  serious  game  depicting  life  in  Little  Manila  in  the  late  1940s.  Little  Manila  was  a  thriving  Filipino  community  located  in  downtown  Stockton,  California,  that  existed  from  the  1920s  to  1960s  until  it  was  destroyed  as  part  of  a  freeway  construction  project.  Using  Unreal  Engine,  Autodesk  Maya,  and  Autodesk  3ds  Max  to  design  the  game  and  historical  research  to  contextualize  in-game  content,  the  authors  have  constructed  a  quest-based  VR  game  intended  for  educational  use  in  both  museum  and  classroom  settings.
2	A  modular  framework  for  deformation  and  fracture  using  gpu  shaders.  Advances  in  the  graphical  realism  of  modern  video  games  have  been  achieved  mainly  through  the  development  of  the  GPU  (Graphics  Processing  Unit),  providing  a  dedicated  graphics  co-processor  and  framebuffer.  The  most  recent  GPU's  are  extremely  capable  and  so  flexible  that  it  is  now  possible  to  implement  a  wide  range  of  algorithms  on  graphics  hardware  that  were  previously  confined  to  the  computer's  CPU  (Central  Processing  Unit).  We  present  a  modular  framework  for  real-time  simulation  of  deformation  and  fracture  for  use  in  computer  games  that,  rather  than  employing  a  General  Purpose  GPU  (GPGPU)  Framework,  implements  aspects  of  the  simulation  within  shader  programs  on  recent  GPU's.
2	Tangential  force  sensing  system  on  forearm.  In  this  paper,  we  propose  a  sensing  system  that  can  detect  one  dimensional  tangential  force  on  a  forearm.  There  are  some  previous  tactile  sensors  that  can  detect  touch  conditions  when  a  user  touches  a  human  skin  surface.  Those  sensors  are  usually  attached  on  a  fingernail,  so  therefore  a  user  cannot  touch  the  skin  with  two  fingers  or  with  their  palm.  In  the  field  of  cosmetics,  for  example,  they  want  to  measure  contact  forces  when  a  customer  puts  their  products  onto  their  skin.  In  this  case,  it  is  preferable  that  the  sensor  can  detect  contact  forces  in  many  different  contact  ways.  In  this  paper,  we  decided  to  restrict  a  target  area  to  a  forearm.  Since  the  forearm  has  a  cylindrical  shape,  its  surface  deformation  propagates  to  neighboring  areas  around  a  wrist  and  an  elbow.  The  deformation  can  be  used  to  estimate  tangential  force  on  the  forearm.  Our  system  does  not  require  any  equipment  for  the  active  side  (i.e.  fingers  or  a  palm).  Thus  a  user  can  touch  the  forearm  in  arbitrary  ways.  We  show  basic  numerical  simulation  and  experimental  results  which  indicate  that  the  proposed  system  can  detect  tangential  force  on  the  forearm.  Also  we  show  some  possible  applications  that  use  the  forearm  as  a  human-computer  interface  device.
2	Understanding  persuasion  and  motivation  in  interactive  stroke  rehabilitation  a  physiotherapists  perspective  on  patient  motivation.  For  the  research  reported  in  this  paper  ethnographic  research  methodologies  were  used  to  explore  patient  motivation,  feedback  and  the  use  of  interactive  technologies  in  the  ward.  We  have  conducted  in-depth  interviews  with  physiotherapists,  who  work  closely  with  stroke  patients  to  help  them  regain  movement  and  function.  From  this  research,  a  set  of  design  guidelines  have  been  developed  which  can  be  applied  in  the  design  of  interactive  rehabilitation  equipment.
2	The  pivis  framework  for  visualization  of  digital  object  memories.  Digital  object  memories,  as  a  shared  storage  place  for  object-related  information,  support  information  reuse  across  domain  and  application  boundaries  throughout  its  whole  life  cycle.  In  this  paper,  we  present  a  reusable  general-purpose  software  framework  for  the  visualization  of  and  interaction  with  such  digital  object  memories.  The  framework  simplifies  the  implementation  of  applications  which  support  end  users  based  on  information  stored  in  object  memories.  Challenges  arise  from  the  open  nature  of  object  memories,  where  format  and  semantic  of  contained  data  is  usually  not  known  in  advance.  The  framework  applies  plugins  for  visualization,  data  conversion  and  inferencing.  A  planning  algorithm  automatically  constructs  a  data  processing  and  visualization  pipeline  for  each  new  object.  Objects  can  become  self-representative  by  providing  their  own  plugins,  which  the  framework  dynamically  integrates  into  the  generated  pipeline.  We  describe  the  framework  architecture  and  give  examples  of  three  different  systems  implemented  with  our  framework.
2	Embedded  non  interactive  captcha  for  fischer  random  chess.  Cheating  in  chess  can  take  many  forms  and  has  existed  almost  as  long  as  the  game  itself.  The  advent  of  computers  has  introduced  a  new  form  of  cheating  into  the  game.  Thanks  to  the  computational  power  of  modern-day  computers,  a  player  can  use  a  program  to  calculate  thousands  of  moves  for  him  or  her,  and  determine  the  best  possible  scenario  for  each  move  and  counter-move.  These  programs  are  often  referred  to  as  “bots,”  and  can  even  play  the  game  without  any  user  interaction.  In  this  paper,  we  describe  a  methodology  aimed  at  preventing  bots  from  participating  in  online  chess  games.  The  proposed  approach  is  based  on  the  integration  of  a  CAPTCHA  protocol  into  a  game  scenario,  and  the  subsequent  inability  of  bots  to  accurately  track  the  game  states.  Preliminary  experimental  results  provide  favorable  feedback  for  further  development  of  the  proposed  algorithm.
2	Using  game  level  design  as  an  applied  method  for  software  engineering  education.  Hands-on  training  is  considered  one  of  the  major  requirements  for  most  of  Computer  Science  curriculum;  some  courses  mandate  lab  components,  while  other  courses  do  not  require  it.  Students'  learning  outcomes  are  usually  enhanced  by  “doing”  rather  than  simply  reading  or  attending  lectures,  therefore  skills  acquired  by  projects  and  hands-on  training  are  a  must  have  for  the  job  market.  This  paper  shows  by  experiments  that  students'  learning  outside  classroom  using  a  hands-on  project.  Students  experienced  first-hand  how  to  work  on  team  to  achieve  one  goal  of  engineering  and  design  a  Video  Game  Level  design.  Level  design  is  the  process  of  creating  levels  for  a  video  game.  Level  design  process  is  usually  split  up  among  a  group  of  designers.  A  level  designer  is  a  crucial  member  to  a  video  game  development  team.  Level  design  team  responsible  of  creating  the  world  that  the  player  is  put  into.  The  design  process  consists  of  4  phases:  the  idea  on  paper,  hammering  the  level,  testing  the  product,  and  back  to  the  drawing  board  phase.  There  are  several  concepts  should  be  taken  into  consideration  during  the  design  process;  geography,  sound  and  lighting  concepts  are  highlighted  and  considered.  The  most  important  finding  from  this  research  is;  group  based  project  even  if  it  is  not  part  of  a  formal  course  offering  using  an  attractive  method,  like  game  level  design,  can  be  of  a  great  impact  on  students  learning  and  skill  building.
2	Monte  carlo  tree  search  in  simultaneous  move  games  with  applications  to  goofspiel.  Monte  Carlo  Tree  Search  (MCTS)  has  become  a  widely  popular  sampled-based  search  algorithm  for  two-player  games  with  perfect  information.  When  actions  are  chosen  simultaneously,  players  may  need  to  mix  between  their  strategies.  In  this  paper,  we  discuss  the  adaptation  of  MCTS  to  simultaneous  move  games.  We  introduce  a  new  algorithm,  Online  Outcome  Sampling  (OOS),  that  approaches  a  Nash  equilibrium  strategy  over  time.  We  compare  both  head-to-head  performance  and  exploitability  of  several  MCTS  variants  in  Goofspiel.  We  show  that  regret  matching  and  OOS  perform  best  and  that  all  variants  produce  less  exploitable  strategies  than  UCT.
2	Movieremix  having  fun  playing  with  videos.  The  process  of  producing  new  creative  videos  by  editing,  combining,  and  organizing  pre-existing  material  (e.g.,  video  shots)  is  a  popular  phenomenon  in  the  current  web  scenario.  Known  as  remix  or  video  remix,  the  produced  videomay  have  new  and  different  meanings  with  respect  to  the  source  material.  Unfortunately,  when  managing  audiovisual  objects,  the  technological  aspect  can  be  a  burden  for  many  creative  users.  Motivated  by  the  large  success  of  the  gaming  market,  we  propose  a  novel  game  and  an  architecture  to  make  the  remix  process  a  pleasant  and  stimulating  gaming  experience.  MovieRemix  allows  people  to  act  like  a  movie  director,  but  instead  of  dealing  with  cast  and  cameras,  the  player  has  to  create  a  remixed  video  starting  from  a  given  screenplay  and  from  video  shots  retrieved  from  the  provided  catalog.  MovieRemix  is  not  a  simple  video  editing  tool  nor  is  a  simple  game:  it  is  a  challenging  environment  that  stimulates  creativity.  To  temp  to  play  the  game,  players  can  access  different  levels  of  screenplay  (original,  outline,  derived)  and  can  also  challenge  other  players.  Computational  and  storage  issues  are  kept  at  the  server  side,  whereas  the  client  device  just  needs  to  have  the  capability  of  playing  streaming  videos.
2	Innovation  in  game  development  it  enablement  and  affordances  on  kickstarter.  The  rise  of  social  media  platforms  has  changed  the  nature  of  interactions  between  computer  games  companies  and  players.  Computer  games  companies  have  greater  access  to  information  on  their  needs  and  wants  as  they  share  information  and  engage  with  players  in  the  development  process.  The  purpose  of  this  paper  is  to  highlight  how  crowd-funding  social  media  platforms,  in  this  case  Kickstarter,  can  enable  computer  games  companies  and  players  to  produce  and  utilize  knowledge  that  can  facilitate  the  game  development  process.  In  this  paper,  the  Poppelbus  and  Malsbender  conceptual  framework  and  the  IT  enablement  model  by  Jarvenpaa  and  Tuunainen  are  combined  in  order  to  evaluate  how  computer  games  companies  can  interact  with  players  and  accelerate  games  development  using  Kickstarter.  The  results  show  that  Kickstarter  is  a  useful  platform  for  gaining  access  to  a  dedicated  user  base  willing  to  share  their  knowledge.  Kickstarter  enables  companies  to  build  strong  relationships  with  their  backers  via  the  processes  of  informing,  responding  and  promoting  response.  Kickstarter  helps  to  integrate  the  role  of  the  games  company  with  that  of  the  users  by  enabling  both  parties  to  engage  actively  with  each  other,  rather  than  the  games  company  taking  a  passive  role  of  simply  gathering  feedback  from  players.
2	Games  you  can  t  win.  A  common  notion  in  games  for  learning  is  that  the  player  must  win  the  game.  But  is  it  always  necessary  for  the  player  to  win  in  order  to  ‘get’  the  message  that  the  game  is  trying  to  portray?  When  we  think  back  on  our  most  memorable  learning  experiences,  we  find  that  these  lessons  are  often  things  we  learned  through  failure  rather  than  success.  There  is  a  class  of  games  where  ‘winning’  doesn’t  look  the  way  we  typically  expect  it  to  look.  Some  games  do  not  allow  their  players  to  win,  and  their  underlying  message  is  more  akin  to  that  found  in  a  cautionary  tale.  We  refer  to  these  games  as  games  you  can’t  win,  and  they  form  a  distinctly  different  approach  to  game  design.  Games  such  as  Sweatshop  (Littleloud,  2011),  Darfur  is  Dying  (MTVu,  2006),  and  September  12th  (Newsgaming,  2005)  are  games  you  cannot  conceivably  win,  and  they  are  designed  that  way  deliberately.  This  paper  presents  a  critique  on  serious  games  that  are  unwinnable  by  design.  We  examine  the  concepts  of  games  and  learning,  the  design  of  unwinnable  games,  design  strategies  for  unhappy  and/or  unwinnable  learning  games,  and  ways  to  measure  the  success  of  games  you  can’t  win.  We  also  briefly  consider  potential  issues  and  future  directions,  and  we  conclude  that  the  messages  delivered  via  games  you  can’t  win  are  more  powerful  than  those  of  games  in  which  you  can  win.
2	Rocomo  a  generic  ontology  for  context  modeling  representation  and  reasoning  in  a  context  aware  middleware.  We  describe  an  abstract,  generic  and  extensible  ontology,  the  Rover  Context  Model  Ontology  (RoCoMO),  which  is  currently  being  designed  and  developed  to  model  and  represent  context  in  an  intelligent  context-aware  middleware  system,  called  Rover  II.  The  Rover  Context  Model  (RoCoM)  is  the  underlying  context  model  for  Rover  II  and  is  centered  on  four  primitives  that  can  be  used  to  represent  a  situation:  entity,  event,  activity  and  relationship.  The  ontology  is  expressed  using  the  Web  Ontology  Language  (OWL)  and  includes  two  components  --  RoCoM  Core  Ontology  and  the  RoCoM  Application  Ontology.  We  also  illustrate  its  usage  with  the  aid  of  a  public  safety  application  called  M-Urgency  that  is  currently  deployed  at  the  UMD  campus.
2	Human  localization  at  home  using  kinect.  In  this  paper  authors  have  presented  a  method  to  localize  and  detect  human  being  from  Kinect  captured  sequence  of  images.  The  proposed  method  takes  a  sequence  of  gray  (G)  scale  image  and  the  corresponding  depth  (D)  image  as  input.  The  gray  scale  image  and  the  depth  information  are  captured  using  two  different  sensors  within  the  same  device,  Kinect  and  the  processing  are  executed  in  the  processor  attached  with  Kinect.  The  proposed  method  localizes  the  human  by  using  their  motion  along  x,  y  direction  and  then  considers  all  pixels  connected  with  those  pixels  and  over  a  3D  plane  to  accomplish  the  segmentation  with  an  accuracy  of  77%.  Experimental  results  demonstrate  that  our  method  is  robust  against  existing  method  for  human  localization.
2	A  perspective  on  multi  user  interaction  design  based  on  an  understanding  of  domestic  lighting  conflicts.  More  and  more  connected  systems  are  entering  the  social  and  shared  home  environment.  Interaction  with  these  systems  is  often  rather  individual  and  based  on  personal  preferences,  leading  to  conflicts  in  multi-user  situations.  In  this  paper,  we  aim  to  develop  a  perspective  on  how  to  design  for  multi-user  interaction  with  connected  lighting  systems,  based  on  a  better  understanding  of  real-life  interpersonal  lighting  conflicts.  In  order  to  understand  everyday  lighting  conflicts,  including  their  causes  and  resolution  strategies,  we  present  two  studies.  First,  we  observe  real-life  lighting  conflicts  between  couples  living  in  single-room  apartments.  Using  probes  for  data  gathering  followed  by  dyadic  interviews,  we  identify  the  role  of  agreements  on  use  in  conflicts  and  we  identify  different  types  of  conflicts  (preference,  activity,  and  attitude  conflicts).  Next,  we  take  a  more  disruptive  approach  based  on  technology  probes,  where  we  provoke  lighting  conflicts  in  family  living  rooms  to  observe  resolution  strategies.  We  find  that  people  try  to  avoid  conflicts  at  all  costs.  If  there  is  a  risk  that  others  are  negatively  affected  by  an  adjustment,  people  rather  not  interact  with  the  system  at  all.  Based  on  these  insights,  we  defined  a  perspective  on  designing  for  multi-user  interaction  that  provides  the  user  with  the  confidence  that  interactions  are  socially  accepted.  This  assurance  can  be  given  by  presenting  the  user  with  information  leading  to  awareness  about  the  acceptance  of  a  lighting  change  by  the  other  users.  We  advise  on  what  information  can  be  visualized,  based  on  the  three  conflict  types  we  observed  in  the  study.  The  combination  of  a  deeper  understanding  of  conflicts  and  a  perspective  on  multi-user  interface  design  can  serve  as  a  starting  point  to  design  better  multi-user  interfaces  for  domestic  connected  systems.
2	Hybrid  computation  offloading  for  smart  home  automation  in  mobile  cloud  computing.  Smart  home  automation  enables  the  users  to  realize  the  access  control  of  the  in-home  appliances  by  the  mobile  devices.  With  the  rapid  development  of  mobile  cloud  computing,  offloading  computation  workloads  of  the  home  automation  applications  to  nearby  cloudlets  has  been  treated  as  a  promising  approach  to  overcoming  inherent  flaws  of  portable  devices,  such  as  low  battery  capacity.  The  computing  capacity  of  cloudlet  is  limited  compared  with  the  distant  public  cloud  whose  elastic  computation  resources  are  almost  infinite.  Therefore,  some  mobile  services  should  wait  for  the  occupied  computation  resources  in  the  cloudlet  to  get  released,  which  is  less  energy-efficient.  In  view  of  this  challenge,  we  model  the  waiting  time  spending  in  the  cloudlet  as  a  M/M/m/ź  queue  and  propose  a  hybrid  computation  offloading  algorithm  for  home  automation  applications  to  minimize  the  total  energy  consumption  of  the  mobile  devices  within  a  given  constant  deadline.  The  proposed  algorithm  combines  cloudlet  with  public  clouds,  providing  a  more  energy-efficient  offloading  strategy  for  home  automation  applications.  Technically,  a  particle  swarm  optimization  (PSO)-based  heuristic  algorithm  is  implemented  to  schedule  mobile  services.  Comprehensive  experiments  are  conducted  to  demonstrate  the  effectiveness  and  efficiency  of  our  proposed  algorithm.
2	Human  activity  recognition  using  federated  learning.  State-of-the-art  deep  learning  models  for  human  activity  recognition  use  large  amount  of  sensor  data  to  achieve  high  accuracy.  However,  training  of  such  models  in  a  data  center  using  data  collected  from  smart  devices  leads  to  high  communication  costs  and  possible  privacy  infringement.  In  order  to  mitigate  aforementioned  issues,  federated  learning  can  be  employed  to  train  a  generic  classifier  by  combining  multiple  local  models  trained  on  data  originating  from  multiple  clients.  In  this  work  we  evaluate  federated  learning  to  train  a  human  activity  recognition  classifier  and  compare  its  performance  to  centralized  learning  by  building  two  models,  namely  a  deep  neural  network  and  a  softmax  regression  trained  on  both  synthetic  and  real-world  datasets.  We  study  communication  costs  as  well  as  the  influence  of  erroneous  clients  with  corrupted  data  in  federated  learning  setting.  We  have  found  that  federated  learning  for  the  task  of  human  activity  recognition  is  capable  of  producing  models  with  slightly  worse,  but  acceptable,  accuracy  compared  to  centralized  models.  In  our  experiments  federated  learning  achieved  an  accuracy  of  up  to  89%  compared  to  93%  in  centralized  training  for  the  deep  neural  network.  The  global  model  trained  with  federated  learning  on  skewed  datasets  achieves  accuracy  comparable  to  centralized  learning.  Furthermore,  we  identified  an  important  issue  of  clients  with  corrupted  data  and  proposed  a  federated  learning  algorithm  that  identifies  and  rejects  erroneous  clients.  Lastly,  we  have  identified  a  trade-off  between  communication  cost  and  the  complexity  of  a  model.  We  show  that  more  complex  models  such  as  deep  neural  network  require  more  communication  in  federated  learning  settings  for  human  activity  recognition  compared  to  less  complex  models,  such  as  multinomial  logistic  regression.
2	Notification  log  an  open  source  framework  for  notification  research  on  mobile  devices.  In  the  past  decade,  the  number  of  always-connected  mobile  devices  exploded.  Smartphones  are  always  with  the  user  and  host  a  large  number  of  applications  and  services  that  use  notifications  to  gain  the  user's  attention.  These  notifications  and  their  effects  on  users  have  been  extensively  researched  in  the  context  of  human-computer  interaction.  In  a  body  of  prior  work,  numerous  small-  and  large-scale  studies  were  conducted  to  understand  notifications  as  well  as  their  effects.  A  common  theme  in  these  studies  is  the  need  for  accessing  users'  notifications,  often  for  logging  purposes.  In  this  paper,  we  present  an  open-source  framework  for  notification  research  on  mobile  devices.  The  framework  has  been  used  as  the  foundation  of  multiple  in-the-wild  and  in-lab  studies,  and  has  been  downloaded  by  over  60,000  users.  We  explain  the  requirements,  the  architecture,  and  past  application  scenarios  of  the  framework.  The  scenarios  range  from  enabling  reflection  on  mobile  notifications  to  rich  experiences  in  multi-device  environments.
2	A  pervasive  game  to  promote  social  offline  interaction.  Human  relationships  are  migrating  from  the  physical  world  to  the  virtual  world.  Pervasive  games  can  be  a  valuable  and  enjoyable  method  to  bring  people  back  to  the  physical  world.  In  this  position  paper,  we  present  a  concept  for  a  pervasive  game,  which  integrates  some  specific  mechanisms  aiming  at  promoting  social  offline  interaction.
2	Design  and  assessment  of  enabling  environments  for  cooking  activities.  Assistive  technologies  can  help  cognitively  impaired  people  in  planning  and  memory  tasks  [1].  To  provide  efficient  assistance,  pervasive  environments  should  be  able  to  adapt  assistance  to  the  user's  capacities,  the  activity  to  perform  and  the  context.  This  paper  presents  an  analysis  of  a  cooking  activity  performed  by  a  user  with  intellectual  disability  in  a  pervasive  environment,  named  Archipel.  The  objective  is  to  assess  the  impact  of  this  environment  on  the  activity  processing.  The  activity  analysis  is  based  on  ergonomic  and  neuropsychological  methods.  The  results  show  that  Archipel  has  a  positive  impact  on  the  user's  self-determination  and  independence.  Further  improvements  should  take  into  account  the  cognitive  deficits  presented  by  the  people  and  ergonomic  principles  to  provide  more  appropriate  assistance.
2	Mood  modeling  accuracy  depends  on  active  logging  and  reflection.  Current  behavior  change  systems  often  demand  extremely  advanced  sensemaking  skills,  requiring  users  to  interpret  personal  datasets  in  order  to  understand  and  change  behavior.  We  describe  EmotiCal,  a  system  to  help  people  better  manage  their  emotions,  that  finesses  such  complex  sensemaking  by  directly  recommending  specific  mood-boosting  behaviors  to  users.  This  paper  first  describes  how  we  develop  the  accurate  mood  models  that  underlie  these  mood-boosting  recommendations.  We  go  on  to  analyze  what  types  of  information  contribute  most  to  the  predictive  power  of  such  models,  and  how  we  might  design  systems  to  reliably  collect  such  predictive  information.  Our  results  show  that  we  can  derive  very  accurate  mood  models  with  relatively  small  samples  of  just  70  users.  These  models  explain  61%  of  variance  by  combining:  (a)  user  reflection  about  the  effects  of  different  activities  on  mood,  (b)  user  explanations  of  how  different  activities  affect  mood,  and  (c)  individual  differences.  We  discuss  the  implications  of  these  findings  for  the  design  of  behavior  change  systems,  as  well  as  for  theory  and  practice.  Contrary  to  many  recent  approaches,  our  findings  argue  for  the  importance  of  active  user  reflection  rather  than  passive  sensing.
2	Effects  of  robots  intonation  and  bodily  appearance  on  robot  mediated  communicative  treatment  outcomes  for  children  with  autism  spectrum  disorder.  Previous  research  has  suggested  that  robot-mediated  therapy  is  effective  in  the  treatment  of  children  with  Autism  Spectrum  Disorder  (ASD),  but  not  all  robots  seem  equally  appropriate  for  this  purpose.  We  investigated  in  an  exploratory  study  whether  a  robot's  intonation  (monotonous  vs.  normal)  and  bodily  appearance  (mechanical  vs.  humanized)  influence  the  treatment  outcomes  of  Pivotal  Response  Treatment  (PRT)  sessions  for  children  with  ASD.  The  children  (age  range  4---8  years)  played  puzzle  games  with  a  robot  which  required  communication  with  the  robot.  The  treatment  outcomes  were  measured  in  terms  of  both  task  performance  and  affective  states.  We  have  found  that  intonation  and  bodily  appearance  have  an  effect  on  children's  affective  states  but  not  on  task  performance.  Specifically,  humanized  bodily  appearance  leads  to  more  positive  affective  states  in  general  and  a  higher  degree  of  interest  in  the  interaction  than  mechanical  bodily  appearance.  Congruence  between  bodily  appearance  and  intonation  triggers  a  higher  degree  of  happiness  in  children  with  ASD  than  incongruence  between  these  two  factors.
2	Consent  reconsidered  reframing  consent  for  ubiquitous  computing  systems.  The  developing  complexity  and  decreasing  visibility  of  pervasive  computing  systems,  coupled  with  increasing  value  and  sensitivity  of  personal  data,  mean  that  it  is  no  longer  sufficient  to  design  systems  that  assume  users  capable  of  making  informed  decisions  at  a  single  moment.  In  particular,  the  unprecedented  sensitivity  of  contextual  data,  and  the  potential  harms  associated  with  inferences  made  on  the  basis  of  that  data,  highlights  the  need  to  revisit  our  design  principles  in  respect  of  consent.  This  thesis  will  use  a  mixed-methods  approach  to  reframe  'consent'  for  ubiquitous  computing  systems,  resulting  in  a  series  of  design  guidelines  to  inform  future  developments.
2	Opportunistic  position  update  protocols  for  mobile  devices.  Many  location-based  applications  such  as  geo-social  networks  rely  on  location  services  storing  mobile  object  positions.  To  update  positions  on  location  servers,  position  update  protocols  are  used.  On  the  one  hand,  these  protocols  decide  when  an  update  has  to  be  sent  to  ensure  a  certain  quality  of  position  information.  On  the  other  hand,  they  try  to  minimize  the  energy  consumption  of  the  mobile  device  by  reducing  communication  to  a  minimum.      In  this  paper,  we  show  how  to  improve  the  energy  efficiency  of  different  update  protocols  by  taking  the  energy  characteristics  of  the  mobile  network  interface  into  account.  In  particular,  we  show  that  the  energy  consumption  can  be  reduced  on  average  by  70%  using  an  opportunistic  update  strategy  sending  position  updates  together  with  messages  of  other  applications.  We  present  a  Markov  model  to  predict  the  arrival  of  messages  and  an  online  optimization  algorithm  calculating  an  optimized  schedule  to  send  position  updates.
2	Awe  an  animated  work  environment  for  working  with  physical  and  digital  tools  and  artifacts.  We  discuss  the  design,  development,  and  testing  of  animated  work  environment  (AWE),  a  novel,  programmable,  AWE  supporting  everyday  human  activities  at  work,  at  home,  or  at  school  in  an  increasingly  digital  society.  A  physical  example  of  the  emerging  genre  of  "architectural  robotics,"  AWE  features  a  programmable,  reconfigurable  "wall,"  three  horizontal,  mobile  work-surfaces,  and  embedded  information  technologies.  AWE  is  the  result  of  an  iterative  design  process  involving  surveys,  task  analyses,  virtual  and  physical  prototyping,  and  usability  testing  accomplished  by  a  transdisciplinary  team  of  engineers,  architects,  sociologists,  and  human  factors  psychologists.  Usability  testing  has  demonstrated  AWE's  potential  to  enhance  working  life:  AWE  adapts  to  variations  in  complex  activities  involving  users  working  in  one  physical  place  with  physical  and  digital  tools  and  artifacts.
2	Hear  the  heart  daily  cardiac  health  monitoring  using  ear  ecg  and  machine  learning.  Daily  cardiac  health  monitoring  is  of  high  importance  for  effective  heart  disease  prediction/management.  In  this  study,  we  propose  a  novel  ear-worn  system  for  long-term  continuous  ECG  QRS  duration  tracking,  to  overcome  challenges  of  current  wearable  ECG  systems  such  as  the  uncomfortableness  and  inconvenience.  Specifically,  we  place  all  the  ECG  electrodes  behind  the  ear  to  enhance  the  wearability,  and  weak/noisy  ear-ECG  is  obtained.  Then,  we  use  a  support  vector  machine  classifier  for  heartbeat  identification,  apply  an  unsupervised  learning  approach  for  heartbeat  purification,  and  a  regression  model  to  derive  the  standard  chest-ECG  QRS  durations  from  the  ear-ECG  QRS  durations.  We  have  evaluated  the  proof-of-concept  system  using  an  ear-ECG  dataset  acquired  by  a  semi-customized  wearable  prototype,  and  demonstrated  the  effectiveness  of  the  proposed  system.  To  the  best  of  our  knowledge,  it  is  the  first  study  on  an  ear-worn  system  for  ECG  QRS  duration  estimation,  which  can  be  used  in  daily  cardiac  health  monitoring  applications.
2	Failures  in  sharing  personal  data  on  social  networking  sites.  Sharing  personal  informatics  data  to  social  networking  sites  is  a  common  and  well-studied  practice  in  both  research  and  commercial  applications,  but  there  have  been  substantial  mistakes  and  failures  within  this  space  that  offer  important  lessons  to  application  developers.  We  discuss  three  common  types  of  failures  salient  in  our  own  work,  other  research,  and  popular  press  stories.  These  failures  surface  important  open  questions  to  the  field  of  personal  informatics.
2	Ubiquitous  fair  bandwidth  allocation  for  multimedia  traffic  on  a  wimax  mesh  network  with  multi  channels.  WiMAX  technology  can  be  used  as  ''last  mile  ubiquitous''  broadband  connections  to  deliver  streaming  audio  or  video  to  clients.  Thus,  Quality  of  Service  (QoS)  is  very  important  for  WiMAX  networks.  Providing  QoS  in  multi-hop  WiMAX  networks  such  as  WiMAX  mesh  networks  is  challenging  since  the  WiMAX  mesh  networks  MAC  is  connectionless  based.  The  scheduling  function  plays  a  crucial  role  in  QoS  support.  However,  multimedia  traffic  is  bursty  in  nature,  therefore,  in  this  paper,  we  will  discuss  the  potential  unfairness  that  bursty  traffic  may  be  subjected  to  and  propose  a  new  frame-based  packet  scheduling  algorithm.
2	Qualitative  investigation  of  multi  device  notifications.  Users  are  confronted  with  more  and  more  notifications  in  their  lives.  Multiple  device  types  in  the  users'  environment  use  visual,  tactile  and  auditory  cues  to  inform  them  about  messages,  events,  and  updates.  All  these  devices  differ  in  the  used  modalities  to  inform  the  users  and  in  the  offered  configuration  options  for  these  modalities.  Prior  work  investigated  the  distracting  effects  of  notifications  and  how  people  interact  with  notifications.  However,  related  work  often  only  focuses  on  one  platform  at  a  time.  Instead,  we  use  interviews  to  investigate  how  users  experience  and  deal  with  notifications  generated  by  their  different  devices  in  their  everyday  lives.  Our  results  show  that  users  developed  strategies  to  deal  with  notifications  on  their  devices  such  as  disabling  (or  not  enabling)  notifications,  uninstalling  applications,  using  do-not-disturb  functionality,  muting  devices  or  even  putting  devices  away.  Only  few  users  change  the  notification  settings  on  their  devices.  As  a  consequence,  the  default  settings  selected  by  the  device  manufactures  can  drastically  change  how  notifications  are  affecting  users.
2	Keystroke  logs  are  strong  passwords  enough.  As  hackers  develop  sophisticated  phishing  and  social  engineering  attacks,  it  is  recommended  that  users  be  aware  of  common  tactics  and  implement  stronger  and  unique  passwords  for  sensitive  accounts.  Malware  typically  involved  in  cyber-attacks  includes  viruses  and  worms.  Keyloggers  are  designed  to  track  and  capture  keystrokes  made  on  devices.  They  are  not  given  the  same  priority  as  the  other  types  of  malware,  but  are  also  considered  malicious  as  it  poses  the  same  level  of  risk.  Having  a  strong  password  in  this  instance  does  not  provide  a  high  level  of  protection  as  keyloggers  will  log  each  key  typed.  This  paper  will  discuss  the  characteristics  of  keylogging  software  while  providing  a  summary  on  methods  of  protection.  Three  keylogging  software  are  tested  against  two  anti-keylogging  programs  to  identify  what  information  is  captured  and  which  method  of  protection  is  stronger.  A  discussion  on  risk  assessment  with  regards  to  keyloggers  and  remediation  techniques  is  also  included.
2	An  exploration  with  online  complex  activity  recognition  using  cellphone  accelerometer.  We  investigate  the  problem  of  online  detection  of  complex  activities  (such  as  cooking,  lunch,  work  at  desk),  i.e.,  recognizing  them  while  the  activities  are  being  performed  using  parts  of  the  sensor  data.  In  contrast  to  prior  work,  where  complex  activity  recognition  is  performed  offline  with  the  observation  of  the  activity  available  for  its  entire  duration  and  utilizing  deeply-instrumented  environments,  we  focus  on  online  activity  detection  using  only  accelerometer  data  from  a  single  body-worn  smartphone  device.  We  present  window  based  algorithms  for  online  detection  that  effectively  perform  different  tradeoffs  between  classification  accuracy  and  detection  latency.  We  present  results  of  our  exploration  using  a  longitudinally-extensive  and  clearly-annotated  cellphone  accelerometer  data  trace  that  captures  the  true-life  complex  activity  behavior  of  five  subjects.
2	Reinforcement  learning  control  for  water  efficient  agricultural  irrigation.  Modern  sensor  technologies,  internet  and  advanced  irrigation  equipment  allow  a  relative  precise  control  of  agricultural  irrigation  that  leads  to  high  water-use  efficiency.  However,  the  core  control  algorithms  that  make  use  of  these  technologies  have  not  been  well  studied.  In  this  work,  a  reinforcement  learning  based  irrigation  control  technique  is  investigated.  The  delayed  reward  of  crop  yield  is  handled  by  the  temporal  difference  technique.  The  learning  process  can  be  based  on  both  off-line  simulation  and  real  data  from  sensors  and  crop  yield.  Neural  network  based  fast  models  for  soil  water  level  and  crop  yield  are  developed  to  improve  the  scalability  of  learning.  Simulations  for  various  geographic  locations  and  crop  types  show  that  the  proposed  method  can  significantly  increase  net  return  considering  both  crop  yield  and  water  expense.
2	A  joint  resource  allocation  scheme  for  ofdma  based  wireless  pervasive  networks  with  carrier  aggregation.  The  mixture  of  users  with  different  capabilities  of  Carrier  Aggregation  CA  presents  new  challenges  to  optimise  the  performance  of  the  next  generation  wireless  pervasive  networks.  This  paper  focuses  on  the  joint  resources  allocation  for  OFDMA-based  multi-carrier  system,  with  the  objective  of  maximising  the  network  utility.  Distinguished  from  many  existing  methods,  our  approach  combines  dynamic  spectrum  resources  assignment  and  adaptive  power  allocation  technologies  for  the  CA  networks.  We  propose  a  novel  scheme  with  Joint  Component  Carrier  CC,  Resource  Block  RB  and  Power  Allocation  JCRPA,  where  the  Minimising  System  Utility  Loss  MSUL  algorithm  is  developed  to  allocate  both  CCs  and  RBs  for  different  users  with  various  CA  capabilities.  Furthermore,  with  the  cooperation  of  the  JCRPA  scheme,  a  CC-switch  control  mechanism  is  devised  to  reduce  the  overhead  due  to  frequent  CC  switching.  Simulation  results  demonstrate  that  JCRPA  is  able  to  significantly  improve  the  system  performance  in  terms  of  network  utility,  average  throughput  and  transmission  fairness.
2	Link  preserving  interference  minimisation  channel  assignment  in  multi  radio  wireless  mesh  networks.  Using  multiple  channels  with  multiple  radios  per  node  in  a  wireless  mesh  network  can  potentially  improve  system  capacity.  This  design  requires  the  appropriate  assignments  of  channels/radios  to  wireless  backhaul  links  and  creates  a  trade-off  between  conflicting  constraints  and  requirements.  This  study  attempts  to  maximise  the  number  of  operative  links,  where  a  link  is  operative  if  radios  at  both  ends  of  the  link  share  a  common  channel  i.e.,  are  link-preserving  and  experience  sufficiently  low  co-channel  interference.  These  two  criteria  are  conflicting  in  nature.  A  link-centric,  channel-first  radio  resource  assignment  scheme  that  considers  physical  interference  model  and  tight  radio  constraint  is  proposed.  The  proposed  approach  ensures  link  preservation  and  assigns  channels  to  links  based  on  the  predicted  upper  bound  and  lower  bound  of  the  accumulated  co-channel  interference  associated  with  particular  assignments.  Simulation  results  indicate  that  the  proposed  algorithm  outperforms  existing  approaches  in  the  number  of  operative  links,  particularly  when  only  a  few  channels,  or  many  radios,  are  available.
2	Lifelogging  for  observer  view  memories  an  infrastructure  approach.  Lifelogging  has  much  to  offer  human  memory.  Traditional  lifelogging  techniques  use  wearable  cameras  to  capture  a  first-person  or  'field'  view.  We  propose  an  alternative  or  complementary  approach  in  which  fixed  infrastructure  cameras  provide  a  third-person  or  'observer'  view  of  daily  events.  In  this  paper  we  identify  key  advantages  and  challenges  for  a  fixed  infrastructure  approach  to  lifelogging.
2	Control  and  scheduling  interface  for  public  displays.  Social  media  platforms  such  as  Flicker,  Twitter,  Instagram,  and  Facebook  have  opened  up  new  possibilities  for  providing  content  on  large  public  displays.  Integrating  interactive  elements  in  a  public  display,  such  as  (virtual)  Keyboards  and  Webcams,  can  additionally  stimulate  in-situ  content  production.  Both  social  media  content  and  such  in-situ  content  are  cheap  to  produce,  always  fresh,  and  potentially  community  sourced,  thus  increasing  relevance  for  passersby.  However,  not  all  social  media  applications  and  content  entries  may  be  appropriate  in  a  particular  display  setting  and  showing  user  contributed  content  on  public  displays  requires  new  forms  of  content  control  and  scheduling.  In  this  demo  we  show:  1)  a  control  interface  for  display  owners  to  manage  the  overall  behavior  of  their  displays,  and  2)  post-moderation  mechanisms  for  controlling  and  removing  potentially  inappropriate  user  contributed  content  from  public  displays.  The  control  interface  and  moderation  mechanisms  are  designed  for  a  university  environment  and  were  inspired  by  two  short  pilot  test  deployments  and  a  focus  group  with  the  university  officials.
2	Inmyday  a  digital  diary  to  promote  self  care  among  elders.  Diaries  allow  users  to  record  personal  events  and  experiences,  and  are  frequently  used  to  collect  participant  data  in  user  studies.  Digital  diaries  have  several  benefits  over  traditional  paper-based  diaries,  reducing  respondents’  burden,  administrative  costs,  and  improving  navigation.  However,  for  elderly  users,  there  are  several  challenges  in  the  use  of  a  digital  diary:  they  may  have  cognitive  and  motor  impairments,  and  fewer  digital  skills  than  other  populations.  We  implemented  a  digital  diary  called  InMyDay,  specifically  designed  for  elderly  users.  The  goal  of  this  diary  is  to  promote  self-care  and  self-reflection,  by  allowing  users  to  register  their  activities  and  emotions.  Ten  elderly  users  tested  the  diary  for  five  days,  recording  entries  related  to  their  days  and  how  they  felt.  All  of  the  participants  used  the  diary  every  day  and  after  the  experiment,  nine  declared  that  they  would  use  such  an  application  at  least  once  a  week.  We  found  that  the  diary  promoted  reflection,  that  users  felt  that  this  allowed  them  a  moment  of  self-care  during  their  day,  and  that  they  felt  this  was  especially  important  for  them  as  elderly  people.  Future  work  will  focus  on  increasing  the  number  of  participants  and  emotions  that  may  be  reported  and  exploring  new  mechanisms  of  interaction.
2	Novative  rendering  and  physics  engines  to  apprehend  special  relativity.  Relativity,  as  introduced  by  Einstein,  is  regarded  as  one  of  the  most  important  revolutions  in  the  history  of  physics.  Nevertheless,  the  observation  of  direct  outcomes  of  this  theory  on  mundane  objects  is  impossible  because  they  can  only  be  witnessed  when  relative  velocities  close  the  speed  of  light  are  involved.  These  effects  are  so  counterintuitive  and  contradicting  with  our  daily  understanding  of  space  and  time  that  physics  students  find  it  hard  to  learn  Special  Relativity  beyond  mathematical  equations  and  to  understand  the  deep  implications  of  the  theory.    Although  we  cannot  travel  at  the  speed  of  light  for  real,  Virtual  Reality  makes  it  possible  to  experiment  the  effects  of  relativity  in  a  3D  immersive  environment.  Our  project  is  a  framework  designed  to  merge  advanced  3D  graphics  with  Virtual  Reality  interfaces  in  order  to  create  an  appropriate  environment  to  study  and  learn  relativity  as  well  as  to  develop  some  intuition  of  the  relativistic  effects  and  the  quadri-dimensional  reality  of  space-time.    In  this  paper,  we  focus  on  designing  and  implementing  an  easy-to-use  game-like  application  :  a  carom  billiard.  Our  implementation  includes  relativistic  effects  in  an  innovative  graphical  rendering  engine  and  a  non-Newtonian  physics  engine  to  treat  the  collisions.    The  innovation  of  our  approach  lies  in  the  ability  i)  to  render  in  real-time  several  relativistic  objects,  each  moving  with  a  different  velocity  vector  (contrary  to  what  was  achieved  in  previous  works),  ii)  to  allow  for  interactions  between  objects,  and  iii)  to  enable  the  user  to  interact  with  the  objects  and  modify  the  scene.    To  achieve  this,  we  implement  the  4D  nature  of  space-time  directly  at  the  heart  of  the  rendering  engine,  and  develop  an  algorithm  allowing  to  access  non-simultaneous  past  events  that  are  visible  to  the  observers  at  their  specific  locations  and  at  a  given  instant  of  their  proper  time.  We  explain  how  to  retrieve  the  collision  event  between  the  pucks  and  the  cushions  of  the  billiard  game  and  we  show  several  counterintuitive  results  for  very  fast  pucks.  The  effectiveness  of  the  approach  is  demonstrated  with  snapshots  of  videos  where  several  independent  objects  travel  at  velocities  close  to  the  speed  of  light,  c.
2	Constructive  visual  analytics  for  text  similarity  detection.  Detecting  similarity  between  texts  is  a  frequently  encountered  text  mining  task.  Because  the  measurement  of  similarity  is  typically  composed  of  a  number  of  metrics,  and  some  measures  are  sensitive  to  subjective  interpretation,  a  generic  detector  obtained  using  machine  learning  often  has  difficulties  balancing  the  roles  of  different  metrics  according  to  the  semantic  context  exhibited  in  a  specific  collection  of  texts.  In  order  to  facilitate  human  interaction  in  a  visual  analytics  process  for  text  similarity  detection,  we  first  map  the  problem  of  pairwise  sequence  comparison  to  that  of  image  processing,  allowing  patterns  of  similarity  to  be  visualized  as  a  2D  pixelmap.  We  then  devise  a  visual  interface  to  enable  users  to  construct  and  experiment  with  different  detectors  using  primitive  metrics,  in  a  way  similar  to  constructing  an  image  processing  pipeline.  We  deployed  this  new  approach  for  the  identification  of  commonplaces  in  18th-century  literary  and  print  culture.  Domain  experts  were  then  able  to  make  use  of  the  prototype  system  to  derive  new  scholarly  discoveries  and  generate  new  hypotheses.
2	Classification  in  cryo  electron  tomograms.  Different  imaging  techniques  allow  us  to  study  the  organization  of  life  at  different  scales.  Cryo-electron  tomography  (cryo-ET)  has  the  ability  to  three-dimensionally  visualize  the  cellular  architecture  as  well  as  the  structural  details  of  macro-molecular  assemblies  under  near-native  conditions.  Due  to  beam  sensitivity  of  biological  samples,  an  inidividual  tomogram  has  a  maximal  resolution  of  5  nanometers.  By  averaging  volumes,  each  depicting  copies  of  the  same  type  of  a  molecule,  resolutions  beyond  4  A  have  been  achieved.  Key  in  this  process  is  the  ability  to  localize  and  classify  the  components  of  interest,  which  is  challenging  due  to  the  low  signal-to-noise  ratio.  Innovation  in  computational  methods  remains  key  to  mine  biological  information  from  the  tomograms.  To  promote  such  innovation,  we  organize  this  SHREC  track  and  provide  a  simulated  dataset  with  the  goal  of  establishing  a  benchmark  in  localization  and  classification  of  biological  particles  in  cryo-electron  tomograms.  The  publicly  available  dataset  contains  ten  reconstructed  tomograms  obtained  from  a  simulated  cell-like  volume.  Each  volume  contains  twelve  different  types  of  proteins,  varying  in  size  and  structure.  Participants  had  access  to  9  out  of  10  of  the  cell-like  ground-truth  volumes  for  learning-based  methods,  and  had  to  predict  protein  class  and  location  in  the  test  tomogram.  Five  groups  submitted  eight  sets  of  results,  using  seven  different  methods.  While  our  sample  size  gives  only  an  anecdotal  overview  of  current  approaches  in  cryo-ET  classification,  we  believe  it  shows  trends  and  highlights  interesting  future  work  areas.  The  results  show  that  learning-based  approaches  is  the  current  trend  in  cryo-ET  classification  research  and  specifically  end-to-end  3D  learning-based  approaches  achieve  the  best  performance.
2	Sync  mac  protocol  to  control  underwater  vehicle  based  on  underwater  acoustic  communication.  In  this  paper,  we  propose  a  sync-mechanism  optimized  for  MAC  protocol,  which  can  be  appropriate  mechanism  for  controlling  an  underwater  vehicle  efficiently  based  on  underwater  acoustic  communication.  At  present,  underwater  vehicle  control  system  has  difficulties  and  limits  due  to  some  underwater  restrictions.  For  controlling  the  underwater  vehicle,  acoustic  communication  has  many  different  features,  unlike  terrestrial  wireless  communication.  The  existing  medium  has  limited  wireless  bandwidth,  high  transmission  energy  consumption  and  long  propagation  delay.  And  most  of  existing  underwater  MAC  protocols  haven't  considered  yet  about  nodes  mobility,  as  well  underwater  vehicles.  In  this  paper,  we  present  our  proposed  sync-mechanism  to  reflect  above-mentioned  underwater  features  and  we  describe  the  methods  to  prevent  data  collision.
2	Solar  production  prediction  based  on  non  linear  meteo  source  adaptation.  This  work  presents  a  data-intensive  solution  to  predict  Photovoltaique  energy  (PV)  production.  PV  and  other  renewable  sources  have  widely  spread  in  recent  years.  Although  those  sources  provide  an  environmentally-friendly  solution,  their  integration  is  a  real  challenge  in  terms  of  power  management  as  it  depends  on  meteorological  conditions.  The  ability  to  predict  those  variable  sources  considering  meteorological  uncertainty  plays  a  key  role  in  the  management  of  the  energy  supply  needs  and  reserves.  This  paper  presents  an  easy-to-use  methodology  to  predict  PV  production  using  time  series  analyses  and  sampling  algorithms.  The  aim  is  to  provide  a  forecasting  model  to  set  the  day-ahead  grid  electricity  need.  This  information  useful  for  power  dispatching  plans  and  grid  charge  control.  The  main  novelties  of  our  approach  is  to  provide  an  easy  implemented  and  flexible  solution  that  combines  classification  algorithms  to  predict  the  PV  plant  efficiency  considering  weather  conditions  and  nonlinear  regression  to  predict  weather  forecasted  errors  in  order  to  improve  prediction  results.  The  results  are  based  on  the  data  collected  in  the  Technople's  micro  grid  in  Sierre  (Switzerland)  described  further  in  the  paper.  The  best  experimental  results  have  been  obtained  using  hourly  historical  weather  measures  (radiation  and  temperature)  and  PV  production  as  training  inputs  and  weather  forecasted  parameters  as  prediction  inputs.  Considering  a  10  month  dataset  and  despite  the  presence  of  17  missing  days,  we  achieved  a  Percentage  Mean  Absolute  Deviation  (PMAD)  of  20%  in  August  and  21%  in  September.  Better  results  can  be  obtained  with  a  larger  dataset  but  as  more  historical  data  were  not  available,  other  months  have  not  been  tested.
2	Wireless  networked  omni  directional  video  distribution  system  based  on  delay  tolerant  network  on  disaster  environment.  In  this  paper,  we  introduce  Wireless  Networked  Omni-directional  video  collect  and  distribution  System  for  both  normal  and  challenged  network  environment.  Omni  directional  video  camera  is  used  to  capture  the  360  degree  of  surround  image  with  various  sensor  including  GPS  location,  3-densional  gyro-sensor,  temperature  sensor  data.  Those  data  are  transmitted  to  all  of  the  users  through  Web  system.  In  normal  network  condition,  the  conventional  IP  protocol  over  mobile  network  or  wireless  network  are  used  to  access  to  Web  server  through  Internet.  The  delay  tolerant  network  protocol  (DTN)  is  used  between  the  vehicles  and  the  web  server  to  support  challenged  network  environment  such  as  mountain  road  and  disaster  area.  The  user  can  see  the  live  video  and  recorded  video  along  the  road  with  360  degree  surround  on  GIS  map  as  Web  services.  The  system  configuration  and  architecture  are  explained  and  a  prototype  system  is  constructed  to  evaluate  the  functional  and  performance.  Through  the  performance  evaluation,  the  usefulness  and  effects  of  the  proposed  system  is  validated.
2	Mobile  gateway  for  ubiquitous  health  care  system  using  zigbee  and  bluetooth.  A  ubiquitous  health  care  system  takes  the  advantage  of  portability  and  small  size  of  wireless  sensor  nodes  to  provide  remote  health  care  services  and  real-time  health  monitoring.  A  gateway  is  needed  to  mediate  communication  between  a  local  sensor  network  and  remote  data  consumers.  In  current  implementations  of  ubiquitous  health  care  systems  ZigBee-based  sensors  are  often  used  to  gather  vital  sign  data  such  as  ECG  and  heart  rate.  Transferring  large  quantities  of  vital  signs  from  a  ZigBee-based  sensor  sink  node  to  a  gateway  requires  a  bandwidth  that  ZigBee  alone  is  not  able  to  provide.  In  this  paper,  we  present  a  technical  design  of  a  Bluetooth-based  mobile  gateway  that  bridges  the  connection  between  a  sensor  network  and  the  Internet.  Our  system  enables  ubiquitous  health  care  experience  while  providing  a  platform  for  additional  services  such  as  alarms,  notifications  and  analysis  of  medical  data.  Controlling  a  sensor  network  from  the  mobile  gateway  is  also  possible.  The  flexible  design  of  the  system  does  not  restrict  its  usage  only  to  health  care  services  -  the  gateway  can  be  configured  to  work  with  any  kind  of  sensor  network  having  a  sink  node  with  Bluetooth  capability.
2	User  comfort  oriented  residential  power  scheduling  in  smart  homes.  Smart  grid  is  an  emerging  technology  which  is  considered  as  an  ultimate  solution  to  meet  the  increasing  power  demand  challenges.  Modern  communication  technologies  has  enabled  the  successful  implementation  of  smart  grid,  which  aims  at  provision  of  demand  side  management  mechanisms,  such  as  demand  response.  In  this  paper,  we  propose  residential  load  scheduling  model  for  demand  side  management.  It  is  assumed  that  electric  prices  are  announced  on  day-ahead  basis.  The  major  focus  of  this  work  is  to  minimize  consumer  electricity  bill  at  minimum  user  discomfort.  Load  scheduling  is  formulated  as  an  optimization  problem,  and  an  optimal  schedule  is  achieved  by  solving  the  minimization  problem.  Simulation  results  validate  that  teacher  learning  based  optimization  performs  better  as  compared  to  genetic  algorithm,  showing  comparable  results  with  linear  programming  with  less  computational  efforts.  TLBO  is  able  to  obtain  the  desired  trade-off  between  consumer  electric  bill  and  user  discomfort.
2	Contention  window  prioritization  for  heterogeneous  traffic  in  wireless  sensor  networks.  Dealing  with  heterogeneous  traffic  has  become  a  major  MAC  layer  challenge  for  the  Wireless  Sensor  Networks  (WSN)  due  to  emerging  applications.  Most  of  the  times,  the  service  requirements  for  WSN  applications  differ  based  on  the  traffic  type  or  priority.  Among  many  other  schemes,  prioritization  of  Contention  Window  (CW)  has  been  proposed  in  the  past  which  offers  advantage  to  the  traffic  of  high  priority.  This  research  aims  at  analyzing  the  influence  of  prioritized  CW  scheme  for  the  basic  Carrier  Sense  Multiple  Access  (CSMA)  protocol,  frequently  deployed  for  the  contention  based  channels.  The  performance  of  CSMA  with  prioritized  CW  while  dealing  with  heterogeneous  traffic  has  been  compared  against  the  basic  CSMA.  Two  types  of  traffics  have  been  used  to  illustrate  heterogeneous  traffic  and  evaluate  the  proposed  scheme.  Performance  metrics  of  throughput  and  average  delay  have  been  investigated  in  order  to  validate  the  advantage  of  using  prioritized  CW  partitioning.  The  scheme  has  been  implemented  using  Tiny-OS  source  code  developed  for  the  Mica2  platform.  Avrora  emulator  has  been  used  for  conducting  experiments.  results  have  revealed  a  positive  impact  of  using  CSMA  with  prioritized  CW  on  the  network  performance  as  compared  to  the  basic  CSMA.
2	Future  internet  in  taiwan  design  and  research  activities  over  twaren  network.  Internet  has  played  an  important  part  in  the  success  of  information  technologies.  With  growing  and  changing  demands,  there  are  many  limitations  faced  by  current  Internet.  A  number  of  network  test  beds  are  created  for  solving  a  set  of  specific  problems  in  Internet.  Traditionally,  these  test  beds  are  lacking  of  large  scale  network  and  flexibility.  Therefore,  it  is  necessary  to  design  and  implement  a  test  bed  which  can  support  wide  range  of  experiments  and  has  an  ability  of  programmable  network.  Besides,  there  has  been  a  big  change  enabled  by  cloud  computing  in  recent  years.  Everything  is  virtualized,  including  networking.  We  focus  on  integrating  management  functions  in  virtual  network.  In  this  paper,  we  design  and  create  a  Future  Internet  test  bed  in  Taiwan  over  TWAREN  Research  Network.  This  test  bed  evolves  into  an  environment  for  programmable  network  and  cloud  computing.  This  paper  also  presents  several  finished  and  ongoing  experiments  on  the  test  bed  for  multiple  aspects  including  topology  discovery,  multimedia  streaming,  and  virtual  network  integration.  We  will  continue  to  extend  our  test  bed  and  propose  innovative  applications  for  the  next  generation  Internet.
2	A  vegetable  category  recognition  system  using  deep  neural  network.  In  recent  years,  Ambient  Intelligence  (AmI)  has  attracted  increased  attention  within  the  advanced  technology  industry  in  an  effort  to  modernize  and  develop  a  more  intelligent  and  reliable  information  system.  Technologies  to  detect  a  specific  object  in  images  are  expected  to  further  expand  to  wide  range  of  applications,  such  as  car  detection  functions  for  intelligent  transport  system  and  other  systems.  Computer  vision  and  pattern  recognition  are  emerging  fast  and  will  continue  to  grow  together  with  local  feature  detection  methods.  In  this  paper,  we  used  the  Deep  Neural  Network  (DNN)  for  object  category  recognition  by  extracting  and  learning  the  object.  We  applied  deep  learning  to  vegetable  object  recognition,  and  explored  the  Convolutional  Neural  Network  (CNN).  From  the  evaluation  results,  we  found  that  for  vegetable  recognition  learning  process  with  CNN,  3  million  iterations  were  suitable.  The  results  of  learning  rate  was  99.14%  and  recognition  rate  was  97.58%,  respectively.
2	Anatomy  of  a  crowdsourcing  platform  using  the  example  of  microworkers  com.  Since  Jeff  Howe  introduced  the  term  "crowdsourcing"  in  2006  for  the  first  time,  crowd  sourcing  has  be  come  a  growing  market  in  the  current  Internet.  Thousands  of  workers  categorize  images,  write  articles  or  perform  other  small  tasks  on  platforms  like  Amazon  Mechanical  Turk  (MTurk),  Micro  workers  or  Short  Task.  In  this  work,  we  want  to  give  an  inside  view  of  the  usage  data  from  Micro  workers  and  show  that  there  are  significant  differences  to  the  well  studied  MTurk.  Further,  we  have  a  look  at  Micro  workers  from  the  perspective  for  a  worker,  an  employer  and  the  platform  owner,  in  order  to  answer  their  most  important  questions:  What  jobs  are  most  paid?  How  do  I  get  my  work  done  most  quickly?  When  are  the  users  of  my  platform  active?
