label	text_a
0	An  analysis  of  the  effect  of  emotional  speech  synthesis  on  non  task  oriented  dialogue  system.  This  paper  explores  the  effect  of  emotional  speech  synthesis  on  a  spoken  dialogue  system  when  the  dialogue  is  non-task-oriented.  Although  the  use  of  emotional  speech  responses  have  been  shown  to  be  effective  in  a  limited  domain,  e.g.,  scenario-based  and  counseling  dialogue,  the  effect  is  still  not  clear  in  the  non-task-oriented  dialogue  such  as  voice  chatting.  For  this  purpose,  we  constructed  a  simple  dialogue  system  with  example-  and  rule-based  dialogue  management.  In  the  system,  two  types  of  emotion  labeling  with  emotion  estimation  are  adopted,  i.e.,  system-driven  and  user-cooperative  emotion  labeling.  We  conducted  a  dialogue  experiment  where  subjects  evaluate  the  subjective  quality  of  the  system  and  the  dialogue  from  the  multiple  aspects  such  as  richness  of  the  dialogue  and  impression  of  the  agent.  We  then  analyze  and  discuss  the  results  and  show  the  advantage  of  using  appropriate  emotions  for  the  expressive  speech  responses  in  the  non-task-oriented  system.
0	Toward  real  time  accurate  fall  fall  recovery  detection  system  by  incorporating  activity  information.  This  study  presents  a  detailed  summary  of  automatic  fall  detection  system  based  on  wearable  sensor(s)  and  a  real-time  fall  detection  system  prototype  developed  based  on  Java  Expert  System  Shell  (JESS),  featuring  the  fall,  fall  recovery,  fall  direction  and  activity  status  before/after  fall.  Through  the  rule  sets  in  the  knowledge  base,  we  illustrate  how  the  activity  information  can  be  used  to  indicate  the  fall  recovery  and  fall  direction,  as  well  as  enhancing  the  accuracy  of  fall  detection.  The  system  has  been  validated  against  13  types  of  falls  and  12  ADLs  acquired  from  12  subjects.
0	A  tongue  activated  emergency  beacon  for  immobile  patients.  This  paper  reports  the  design  and  implementation  of  an  infrared  signal  transmitting  tongue  activated  emergency  beacon.  This  low-cost,  simple  and  reliable  device  can  help  immobile  patients  communicate  with  the  medical  staff  in  the  event  of  an  emergency  without  interfering  with  other  equipment.  The  physical  dimensions  of  this  device  were  minimized  to  provide  flexibility  and  suit  the  most  vulnerable  and  impaired  patients.  The  presented  sensor-microcontroller  configuration  results  in  a  robust  and  intelligent  functionality  that  would  allow  this  device  to  outperform  many  of  the  commercially  available  systems  used  in  similar  environments.
0	Racing  multi  objective  selection  probabilities.  In  the  context  of  Noisy  Multi-Objective  Optimization,  dealing  with  uncertainties  requires  the  decision  maker  to  define  some  preferences  about  how  to  handle  them,  through  some  statistics  (e.g.,  mean,  median)  to  be  used  to  evaluate  the  qualities  of  the  solutions,  and  define  the  corresponding  Pareto  set.  Approximating  these  statistics  requires  repeated  samplings  of  the  population,  drastically  increasing  the  overall  computational  cost.  To  tackle  this  issue,  this  paper  proposes  to  directly  estimate  the  probability  of  each  individual  to  be  selected,  using  some  Hoeffding  races  to  dynamically  assign  the  estimation  budget  during  the  selection  step.  The  proposed  racing  approach  is  validated  against  static  budget  approaches  with  NSGA-II  on  noisy  versions  of  the  ZDT  benchmark  functions.
0	Anti  logarithmic  quantization  for  data  reduction  in  multi  channel  intra  cortical  neural  recording  implants.  In  this  paper,  the  anti-logarithmic  quantization  approach,  recently  proposed  by  the  authors  to  be  used  in  multi-channel  implantable  neural  recording  systems  is  studied  in  depth.  It  is  shown  that  not  only  quantization  noise  does  not  degrade  the  quality  of  the  neural  signals  being  digitized,  it  helps  reject  significant  portion  of  their  noise  content.  As  a  result  of  using  this  method  of  quantization,  bitrate  and  power  saving  is  achieved  in  the  system  level.  To  quantify  the  properties  associated  with  the  proposed  quantization  method,  a  four-channel  proof-of-concept  prototype  is  developed  and  tested  using  prerecorded  neural  data.
0	Runtime  analysis  of  evolutionary  algorithms  on  randomly  constructed  high  density  satisfiable  3  cnf  formulas.  We  show  that  simple  mutation-only  evolutionary  algorithms  find  a  satisfying  assignment  on  two  similar  models  of  random  planted  3-CNF  Boolean  formulas  in  polynomial  time  with  high  probability  in  the  high  constraint  density  regime.  We  extend  the  analysis  to  random  formulas  conditioned  on  satisfiability  (i.e.,  the  so-called  filtered  distribution)  and  conclude  that  most  high-density  satisfiable  formulas  are  easy  for  simple  evolutionary  algorithms.  With  this  paper,  we  contribute  the  first  rigorous  study  of  randomized  search  heuristics  from  the  evolutionary  computation  community  on  well-studied  distributions  of  random  satisfiability  problems.
0	Population  exploration  on  genotype  networks  in  genetic  programming.  Redundant  genotype-to-phenotype  mappings  are  pervasive  in  evolutionary  computation.  Such  redundancy  allows  populations  to  expand  in  neutral  genotypic  regions  where  mutations  to  a  genotype  do  not  alter  the  phenotypic  outcome.  Genotype  networks  have  been  proposed  as  a  useful  framework  to  characterize  the  distribution  of  neutrality  among  genotypes  and  phenotypes.  In  this  study,  we  examine  a  simple  Genetic  Programming  model  that  has  a  finite  and  compact  genotype  space  by  characterizing  its  genotype  networks.  We  study  the  topology  of  individual  genotype  networks  underlying  unique  phenotypes,  investigate  the  genotypic  properties  as  vertices  in  genotype  networks,  and  discuss  the  correlation  of  these  network  properties  with  robustness  and  evolvability.  Using  GP  simulations  of  a  population,  we  demonstrate  how  an  evolutionary  population  diffuses  on  genotype  networks.
0	Maximizing  submodular  or  monotone  functions  under  partition  matroid  constraints  by  multi  objective  evolutionary  algorithms.  Many  important  problems  can  be  regarded  as  maximizing  submodular  functions  under  some  constraints.  A  simple  multi-objective  evolutionary  algorithm  called  GSEMO  has  been  shown  to  achieve  good  approximation  for  submodular  functions  efficiently.  While  there  have  been  many  studies  on  the  subject,  most  of  existing  run-time  analyses  for  GSEMO  assume  a  single  cardinality  constraint.  In  this  work,  we  extend  the  theoretical  results  to  partition  matroid  constraints  which  generalize  cardinality  constraints,  and  show  that  GSEMO  can  generally  guarantee  good  approximation  performance  within  polynomial  expected  run  time.  Furthermore,  we  conducted  experimental  comparison  against  a  baseline  GREEDY  algorithm  in  maximizing  undirected  graph  cuts  on  random  graphs,  under  various  partition  matroid  constraints.  The  results  show  GSEMO  tends  to  outperform  GREEDY  in  quadratic  run  time.
0	Bilingual  word  embeddings  from  parallel  and  non  parallel  corpora  for  cross  language  text  classification.  In  many  languages,  sparse  availability  of  resources  causes  numerous  challenges  for  textual  analysis  tasks.  Text  classification  is  one  of  such  standard  tasks  that  is  hindered  due  to  limited  availability  of  label  information  in  lowresource  languages.  Transferring  knowledge  (i.e.  label  information)  from  high-resource  to  low-resource  languages  might  improve  text  classification  as  compared  to  the  other  approaches  like  machine  translation.  We  introduce  BRAVE  (Bilingual  paRAgraph  VEctors),  a  model  to  learn  bilingual  distributed  representations  (i.e.  embeddings)  of  words  without  word  alignments  either  from  sentencealigned  parallel  or  label-aligned  non-parallel  document  corpora  to  support  cross-language  text  classification.  Empirical  analysis  shows  that  classification  models  trained  with  our  bilingual  embeddings  outperforms  other  stateof-the-art  systems  on  three  different  crosslanguage  text  classification  tasks.
0	Shared  components  topic  models.  With  a  few  exceptions,  extensions  to  latent  Dirichlet  allocation  (LDA)  have  focused  on  the  distribution  over  topics  for  each  document.  Much  less  attention  has  been  given  to  the  underlying  structure  of  the  topics  themselves.  As  a  result,  most  topic  models  generate  topics  independently  from  a  single  underlying  distribution  and  require  millions  of  parameters,  in  the  form  of  multinomial  distributions  over  the  vocabulary.  In  this  paper,  we  introduce  the  Shared  Components  Topic  Model  (SCTM),  in  which  each  topic  is  a  normalized  product  of  a  smaller  number  of  underlying  component  distributions.  Our  model  learns  these  component  distributions  and  the  structure  of  how  to  combine  subsets  of  them  into  topics.  The  SCTM  can  represent  topics  in  a  much  more  compact  representation  than  LDA  and  achieves  better  perplexity  with  fewer  parameters.
0	Unified  expectation  maximization.  We  present  a  general  framework  containing  a  graded  spectrum  of  Expectation  Maximization  (EM)  algorithms  called  Unified  Expectation  Maximization  (UEM.)  UEM  is  parameterized  by  a  single  parameter  and  covers  existing  algorithms  like  standard  EM  and  hard  EM,  constrained  versions  of  EM  such  as  Constraint-Driven  Learning  (Chang  et  al.,  2007)  and  Posterior  Regularization  (Ganchev  et  al.,  2010),  along  with  a  range  of  new  EM  algorithms.  For  the  constrained  inference  step  in  UEM  we  present  an  efficient  dual  projected  gradient  ascent  algorithm  which  generalizes  several  dual  decomposition  and  Lagrange  relaxation  algorithms  popularized  recently  in  the  NLP  literature  (Ganchev  et  al.,  2008;  Koo  et  al.,  2010;  Rush  and  Collins,  2011).  UEM  is  as  efficient  and  easy  to  implement  as  standard  EM.  Furthermore,  experiments  on  POS  tagging,  information  extraction,  and  word-alignment  show  that  often  the  best  performing  algorithm  in  the  UEM  family  is  a  new  algorithm  that  wasn't  available  earlier,  exhibiting  the  benefits  of  the  UEM  framework.
0	Fbk  hlt  a  new  framework  for  semantic  textual  similarity.  This  paper  reports  the  description  and  performance  of  our  system,  FBK-HLT,  participating  in  the  SemEval  2015,  Task  #2  “Semantic  Textual  Similarity”,  English  subtask.  We  submitted  three  runs  with  different  hypothesis  in  combining  typical  features  (lexical  similarity,  string  similarity,  word  n-grams,  etc)  with  syntactic  structure  features,  resulting  in  different  sets  of  features.  The  results  evaluated  on  both  STS  2014  and  2015  datasets  prove  our  hypothesis  of  building  a  STS  system  taking  into  consideration  of  syntactic  information.  We  outperform  the  best  system  on  STS  2014  datasets  and  achieve  a  very  competitive  result  to  the  best  system  on  STS  2015  datasets.
0	Duth  at  semeval  2018  task  2  emoji  prediction  in  tweets.  This  paper  describes  the  approach  that  was  developed  for  SemEval  2018  Task  2  (Multilingual  Emoji  Prediction)  by  the  DUTH  Team.  First,  we  employed  a  combination  of  pre-processing  techniques  to  reduce  the  noise  of  tweets  and  produce  a  number  of  features.  Then,  we  built  several  N-grams,  to  represent  the  combination  of  word  and  emojis.  Finally,  we  trained  our  system  with  a  tuned  LinearSVC  classifier.  Our  approach  in  the  leaderboard  ranked  18th  amongst  48  teams.
0	Imposing  label  relational  inductive  bias  for  extremely  fine  grained  entity  typing.  Existing  entity  typing  systems  usually  exploit  the  type  hierarchy  provided  by  knowledge  base  (KB)  schema  to  model  label  correlations  and  thus  improve  the  overall  performance.  Such  techniques,  however,  are  not  directly  applicable  to  more  open  and  practical  scenarios  where  the  type  set  is  not  restricted  by  KB  schema  and  includes  a  vast  number  of  free-form  types.  To  model  the  underlying  label  correlations  without  access  to  manually  annotated  label  structures,  we  introduce  a  novel  label-relational  inductive  bias,  represented  by  a  graph  propagation  layer  that  effectively  encodes  both  global  label  co-occurrence  statistics  and  word-level  similarities.  On  a  large  dataset  with  over  10,000  free-form  types,  the  graph-enhanced  model  equipped  with  an  attention-based  matching  module  is  able  to  achieve  a  much  higher  recall  score  while  maintaining  a  high-level  precision.  Specifically,  it  achieves  a  15.3%  relative  F1  improvement  and  also  less  inconsistency  in  the  outputs.  We  further  show  that  a  simple  modification  of  our  proposed  graph  layer  can  also  improve  the  performance  on  a  conventional  and  widely-tested  dataset  that  only  includes  KB-schema  types.
0	I  m  not  mad  commonsense  implications  of  negation  and  contradiction.  Natural  language  inference  requires  reasoning  about  contradictions,  negations,  and  their  commonsense  implications.  Given  a  simple  premise  (e.g.,  “I’m  mad  at  you”),  humans  can  reason  about  the  varying  shades  of  contradictory  statements  ranging  from  straightforward  negations  (“I’m  not  mad  at  you”)  to  commonsense  contradictions  (“I’m  happy”).  Moreover,  these  negated  or  contradictory  statements  shift  the  commonsense  implications  of  the  original  premise  in  interesting  and  nontrivial  ways.  For  example,  while  “I’m  mad”  implies  “I’m  unhappy  about  something,”  negating  the  premise  does  not  necessarily  negate  the  corresponding  commonsense  implications.  In  this  paper,  we  present  the  first  comprehensive  study  focusing  on  commonsense  implications  of  negated  statements  and  contradictions.  We  introduce  ANION,  a  new  commonsense  knowledge  graph  with  624K  if-then  rules  focusing  on  negated  and  contradictory  events.  We  then  present  joint  generative  and  discriminative  inference  models  for  this  new  resource,  providing  novel  empirical  insights  on  how  logical  negations  and  commonsense  contradictions  reshape  the  commonsense  implications  of  their  original  premises.
0	Unsupervised  pos  induction  with  word  embeddings.  Unsupervised  word  embeddings  have  been  shown  to  be  valuable  as  features  in  supervised  learning  problems;  however,  their  role  in  unsupervised  problems  has  been  less  thoroughly  explored.  In  this  paper,  we  show  that  embeddings  can  likewise  add  value  to  the  problem  of  unsupervised  POS  induction.  In  two  representative  models  of  POS  induction,  we  replace  multinomial  distributions  over  the  vocabulary  with  multivariate  Gaussian  distributions  over  word  embeddings  and  observe  consistent  improvements  in  eight  languages.  We  also  analyze  the  e  ect  of  various  choices  while  inducing  word  embeddings  on  “downstream”  POS  induction  results.
0	Pair2vec  compositional  word  pair  embeddings  for  cross  sentence  inference.  Reasoning  about  implied  relationships  (e.g.  paraphrastic,  common  sense,  encyclopedic)  between  pairs  of  words  is  crucial  for  many  cross-sentence  inference  problems.  This  paper  proposes  new  methods  for  learning  and  using  embeddings  of  word  pairs  that  implicitly  represent  background  knowledge  about  such  relationships.  Our  pairwise  embeddings  are  computed  as  a  compositional  function  of  each  word’s  representation,  which  is  learned  by  maximizing  the  pointwise  mutual  information  (PMI)  with  the  contexts  in  which  the  the  two  words  co-occur.  We  add  these  representations  to  the  cross-sentence  attention  layer  of  existing  inference  models  (e.g.  BiDAF  for  QA,  ESIM  for  NLI),  instead  of  extending  or  replacing  existing  word  embeddings.  Experiments  show  a  gain  of  2.7%  on  the  recently  released  SQuAD  2.0  and  1.3%  on  MultiNLI.  Our  representations  also  aid  in  better  generalization  with  gains  of  around  6-7%  on  adversarial  SQuAD  datasets,  and  8.8%  on  the  adversarial  entailment  test  set  by  Glockner  et  al.  (2018).
0	Event  detection  without  triggers.  The  goal  of  event  detection  (ED)  is  to  detect  the  occurrences  of  events  and  categorize  them.  Previous  work  solved  this  task  by  recognizing  and  classifying  event  triggers,  which  is  defined  as  the  word  or  phrase  that  most  clearly  expresses  an  event  occurrence.  As  a  consequence,  existing  approaches  required  both  annotated  triggers  and  event  types  in  training  data.  However,  triggers  are  nonessential  to  event  detection,  and  it  is  time-consuming  for  annotators  to  pick  out  the  “most  clearly”  word  from  a  given  sentence,  especially  from  a  long  sentence.  The  expensive  annotation  of  training  corpus  limits  the  application  of  existing  approaches.  To  reduce  manual  effort,  we  explore  detecting  events  without  triggers.  In  this  work,  we  propose  a  novel  framework  dubbed  as  Type-aware  Bias  Neural  Network  with  Attention  Mechanisms  (TBNNAM),  which  encodes  the  representation  of  a  sentence  based  on  target  event  types.  Experimental  results  demonstrate  the  effectiveness.  Remarkably,  the  proposed  approach  even  achieves  competitive  performances  compared  with  state-of-the-arts  that  used  annotated  triggers.
0	Semantic  structural  evaluation  for  text  simplification.  Current  measures  for  evaluating  text  simplification  systems  focus  on  evaluating  lexical  text  aspects,  neglecting  its  structural  aspects.  In  this  paper  we  propose  the  first  measure  to  address  structural  aspects  of  text  simplification,  called  SAMSA.  It  leverages  recent  advances  in  semantic  parsing  to  assess  simplification  quality  by  decomposing  the  input  based  on  its  semantic  structure  and  comparing  it  to  the  output.  SAMSA  provides  a  reference-less  automatic  evaluation  procedure,  avoiding  the  problems  that  reference-based  methods  face  due  to  the  vast  space  of  valid  simplifications  for  a  given  sentence.  Our  human  evaluation  experiments  show  both  SAMSA’s  substantial  correlation  with  human  judgments,  as  well  as  the  deficiency  of  existing  reference-based  measures  in  evaluating  structural  simplification.
0	Nlprl  iitbhu  at  semeval  2018  task  3  combining  linguistic  features  and  emoji  pre  trained  cnn  for  irony  detection  in  tweets.  This  paper  describes  our  participation  in  SemEval  2018  Task  3  on  Irony  Detection  in  Tweets.  We  combine  linguistic  features  with  pre-trained  activations  of  a  neural  network.  The  CNN  is  trained  on  the  emoji  prediction  task.  We  combine  the  two  feature  sets  and  feed  them  into  an  XGBoost  Classifier  for  classification.  Subtask-A  involves  classification  of  tweets  into  ironic  and  non-ironic  instances  whereas  Subtask-B  involves  classification  of  the  tweet  into  -  non-ironic,  verbal  irony,  situational  irony  or  other  verbal  irony.  It  is  observed  that  combining  features  from  these  two  different  feature  spaces  improves  our  system  results.  We  leverage  the  SMOTE  algorithm  to  handle  the  problem  of  class  imbalance  in  Subtask-B.  Our  final  model  achieves  an  F1-score  of  0.65  and  0.47  on  Subtask-A  and  Subtask-B  respectively.  Our  system  ranks  4th  on  both  tasks  respectively,  outperforming  the  baseline  by  6%  on  Subtask-A  and  14%  on  Subtask-B.
0	Understanding  by  understanding  not  modeling  negation  in  language  models.  Negation  is  a  core  construction  in  natural  language.  Despite  being  very  successful  on  many  tasks,  state-of-the-art  pre-trained  language  models  often  handle  negation  incorrectly.  To  improve  language  models  in  this  regard,  we  propose  to  augment  the  language  modeling  objective  with  an  unlikelihood  objective  that  is  based  on  negated  generic  sentences  from  a  raw  text  corpus.  By  training  BERT  with  the  resulting  combined  objective  we  reduce  the  mean  top  1  error  rate  to  4%  on  the  negated  LAMA  dataset.  We  also  see  some  improvements  on  the  negated  NLI  benchmarks.
0	Relation  extraction  perspective  from  convolutional  neural  networks.  Up  to  now,  relation  extraction  systems  have  made  extensive  use  of  features  generated  by  linguistic  analysis  modules.  Errors  in  these  features  lead  to  errors  of  relation  detection  and  classification.  In  this  work,  we  depart  from  these  traditional  approaches  with  complicated  feature  engineering  by  introducing  a  convolutional  neural  network  for  relation  extraction  that  automatically  learns  features  from  sentences  and  minimizes  the  dependence  on  external  toolkits  and  resources.  Our  model  takes  advantages  of  multiple  window  sizes  for  filters  and  pre-trained  word  embeddings  as  an  initializer  on  a  non-static  architecture  to  improve  the  performance.  We  emphasize  the  relation  extraction  problem  with  an  unbalanced  corpus.  The  experimental  results  show  that  our  system  significantly  outperforms  not  only  the  best  baseline  systems  for  relation  extraction  but  also  the  state-of-the-art  systems  for  relation  classification.
0	The  perfect  solution  for  detecting  sarcasm  in  tweets  not.  To  avoid  a  sarcastic  message  being  understood  in  its  unintended  literal  meaning,  in  microtexts  such  as  messages  on  Twitter.com  sarcasm  is  often  explicitly  marked  with  the  hashtag  ‘#sarcasm’.  We  collected  a  training  corpus  of  about  78  thousand  Dutch  tweets  with  this  hashtag.  Assuming  that  the  human  labeling  is  correct  (annotation  of  a  sample  indicates  that  about  85%  of  these  tweets  are  indeed  sarcastic),  we  train  a  machine  learning  classifier  on  the  harvested  examples,  and  apply  it  to  a  test  set  of  a  day’s  stream  of  3.3  million  Dutch  tweets.  Of  the  135  explicitly  marked  tweets  on  this  day,  we  detect  101  (75%)  when  we  remove  the  hashtag.  We  annotate  the  top  of  the  ranked  list  of  tweets  most  likely  to  be  sarcastic  that  do  not  have  the  explicit  hashtag.  30%  of  the  top-250  ranked  tweets  are  indeed  sarcastic.  Analysis  shows  that  sarcasm  is  often  signalled  by  hyperbole,  using  intensifiers  and  exclamations;  in  contrast,  non-hyperbolic  sarcastic  messages  often  receive  an  explicit  marker.  We  hypothesize  that  explicit  markers  such  as  hashtags  are  the  digital  extralinguistic  equivalent  of  nonverbal  expressions  that  people  employ  in  live  interaction  when  conveying  sarcasm.
0	Expanding  paraphrase  lexicons  by  exploiting  lexical  variants.  This  study  tackles  the  problem  of  paraphrase  acquisition:  achieving  high  coverage  as  well  as  accuracy.  Our  method  first  induces  paraphrase  patterns  from  given  seed  paraphrases,  exploiting  the  generality  of  paraphrases  exhibited  by  pairs  of  lexical  variants,  e.g.,  “amendment”  and  “amending,”  in  a  fully  empirical  way.  It  then  searches  monolingual  corpora  for  new  paraphrases  that  match  the  patterns.  This  can  extract  paraphrases  comprising  words  that  are  completely  different  from  those  of  the  given  seeds.  In  experiments,  our  method  expanded  seed  sets  by  factors  of  42  to  206,  gaining  84%  to  208%  more  coverage  than  a  previous  method  that  generalizes  only  identical  word  forms.  Human  evaluation  through  a  paraphrase  substitution  test  demonstrated  that  the  newly  acquired  paraphrases  retained  reasonable  quality,  given  substantially  high-quality  seeds.
0	Embedding  syntax  and  semantics  of  prepositions  via  tensor  decomposition.  Prepositions  are  among  the  most  frequent  words  in  English  and  play  complex  roles  in  the  syntax  and  semantics  of  sentences.  Not  surprisingly,  they  pose  well-known  difficulties  in  automatic  processing  of  sentences  (prepositional  attachment  ambiguities  and  idiosyncratic  uses  in  phrases).  Existing  methods  on  preposition  representation  treat  prepositions  no  different  from  content  words  (e.g.,  word2vec  and  GloVe).  In  addition,  recent  studies  aiming  at  solving  prepositional  attachment  and  preposition  selection  problems  depend  heavily  on  external  linguistic  resources  and  use  dataset-specific  word  representations.  In  this  paper  we  use  word-triple  counts  (one  of  the  triples  being  a  preposition)  to  capture  a  preposition’s  interaction  with  its  attachment  and  complement.  We  then  derive  preposition  embeddings  via  tensor  decomposition  on  a  large  unlabeled  corpus.  We  reveal  a  new  geometry  involving  Hadamard  products  and  empirically  demonstrate  its  utility  in  paraphrasing  phrasal  verbs.  Furthermore,  our  preposition  embeddings  are  used  as  simple  features  in  two  challenging  downstream  tasks:  preposition  selection  and  prepositional  attachment  disambiguation.  We  achieve  results  comparable  to  or  better  than  the  state-of-the-art  on  multiple  standardized  datasets.
0	Learning  word  embeddings  for  low  resource  languages  by  pu  learning.  Word  embedding  is  a  key  component  in  many  downstream  applications  in  processing  natural  languages.  Existing  approaches  often  assume  the  existence  of  a  large  collection  of  text  for  learning  effective  word  embedding.  However,  such  a  corpus  may  not  be  available  for  some  low-resource  languages.  In  this  paper,  we  study  how  to  effectively  learn  a  word  embedding  model  on  a  corpus  with  only  a  few  million  tokens.  In  such  a  situation,  the  co-occurrence  matrix  is  sparse  as  the  co-occurrences  of  many  word  pairs  are  unobserved.  In  contrast  to  existing  approaches  often  only  sample  a  few  unobserved  word  pairs  as  negative  samples,  we  argue  that  the  zero  entries  in  the  co-occurrence  matrix  also  provide  valuable  information.  We  then  design  a  Positive-Unlabeled  Learning  (PU-Learning)  approach  to  factorize  the  co-occurrence  matrix  and  validate  the  proposed  approaches  in  four  different  languages.
0	Discol  toward  engaging  dialogue  systems  through  conversational  line  guided  response  generation.  Having  engaging  and  informative  conversations  with  users  is  the  utmost  goal  for  open-domain  conversational  systems.  Recent  advances  in  transformer-based  language  models  and  their  applications  to  dialogue  systems  have  succeeded  to  generate  fluent  and  human-like  responses.  However,  they  still  lack  control  over  the  generation  process  towards  producing  contentful  responses  and  achieving  engaging  conversations.  To  achieve  this  goal,  we  present  DiSCoL  (Dialogue  Systems  through  Coversational  Line  guided  response  generation).  DiSCoL  is  an  open-domain  dialogue  system  that  leverages  conversational  lines  (briefly  convlines)  as  controllable  and  informative  content-planning  elements  to  guide  the  generation  model  produce  engaging  and  informative  responses.  Two  primary  modules  in  DiSCoL’s  pipeline  are  conditional  generators  trained  for  1)  predicting  relevant  and  informative  convlines  for  dialogue  contexts  and  2)  generating  high-quality  responses  conditioned  on  the  predicted  convlines.  Users  can  also  change  the  returned  convlines  to  control  the  direction  of  the  conversations  towards  topics  that  are  more  interesting  for  them.  Through  automatic  and  human  evaluations,  we  demonstrate  the  efficiency  of  the  convlines  in  producing  engaging  conversations.
0	Analyzing  the  perceived  severity  of  cybersecurity  threats  reported  on  social  media.  Breaking  cybersecurity  events  are  shared  across  a  range  of  websites,  including  security  blogs  (FireEye,  Kaspersky,  etc.),  in  addition  to  social  media  platforms  such  as  Facebook  and  Twitter.  In  this  paper,  we  investigate  methods  to  analyze  the  severity  of  cybersecurity  threats  based  on  the  language  that  is  used  to  describe  them  online.  A  corpus  of  6,000  tweets  describing  software  vulnerabilities  is  annotated  with  authors’  opinions  toward  their  severity.  We  show  that  our  corpus  supports  the  development  of  automatic  classifiers  with  high  precision  for  this  task.  Furthermore,  we  demonstrate  the  value  of  analyzing  users’  opinions  about  the  severity  of  threats  reported  online  as  an  early  indicator  of  important  software  vulnerabilities.  We  present  a  simple,  yet  effective  method  for  linking  software  vulnerabilities  reported  in  tweets  to  Common  Vulnerabilities  and  Exposures  (CVEs)  in  the  National  Vulnerability  Database  (NVD).  Using  our  predicted  severity  scores,  we  show  that  it  is  possible  to  achieve  a  Precision@50  of  0.86  when  forecasting  high  severity  vulnerabilities,  significantly  outperforming  a  baseline  that  is  based  on  tweet  volume.  Finally  we  show  how  reports  of  severe  vulnerabilities  online  are  predictive  of  real-world  exploits.
0	The  curious  case  of  hallucinations  in  neural  machine  translation.  In  this  work,  we  study  hallucinations  in  Neural  Machine  Translation  (NMT),  which  lie  at  an  extreme  end  on  the  spectrum  of  NMT  pathologies.  Firstly,  we  connect  the  phenomenon  of  hallucinations  under  source  perturbation  to  the  Long-Tail  theory  of  Feldman,  and  present  an  empirically  validated  hypothesis  that  explains  hallucinations  under  source  perturbation.  Secondly,  we  consider  hallucinations  under  corpus-level  noise  (without  any  source  perturbation)  and  demonstrate  that  two  prominent  types  of  natural  hallucinations  (detached  and  oscillatory  outputs)  could  be  generated  and  explained  through  specific  corpus-level  noise  patterns.  Finally,  we  elucidate  the  phenomenon  of  hallucination  amplification  in  popular  data-generation  processes  such  as  Backtranslation  and  sequence-level  Knowledge  Distillation.  We  have  released  the  datasets  and  code  to  replicate  our  results.
0	Structural  and  topical  dimensions  in  multi  task  patent  translation.  Patent  translation  is  a  complex  problem  due  to  the  highly  specialized  technical  vocabulary  and  the  peculiar  textual  structure  of  patent  documents.  In  this  paper  we  analyze  patents  along  the  orthogonal  dimensions  of  topic  and  textual  structure.  We  view  different  patent  classes  and  different  patent  text  sections  such  as  title,  abstract,  and  claims,  as  separate  translation  tasks,  and  investigate  the  influence  of  such  tasks  on  machine  translation  performance.  We  study  multitask  learning  techniques  that  exploit  commonalities  between  tasks  by  mixtures  of  translation  models  or  by  multi-task  metaparameter  tuning.  We  find  small  but  significant  gains  over  task-specific  training  by  techniques  that  model  commonalities  through  shared  parameters.  A  by-product  of  our  work  is  a  parallel  patent  corpus  of  23  million  German-English  sentence  pairs.
0	Process  level  representation  of  scientific  protocols  with  interactive  annotation.  We  develop  Process  Execution  Graphs  (PEG),  a  document-level  representation  of  real-world  wet  lab  biochemistry  protocols,  addressing  challenges  such  as  cross-sentence  relations,  long-range  coreference,  grounding,  and  implicit  arguments.  We  manually  annotate  PEGs  in  a  corpus  of  complex  lab  protocols  with  a  novel  interactive  textual  simulator  that  keeps  track  of  entity  traits  and  semantic  constraints  during  annotation.  We  use  this  data  to  develop  graph-prediction  models,  finding  them  to  be  good  at  entity  identification  and  local  relation  extraction,  while  our  corpus  facilitates  further  exploration  of  challenging  long-range  relations.
0	Trankit  a  light  weight  transformer  based  toolkit  for  multilingual  natural  language  processing.  We  introduce  Trankit,  a  light-weight  Transformer-based  Toolkit  for  multilingual  Natural  Language  Processing  (NLP).  It  provides  a  trainable  pipeline  for  fundamental  NLP  tasks  over  100  languages,  and  90  pretrained  pipelines  for  56  languages.  Built  on  a  state-of-the-art  pretrained  language  model,  Trankit  significantly  outperforms  prior  multilingual  NLP  pipelines  over  sentence  segmentation,  part-of-speech  tagging,  morphological  feature  tagging,  and  dependency  parsing  while  maintaining  competitive  performance  for  tokenization,  multi-word  token  expansion,  and  lemmatization  over  90  Universal  Dependencies  treebanks.  Despite  the  use  of  a  large  pretrained  transformer,  our  toolkit  is  still  efficient  in  memory  usage  and  speed.  This  is  achieved  by  our  novel  plug-and-play  mechanism  with  Adapters  where  a  multilingual  pretrained  transformer  is  shared  across  pipelines  for  different  languages.  Our  toolkit  along  with  pretrained  models  and  code  are  publicly  available  at:  https://github.com/nlp-uoregon/trankit.  A  demo  website  for  our  toolkit  is  also  available  at:  http://nlp.uoregon.edu/trankit.  Finally,  we  create  a  demo  video  for  Trankit  at:  https://youtu.be/q0KGP3zGjGc.
0	Measuring  and  improving  faithfulness  of  attention  in  neural  machine  translation.  While  the  attention  heatmaps  produced  by  neural  machine  translation  (NMT)  models  seem  insightful,  there  is  little  evidence  that  they  reflect  a  model’s  true  internal  reasoning.  We  provide  a  measure  of  faithfulness  for  NMT  based  on  a  variety  of  stress  tests  where  attention  weights  which  are  crucial  for  prediction  are  perturbed  and  the  model  should  alter  its  predictions  if  the  learned  weights  are  a  faithful  explanation  of  the  predictions.  We  show  that  our  proposed  faithfulness  measure  for  NMT  models  can  be  improved  using  a  novel  differentiable  objective  that  rewards  faithful  behaviour  by  the  model  through  probability  divergence.  Our  experimental  results  on  multiple  language  pairs  show  that  our  objective  function  is  effective  in  increasing  faithfulness  and  can  lead  to  a  useful  analysis  of  NMT  model  behaviour  and  more  trustworthy  attention  heatmaps.  Our  proposed  objective  improves  faithfulness  without  reducing  the  translation  quality  and  has  a  useful  regularization  effect  on  the  NMT  model  and  can  even  improve  translation  quality  in  some  cases.
0	Multi  layer  joint  gait  pose  manifold  for  human  motion  modeling.  We  present  a  multi-layer  joint  gait-pose  manifold  (multi-layer  JGPM)  for  human  motion  modeling  to  enhance  the  representative  capability  of  the  original  JGPM  that  represents  gait  kinematics  by  two  variables.  One  is  the  pose  to  denote  a  series  of  stages  in  a  walking  cycle  and  the  other  is  the  gait  to  reflect  the  individual  walking  styles.  Coupling  pose  and  gait  variables  in  the  same  latent  space  was  shown  effective  for  human  motion  estimation.  However,  the  original  JGPM  is  limited  to  one  kind  of  human  gaits,  and  its  learning  cannot  be  scaled  up  to  a  large  dataset  due  to  a  high  computational  load.  This  work  overcomes  the  limitations  of  the  previous  method  by  involving  a  multi-layer  topology  prior  that  is  able  to  accommodate  a  variety  of  walking  styles,  leading  to  better  motion  synthesis  results.  Moreover,  to  learn  multi-layer  JGPM  effectively  and  efficiently,  we  adopted  two  techniques,  training  data  diversification  and  topology-aware  local  learning.  The  experimental  results  confirm  the  advantages  and  superiority  of  our  proposed  method  over  several  existing  Gaussian  process-based  motion  models.
0	Prototype  based  feature  learning  for  face  image  set  classification.  Recognizing  human  face  from  image  set  has  recently  seen  its  prosperity  because  of  its  effectiveness  in  dealing  with  variations  in  illumination,  expressions,  or  poses.  In  this  paper,  inspired  by  the  prototype  notion  originating  from  cognition  field,  we  obtain  discriminative  feature  representation  for  face  recognition  by  implementing  prototype  formation  on  image  set.  The  contribution  of  this  paper  is  twofold:  first,  we  propose  to  use  prototype  image  sets  as  a  common  reference  to  sufficiently  represent  any  image  set  with  the  same  type;  in  addition,  we  propose  a  novel  framework  to  extract  image  set's  features  through  hyperplane  supervised  by  max-margin  criterion  between  any  image  set  and  prototype  image  set.  The  final  features  are  summarized  through  pooling  technique  along  the  prototype  image  sets.  We  experimentally  prove  the  effectiveness  of  the  method  through  extensive  experiments  on  several  databases,  and  show  that  it  is  superior  to  the  state-of-the-art  methods  in  terms  of  both  time  complexity  and  recognition  accuracy.
0	The  optimal  camera  arrangement  by  a  performance  model  for  gait  recognition.  Recently,  many  gait  recognition  algorithms  are  proposed,  and  the  optimal  camera  arrangement  is  necessary  to  maximize  the  performance.  In  this  paper,  we  propose  the  optimal  camera  arrangement  by  using  a  performance  model  that  considers  observation  conditions  comprehensively.  We  select  silhouette  resolution,  observation  view,  and  its  local  and  global  changes  as  the  observation  conditions  affecting  the  performance.  Then,  training  sets  composed  of  pairs  of  the  observation  conditions  and  the  performance  is  obtained  by  gait  recognition  experiments  under  several  camera  arrangements.  A  performance  model  is  constructed  by  applying  Gaussian  Processes  Regression  to  the  training  set.  The  optimal  arrangement  is  determined  by  estimating  the  performance  for  each  camera  arrangement  with  the  performance  model.  The  effectiveness  of  the  proposed  method  is  demonstrated  by  experiments  of  performance  estimation  with  a  training  set  including  17  subjects  and  the  optimal  camera  arrangement.
0	What  will  your  future  child  look  like  modeling  and  synthesis  of  hereditary  patterns  of  facial  dynamics.  Analysis  of  kinship  from  facial  images  or  videos  is  an  important  problem.  Prior  machine  learning  and  computer  vision  studies  approach  kinship  analysis  as  a  verification  or  recognition  task.  In  this  paper,  first  time  in  the  literature,  we  propose  a  kinship  synthesis  framework,  which  generates  smile  videos  of  (probable)  children  from  the  smile  videos  of  parents.  While  the  appearance  of  a  child’s  smile  is  learned  using  a  convolutional  encoder-decoder  network,  another  neural  network  models  the  dynamics  of  the  corresponding  smile.  The  smile  video  of  the  estimated  child  is  synthesized  by  the  combined  use  of  appearance  and  dynamics  models.  In  order  to  validate  our  results,  we  perform  kinship  verification  experiments  using  videos  of  real  parents  and  estimated  children  generated  by  our  framework.  The  results  show  that  generated  videos  of  children  achieve  higher  correct  verification  rates  than  those  of  real  children.  Our  results  also  indicate  that  the  use  of  generated  videos  together  with  the  real  ones  in  the  training  of  kinship  verification  models,  increases  the  accuracy,  suggesting  that  such  videos  can  be  used  as  a  synthetic  dataset.
0	Rapid  synthesis  of  massive  face  sets  for  improved  face  recognition.  Recent  work  demonstrated  that  computer  graphics  techniques  can  be  used  to  improve  face  recognition  performances  by  synthesizing  multiple  new  views  of  faces  available  in  existing  face  collections.  By  so  doing,  more  images  and  more  appearance  variations  are  available  for  training,  thereby  improving  the  deep  models  trained  on  these  images.  Similar  rendering  techniques  were  also  applied  at  test  time  to  align  faces  in  3D  and  reduce  appearance  variations  when  comparing  faces.  These  previous  results,  however,  did  not  consider  the  computational  cost  of  rendering:  At  training,  rendering  millions  of  face  images  can  be  prohibitive;  at  test  time,  rendering  can  quickly  become  a  bottleneck,  particularly  when  multiple  images  represent  a  subject.  This  paper  builds  on  a  number  of  observations  which,  under  certain  circumstances,  allow  rendering  new  3D  views  of  faces  at  a  computational  cost  which  is  equivalent  to  simple  2D  image  warping.  We  demonstrate  this  by  showing  that  the  run-time  of  an  optimized  OpenGL  rendering  engine  is  slower  than  the  simple  Python  implementation  we  designed  for  the  same  purpose.  The  proposed  rendering  is  used  in  a  face  recognition  pipeline  and  tested  on  the  challenging  IJB-A  and  Janus  CS2  benchmarks.  Our  results  show  that  our  rendering  is  not  only  fast,  but  improves  recognition  accuracy.
0	Primate  face  identification  in  the  wild.  Ecological  imbalance  owing  to  rapid  urbanization  and  deforestation  has  adversely  affected  the  population  of  several  wild  animals.  This  loss  of  habitat  has  skewed  the  population  of  several  non-human  primate  species  like  chimpanzees  and  macaques  and  has  constrained  them  to  co-exist  in  close  proximity  of  human  settlements,  often  leading  to  human-wildlife  conflicts  while  competing  for  resources.  For  effective  wildlife  conservation  and  conflict  management,  regular  monitoring  of  population  and  of  conflicted  regions  is  necessary.  However,  existing  approaches  like  field  visits  for  data  collection  and  manual  analysis  by  experts  is  resource  intensive,  tedious  and  time  consuming,  thus  necessitating  an  automated,  non-invasive,  more  efficient  alternative  like  image  based  facial  recognition.  The  challenge  in  individual  identification  arises  due  to  unrelated  factors  like  pose,  lighting  variations  and  occlusions  due  to  the  uncontrolled  environments,  that  is  further  exacerbated  by  limited  training  data.  Inspired  by  human  perception,  we  propose  to  learn  representations  that  are  robust  to  such  nuisance  factors  and  capture  the  notion  of  similarity  over  the  individual  identity  sub-manifolds.  The  proposed  approach,  Primate  Face  Identification  (PFID),  achieves  this  by  training  the  network  to  distinguish  between  positive  and  negative  pairs  of  images.  The  PFID  loss  augments  the  standard  cross  entropy  loss  with  a  pairwise  loss  to  learn  more  discriminative  and  generalizable  features,  thus  making  it  appropriate  for  other  related  identification  tasks  like  open-set,  closed  set  and  verification.  We  report  state-of-the-art  accuracy  on  facial  recognition  of  two  primate  species,  rhesus  macaques  and  chimpanzees  under  the  four  protocols  of  classification,  verification,  closed-set  identification  and  open-set  recognition.
0	What  you  see  is  what  you  get  visual  pronoun  coreference  resolution  in  dialogues.  Grounding  a  pronoun  to  a  visual  object  it  refers  to  requires  complex  reasoning  from  various  information  sources,  especially  in  conversational  scenarios.  For  example,  when  people  in  a  conversation  talk  about  something  all  speakers  can  see,  they  often  directly  use  pronouns  (e.g.,  it)  to  refer  to  it  without  previous  introduction.  This  fact  brings  a  huge  challenge  for  modern  natural  language  understanding  systems,  particularly  conventional  context-based  pronoun  coreference  models.  To  tackle  this  challenge,  in  this  paper,  we  formally  define  the  task  of  visual-aware  pronoun  coreference  resolution  (PCR)  and  introduce  VisPro,  a  large-scale  dialogue  PCR  dataset,  to  investigate  whether  and  how  the  visual  information  can  help  resolve  pronouns  in  dialogues.  We  then  propose  a  novel  visual-aware  PCR  model,  VisCoref,  for  this  task  and  conduct  comprehensive  experiments  and  case  studies  on  our  dataset.  Results  demonstrate  the  importance  of  the  visual  information  in  this  PCR  case  and  show  the  effectiveness  of  the  proposed  model.
0	Online  updating  of  word  representations  for  part  of  speech  tagging.  We  propose  online  unsupervised  domain  adaptation  (DA),  which  is  performed  incrementally  as  data  comes  in  and  is  applicable  when  batch  DA  is  not  possible.  In  a  part-of-speech  (POS)  tagging  evaluation,  we  find  that  online  unsupervised  DA  performs  as  well  as  batch  DA.
0	Joint  inference  for  event  timeline  construction.  This  paper  addresses  the  task  of  constructing  a  timeline  of  events  mentioned  in  a  given  text.  To  accomplish  that,  we  present  a  novel  representation  of  the  temporal  structure  of  a  news  article  based  on  time  intervals.  We  then  present  an  algorithmic  approach  that  jointly  optimizes  the  temporal  structure  by  coupling  local  classifiers  that  predict  associations  and  temporal  relations  between  pairs  of  temporal  entities  with  global  constraints.  Moreover,  we  present  ways  to  leverage  knowledge  provided  by  event  coreference  to  further  improve  the  system  performance.  Overall,  our  experiments  show  that  the  joint  inference  model  significantly  outperformed  the  local  classifiers  by  9.2%  of  relative  improvement  in  F1.  The  experiments  also  suggest  that  good  event  coreference  could  make  remarkable  contribution  to  a  robust  event  timeline  construction  system.
0	Attribute  aware  sequence  network  for  review  summarization.  Review  summarization  aims  to  generate  a  condensed  summary  for  a  review  or  multiple  reviews.  Existing  review  summarization  systems  mainly  generate  summary  only  based  on  review  content  and  neglect  the  authors’  attributes  (e.g.,  gender,  age,  and  occupation).  In  fact,  when  summarizing  a  review,  users  with  different  attributes  usually  pay  attention  to  specific  aspects  and  have  their  own  word-using  habits  or  writing  styles.  Therefore,  we  propose  an  Attribute-aware  Sequence  Network  (ASN)  to  take  the  aforementioned  users’  characteristics  into  account,  which  includes  three  modules:  an  attribute  encoder  encodes  the  attribute  preferences  over  the  words;  an  attribute-aware  review  encoder  adopts  an  attribute-based  selective  mechanism  to  select  the  important  information  of  a  review;  and  an  attribute-aware  summary  decoder  incorporates  attribute  embedding  and  attribute-specific  word-using  habits  into  word  prediction.  To  validate  our  model,  we  collect  a  new  dataset  TripAtt,  comprising  495,440  attribute-review-summary  triplets  with  three  kinds  of  attribute  information:  gender,  age,  and  travel  status.  Extensive  experiments  show  that  ASN  achieves  state-of-the-art  performance  on  review  summarization  in  both  auto-metric  ROUGE  and  human  evaluation.
0	Redcoat  a  collaborative  annotation  tool  for  hierarchical  entity  typing.  We  introduce  Redcoat,  a  web-based  annotation  tool  that  supports  collaborative  hierarchical  entity  typing.  As  an  annotation  tool,  Redcoat  also  facilitates  knowledge  elicitation  by  allowing  the  creation  and  continuous  refinement  of  concept  hierarchies  during  annotation.  It  aims  to  minimise  not  only  annotation  time  but  the  time  it  takes  for  project  creators  to  set  up  and  distribute  projects  to  annotators.  Projects  created  using  the  web-based  interface  can  be  rapidly  distributed  to  a  list  of  email  addresses.  Redcoat  handles  the  propagation  of  documents  amongst  annotators  and  automatically  scales  the  annotation  workload  depending  on  the  number  of  active  annotators.  In  this  paper  we  discuss  these  key  features  and  outline  Redcoat’s  system  architecture.  We  also  highlight  Redcoat’s  unique  benefits  over  existing  annotation  tools  via  a  qualitative  comparison.
0	Kagnet  knowledge  aware  graph  networks  for  commonsense  reasoning.  Commonsense  reasoning  aims  to  empower  machines  with  the  human  ability  to  make  presumptions  about  ordinary  situations  in  our  daily  life.  In  this  paper,  we  propose  a  textual  inference  framework  for  answering  commonsense  questions,  which  effectively  utilizes  external,  structured  commonsense  knowledge  graphs  to  perform  explainable  inferences.  The  framework  first  grounds  a  question-answer  pair  from  the  semantic  space  to  the  knowledge-based  symbolic  space  as  a  schema  graph,  a  related  sub-graph  of  external  knowledge  graphs.  It  represents  schema  graphs  with  a  novel  knowledge-aware  graph  network  module  named  KagNet,  and  finally  scores  answers  with  graph  representations.  Our  model  is  based  on  graph  convolutional  networks  and  LSTMs,  with  a  hierarchical  path-based  attention  mechanism.  The  intermediate  attention  scores  make  it  transparent  and  interpretable,  which  thus  produce  trustworthy  inferences.  Using  ConceptNet  as  the  only  external  resource  for  Bert-based  models,  we  achieved  state-of-the-art  performance  on  the  CommonsenseQA,  a  large-scale  dataset  for  commonsense  reasoning.
0	Interpreting  word  level  hidden  state  behaviour  of  character  level  lstm  language  models.  While  Long  Short-Term  Memory  networks  (LSTMs)  and  other  forms  of  recurrent  neural  network  have  been  successfully  applied  to  language  modeling  on  a  character  level,  the  hidden  state  dynamics  of  these  models  can  be  difficult  to  interpret.  We  investigate  the  hidden  states  of  such  a  model  by  using  the  HDBSCAN  clustering  algorithm  to  identify  points  in  the  text  at  which  the  hidden  state  is  similar.  Focusing  on  whitespace  characters  prior  to  the  beginning  of  a  word  reveals  interpretable  clusters  that  offer  insight  into  how  the  LSTM  may  combine  contextual  and  character-level  information  to  identify  parts  of  speech.  We  also  introduce  a  method  for  deriving  word  vectors  from  the  hidden  state  representation  in  order  to  investigate  the  word-level  knowledge  of  the  model.  These  word  vectors  encode  meaningful  semantic  information  even  for  words  that  appear  only  once  in  the  training  text.
0	Phrase  based  neural  unsupervised  machine  translation.  Machine  translation  systems  achieve  near  human-level  performance  on  some  languages,  yet  their  effectiveness  strongly  relies  on  the  availability  of  large  amounts  of  parallel  sentences,  which  hinders  their  applicability  to  the  majority  of  language  pairs.  This  work  investigates  how  to  learn  to  translate  when  having  access  to  only  large  monolingual  corpora  in  each  language.  We  propose  two  model  variants,  a  neural  and  a  phrase-based  model.  Both  versions  leverage  a  careful  initialization  of  the  parameters,  the  denoising  effect  of  language  models  and  automatic  generation  of  parallel  data  by  iterative  back-translation.  These  models  are  significantly  better  than  methods  from  the  literature,  while  being  simpler  and  having  fewer  hyper-parameters.  On  the  widely  used  WMT’14  English-French  and  WMT’16  German-English  benchmarks,  our  models  respectively  obtain  28.1  and  25.2  BLEU  points  without  using  a  single  parallel  sentence,  outperforming  the  state  of  the  art  by  more  than  11  BLEU  points.  On  low-resource  languages  like  English-Urdu  and  English-Romanian,  our  methods  achieve  even  better  results  than  semi-supervised  and  supervised  approaches  leveraging  the  paucity  of  available  bitexts.  Our  code  for  NMT  and  PBSMT  is  publicly  available.
0	Mapping  instructions  to  actions  in  3d  environments  with  visual  goal  prediction.  We  propose  to  decompose  instruction  execution  to  goal  prediction  and  action  generation.  We  design  a  model  that  maps  raw  visual  observations  to  goals  using  LINGUNET,  a  language-conditioned  image  generation  network,  and  then  generates  the  actions  required  to  complete  them.  Our  model  is  trained  from  demonstration  only  without  external  resources.  To  evaluate  our  approach,  we  introduce  two  benchmarks  for  instruction  following:  LANI,  a  navigation  task;  and  CHAI,  where  an  agent  executes  household  instructions.  Our  evaluation  demonstrates  the  advantages  of  our  model  decomposition,  and  illustrates  the  challenges  posed  by  our  new  benchmarks.
0	Generalizable  and  explainable  dialogue  generation  via  explicit  action  learning.  Response  generation  for  task-oriented  dialogues  implicitly  optimizes  two  objectives  at  the  same  time:  task  completion  and  language  quality.  Conditioned  response  generation  serves  as  an  effective  approach  to  separately  and  better  optimize  these  two  objectives.  Such  an  approach  relies  on  system  action  annotations  which  are  expensive  to  obtain.  To  alleviate  the  need  of  action  annotations,  latent  action  learning  is  introduced  to  map  each  utterance  to  a  latent  representation.  However,  this  approach  is  prone  to  over-dependence  on  the  training  data,  and  the  generalization  capability  is  thus  restricted.  To  address  this  issue,  we  propose  to  learn  natural  language  actions  that  represent  utterances  as  a  span  of  words.  This  explicit  action  representation  promotes  generalization  via  the  compositional  structure  of  language.  It  also  enables  an  explainable  generation  process.  Our  proposed  unsupervised  approach  learns  a  memory  component  to  summarize  system  utterances  into  a  short  span  of  words.  To  further  promote  a  compact  action  representation,  we  propose  an  auxiliary  task  that  restores  state  annotations  as  the  summarized  dialogue  context  using  the  memory  component.  Our  proposed  approach  outperforms  latent  action  baselines  on  MultiWOZ,  a  benchmark  multi-domain  dataset.
0	Higher  order  logical  inference  with  compositional  semantics.  We  present  a  higher-order  inference  system  based  on  a  formal  compositional  semantics  and  the  wide-coverage  CCG  parser.  We  develop  an  improved  method  to  bridge  between  the  parser  and  semantic  composition.  The  system  is  evaluated  on  the  FraCaS  test  suite.  In  contrast  to  the  widely  held  view  that  higher-order  logic  is  unsuitable  for  efficient  logical  inferences,  the  results  show  that  a  system  based  on  a  reasonably-sized  semantic  lexicon  and  a  manageable  number  of  non-first-order  axioms  enables  efficient  logical  inferences,  including  those  concerned  with  generalized  quantifiers  and  intensional  operators,  and  outperforms  the  state-of-the-art  firstorder  inference  system.
0	Collective  personal  profile  summarization  with  social  networks.  Personal  profile  information  on  social  media  like  LinkedIn.com  and  Facebook.com  is  at  the  core  of  many  interesting  applications,  such  as  talent  recommendation  and  contextual  advertising.  However,  personal  profiles  usually  lack  organization  confronted  with  the  large  amount  of  available  information.  Therefore,  it  is  always  a  challenge  for  people  to  find  desired  information  from  them.  In  this  paper,  we  address  the  task  of  personal  profile  summarization  by  leveraging  both  personal  profile  textual  information  and  social  networks.  Here,  using  social  networks  is  motivated  by  the  intuition  that,  people  with  similar  academic,  business  or  social  connections  (e.g.  co-major,  co-university,  and  cocorporation)  tend  to  have  similar  experience  and  summaries.  To  achieve  the  learning  process,  we  propose  a  collective  factor  graph  (CoFG)  model  to  incorporate  all  these  resources  of  knowledge  to  summarize  personal  profiles  with  local  textual  attribute  functions  and  social  connection  factors.  Extensive  evaluation  on  a  large-scale  dataset  from  LinkedIn.com  demonstrates  the  effectiveness  of  the  proposed  approach.  *
0	Financial  keyword  expansion  via  continuous  word  vector  representations.  This  paper  proposes  to  apply  the  continuous  vector  representations  of  words  for  discovering  keywords  from  a  financial  sentiment  lexicon.  In  order  to  capture  more  keywords,  we  also  incorporate  syntactic  information  into  the  Continuous  Bag-ofWords  (CBOW)  model.  Experimental  results  on  a  task  of  financial  risk  prediction  using  the  discovered  keywords  demonstrate  that  the  proposed  approach  is  good  at  predicting  financial  risk.
0	Parameterized  convolutional  neural  networks  for  aspect  level  sentiment  classification.  We  introduce  a  novel  parameterized  convolutional  neural  network  for  aspect  level  sentiment  classification.  Using  parameterized  filters  and  parameterized  gates,  we  incorporate  aspect  information  into  convolutional  neural  networks  (CNN).  Experiments  demonstrate  that  our  parameterized  filters  and  parameterized  gates  effectively  capture  the  aspect-specific  features,  and  our  CNN-based  models  achieve  excellent  results  on  SemEval  2014  datasets.
0	Comprehensive  multi  dataset  evaluation  of  reading  comprehension.  Reading  comprehension  is  one  of  the  crucial  tasks  for  furthering  research  in  natural  language  understanding.  A  lot  of  diverse  reading  comprehension  datasets  have  recently  been  introduced  to  study  various  phenomena  in  natural  language,  ranging  from  simple  paraphrase  matching  and  entity  typing  to  entity  tracking  and  understanding  the  implications  of  the  context.  Given  the  availability  of  many  such  datasets,  comprehensive  and  reliable  evaluation  is  tedious  and  time-consuming  for  researchers  working  on  this  problem.  We  present  an  evaluation  server,  ORB,  that  reports  performance  on  seven  diverse  reading  comprehension  datasets,  encouraging  and  facilitating  testing  a  single  model’s  capability  in  understanding  a  wide  variety  of  reading  phenomena.  The  evaluation  server  places  no  restrictions  on  how  models  are  trained,  so  it  is  a  suitable  test  bed  for  exploring  training  paradigms  and  representation  learning  for  general  reading  facility.  As  more  suitable  datasets  are  released,  they  will  be  added  to  the  evaluation  server.  We  also  collect  and  include  synthetic  augmentations  for  these  datasets,  testing  how  well  models  can  handle  out-of-domain  questions.
0	Profile  consistency  identification  for  open  domain  dialogue  agents.  Maintaining  a  consistent  attribute  profile  is  crucial  for  dialogue  agents  to  naturally  converse  with  humans.  Existing  studies  on  improving  attribute  consistency  mainly  explored  how  to  incorporate  attribute  information  in  the  responses,  but  few  efforts  have  been  made  to  identify  the  consistency  relations  between  response  and  attribute  profile.  To  facilitate  the  study  of  profile  consistency  identification,  we  create  a  large-scale  human-annotated  dataset  with  over  110K  single-turn  conversations  and  their  key-value  attribute  profiles.  Explicit  relation  between  response  and  profile  is  manually  labeled.  We  also  propose  a  key-value  structure  information  enriched  BERT  model  to  identify  the  profile  consistency,  and  it  gained  improvements  over  strong  baselines.  Further  evaluations  on  downstream  tasks  demonstrate  that  the  profile  consistency  identification  model  is  conducive  for  improving  dialogue  consistency.
0	Hero  hierarchical  encoder  for  video  language  omni  representation  pre  training.  We  present  HERO,  a  novel  framework  for  large-scale  video+language  omni-representation  learning.  HERO  encodes  multimodal  inputs  in  a  hierarchical  structure,  where  local  context  of  a  video  frame  is  captured  by  a  Cross-modal  Transformer  via  multimodal  fusion,  and  global  video  context  is  captured  by  a  Temporal  Transformer.  In  addition  to  standard  Masked  Language  Modeling  (MLM)  and  Masked  Frame  Modeling  (MFM)  objectives,  we  design  two  new  pre-training  tasks:  (i)  Video-Subtitle  Matching  (VSM),  where  the  model  predicts  both  global  and  local  temporal  alignment;  and  (ii)  Frame  Order  Modeling  (FOM),  where  the  model  predicts  the  right  order  of  shuffled  video  frames.  HERO  is  jointly  trained  on  HowTo100M  and  large-scale  TV  datasets  to  gain  deep  understanding  of  complex  social  dynamics  with  multi-character  interactions.  Comprehensive  experiments  demonstrate  that  HERO  achieves  new  state  of  the  art  on  multiple  benchmarks  over  Text-based  Video/Video-moment  Retrieval,  Video  Question  Answering  (QA),  Video-and-language  Inference  and  Video  Captioning  tasks  across  different  domains.  We  also  introduce  two  new  challenging  benchmarks  How2QA  and  How2R  for  Video  QA  and  Retrieval,  collected  from  diverse  video  content  over  multimodalities.
0	An  in  depth  analysis  of  the  effect  of  lexical  normalization  on  the  dependency  parsing  of  social  media.  Existing  natural  language  processing  systems  have  often  been  designed  with  standard  texts  in  mind.  However,  when  these  tools  are  used  on  the  substantially  different  texts  from  social  media,  their  performance  drops  dramatically.  One  solution  is  to  translate  social  media  data  to  standard  language  before  processing,  this  is  also  called  normalization.  It  is  well-known  that  this  improves  performance  for  many  natural  language  processing  tasks  on  social  media  data.  However,  little  is  known  about  which  types  of  normalization  replacements  have  the  most  effect.  Furthermore,  it  is  unknown  what  the  weaknesses  of  existing  lexical  normalization  systems  are  in  an  extrinsic  setting.  In  this  paper,  we  analyze  the  effect  of  manual  as  well  as  automatic  lexical  normalization  for  dependency  parsing.  After  our  analysis,  we  conclude  that  for  most  categories,  automatic  normalization  scores  close  to  manually  annotated  normalization  and  that  small  annotation  differences  are  important  to  take  into  consideration  when  exploiting  normalization  in  a  pipeline  setup.
0	Structured  lexical  similarity  via  convolution  kernels  on  dependency  trees.  A  central  topic  in  natural  language  processing  is  the  design  of  lexical  and  syntactic  features  suitable  for  the  target  application.  In  this  paper,  we  study  convolution  dependency  tree  kernels  for  automatic  engineering  of  syntactic  and  semantic  patterns  exploiting  lexical  similarities.  We  define  efficient  and  powerful  kernels  for  measuring  the  similarity  between  dependency  structures,  whose  surface  forms  of  the  lexical  nodes  are  in  part  or  completely  different.  The  experiments  with  such  kernels  for  question  classification  show  an  unprecedented  results,  e.g.  41%  of  error  reduction  of  the  former  state-of-the-art.  Additionally,  semantic  role  classification  confirms  the  benefit  of  semantic  smoothing  for  dependency  kernels.
0	Tunisian  dialect  wordnet  creation  and  enrichment  using  web  resources  and  other  wordnets.  In  this  paper,  we  propose  TunDiaWN  (Tunisian  dialect  Wordnet)  a  lexical  resource  for  the  dialect  language  spoken  in  Tunisia.  Our  TunDiaWN  construction  approach  is  founded,  in  one  hand,  on  a  corpus  based  method  to  analyze  and  extract  Tunisian  dialect  words.  A  clustering  technique  is  adapted  and  applied  to  mine  the  possible  relations  existing  between  the  Tunisian  dialect  extracted  words  and  to  group  them  into  meaningful  groups.  All  these  suggestions  are  then  evaluated  and  validated  by  the  experts  to  perform  the  resource  enrichment  task.  We  reuse  other  Wordnet  versions,  mainly  for  English  and  Arabic  language  to  propose  a  new  database  structure  enriched  by  innovative  features  and  entities.
0	Large  scale  reordering  model  for  statistical  machine  translation  using  dual  multinomial  logistic  regression.  Phrase  reordering  is  a  challenge  for  statistical  machine  translation  systems.  Posing  phrase  movements  as  a  prediction  problem  using  contextual  features  modeled  by  maximum  entropy-based  classifier  is  superior  to  the  commonly  used  lexicalized  reordering  model.  However,  Training  this  discriminative  model  using  large-scale  parallel  corpus  might  be  computationally  expensive.  In  this  paper,  we  explore  recent  advancements  in  solving  large-scale  classification  problems.  Using  the  dual  problem  to  multinomial  logistic  regression,  we  managed  to  shrink  the  training  data  while  iterating  and  produce  significant  saving  in  computation  and  memory  while  preserving  the  accuracy.
0	Subword  language  model  for  query  auto  completion.  Current  neural  query  auto-completion  (QAC)  systems  rely  on  character-level  language  models,  but  they  slow  down  when  queries  are  long.  We  present  how  to  utilize  subword  language  models  for  the  fast  and  accurate  generation  of  query  completion  candidates.  Representing  queries  with  subwords  shorten  a  decoding  length  significantly.  To  deal  with  issues  coming  from  introducing  subword  language  model,  we  develop  a  retrace  algorithm  and  a  reranking  method  by  approximate  marginalization.  As  a  result,  our  model  achieves  up  to  2.5  times  faster  while  maintaining  a  similar  quality  of  generated  results  compared  to  the  character-level  baseline.  Also,  we  propose  a  new  evaluation  metric,  mean  recoverable  length  (MRL),  measuring  how  many  upcoming  characters  the  model  could  complete  correctly.  It  provides  more  explicit  meaning  and  eliminates  the  need  for  prefix  length  sampling  for  existing  rank-based  metrics.  Moreover,  we  performed  a  comprehensive  analysis  with  ablation  study  to  figure  out  the  importance  of  each  component.
0	Closed  book  training  to  improve  summarization  encoder  memory.  A  good  neural  sequence-to-sequence  summarization  model  should  have  a  strong  encoder  that  can  distill  and  memorize  the  important  information  from  long  input  texts  so  that  the  decoder  can  generate  salient  summaries  based  on  the  encoder’s  memory.  In  this  paper,  we  aim  to  improve  the  memorization  capabilities  of  the  encoder  of  a  pointer-generator  model  by  adding  an  additional  ‘closed-book’  decoder  without  attention  and  pointer  mechanisms.  Such  a  decoder  forces  the  encoder  to  be  more  selective  in  the  information  encoded  in  its  memory  state  because  the  decoder  can’t  rely  on  the  extra  information  provided  by  the  attention  and  possibly  copy  modules,  and  hence  improves  the  entire  model.  On  the  CNN/Daily  Mail  dataset,  our  2-decoder  model  outperforms  the  baseline  significantly  in  terms  of  ROUGE  and  METEOR  metrics,  for  both  cross-entropy  and  reinforced  setups  (and  on  human  evaluation).  Moreover,  our  model  also  achieves  higher  scores  in  a  test-only  DUC-2002  generalizability  setup.  We  further  present  a  memory  ability  test,  two  saliency  metrics,  as  well  as  several  sanity-check  ablations  (based  on  fixed-encoder,  gradient-flow  cut,  and  model  capacity)  to  prove  that  the  encoder  of  our  2-decoder  model  does  in  fact  learn  stronger  memory  representations  than  the  baseline  encoder.
0	Refer  reuse  reduce  generating  subsequent  references  in  visual  and  conversational  contexts.  Dialogue  participants  often  refer  to  entities  or  situations  repeatedly  within  a  conversation,  which  contributes  to  its  cohesiveness.  Subsequent  references  exploit  the  common  ground  accumulated  by  the  interlocutors  and  hence  have  several  interesting  properties,  namely,  they  tend  to  be  shorter  and  reuse  expressions  that  were  effective  in  previous  mentions.  In  this  paper,  we  tackle  the  generation  of  first  and  subsequent  references  in  visually  grounded  dialogue.  We  propose  a  generation  model  that  produces  referring  utterances  grounded  in  both  the  visual  and  the  conversational  context.  To  assess  the  referring  effectiveness  of  its  output,  we  also  implement  a  reference  resolution  system.  Our  experiments  and  analyses  show  that  the  model  produces  better,  more  effective  referring  utterances  than  a  model  not  grounded  in  the  dialogue  context,  and  generates  subsequent  references  that  exhibit  linguistic  patterns  akin  to  humans.
0	Compressive  summarization  with  plausibility  and  salience  modeling.  Compressive  summarization  systems  typically  rely  on  a  seed  set  of  syntactic  rules  to  determine  under  what  circumstances  deleting  a  span  is  permissible,  then  learn  which  compressions  to  actually  apply  by  optimizing  for  ROUGE.  In  this  work,  we  propose  to  relax  these  explicit  syntactic  constraints  on  candidate  spans,  and  instead  leave  the  decision  about  what  to  delete  to  two  data-driven  criteria:  plausibility  and  salience.  Deleting  a  span  is  plausible  if  removing  it  maintains  the  grammaticality  and  factuality  of  a  sentence,  and  it  is  salient  if  it  removes  important  information  from  the  summary.  Each  of  these  is  judged  by  a  pre-trained  Transformer  model,  and  only  deletions  that  are  both  plausible  and  not  salient  can  be  applied.  When  integrated  into  a  simple  extraction-compression  pipeline,  our  method  achieves  strong  in-domain  results  on  benchmark  datasets,  and  human  evaluation  shows  that  the  plausibility  model  generally  selects  for  grammatical  and  factual  deletions.  Furthermore,  the  flexibility  of  our  approach  allows  it  to  generalize  cross-domain,  and  we  show  that  our  system  fine-tuned  on  only  500  samples  from  a  new  domain  can  match  or  exceed  a  strong  in-domain  extractive  model.
0	Understanding  weekly  covid  19  concerns  through  dynamic  content  specific  lda  topic  modeling.  The  novelty  and  global  scale  of  the  COVID-19  pandemic  has  lead  to  rapid  societal  changes  in  a  short  span  of  time.  As  government  policy  and  health  measures  shift,  public  perceptions  and  concerns  also  change,  an  evolution  documented  within  discourse  on  social  media.  We  propose  a  dynamic  content-specific  LDA  topic  modeling  technique  that  can  help  to  identify  different  domains  of  COVID-specific  discourse  that  can  be  used  to  track  societal  shifts  in  concerns  or  views.  Our  experiments  show  that  these  model-derived  topics  are  more  coherent  than  standard  LDA  topics,  and  also  provide  new  features  that  are  more  helpful  in  prediction  of  COVID-19  related  outcomes  including  mobility  and  unemployment  rate.
0	Unsupervised  stance  detection  for  arguments  from  consequences.  Social  media  platforms  have  become  an  essential  venue  for  online  deliberation  where  users  discuss  arguments,  debate,  and  form  opinions.  In  this  paper,  we  propose  an  unsupervised  method  to  detect  the  stance  of  argumentative  claims  with  respect  to  a  topic.  Most  related  work  focuses  on  topic-specific  supervised  models  that  need  to  be  trained  for  every  emergent  debate  topic.  To  address  this  limitation,  we  propose  a  topic  independent  approach  that  focuses  on  a  frequently  encountered  class  of  arguments,  specifically,  on  arguments  from  consequences.  We  do  this  by  extracting  the  effects  that  claims  refer  to,  and  proposing  a  means  for  inferring  if  the  effect  is  a  good  or  bad  consequence.  Our  experiments  provide  promising  results  that  are  comparable  to,  and  in  particular  regards  even  outperform  BERT.  Furthermore,  we  publish  a  novel  dataset  of  arguments  relating  to  consequences,  annotated  with  Amazon  Mechanical  Turk.
0	Why  question  answering  using  intra  and  inter  sentential  causal  relations.  In  this  paper,  we  explore  the  utility  of  intra-  and  inter-sentential  causal  relations  between  terms  or  clauses  as  evidence  for  answering  why-questions.  To  the  best  of  our  knowledge,  this  is  the  first  work  that  uses  both  intra-  and  inter-sentential  causal  relations  for  why-QA.  We  also  propose  a  method  for  assessing  the  appropriateness  of  causal  relations  as  answers  to  a  given  question  using  the  semantic  orientation  of  excitation  proposed  by  Hashimoto  et  al.  (2012).  By  applying  these  ideas  to  Japanese  why-QA,  we  improved  precision  by  4.4%  against  all  the  questions  in  our  test  set  over  the  current  state-of-theart  system  for  Japanese  why-QA.  In  addition,  unlike  the  state-of-the-art  system,  our  system  could  achieve  very  high  precision  (83.2%)  for  25%  of  all  the  questions  in  the  test  set  by  restricting  its  output  to  the  confident  answers  only.
0	Discriminative  strategies  to  integrate  multiword  expression  recognition  and  parsing.  The  integration  of  multiword  expressions  in  a  parsing  procedure  has  been  shown  to  improve  accuracy  in  an  artificial  context  where  such  expressions  have  been  perfectly  pre-identified.  This  paper  evaluates  two  empirical  strategies  to  integrate  multiword  units  in  a  real  constituency  parsing  context  and  shows  that  the  results  are  not  as  promising  as  has  sometimes  been  suggested.  Firstly,  we  show  that  pre-grouping  multiword  expressions  before  parsing  with  a  state-of-the-art  recognizer  improves  multiword  recognition  accuracy  and  unlabeled  attachment  score.  However,  it  has  no  statistically  significant  impact  in  terms  of  F-score  as  incorrect  multiword  expression  recognition  has  important  side  effects  on  parsing.  Secondly,  integrating  multiword  expressions  in  the  parser  grammar  followed  by  a  reranker  specific  to  such  expressions  slightly  improves  all  evaluation  metrics.
0	Boosting  named  entity  recognition  with  neural  character  embeddings.  Most  state-of-the-art  named  entity  recognition  (NER)  systems  rely  on  handcrafted  features  and  on  the  output  of  other  NLP  tasks  such  as  part-of-speech  (POS)  tagging  and  text  chunking.  In  this  work  we  propose  a  language-independent  NER  system  that  uses  automatically  learned  features  only.  Our  approach  is  based  on  the  CharWNN  deep  neural  network,  which  uses  word-level  and  character-level  representations  (embeddings)  to  perform  sequential  classification.  We  perform  an  extensive  number  of  experiments  using  two  annotated  corpora  in  two  different  languages:  HAREM  I  corpus,  which  contains  texts  in  Portuguese;  and  the  SPA  CoNLL2002  corpus,  which  contains  texts  in  Spanish.  Our  experimental  results  give  evidence  of  the  contribution  of  neural  character  embeddings  for  NER.  Moreover,  we  demonstrate  that  the  same  neural  network  which  has  been  successfully  applied  to  POS  tagging  can  also  achieve  state-of-theart  results  for  language-independet  NER,  using  the  same  hyperparameters,  and  without  any  handcrafted  features.  For  the  HAREM  I  corpus,  CharWNN  outperforms  the  state-of-the-art  system  by  7.9  points  in  the  F1-score  for  the  total  scenario  (ten  NE  classes).  For  the  SPA  CoNLL-2002  corpus,  CharWNN  outperforms  the  state-ofthe-art  system  by  0.8  point  in  the  F1.
0	Do  human  rationales  improve  machine  explanations.  Work  on  “learning  with  rationales”  shows  that  humans  providing  explanations  to  a  machine  learning  system  can  improve  the  system’s  predictive  accuracy.  However,  this  work  has  not  been  connected  to  work  in  “explainable  AI”  which  concerns  machines  explaining  their  reasoning  to  humans.  In  this  work,  we  show  that  learning  with  rationales  can  also  improve  the  quality  of  the  machine’s  explanations  as  evaluated  by  human  judges.  Specifically,  we  present  experiments  showing  that,  for  CNN-based  text  classification,  explanations  generated  using  “supervised  attention”  are  judged  superior  to  explanations  generated  using  normal  unsupervised  attention.
0	Dureader  a  chinese  machine  reading  comprehension  dataset  from  real  world  applications.  This  paper  introduces  DuReader,  a  new  large-scale,  open-domain  Chinese  machine  reading  comprehension  (MRC)  dataset,  designed  to  address  real-world  MRC.  DuReader  has  three  advantages  over  previous  MRC  datasets:  (1)  data  sources:  questions  and  documents  are  based  on  Baidu  Search  and  Baidu  Zhidao;  answers  are  manually  generated.  (2)  question  types:  it  provides  rich  annotations  for  more  question  types,  especially  yes-no  and  opinion  questions,  that  leaves  more  opportunity  for  the  research  community.  (3)  scale:  it  contains  200K  questions,  420K  answers  and  1M  documents;  it  is  the  largest  Chinese  MRC  dataset  so  far.  Experiments  show  that  human  performance  is  well  above  current  state-of-the-art  baseline  systems,  leaving  plenty  of  room  for  the  community  to  make  improvements.  To  help  the  community  make  these  improvements,  both  DuReader  and  baseline  systems  have  been  posted  online.  We  also  organize  a  shared  competition  to  encourage  the  exploration  of  more  models.  Since  the  release  of  the  task,  there  are  significant  improvements  over  the  baselines.
0	Combining  knowledge  hunting  and  neural  language  models  to  solve  the  winograd  schema  challenge.  Winograd  Schema  Challenge  (WSC)  is  a  pronoun  resolution  task  which  seems  to  require  reasoning  with  commonsense  knowledge.  The  needed  knowledge  is  not  present  in  the  given  text.  Automatic  extraction  of  the  needed  knowledge  is  a  bottleneck  in  solving  the  challenge.  The  existing  state-of-the-art  approach  uses  the  knowledge  embedded  in  their  pre-trained  language  model.  However,  the  language  models  only  embed  part  of  the  knowledge,  the  ones  related  to  frequently  co-existing  concepts.  This  limits  the  performance  of  such  models  on  the  WSC  problems.  In  this  work,  we  build-up  on  the  language  model  based  methods  and  augment  them  with  a  commonsense  knowledge  hunting  (using  automatic  extraction  from  text)  module  and  an  explicit  reasoning  module.  Our  end-to-end  system  built  in  such  a  manner  improves  on  the  accuracy  of  two  of  the  available  language  model  based  approaches  by  5.53%  and  7.7%  respectively.  Overall  our  system  achieves  the  state-of-the-art  accuracy  of  71.06%  on  the  WSC  dataset,  an  improvement  of  7.36%  over  the  previous  best.
0	Syntax  augmented  multilingual  bert  for  cross  lingual  transfer.  In  recent  years,  we  have  seen  a  colossal  effort  in  pre-training  multilingual  text  encoders  using  large-scale  corpora  in  many  languages  to  facilitate  cross-lingual  transfer  learning.  However,  due  to  typological  differences  across  languages,  the  cross-lingual  transfer  is  challenging.  Nevertheless,  language  syntax,  e.g.,  syntactic  dependencies,  can  bridge  the  typological  gap.  Previous  works  have  shown  that  pre-trained  multilingual  encoders,  such  as  mBERT  (CITATION),  capture  language  syntax,  helping  cross-lingual  transfer.  This  work  shows  that  explicitly  providing  language  syntax  and  training  mBERT  using  an  auxiliary  objective  to  encode  the  universal  dependency  tree  structure  helps  cross-lingual  transfer.  We  perform  rigorous  experiments  on  four  NLP  tasks,  including  text  classification,  question  answering,  named  entity  recognition,  and  task-oriented  semantic  parsing.  The  experiment  results  show  that  syntax-augmented  mBERT  improves  cross-lingual  transfer  on  popular  benchmarks,  such  as  PAWS-X  and  MLQA,  by  1.4  and  1.6  points  on  average  across  all  languages.  In  the  generalized  transfer  setting,  the  performance  boosted  significantly,  with  3.9  and  3.1  points  on  average  in  PAWS-X  and  MLQA.
0	Liir  at  semeval  2021  task  6  detection  of  persuasion  techniques  in  texts  and  images  using  clip  features.  We  describe  our  approach  for  SemEval-2021  task  6  on  detection  of  persuasion  techniques  in  multimodal  content  (memes).  Our  system  combines  pretrained  multimodal  models  (CLIP)  and  chained  classifiers.  Also,  we  propose  to  enrich  the  data  by  a  data  augmentation  technique.  Our  submission  achieves  a  rank  of  8/16  in  terms  of  F1-micro  and  9/16  with  F1-macro  on  the  test  set.
0	Transformers  on  sarcasm  detection  with  context.  Sarcasm  Detection  with  Context,  a  shared  task  of  Second  Workshop  on  Figurative  Language  Processing  (co-located  with  ACL  2020),  is  study  of  effect  of  context  on  Sarcasm  detection  in  conversations  of  Social  media.  We  present  different  techniques  and  models,  mostly  based  on  transformer  for  Sarcasm  Detection  with  Context.  We  extended  latest  pre-trained  transformers  like  BERT,  RoBERTa,  spanBERT  on  different  task  objectives  like  single  sentence  classification,  sentence  pair  classification,  etc.  to  understand  role  of  conversation  context  for  sarcasm  detection  on  Twitter  conversations  and  conversation  threads  from  Reddit.  We  also  present  our  own  architecture  consisting  of  LSTM  and  Transformers  to  achieve  the  objective.
0	Attend  to  medical  ontologies  content  selection  for  clinical  abstractive  summarization.  Sequence-to-sequence  (seq2seq)  network  is  a  well-established  model  for  text  summarization  task.  It  can  learn  to  produce  readable  content;  however,  it  falls  short  in  effectively  identifying  key  regions  of  the  source.  In  this  paper,  we  approach  the  content  selection  problem  for  clinical  abstractive  summarization  by  augmenting  salient  ontological  terms  into  the  summarizer.  Our  experiments  on  two  publicly  available  clinical  data  sets  (107,372  reports  of  MIMIC-CXR,  and  3,366  reports  of  OpenI)  show  that  our  model  statistically  significantly  boosts  state-of-the-art  results  in  terms  of  ROUGE  metrics  (with  improvements:  2.9%  RG-1,  2.5%  RG-2,  1.9%  RG-L),  in  the  healthcare  domain  where  any  range  of  improvement  impacts  patients’  welfare.
0	Calibrating  structured  output  predictors  for  natural  language  processing.  We  address  the  problem  of  calibrating  prediction  confidence  for  output  entities  of  interest  in  natural  language  processing  (NLP)  applications.  It  is  important  that  NLP  applications  such  as  named  entity  recognition  and  question  answering  produce  calibrated  confidence  scores  for  their  predictions,  especially  if  the  applications  are  to  be  deployed  in  a  safety-critical  domain  such  as  healthcare.  However  the  output  space  of  such  structured  prediction  models  are  often  too  large  to  directly  adapt  binary  or  multi-class  calibration  methods.  In  this  study,  we  propose  a  general  calibration  scheme  for  output  entities  of  interest  in  neural  network  based  structured  prediction  models.  Our  proposed  method  can  be  used  with  any  binary  class  calibration  scheme  and  a  neural  network  model.  Additionally,  we  show  that  our  calibration  method  can  also  be  used  as  an  uncertainty-aware,  entity-specific  decoding  step  to  improve  the  performance  of  the  underlying  model  at  no  additional  training  cost  or  data  requirements.  We  show  that  our  method  outperforms  current  calibration  techniques  for  Named  Entity  Recognition,  Part-of-speech  tagging  and  Question  Answering  systems.  We  also  observe  an  improvement  in  model  performance  from  our  decoding  step  across  several  tasks  and  benchmark  datasets.  Our  method  improves  the  calibration  and  model  performance  on  out-of-domain  test  scenarios  as  well.
0	Dca  based  algorithms  for  feature  selection  in  semi  supervised  support  vector  machines.  In  this  paper,  we  develop  an  efficient  method  for  feature  selection  in  Semi-Supervised  Support  Vector  Machine  (S3VM).  Using  an  appropriate  continuous  approximation  of  the  l0−norm,  we  reformulate  the  feature  selection  S3VM  problem  as  a  DC  (Difference  of  Convex  functions)  program.  DCA  (DC  Algorithm),  an  innovative  approach  in  nonconvex  programming  is  then  developed  to  solve  the  resulting  problem.  Computational  experiments  on  several  real-world  datasets  show  the  efficiency  and  the  scalability  of  our  method.
0	Automatically  mining  question  reformulation  patterns  from  search  log  data.  Natural  language  questions  have  become  popular  in  web  search.  However,  various  questions  can  be  formulated  to  convey  the  same  information  need,  which  poses  a  great  challenge  to  search  systems.  In  this  paper,  we  automatically  mined  5w1h  question  reformulation  patterns  from  large  scale  search  log  data.  The  question  reformulations  generated  from  these  patterns  are  further  incorporated  into  the  retrieval  model.  Experiments  show  that  using  question  reformulation  patterns  can  significantly  improve  the  search  performance  of  natural  language  questions.
0	Arbengvec  arabic  english  cross  lingual  word  embedding  model.  Word  Embeddings  (WE)  are  getting  increasingly  popular  and  widely  applied  in  many  Natural  Language  Processing  (NLP)  applications  due  to  their  effectiveness  in  capturing  semantic  properties  of  words;  Machine  Translation  (MT),  Information  Retrieval  (IR)  and  Information  Extraction  (IE)  are  among  such  areas.  In  this  paper,  we  propose  an  open  source  ArbEngVec  which  provides  several  Arabic-English  cross-lingual  word  embedding  models.  To  train  our  bilingual  models,  we  use  a  large  dataset  with  more  than  93  million  pairs  of  Arabic-English  parallel  sentences.  In  addition  ,  we  perform  both  extrinsic  and  intrinsic  evaluations  for  the  different  word  embedding  model  variants.  The  extrinsic  evaluation  assesses  the  performance  of  models  on  the  cross-language  Semantic  Textual  Similarity  (STS),  while  the  intrinsic  evaluation  is  based  on  the  Word  Translation  (WT)  task.
0	Modeling  financial  analysts  decision  making  via  the  pragmatics  and  semantics  of  earnings  calls.  Every  fiscal  quarter,  companies  hold  earnings  calls  in  which  company  executives  respond  to  questions  from  analysts.  After  these  calls,  analysts  often  change  their  price  target  recommendations,  which  are  used  in  equity  re-  search  reports  to  help  investors  make  deci-  sions.  In  this  paper,  we  examine  analysts’  decision  making  behavior  as  it  pertains  to  the  language  content  of  earnings  calls.  We  identify  a  set  of  20  pragmatic  features  of  analysts’  questions  which  we  correlate  with  analysts’  pre-call  investor  recommendations.  We  also  analyze  the  degree  to  which  semantic  and  pragmatic  features  from  an  earnings  call  complement  market  data  in  predicting  analysts’  post-call  changes  in  price  targets.  Our  results  show  that  earnings  calls  are  moderately  predictive  of  analysts’  decisions  even  though  these  decisions  are  influenced  by  a  number  of  other  factors  including  private  communication  with  company  executives  and  market  conditions.  A  breakdown  of  model  errors  indicates  disparate  performance  on  calls  from  different  market  sectors.
0	An  empirical  study  of  building  a  strong  baseline  for  constituency  parsing.  This  paper  investigates  the  construction  of  a  strong  baseline  based  on  general  purpose  sequence-to-sequence  models  for  constituency  parsing.  We  incorporate  several  techniques  that  were  mainly  developed  in  natural  language  generation  tasks,  e.g.,  machine  translation  and  summarization,  and  demonstrate  that  the  sequence-to-sequence  model  achieves  the  current  top-notch  parsers’  performance  (almost)  without  requiring  any  explicit  task-specific  knowledge  or  architecture  of  constituent  parsing.
0	Discriminative  reasoning  for  document  level  relation  extraction.  Document-level  relation  extraction  (DocRE)  models  generally  use  graph  networks  to  implicitly  model  the  reasoning  skill  (i.e.,  pattern  recognition,  logical  reasoning,  coreference  reasoning,  etc.)  related  to  the  relation  between  one  entity  pair  in  a  document.  In  this  paper,  we  propose  a  novel  discriminative  reasoning  framework  to  explicitly  model  the  paths  of  these  reasoning  skills  between  each  entity  pair  in  this  document.  Thus,  a  discriminative  reasoning  network  is  designed  to  estimate  the  relation  probability  distribution  of  different  reasoning  paths  based  on  the  constructed  graph  and  vectorized  document  contexts  for  each  entity  pair,  thereby  recognizing  their  relation.  Experimental  results  show  that  our  method  outperforms  the  previous  state-of-the-art  performance  on  the  large-scale  DocRE  dataset.  The  code  is  publicly  available  at  this  https  URL.
0	Social  biases  in  nlp  models  as  barriers  for  persons  with  disabilities.  Building  equitable  and  inclusive  NLP  technologies  demands  consideration  of  whether  and  how  social  attitudes  are  represented  in  ML  models.  In  particular,  representations  encoded  in  models  often  inadvertently  perpetuate  undesirable  social  biases  from  the  data  on  which  they  are  trained.  In  this  paper,  we  present  evidence  of  such  undesirable  biases  towards  mentions  of  disability  in  two  different  English  language  models:  toxicity  prediction  and  sentiment  analysis.  Next,  we  demonstrate  that  the  neural  embeddings  that  are  the  critical  first  step  in  most  NLP  pipelines  similarly  contain  undesirable  biases  towards  mentions  of  disability.  We  end  by  highlighting  topical  biases  in  the  discourse  about  disability  which  may  contribute  to  the  observed  model  biases;  for  instance,  gun  violence,  homelessness,  and  drug  addiction  are  over-represented  in  texts  discussing  mental  illness.
0	A  new  representation  in  genetic  programming  for  evolving  dispatching  rules  for  dynamic  flexible  job  shop  scheduling.  Dynamic  flexible  job  shop  scheduling  (DFJSS)  is  a  very  important  problem  with  a  wide  range  of  real-world  applications  such  as  cloud  computing  and  manufacturing.  In  DFJSS,  it  is  critical  to  make  two  kinds  of  real-time  decisions  (i.e.  the  routing  decision  that  assigns  machine  to  each  job  and  the  sequencing  decision  that  prioritises  the  jobs  in  a  machine’s  queue)  effectively  in  the  dynamic  environment  with  unpredicted  events  such  as  new  job  arrivals  and  machine  breakdowns.  Dispatching  rule  is  an  ideal  technique  for  this  purpose.  In  DFJSS,  one  has  to  design  a  routing  rule  and  a  sequencing  rule  for  making  the  two  kinds  of  decisions.  Manually  designing  these  rules  is  time  consuming  and  requires  human  expertise  which  is  not  always  available.  Genetic  programming  (GP)  has  been  applied  to  automatically  evolve  more  effective  rules  than  the  manually  designed  ones.  In  GP  for  DFJSS,  different  features  in  the  terminal  set  have  different  contributions  to  the  decision  making.  However,  the  current  GP  approaches  cannot  perfectly  find  proper  combinations  between  the  features  in  accordance  with  their  contributions.  In  this  paper,  we  propose  a  new  representation  for  GP  that  better  considers  the  different  contributions  of  different  features  and  combines  them  in  a  sophisticated  way,  thus  to  evolve  more  effective  rules.  The  results  show  that  the  proposed  GP  approach  can  achieve  significantly  better  performance  than  the  baseline  GP  in  a  range  of  job  shop  scenarios.
0	Robust  conversion  of  ccg  derivations  to  phrase  structure  trees.  We  propose  an  improved,  bottom-up  method  for  converting  CCG  derivations  into  PTB-style  phrase  structure  trees.  In  contrast  with  past  work  (Clark  and  Curran,  2009),  which  used  simple  transductions  on  category  pairs,  our  approach  uses  richer  transductions  attached  to  single  categories.  Our  conversion  preserves  more  sentences  under  round-trip  conversion  (51.1%  vs.  39.6%)  and  is  more  robust.  In  particular,  unlike  past  methods,  ours  does  not  require  ad-hoc  rules  over  non-local  features,  and  so  can  be  easily  integrated  into  a  parser.
0	Knowledgeable  reader  enhancing  cloze  style  reading  comprehension  with  external  commonsense  knowledge.  We  introduce  a  neural  reading  comprehension  model  that  integrates  external  commonsense  knowledge,  encoded  as  a  key-value  memory,  in  a  cloze-style  setting.  Instead  of  relying  only  on  document-to-question  interaction  or  discrete  features  as  in  prior  work,  our  model  attends  to  relevant  external  knowledge  and  combines  this  knowledge  with  the  context  representation  before  inferring  the  answer.  This  allows  the  model  to  attract  and  imply  knowledge  from  an  external  knowledge  source  that  is  not  explicitly  stated  in  the  text,  but  that  is  relevant  for  inferring  the  answer.  Our  model  improves  results  over  a  very  strong  baseline  on  a  hard  Common  Nouns  dataset,  making  it  a  strong  competitor  of  much  more  complex  models.  By  including  knowledge  explicitly,  our  model  can  also  provide  evidence  about  the  background  knowledge  used  in  the  RC  process.
0	Positional  artefacts  propagate  through  masked  language  model  embeddings.  In  this  work,  we  demonstrate  that  the  contextualized  word  vectors  derived  from  pretrained  masked  language  model-based  encoders  share  a  common,  perhaps  undesirable  pattern  across  layers.  Namely,  we  find  cases  of  persistent  outlier  neurons  within  BERT  and  RoBERTa’s  hidden  state  vectors  that  consistently  bear  the  smallest  or  largest  values  in  said  vectors.  In  an  attempt  to  investigate  the  source  of  this  information,  we  introduce  a  neuron-level  analysis  method,  which  reveals  that  the  outliers  are  closely  related  to  information  captured  by  positional  embeddings.  We  also  pre-train  the  RoBERTa-base  models  from  scratch  and  find  that  the  outliers  disappear  without  using  positional  embeddings.  These  outliers,  we  find,  are  the  major  cause  of  anisotropy  of  encoders’  raw  vector  spaces,  and  clipping  them  leads  to  increased  similarity  across  vectors.  We  demonstrate  this  in  practice  by  showing  that  clipped  vectors  can  more  accurately  distinguish  word  senses,  as  well  as  lead  to  better  sentence  embeddings  when  mean  pooling.  In  three  supervised  tasks,  we  find  that  clipping  does  not  affect  the  performance.
0	Bertifying  the  hidden  markov  model  for  multi  source  weakly  supervised  named  entity  recognition.  We  study  the  problem  of  learning  a  named  entity  recognition  (NER)  tagger  using  noisy  labels  from  multiple  weak  supervision  sources.  Though  cheap  to  obtain,  the  labels  from  weak  supervision  sources  are  often  incomplete,  inaccurate,  and  contradictory,  making  it  difficult  to  learn  an  accurate  NER  model.  To  address  this  challenge,  we  propose  a  conditional  hidden  Markov  model  (CHMM),  which  can  effectively  infer  true  labels  from  multi-source  noisy  labels  in  an  unsupervised  way.  CHMM  enhances  the  classic  hidden  Markov  model  with  the  contextual  representation  power  of  pre-trained  language  models.  Specifically,  CHMM  learns  token-wise  transition  and  emission  probabilities  from  the  BERT  embeddings  of  the  input  tokens  to  infer  the  latent  true  labels  from  noisy  observations.  We  further  refine  CHMM  with  an  alternate-training  approach  (CHMM-ALT).  It  fine-tunes  a  BERT-NER  model  with  the  labels  inferred  by  CHMM,  and  this  BERT-NER’s  output  is  regarded  as  an  additional  weak  source  to  train  the  CHMM  in  return.  Experiments  on  four  NER  benchmarks  from  various  domains  show  that  our  method  outperforms  state-of-the-art  weakly  supervised  NER  models  by  wide  margins.
0	Multiobjectivizing  the  hp  model  for  protein  structure  prediction.  The  hydrophobic-polar  (HP)  model  for  protein  structure  prediction  abstracts  the  fact  that  hydrophobic  interactions  are  a  dominant  force  in  the  protein  folding  process.  This  model  represents  a  hard  combinatorial  optimization  problem,  which  has  been  widely  addressed  using  evolutionary  algorithms  and  other  metaheuristics.  In  this  paper,  the  multiobjectivization  of  the  HP  model  is  proposed.  This  originally  single-objective  problem  is  restated  as  a  multiobjective  one  by  decomposing  the  conventional  objective  function  into  two  independent  objectives.  By  using  different  evolutionary  algorithms  and  a  large  set  of  test  cases,  the  new  alternative  formulation  was  compared  against  the  conventional  single-objective  problem  formulation.  As  a  result,  the  proposed  formulation  increased  the  search  performance  of  the  implemented  algorithms  in  most  of  the  cases.  Both  two-  and  three-dimensional  lattices  are  considered.  To  the  best  of  authors'  knowledge,  this  is  the  first  study  where  multiobjective  optimization  methods  are  used  for  solving  the  HP  model.
0	An  effective  and  efficient  grid  based  data  clustering  algorithm  using  intuitive  neighbor  relationship  for  data  mining.  This  paperpresents  a  new  data  clustering  technique.  It  is  a  new  grid-based  clustering  scheme  by  intuitive  neighbor  relationship  for  enhancing  data  clustering  performance.  Compared  to  other  algorithms,  this  improved  grid-based  clustering  algorithm  substantially  decreases  repetitive  clustering  checks  of  neighboring  grids  and  greatly  improve  the  efficiency  of  data  processing.  Our  simulations  demonstrate  that  the  proposed  data  clustering  technique  delivers  better  performance,  in  terms  of  clustering  correctness  rate  and  noise  filtering  rate,  than  perform  other  well-known  existing  algorithms,  GOD-CS,  CLIQUE  and  TING.  To  our  best  knowledge,  the  proposed  data  clustering  technique  may  be  the  rapid  method  in  the  world  currently.
0	Impact  of  mwe  resources  on  multiword  recognition.  In  this  paper,  we  demonstrate  the  impact  of  Multiword  Expression  (MWE)  resources  in  the  task  of  MWE  recognition  in  text.  We  present  results  based  on  the  Wiki50  corpus  for  MWE  resources,  generated  using  unsupervised  methods  from  raw  text  and  resources  that  are  extracted  using  manual  text  markup  and  lexical  resources.  We  show  that  resources  acquired  from  manual  annotation  yield  the  best  MWE  tagging  performance.  However,  a  more  finegrained  analysis  that  differentiates  MWEs  according  to  their  part  of  speech  (POS)  reveals  that  automatically  acquired  MWE  lists  outperform  the  resources  generated  from  human  knowledge  for  three  out  of  four  classes.
0	Deep  reinforcement  learning  with  a  natural  language  action  space.  This  paper  introduces  a  novel  architecture  for  reinforcement  learning  with  deep  neural  networks  designed  to  handle  state  and  action  spaces  characterized  by  natural  language,  as  found  in  text-based  games.  Termed  a  deep  reinforcement  relevance  network  (DRRN),  the  architecture  represents  action  and  state  spaces  with  separate  embedding  vectors,  which  are  combined  with  an  interaction  function  to  approximate  the  Q-function  in  reinforcement  learning.  We  evaluate  the  DRRN  on  two  popular  text  games,  showing  superior  performance  over  other  deep  Qlearning  architectures.  Experiments  with  paraphrased  action  descriptions  show  that  the  model  is  extracting  meaning  rather  than  simply  memorizing  strings  of  text.
0	Incorporating  priors  with  feature  attribution  on  text  classification.  Feature  attribution  methods,  proposed  recently,  help  users  interpret  the  predictions  of  complex  models.  Our  approach  integrates  feature  attributions  into  the  objective  function  to  allow  machine  learning  practitioners  to  incorporate  priors  in  model  building.  To  demonstrate  the  effectiveness  our  technique,  we  apply  it  to  two  tasks:  (1)  mitigating  unintended  bias  in  text  classifiers  by  neutralizing  identity  terms;  (2)  improving  classifier  performance  in  scarce  data  setting  by  forcing  model  to  focus  on  toxic  terms.  Our  approach  adds  an  L2  distance  loss  between  feature  attributions  and  task-specific  prior  values  to  the  objective.  Our  experiments  show  that  i)  a  classifier  trained  with  our  technique  reduces  undesired  model  biases  without  a  tradeoff  on  the  original  task;  ii)  incorporating  prior  helps  model  performance  in  scarce  data  settings.
0	Multi  granularity  hierarchical  attention  fusion  networks  for  reading  comprehension  and  question  answering.  This  paper  describes  a  novel  hierarchical  attention  network  for  reading  comprehension  style  question  answering,  which  aims  to  answer  questions  for  a  given  narrative  paragraph.  In  the  proposed  method,  attention  and  fusion  are  conducted  horizontally  and  vertically  across  layers  at  different  levels  of  granularity  between  question  and  paragraph.  Specifically,  it  first  encode  the  question  and  paragraph  with  fine-grained  language  embeddings,  to  better  capture  the  respective  representations  at  semantic  level.  Then  it  proposes  a  multi-granularity  fusion  approach  to  fully  fuse  information  from  both  global  and  attended  representations.  Finally,  it  introduces  a  hierarchical  attention  network  to  focuses  on  the  answer  span  progressively  with  multi-level  soft-alignment.  Extensive  experiments  on  the  large-scale  SQuAD,  TriviaQA  dataset  validate  the  effectiveness  of  the  proposed  method.  At  the  time  of  writing  the  paper,  our  model  achieves  state-of-the-art  on  the  both  SQuAD  and  TriviaQA  Wiki  leaderboard  as  well  as  two  adversarial  SQuAD  datasets.
0	Applying  prosodic  speech  features  in  mental  health  care  an  exploratory  study  in  a  life  review  intervention  for  depression.  The  present  study  aims  to  investigate  the  application  of  prosodic  speech  features  in  a  psychological  intervention  based  on  lifereview.  Several  studies  have  shown  that  speech  features  can  be  used  as  indicators  of  depression  severity,  but  these  studies  are  mainly  based  on  controlled  speech  recording  tasks  instead  of  natural  conversations.  The  present  exploratory  study  investigated  speech  features  as  indicators  of  depression  in  conversations  of  a  therapeutic  intervention.  The  changes  in  the  prosodic  speech  features  pitch,  duration  of  pauses,  and  total  duration  of  the  participant’s  speaking  time  were  studied  over  four  sessions  of  a  life-review  intervention  for  three  older  participants.  The  ecological  validity  of  the  dynamics  observed  for  prosodic  speech  features  could  not  be  established  in  the  present  study.  The  changes  in  speech  features  differed  from  what  can  be  expected  in  an  intervention  that  is  effective  in  decreasing  depression  and  were  inconsistent  with  each  other  for  each  of  the  participants.  We  suggest  future  research  to  investigate  changes  within  the  intervention  sessions,  to  relate  the  changes  in  feature  values  to  the  topical  content  of  the  speech,  and  to  relate  the  speech  features  directly  to  depression  scores.
0	Model  based  aligner  combination  using  dual  decomposition.  Unsupervised  word  alignment  is  most  often  modeled  as  a  Markov  process  that  generates  a  sentence  f  conditioned  on  its  translation  e.  A  similar  model  generating  e  from  f  will  make  different  alignment  predictions.  Statistical  machine  translation  systems  combine  the  predictions  of  two  directional  models,  typically  using  heuristic  combination  procedures  like  grow-diag-final.  This  paper  presents  a  graphical  model  that  embeds  two  directional  aligners  into  a  single  model.  Inference  can  be  performed  via  dual  decomposition,  which  reuses  the  efficient  inference  algorithms  of  the  directional  models.  Our  bidirectional  model  enforces  a  one-to-one  phrase  constraint  while  accounting  for  the  uncertainty  in  the  underlying  directional  models.  The  resulting  alignments  improve  upon  baseline  combination  heuristics  in  word-level  and  phrase-level  evaluations.
0	Learning  to  transform  and  select  elementary  trees  for  improved  syntax  based  machine  translations.  We  propose  a  novel  technique  of  learning  how  to  transform  the  source  parse  trees  to  improve  the  translation  qualities  of  syntax-based  translation  models  using  synchronous  context-free  grammars.  We  transform  the  source  tree  phrasal  structure  into  a  set  of  simpler  structures,  expose  such  decisions  to  the  decoding  process,  and  find  the  least  expensive  transformation  operation  to  better  model  word  reordering.  In  particular,  we  integrate  synchronous  binarizations,  verb  regrouping,  removal  of  redundant  parse  nodes,  and  incorporate  a  few  important  features  such  as  translation  boundaries.  We  learn  the  structural  preferences  from  the  data  in  a  generative  framework.  The  syntax-based  translation  system  integrating  the  proposed  techniques  outperforms  the  best  Arabic-English  unconstrained  system  in  NIST-08  evaluations  by  1.3  absolute  BLEU,  which  is  statistically  significant.
0	Acl2  ml  machine  learning  for  acl2.  ACL2(ml)  is  an  extension  for  the  Emacs  interface  of  ACL2.  This  tool  uses  machine-learning  to  help  the  ACL2  user  during  the  proof-development.  Namely,  ACL2(ml)  gives  hints  to  the  user  in  the  form  of  families  of  similar  theorems,  and  generates  auxiliary  lemmas  automatically.  In  this  paper,  we  present  the  two  most  recent  extensions  for  ACL2(ml).  First,  ACL2(ml)  can  suggest  now  families  of  similar  function  definitions,  in  addition  to  the  families  of  similar  theorems.  Second,  the  lemma  generation  tool  implemented  in  ACL2(ml)  has  been  improved  with  a  method  to  generate  preconditions  using  the  guard  mechanism  of  ACL2.  The  user  of  ACL2(ml)  can  also  invoke  directly  the  latter  extension  to  obtain  preconditions  for  his  own  conjectures.
0	Albert  bilstm  for  sequential  metaphor  detection.  In  our  daily  life,  metaphor  is  a  common  way  of  expression.  To  understand  the  meaning  of  a  metaphor,  we  should  recognize  the  metaphor  words  which  play  important  roles.  In  the  metaphor  detection  task,  we  design  a  sequence  labeling  model  based  on  ALBERT-LSTM-softmax.  By  applying  this  model,  we  carry  out  a  lot  of  experiments  and  compare  the  experimental  results  with  different  processing  methods,  such  as  with  different  input  sentences  and  tokens,  or  the  methods  with  CRF  and  softmax.  Then,  some  tricks  are  adopted  to  improve  the  experimental  results.  Finally,  our  model  achieves  a  0.707  F1-score  for  the  all  POS  subtask  and  a  0.728  F1-score  for  the  verb  subtask  on  the  TOEFL  dataset.
0	An  encryption  based  approach  for  protecting  privacy  in  network  based  location  systems.  Since  the  privacy  of  the  users  can  be  threatened  by  location-based  services  (LBS),  many  privacy  preserving  approaches  have  been  proposed  to  protect  the  user  location  information  during  a  request  for  LBS.  Nevertheless,  the  user's  location  information  may  still  be  exposed  during  positioning  because  of  the  nature  of  the  unreliable  wireless  environment.  This  paper  presents  a  secure  network-based  positioning  system  that  can  protect  the  privacy  of  the  users  by  encrypting  the  location  estimation.  To  preserve  privacy,  the  proposed  method  exposes  only  an  encrypted  version  of  the  user  location  based  on  asymmetric  encryption  schemes.  When  a  network  of  base  stations  sends  back  the  estimated  result,  it  uses  the  public  key  to  encrypt  the  data.  The  device  will  then  use  its  private  key  to  decrypt  the  location  information.  This  intelligent  scheme  prevents  the  exposure  of  location  information  in  the  radio  transmission  and  thus,  protects  the  location  privacy.  Our  experimental  results  show  that,  although  the  encryption  in  mobile  device  results  in  extra  energy  consumption,  the  computational  time  is  acceptable  in  current  high-level  devices.  The  positioning  accuracy  is  also  comparable  to  the  unencrypted  results  with  a  suitable  choice  of  the  scaling  factor  in  the  adopted  RSA  encryption  techniques.
0	Model  reference  adaptive  position  controller  with  smith  predictor  for  a  shaking  table  in  two  axes.  In  structural  behavior,  the  analysis  of  civil  buildings  by  seismic  tests  has  been  generalized  by  the  use  of  shaking  tables.  This  method  requires  advanced  control  systems.  In  our  research  we  show  the  implementation  of  a  Model  Reference  Adaptive  Control  (MRAC)  to  control  the  position  of  a  shaking  table,  modified  by  the  introduction  of  a  Smith  predictor  to  compensate  the  error  produced  by  the  system  delay.  The  mechanic  is  based  on  a  Slider-crank  device.  The  control  system  is  implemented  on  a  32  bits  platform  by  Microchip,  and  the  control  is  done  via  a  remote  server  using  the  RENATA  network.  The  results  of  our  adaptive  control  system  were  experimentally  verified  using  the  shaking  table  with  a  24  kg  mass  as  a  load.
0	Research  and  improvement  of  dv_hop  localization  algorithm  in  wireless  sensor  networks.  Dv_Hop  algorithm  is  a  distributed  localization  algorithm  based  on  hop  count  information  with  low  cost,  simple  implementation.  But  because  of  the  hop  distance  error  which  is  large,  it  will  result  in  a  lower  positioning  accuracy.  This  paper  has  carried  out  the  analysis  to  the  DV-Hop  algorithm,  and  the  location  technology  of  Dv-Hop  localization  algorithm  gets  improved.  The  improved  algorithm  revised  the  network  average  hop  distance  estimation  range  and  the  unknown  node  coordinates,  the  original  algorithm  and  the  improved  algorithm  are  simulated.  The  simulation  results  show  that,  in  the  same  network  environment,  the  improved  algorithm  reduces  the  location  error  by  distance  calculation  effectively,  the  localization  accuracy  is  improved  obviously.
0	Including  modal  improvisation  and  music  inspired  components  to  improve  harmony  search.  In  this  paper  we  present  new  components  to  be  included  in  harmony  search  algorithms.  These  components  are  inspired  from  music  improvisation.  The  Modal  improvisation  uses  musical  modes  rather  than  chord  progressions  as  a  harmonic  framework.  We  also  include  the  notion  of  tone  scales  that  allows  the  algorithm  to  visit  different  parts  of  the  search  space.  We  evaluate  our  approach  solving  instances  of  the  Multidimensional  Knapsack  Problem  instances.  We  compare  our  results  with  those  obtained  by  the  former  harmony  search  algorithm,  and  with  the  well-known  state-of-the-art  results.
0	Monocular  visual  odometry  based  navigation  for  a  differential  mobile  robot  with  android  os.  In  this  work,  a  real  time  Monocular  Visual  Odometry  system  to  estimate  camera  position  and  orientation  based  solely  on  image  measurements  is  proposed.  The  system  is  built  on  the  basis  of  the  fundamentals  of  Structure  from  Motion  theory,  and  requires  only  a  single  camera  to  estimate  positional  information.  Experiments  were  conducted  on  flat  ground,  under  controlled  light  conditions  environment,  in  which  and  an  Android  mobile  device  camera  was  employed  as  the  processor  and  the  system  sensor  due  to  ease  of  acquisition  and  low  price.  The  proposed  system  resulted  in  absolute  navigation  error  rates  ranging  from  0.14%  to  0.4%  of  the  travelled  distance  at  processing  rates  of  up  to  5Hz.
0	Agent  communication  for  believable  human  like  interactions  between  virtual  characters.  In  this  paper  we  present  a  model  for  realizing  believable  human-like  interaction  between  virtual  agents  situated  cognitively  in  a  MAS  on  one  side  while  embodied  in  a  virtual  environment  within  a  game  engine  on  the  other  side.  A  middleware  approach  is  taken  to  facilitate  such  agents  in  communication,  hereby  making  a  tradeoff  between  efficiency  and  believability  while  taking  into  account  the  real-time  requirements  of  games  and  simulations.
0	Attention  based  deep  reinforcement  learning  for  multi  view  environments.  In  reinforcement  learning  algorithms,  it  is  a  common  practice  to  account  for  only  a  single  view  of  the  environment  to  make  the  desired  decisions;  however,  utilizing  multiple  views  of  the  environment  can  help  to  promote  the  learning  of  complicated  policies.  Since  the  views  may  frequently  suffer  from  partial  observability,  their  provided  observation  can  have  different  levels  of  importance.  In  this  paper,  we  present  a  novel  attention-based  deep  reinforcement  learning  method  in  a  multi-view  environment  in  which  each  view  can  provide  various  representative  information  about  the  environment.  Specifically,  our  method  learns  a  policy  to  dynamically  attend  to  views  of  the  environment  based  on  their  importance  in  the  decision-making  process.  We  evaluate  the  performance  of  our  method  on  TORCS  racing  car  simulator  and  three  other  complex  3D  environments  with  obstacles.
0	Clonal  plasticity  an  autonomic  mechanism  for  multi  agent  systems  to  self  diversify.  Diversity  has  long  been  used  as  a  design  tactic  in  computer  systems  to  achieve  various  properties.  Multi-agent  systems,  in  particular,  have  utilized  diversity  to  achieve  aggregate  properties  such  as  efficiency  of  resource  allocations,  and  fairness  in  these  allocations.  However,  diversity  has  usually  been  introduced  manually  by  the  system  designer.  This  paper  proposes  a  decentralized  technique,  clonal  plasticity,  that  makes  homogeneous  agents  self-diversify,  in  an  autonomic  way.  We  show  that  clonal  plasticity  is  competitive  with  manual  diversification,  at  achieving  efficient  resource  allocations  and  fairness.
0	Coalition  structure  generation  and  cs  core  results  on  the  tractability  frontier  for  games  represented  by  mc  nets.  The  coalition  structure  generation  (CSG)  problem  consists  in  partitioning  a  group  of  agents  into  coalitions  to  maximize  the  sum  of  their  values.  We  consider  here  the  case  of  coalitional  games  whose  characteristic  function  is  compactly  represented  by  a  set  of  weighted  conjunctive  formulae  (an  MC-net).  In  this  context  the  CSG  problem  is  known  to  be  computationally  hard  in  general.    In  this  paper,  we  first  study  some  key  parameters  of  MC-nets  that  complicate  solving  make  the  CSG  problem.  Then  we  consider  a  specific  class  of  MC-nets,  called  bipolar  MC-nets,  and  prove  that  the  CSG  problem  is  polynomial  for  this  class.  Finally,  we  show  that  the  CS-core  of  a  game  represented  by  a  bipolar  MC-net  is  never  empty,  and  that  an  imputation  belonging  to  the  CS-core  can  be  computed  in  polynomial  time.
0	Combining  human  and  machine  intelligence  in  large  scale  crowdsourcing.  We  show  how  machine  learning  and  inference  can  be  harnessed  to  leverage  the  complementary  strengths  of  humans  and  computational  agents  to  solve  crowdsourcing  tasks.  We  construct  a  set  of  Bayesian  predictive  models  from  data  and  describe  how  the  models  operate  within  an  overall  crowd-sourcing  architecture  that  combines  the  efforts  of  people  and  machine  vision  on  the  task  of  classifying  celestial  bodies  defined  within  a  citizens'  science  project  named  Galaxy  Zoo.  We  show  how  learned  probabilistic  models  can  be  used  to  fuse  human  and  machine  contributions  and  to  predict  the  behaviors  of  workers.  We  employ  multiple  inferences  in  concert  to  guide  decisions  on  hiring  and  routing  workers  to  tasks  so  as  to  maximize  the  efficiency  of  large-scale  crowdsourcing  processes  based  on  expected  utility.
0	Self  organized  collective  decision  making  the  weighted  voter  model.  Collective  decision  making  in  self-organized  systems  is  challenging  because  it  relies  on  local  perception  and  local  communication.  Globally  defined  qualities  such  as  consensus  time  and  decision  accuracy  are  both  difficult  to  predict  and  difficult  to  guarantee.  We  present  the  weighted  voter  model  which  implements  a  self-organized  collective  decision  making  process.  We  provide  an  ODE  model,  a  master  equation  model  (numerically  solved  by  the  Gillespie  algorithm),  and  agent-based  simulations  of  the  proposed  decision-making  strategy.  This  set  of  models  enables  us  to  investigate  the  system  behavior  in  the  thermodynamic  limit  and  to  investigate  finite-size  effects  due  to  random  fluctuations.  Based  on  our  results,  we  give  minimum  requirements  to  guarantee  consensus  on  the  optimal  decision,  a  minimum  swarm  size  to  guarantee  a  certain  accuracy,  and  we  show  that  the  proposed  approach  scales  with  system  size  and  is  robust  to  noise.
0	Deep  abstract  q  networks.  We  examine  the  problem  of  learning  and  planning  on  high-dimensional  domains  with  long  horizons  and  sparse  rewards.  Recent  approaches  have  shown  great  successes  in  many  Atari  2600  domains.  However,  domains  with  long  horizons  and  sparse  rewards,  such  as  Montezuma's  Revenge  and  Venture,  remain  challenging  for  existing  methods.  Methods  using  abstraction  (Dietterich  2000;  Sutton,  Precup,  and  Singh  1999)  have  shown  to  be  useful  in  tackling  long-horizon  problems.  We  combine  recent  techniques  of  deep  reinforcement  learning  with  existing  model-based  approaches  using  an  expert-provided  state  abstraction.  We  construct  toy  domains  that  elucidate  the  problem  of  long  horizons,  sparse  rewards  and  high-dimensional  inputs,  and  show  that  our  algorithm  significantly  outperforms  previous  methods  on  these  domains.  Our  abstraction-based  approach  outperforms  Deep  Q-Networks  (Mnih  et  al.  2015)  on  Montezuma's  Revenge  and  Venture,  and  exhibits  backtracking  behavior  that  is  absent  from  previous  methods.
0	Testing  axioms  against  human  reward  divisions  in  cooperative  games.  Axiomatic  approaches  are  an  appealing  method  for  designing  fair  algorithms,  as  they  provide  formal  structure  for  reasoning  about  and  rationalizing  individual  decisions.  However,  to  make  these  algorithms  useful  in  practice,  their  axioms  must  appropriately  capture  social  norms.  We  explore  this  tension  between  fairness  axioms  and  socially  acceptable  decisions  in  the  context  of  cooperative  game  theory.  We  use  two  crowdsourced  experiments  to  study  people's  impartial  reward  divisions  in  cooperative  games,  focusing  on  games  that  systematically  vary  the  values  of  the  single-player  coalitions.  Our  results  show  that  people  select  rewards  that  are  remarkably  consistent,  but  place  much  more  emphasis  on  the  single-player  coalitions  than  the  Shapley  value  does.  Further,  their  reward  divisions  violate  both  the  null  player  and  additivity  axioms,  but  support  weaker  axioms.  We  argue  for  a  more  general  methodology  of  testing  axioms  against  experimental  data,  retaining  some  of  the  conceptual  simplicity  of  the  axiomatic  approach  while  still  using  people's  opinions  to  drive  the  design  of  fair  algorithms.
0	Enabling  intelligence  analysis  through  agent  support  the  cispaces  toolkit.  We  demonstrate  CISpaces,  a  system  for  agent-aided  collaborative  intelligence  analysis.  CISpaces  exploits  collaboration  to  ease  the  effort  of  constructing  hypotheses  from  acquired  information.  Argumentation-based  reasoning  is  employed  by  a  sensemaking  agent  to  identify  plausible  hypotheses  and  to  compute  their  likelihood  to  be  justified.  Information  requirements  are  handled  by  a  crowdsourcing  agent  that  elaborates  responses  mitigating  biases  and  a  provenance  agent  assists  analysts  in  assessing  the  credibility  of  hypotheses.
0	A  stitch  in  time  autonomous  model  management  via  reinforcement  learning.  Concept  drift  -  a  change,  either  sudden  or  gradual,  in  the  underlying  properties  of  data  -  is  one  of  the  most  prevalent  challenges  to  maintaining  high-performing  learned  models  over  time  in  autonomous  systems.  In  the  face  of  concept  drift,  one  can  hope  that  the  old  model  is  sufficiently  representative  of  the  new  data  despite  the  concept  drift,  one  can  discard  the  old  data  and  retrain  a  new  model  with  (often  limited)  new  data,  or  one  can  use  transfer  learning  methods  to  combine  the  old  data  with  the  new  to  create  an  updated  model.  Which  of  these  three  options  is  chosen  affects  not  only  near-term  decisions,  but  also  future  needs  to  transfer  or  retrain.  In  this  paper,  we  thus  model  response  to  concept  drift  as  a  sequential  decision  making  problem  and  formally  frame  it  as  a  Markov  Decision  Process.  Our  reinforcement  learning  approach  to  the  problem  shows  promising  results  on  one  synthetic  and  two  real-world  datasets.
0	Probabilistic  verification  for  obviously  strategyproof  mechanisms.  Obviously  strategyproof  (OSP)  mechanisms  maintain  the  incentive  compatibility  of  agents  that  are  not  fully  rational.  They  have  been  object  of  a  number  of  studies  since  their  recent  definition.  We  are  motivated  by  the  result  showing  that  OSP  mechanisms  without  money  cannot  return  good  approximations,  even  if  the  designer  monitors  the  agents  during  the  execution  of  the  mechanism  [Ferraioli  and  Ventre,  AAAI  2017].  We  ask  whether  there  are  different  (harsher)  forms  of  punishments  and  novel  ways  to  exert  control  over  the  agents  that  can  overcome  this  impossibility.  We  define  a  model  of  probabilistic  verification  wherein  agents  are  caught  misbehaving  with  a  certain  probability  and  show  how  OSP  mechanisms  without  money  can  implement  a  given  social  choice  function  at  the  cost  of  either  imposing  very  large  fines  for  lying  or  verifying  a  linear  number  of  agents.
0	You  only  lie  twice  a  multi  round  cyber  deception  game  of  questionable  veracity.  Cyber  deception  focuses  on  providing  advantage  to  defenders  through  manipulation  of  the  information  provided  to  attackers.  Game  theory  is  one  of  the  methods  that  has  been  used  to  model  cyber  deception.  In  this  work,  we  first  introduce  a  simple  game  theoretic  model  of  deception  that  captures  the  essence  of  interactions  between  an  attacker  and  defender  in  a  cyber  environment.  Second,  we  derive  closed  form  expressions  for  a  version  of  this  model  that  capture  the  optimal  strategies  for  both  the  attacker  and  defender.  Third,  we  identify  the  potential  behavior  of  an  attacker  at  noncritical  points  via  a  Markov  Decision  Process  (MDP)  simulation.
0	The  imitation  game  learned  reciprocity  in  markov  games.  Reciprocity  is  an  important  feature  of  human  social  interaction  and  underpins  our  cooperative  nature.  What  is  more,  simple  forms  of  reciprocity  have  proved  remarkably  resilient  in  matrix  game  social  dilemmas.  Most  famously,  the  tit-for-tat  strategy  performs  very  well  in  tournaments  of  Prisoner's  Dilemma.  Unfortunately  this  strategy  is  not  readily  applicable  to  the  real  world,  in  which  options  to  cooperate  or  defect  are  temporally  and  spatially  extended.  Here,  we  present  a  general  online  reinforcement  learning  algorithm  that  displays  reciprocal  behavior  towards  its  co-players.  We  show  that  it  can  induce  pro-social  outcomes  for  the  wider  group  when  learning  alongside  selfish  agents,  both  in  a  2-player  Markov  game,  and  in  5-player  intertemporal  social  dilemmas.  We  analyse  the  resulting  policies  to  show  that  the  reciprocating  agents  are  strongly  influenced  by  their  co-players'  behavior.
0	Faster  fusion  analytics  for  public  transport  event  response.  Increasing  urban  concentration  raises  operational  challenges  that  can  benefit  from  integrated  monitoring  and  decision  support.  Such  complex  systems  need  to  leverage  the  full  stack  of  analytical  methods,  from  state  estimation  using  multi-sensor  fusion  for  situational  awareness,  to  prediction  and  computation  of  optimal  responses.  The  FASTER  platform  that  we  describe  in  this  work,  deployed  at  nation  scale  and  handling  1.5  billion  public  transport  trips  a  year,  offers  such  a  full  stack  of  techniques  for  this  large-scale,  real-time  problem.  FASTER  provides  fine-grained  situational  awareness  and  real-time  decision  support  with  the  objective  of  improving  the  public  transport  commuter  experience.  The  methods  employed  range  from  statistical  machine  learning  to  agent-based  simulation  and  mixed-integer  optimization.  In  this  work  we  present  an  overview  of  the  challenges  and  methods  involved,  with  details  of  the  commuter  movement  prediction  module,  as  well  as  a  discussion  of  open  problems.
0	Monte  carlo  continual  resolving  for  online  strategy  computation  in  imperfect  information  games.  Online  game  playing  algorithms  produce  high-quality  strategies  with  a  fraction  of  memory  and  computation  required  by  their  offline  alternatives.  Continual  Resolving  (CR)  is  a  recent  theoretically  sound  approach  to  online  game  playing  that  has  been  used  to  outperform  human  professionals  in  poker.  However,  parts  of  the  algorithm  were  specific  to  poker,  which  enjoys  many  properties  not  shared  by  other  imperfect  information  games.  We  present  a  domain-independent  formulation  of  CR  applicable  to  any  two-player  zero-sum  extensive-form  games  (EFGs).  It  works  with  an  abstract  resolving  algorithm,  which  can  be  instantiated  by  various  EFG  solvers.  We  further  describe  and  implement  its  Monte  Carlo  variant  (MCCR)  which  uses  Monte  Carlo  Counterfactual  Regret  Minimization  (MCCFR)  as  a  resolver.  We  prove  the  correctness  of  CR  and  show  an  $O(T^-1/2  )$-dependence  of  MCCR's  exploitability  on  the  computation  time.  Furthermore,  we  present  an  empirical  comparison  of  MCCR  with  incremental  tree  building  to  Online  Outcome  Sampling  and  Information-set  MCTS  on  several  domains.
0	On  the  construction  of  covert  networks.  Centrality  measures  are  widely  used  to  identify  leaders  of  covert  networks.  We  study  how  a  group  of  such  leaders  can  avoid  being  detected  by  such  measures.  More  concretely,  we  study  the  hardness  of  choosing  a  set  of  edges  that  can  be  added  to  the  network  in  order  to  decrease  the  leaders'  ranking  according  to  two  fundamental  centrality  measures,  namely  degree,  and  closeness.  We  prove  that  this  problem  is  NP-complete  for  each  measure.  We  then  study  how  the  leaders  can  construct  a  network  from  scratch,  designed  specifically  for  them  to  hide  in  disguise.  We  identify  a  network  structure  that  not  only  guarantees  to  hide  the  leaders  to  a  certain  extent,  but  also  allows  them  to  spread  their  influence  across  the  network.
0	Communicating  with  unknown  teammates.  Past  research  has  investigated  a  number  of  methods  for  coordinating  teams  of  agents,  but,  with  the  growing  number  of  sources  of  agents,  it  is  likely  that  agents  will  encounter  teammates  that  do  not  share  their  coordination  methods.  Therefore,  it  is  desirable  for  agents  to  form  an  effective  ad  hoc  team.  This  research  tackles  the  problem  of  communication  in  ad  hoc  teams,  introducing  a  minimal  version  of  the  multiagent,  multi-armed  bandit  problem  with  limited  communication  between  the  agents.  This  abstract  summarizes  theoretical  results  that  prove  that  this  problem  setting  can  be  solved  in  polynomial  time  when  the  agent  knows  the  set  of  possible  teammates,  and  the  empirical  results  that  show  that  the  problems  can  be  solved  in  practice.
0	Learning  self  game  play  agents  for  combinatorial  optimization  problems.  Recent  progress  in  reinforcement  learning  (RL)  using  self-game-play  has  shown  remarkable  performance  on  several  board  games  as  well  as  video  games  (e.g.,  Atari  games  and  Dota2).  DeepMind  researchers  have  already  implemented  model-free  RL  to  play  Go  and  Chess  at  a  superhuman  level  using  neural  Monte-Carlo-Tree-Search  (neural  MCTS).  Therefore,  it  is  plausible  to  consider  that  RL,  starting  from  zero  knowledge,  can  be  applied  to  other  problems  which  can  be  converted  into  games.  We  try  to  leverage  the  computational  power  of  neural  MCTS  to  solve  a  class  of  combinatorial  optimization  problems.  Following  the  idea  of  Hintikka's  Game-Theoretical  Semantics,  we  propose  the  Zermelo  Gamification  (ZG)  to  transform  specific  combinatorial  optimization  problems  into  Zermelo  games  whose  winning  strategies  correspond  to  the  solutions  of  the  original  optimization  problem.  The  ZG  also  provides  a  specially  designed  neural  MCTS.  We  use  a  combinatorial  planning  problem  for  which  the  ground-truth  policy  is  efficiently  computable  to  demonstrate  that  ZG  is  promising.
0	Hrteam  a  framework  to  support  research  on  human  multi  robot  interaction.  The  HRTeam  framework  supports  research  on  discovering  and  evaluating  methods  for  addressing  a  range  of  issues  in  human/multi-robot  team  interaction.  Three  sample  tasks  illustrate  the  methods  currently  being  investigated:  mission  selection,  dictated  by  a  human  operator;  collision  avoidance,  taught  by  a  human  trainer;  and  targeted  exploration,  jointly  achieved  with  a  human  collaborator.  Physical  and  simulated  multi-robot  environments  are  used  to  support  this  research.
0	Boosted  and  reward  regularized  classification  for  apprenticeship  learning.  This  paper  deals  with  the  problem  of  learning  from  demonstrations,  where  an  agent  called  the  apprentice  tries  to  learn  a  behavior  from  demonstrations  of  another  agent  called  the  expert.  To  address  this  problem,  we  place  ourselves  into  the  Markov  Decision  Process  (MDP)  framework,  which  is  well  suited  for  sequential  decision  making  problems.  A  way  to  tackle  this  problem  is  to  reduce  it  to  classification  but  doing  so  we  do  not  take  into  account  the  MDP  structure.  Other  methods  which  take  into  account  the  MDP  structure  need  to  solve  MDPs  which  is  a  difficult  task  and/or  need  a  choice  of  features  which  is  problem-dependent.  The  main  contribution  of  the  paper  is  to  extend  a  large  margin  approach,  which  is  a  classification  method,  by  adding  a  regularization  term  which  takes  into  account  the  MDP  structure.  The  derived  algorithm,  called  Reward-regularized  Classification  for  Apprenticeship  Learning  (RCAL),  does  not  need  to  solve  MDPs.  But,  the  major  advantage  is  that  it  can  be  boosted:  this  avoids  the  choice  of  features,  which  is  a  drawback  of  parametric  approaches.  A  state  of  the  art  experiment  (Highway)  and  generic  experiments  (structured  Garnets)  are  conducted  to  show  the  performance  of  RCAL  compared  to  algorithms  from  the  literature.
0	Efficient  decision  making  in  a  self  organizing  robot  swarm  on  the  speed  versus  accuracy  trade  off.  We  study  a  self-organized  collective  decision-making  strategy  to  solve  the  best-of-n  decision  problem  in  a  swarm  of  robots.  We  define  a  distributed  and  iterative  decision-making  strategy.  Using  this  strategy,  robots  explore  the  available  options,  determine  the  options'  qualities,  decide  autonomously  which  option  to  take,  and  communicate  their  decision  to  neighboring  robots.  We  study  the  effectiveness  and  robustness  of  the  proposed  strategy  using  a  swarm  of  100  Kilobots.  We  study  the  well-known  speed  versus  accuracy  trade-off  analytically  by  developing  a  mean-field  model.  Compared  to  a  previously  published  simpler  method,  our  decision-making  strategy  shows  a  considerable  speed-up  but  has  lower  accuracy.  We  analyze  our  decision-making  strategy  with  particular  focus  on  how  the  spatial  density  of  robots  impacts  the  dynamics  of  decisions.  The  number  of  neighboring  robots  is  found  to  influence  the  speed  and  accuracy  of  the  decision-making  process.  Larger  neighborhoods  speed  up  the  decision  but  lower  its  accuracy.  We  observe  that  the  parity  of  the  neighborhood  cardinality  determines  whether  the  system  will  over-  or  under-perform.
0	Prediction  in  intelligence  an  empirical  comparison  of  off  policy  algorithms  on  robots.  The  ability  to  continually  make  predictions  about  the  world  may  be  central  to  intelligence.  Off-policy  learning  and  general  value  functions  (GVFs)  are  well-established  algorithmic  techniques  for  learning  about  many  signals  while  interacting  with  the  world.  In  the  past  couple  of  years,  many  ambitious  works  have  used  off-policy  GVF  learning  to  improve  control  performance  in  both  simulation  and  robotic  control  tasks.  Many  of  these  works  use  semi-gradient  temporal-difference  (TD)  learning  algorithms,  like  Q-learning,  which  are  potentially  divergent.  In  the  last  decade,  several  TD  learning  algorithms  have  been  proposed  that  are  convergent  and  computationally  efficient,  but  not  much  is  known  about  how  they  perform  in  practice,  especially  on  robots.  In  this  work,  we  perform  an  empirical  comparison  of  modern  off-policy  GVF  learning  algorithms  on  three  different  robot  platforms,  providing  insights  into  their  strengths  and  weaknesses.  We  also  discuss  the  challenges  of  conducting  fair  comparative  studies  of  off-policy  learning  on  robots  and  develop  a  new  evaluation  methodology  that  is  successful  and  applicable  to  a  relatively  complicated  robot  domain.
0	Bribery  in  balanced  knockout  tournaments.  Balanced  knockout  tournaments  comprise  a  common  format  for  sporting  competitions  and  pairwise  decision-making.  In  this  paper,  we  investigate  the  computational  complexity  of  arranging  the  tournament's  initial  seeding  and  bribing  players  to  guarantee  one  player's  victory.  We  give  a  model  of  bribery  in  which  the  organizer  can  both  arrange  the  seeding  and  bribe  players  to  decrease  their  probability  of  beating  other  players  at  a  cost,  without  exceeding  a  budget.  We  also  show  that  it  is  \NP-hard  to  determine  a  bribery  and  a  seeding  under  which  a  given  player  wins  the  tournament  with  probability  1,  even  when  the  pre-bribery  matrix  is  monotonic,  and  the  post-bribery  matrix  is  e-monotonic  and  very  close  to  the  initial  one.  We  also  show  that  for  almost  all  n  player  inputs  generated  by  a  well  known  deterministic  model  due  to  Condorcet,  one  can  always  bribe  the  "top"  $O(logn  )$  players  so  that  there  is  an  efficiently  constructible  seeding  for  whichany  player  wins.
0	Gerrymandering  over  graphs.  In  many  real-life  scenarios,  voting  problems  consist  of  several  phases:  an  overall  set  of  voters  is  partitioned  into  subgroups,  each  subgroup  chooses  a  preferred  candidate,  and  the  final  winner  is  selected  from  among  those  candidates.  The  attempt  to  skew  the  outcome  of  such  a  voting  system  through  strategic  partitioning  of  the  overall  set  of  voters  into  subgroups  is  known  as  "gerrymandering''.  We  investigate  the  problem  of  gerrymandering  over  a  network  structure;  the  voters  are  embedded  in  a  social  network,  and  the  task  is  to  divide  the  network  into  connected  components  such  that  a  target  candidate  will  win  in  a  plurality  of  the  components.  We  first  show  that  the  problem  is  NP-complete  in  the  worst  case.  We  then  perform  a  series  of  simulations,  using  random  graph  models  incorporating  a  homophily  factor.  In  these  simulations,  we  show  that  a  simple  greedy  algorithm  can  be  quite  successful  in  finding  a  partition  in  favor  of  a  specific  candidate.
0	A  graph  based  brain  parcellation  method  extracting  sparse  networks.  fMRI  is  a  powerful  tool  for  assessing  the  functioning  of  the  brain.  The  analysis  of  resting-state  fMRI  allows  to  describe  the  functional  relationship  between  the  cortical  areas.  Since  most  connectivity  analysis  methods  suffer  from  the  curse  of  dimensionality,  the  cortex  needs  to  be  first  partitioned  into  regions  of  coherent  activation  patterns.  Once  the  signals  of  these  regions  of  interest  have  been  extracted,  estimating  a  sparse  approximation  of  the  inverse  of  their  correlation  matrix  is  a  classical  way  to  robustly  describe  their  functional  interactions.  In  this  paper,  we  address  both  objectives  with  a  novel  parcellation  method  based  on  Markov  Random  Fields  that  favors  the  extraction  of  sparse  networks  of  regions.  Our  method  relies  on  state  of  the  art  rsfMRI  models,  naturally  adapts  the  number  of  parcels  to  the  data  and  is  guaranteed  to  provide  connected  regions  due  to  the  use  of  shape  priors.  The  second  contribution  of  this  paper  resides  in  two  novel  sparsity  enforcing  potentials.  Our  approach  is  validated  with  a  publicly  available  dataset.
0	Biomarker  evaluation  by  multiple  kernel  learning  for  schizophrenia  detection.  In  this  paper,  we  use  the  promising  paradigm  of  Multiple  Kernel  Learning  (MKL)  to  challenge  the  problem  of  biomarker  evaluation  for  schizophrenia  detection.  We  use  eight  different  Regions  of  Interest  (ROIs)  extracted  from  Magnetic  Resonance  Images  (MRIs).  For  each  region  we  evaluate  both  tissue  and  geometric  properties.  We  show  that  with  MKL  we  not  only  obtain  more  accurate  classifiers  than  using  single  source  support  vector  machines  (SVMs),  feature  concatenation  and  kernel  averaging  but  also  we  evaluate  the  relevance  of  the  brain  biomarkers  in  predicting  this  disease.  On  a  data  set  of  50  patients  and  50  healthy  controls  we  can  achieve  an  increase  of  7%  accuracy  compared  to  standard  methods.  Moreover,  we  are  able  to  quantify  the  importance  of  each  source  of  information  by  highlighting  the  synergies  between  the  involved  brain  characteristics.
0	Heuristic  npn  classification  for  large  functions  using  aigs  and  lexsat.  Two  Boolean  functions  are  NPN  equivalent  if  one  can  be  obtained  from  the  other  by  negating  inputs,  permuting  inputs,  or  negating  the  output.  NPN  equivalence  is  an  equivalence  relation  and  the  number  of  equivalence  classes  is  significantly  smaller  than  the  number  of  all  Boolean  functions.  This  property  has  been  exploited  successfully  to  increase  the  efficiency  of  various  logic  synthesis  algorithms.  Since  computing  the  NPN  representative  of  a  Boolean  function  is  not  scalable,  heuristics  have  been  proposed  that  are  not  guaranteed  to  find  the  representative  for  all  functions.  So  far,  these  heuristics  have  been  implemented  using  the  function’s  truth  table  representation,  and  therefore  do  not  scale  for  functions  exceeding  16  variables.
0	Generalized  optimal  reverse  prediction.  Recently  it  has  been  shown  that  classical  supervised  and  unsupervised  training  methods  can  be  unified  as  special  cases  of  so-called  “optimal  reverse  prediction”:  predicting  inputs  from  target  labels  while  optimizing  over  both  model  parameters  and  missing  labels.  Although  this  perspective  establishes  links  between  classical  training  principles,  the  existing  formulation  only  applies  to  linear  predictors  under  squared  loss,  hence  is  extremely  limited.  We  generalize  the  formulation  of  optimal  reverse  prediction  to  arbitrary  Bregman  divergences,  and  more  importantly  to  non-linear  predictors.  This  extension  is  achieved  by  establishing  a  generalized  form  of  forwardreverse  minimization  equivalence  that  holds  for  arbitrary  matching  losses.  Several  benefits  follow.  First,  a  new  variant  of  Bregman  divergence  clustering  can  be  recovered  that  incorporates  a  non-linear  data  reconstruction  model.  Second,  normalized-cut  and  kernel-based  extensions  can  be  formulated  coherently.  Finally,  a  new  semi-supervised  training  principle  can  be  recovered  for  classification  problems  that  demonstrates  some  advantages  over  the  state  of  the  art.
0	Relational  learning  with  one  network  an  asymptotic  analysis.  Theoretical  analysis  of  structured  learning  methods  has  focused  primarily  on  domains  where  the  data  consist  of  independent  (albeit  structured)  examples.  Although  the  statistical  relational  learning  (SRL)  community  has  recently  developed  many  classification  methods  for  graph  and  network  domains,  much  of  this  work  has  focused  on  modeling  domains  where  there  is  a  single  network  for  learning.  For  example,  we  could  learn  a  model  to  predict  the  political  views  of  users  in  an  online  social  network,  based  on  the  friendship  relationships  among  users.  In  this  example,  the  data  would  be  drawn  from  a  single  large  network  (e.g.,  Facebook)  and  increasing  the  data  size  would  correspond  to  acquiring  a  larger  graph.  Although  SRL  methods  can  successfully  improve  classification  in  these  types  of  domains,  there  has  been  little  theoretical  analysis  addressing  the  issue  of  single  network  domains.  In  particular,  the  asymptotic  properties  of  estimation  are  not  clear  if  the  size  of  the  model  grows  with  the  size  of  the  network.  In  this  work,  we  focus  on  outlining  the  conditions  under  which  learning  from  a  single  network  will  be  asymptotically  consistent  and  normal.  Moreover,  we  compare  the  properties  of  maximum  likelihood  estimation  (MLE)  with  that  of  generalized  maximum  pseudolikelihood  estimation  (MPLE)  and  use  the  resulting  understanding  to  propose  novel  MPLE  estimators  for  single  network  domains.  We  include  empirical  analysis  on  both  synthetic  and  real  network  data  to  illustrate  the  findings.  Appearing  in  Proceedings  of  the  14  International  Conference  on  Artificial  Intelligence  and  Statistics  (AISTATS)  2011,  Fort  Lauderdale,  FL,  USA.  Volume  15  of  JMLR:  W&CP  15.  Copyright  2011  by  the  authors.
0	Stochastic  spectral  descent  for  restricted  boltzmann  machines.  Restricted  Boltzmann  Machines  (RBMs)  are  widely  used  as  building  blocks  for  deep  learning  models.  Learning  typically  proceeds  by  using  stochastic  gradient  descent,  and  the  gradients  are  estimated  with  sampling  methods.  However,  the  gradient  estimation  is  a  computational  bottleneck,  so  better  use  of  the  gradients  will  speed  up  the  descent  algorithm.  To  this  end,  we  rst  derive  upper  bounds  on  the  RBM  cost  function,  then  show  that  descent  methods  can  have  natural  advantages  by  operating  in  the‘1  and  Shatten1  norm.  We  introduce  a  new  method  called  \Stochastic  Spectral  Descent"  that  updates  parameters  in  the  normed  space.  Empirical  results  show  dramatic  improvements  over  stochastic  gradient  descent,  and  have  only  have  a  fractional  increase  on  the  per-iteration  cost.
0	Connected  sub  graph  detection.  We  characterize  the  family  of  connected  subgraphs  in  terms  of  linear  matrix  inequalities  (LMI)  with  additional  integrality  constraints.  We  then  show  that  convex  relaxations  of  the  integral  LMI  lead  to  parameterization  of  all  weighted  connected  subgraphs.  These  developments  allow  for  optimizing  arbitrary  graph  functionals  under  connectivity  constraints.  For  concreteness  we  consider  the  connected  sub-graph  detection  problem  that  arises  in  a  number  of  applications  including  network  intrusion,  disease  outbreaks,  and  video  surveillance.  In  these  applications  feature  vectors  are  associated  with  nodes  and  edges  of  a  graph.  The  problem  is  to  decide  whether  or  not  the  null  hypothesis  is  true  based  on  the  measured  features.  For  simplicity  we  consider  the  elevated  mean  problem  wherein  feature  values  at  various  nodes  are  distributed  IID  under  the  null  hypothesis.  The  non-null  (positive)  hypothesis  is  distinguished  from  the  null  hypothesis  by  the  fact  that  feature  values  on  some  unknown  connected  sub-graph  has  elevated  mean.
0	Efficient  distributed  linear  classification  algorithms  via  the  alternating  direction  method  of  multipliers.  Linear  classification  has  demonstrated  success  in  many  areas  of  applications.  Modern  algorithms  for  linear  classification  can  train  reasonably  good  models  while  going  through  the  data  in  only  tens  of  rounds.  However,  large  data  often  does  not  fit  in  the  memory  of  a  single  machine,  which  makes  the  bottleneck  in  large-scale  learning  the  disk  I/O,  not  the  CPU.  Following  this  observation,  Yu  et  al.  (2010)  made  significant  progress  in  reducing  disk  usage,  and  their  algorithms  now  outperform  LIBLINEAR.  In  this  paper,  rather  than  optimizing  algorithms  on  a  single  machine,  we  propose  and  implement  distributed  algorithms  that  achieve  parallel  disk  loading  and  access  the  disk  only  once.  Our  large-scale  learning  algorithms  are  based  on  the  framework  of  alternating  direction  methods  of  multipliers.  The  framework  derives  a  subproblem  that  remains  to  be  solved  eciently  for  which  we  propose  using  dual  coordinate  descent  and  trust  region  Newton  method.  Our  experimental  evaluations  on  large  datasets  demonstrate  that  the  proposed  algorithms  achieve  significant  speedup  over  the  classifier  proposed  by  Yu  et  al.  running  on  a  single  machine.  Our  algorithms  are  faster  than  existing  distributed  solvers,  such  as  Zinkevich  et  al.  (2010)’s  parallel  stochastic  gradient  descent  and  Vowpal  Wabbit.
0	Heterogeneous  domain  adaptation  for  multiple  classes.  In  this  paper,  we  present  an  efficient  multi-class  heterogeneous  domain  adaptation  method,  where  data  from  source  and  target  domains  are  represented  by  heterogeneous  features  of  different  dimensions.  Specifically,  we  propose  to  reconstruct  a  sparse  feature  transformation  matrix  to  map  the  weight  vector  of  classifiers  learned  from  the  source  domain  to  the  target  domain.  We  cast  this  learning  task  as  a  compressed  sensing  problem,  where  each  binary  classifier  induced  from  multiple  classes  can  be  deemed  as  a  measurement  sensor.  Based  on  the  compressive  sensing  theory,  the  estimation  error  of  the  transformation  matrix  decreases  with  the  increasing  number  of  classifiers.  Therefore,  to  guarantee  reconstruction  performance,  we  construct  sufficiently  many  binary  classifiers  based  on  the  error  correcting  output  coding.  Extensive  experiments  are  conducted  on  both  a  toy  dataset  and  three  real-world  datasets  to  verify  the  superiority  of  our  proposed  method  over  existing  state-of-the-art  HDA  methods  in  terms  of  prediction  accuracy.
0	Graphlet  decomposition  of  a  weighted  network.  We  introduce  the  graphlet  decomposition  of  a  weighted  network,  which  encodes  a  notion  of  social  information  based  on  social  structure.  We  develop  a  scalable  algorithm,  which  combines  EM  with  Bron-Kerbosch  in  a  novel  fashion,  for  estimating  the  parameters  of  the  model  underlying  graphlets  using  one  network  sample.  We  explore  theoretical  properties  of  graphlets,  including  computational  complexity,  redundancy  and  expected  accuracy.  We  test  graphlets  on  synthetic  data,  and  we  analyze  messaging  on  Facebook  and  crime  associations  in  the  19th  century.
0	Correcting  the  bias  in  least  squares  regression  with  volume  rescaled  sampling.  Consider  linear  regression  where  the  examples  are  generated  by  an  unknown  distribution  on  R^d  x  R.  Without  any  assumptions  on  the  noise,  the  linear  least  squares  solution  for  any  i.i.d.  sample  will  typically  be  biased  w.r.t.  the  least  squares  optimum  over  the  entire  distribution.  However,  we  show  that  if  an  i.i.d.  sample  of  any  size  k  is  augmented  by  a  certain  small  additional  sample,  then  the  solution  of  the  combined  sample  becomes  unbiased.  We  show  this  when  the  additional  sample  consists  of  d  points  drawn  jointly  according  to  the  input  distribution  rescaled  by  the  squared  volume  spanned  by  the  points.  Furthermore,  we  propose  algorithms  to  sample  from  this  volume-rescaled  distribution  when  the  data  distribution  is  only  known  through  an  i.i.d  sample.
0	Recovery  guarantees  for  quadratic  tensors  with  limited  observations.  We  consider  the  tensor  completion  problem  of  predicting  the  missing  entries  of  a  tensor.  The  commonly  used  CP  model  has  a  triple  product  form,  but  an  alternate  family  of  quadratic  models  which  are  the  sum  of  pairwise  products  instead  of  a  triple  product  have  emerged  from  applications  such  as  recommendation  systems.  Non-convex  methods  are  the  method  of  choice  for  learning  quadratic  models,  and  this  work  examines  their  sample  complexity  and  error  guarantee.  Our  main  result  is  that  with  the  number  of  samples  being  only  linear  in  the  dimension,  all  local  minima  of  the  mean  squared  error  objective  are  global  minima  and  recover  the  original  tensor.  We  substantiate  our  theoretical  results  with  experiments  on  synthetic  and  real-world  data.
0	Semi  supervised  kernel  based  temporal  clustering.  In  this  paper,  we  adapt  two  existing  methods  to  perform  semi-supervised  temporal  clustering:  Aligned  Cluster  Analysis  (ACA),  a  temporal  clustering  algorithm,  and  Constrained  Spectral  Clustering,  a  semi-supervised  clustering  algorithm.  In  the  first  method,  we  add  side  information  in  the  form  of  pair  wise  constraints  to  its  objective  function,  and  in  the  second,  we  add  a  temporal  search  to  its  framework.  We  also  extend  both  methods  by  propagating  the  constraints  throughout  the  whole  similarity  matrix.  In  order  to  validate  the  advantage  of  the  proposed  semi-supervised  methods  to  temporal  clustering,  we  evaluate  them  in  comparison  to  their  original  versions  as  well  as  another  semi-supervised  temporal  cluster  on  three  temporal  datasets.  The  results  show  that  the  proposed  methods  are  competitive  and  provide  good  improvement  over  the  unsupervised  approaches.
0	Supervised  transfer  learning  for  product  information  question  answering.  Popular  e-commerce  websites  such  as  Amazon  offer  community  question  answering  systems  for  users  to  pose  product-related  questions  and  experienced  customers  may  provide  answers  voluntarily.  In  this  paper,  we  show  that  the  large  volume  of  existing  community  question  answering  data  can  be  beneficial  when  building  a  system  for  answering  questions  related  to  product  facts  and  specifications.  Our  experimental  results  demonstrate  that  the  performance  of  a  model  for  answering  questions  related  to  products  listed  in  the  Home  Depot  website  can  be  improved  by  a  large  margin  via  a  simple  transfer  learning  technique  from  an  existing  large-scale  Amazon  community  question  answering  dataset.  Transfer  learning  can  result  in  an  increase  of  about  10%  in  accuracy  in  the  experimental  setting  where  we  restrict  the  size  of  the  data  of  the  target  task  used  for  training.  As  an  application  of  this  work,  we  integrate  the  best  performing  model  trained  in  this  work  into  a  mobile-based  shopping  assistant  and  show  its  usefulness.
0	Deep  transfer  learning  via  restricted  boltzmann  machine  for  document  classification.  Transfer  learning  aims  to  improve  a  targeted  learning  task  using  other  related  auxiliary  learning  tasks  and  data.  Most  current  transfer-learning  methods  focus  on  scenarios  where  the  auxiliary  and  the  target  learning  tasks  are  very  similar:  either  (some  of)  the  auxiliary  data  can  be  directly  used  as  training  examples  for  the  target  task  or  the  auxiliary  and  the  target  data  share  the  same  representation.  However,  in  many  cases  the  connection  between  the  auxiliary  and  the  target  tasks  can  be  remote.  Only  a  few  features  derived  from  the  auxiliary  data  may  be  helpful  for  the  target  learning.  We  call  such  scenario  the  deep  transfer-learning  scenario  and  we  introduce  a  novel  transfer-learning  method  for  deep  transfer.  Our  method  uses  restricted  Boltzmann  machine  to  discover  a  set  of  hierarchical  features  from  the  auxiliary  data.  We  then  select  from  these  features  a  subset  that  are  helpful  for  the  target  learning,  using  a  selection  criterion  based  on  the  concept  of  kernel-target  alignment.  Finally,  the  target  data  are  augmented  with  the  selected  features  before  training.  Our  experiment  results  show  that  this  transfer  method  is  effective.  It  can  improve  classification  accuracy  by  up  to  more  than  10%,  even  when  the  connection  between  the  auxiliary  and  the  target  tasks  is  not  apparent.
0	Machine  learning  and  text  mining  of  trophic  links.  Machine  Learning  has  been  used  to  automatically  generate  a  probabilistic  food-web  from  Farm  Scale  Evaluation  (FSE)  data.  The  initial  food  web  proposed  by  machine  learning  has  been  examined  by  domain  experts  and  comparison  with  the  literature  shows  that  many  of  the  links  are  corroborated.  The  FSE  data  were  collected  using  two  different  sampling  techniques,  namely  Vortis  and  pitfall.  The  corroboration  of  the  initial  Vortis  food  web,  generated  by  machine  learning,  was  performed  manually  by  the  domain  experts.  However,  manual  corroboration  of  hypothetical  trophic  links  is  difficult  and  requires  significant  amounts  of  time.  In  this  paper  we  review  the  method  and  the  main  results  on  machine  learning  of  trophic  links.  We  study  common  trophic  links  from  Vortis  and  pitfall  data.  We  also  describe  a  new  method  and  present  initial  results  on  automatic  corroboration  of  trophic  links  using  text  mining.
0	Machine  learning  for  predicting  the  impact  point  of  a  low  speed  vehicle  crash.  Using  time  series  in-car  data,  this  research  focuses  on  predicting  the  point  of  impact  of  a  low  speed  crash  by  developing  an  automatized  machine  learning  approach  for  time  series  applications.  After  an  initial  data  exploration,  we  discuss  the  extraction  of  features  from  time  series  and  different  ways  to  select  the  most  relevant  features.  From  3,176  extracted  features  9  are  selected  and  used  for  a  classification  with  a  decision  tree.  To  optimize  the  hyper-parameters  of  the  decision  tree  algorithm,  a  randomized  search  with  50,000  iterations  is  conducted.  The  modeling  results  are  graphically  presented  and  discussed.  With  a  final  prediction  accuracy  of  89%  (cross-validated  76%),  the  optimized  decision  tree  offers  great  potential  for  utilization  in  vehicle  insurance  processing  for  automatized  settlement  of  low-speed  crash  damages.
0	A  hybrid  approach  for  incorporating  deep  visual  features  and  side  channel  information  with  applications  to  amd  detection.  This  work  investigates  a  hybrid  method  based  on  random  forests  and  deep  image  features  to  combine  non-visual  side  channel  information  with  image  data  for  classification.  We  apply  this  to  automated  retinal  image  analysis  (ARIA)  and  the  detection  of  age-related  macular  degeneration  (AMD).  For  evaluation,  we  use  a  dataset  collected  by  the  National  Institute  of  Health  with  over  4000  study  participants.  The  non-visual  side  channel  data  includes  information  related  to  demographics  (e.g.  ethnicity),  lifestyle  (e.g.  sunlight  exposure),  and  prior  conditions  (e.g.  cataracts).  Our  study,  which  compares  the  performance  of  different  feature  combinations,  offers  preliminary  results  that  constitute  a  baseline  for  future  investigations  on  joint  deep  visual  and  side  channel  feature  exploitation  for  AMD  detection.  This  approach  could  potentially  be  used  for  other  medical  image  analysis  problems.
0	Underwater  place  recognition  in  unknown  environments  with  triplet  based  acoustic  image  retrieval.  Forward-looking  sonars  (FLS)  are  perception  sensors  that  are  not  affected  by  underwater  turbidity.  FLS  are  used  in  Remotely  Operated  Vehicles  (ROVs)  to  help  them  in  the  tasks  of  exploration,  navigation  and  region  mapping.  Besides  the  advantages  of  working  with  acoustic  images  rather  than  optical  images,  the  former  presents  various  challenges  inherent  to  their  construction.  Classic  Computer  Vision  (CV)  algorithms  do  not  achieve  the  same  success  with  acoustic  images.  Furthermore,  data-driven  approaches  are  dictating  the  state-of-the-art  in  several  tasks  that  require  feature  extraction.  For  example,  Convolutional  Neural  Networks  (CNNs)  are  already  been  used  in  several  CV  problems  such  as  classification,  image  matching,  image  retrieval,  place  recognition  and  one-shot  learning.  CNNs  are  showing  promising  results  for  problems  with  FLS  images  as  well.  Unfortunately,  there  are  as  not  as  many  public  datasets  and  methods  for  FLS  problems  as  we  have  for  optical  images.  Knowing  that  CNNs  are  capable  of  mapping  correctly  millions  of  images  into  thousands  of  labels,  we  are  proposing  a  novel  framework  of  feature  learning  strategy  for  FLS  images.  In  order  to  evaluate  how  well  the  methods  generalize,  we  selected  three  different  FLS  annotated  datasets  for  our  experiments.  Two  of  them  are  real-world  FLS  images  from  a  harbour  environment  from  different  locations.  The  third  is  generated  from  a  custom  3D  scene  integrated  with  open-source  underwater  robot  simulators.  In  our  experiments,  we  compared  our  method  with  state-of-theart  approaches  in  an  unknown  environment  achieving  superior  results.
0	Improving  explainability  of  image  classification  in  scenarios  with  class  overlap  application  to  covid  19  and  pneumonia.  Trust  in  predictions  made  by  machine  learning  models  is  increased  if  the  model  generalizes  well  on  previously  unseen  samples  and  when  inference  is  accompanied  by  cogent  explanations  of  the  reasoning  behind  predictions.  In  the  image  classification  domain,  generalization  can  be  assessed  through  accuracy,  sensitivity,  and  specificity.  Explainability  can  be  assessed  by  how  well  the  model  localizes  the  object  of  interest  within  an  image.  However,  both  generalization  and  explainability  through  localization  are  degraded  in  scenarios  with  significant  overlap  between  classes.  We  propose  a  method  based  on  binary  expert  networks  that  enhances  the  explainability  of  image  classifications  through  better  localization  by  mitigating  the  model  uncertainty  induced  by  class  overlap.  Our  technique  performs  discriminative  localization  on  images  that  contain  features  with  significant  class  overlap,  without  explicitly  training  for  localization.  Our  method  is  particularly  promising  in  real-world  class  overlap  scenarios,  such  as  COVID-19  and  pneumonia,  where  expertly  labeled  data  for  localization  is  not  readily  available.  This  can  be  useful  for  early,  rapid,  and  trustworthy  screening  for  COVID-19.
0	An  evolutionary  many  objective  optimization  algorithm  using  reference  point  based  nondominated  sorting  approach  part  ii  handling  constraints  and  extending  to  an  adaptive  approach.  In  the  precursor  paper,  a  many-objective  optimization  method  (NSGA-III),  based  on  the  NSGA-II  framework,  was  suggested  and  applied  to  a  number  of  unconstrained  test  and  practical  problems  with  box  constraints  alone.  In  this  paper,  we  extend  NSGA-III  to  solve  generic  constrained  many-objective  optimization  problems.  In  the  process,  we  also  suggest  three  types  of  constrained  test  problems  that  are  scalable  to  any  number  of  objectives  and  provide  different  types  of  challenges  to  a  many-objective  optimizer.  A  previously  suggested  MOEA/D  algorithm  is  also  extended  to  solve  constrained  problems.  Results  using  constrained  NSGA-III  and  constrained  MOEA/D  show  an  edge  of  the  former,  particularly  in  solving  problems  with  a  large  number  of  objectives.  Furthermore,  the  NSGA-III  algorithm  is  made  adaptive  in  updating  and  including  new  reference  points  on  the  fly.  The  resulting  adaptive  NSGA-III  is  shown  to  provide  a  denser  representation  of  the  Pareto-optimal  front,  compared  to  the  original  NSGA-III  with  an  identical  computational  effort.  This,  and  the  original  NSGA-III  paper,  together  suggest  and  amply  test  a  viable  evolutionary  many-objective  optimization  algorithm  for  handling  constrained  and  unconstrained  problems.  These  studies  should  encourage  researchers  to  use  and  pay  further  attention  in  evolutionary  many-objective  optimization.
0	Safe  scale  adaptive  fitness  evaluation  method  for  expensive  optimization  problems.  The  key  challenge  of  expensive  optimization  problems  (EOP)  is  that  evaluating  the  true  fitness  value  of  the  solution  is  computationally  expensive.  A  common  method  to  deal  with  this  issue  is  to  seek  for  a  less  expensive  surrogate  model  to  replace  the  original  expensive  objective  function.  However,  this  method  also  brings  in  model  approximation  error.  To  efficiently  solve  the  EOP,  a  novel  scale-adaptive  fitness  evaluation  (SAFE)  method  is  proposed  in  this  article  to  directly  evaluate  the  true  fitness  value  of  the  solution  on  the  original  objective  function.  To  reduce  the  computational  cost,  the  SAFE  method  uses  a  set  of  evaluation  methods  (EM)  with  different  accuracy  scales  to  cooperatively  complete  the  fitness  evaluation  process.  The  basic  idea  is  to  adopt  the  low-accuracy  scale  EM  to  fast  locate  promising  regions  and  utilize  the  high-accuracy  scale  EM  to  refine  the  solution  accuracy.  To  this  aim,  two  EM  switch  strategies  are  proposed  in  the  SAFE  method  to  adaptively  control  the  multiple  EMs  according  to  different  evolutionary  stages  and  search  requirements.  Moreover,  a  neighbor  best-based  evaluation  (NBE)  strategy  is  also  put  forward  to  evaluate  the  solution  according  to  its  nearest  high-quality  evaluated  solution,  which  can  further  reduce  computational  cost.  Extensive  experiments  are  carried  out  on  the  case  study  of  crowdshipping  scheduling  problem  in  the  smart  city  to  verify  the  effectiveness  and  efficiency  of  the  proposed  SAFE  method,  and  to  investigate  the  effects  of  the  two  EM  switch  strategies  and  the  NBE  strategy.  Experimental  results  show  that  the  proposed  SAFE  method  achieves  better  solution  quality  than  some  baseline  and  state-of-the-art  algorithms,  indicating  an  efficient  method  for  solving  EOP  with  a  better  balance  between  solution  accuracy  and  computational  cost.
0	A  generic  test  suite  for  evolutionary  multifidelity  optimization.  Many  real-world  optimization  problems  involve  computationally  intensive  numerical  simulations  to  accurately  evaluate  the  quality  of  solutions.  Usually,  the  fidelity  of  the  simulations  can  be  controlled  using  certain  parameters  and  there  is  a  tradeoff  between  simulation  fidelity  and  computational  cost,  i.e.,  the  higher  the  fidelity,  the  more  complex  the  simulation  will  be.  To  reduce  the  computational  time  in  simulation-driven  optimization,  it  is  a  common  practice  to  use  multiple  fidelity  levels  in  search  for  the  optimal  solution.  So  far,  not  much  work  has  been  done  in  evolutionary  optimization  that  considers  multiple  fidelity  levels  in  fitness  evaluations.  In  this  paper,  we  aim  to  develop  test  suites  that  are  able  to  capture  some  important  characteristics  in  real-world  multifidelity  optimization,  thereby  offering  a  useful  benchmark  for  developing  evolutionary  algorithms  for  multifidelity  optimization.  To  demonstrate  the  usefulness  of  the  proposed  test  suite,  three  strategies  for  adapting  the  fidelity  level  of  the  test  problems  during  optimization  are  suggested  and  embedded  in  a  particle  swarm  optimization  (PSO)  algorithm.  Our  simulation  results  indicate  that  the  use  of  changing  fidelity  is  able  to  enhance  the  performance  and  reduce  the  computational  cost  of  the  PSO,  which  is  desired  in  solving  expensive  optimization  problems.
0	A  multi  facet  survey  on  memetic  computation.  Memetic  computation  is  a  paradigm  that  uses  the  notion  of  meme(s)  as  units  of  information  encoded  in  computational  representations  for  the  purpose  of  problem-solving.  It  covers  a  plethora  of  potentially  rich  meme-inspired  computing  methodologies,  frameworks  and  operational  algorithms  including  simple  hybrids,  adaptive  hybrids  and  memetic  automaton.  In  this  paper,  a  comprehensive  multi-facet  survey  of  recent  research  in  memetic  computation  is  presented.
0	Evolution  of  repertoire  based  control  for  robots  with  complex  locomotor  systems.  The  evolution  of  task-oriented  control  for  robots  with  complex  locomotor  systems  is  currently  out  of  reach  for  traditional  evolutionary  computation  techniques,  as  the  coordination  of  a  high  number  of  locomotion  parameters  in  response  to  the  robot’s  sensory  inputs  is  extremely  challenging.  Evolutionary  techniques  have  therefore  mainly  been  applied  to  the  optimization  of  specific  locomotion  patterns,  such  as  forward  motion.  In  this  paper,  we  explore  the  evolutionary  repertoire-based  control  (EvoRBC)  approach,  which  divides  the  synthesis  of  control  into  two  steps:  1)  the  evolution  of  a  repertoire  of  locomotion  primitives  using  a  quality  diversity  algorithm  and  2)  the  evolution  of  a  high-level  arbitrator  that  leverages  the  locomotion  primitives  in  the  repertoire  to  solve  a  given  task.  We  comprehensively  study  the  main  components  of  the  EvoRBC  approach  using  a  four-wheel  steering  robot.  We  then  conduct  a  set  of  experiments  in  simulation  using  a  hexapod  robot.  Our  results  show  that  EvoRBC  is  robust  to  parameter  variations,  and  for  all  the  robots  tested,  it  is  able  to  evolve  controllers  for  a  maze  navigation  task  and  significantly  outperforms  the  traditional  evolutionary  robotics  approach.
0	Cooperative  coevolution  with  route  distance  grouping  for  large  scale  capacitated  arc  routing  problems.  In  this  paper,  a  divide-and-conquer  approach  is  proposed  to  solve  the  large-scale  capacitated  arc  routing  problem  (LSCARP)  more  effectively.  Instead  of  considering  the  problem  as  a  whole,  the  proposed  approach  adopts  the  cooperative  coevolution  (CC)  framework  to  decompose  it  into  smaller  ones  and  solve  them  separately.  An  effective  decomposition  scheme  called  the  route  distance  grouping  (RDG)  is  developed  to  decompose  the  problem.  Its  merit  is  twofold.  First,  it  employs  the  route  information  of  the  best-so-far  solution,  so  that  the  quality  of  the  decomposition  is  upper  bounded  by  that  of  the  best-so-far  solution.  Thus,  it  can  keep  improving  the  decomposition  by  updating  the  best-so-far  solution  during  the  search.  Second,  it  defines  a  distance  between  routes,  based  on  which  the  potentially  better  decompositions  can  be  identified.  Therefore,  RDG  is  able  to  obtain  promising  decompositions  and  focus  the  search  on  the  promising  regions  of  the  vast  solution  space.  Experimental  studies  verified  the  efficacy  of  RDG  on  the  instances  with  a  large  number  of  tasks  and  tight  capacity  constraints,  where  it  managed  to  obtain  significantly  better  results  than  its  counterpart  without  decomposition  in  a  much  shorter  time.  Furthermore,  the  best-known  solutions  of  the  EGL-G  LSCARP  instances  are  much  improved.
0	Shift  based  density  estimation  for  pareto  based  algorithms  in  many  objective  optimization.  It  is  commonly  accepted  that  Pareto-based  evolutionary  multiobjective  optimization  (EMO)  algorithms  encounter  difficulties  in  dealing  with  many-objective  problems.  In  these  algorithms,  the  ineffectiveness  of  the  Pareto  dominance  relation  for  a  high-dimensional  space  leads  diversity  maintenance  mechanisms  to  play  the  leading  role  during  the  evolutionary  process,  while  the  preference  of  diversity  maintenance  mechanisms  for  individuals  in  sparse  regions  results  in  the  final  solutions  distributed  widely  over  the  objective  space  but  distant  from  the  desired  Pareto  front.  Intuitively,  there  are  two  ways  to  address  this  problem:  1)  modifying  the  Pareto  dominance  relation  and  2)  modifying  the  diversity  maintenance  mechanism  in  the  algorithm.  In  this  paper,  we  focus  on  the  latter  and  propose  a  shift-based  density  estimation  (SDE)  strategy.  The  aim  of  our  study  is  to  develop  a  general  modification  of  density  estimation  in  order  to  make  Pareto-based  algorithms  suitable  for  many-objective  optimization.  In  contrast  to  traditional  density  estimation  that  only  involves  the  distribution  of  individuals  in  the  population,  SDE  covers  both  the  distribution  and  convergence  information  of  individuals.  The  application  of  SDE  in  three  popular  Pareto-based  algorithms  demonstrates  its  usefulness  in  handling  many-objective  problems.  Moreover,  an  extensive  comparison  with  five  state-of-the-art  EMO  algorithms  reveals  its  competitiveness  in  balancing  convergence  and  diversity  of  solutions.  These  findings  not  only  show  that  SDE  is  a  good  alternative  to  tackle  many-objective  problems,  but  also  present  a  general  extension  of  Pareto-based  algorithms  in  many-objective  optimization.
0	Evolving  classifiers  to  recognize  the  movement  characteristics  of  parkinson  s  disease  patients.  Parkinson's  disease  is  a  debilitating  neurological  condition  that  affects  approximately  1  in  500  people  and  often  leads  to  severe  disability.  To  improve  clinical  care,  better  assessment  tools  are  needed  that  increase  the  accuracy  of  differential  diagnosis  and  disease  monitoring.  In  this  paper,  we  report  how  we  have  used  evolutionary  algorithms  to  induce  classifiers  capable  of  recognizing  the  movement  characteristics  of  Parkinson's  disease  patients.  These  diagnostically  relevant  patterns  of  movement  are  known  to  occur  over  multiple  time  scales.  To  capture  this,  we  used  two  different  classifier  architectures:  sliding-window  genetic  programming  classifiers,  which  model  over-represented  local  patterns  that  occur  within  time  series  data,  and  artificial  biochemical  networks,  computational  dynamical  systems  that  respond  to  dynamical  patterns  occurring  over  longer  time  scales.  Classifiers  were  trained  and  validated  using  movement  recordings  of  49  patients  and  41  age-matched  controls  collected  during  a  recent  clinical  study.  By  combining  classifiers  with  diverse  behaviors,  we  were  able  to  construct  classifier  ensembles  with  diagnostic  accuracies  in  the  region  of  95%,  comparable  to  the  accuracies  achieved  by  expert  clinicians.  Further  analysis  indicated  a  number  of  features  of  diagnostic  relevance,  including  the  differential  effect  of  handedness  and  the  over-representation  of  certain  patterns  of  acceleration.
0	Asvuniofleipzig  sentiment  analysis  in  twitter  using  data  driven  machine  learning  techniques.  This  paper  describes  University  of  Leipzig’s  approach  to  SemEval-2013  task  2B  on  Sentiment  Analysis  in  Twitter:  message  polarity  classification.  Our  system  is  designed  to  function  as  a  baseline,  to  see  what  we  can  accomplish  with  well-understood  and  purely  data-driven  lexical  features,  simple  generalizations  as  well  as  standard  machine  learning  techniques:  We  use  one-against-one  Support  Vector  Machines  with  asymmetric  cost  factors  and  linear  “kernels”  as  classifiers,  word  uni-  and  bigrams  as  features  and  additionally  model  negation  of  word  uni-  and  bigrams  in  word  n-gram  feature  space.  We  consider  generalizations  of  URLs,  user  names,  hash  tags,  repeated  characters  and  expressions  of  laughter.  Our  method  ranks  23  out  of  all  48  participating  systems,  achieving  an  averaged  (positive,  negative)  F-Score  of  0.5456  and  an  averaged  (positive,  negative,  neutral)  F-Score  of  0.595,  which  is  above  median  and  average.
0	Distributed  containment  control  for  multiple  unknown  second  order  nonlinear  systems  with  application  to  networked  lagrangian  systems.  In  this  paper,  we  consider  the  distributed  containment  control  problem  for  multiagent  systems  with  unknown  nonlinear  dynamics.  More  specifically,  we  focus  on  multiple  second-order  nonlinear  systems  and  networked  Lagrangian  systems.  We  first  study  the  distributed  containment  control  problem  for  multiple  second-order  nonlinear  systems  with  multiple  dynamic  leaders  in  the  presence  of  unknown  nonlinearities  and  external  disturbances  under  a  general  directed  graph  that  characterizes  the  interaction  among  the  leaders  and  the  followers.  A  distributed  adaptive  control  algorithm  with  an  adaptive  gain  design  based  on  the  approximation  capability  of  neural  networks  is  proposed.  We  present  a  necessary  and  sufficient  condition  on  the  directed  graph  such  that  the  containment  error  can  be  reduced  as  small  as  desired.  As  a  byproduct,  the  leaderless  consensus  problem  is  solved  with  asymptotical  convergence.  Because  relative  velocity  measurements  between  neighbors  are  generally  more  difficult  to  obtain  than  relative  position  measurements,  we  then  propose  a  distributed  containment  control  algorithm  without  using  neighbors’  velocity  information.  A  two-step  Lyapunov-based  method  is  used  to  study  the  convergence  of  the  closed-loop  system.  Next,  we  apply  the  ideas  to  deal  with  the  containment  control  problem  for  networked  unknown  Lagrangian  systems  under  a  general  directed  graph.  All  the  proposed  algorithms  are  distributed  and  can  be  implemented  using  only  local  measurements  in  the  absence  of  communication.  Finally,  simulation  examples  are  provided  to  show  the  effectiveness  of  the  proposed  control  algorithms.
0	A  limitation  of  gradient  descent  learning.  Over  decades,  gradient  descent  has  been  applied  to  develop  learning  algorithm  to  train  a  neural  network  (NN).  In  this  brief,  a  limitation  of  applying  such  algorithm  to  train  an  NN  with  persistent  weight  noise  is  revealed.  Let  $V({\mathbf  w})$  be  the  performance  measure  of  an  ideal  NN.  $V({\mathbf  w})$  is  applied  to  develop  the  gradient  descent  learning  (GDL).  With  weight  noise,  the  desired  performance  measure  (denoted  as  ${\mathcal{  J}}({\mathbf  w})$  )  is  $E[V(\tilde  {\mathbf  w})|{\mathbf  w}]$  ,  where  $\tilde  {\mathbf  w}$  is  the  noisy  weight  vector.  Applying  GDL  to  train  an  NN  with  weight  noise,  the  actual  learning  objective  is  clearly  not  $V({\mathbf  w})$  but  another  scalar  function  ${\mathcal{  L}}({\mathbf  w})$  .  For  decades,  there  is  a  misconception  that  ${\mathcal{  L}}({\mathbf  w})  =  {\mathcal{  J}}({\mathbf  w})$  ,  and  hence,  the  actual  model  attained  by  the  GDL  is  the  desired  model.  However,  we  show  that  it  might  not:  1)  with  persistent  additive  weight  noise,  the  actual  model  attained  is  the  desired  model  as  ${\mathcal{  L}}({\mathbf  w})  =  {\mathcal{  J}}({\mathbf  w})$  ;  and  2)  with  persistent  multiplicative  weight  noise,  the  actual  model  attained  is  unlikely  the  desired  model  as  ${\mathcal{  L}}({\mathbf  w})  \neq  {\mathcal{  J}}({\mathbf  w})$  .  Accordingly,  the  properties  of  the  models  attained  as  compared  with  the  desired  models  are  analyzed  and  the  learning  curves  are  sketched.  Simulation  results  on  1)  a  simple  regression  problem  and  2)  the  MNIST  handwritten  digit  recognition  are  presented  to  support  our  claims.
0	Self  paced  clustering  ensemble.  The  clustering  ensemble  has  emerged  as  an  important  extension  of  the  classical  clustering  problem.  It  provides  an  elegant  framework  to  integrate  multiple  weak  base  clusterings  to  generate  a  strong  consensus  result.  Most  existing  clustering  ensemble  methods  usually  exploit  all  data  to  learn  a  consensus  clustering  result,  which  does  not  sufficiently  consider  the  adverse  effects  caused  by  some  difficult  instances.  To  handle  this  problem,  we  propose  a  novel  self-paced  clustering  ensemble  (SPCE)  method,  which  gradually  involves  instances  from  easy  to  difficult  ones  into  the  ensemble  learning.  In  our  method,  we  integrate  the  evaluation  of  the  difficulty  of  instances  and  ensemble  learning  into  a  unified  framework,  which  can  automatically  estimate  the  difficulty  of  instances  and  ensemble  the  base  clusterings.  To  optimize  the  corresponding  objective  function,  we  propose  a  joint  learning  algorithm  to  obtain  the  final  consensus  clustering  result.  Experimental  results  on  benchmark  data  sets  demonstrate  the  effectiveness  of  our  method.
0	Neural  network  based  motion  control  of  an  underactuated  wheeled  inverted  pendulum  model.  In  this  paper,  automatic  motion  control  is  investigated  for  wheeled  inverted  pendulum  (WIP)  models,  which  have  been  widely  applied  for  modeling  of  a  large  range  of  two  wheeled  modern  vehicles.  First,  the  underactuated  WIP  model  is  decomposed  into  a  fully  actuated  second-order  subsystem  Σ    a    consisting  of  planar  movement  of  vehicle  forward  motion  and  yaw  angular  motions,  and  a  passive  (nonactuated)  first-order  subsystem  Σ    b    of  pendulum  tilt  motion.  Due  to  the  unknown  dynamics  of  subsystem  Σ    a    and  universal  approximation  ability  of  neural  network  (NN),  an  adaptive  NN  scheme  has  been  employed  for  motion  control  of  subsystem  Σ    a  .  Model  reference  approach  has  been  used,  whereas  the  reference  model  is  optimized  by  finite  time  linear  quadratic  regulation  technique.  Inspired  by  human  control  strategy  of  inverted  pendulum,  the  tilt  angular  motion  in  the  passive  subsystem  Σ    b    has  been  indirectly  controlled  using  the  dynamic  coupling  with  planar  forward  motion  of  subsystem  Σ    a  ,  such  that  the  satisfactory  tracking  of  set  tilt  angle  can  be  guaranteed.  Rigorous  theoretic  analysis  has  been  established,  and  simulation  studies  have  been  performed  to  demonstrate  the  developed  method.
0	Ls  svr  as  a  bayesian  rbf  network.  We  show  theoretical  similarities  between  the  least  squares  support  vector  regression  (LS-SVR)  model  with  a  radial  basis  functions  (RBFs)  kernel  and  maximum  a  posteriori  (MAP)  inference  on  Bayesian  RBF  networks  with  a  specific  Gaussian  prior  on  the  regression  weights.  Although  previous  articles  have  pointed  out  similar  expressions  between  those  learning  approaches,  we  explicitly  and  formally  state  the  existing  correspondences.  We  empirically  demonstrate  our  result  by  performing  computational  experiments  with  standard  regression  benchmarks.  Our  findings  open  a  range  of  possibilities  to  improve  LS-SVR  by  borrowing  strength  from  well-established  developments  in  Bayesian  methodology.
0	Random  sampler  m  estimator  algorithm  with  sequential  probability  ratio  test  for  robust  function  approximation  via  feed  forward  neural  networks.  This  paper  addresses  the  problem  of  fitting  a  functional  model  to  data  corrupted  with  outliers  using  a  multilayered  feed-forward  neural  network.  Although  it  is  of  high  importance  in  practical  applications,  this  problem  has  not  received  careful  attention  from  the  neural  network  research  community.  One  recent  approach  to  solving  this  problem  is  to  use  a  neural  network  training  algorithm  based  on  the  random  sample  consensus  (RANSAC)  framework.  This  paper  proposes  a  new  algorithm  that  offers  two  enhancements  over  the  original  RANSAC  algorithm.  The  first  one  improves  the  algorithm  accuracy  and  robustness  by  employing  an  M-estimator  cost  function  to  decide  on  the  best  estimated  model  from  the  randomly  selected  samples.  The  other  one  improves  the  time  performance  of  the  algorithm  by  utilizing  a  statistical  pretest  based  on  Wald's  sequential  probability  ratio  test.  The  proposed  algorithm  is  successfully  evaluated  on  synthetic  and  real  data,  contaminated  with  varying  degrees  of  outliers,  and  compared  with  existing  neural  network  training  algorithms.
0	Holographic  graph  neuron  a  bioinspired  architecture  for  pattern  processing.  In  this  paper,  we  propose  a  new  approach  to  implementing  hierarchical  graph  neuron  (HGN),  an  architecture  for  memorizing  patterns  of  generic  sensor  stimuli,  through  the  use  of  vector  symbolic  architectures.  The  adoption  of  a  vector  symbolic  representation  ensures  a  single-layer  design  while  retaining  the  existing  performance  characteristics  of  HGN.  This  approach  significantly  improves  the  noise  resistance  of  the  HGN  architecture,  and  enables  a  linear  (with  respect  to  the  number  of  stored  entries)  time  search  for  an  arbitrary  subpattern.
0	Incremental  linear  discriminant  analysis  a  fast  algorithm  and  comparisons.  It  has  always  been  a  challenging  task  to  develop  a  fast  and  an  efficient  incremental  linear  discriminant  analysis  (ILDA)  algorithm.  For  this  purpose,  we  conduct  a  new  study  for  linear  discriminant  analysis  (LDA)  in  this  paper  and  develop  a  new  ILDA  algorithm.  We  propose  a  new  batch  LDA  algorithm  called  LDA/QR.  LDA/QR  is  a  simple  and  fast  LDA  algorithm,  which  is  obtained  by  computing  the  economic  QR  factorization  of  the  data  matrix  followed  by  solving  a  lower  triangular  linear  system.  The  relationship  between  LDA/QR  and  uncorrelated  LDA  (ULDA)  is  also  revealed.  Based  on  LDA/QR,  we  develop  a  new  incremental  LDA  algorithm  called  ILDA/QR.  The  main  features  of  our  ILDA/QR  include  that:  1)  it  can  easily  handle  the  update  from  one  new  sample  or  a  chunk  of  new  samples;  2)  it  has  efficient  computational  complexity  and  space  complexity;  and  3)  it  is  very  fast  and  always  achieves  competitive  classification  accuracy  compared  with  ULDA  algorithm  and  existing  ILDA  algorithms.  Numerical  experiments  based  on  some  real-world  data  sets  demonstrate  that  our  ILDA/QR  is  very  efficient  and  competitive  with  the  state-of-the-art  ILDA  algorithms  in  terms  of  classification  accuracy,  computational  complexity,  and  space  complexity.
0	Online  motor  fault  detection  and  diagnosis  using  a  hybrid  fmm  cart  model.  In  this  brief,  a  hybrid  model  combining  the  fuzzy  min-max  (FMM)  neural  network  and  the  classification  and  regression  tree  (CART)  for  online  motor  detection  and  diagnosis  tasks  is  described.  The  hybrid  model,  known  as  FMM-CART,  exploits  the  advantages  of  both  FMM  and  CART  for  undertaking  data  classification  and  rule  extraction  problems.  To  evaluate  the  applicability  of  the  proposed  FMM-CART  model,  an  evaluation  with  a  benchmark  data  set  pertaining  to  electrical  motor  bearing  faults  is  first  conducted.  The  results  obtained  are  equivalent  to  those  reported  in  the  literature.  Then,  a  laboratory  experiment  for  detecting  and  diagnosing  eccentricity  faults  in  an  induction  motor  is  performed.  In  addition  to  producing  accurate  results,  useful  rules  in  the  form  of  a  decision  tree  are  extracted  to  provide  explanation  and  justification  for  the  predictions  from  FMM-CART.  The  experimental  outcome  positively  shows  the  potential  of  FMM-CART  in  undertaking  online  motor  fault  detection  and  diagnosis  tasks.
0	Active  learning  of  pareto  fronts.  This  paper  introduces  the  active  learning  of  Pareto  fronts  (ALP)  algorithm,  a  novel  approach  to  recover  the  Pareto  front  of  a  multiobjective  optimization  problem.  ALP  casts  the  identification  of  the  Pareto  front  into  a  supervised  machine  learning  task.  This  approach  enables  an  analytical  model  of  the  Pareto  front  to  be  built.  The  computational  effort  in  generating  the  supervised  information  is  reduced  by  an  active  learning  strategy.  In  particular,  the  model  is  learned  from  a  set  of  informative  training  objective  vectors.  The  training  objective  vectors  are  approximated  Pareto-optimal  vectors  obtained  by  solving  different  scalarized  problem  instances.  The  experimental  results  show  that  ALP  achieves  an  accurate  Pareto  front  approximation  with  a  lower  computational  effort  than  state-of-the-art  estimation  of  distribution  algorithms  and  widely  known  genetic  techniques.
0	A  load  balancing  self  organizing  incremental  neural  network.  Clustering  is  widely  used  in  machine  learning,  feature  extraction,  pattern  recognition,  image  analysis,  information  retrieval,  and  bioinformatics.  Online  unsupervised  incremental  learning  is  an  important  branch  of  data  clustering.  However,  accurately  separating  high-density  overlapped  areas  in  a  network  has  a  direct  impact  on  the  performance  of  the  clustering  algorithm.  In  this  paper,  we  propose  a  load-balancing  self-organizing  incremental  neural  network  (LB-SOINN)  to  achieve  good  clustering  results  and  demonstrate  that  it  is  more  stable  than  an  enhanced  SOINN  (E-SOINN).  LB-SOINN  has  all  the  advantages  of  E-SOINN,  such  as  robustness  to  noise  and  online  unsupervised  incremental  learning.  It  overcomes  the  shortcomings  of  the  topology  structure  generated  by  E-SOINN,  such  as  dependence  on  the  sequence  of  the  input  data,  and  avoids  the  turbulence  that  occurs  when  separating  a  composite  class  into  subclasses.  Furthermore,  we  also  introduce  a  distance  combination  framework  to  obtain  good  performance  for  high-dimensional  space-clustering  tasks.  Experiments  involving  both  artificial  and  real  world  data  sets  indicate  that  LB-SOINN  has  superior  performance  in  comparison  with  E-SOINN  and  other  methods.
0	An  efficient  representation  based  method  for  boundary  point  and  outlier  detection.  Detecting  boundary  points  (including  outliers)  is  often  more  interesting  than  detecting  normal  observations,  since  they  represent  valid,  interesting,  and  potentially  valuable  patterns.  Since  data  representation  can  uncover  the  intrinsic  data  structure,  we  present  an  efficient  representation-based  method  for  detecting  such  points,  which  are  generally  located  around  the  margin  of  densely  distributed  data,  such  as  a  cluster.  For  each  point,  the  negative  components  in  its  representation  generally  correspond  to  the  boundary  points  among  its  affine  combination  of  points.  In  the  presented  method,  the  reverse  unreachability  of  a  point  is  proposed  to  evaluate  to  what  degree  this  observation  is  a  boundary  point.  The  reverse  unreachability  can  be  calculated  by  counting  the  number  of  zero  and  negative  components  in  the  representation.  The  reverse  unreachability  explicitly  takes  into  account  the  global  data  structure  and  reveals  the  disconnectivity  between  a  data  point  and  other  points.  This  paper  reveals  that  the  reverse  unreachability  of  points  with  lower  density  has  a  higher  score.  Note  that  the  score  of  reverse  unreachability  of  an  outlier  is  greater  than  that  of  a  boundary  point.  The  top-  $m$  ranked  points  can  thus  be  identified  as  outliers.  The  greater  the  value  of  the  reverse  unreachability,  the  more  likely  the  point  is  a  boundary  point.  Compared  with  related  methods,  our  method  better  reflects  the  characteristics  of  the  data,  and  simultaneously  detects  outliers  and  boundary  points  regardless  of  their  distribution  and  the  dimensionality  of  the  space.  Experimental  results  obtained  for  a  number  of  synthetic  and  real-world  data  sets  demonstrate  the  effectiveness  and  efficiency  of  our  method.
0	Robust  object  tracking  by  nonlinear  learning.  We  propose  a  method  that  obtains  a  discriminative  visual  dictionary  and  a  nonlinear  classifier  for  visual  tracking  tasks  in  a  sparse  coding  manner  based  on  the  globally  linear  approximation  for  a  nonlinear  learning  theory.  Traditional  discriminative  tracking  methods  based  on  sparse  representation  learn  a  dictionary  in  an  unsupervised  way  and  then  train  a  classifier,  which  may  not  generate  both  descriptive  and  discriminative  models  for  targets  by  treating  dictionary  learning  and  classifier  learning  separately.  In  contrast,  the  proposed  tracking  approach  can  construct  a  dictionary  that  fully  reflects  the  intrinsic  manifold  structure  of  visual  data  and  introduces  more  discriminative  ability  in  a  unified  learning  framework.  Finally,  an  iterative  optimization  approach,  which  computes  the  optimal  dictionary,  the  associated  sparse  coding,  and  a  classifier,  is  introduced.  Experiments  on  two  benchmarks  show  that  our  tracker  achieves  a  better  performance  compared  with  some  popular  tracking  algorithms.
0	Comet  integrating  different  levels  of  linguistic  modeling  for  meaning  assessment.  This  paper  describes  the  CoMeT  system,  our  contribution  to  the  SemEval  2013  Task  7  challenge,  focusing  on  the  task  of  automatically  assessing  student  answers  to  factual  questions.  CoMeT  is  based  on  a  meta-classifier  that  uses  the  outputs  of  the  sub-systems  we  developed:  CoMiC,  CoSeC,  and  three  shallower  bag  approaches.  We  sketch  the  functionality  of  all  sub-systems  and  evaluate  their  performance  against  the  official  test  set  of  the  challenge.  CoMeT  obtained  the  best  result  (73.1%  accuracy)  for  the  3-way  unseen  answers  in  Beetle  among  all  challenge  participants.  We  also  discuss  possible  improvements  and  directions  for  future  research.
0	Limit  set  dichotomy  and  multistability  for  a  class  of  cooperative  neural  networks  with  delays.  Recent  papers  have  pointed  out  the  interest  to  study  convergence  in  the  presence  of  multiple  equilibrium  points  (EPs)  (multistability)  for  neural  networks  (NNs)  with  nonsymmetric  cooperative  (nonnegative)  interconnections  and  neuron  activations  modeled  by  piecewise  linear  (PL)  functions.  One  basic  difficulty  is  that  the  semiflows  generated  by  such  NNs  are  monotone  but,  due  to  the  horizontal  segments  in  the  PL  functions,  are  not  eventually  strongly  monotone  (ESM).  This  notwithstanding,  it  has  been  shown  that  there  are  subclasses  of  irreducible  interconnection  matrices  for  which  the  semiflows,  although  they  are  not  ESM,  enjoy  convergence  properties  similar  to  those  of  ESM  semiflows.  The  results  obtained  so  far  concern  the  case  of  cooperative  NNs  without  delays.  The  goal  of  this  paper  is  to  extend  some  of  the  existing  results  to  the  relevant  case  of  NNs  with  delays.  More  specifically,  this  paper  considers  a  class  of  NNs  with  PL  neuron  activations,  concentrated  delays,  and  a  nonsymmetric  cooperative  interconnection  matrix  A  and  delay  interconnection  matrix  Aτ.  The  main  result  is  that  when  A+Aτ  satisfies  a  full  interconnection  condition,  then  the  generated  semiflows,  which  are  monotone  but  not  ESM,  satisfy  a  limit  set  dichotomy  analogous  to  that  valid  for  ESM  semiflows.  It  follows  that  there  is  an  open  and  dense  set  of  initial  conditions,  in  the  state  space  of  continuous  functions  on  a  compact  interval,  for  which  the  solutions  converge  toward  an  EP.  The  result  holds  in  the  general  case  where  the  NNs  possess  multiple  EPs,  i.e.,  is  a  result  on  multistability,  and  is  valid  for  any  constant  value  of  the  delays.
0	Learning  to  map  social  network  users  by  unified  manifold  alignment  on  hypergraph.  Nowadays,  a  lot  of  people  possess  accounts  on  multiple  online  social  networks,  e.g.,  Facebook  and  Twitter.  These  networks  are  overlapped,  but  the  correspondences  between  their  users  are  not  explicitly  given.  Mapping  common  users  across  these  social  networks  will  be  beneficial  for  applications  such  as  cross-network  recommendation.  In  recent  years,  a  lot  of  mapping  algorithms  have  been  proposed  which  exploited  social  and/or  profile  relations  between  users  from  different  networks.  However,  there  is  still  a  lack  of  unified  mapping  framework  which  can  well  exploit  high-order  relational  information  in  both  social  structures  and  profiles.  In  this  paper,  we  propose  a  unified  hypergraph  learning  framework  named  unified  manifold  alignment  on  hypergraph  (UMAH)  for  this  task.  UMAH  models  social  structures  and  user  profile  relations  in  a  unified  hypergraph  where  the  relative  weights  of  profile  hyperedges  are  determined  automatically.  Given  a  set  of  training  user  correspondences,  a  common  subspace  is  learned  by  preserving  the  hypergraph  structure  as  well  as  the  correspondence  relations  of  labeled  users.  UMAH  intrinsically  performs  semisupervised  manifold  alignment  with  profile  information  for  calibration.  For  a  target  user  in  one  network,  UMAH  ranks  all  the  users  in  the  other  network  by  their  probabilities  of  being  the  corresponding  user  (measured  by  similarity  in  the  subspace).  In  experiments,  we  evaluate  UMAH  on  three  real  world  data  sets  and  compare  it  to  state-of-art  baseline  methods.  Experimental  results  have  demonstrated  the  effectiveness  of  UMAH  in  mapping  users  across  networks.
0	Rm  h  _  infty  output  tracking  control  of  discrete  time  nonlinear  systems  via  standard  neural  network  models.  This  brief  proposes  an  output  tracking  control  for  a  class  of  discrete-time  nonlinear  systems  with  disturbances.  A  standard  neural  network  model  is  used  to  represent  discrete-time  nonlinear  systems  whose  nonlinearity  satisfies  the  sector  conditions.  H    ∞    control  performance  for  the  closed-loop  system  including  the  standard  neural  network  model,  the  reference  model,  and  state  feedback  controller  is  analyzed  using  Lyapunov-Krasovskii  stability  theorem  and  linear  matrix  inequality  (LMI)  approach.  The  H    ∞    controller,  of  which  the  parameters  are  obtained  by  solving  LMIs,  guarantees  that  the  output  of  the  closedloop  system  closely  tracks  the  output  of  a  given  reference  model  well,  and  reduces  the  influence  of  disturbances  on  the  tracking  error.  Three  numerical  examples  are  provided  to  show  the  effectiveness  of  the  proposed  H    ∞    output  tracking  design  approach.
0	Calibration  and  uncertainty  in  neural  time  to  event  modeling.  Models  for  predicting  the  time  of  a  future  event  are  crucial  for  risk  assessment,  across  a  diverse  range  of  applications.  Existing  time-to-event  (survival)  models  have  focused  primarily  on  preserving  pairwise  ordering  of  estimated  event  times  (i.e.,  relative  risk).  We  propose  neural  time-to-event  models  that  account  for  calibration  and  uncertainty  while  predicting  accurate  absolute  event  times.  Specifically,  an  adversarial  nonparametric  model  is  introduced  for  estimating  matched  time-to-event  distributions  for  probabilistically  concentrated  and  accurate  predictions.  We  also  consider  replacing  the  discriminator  of  the  adversarial  nonparametric  model  with  a  survival-function  matching  estimator  that  accounts  for  model  calibration.  The  proposed  estimator  can  be  used  as  a  means  of  estimating  and  comparing  conditional  survival  distributions  while  accounting  for  the  predictive  uncertainty  of  probabilistic  models.  Extensive  experiments  show  that  the  distribution  matching  methods  outperform  existing  approaches  in  terms  of  both  calibration  and  concentration  of  time-to-event  distributions.
0	Actor  critic  learning  control  based  on  ell_  2  regularized  temporal  difference  prediction  with  gradient  correction.  Actor-critic  based  on  the  policy  gradient  (PG-based  AC)  methods  have  been  widely  studied  to  solve  learning  control  problems.  In  order  to  increase  the  data  efficiency  of  learning  prediction  in  the  critic  of  PG-based  AC,  studies  on  how  to  use  recursive  least-squares  temporal  difference  (RLS-TD)  algorithms  for  policy  evaluation  have  been  conducted  in  recent  years.  In  such  contexts,  the  critic  RLS-TD  evaluates  an  unknown  mixed  policy  generated  by  a  series  of  different  actors,  but  not  one  fixed  policy  generated  by  the  current  actor.  Therefore,  this  AC  framework  with  RLS-TD  critic  cannot  be  proved  to  converge  to  the  optimal  fixed  point  of  learning  problem.  To  address  the  above  problem,  this  paper  proposes  a  new  AC  framework  named  critic-iteration  PG  (CIPG),  which  learns  the  state-value  function  of  current  policy  in  an  on-policy  way  and  performs  gradient  ascent  in  the  direction  of  improving  discounted  total  reward.  During  each  iteration,  CIPG  keeps  the  policy  parameters  fixed  and  evaluates  the  resulting  fixed  policy  by  $\ell  _{2}$  -regularized  RLS-TD  critic.  Our  convergence  analysis  extends  previous  convergence  analysis  of  PG  with  function  approximation  to  the  case  of  RLS-TD  critic.  The  simulation  results  demonstrate  that  the  $\ell  _{2}$  -regularization  term  in  the  critic  of  CIPG  is  undamped  during  the  learning  process,  and  CIPG  has  better  learning  efficiency  and  faster  convergence  rate  than  conventional  AC  learning  control  methods.
0	Learning  harmonium  models  with  infinite  latent  features.  Undirected  latent  variable  models  represent  an  important  class  of  graphical  models  that  have  been  successfully  developed  to  deal  with  various  tasks.  One  common  challenge  in  learning  such  models  is  to  determine  the  number  of  hidden  units  that  are  unknown  a  priori.  Although  Bayesian  nonparametrics  have  provided  promising  results  in  bypassing  the  model  selection  problem  in  learning  directed  Bayesian  Networks,  very  little  effort  has  been  made  toward  applying  Bayesian  nonparametrics  to  learn  undirected  latent  variable  models.  In  this  paper,  we  present  the  infinite  exponential  family  Harmonium  (iEFH),  a  bipartite  undirected  latent  variable  model  that  automatically  determines  the  number  of  latent  units  from  an  unbounded  pool.  We  also  present  two  important  extensions  of  iEFH  to  1)  multiview  iEFH  for  dealing  with  heterogeneous  data,  and  2)  infinite  maximum-margin  Harmonium  (iMMH)  for  incorporating  supervising  side  information  to  learn  predictive  latent  features.  We  develop  variational  inference  algorithms  to  learn  model  parameters.  Our  methods  are  computationally  competitive  because  of  the  avoidance  of  selecting  the  number  of  latent  units.  Our  extensive  experiments  on  real  image  datasets  and  text  datasets  appear  to  demonstrate  the  benefits  of  iEFH  and  iMMH  inherited  from  Bayesian  nonparametrics  and  max-margin  learning.  Such  results  were  not  available  until  now  and  contribute  to  expanding  the  scope  of  Bayesian  nonparametrics  to  learn  the  structures  of  undirected  latent  variable  models.
0	Pm  monitoring  use  information  abundance  measurement  and  wide  and  deep  learning.  This  article  devises  a  photograph-based  monitoring  model  to  estimate  the  real-time  PM2.5  concentrations,  overcoming  currently  popular  electrochemical  sensor-based  PM2.5  monitoring  methods’  shortcomings  such  as  low-density  spatial  distribution  and  time  delay.  Combining  the  proposed  monitoring  model,  the  photographs  taken  by  various  camera  devices  (e.g.,  surveillance  camera,  automobile  data  recorder,  and  mobile  phone)  can  widely  monitor  PM2.5  concentration  in  megacities.  This  is  beneficial  to  offering  helpful  decision-making  information  for  atmospheric  forecast  and  control,  thus  reducing  the  epidemic  of  COVID-19.  To  specify,  the  proposed  model  fuses  Information  Abundance  measurement  and  Wide  and  Deep  learning,  dubbed  as  IAWD,  for  PM2.5  monitoring.  First,  our  model  extracts  two  categories  of  features  in  a  newly  proposed  DS  transform  space  to  measure  the  information  abundance  (IA)  of  a  given  photograph  since  the  growth  of  PM2.5  concentration  decreases  its  IA.  Second,  to  simultaneously  possess  the  advantages  of  memorization  and  generalization,  a  new  wide  and  deep  neural  network  is  devised  to  learn  a  nonlinear  mapping  between  the  above-mentioned  extracted  features  and  the  groundtruth  PM2.5  concentration.  Experiments  on  two  recently  established  datasets  totally  including  more  than  100  000  photographs  demonstrate  the  effectiveness  of  our  extracted  features  and  the  superiority  of  our  proposed  IAWD  model  as  compared  to  state-of-the-art  relevant  computing  techniques.
0	Qualitative  analysis  and  bifurcation  in  a  neuron  system  with  memristor  characteristics  and  time  delay.  This  article  focuses  on  the  hybrid  effects  of  memristor  characteristics,  time  delay,  and  biochemical  parameters  on  neural  networks.  First,  we  propose  a  novel  neuron  system  with  memristor  and  time  delays  in  which  the  memristor  is  characterized  by  a  smooth  continuous  cubic  function.  Second,  the  existence  of  equilibria  of  this  type  of  neuron  system  is  examined  in  the  parameter  space.  Sufficient  conditions  that  ensure  the  stability  of  equilibria  and  occurrence  of  pitchfork  bifurcation  are  given  for  the  memristor-based  neuron  system  without  delay.  Third,  some  novel  criteria  of  the  addressed  neuron  system  are  constructed  for  guaranteeing  the  delay-dependent  and  delay-independent  stability.  The  specific  conditions  are  provided  for  Hopf  bifurcations,  and  the  properties  of  Hopf  bifurcation  are  ascertained  using  the  center  manifold  reduction  and  the  normal  form  theory.  Moreover,  there  exists  a  phenomenon  of  bistability  for  the  delayed  memristor-based  neuron  system  having  three  equilibria.  Finally,  the  effectiveness  of  the  theoretical  results  is  demonstrated  by  numerical  examples.
0	Robust  state  output  feedback  control  of  coaxial  rotor  mavs  based  on  adaptive  nn  approach.  The  coaxial-rotor  micro-aerial  vehicles  (CRMAVs)  have  been  proven  to  be  a  powerful  tool  in  forming  small  and  agile  manned–unmanned  hybrid  applications.  However,  the  operation  of  them  is  usually  subject  to  unpredictable  time-varying  aerodynamic  disturbances  and  model  uncertainties.  In  this  paper,  an  adaptive  robust  controller  based  on  a  neural  network  (NN)  approach  is  proposed  to  reject  such  perturbations  and  track  both  the  desired  position  and  orientation  trajectories.  A  complete  dynamic  model  of  a  CRMAV  is  first  constructed.  When  all  system  states  are  assumed  to  be  available,  an  NN-based  state-feedback  controller  is  proposed  through  feedback  linearization  and  Lyapunov  analysis.  Furthermore,  to  overcome  the  practical  challenge  that  certain  states  are  not  measurable,  a  high-gain  observer  is  introduced  to  estimate  the  unavailable  states,  and  then,  an  output-feedback  controller  is  developed.  Rigorous  theoretical  analysis  verifies  the  stability  of  the  entire  closed-loop  system.  In  addition,  extensive  simulation  studies  are  conducted  to  validate  the  feasibility  of  the  proposed  scheme.
0	Zhang  neural  network  versus  gradient  neural  network  for  solving  time  varying  linear  inequalities.  By  following  Zhang  design  method,  a  new  type  of  recurrent  neural  network  [i.e.,  Zhang  neural  network  (ZNN)]  is  presented,  investigated,  and  analyzed  for  online  solution  of  time-varying  linear  inequalities.  Theoretical  analysis  is  given  on  convergence  properties  of  the  proposed  ZNN  model.  For  comparative  purposes,  the  conventional  gradient  neural  network  is  developed  and  exploited  for  solving  online  time-varying  linear  inequalities  as  well.  Computer  simulation  results  further  verify  and  demonstrate  the  efficacy,  novelty,  and  superiority  of  such  a  ZNN  model  and  its  method  for  solving  time-varying  linear  inequalities.
0	Somke  kernel  density  estimation  over  data  streams  by  sequences  of  self  organizing  maps.  In  this  paper,  we  propose  a  novel  method  SOMKE,  for  kernel  density  estimation  (KDE)  over  data  streams  based  on  sequences  of  self-organizing  map  (SOM).  In  many  stream  data  mining  applications,  the  traditional  KDE  methods  are  infeasible  because  of  the  high  computational  cost,  processing  time,  and  memory  requirement.  To  reduce  the  time  and  space  complexity,  we  propose  a  SOM  structure  in  this  paper  to  obtain  well-defined  data  clusters  to  estimate  the  underlying  probability  distributions  of  incoming  data  streams.  The  main  idea  of  this  paper  is  to  build  a  series  of  SOMs  over  the  data  streams  via  two  operations,  that  is,  creating  and  merging  the  SOM  sequences.  The  creation  phase  produces  the  SOM  sequence  entries  for  windows  of  the  data,  which  obtains  clustering  information  of  the  incoming  data  streams.  The  size  of  the  SOM  sequences  can  be  further  reduced  by  combining  the  consecutive  entries  in  the  sequence  based  on  the  measure  of  Kullback-Leibler  divergence.  Finally,  the  probability  density  functions  over  arbitrary  time  periods  along  the  data  streams  can  be  estimated  using  such  SOM  sequences.  We  compare  SOMKE  with  two  other  KDE  methods  for  data  streams,  the  M-kernel  approach  and  the  cluster  kernel  approach,  in  terms  of  accuracy  and  processing  time  for  various  stationary  data  streams.  Furthermore,  we  also  investigate  the  use  of  SOMKE  over  nonstationary  (evolving)  data  streams,  including  a  synthetic  nonstationary  data  stream,  a  real-world  financial  data  stream  and  a  group  of  network  traffic  data  streams.  The  simulation  results  illustrate  the  effectiveness  and  efficiency  of  the  proposed  approach.
0	Srsc  selective  robust  and  supervised  constrained  feature  representation  for  image  classification.  Feature  representation  learning,  an  emerging  topic  in  recent  years,  has  achieved  great  progress.  Powerful  learned  features  can  lead  to  excellent  classification  accuracy.  In  this  article,  a  selective  and  robust  feature  representation  framework  with  a  supervised  constraint  (SRSC)  is  presented.  SRSC  seeks  a  selective,  robust,  and  discriminative  subspace  by  transforming  the  original  feature  space  into  the  category  space.  Particularly,  we  add  a  selective  constraint  to  the  transformation  matrix  (or  classifier  parameter)  that  can  select  discriminative  dimensions  of  the  input  samples.  Moreover,  a  supervised  regularization  is  tailored  to  further  enhance  the  discriminability  of  the  subspace.  To  relax  the  hard  zero-one  label  matrix  in  the  category  space,  an  additional  error  term  is  also  incorporated  into  the  framework,  which  can  lead  to  a  more  robust  transformation  matrix.  SRSC  is  formulated  as  a  constrained  least  square  learning  (feature  transforming)  problem.  For  the  SRSC  problem,  an  inexact  augmented  Lagrange  multiplier  method  (ALM)  is  utilized  to  solve  it.  Extensive  experiments  on  several  benchmark  data  sets  adequately  demonstrate  the  effectiveness  and  superiority  of  the  proposed  method.  The  proposed  SRSC  approach  has  achieved  better  performances  than  the  compared  counterpart  methods.
0	Morphological  analysis  using  a  sequence  decoder.  We  introduce  Morse,  a  recurrent  encoder-decoder  model  that  produces  morphological  analyses  of  each  word  in  a  sentence.   The  encoder  turns  the  relevant  information  about  the  word  and  its  context  into  a  fixed  size  vector  representation  and  the  decoder  generates  the  sequence  of  characters  for  the  lemma  followed  by  a  sequence  of  individual  morphological  features.  We  show  that  generating  morphological  features  individually  rather  than  as  a  combined  tag  allows  the  model  to  handle  rare  or  unseen  tags  and  outperform  whole-tag  models.  In  addition,  generating  morphological  features  as  a  sequence  rather  than  e.g.  an  unordered  set  allows  our  model  to  produce  an  arbitrary  number  of  features  that  represent  multiple  inflectional  groups  in  morphologically  complex  languages.  We  obtain  state-of-the  art  results  in  nine  languages  of  different  morphological  complexity  under  low-resource,  high-resource  and  transfer  learning  settings.  We  also  introduce  TrMor2018,  a  new  high  accuracy  Turkish  morphology  dataset.  Our  Morse  implementation  and  the  TrMor2018  dataset  are  available  online  to  support  future  research.
0	The  narrativeqa  reading  comprehension  challenge.  Reading  comprehension  (RC)—in  contrast  to  information  retrieval—requires  integrating  information  and  reasoning  about  events,  entities,  and  their  relations  across  a  full  document.  Question  answering...
0	Distributed  adaptive  containment  control  for  a  class  of  nonlinear  multiagent  systems  with  input  quantization.  This  paper  is  devoted  to  distributed  adaptive  containment  control  for  a  class  of  nonlinear  multiagent  systems  with  input  quantization.  By  employing  a  matrix  factorization  and  a  novel  matrix  normalization  technique,  some  assumptions  involving  control  gain  matrices  in  existing  results  are  relaxed.  By  fusing  the  techniques  of  sliding  mode  control  and  backstepping  control,  a  two-step  design  method  is  proposed  to  construct  controllers  and,  with  the  aid  of  neural  networks,  all  system  nonlinearities  are  allowed  to  be  unknown.  Moreover,  a  linear  time-varying  model  and  a  similarity  transformation  are  introduced  to  circumvent  the  obstacle  brought  by  quantization,  and  the  controllers  need  no  information  about  the  quantizer  parameters.  The  proposed  scheme  is  able  to  ensure  the  boundedness  of  all  closed-loop  signals  and  steer  the  containment  errors  into  an  arbitrarily  small  residual  set.  The  simulation  results  illustrate  the  effectiveness  of  the  scheme.
0	Integration  enhanced  zhang  neural  network  for  real  time  varying  matrix  inversion  in  the  presence  of  various  kinds  of  noises.  Matrix  inversion  often  arises  in  the  fields  of  science  and  engineering.  Many  models  for  matrix  inversion  usually  assume  that  the  solving  process  is  free  of  noises  or  that  the  denoising  has  been  conducted  before  the  computation.  However,  time  is  precious  for  the  real-time-varying  matrix  inversion  in  practice,  and  any  preprocessing  for  noise  reduction  may  consume  extra  time,  possibly  violating  the  requirement  of  real-time  computation.  Therefore,  a  new  model  for  time-varying  matrix  inversion  that  is  able  to  handle  simultaneously  the  noises  is  urgently  needed.  In  this  paper,  an  integration-enhanced  Zhang  neural  network  (IEZNN)  model  is  first  proposed  and  investigated  for  real-time-varying  matrix  inversion.  Then,  the  conventional  ZNN  model  and  the  gradient  neural  network  model  are  presented  and  employed  for  comparison.  In  addition,  theoretical  analyses  show  that  the  proposed  IEZNN  model  has  the  global  exponential  convergence  property.  Moreover,  in  the  presence  of  various  kinds  of  noises,  the  proposed  IEZNN  model  is  proven  to  have  an  improved  performance.  That  is,  the  proposed  IEZNN  model  converges  to  the  theoretical  solution  of  the  time-varying  matrix  inversion  problem  no  matter  how  large  the  matrix-form  constant  noise  is,  and  the  residual  errors  of  the  proposed  IEZNN  model  can  be  arbitrarily  small  for  time-varying  noises  and  random  noises.  Finally,  three  illustrative  simulation  examples,  including  an  application  to  the  inverse  kinematic  motion  planning  of  a  robot  manipulator,  are  provided  and  analyzed  to  substantiate  the  efficacy  and  superiority  of  the  proposed  IEZNN  model  for  real-time-varying  matrix  inversion.
0	Mutual  collision  avoidance  scheme  synthesized  by  neural  networks  for  dual  redundant  robot  manipulators  executing  cooperative  tasks.  Collision  between  dual  robot  manipulators  during  working  process  will  lead  to  task  failure  and  even  robot  damage.  To  avoid  mutual  collision  of  dual  robot  manipulators  while  doing  collaboration  tasks,  a  novel  recurrent  neural  network  (RNN)-based  mutual-collision-avoidance  (MCA)  scheme  for  solving  the  motion  planning  problem  of  dual  manipulators  is  proposed  and  exploited.  Because  of  the  high  accuracy  and  low  computation  complexity,  the  linear  variational  inequality-based  primal–dual  neural  network  is  used  to  solve  the  proposed  scheme.  The  proposed  scheme  is  applied  to  the  collaboration  trajectory  tracking  and  cup-stacking  tasks,  and  shows  its  effectiveness  for  avoiding  collision  between  the  dual  robot  manipulators.  Through  network  iteration  and  online  learning,  the  dual  robot  manipulators  will  learn  the  ability  of  MCA.  Moreover,  a  line-segment-based  distance  measure  algorithm  is  proposed  to  calculate  the  minimum  distance  between  the  dual  manipulators.  If  the  computed  minimum  distance  is  less  than  the  first  safe-related  distance  threshold,  a  speed  brake  operation  is  executed  and  guarantees  that  the  robot  cannot  exceed  the  second  safe-related  distance  threshold.  Furthermore,  the  proposed  MCA  strategy  is  formulated  as  a  standard  quadratic  programming  problem,  which  is  further  solved  by  an  RNN.  Computer  simulations  and  a  real  dual  robot  experiment  further  verify  the  effectiveness,  accuracy,  and  physical  realizability  of  the  RNN-based  MCA  scheme  when  manipulators  cooperatively  execute  the  end-effector  tasks.
0	Co  operative  coevolutionary  neural  networks  for  mining  functional  association  rules.  In  this  paper,  we  introduce  a  novel  form  of  association  rules  (ARs)  that  do  not  require  discretization  of  continuous  variables  or  the  use  of  intervals  in  either  sides  of  the  rule.  This  rule  form  captures  nonlinear  relationships  among  variables,  and  provides  an  alternative  pattern  representation  for  mining  essential  relations  hidden  in  a  given  data  set.  We  refer  to  the  new  rule  form  as  a  functional  AR  (FAR).  A  new  neural  network-based,  co-operative,  coevolutionary  algorithm  is  presented  for  FAR  mining.  The  algorithm  is  applied  to  both  synthetic  and  real-world  data  sets,  and  its  performance  is  analyzed.  The  experimental  results  show  that  the  proposed  mining  algorithm  is  able  to  discover  valid  and  essential  underlying  relations  in  the  data.  Comparison  experiments  are  also  carried  out  with  the  two  state-of-the-art  AR  mining  algorithms  that  can  handle  continuous  variables  to  demonstrate  the  competitive  performance  of  the  proposed  method.
0	Generating  face  images  with  attributes  for  free.  With  superhuman-level  performance  of  face  recognition,  we  are  more  concerned  about  the  recognition  of  fine-grained  attributes,  such  as  emotion,  age,  and  gender.  However,  given  that  the  label  space  is  extremely  large  and  follows  a  long-tail  distribution,  it  is  quite  expensive  to  collect  sufficient  samples  for  fine-grained  attributes.  This  results  in  imbalanced  training  samples  and  inferior  attribute  recognition  models.  To  this  end,  we  propose  the  use  of  arbitrary  attribute  combinations,  without  human  effort,  to  synthesize  face  images.  In  particular,  to  bridge  the  semantic  gap  between  high-level  attribute  label  space  and  low-level  face  image,  we  propose  a  novel  neural-network-based  approach  that  maps  the  target  attribute  labels  to  an  embedding  vector,  which  can  be  fed  into  a  pretrained  image  decoder  to  synthesize  a  new  face  image.  Furthermore,  to  regularize  the  attribute  for  image  synthesis,  we  propose  to  use  a  perceptual  loss  to  make  the  new  image  explicitly  faithful  to  target  attributes.  Experimental  results  show  that  our  approach  can  generate  photorealistic  face  images  from  attribute  labels,  and  more  importantly,  by  serving  as  augmented  training  samples,  these  images  can  significantly  boost  the  performance  of  attribute  recognition  model.  The  code  is  open-sourced  at  this  link.
0	Learning  from  crowds  with  multiple  noisy  label  distribution  propagation.  Crowdsourcing  services  provide  a  fast,  efficient,  and  cost-effective  way  to  obtain  large  labeled  data  for  supervised  learning.  Unfortunately,  the  quality  of  crowdsourced  labels  cannot  satisfy  the  standards  of  practical  applications.  Ground-truth  inference,  simply  called  label  integration,  designs  proper  aggregation  methods  to  infer  the  unknown  true  label  of  each  instance  (sample)  from  the  multiple  noisy  label  set  provided  by  ordinary  crowd  labelers  (workers).  However,  nearly  all  existing  label  integration  methods  focus  solely  on  the  multiple  noisy  label  set  per  individual  instance  while  totally  ignoring  the  intercorrelation  among  multiple  noisy  label  sets  of  different  instances.  To  solve  this  problem,  a  multiple  noisy  label  distribution  propagation  (MNLDP)  method  is  proposed  in  this  article.  MNLDP  at  first  estimates  the  multiple  noisy  label  distribution  of  each  instance  from  its  multiple  noisy  label  set  and  then  propagates  its  multiple  noisy  label  distribution  to  its  nearest  neighbors.  Consequently,  each  instance  absorbs  a  fraction  of  the  multiple  noisy  label  distributions  from  its  nearest  neighbors  and  yet  simultaneously  maintains  a  fraction  of  its  own  original  multiple  noisy  label  distribution.  Empirical  studies  on  a  collection  of  an  artificial  dataset,  six  simulated  UCI  datasets,  and  three  real-world  crowdsourced  datasets  show  that  MNLDP  outperforms  all  other  existing  state-of-the-art  label  integration  methods  in  terms  of  the  integration  accuracy  and  classification  accuracy.
0	Unsupervised  active  learning  of  crf  model  for  cross  lingual  named  entity  recognition.  Manual  annotation  of  the  training  data  of  information  extraction  models  is  a  time  consuming  and  expensive  process  but  necessary  for  the  building  of  information  extraction  systems.  Active  learning  has  been  proven  to  be  effective  in  reducing  manual  annotation  efforts  for  supervised  learning  tasks  where  a  human  judge  is  asked  to  annotate  the  most  informative  examples  with  respect  to  a  given  model.  However,  in  most  cases  reliable  human  judges  are  not  available  for  all  languages.  In  this  paper,  we  propose  a  cross-lingual  unsupervised  active  learning  paradigm  (XLADA)  that  generates  high-quality  automatically  annotated  training  data  from  a  word-aligned  parallel  corpus.  To  evaluate  our  paradigm,  we  applied  XLADA  on  English-French  and  English-Chinese  bilingual  corpora  then  we  trained  French  and  Chinese  information  extraction  models.  The  experimental  results  show  that  XLADA  can  produce  effective  models  without  manually-annotated  training  data.
0	Beyond  winning  and  losing  modeling  human  motivations  and  behaviors  with  vector  valued  inverse  reinforcement  learning.  In  recent  years,  reinforcement  learning  (RL)  methods  have  been  applied  to  model  gameplay  with  great  success,  achieving  super-human  performance  in  various  environments,  such  as  Atari,  Go  and  Poker.  However,  these  studies  mostly  focus  on  winning  the  game  and  have  largely  ignored  the  rich  and  complex  human  motivations,  which  are  essential  for  understanding  humans’  diverse  behavior.  In  this  paper,  we  present  a  multi-motivation  behavior  model  which  investigates  the  multifaceted  human  motivations  and  learns  the  underlying  value  structure  of  the  agents.  Our  approach  extends  inverse  RL  to  vectored-valued  rewards  with  Pareto  optimality  which  significantly  weakens  the  inverse  RL  assumption.  Our  model  therefore  incorporates  a  wider  range  of  behavior  that  commonly  appears  in  real-world  environments.  For  practical  assessment,  our  algorithm  is  tested  on  World  of  Warcraft  datasets  and  demonstrates  the  improvement  over  existing  methods.
0	Deephaul  a  deep  learning  and  reinforcement  learning  based  smart  automation  framework  for  dump  trucks.  In  surface  mining  operations,  the  haul  truck  contributes  the  largest  share  to  the  overall  operating  cost,  along  with  being  a  significant  contributor  to  the  injuries  and  fatalities  to  the  operators;  therefore,  measures  need  to  be  taken  for  improving  truck  haulage  safety  and  efficiency.  In  the  absence  of  any  major  advancement  in  the  automation  technology  for  the  mining  sector,  this  study  attempted  to  eliminate  the  existing  technology  lull  by  developing  a  novel  DeepHaul  framework  for  inducing  smartness  and  intelligence  within  any  dump  truck,  using  the  advanced  algorithmic  knowledge  of  artificial  intelligence  and  machine  learning.  The  DeepHaul  framework  consisted  of  two  major  components:  first,  inducing  an  object  recognition  ability  by  using  deep  learning  methodology,  for  any  dump  truck.  Experiments  were  conducted  with  different  deep  learning  architectures,  different  training  batch  size  ranges,  and  various  image  sizes  for  developing  an  optimum  deep  learning  model  with  state-of-the-art  performance  for  the  haul  truck.  The  second  component  consisted  of  the  steering  action  decision  making  ability  for  the  dump  truck  based  on  the  given  state  of  the  haulage  route.  A  reinforcement  learning-based  algorithm  was  designed,  implemented,  and  tested  for  achieving  the  aforementioned  objective.  The  algorithm  exhibited  an  accuracy  of  100%  regarding  safety  and  an  average  accuracy  score  of  more  than  97%  regarding  haulage  efficiency.  With  the  implementation  of  this  DeepHaul  framework,  the  existing  autonomous  haulage  truck  control  technology  can  be  greatly  enhanced  by  inducing  intelligence  and  smartness  within  the  haul  trucks,  which  would  result  in  improved  safety,  efficiency,  and  the  effectiveness  of  mining  operations.
0	Gene  selection  and  disease  prediction  from  gene  expression  data  using  a  two  stage  hetero  associative  memory.  In  general,  gene  expression  microarrays  consist  of  a  vast  number  of  genes  and  very  few  samples,  which  represents  a  critical  challenge  for  disease  prediction  and  diagnosis.  This  paper  develops  a  two-stage  algorithm  that  integrates  feature  selection  and  prediction  by  extending  a  type  of  hetero-associative  neural  networks.  In  the  first  level,  the  algorithm  generates  the  associative  memory,  whereas  the  second  level  picks  the  most  relevant  genes.  With  the  purpose  of  illustrating  the  applicability  and  efficiency  of  the  method  proposed  here,  we  use  four  different  gene  expression  microarray  databases  and  compare  their  classification  performance  against  that  of  other  renowned  classifiers  built  on  the  whole  (original)  feature  (gene)  space.  The  experimental  results  show  that  the  two-stage  hetero-associative  memory  is  quite  competitive  with  standard  classification  models  regarding  the  overall  accuracy,  sensitivity  and  specificity.  In  addition,  it  also  produces  a  significant  decrease  in  computational  efforts  and  an  increase  in  the  biological  interpretability  of  microarrays  because  worthless  (irrelevant  and/or  redundant)  genes  are  discarded.
0	Added  value  of  automatic  multilingual  text  analysis  for  epidemic  surveillance.  The  early  detection  of  disease  outbursts  is  an  important  objective  of  epidemic  surveillance.  The  web  news  are  one  of  the  information  bases  for  detecting  epidemic  events  as  soon  as  possible,  but  to  analyze  tens  of  thousands  articles  published  daily  is  costly.  Recently,  automatic  systems  have  been  devoted  to  epidemiological  surveillance.  The  main  issue  for  these  systems  is  to  process  more  languages  at  a  limited  cost.  However,  existing  systems  mainly  process  major  languages  (English,  French,  Russian,  Spanish…).  Thus,  when  the  first  news  reporting  a  disease  is  in  a  minor  language,  the  timeliness  of  event  detection  is  worsened.  In  this  paper,  we  test  an  automatic  style-based  method,  designed  to  fill  the  gaps  of  existing  automatic  systems.  It  is  parsimonious  in  resources  and  specially  designed  for  multilingual  issues.  The  events  detected  by  the  human-moderated  ProMED  mail  between  November  2011  and  January  2012  are  used  as  a  reference  dataset  and  compared  to  events  detected  in  17  languages  by  the  system  DAnIEL2  from  web  articles  of  this  time-window.  We  show  how  being  able  to  process  press  articles  in  languages  less-spoken  allows  quicker  detection  of  epidemic  events  in  some  regions  of  the  world.
0	Grouping  historical  postcards  using  query  by  example  word  spotting.  Handwritten  historical  documents  pose  extremely  challenging  problems  for  automatic  analysis.  This  is  due  to  the  high  variability  observed  in  handwritten  script,  the  use  of  writing  styles  and  script  types  unknown  today,  the  frequently  lacking  orthographic  standardization,  and  the  degradation  of  the  respective  documents.  Therefore,  it  is  currently  out  of  question  to  develop  general  purpose  handwriting  recognition  systems  for  historical  document  collections.  It  is,  however,  possible  to  search  relatively  homogeneous  document  collections  using  word  spotting  techniques.  In  this  paper  we  consider  the  analysis  of  a  challenging  collection  of  postcards  from  the  period  of  World  War  I  delivered  by  the  German  military  postal  service.  More  specifically,  we  consider  the  automatic  grouping  of  mail  pieces  by  spotting  potentially  identical  addressees.  As  the  annotation  of  such  documents  is  extremely  challenging  even  for  trained  experts,  a  manually  developed  ground  truth  annotation  will,  in  general,  not  be  available.  Furthermore,  a  reliable  segmentation  on  word  level  will  hardly  be  possible.  With  our  segmentation-free  query-by-example  word  spotting  method  we  investigate  modifications  addressing  the  better  generalization  to  a  multi-writer  scenario  and  its  application  to  degraded  documents.  Promising  results  could  be  achieved  in  this  highly  challenging  scenario.
0	A  comparison  of  sequential  and  combined  approaches  for  named  entity  recognition  in  a  corpus  of  handwritten  medieval  charters.  This  paper  introduces  a  new  corpus  of  multilingual  medieval  handwritten  charter  images,  annotated  with  full  transcription  and  named  entities.  The  corpus  is  used  to  compare  two  approaches  for  named  entity  recognition  in  historical  document  images  in  several  languages:  on  the  one  hand,  a  sequential  approach,  more  commonly  used,  that  sequentially  applies  handwritten  text  recognition  (HTR)  and  named  entity  recognition  (NER),  on  the  other  hand,  a  combined  approach  that  simultaneously  transcribes  the  image  text  line  and  extracts  the  entities.  Experiments  conducted  on  the  charter  corpus  in  Latin,  early  new  high  German  and  old  Czech  for  name,  date  and  location  recognition  demonstrate  a  superior  performance  of  the  combined  approach.
0	Automatic  chinese  pronunciation  error  detection  using  svm  trained  with  structural  features.  Pronunciation  errors  are  often  made  by  learners  of  a  foreign  language.  To  build  a  Computer-Assisted  Language  Learning  (CALL)  system  to  support  them,  automatic  error  detection  is  essential.  In  this  study,  Japanese  learners  of  Chinese  are  focused  on.  We  investigated  in  automatic  detection  of  their  typical  and  frequent  phoneme  production  errors.  For  this  aim,  four  databases  are  newly  created  and  we  propose  a  detection  method  using  Support  Vector  Machine  (SVM)  with  structural  features.  The  proposed  method  is  compared  to  two  baseline  methods  of  Goodness  Of  Pronunciation  (GOP)  and  Likelihood  Ratio  (LR)  under  the  task  of  phoneme  error  detection.  Experiments  show  that  the  proposed  method  performs  much  better  than  both  of  the  two  baseline  methods.  For  example,  the  false  rejection  rate  is  reduced  by  as  much  as  82%.  However,  the  results  also  indicate  some  drawbacks  of  using  SVM  with  structural  features.  In  this  paper,  we  discuss  merits  and  demerits  of  the  proposed  method  and  in  what  kind  of  real  applications  it  works  effectively.
0	Speaker  adaptation  of  deep  neural  networks  using  a  hierarchy  of  output  layers.  Deep  neural  networks  (DNN)  used  for  acoustic  modeling  in  speech  recognition  often  have  a  very  large  number  of  output  units  corresponding  to  context  dependent  (CD)  triphone  HMM  states.  The  amount  of  data  available  for  speaker  adaptation  is  often  limited  so  a  large  majority  of  these  CD  states  may  not  be  observed  during  adaptation.  In  this  case,  the  posterior  probabilities  of  unseen  CD  states  are  only  pushed  towards  zero  during  DNN  speaker  adaptation  and  the  ability  to  predict  these  states  can  be  degraded  relative  to  the  speaker  independent  network.  We  address  this  problem  by  appending  an  additional  output  layer  which  maps  the  original  set  of  DNN  output  classes  to  a  smaller  set  of  phonetic  classes  (e.g.  monophones)  thereby  reducing  the  occurrences  of  unseen  states  in  the  adaptation  data.  Adaptation  proceeds  by  backpropagation  of  errors  from  the  new  output  layer,  which  is  disregarded  at  recognition  time  when  posterior  probabilities  over  the  original  set  of  CD  states  are  used.  We  demonstrate  the  benefits  of  this  approach  over  adapting  the  network  with  the  original  set  of  CD  states  using  experiments  on  a  Japanese  voice  search  task  and  obtain  5.03%  relative  reduction  in  character  error  rate  with  approximately  60  seconds  of  adaptation  data.
0	Lightweight  voice  anonymization  based  on  data  driven  optimization  of  cascaded  voice  modification  modules.  In  this  paper,  we  propose  a  voice  anonymization  framework  based  on  data-driven  optimization  of  cascaded  voice  modification  modules.  With  increasing  opportunities  to  use  speech  dialogue  with  machines  nowadays,  research  regarding  privacy  protection  of  speaker  information  encapsulated  in  speech  data  is  attracting  attention.  Anonymization,  which  is  one  of  the  methods  for  privacy  protection,  is  based  on  signal  processing  manners,  and  the  other  one  based  on  machine  learning  ones.  Both  approaches  have  a  trade  off  between  intelligibility  of  speech  and  degree  of  anonymization.  The  proposed  voice  anonymization  framework  utilizes  advantages  of  machine  learning  and  signal  processing-based  approaches  to  find  the  optimized  trade  off  between  the  two.  We  use  signal  processing  methods  with  training  data  for  optimizing  hyperparameters  in  a  data-driven  manner.  The  speech  is  modified  using  cascaded  lightweight  signal  processing  methods  and  then  evaluated  using  black-box  ASR  and  ASV,  respectively.  Our  proposed  method  succeeded  in  deteriorating  the  speaker  recognition  rate  by  approximately  22%  while  simultaneously  improved  the  speech  recognition  rate  by  over  3%  compared  to  a  signal  processing-based  conventional  method.
0	Dialog  context  aware  end  to  end  speech  recognition.  Existing  speech  recognition  systems  are  typically  built  at  the  sentence  level,  although  it  is  known  that  dialog  context,  e.g.  higher-level  knowledge  that  spans  across  sentences  or  speakers,  can  help  the  processing  of  long  conversations.  The  recent  progress  in  end-to-end  speech  recognition  systems  promises  to  integrate  all  available  information  (e.g.  acoustic,  language  resources)  into  a  single  model,  which  is  then  jointly  optimized.  It  seems  natural  that  such  dialog  context  information  should  thus  also  be  integrated  into  the  end-to-end  models  to  improve  recognition  accuracy  further.  In  this  work,  we  present  a  dialog-context  aware  speech  recognition  model,  which  explicitly  uses  context  information  beyond  sentence-level  information,  in  an  end-to-end  fashion.  Our  dialog-context  model  captures  a  history  of  sentence-level  contexts,  so  that  the  whole  system  can  be  trained  with  dialog-context  information  in  an  end-to-end  manner.  We  evaluate  our  proposed  approach  on  the  Switchboard  conversational  speech  corpus,  and  show  that  our  system  outperforms  a  comparable  sentence-level  end-to-end  speech  recognition  system.
0	Keyword  spotting  framework  using  dynamic  background  model.  An  important  task  in  Keyword  Spotting  in  handwritten  documents  is  to  separate  Keywords  from  Non  Keywords.  Very  often  this  is  achieved  by  learning  a  filler  or  background  model.  A  common  method  of  building  a  background  model  is  to  allow  all  possible  sequences  or  transitions  of  characters.  However,  due  to  large  variation  in  handwriting  styles,  allowing  all  possible  sequences  of  characters  as  background  might  result  in  an  increased  false  reject.  A  weak  background  model  could  result  in  high  false  accept.  We  propose  a  novel  way  of  learning  the  background  model  dynamically.  The  approach  first  used  in  word  spotting  in  speech  uses  a  feature  vector  of  top  K  local  scores  per  character  and  top  N  global  scores  of  matching  hypotheses.  A  two  class  classifier  is  learned  on  these  features  to  classify  between  Keyword  and  Non  Keyword.
0	Writer  code  based  adaptation  of  deep  neural  network  for  offline  handwritten  chinese  text  recognition.  Recently,  we  propose  deep  neural  network  based  hidden  Markov  models  (DNN-HMMs)  for  offline  handwritten  Chinese  text  recognition.  In  this  study,  we  design  a  novel  writer  code  based  adaptation  on  top  of  the  DNN-HMM  to  further  improve  the  accuracy  via  a  customized  recognizer.  The  writer  adaptation  is  implemented  by  incorporating  the  new  layers  with  the  original  input  or  hidden  layers  of  the  writer-independent  DNN.  These  new  layers  are  driven  by  the  so-called  writer  code,  which  guides  and  adapts  the  DNN-based  recognizer  with  the  writer  information.  In  the  training  stage,  the  writer-aware  layers  are  jointly  learned  with  the  conventional  DNN  layers  in  an  alternative  manner.  In  the  recognition  stage,  with  the  initial  recognition  results  from  the  first-pass  decoding  with  the  writer-independent  DNN,  an  unsupervised  adaptation  is  performed  to  generate  the  writer  code  via  the  cross-entropy  criterion  for  the  subsequent  second-pass  decoding.  The  experiments  on  the  most  challenging  task  of  ICDAR  2013  Chinese  handwriting  competition  show  that  our  proposed  adaptation  approach  can  achieve  consistent  and  significant  improvements  of  recognition  accuracy  over  a  highperformance  writer-independent  DNN-HMM  based  recognizer  across  all  60  writers,  yielding  a  relative  character  error  rate  reduction  of  23.62%  in  average.
0	Lstm  based  whisper  detection.  This  article  presents  a  whisper  speech  detector  in  the  far-field  domain.  The  proposed  system  consists  of  a  long-short  term  memory  (LSTM)  neural  network  trained  on  log-filterbank  energy  (LFBE)  acoustic  features.  This  model  is  trained  and  evaluated  on  recordings  of  human  interactions  with  voice-controlled,  far-field  devices  in  whisper  and  normal  phonation  modes.  We  compare  multiple  inference  approaches  for  utterance-level  classification  by  examining  trajectories  of  the  LSTM  posteriors.  In  addition,  we  engineer  a  set  of  features  based  on  the  signal  characteristics  inherent  to  whisper  speech,  and  evaluate  their  effectiveness  in  further  separating  whisper  from  normal  speech.  A  benchmarking  of  these  features  using  multilayer  perceptrons  (MLP)  and  LSTMs  suggests  that  the  proposed  features,  in  combination  with  LFBE  features,  can  help  us  further  improve  our  classifiers.  We  prove  that,  with  enough  data,  the  LSTM  model  is  indeed  as  capable  of  learning  whisper  characteristics  from  LFBE  features  alone  compared  to  a  simpler  MLP  model  that  uses  both  LFBE  and  features  engineered  for  separating  whisper  and  normal  speech.  In  addition,  we  prove  that  the  LSTM  classifiers  accuracy  can  be  further  improved  with  the  incorporation  of  the  proposed  engineered  features.
0	From  terminology  to  evaluation  performance  assessment  of  automatic  signature  verification  systems.  This  paper  is  an  effort  towards  the  development  of  a  shared  conceptualization  regarding  automatic  signature  verification  systems.  The  requirements  of  both  communities,  Pattern  Recognition  and  Forensic  Handwriting  Examiners,  are  explicitly  focused.  This  is  required  because  an  increasing  gap  regarding  evaluation  of  automatic  verification  systems  is  observed  in  the  recent  past.  The  paper  addresses  three  major  areas.  First,  it  highlights  how  signature  verification  is  taken  differently  in  the  above  mentioned  communities  and  why  this  gap  is  increasing.  Various  factors  that  widen  this  gap  are  discussed  with  reference  to  some  of  the  recent  signature  verification  studies  and  probable  solutions  are  suggested.  Second,  it  discusses  the  state-of-the-art  evaluation  and  its  problems  as  seen  by  FHEs.  The  real  evaluation  issues  faced  by  FHEs,  when  trying  to  incorporate  automatic  signature  verification  systems  in  their  routine  casework,  are  presented.  Third,  it  reports  a  standardized  evaluation  scheme  capable  of  fulfilling  the  requirements  of  both  PR  researchers  and  FHEs.
0	Zoning  aggregated  hypercolumns  for  keyword  spotting.  In  this  paper  we  present  a  novel  descriptor  and  method  for  segmentation-based  keyword  spotting.  We  introduce  Zoning-Aggregated  Hypercolumn  features  as  pixel-level  cues  for  document  images.  Motivated  by  recent  research  in  machine  vision,  we  use  an  appropriately  pretrained  convolutional  network  as  a  feature  extraction  tool.  The  resulting  local  cues  are  subsequently  aggregated  to  form  word-level  fixed-length  descriptors.  Encoding  is  computationally  inexpensive  and  does  not  require  learning  a  separate  feature  generative  model,  in  contrast  to  other  widely  used  encoding  methods  (such  as  Fisher  Vectors).  Keyword  spotting  trials  on  machine-printed  and  handwritten  documents  show  that  the  proposed  model  gives  very  competitive  results.
0	Local  co  occurrence  and  contrast  mapping  for  document  image  binarization.  Document  Image  Binarization  refers  to  the  task  of  transforming  a  scanned  image  of  a  handwritten  or  printed  document  into  a  bi-level  representation  containing  only  characters  and  background.  Here,  we  address  the  historic  document  image  binarization  problem  using  a  three-stage  methodology.  Firstly,  we  remove  possible  stains  and  noise  from  the  document  image  by  estimating  the  document  background  image.  The  remaining  background  and  character  pixels  are  separated  using  a  Local  Co-occurrence  Mapping,  local  contrast  and  a  two-state  Gaussian  Mixture  Model.  In  the  last  stage,  possible  isolated  misclassified  blobs  are  removed  by  a  morphology  operator.  The  proposed  scheme  offers  robust  and  fast  performance,  especially  for  handwritten  documents.
0	On  the  generalization  ability  of  distributed  online  learners.  We  propose  a  fully-distributed  stochastic-gradient  strategy  based  on  diffusion  adaptation  techniques.  We  show  that,  for  strongly  convex  risk  functions,  the  excess-risk  at  every  node  decays  at  the  rate  of  O(1/Ni),  where  N  is  the  number  of  learners  and  i  is  the  iteration  index.  In  this  way,  the  distributed  diffusion  strategy,  which  relies  only  on  local  interactions,  is  able  to  achieve  the  same  convergence  rate  as  centralized  strategies  that  have  access  to  all  data  from  the  nodes  at  every  iteration.  We  also  show  that  every  learner  is  able  to  improve  its  excess-risk  in  comparison  to  the  non-cooperative  mode  of  operation  where  each  learner  would  operate  independently  of  the  other  learners.
0	L3das21  challenge  machine  learning  for  3d  audio  signal  processing.  The  L3DAS21  Challenge11www.13das.com/mlsp2021  is  aimed  at  encouraging  and  fostering  collaborative  research  on  machine  learning  for  3D  audio  signal  processing,  with  particular  focus  on  3D  speech  enhancement  (SE)  and  3D  sound  localization  and  detection  (SELD).  Alongside  with  the  challenge,  we  release  the  L3DAS21  dataset,  a  65  hours  3D  audio  corpus,  accompanied  with  a  Python  API  that  facilitates  the  data  usage  and  results  submission  stage.  Usually,  machine  learning  approaches  to  3D  audio  tasks  are  based  on  single-perspective  Ambisonics  recordings  or  on  arrays  of  single-capsule  microphones.  We  propose,  instead,  a  novel  multichannel  audio  configuration  based  multiple-source  and  multiple-perspective  Ambisonics  recordings,  performed  with  an  array  of  two  first-order  Ambisonics  microphones.  To  the  best  of  our  knowledge,  it  is  the  first  time  that  a  dualmic  Ambisonics  configuration  is  used  for  these  tasks.  We  provide  baseline  models  and  results  for  both  tasks,  obtained  with  state-of-the-art  architectures:  FaSNet  for  SE  and  SELDnet  for  SELD.
0	Learning  of  scanning  strategies  for  electronic  support  using  predictive  state  representations.  In  Electronic  Support,  a  receiver  must  monitor  a  wide  frequency  spectrum  in  which  threatening  emitters  operate.  A  common  approach  is  to  use  sensors  with  high  sensitivity  but  a  narrow  bandwidth.  To  maintain  surveillance  over  the  whole  spectrum,  the  sensor  has  to  sweep  between  frequency  bands  but  requires  a  scanning  strategy.  Search  strategies  are  usually  designed  prior  to  the  mission  using  an  approximate  knowledge  of  illumination  patterns.  This  often  results  in  open-loop  policies  that  cannot  take  advantage  of  previous  observations.  As  pointed  out  in  past  researches,  these  strategies  lack  of  robustness  to  the  prior.  We  propose  a  new  closed  loop  search  strategy  that  learns  a  stochastic  model  of  each  radar  using  predictive  state  representations.  The  learning  algorithm  benefits  from  the  recent  advances  in  spectral  learning  and  rank  minimization  using  nuclear  norm  penalization.
0	Ppo  cma  proximal  policy  optimization  with  covariance  matrix  adaptation.  Proximal  Policy  Optimization  (PPO)  is  a  highly  popular  model-free  reinforcement  learning  (RL)  approach.  However,  we  observe  that  in  a  continuous  action  space,  PPO  can  prematurely  shrink  the  exploration  variance,  which  leads  to  slow  progress  and  may  make  the  algorithm  prone  to  getting  stuck  in  local  optima.  Drawing  inspiration  from  CMA-ES,  a  black-box  evolutionary  optimization  method  designed  for  robustness  in  similar  situations,  we  propose  PPO-CMA,  a  proximal  policy  optimization  approach  that  adaptively  expands  the  exploration  variance  to  speed  up  progress.  With  only  minor  changes  to  PPO,  our  algorithm  considerably  improves  performance  in  Roboschool  continuous  control  benchmarks.  Our  results  also  show  that  PPO-CMA,  as  opposed  to  PPO,  is  significantly  less  sensitive  to  the  choice  of  hyperparameters,  allowing  one  to  use  it  in  complex  movement  optimization  tasks  without  requiring  tedious  tuning.
0	Online  supervised  acoustic  system  identification  exploiting  prelearned  local  affine  subspace  models.  In  this  paper  we  present  a  novel  algorithm  for  improved  block-online  supervised  acoustic  system  identification  in  adverse  noise  scenarios  by  exploiting  prior  knowledge  about  the  space  of  Room  Impulse  Responses  (RIRs).  The  method  is  based  on  the  assumption  that  the  variability  of  the  unknown  RIRs  is  controlled  by  only  few  physical  parameters,  describing,  e.g.,  source  position  movements,  and  thus  is  confined  to  a  low-dimensional  manifold  which  is  modelled  by  a  union  of  affine  subspaces.  The  offsets  and  bases  of  the  affine  subspaces  are  learned  in  advance  from  training  data  by  unsupervised  clustering  followed  by  Principal  Component  Analysis.  We  suggest  to  denoise  the  parameter  update  of  any  supervised  adaptive  filter  by  projecting  it  onto  an  optimal  affine  subspace  which  is  selected  based  on  a  novel  computationally  efficient  approximation  of  the  associated  evidence.  The  proposed  method  significantly  improves  the  system  identification  performance  of  state-of-the-art  algorithms  in  adverse  noise  scenarios.
0	Recurrent  neural  networks  with  flexible  gates  using  kernel  activation  functions.  Gated  recurrent  neural  networks  have  achieved  remarkable  results  in  the  analysis  of  sequential  data.  Inside  these  networks,  gates  are  used  to  control  the  flow  of  information,  allowing  to  model  even  very  long-term  dependencies  in  the  data.  In  this  paper,  we  investigate  whether  the  original  gate  equation  (a  linear  projection  followed  by  an  element-wise  sigmoid)  can  be  improved.  In  particular,  we  design  a  more  flexible  architecture,  with  a  small  number  of  adaptable  parameters,  which  is  able  to  model  a  wider  range  of  gating  functions  than  the  classical  one.  To  this  end,  we  replace  the  sigmoid  function  in  the  standard  gate  with  a  non-parametric  formulation  extending  the  recently  proposed  kernel  activation  function  (KAF),  with  the  addition  of  a  residual  skip-connection.  A  set  of  experiments  on  sequential  variants  of  the  MNIST  dataset  shows  that  the  adoption  of  this  novel  gate  allows  to  improve  accuracy  with  a  negligible  cost  in  terms  of  computational  power  and  with  a  large  speed-up  in  the  number  of  training  iterations.
0	Microvascular  blood  flow  estimation  in  sublingual  microcirculation  videos  based  on  a  principal  curve  tracing  algorithm.  Microcirculatory  perfusion  is  an  important  metric  for  diagnosing  pathological  conditions  in  patients.  Capillary  density  and  red  blood  cell  (RBC)  velocity  provide  a  measure  of  tissue  perfusion.  Estimating  RBC  velocity  is  a  challenging  problem  due  to  noisy  video  sequences,  low  contrast  between  the  vessels  and  the  background,  and  thousands  of  RBCs  moving  rapidly  through  video  sequences.  Typically,  physicians  manually  trace  small  blood  vessels  and  visually  estimate  RBC  velocities.  The  task  is  labor  intensive,  tedious,  and  time-consuming.  In  this  paper,  we  present  a  novel  application  of  a  principal  curve  tracing  algorithm  to  automatically  track  RBCs  across  video  frames  and  estimate  their  velocity  based  on  the  displacements  of  RBCs  between  two  consecutive  frames.  The  proposed  method  is  implemented  in  one  sublingual  microcirculation  video  of  a  healthy  subject.
0	Memory  augmented  neural  network  for  source  separation.  Recurrent  neural  network  (RNN)  based  on  long  short-term  memory  (LSTM)  has  been  successfully  developed  for  single-channel  source  separation.  Temporal  information  is  learned  by  using  dynamic  states  which  are  evolved  through  time  and  stored  as  an  internal  memory.  The  performance  of  source  separation  is  constrained  due  to  the  limitation  of  internal  memory  which  could  not  sufficiently  preserve  long-term  characteristics  from  different  sources.  This  study  deals  with  this  limitation  by  incorporating  an  external  memory  in  RNN  and  accordingly  presents  a  memory  augmented  neural  network  for  source  separation.  In  particular,  we  carry  out  a  neural  Turing  machine  to  learn  a  separation  model  for  sequential  signals  of  speech  and  noise  in  presence  of  different  speakers  and  noise  types.  Experiments  show  that  speech  enhancement  based  on  memory  augmented  neural  network  consistently  outperforms  that  using  deep  neural  network  and  LSTM  in  terms  of  short-term  objective  intelligibility  measure.
0	Tweetcat  a  tool  for  building  twitter  corpora  of  smaller  languages.  This  paper  presents  TweetCaT,  an  open-source  Python  tool  for  building  Twitter  corpora  that  was  designed  for  smaller  languages.  Using  the  Twitter  search  API  and  a  set  of  seed  terms,  the  tool  identifies  users  tweeting  in  the  language  of  interest  together  with  their  friends  and  followers.  By  running  the  tool  for  235  days  we  tested  it  on  the  task  of  collecting  two  monitor  corpora,  one  for  Croatian  and  Serbian  and  the  other  for  Slovene,  thus  also  creating  new  and  valuable  resources  for  these  languages.  A  post-processing  step  on  the  collected  corpus  is  also  described,  which  filters  out  users  that  tweet  predominantly  in  a  foreign  language  thus  further  cleans  the  collected  corpora.  Finally,  an  experiment  on  discriminating  between  Croatian  and  Serbian  Twitter  users  is  reported.
0	Analysis  of  short  texts  on  the  web  introduction  to  special  issue.  1  Analysis  of  short  texts  on  the  Web  Analysis  of  web  and  social  media  data  is  a  rapidly  growing  area  of  research.  Researchers  seek  to  extract  a  wide  variety  of  information  from  these  texts  in  order  to  address  specific  user  needs,  profile  attitudes  and  intentions,  and  target  advertising,  etc.,  which  may  require  application  of  the  full  range  of  natural  processing  techniques.  However,  many  of  the  texts  in  question—including  news  feeds,  document  titles,  FAQs,  and  tweets—exist  as  short,  sometimes  barely  sentence-like  snippets  that  do  not  always  follow  the  lexical  and  syntactic  conventions  assumed  by  many  language  processing  tools.  Many  NLP  analyses  rely  on  the  repetition  of  specific  lexical  items  throughout  the  text  in  order  to  identify  topic,  genre,  and  other  features;  without  sufficient  context  to  enable  such  analyses,  and  because  of  their  often  eccentric  grammatical  style,  short  texts  pose  a  new  kind  of  challenge  for
0	Whad  wikipedia  historical  attributes  data.  This  paper  describes  the  generation  of  temporally  anchored  infobox  attribute  data  from  the  Wikipedia  history  of  revisions.  By  mining  (attribute,  value)  pairs  from  the  revision  history  of  the  English  Wikipedia  we  are  able  to  collect  a  comprehensive  knowledge  base  that  contains  data  on  how  attributes  change  over  time.  When  dealing  with  the  Wikipedia  edit  history,  vandalic  and  erroneous  edits  are  a  concern  for  data  quality.  We  present  a  study  of  vandalism  identification  in  Wikipedia  edits  that  uses  only  features  from  the  infoboxes,  and  show  that  we  can  obtain,  on  this  dataset,  an  accuracy  comparable  to  a  state-of-the-art  vandalism  identification  method  that  is  based  on  the  whole  article.  Finally,  we  discuss  different  characteristics  of  the  extracted  dataset,  which  we  make  available  for  further  study.
0	Example  based  treebank  querying.  The  recent  construction  of  large  linguistic  treebanks  for  spoken  and  written  Dutch  (e.g.  CGN,  LASSY,  Alpino)  has  created  new  and  exciting  opportunities  for  the  empirical  investigation  of  Dutch  syntax  and  semantics.  However,  the  exploitation  of  those  treebanks  requires  knowledge  of  specific  data  structures  and  query  languages  such  as  XPath.  Linguists  who  are  unfamiliar  with  formal  languages  are  often  reluctant  towards  learning  such  a  language.  In  order  to  make  treebank  querying  more  attractive  for  non-technical  users  we  developed  GrETEL  (Greedy  Extraction  of  Trees  for  Empirical  Linguistics),  a  query  engine  in  which  linguists  can  use  natural  language  examples  as  a  starting  point  for  searching  the  Lassy  treebank  without  knowledge  about  tree  representations  nor  formal  query  languages.  By  allowing  linguists  to  search  for  similar  constructions  as  the  example  they  provide,  we  hope  to  bridge  the  gap  between  traditional  and  computational  linguistics.  Two  case  studies  are  conducted  to  provide  a  concrete  demonstration  of  the  tool.  The  architecture  of  the  tool  is  optimised  for  searching  the  LASSY  treebank,  but  the  approach  can  be  adapted  to  other  treebank  lay-outs.
0	Illinoiscloudnlp  text  analytics  services  in  the  cloud.  Natural  Language  Processing  (NLP)  continues  to  grow  in  popularity  in  a  range  of  research  and  commercial  applications.  However,  installing,  maintaining,  and  running  NLP  tools  can  be  time  consuming,  and  many  commercial  and  research  end  users  have  only  intermittent  need  for  large  processing  capacity.  This  paper  describes  ILLINOISCLOUDNLP,  an  on-demand  framework  built  around  NLPCURATOR  and  Amazon  Web  Services’  Elastic  Compute  Cloud  (EC2).  This  framework  provides  a  simple  interface  to  end  users  via  which  they  can  deploy  one  or  more  NLPCURATOR  instances  on  EC2,  upload  plain  text  documents,  specify  a  set  of  Text  Analytics  tools  (NLP  annotations)  to  apply,  and  process  and  store  or  download  the  processed  data.  It  also  allows  end  users  to  use  a  model  trained  on  their  own  data:  ILLINOISCLOUDNLP  takes  care  of  training,  hosting,  and  applying  it  to  new  data  just  as  it  does  with  existing  models  within  NLPCURATOR.  As  a  representative  use  case,  we  describe  our  use  of  ILLINOISCLOUDNLP  to  process  3:05  million  documents  used  in  the  2012  and  2013  Text  Analysis  Conference  Knowledge  Base  Population  tasks  at  a  relatively  deep  level  of  processing,  in  approximately  20  hours,  at  an  approximate  cost  of  US$500;  this  is  about  20  times  faster  than  doing  so  on  a  single  server  and  requires  no  human  supervision  and  no  NLP  or  Machine  Learning  expertise.
0	The  french  algerian  code  switching  triggered  audio  corpus  facst.  The  French  Algerian  Code-Switching  Triggered  corpus  (FACST)  was  created  in  order  to  support  a  variety  of  studies  in  phonetics,  prosody  and  natural  language  processing.  The  first  aim  of  the  FACST  corpus  is  to  collect  a  spontaneous  Code-switching  speech  (CS)  corpus.  In  order  to  obtain  a  large  quantity  of  spontaneous  CS  utterances  in  natural  conversations  experiments  were  carried  out  on  how  to  elicit  CS.  Applying  a  triggering  protocol  by  means  of  code-switched  questions  was  found  to  be  effective  in  eliciting  CS  in  the  responses.  To  ensure  good  audio  quality,  all  recordings  were  made  in  a  soundproof  room  or  in  a  very  calm  room.  This  paper  describes  FACST  corpus,  along  with  the  principal  steps  to  build  a  CS  speech  corpus  in  French-Algerian  languages  and  data  collection  steps.  We  also  explain  the  selection  criteria  for  the  CS  speakers  and  the  recording  protocols  used.  We  present  the  methods  used  for  data  segmentation  and  annotation,  and  propose  a  conventional  transcription  of  this  type  of  speech  in  each  language  with  the  aim  of  being  well-suited  for  both  computational  linguistic  and  acoustic-phonetic  studies.  We  provide  an  a  quantitative  description  of  the  FACST  corpus  along  with  results  of  linguistic  studies,  and  discuss  some  of  the  challenges  we  faced  in  collecting  CS  data.
0	A  multi  platform  arabic  news  comment  dataset  for  offensive  language  detection.  Access  to  social  media  often  enables  users  to  engage  in  conversation  with  limited  accountability.  This  allows  a  user  to  share  their  opinions  and  ideology,  especially  regarding  public  content,  occasionally  adopting  offensive  language.  This  may  encourage  hate  crimes  or  cause  mental  harm  to  targeted  individuals  or  groups.  Hence,  it  is  important  to  detect  offensive  comments  in  social  media  platforms.  Typically,  most  studies  focus  on  offensive  commenting  in  one  platform  only,  even  though  the  problem  of  offensive  language  is  observed  across  multiple  platforms.  Therefore,  in  this  paper,  we  introduce  and  make  publicly  available  a  new  dialectal  Arabic  news  comment  dataset,  collected  from  multiple  social  media  platforms,  including  Twitter,  Facebook,  and  YouTube.  We  follow  two-step  crowd-annotator  selection  criteria  for  low-representative  language  annotation  task  in  a  crowdsourcing  platform.  Furthermore,  we  analyze  the  distinctive  lexical  content  along  with  the  use  of  emojis  in  offensive  comments.  We  train  and  evaluate  the  classifiers  using  the  annotated  multi-platform  dataset  along  with  other  publicly  available  data.  Our  results  highlight  the  importance  of  multiple  platform  dataset  for  (a)  cross-platform,  (b)  cross-domain,  and  (c)  cross-dialect  generalization  of  classifier  performance.
0	A  scalable  architecture  for  web  deployment  of  spoken  dialogue  systems.  We  describe  a  scalable  architecture,  particularly  well-suited  to  cloud-based  computing,  which  can  be  used  for  Web-deployment  of  spoken  dialogue  systems.  In  common  with  similar  platforms,  like  WAMI  and  the  Nuance  Mobile  Developer  Platform,  we  use  a  client/server  approach  in  which  speech  recognition  is  carried  out  on  the  server  side;  our  architecture,  however,  differs  from  these  systems  in  offering  considerably  more  elaborate  server-side  functionality,  based  on  large-scale  grammar-based  language  processing  and  generic  dialogue  management.  We  describe  two  substantial  applications,  built  using  our  framework,  which  we  argue  would  have  been  hard  to  construct  in  WAMI  or  NMDP.  Finally,  we  present  a  series  of  evaluations  carried  out  using  CALL-SLT,  a  speech  translation  game,  where  we  contrast  performance  in  Web  and  desktop  versions.  Task  Error  Rate  in  the  Web  version  is  only  slightly  inferior  that  in  the  desktop  one,  and  the  average  additional  latency  is  under  half  a  second.  The  software  is  generally  available  for  research  purposes.
0	Exploring  the  role  of  lexis  and  grammar  for  the  stable  identification  of  register  in  an  unrestricted  corpus  of  web  documents.  The  Internet  offers  great  possibilities  for  many  scientific  disciplines  that  utilize  text  data.  However,  the  potential  of  online  data  can  be  limited  by  the  lack  of  information  on  the  genre  or  register  of  the  documents,  as  register—whether  a  text  is,  e.g.,  a  news  article  or  a  recipe—is  arguably  the  most  important  predictor  of  linguistic  variation  (see  Biber  in  Corpus  Linguist  Linguist  Theory  8:9–37,  2012).  Despite  having  received  significant  attention  in  recent  years,  the  modeling  of  online  registers  has  faced  a  number  of  challenges,  and  previous  studies  have  presented  contradictory  results.  In  particular,  these  have  concerned  (1)  the  extent  to  which  registers  can  be  automatically  identified  in  a  large,  unrestricted  corpus  of  web  documents  and  (2)  the  stability  of  the  models,  specifically  the  kinds  of  linguistic  features  that  achieve  the  best  performance  while  reflecting  the  registers  instead  of  corpus  idiosyncrasies.  Furthermore,  although  the  linguistic  properties  of  registers  vary  importantly  in  a  number  of  ways  that  may  affect  their  modeling,  this  variation  is  often  bypassed.  In  this  article,  we  tackle  these  issues.  We  model  online  registers  in  the  largest  available  corpus  of  online  registers,  the  Corpus  of  Online  Registers  of  English  (CORE).  Additionally,  we  evaluate  the  stability  of  the  models  towards  corpus  idiosyncrasies,  analyze  the  role  of  different  linguistic  features  in  them,  and  examine  how  individual  registers  differ  in  these  two  aspects.  We  show  that  (1)  competitive  classification  performance  on  a  large-scale,  unrestricted  corpus  can  be  achieved  through  a  combination  of  lexico-grammatical  features,  (2)  the  inclusion  of  grammatical  information  improves  the  stability  of  the  model,  whereas  many  of  the  previously  best-performing  feature  sets  are  less  stable,  and  that  (3)  registers  can  be  placed  in  a  continuum  based  on  the  discriminative  importance  of  lexis  and  grammar.  These  register-specific  characteristics  can  explain  the  variation  observed  in  previous  studies  concerning  the  automatic  identification  of  online  registers  and  the  importance  of  different  linguistic  features  for  them.  Thus,  our  results  offer  explanations  for  the  jungle-likeness  of  online  data  and  provide  essential  information  on  online  registers  for  all  studies  using  online  data.
0	Automatic  term  recognition  needs  multiple  evidence.  In  this  paper  we  argue  that  the  automatic  term  extraction  procedure  is  an  inherently  multifactor  process  and  the  term  extraction  models  needs  to  be  based  on  multiple  features  including  a  specific  type  of  a  terminological  resource  under  development.  We  proposed  to  use  three  types  of  features  for  extraction  of  two-word  terms  and  showed  that  all  these  types  of  features  are  useful  for  term  extraction.  The  set  of  features  includes  new  features  such  as  features  extracted  from  an  existing  domain-specific  thesaurus  and  features  based  on  Internet  search  results.  We  studied  the  set  of  features  for  term  extraction  in  two  different  domains  and  showed  that  the  combination  of  several  types  of  features  considerably  enhances  the  quality  of  the  term  extraction  procedure.  We  found  that  for  developing  term  extraction  models  in  a  specific  domain,  it  is  important  to  take  into  account  some  properties  of  the  domain.
0	Investigating  the  image  of  entities  in  social  media  dataset  design  and  first  results.  The  objective  of  this  paper  is  to  describe  the  design  of  a  dataset  that  deals  with  the  image  (i.e.,  representation,  web  reputation)  of  various  entities  populating  the  Internet:  politicians,  celebrities,  companies,  brands  etc.  Our  main  contribution  is  to  build  and  provide  an  original  annotated  French  dataset.  This  dataset  consists  of  11  527  manually  annotated  tweets  expressing  the  opinion  on  specific  facets  (e.g.,  ethic,  communication,  economic  project)  describing  two  French  policitians  over  time.  We  believe  that  other  researchers  might  benefit  from  this  experience,  since  designing  and  implementing  such  a  dataset  has  proven  quite  an  interesting  challenge.  This  design  comprises  different  processes  such  as  data  selection,  formal  definition  and  instantiation  of  an  image.  We  have  set  up  a  full  open-source  annotation  platform.  In  addition  to  the  dataset  design,  we  present  the  first  results  that  we  obtained  by  applying  clustering  methods  to  the  annotated  dataset  in  order  to  extract  the  entity  images.
0	The  mmascs  multi  modal  annotated  synchronous  corpus  of  audio  video  facial  motion  and  tongue  motion  data  of  normal  fast  and  slow  speech.  In  this  paper,  we  describe  and  analyze  a  corpus  of  speech  data  that  we  have  recorded  in  multiple  modalities  simultaneously:  facial  motion  via  optical  motion  capturing,  tongue  motion  via  electro-magnetic  articulography,  as  well  as  conventional  video  and  highquality  audio.  The  corpus  consists  of  320  phonetically  diverse  sentences  uttered  by  a  male  Austrian  German  speaker  at  normal,  fast  and  slow  speaking  rate.  We  analyze  the  influence  of  speaking  rate  on  phone  durations  and  on  tongue  motion.  Furthermore,  we  investigate  the  correlation  between  tongue  and  facial  motion.  The  data  corpus  is  available  free  of  charge  for  research  use,  including  phonetic  annotations  and  a  playback  software  which  visualizes  the  3D  data,  from  the  website  http://cordelia.ftw.at/mmascs
0	Annotating  and  extracting  synthesis  process  of  all  solid  state  batteries  from  scientific  literature.  The  synthesis  process  is  essential  for  achieving  computational  experiment  design  in  the  field  of  inorganic  materials  chemistry.  In  this  work,  we  present  a  novel  corpus  of  the  synthesis  process  for  all-solid-state  batteries  and  an  automated  machine  reading  system  for  extracting  the  synthesis  processes  buried  in  the  scientific  literature.  We  define  the  representation  of  the  synthesis  processes  using  flow  graphs,  and  create  a  corpus  from  the  experimental  sections  of  243  papers.  The  automated  machine-reading  system  is  developed  by  a  deep  learning-based  sequence  tagger  and  simple  heuristic  rule-based  relation  extractor.  Our  experimental  results  demonstrate  that  the  sequence  tagger  with  the  optimal  setting  can  detect  the  entities  with  a  macro-averaged  F1  score  of  0.826,  while  the  rule-based  relation  extractor  can  achieve  high  performance  with  a  macro-averaged  F1  score  of  0.887.
0	Kontext  advanced  and  flexible  corpus  query  interface.  We  present  an  advanced,  highly  customizable  corpus  query  interface  KonText  built  on  top  of  core  libraries  of  the  open-source  corpus  search  engine  NoSketch  Engine  (NoSkE).  The  aim  is  to  overcome  some  limitations  of  the  original  NoSkE  user  interface  and  provide  integration  capabilities  allowing  connection  of  the  basic  search  service  with  other  language  resources  (LRs).  The  introduced  features  are  based  on  long-term  feedback  given  by  the  users  and  researchers  of  the  Czech  National  Corpus  (CNC)  along  with  other  LRs  providers  running  KonText  as  a  part  of  their  services.  KonText  is  a  fully  operational  and  mature  software  deployed  at  the  CNC  since  2014  that  currently  handles  thousands  user  queries  per  day.
0	Multi  task  and  lifelong  learning  of  kernels.  We  consider  a  problem  of  learning  kernels  for  use  in  SVM  classification  in  the  multi-task  and  lifelong  scenarios  and  provide  generalization  bounds  on  the  error  of  a  large  margin  classifier.  Our  results  show  that,  under  mild  conditions  on  the  family  of  kernels  used  for  learning,  solving  several  related  tasks  simultaneously  is  beneficial  over  single  task  learning.  In  particular,  as  the  number  of  observed  tasks  grows,  assuming  that  in  the  considered  family  of  kernels  there  exists  one  that  yields  low  approximation  error  on  all  tasks,  the  overhead  associated  with  learning  such  a  kernel  vanishes  and  the  complexity  converges  to  that  of  learning  when  this  good  kernel  is  given  to  the  learner.
0	Ising  models  with  latent  conditional  gaussian  variables.  Ising  models  describe  the  joint  probability  distribution  of  a  vector  of  binary  feature  variables.  Typically,  not  all  the  variables  interact  with  each  other  and  one  is  interested  in  learning  the  presumably  sparse  network  structure  of  the  interacting  variables.  However,  in  the  presence  of  latent  variables,  the  conventional  method  of  learning  a  sparse  model  might  fail.  This  is  because  the  latent  variables  induce  indirect  interactions  of  the  observed  variables.  In  the  case  of  only  a  few  latent  conditional  Gaussian  variables  these  spurious  interactions  contribute  an  additional  low-rank  component  to  the  interaction  parameters  of  the  observed  Ising  model.  Therefore,  we  propose  to  learn  a  sparse  +  low-rank  decomposition  of  the  parameters  of  an  Ising  model  using  a  convex  regularized  likelihood  problem.  We  show  that  the  same  problem  can  be  obtained  as  the  dual  of  a  maximum-entropy  problem  with  a  new  type  of  relaxation,  where  the  sample  means  collectively  need  to  match  the  expected  values  only  up  to  a  given  tolerance.  The  solution  to  the  convex  optimization  problem  has  consistency  properties  in  the  high-dimensional  setting,  where  the  number  of  observed  binary  variables  and  the  number  of  latent  conditional  Gaussian  variables  are  allowed  to  grow  with  the  number  of  training  samples.
0	La  educacion  inclusiva  analisis  y  reflexiones  en  la  educacion  superior  ecuatoriana.  El  acceso  a  una  educacion  en  igualdad  de  condiciones  y  oportunidades  de  aprendizaje  es  una  aspiracion  y  un  compromiso  que  debe  ser  asumido  por  todos  los  gobiernos  e  instituciones  publicas  y  privadas.  A  nivel  internacional,  y  especificamente  en  Ecuador,  se  han  generado  leyes  y  acuerdos  que  han  apoyado  el  proceso  de  educacion  inclusiva  en  todas  las  etapas.  El  proposito  de  este  articulo  es  realizar  una  aportacion  sobre  la  situacion  y  los  retos  que  plantea  la  educacion  inclusiva  en  Ecuador,  con  especial  incidencia  en  el  ambito  de  la  educacion  superior.  Para  ello  se  ha  recopilado  y  seleccionado,  con  criterios  de  relevancia,  los  acuerdos  internacionales  y  la  normativa  nacional  que  afecta  a  la  inclusion.  A  traves  de  este  analisis  hemos  podido  constar  el  avance  en  el  tratamiento  de  esta  a  nivel  internacional  y  su  reflejo  en  la  normativa  nacional.  A  pesar  de  ello,  transformar  las  politicas,  la  cultura  y  las  practicas  de  las  Universidades  para  atender  a  la  diversidad  sigue  constituyendo  un  reto  para  todos  los  actores  implicados.  Como  principales  conclusiones  de  este  trabajo  se  establece  la  necesidad  de  avanzar  hacia  un  modelo  de  universidad  basado  en  los  principios  de  la  inclusion  educativa  ya  establecidos,  no  solo  como  una  cuestion  imprescindible  para  mejorar  los  procesos  de  calidad,  sino  como  un  elemento  clave  en  orden  a  construir  sociedades  mas  justas,  democraticas  y  equitativas.
0	Online  pca  with  optimal  regrets.  We  carefully  investigate  the  online  version  of  PCA,  where  in  each  trial  a  learning  algorithm  plays  a  k-dimensional  subspace,  and  suffers  the  compression  loss  on  the  next  instance  when  projected  into  the  chosen  subspace.  In  this  setting,  we  give  regret  bounds  for  two  popular  online  algorithms,  Gradient  Descent  (GD)  and  Matrix  Exponentiated  Gradient  (MEG).  We  show  that  both  algorithms  are  essentially  optimal  in  the  worst-case  when  the  regret  is  expressed  as  a  function  of  the  number  of  trials.  This  comes  as  a  surprise,  since  MEG  is  commonly  believed  to  perform  sub-optimally  when  the  instances  are  sparse.  This  different  behavior  of  MEG  for  PCA  is  mainly  related  to  the  non-negativity  of  the  loss  in  this  case,  which  makes  the  PCA  setting  qualitatively  different  from  other  settings  studied  in  the  literature.  Furthermore,  we  show  that  when  considering  regret  bounds  as  a  function  of  a  loss  budget,  MEG  remains  optimal  and  strictly  outperforms  GD.
0	A  topic  based  recommender  system  for  electronic  marketplace  platforms.  A  large  number  of  items  are  placed,  bought  and  sold  every  day  in  auction  marketplaces  across  the  web.  The  amount  of  information  and  the  number  of  available  items  makes  finding  what  to  buy,  as  well  as  describing  an  item  to  sell,  a  challenge  for  the  participants.  In  this  paper  we  propose  a  topic-based  recommender  system  that  exploits  the  latent  semantics  in  the  item  descriptions  in  order  to  support  the  activities  of  buyers  and  sellers  in  auction  electronic  marketplaces.  We  present  the  design  of  our  system  and  demonstrate  how  it  can  be  used  in  real  life  scenarios.
0	Twitter  based  recommender  system  to  address  cold  start  a  genetic  algorithm  based  trust  modelling  and  probabilistic  sentiment  analysis.  This  paper  investigates  the  design  and  evaluation  of  a  personalized  Recommender  System  (RS)  using  implicit  social  trust  from  Online  Social  Networks  (OSNs),  particularly  to  solve  new  users'  recommendation  problems.  The  proposed  system  builds  implicit  trust  based  on  the  interrelation  between  an  active  user  and  his/  her  friends  in  the  popular  social  microblogger  Twitter,  by  considering  aspects  such  as  retweet  actions  and  followers/followings  lists.  The  measured  trust  values  are  used  to  vote  for  friends'  opinions  held  in  the  posted  tweets  about  a  certain  product  such  as  movies.  The  higher  trust  parameters  to  a  friend  the  more  his/her  opinions  anticipate  in  recommendations  encounter.  Firstly,  Friends'  opinions  are  obtained  by  a  probabilistic  sentiment  analysis  technique  to  extract  the  opinions  in  form  of  multi-point  scale  of  ratings  from  short  tweets.  Secondly,  trust  relation  aspects  are  extracted  from  user's  friends  accounts.  Further,  a  genetic  algorithm  is  used  to  optimize  social  trust  parameters.  Thirdly,  this  paper  considers  the  Support  Vector  Regression  algorithm  (SVR)  to  predict  ratings  for  the  active  user.  Our  experimental  results  show  that  the  proposed  approach  outperforms  several  related  works  in  terms  of  accuracy  using  real  world  data  from  Twitter.  These  results  can  have  a  promising  effect  when  solving  new  users,  so  called  cold-start  problem,  to  the  systems  by  integrating  users'  OSNs.
0	A  temporal  difference  gng  based  approach  for  the  state  space  quantization  in  reinforcement  learning  environments.  The  main  issue  when  using  reinforcement  learning  algorithms  is  how  the  estimation  of  the  value  function  can  be  mapped  into  states.  In  very  few  cases  it  is  possible  to  use  tables  but  in  the  majority  of  cases,  the  number  of  states  either  can  be  too  large  to  be  kept  into  computer  memory  or  it  is  computationally  too  expensive  to  visit  all  states.  State  aggregation  models  like  the  self-organizing  maps  have  been  used  to  make  this  possible  by  generalizing  the  input  space  and  mapping  the  value  functions  into  the  states.  This  paper  proposes  a  new  algorithm  called  TD-GNG  that  uses  the  Growing  Neural  Gas  (GNG)  network  to  solve  reinforcement  learning  problems  by  providing  a  way  to  map  value  functions  into  states.  In  experimental  comparison  against  TD-AVQ  and  uniform  discretization  in  three  reinforcement  problems,  the  TD-GNG  showed  improvements  in  three  aspects,  namely,  1)  reduction  of  the  dimensionality  of  the  problem,  2)  increase  the  generalization  and  3)  reduction  of  the  convergence  time.  Experiments  have  also  show  that  TD-GNG  found  a  solution  using  less  memory  than  TD-AVQ  and  uniform  discretization  without  loosing  quality  in  the  policy  obtained.
0	Learning  to  speed  up  evolutionary  content  generation  in  physics  based  puzzle  games.  Procedural  content  generation  (PCG)  systems  are  designed  to  automatically  generate  content  for  video  games.  PCG  for  physics-based  puzzles  requires  one  to  simulate  the  game  to  ensure  feasibility  and  stability  of  the  objects  composing  the  puzzle.  The  major  drawback  of  this  simulation-based  approach  is  the  overall  running  time  of  the  PCG  process,  as  the  simulations  can  be  computationally  expensive.  This  paper  introduces  a  method  that  uses  machine  learning  to  reduce  the  number  of  simulations  performed  by  an  evolutionary  approach  while  generating  levels  of  Angry  Birds,  a  physics-based  puzzle  game.  Our  method  uses  classifiers  to  verify  the  stability  and  feasibility  of  the  levels  considered  during  search.  The  fitness  function  is  computed  only  for  levels  that  are  classified  as  stable  and  feasible.  An  approximation  of  the  fitness  that  does  not  require  simulations  is  used  for  levels  that  are  deemed  as  unstable  or  unfeasible  by  the  classifiers.  Our  experiments  show  that  naively  approximating  the  fitness  values  can  lead  to  poor  solutions.  We  then  introduce  an  approach  in  which  the  fitness  values  are  approximated  with  the  average  fitness  value  of  the  levels'  parents  added  to  a  penalty  value.  This  approximation  scheme  allows  the  search  procedure  to  find  good-quality  solutions  much  more  quickly  than  a  competing  approach—we  reduce  from  43  to  25  minutes  the  running  time  required  to  generate  one  level  of  Angry  Birds.
0	Using  state  predictions  for  value  regularization  in  curiosity  driven  deep  reinforcement  learning.  Learning  in  sparse  reward  settings  remains  a  challenge  in  Reinforcement  Learning,  which  is  often  addressed  by  using  intrinsic  rewards.  One  promising  strategy  is  inspired  by  human  curiosity,  requiring  the  agent  to  learn  to  predict  the  future.  In  this  paper  a  curiosity-driven  agent  is  extended  to  use  these  predictions  directly  for  training.  To  achieve  this,  the  agent  predicts  the  value  function  of  the  next  state  at  any  point  in  time.  Subsequently,  the  consistency  of  this  prediction  with  the  current  value  function  is  measured,  which  is  then  used  as  a  regularization  term  in  the  loss  function  of  the  algorithm.  Experiments  were  made  on  grid-world  environments  as  well  as  on  a  3D  navigation  task,  both  with  sparse  rewards.  In  the  first  case  the  extended  agent  is  able  to  learn  significantly  faster  than  the  baselines.
0	Efficient  reinforcement  learning  in  adversarial  games.  The  ability  of  learning  is  critical  for  agents  designed  to  compete  in  a  variety  of  two-player,  turn-taking,  tactical  adversarial  games,  such  as  Backgammon,  Othello/Reversi,  Chess,  Hex,  etc.  The  mainstream  approach  to  learning  in  such  games  consists  of  updating  some  state  evaluation  function  usually  in  a  Temporal  Difference  (TD)  sense  either  under  the  MiniMax  optimality  criterion  or  under  optimization  against  a  specific  opponent.  However,  this  approach  is  limited  by  several  factors:  (a)  updates  to  the  evaluation  function  are  incremental,  (b)  stored  samples  from  past  games  cannot  be  utilized,  and  (c)  the  quality  of  each  update  depends  on  the  current  evaluation  function  due  to  bootstrapping.  In  this  paper,  we  present  a  learning  approach  based  on  the  Least-Squares  Policy  Iteration  (LSPI)  algorithm  that  overcomes  these  limitations  by  focusing  on  learning  a  state-action  evaluation  function.  The  key  advantage  of  the  proposed  approach  is  that  the  agent  can  make  batch  updates  to  the  evaluation  function  with  any  collection  of  samples,  can  utilize  samples  from  past  games,  and  can  make  updates  that  do  not  depend  on  the  current  evaluation  function  since  there  is  no  bootstrapping.  We  demonstrate  the  efficiency  of  the  LSPI  agent  over  the  TD  agent  in  the  classical  board  game  of  Othello/Reversi.
0	Segmentation  and  measurement  of  chronic  wounds  for  bioprinting.  Objective:  to  provide  a  proof-of-concept  tool  for  segmenting  chronic  wounds  and  transmitting  the  results  as  instructions  and  coordinates  to  a  bioprinter  robot  and  thus  facilitate  the  treatment  of  chronic  wounds.  Methods:  several  segmentation  methods  used  for  measuring  wound  geometry,  including  edge-detection  and  morphological  operations,  region-growing,  Livewire,  active  contours,  and  texture  segmentation,  were  compared  on  26  images  from  15  subjects.  Ground-truth  wound  delineations  were  generated  by  a  dermatologist.  The  wound  coordinates  were  converted  into  G-code  understandable  by  the  bioprinting  robot.  Due  to  its  desirable  properties,  alginate  hydrogel  was  synthesized  by  dissolving  16%  (w/v)  sodium-alginate  and  4%  (w/v)  gelatin  in  deionized  water  and  used  for  cell  encapsulation.  Results:  Livewire  achieved  the  best  performance,  with  minimal  user  interaction:  97.08%,  99.68%  96.67%,  96.22,  98.15,  and  32.26,  mean  values,  respectively,  for  accuracy,  sensitivity,  specificity,  Jaccard  index,  Dice  similarity  coefficient,  and  Hausdorff  distance.  The  bioprinter  robot  was  able  to  print  skin  cells  on  the  surface  of  skin  with  a  95.56%  similarity  between  the  bioprinted  patch's  dimensions  and  the  desired  wound  geometry.  Conclusion:  we  have  designed  a  novel  approach  for  the  healing  of  chronic  wounds,  based  on  semiautomatic  segmentation  of  wound  images,  improving  clinicians’  control  of  the  bioprinting  process  through  more  accurate  coordinates.  Significance:  this  study  is  the  first  to  perform  wound  bioprinting  based  on  image  segmentation.  It  also  compares  several  segmentation  methods  used  for  this  purpose  to  determine  the  best.
0	A  sequence  of  relaxations  constraining  hidden  variable  models.  Many  widely  studied  graphical  models  with  latent  variables  lead  to  nontrivial  constraints  on  the  distribution  of  the  observed  variables.  Inspired  by  the  Bell  inequalities  in  quantum  mechanics,  we  refer  to  any  linear  inequality  whose  violation  rules  out  some  latent  variable  model  as  a  "hidden  variable  test"  for  that  model.  Our  main  contribution  is  to  introduce  a  sequence  of  relaxations  which  provides  progressively  tighter  hidden  variable  tests.  We  demonstrate  applicability  to  mixtures  of  sequences  of  i.i.d.  variables,  Bell  inequalities,  and  homophily  models  in  social  networks.  For  the  last,  we  demonstrate  that  our  method  provides  a  test  that  is  able  to  rule  out  latent  homophily  as  the  sole  explanation  for  correlations  on  a  real  social  network  that  are  known  to  be  due  to  influence.
0	Sylvester  normalizing  flows  for  variational  inference.  Variational  inference  relies  on  flexible  approximate  posterior  distributions.  Normalizing  flows  provide  a  general  recipe  to  construct  flexible  variational  posteriors.  We  introduce  Sylvester  normalizing  flows,  which  can  be  seen  as  a  generalization  of  planar  flows.  Sylvester  normalizing  flows  remove  the  well-known  single-unit  bottleneck  from  planar  flows,  making  a  single  transformation  much  more  flexible.  We  compare  the  performance  of  Sylvester  normalizing  flows  against  planar  flows  and  inverse  autoregressive  flows  and  demonstrate  that  they  compare  favorably  on  several  datasets.
0	Constructing  separators  and  adjustment  sets  in  ancestral  graphs.  Ancestral  graphs  (AGs)  are  graphical  causal  models  that  can  represent  uncertainty  about  the  presence  of  latent  confounders,  and  can  be  inferred  from  data.  Here,  we  present  an  algorithmic  framework  for  efficiently  testing,  constructing,  and  enumerating  m-separators  in  AGs.  Moreover,  we  present  a  new  constructive  criterion  for  covariate  adjustment  in  directed  acyclic  graphs  (DAGs)  and  maximal  ancestral  graphs  (MAGs)  that  characterizes  adjustment  sets  as  m-separators  in  a  subgraph.  Jointly,  these  results  allow  to  find  all  adjustment  sets  that  can  identify  a  desired  causal  effect  with  multivariate  exposures  and  outcomes  in  the  presence  of  latent  confounding.  Our  results  generalize  and  improve  upon  several  existing  solutions  for  special  cases  of  these  problems.
0	Unsupervised  multi  view  nonlinear  graph  embedding.  In  this  paper,  we  study  the  unsupervised  multi-view  graph  embedding  (UMGE)  problem,  which  aims  to  learn  graph  embedding  from  multiple  perspectives  in  an  unsupervised  manner.  However,  the  vast  majority  of  multi-view  learning  work  focuses  on  non-graph  data,  and  surprisingly  there  are  limited  work  on  UMGE.  By  systematically  analyzing  different  existing  methods  for  UMGE,  we  discover  that  cross-view  and  nonlinearity  play  a  vital  role  in  efficiently  improving  graph  embedding  quality.  Motivated  by  this  concept,  we  develop  an  unsupervised  Multi-viEw  nonlineaR  Graph  Embedding  (MERGE)  approach  to  model  relational  multi-view  consistency.  Experimental  results  on  five  benchmark  datasets  demonstrate  that  MERGE  significantly  outperforms  the  state-of-the-art  baselines  in  terms  of  accuracy  in  node  classification  tasks  without  sacrificing  the  computational  efficiency.
0	Learning  monocular  3d  vehicle  detection  without  3d  bounding  box  labels.  The  training  of  deep-learning-based  3D  object  detectors  requires  large  datasets  with  3D  bounding  box  labels  for  supervision  that  have  to  be  generated  by  hand-labeling.  We  propose  a  network  architecture  and  training  procedure  for  learning  monocular  3D  object  detection  without  3D  bounding  box  labels.  By  representing  the  objects  as  triangular  meshes  and  employing  differentiable  shape  rendering,  we  define  loss  functions  based  on  depth  maps,  segmentation  masks,  and  ego-  and  object-motion,  which  are  generated  by  pre-trained,  off-the-shelf  networks.  We  evaluate  the  proposed  algorithm  on  the  real-world  KITTI  dataset  and  achieve  promising  performance  in  comparison  to  state-of-the-art  methods  requiring  3D  bounding  box  labels  for  training  and  superior  performance  to  conventional  baseline  methods.
0	Probabilistic  program  abstractions.  Abstraction  is  a  fundamental  tool  for  reasoning  about  a  complex  system.  Program  abstraction  has  been  utilized  to  great  effect  for  analyzing  deterministic  programs.  At  the  heart  of  a  program  abstraction  is  a  connection  between  the  abstract  program,  which  is  simple  to  analyze,  and  the  concrete  program,  which  may  be  extremely  complex.  Program  abstractions,  however,  are  typically  not  probabilistic.  In  this  thesis  I  generalize  a  particular  class  of  non-  deterministic  program  abstractions  known  as  sound  over-approximations  to  the  probabilistic  context.  Sound  over-approximations  are  a  family  of  abstract  programs  which  are  guaranteed  to  contain  the  original  program  as  a  subset  of  their  behavior.  This  thesis  shows  that  when  imbued  with  a  probabilistic  semantics,  sound  over-approximations  define  a  family  of  probabilistic  programs  which  capture  key  properties  of  the  original  program.  It  then  introduces  a  mechanism  for  generating  sound  probabilistic  over-approximations  as  a  generalization  of  a  well-known  program  abstraction  technique  known  as  predicate  abstraction.  Finally,  the  problem  of  inference  and  learning  in  the  context  of  probabilistic  program  abstractions  are  briefly  described.
0	Gas  bubble  shape  measurement  and  analysis.  This  work  focuses  on  the  precise  quantification  of  bubble  streams  from  underwater  gas  seeps.  The  performance  of  the  snake  based  method  and  of  ellipse  fitting  with  the  CMA-ES  non-linear  optimization  algorithm  is  evaluated.  A  novel  improved  snake  based  method  is  presented  and  the  optimal  choice  of  snake  parameters  is  studied.  A  Kalman  filter  is  used  for  bubble  tracking.  The  deviation  between  the  measured  flux  and  a  calibrated  flux  meter  is  4  %  for  small  and  9  %  for  larger  bubbles.  This  work  will  allow  a  better  data  gathering  on  marine  gas  seeps  for  future  climatology  and  marine  research.
0	Deep  archetypal  analysis.  Deep  Archetypal  Analysis  (DeepAA)  generates  latent  representations  of  high-dimensional  datasets  in  terms  of  intuitively  understandable  basic  entities  called  archetypes.  The  proposed  method  extends  linear  Archetypal  Analysis  (AA),  an  unsupervised  method  to  represent  multivariate  data  points  as  convex  combinations  of  extremal  data  points.  Unlike  the  original  formulation,  Deep  AA  is  generative  and  capable  of  handling  side  information.  In  addition,  our  model  provides  the  ability  for  data-driven  representation  learning  which  reduces  the  dependence  on  expert  knowledge.  We  empirically  demonstrate  the  applicability  of  our  approach  by  exploring  the  chemical  space  of  small  organic  molecules.  In  doing  so,  we  employ  the  archetype  constraint  to  learn  two  different  latent  archetype  representations  for  the  same  dataset,  with  respect  to  two  chemical  properties.  This  type  of  supervised  exploration  marks  a  distinct  starting  point  and  let  us  steer  de  novo  molecular  design.
0	Arc  hybrid  non  projective  dependency  parsing  with  a  static  dynamic  oracle.  We  extend  the  arc-hybrid  transition  system  for  dependency  parsing  with  a  SWAP  transition  that  enables  reordering  of  the  words  and  construction  of  non-projective  trees.  Although  this  extension  poten  ...
0	Parallel  implementation  of  the  wu  manber  algorithm  using  the  opencl  framework.  One  of  the  most  significant  issues  of  the  computational  biology  is  the  multiple  pattern  matching  for  locating  nucleotides  and  amino  acid  sequence  patterns  into  biological  databases.  Sequential  implementations  for  these  processes  have  become  inadequate,  due  to  an  increasing  demand  for  more  computational  power.  Graphic  cards  offer  a  high  parallelism  computational  power  improving  the  performance  of  applications.  This  paper  evaluates  the  performance  of  the  Wu-Manber  algorithm  implemented  with  the  OpenCL  framework,  by  presenting  the  running  time  of  the  experiments  compared  with  the  corresponding  sequential  time.
0	Discourse  structure  interacts  with  reference  but  not  syntax  in  neural  language  models.  Language  models  (LMs)  trained  on  large  quantities  of  text  have  been  claimed  to  acquire  abstract  linguistic  representations.  Our  work  tests  the  robustness  of  these  abstractions  by  focusing  on  the  ability  of  LMs  to  learn  interactions  between  different  linguistic  representations.  In  particular,  we  utilized  stimuli  from  psycholinguistic  studies  showing  that  humans  can  condition  reference  (i.e.  coreference  resolution)  and  syntactic  processing  on  the  same  discourse  structure  (implicit  causality).  We  compared  both  transformer  and  long  short-term  memory  LMs  to  find  that,  contrary  to  humans,  implicit  causality  only  influences  LM  behavior  for  reference,  not  syntax,  despite  model  representations  that  encode  the  necessary  discourse  information.  Our  results  further  suggest  that  LM  behavior  can  contradict  not  only  learned  representations  of  discourse  but  also  syntactic  agreement,  pointing  to  shortcomings  of  standard  language  modeling.
0	Siamese  lstm  with  convolutional  similarity  for  similar  question  retrieval.  In  this  paper,  we  model  the  similar  question  retrieval  task  as  a  binary  classification  problem.  We  propose  a  novel  approach  of  “ID-Siamese  LSTM  for  cQA  (1D-SLcQA)”  to  find  the  semantic  similarity  between  a  new  question  and  existing  question(s).  In  1D-SLcQA,  we  use  a  combination  of  twin  LSTM  networks  and  a  contrastive  loss  function  to  effectively  memorize  the  long  term  dependencies  i.e.,  capture  semantic  similarity  even  when  the  length  of  the  answers/questions  is  very  large  (200  words).  The  similarity  of  the  questions  is  modeled  using  a  single  network  with  (1D)  (feature)  convolution  between  feature  vectors  learned  from  twin  LSTM  layers.  Experiments  on  large  scale  real  world  Yahoo  Answers  dataset  show  that  1D-SLcQA  outperform  the  state  of  the  art  approach  of  Siamese  cQA  approach(SCQA).
0	A  library  to  run  evolutionary  algorithms  in  the  cloud  using  mapreduce.  We  discuss  ongoing  development  of  an  evolutionary  algorithm  library  to  run  on  the  cloud.  We  relate  how  we  have  used  the  Hadoop  open-source  MapReduce  distributed  data  processing  framework  to  implement  a  single  "island"  with  a  potentially  very  large  population.  The  design  generalizes  beyond  the  current,  one-off  kind  of  MapReduce  implementations.  It  is  in  preparation  for  the  library  becoming  a  modeling  or  optimization  service  in  a  service  oriented  architecture  or  a  development  tool  for  designing  new  evolutionary  algorithms.
0	I  like  your  shirt  dialogue  acts  for  enabling  social  talk  in  conversational  agents.  This  paper  presents  a  set  of  dialogue  acts  which  can  be  used  to  implement  small  talk  conversations  in  conversational  agents.  Although  many  conversational  agents  are  supposed  to  engage  in  small  talk,  no  systematic  development  of  social  dialogue  acts  and  sequences  for  dialogue  systems  was  made  so  far.  Instead  systems  reuse  the  same  conversation  sequence  every  time  they  engage  in  small  talk  or  integrate  stateless  chatbots  without  any  knowledge  about  the  ongoing  conversation.  The  small  talk  dialogue  act  taxonomy  presented  in  this  paper  consists  of  functionally  motivated  acts  inspired  by  the  social  science  work  of  "face".  Moreover,  a  corpus  annotation  and  possible  dialogue  act  sequences  extracted  from  this  annotated  corpus  are  described.  The  dialogue  act  set  and  the  sequences  are  used  in  our  dialogue  system  to  provide  a  more  knowledgedriven  treatment  of  small  talk  than  chatbots  can  offer.
0	Influence  of  individual  differences  when  training  public  speaking  with  virtual  audiences.  Multimodal  interaction  technologies  have  enabled  new  applications  for  training  interpersonal  skills  such  as  public  speaking.  Various  training  paradigms  have  been  proposed,  most  of  them  relying  on  some  form  of  graphical  feedback  provided  to  the  trainee  in  real-time  during  their  training  or  after  training  using  an  after-action  review  tool.  Another  paradigm  consists  of  using  virtual  characters  to  provide  feedback  through  their  behavior  during  simulated  social  interactions.  Preliminary  studies  have  started  to  explore  the  effectiveness  of  these  different  training  paradigms;  however,  these  have  not  investigated  the  impact  of  individual  differences  on  which  interaction  paradigm  is  more  efficient  or  motivating  for  different  populations  of  users.  In  this  article,  we  explore  the  impact  of  personality,  public  speaking  anxiety,  and  immersive  tendencies  on  the  experiences  of  users  training  public  speaking  with  an  interactive  virtual  audience  system  providing  realtime  feedback  through  virtual  audience  behavior  as  well  as  delayed  feedback  with  an  after-action  review  tool.  We  found  that  these  three  factors  impacted  different  output  measures  of  user  experience  and  user  ratings  of  the  system's  quality.
0	Planning  motions  for  virtual  demonstrators.  In  order  to  deliver  information  effectively,  virtual  human  demonstrators  must  be  able  to  address  complex  spatial  constraints  and  at  the  same  time  replicate  motion  coordination  patterns  observed  in  human-human  interactions.  We  introduce  in  this  paper  a  whole-body  motion  planning  and  synthesis  framework  that  coordinates  locomotion,  body  positioning,  action  execution  and  gaze  behavior  for  generic  demonstration  tasks  among  obstacles.
0	The  effect  of  virtual  agents  emotion  displays  and  appraisals  on  people  s  decision  making  in  negotiation.  There  is  growing  evidence  that  emotion  displays  can  impact  people's  decision  making  in  negotiation.  However,  despite  increasing  interest  in  AI  and  HCI  on  negotiation  as  a  means  to  resolve  differences  between  humans  and  agents,  emotion  has  been  largely  ignored.  We  explore  how  emotion  displays  in  virtual  agents  impact  people's  decision  making  in  human-agent  negotiation.  This  paper  presents  an  experiment  (N=204)  that  studies  the  effects  of  virtual  agents'  displays  of  joy,  sadness,  anger  and  guilt  on  people's  decision  to  counteroffer,  accept  or  drop  out  from  the  negotiation,  as  well  as  on  people's  expectations  about  the  agents'  decisions.  The  paper  also  presents  evidence  for  a  mechanism  underlying  such  effects  based  on  appraisal  theories  of  emotion  whereby  people  retrieve,  from  emotion  displays,  information  about  how  the  agent  is  appraising  the  ongoing  interaction  and,  from  this  information,  infer  about  the  agent's  intentions  and  reach  decisions  themselves.  We  discuss  implications  for  the  design  of  intelligent  virtual  agents  that  can  negotiate  effectively.
0	Design  intention  inference  for  virtual  co  design  agents.  We  address  the  challenge  of  inferring  the  design  intentions  of  a  human  by  an  intelligent  virtual  agent  that  collaborates  with  the  human.  First,  we  propose  a  dynamic  Bayesian  network  model  that  relates  design  intentions,  objectives,  and  solutions  during  a  human's  exploration  of  a  problem  space.  We  then  train  the  model  on  design  behaviors  generated  by  a  search  agent  and  use  the  model  parameters  to  infer  the  design  intentions  in  a  test  set  of  real  human  behaviors.  We  find  that  our  model  is  able  to  infer  the  exact  intentions  across  three  objectives  associated  with  a  sequence  of  design  outcomes  31.3%  of  the  time.  Inference  accuracy  is  50.9%  for  the  top  two  predictions  and  67.2%  for  the  top  three  predictions.  For  any  singular  intention  over  an  objective,  the  model's  mean  F1-score  is  0.719.  This  provides  a  reasonable  foundation  for  an  intelligent  virtual  agent  to  infer  design  intentions  purely  from  design  outcomes  toward  establishing  joint  intentions  with  a  human  designer.  These  results  also  shed  light  on  the  potential  benefits  and  pitfalls  in  using  simulated  data  to  train  a  model  for  human  design  intentions.
0	The  virtual  apprentice.  Over  the  past  couple  of  decades,  virtual  humans  have  been  attracting  more  and  more  attention.  Many  applications  including,  video  games,  movies,  and  various  training  and  tutoring  systems  have  benefited  from  work  in  this  area.  While  the  visual  quality  of  virtual  agents  has  improved  dramatically,  their  intelligence  and  socialization  still  needs  improvement.  In  this  paper,  we  present  work  towards  endowing  agents  with  social  roles  and  exploiting  Explanation-Based  Learning  (EBL)  to  enable  them  to  acquire  additional,  contextual  behaviors  from  other  agents.  These  virtual  humans  are  capable  of  learning  and  applying  role  related  actions  from  multiple  agents  and  only  adopt  behaviors  that  have  been  explained  to  them,  meaning  that  their  definition  of  a  role  may  be  a  subset  from  one  or  more  agents.  This  results  in  emergent  behaviors  in  heterogeneous  populations.
0	An  empirical  evaluation  of  translational  and  rotational  invariance  of  descriptors  and  the  classification  of  flower  dataset.  Object  recognition  and  identification  is  used  in  the  development  of  automatic  systems  in  various  domains.  Latest  research  indicates  that  the  performance  of  such  systems  depend  on  the  efficiency  in  feature  extraction;  robust  feature  description  and  optimized  classification  or  matching.  This  paper  presents  an  empirical  evaluation  of  efficiency  and  robustness  of  various  gradient  and  binary  descriptors  with  respect  to  translation,  rotation  and  scaling  etc.  The  performance  of  each  descriptor  is  evaluated  against  the  parameters  such  as  size  of  feature  set  in  terms  of  number  of  keypoints,  matching  accuracy  and  execution  time.  The  detailed  experiments  were  conducted  on  17  category  Oxford  flower  dataset  to  evaluate  the  robustness  of  descriptors  against  various  rotations,  scaling  and  noise  using  precision  and  recall  values.  Experimental  results  shows  that  the  PCA-SIFT  and  SURF  gives  less  matching  rate  but  faster  as  compared  to  SIFT  due  to  reduction  in  dimension  in  PCA-SIFT  and  use  of  integral  images  in  SURF.  ORB  gives  the  best  classification  and  outperforms  the  other  descriptors  with  less  memory  requirement  and  is  compact  in  size.
0	Understanding  temporal  structure  for  video  captioning.  Recent  research  in  convolutional  and  recurrent  neural  networks  has  fueled  incredible  advances  in  video  understanding.  We  propose  a  video  captioning  framework  that  achieves  the  performance  and  quality  necessary  to  be  deployed  in  distributed  surveillance  systems.  Our  method  combines  an  efficient  hierarchical  architecture  with  novel  attention  mechanisms  at  both  the  local  and  global  levels.  By  shifting  focus  to  different  spatiotemporal  locations,  attention  mechanisms  correlate  sequential  outputs  with  activation  maps,  offering  a  clever  way  to  adaptively  combine  multiple  frames  and  locations  of  video.  As  soft  attention  mixing  weights  are  solved  via  back-propagation,  the  number  of  weights  or  input  frames  needs  to  be  known  in  advance.  To  remove  this  restriction,  our  video  understanding  framework  combines  continuous  attention  mechanisms  over  a  family  of  Gaussian  distributions.  Our  efficient  multistream  hierarchical  model  combines  a  recurrent  architecture  with  a  soft  hierarchy  layer  using  both  equally  spaced  and  dynamically  localized  boundary  cuts.  As  opposed  to  costly  volumetric  attention  approaches,  we  use  video  attributes  to  steer  temporal  attention.  Our  fully  learnable  end-to-end  approach  helps  predict  salient  temporal  regions  of  action/objects  in  the  video.  We  demonstrate  state-of-the-art  captioning  results  on  the  popular  MSVD,  MSR-VTT  and  M-VAD  video  datasets  and  compare  several  variants  of  the  algorithm  suitable  for  real-time  applications.  By  adjusting  the  frame  rate,  we  show  a  single  computer  can  generate  effective  video  captions  for  100  simultaneous  cameras.  We  additionally  perform  studies  to  show  how  bit  rate  compression  modifies  captioning  results.
0	Myvisitplanner  gr  personalized  itinerary  planning  system  for  tourism.  This  application  paper  presents  myVisitPlanner  GR,  an  intelligent  web-based  system  aiming  at  making  recommendations  that  help  visitors  and  residents  of  the  region  of  Northern  Greece  to  plan  their  leisure,  cultural  and  other  activities  during  their  stay  in  this  area.  The  system  encompasses  a  rich  ontology  of  activities,  categorized  across  dimensions  such  as  activity  type,  historical  era,  user  profile  and  age  group.  Each  activity  is  characterized  by  attributes  describing  its  location,  cost,  availability  and  duration  range.  The  system  makes  activity  recommendations  based  on  user-selected  criteria,  such  as  visit  duration  and  timing,  geographical  areas  of  interest  and  visit  profiling.  The  user  edits  the  proposed  list  and  the  system  creates  a  plan,  taking  into  account  temporal  and  geographical  constraints  imposed  by  the  selected  activities,  as  well  as  by  other  events  in  the  user’s  calendar.  The  user  may  edit  the  proposed  plan  or  request  alternative  plans.  A  recommendation  engine  employs  non-intrusive  machine  learning  techniques  to  dynamically  infer  and  update  the  user’s  profile,  concerning  his  preferences  for  both  activities  and  resulting  plans,  while  taking  privacy  concerns  into  account.  The  system  is  coupled  with  a  module  to  semi-automatically  feed  its  database  with  new  activities  in  the  area.
0	Identification  of  different  types  of  minority  class  examples  in  imbalanced  data.  The  characteristics  of  the  minority  class  distribution  in  imbalanced  data  is  studied.  Four  types  of  minority  examples  ---  safe,  borderline,  rare  and  outlier  ---  are  distinguished  and  analysed.  We  propose  a  new  method  for  identification  of  these  examples  in  the  data,  based  on  analysing  the  local  neighbourhoods  of  examples.  Its  application  to  UCI  imbalanced  datasets  shows  that  the  minority  class  is  often  scattered  without  too  many  safe  examples.  This  characteristics  of  data  distributions  is  also  confirmed  by  another  analysis  with  Multidimensional  Scaling  visualization.  We  examine  the  influence  of  these  types  of  examples  on  6  different  classifiers  learned  over  various  real-world  datasets.  Results  of  experiments  show  that  the  particular  classifiers  reveal  different  sensitivity  to  the  type  of  examples.
0	Solving  nurikabe  with  ant  colony  optimization.  We  present  the  first  nature-inspired  algorithm  for  the  NP-complete  Nurikabe  pencil  puzzle.  Our  method,  based  on  Ant  Colony  Optimization  (ACO),  offers  competitive  performance  with  a  direct  logic-based  solver,  with  improved  run-time  performance  on  smaller  instances,  but  poorer  performance  on  large  instances.  Importantly,  our  algorithm  is  "problem  agnostic",  and  requires  no  heuristic  information.  This  suggests  the  possibility  of  a  generic  ACO-based  framework  for  the  efficient  solution  of  a  wide  range  of  similar  logic  puzzles  and  games.  We  further  suggest  that  Nurikabe  may  provide  a  challenging  benchmark  for  nature-inspired  optimization.
0	A  multiset  genetic  algorithm  for  the  optimization  of  deceptive  problems.  MuGA  is  an  evolutionary  algorithm  (EA)  that  represents  populations  as  multisets,  instead  of  the  conventional  collection.  Such  representation  can  be  explored  to  adapt  genetic  operators  in  order  to  increase  performance  in  difficult  problems.  In  this  paper  we  present  an  adaptation  of  the  mutation  operator,  multiset  wave  mutation  (MWM),  that  explores  the  multiset  representation  to  apply  different  mutation  ratios  to  the  same  chromosome,  and  an  adaptation  of  the  replacement  operator,  multiset  decimation  replacement  (MDR)  that  preserves  multiset  representation  in  the  main  population  and  helps  MuGA  to  solve  hard  deceptive  problems.  Results  obtained  in  different  deceptive  functions  show  that  pairing  both  operators  is  a  robust  approach  with  a  high  success  ratio  in  most  of  the  problems.
0	Parameter  less  local  optimizer  with  linkage  identification  for  deterministic  order  k  decomposable  problems.  A  simple  parameter-less  local  optimizer  able  to  solve  deterministic  problems  with  building  blocks  of  bounded  order  is  proposed  in  this  article.  The  algorithm  is  able  to  learn  and  use  linkage  information  during  the  run.  The  algorithm  is  algorithmically  simple,  easy  to  implement  and  with  the  exception  of  termination  condition,  it  is  completely  parameter-free---there  is  thus  no  need  to  tune  the  population  size  and  other  parameters  to  the  problem  at  hand.  An  empirical  comparison  on  3  decomposable  functions,  each  with  uniformly  scaled  building  blocks  of  size  5  and  8,  was  carried  out.  The  algorithm  exhibits  quadratic  scaling  with  the  problem  dimensionality,  but  the  comparison  with  the  extended  compact  genetic  algorithm  and  Bayesian  optimization  algorithm  shows  that  it  needs  lower  or  comparable  number  of  fitness  function  evaluations  on  the  majority  of  functions  for  the  tested  problem  dimensionalities.  The  results  also  suggest  that  the  efficiency  of  the  local  optimizer  compared  to  both  the  estimation-of-distribution  algorithms  should  be  better  for  problem  sizes  under  at  least  a  few  hundreds  of  bits.
0	A  dispersion  operator  for  geometric  semantic  genetic  programming.  Recent  advances  in  geometric  semantic  genetic  programming  (GSGP)  have  shown  that  the  results  obtained  by  these  methods  can  outperform  those  obtained  by  classical  genetic  programming  algorithms,  in  particular  in  the  context  of  symbolic  regression.  However,  there  are  still  many  open  issues  on  how  to  improve  their  search  mechanism.  One  of  these  issues  is  how  to  get  around  the  fact  that  the  GSGP  crossover  operator  cannot  generate  solutions  that  are  placed  outside  the  convex  hull  formed  by  the  individuals  of  the  current  population.  Although  the  mutation  operator  alleviates  this  problem,  we  cannot  guarantee  it  will  find  promising  regions  of  the  search  space  within  feasible  computational  time.  In  this  direction,  this  paper  proposes  a  new  geometric  dispersion  operator  that  uses  multiplicative  factors  to  move  individuals  to  less  dense  areas  of  the  search  space  around  the  target  solution  before  applying  semantic  genetic  operators.  Experiments  in  sixteen  datasets  show  that  the  results  obtained  by  the  proposed  operator  are  statistically  significantly  better  than  those  produced  by  GSGP  and  that  the  operator  does  indeed  spread  the  solutions  around  the  target  solution.
0	Characterisation  of  movement  disorder  in  parkinson  s  disease  using  evolutionary  algorithms.  Parkinson's  Disease  is  a  devastating  illness  with  no  currently  available  cure.  As  the  population  ages,  the  disease  becomes  more  common  with  a  large  financial  cost  to  society.  A  rapid  and  accurate  diagnosis,  as  well  as  practical  monitoring  methods  are  essential  for  managing  the  disease  as  best  as  possible.  This  paper  discusses  two  approaches  to  discriminating  movement  data  between  healthy  controls  or  Parkinson's  Disease  patients.  One  is  a  standard  statistical  analysis,  influenced  by  prior  work  into  classifying  patients.  The  other  is  a  programmatic  expression  evolved  using  genetic  programming,  which  is  trained  to  observe  differences  in  specific  motion  segments,  rather  than  using  arbitrary  windows  of  a  full  data  series.  The  performance  of  the  statistical  analysis  method  is  relatively  high,  but  it  still  cannot  discriminate  as  well  as  the  evolved  classifier.  This  study  compares  favourably  to  previous  work,  highlighting  the  usefulness  of  analysing  a  successful  classifier  to  influence  design  decisions  for  future  work.  Examination  of  the  evolved  programmatic  expressions  that  had  high  discriminatory  ability  provided  useful  insight  into  how  Parkinson's  Disease  patients  and  healthy  subjects  have  differing  movement  characteristics.  This  could  be  used  to  inform  future  research  into  the  physiology  of  repetitive  motions  in  Parkinson's  Disease  patients.
0	Computation  of  the  improvement  directions  of  the  pareto  front  and  its  application  to  moeas.  This  paper  introduces  the  mathematical  development  and  algorithm  of  the  Improvement-Directions  Mapping  (IDM)  method,  which  computes  improvement  directions  to  "push"  the  current  solutions  toward  the  true  Pareto  front.  The  main  idea  is  to  compute  normal  vectors  to  the  front,  as  improvement  directions  in  the  objective  space,  to  be  then  transformed  into  search  directions  in  the  variable  space  through  a  transformation  tensor.  The  main  contributions  of  the  IDM  as  a  local  search  operator  versus  previous  approaches  are  the  following:  1)  It  does  not  require  of  a  priori  information  about  improvement  directions  or  location  of  the  true  Pareto  front,  2)  It  uses  a  local  quadratic  approximation  of  the  Pareto  front  to  compute  the  transformation  tensor,  thus,  reducing  numerical  problems  and  avoiding  abrupt  changes  in  the  search  direction  which  could  lead  to  erratic  searches.  These  features  allow  the  IDM  to  be  implemented  as  a  local  search  operator  within  any  Multi-objective  Evolutionary  Algorithm  (MOEA).  The  potential  of  the  IDM  is  shown  by  hybridizing  two  well-known  multi-objective  algorithms:  a)  MOEA/D  +  IDM;  b)  NSGA-II  +  IDM.  In  the  first  approach,  IDM  "pushes"  the  offspring  population  in  each  iteration.  A  similar  experiment  is  performed  with  the  second  approach.  Furthermore,  one  more  experiment  evaluates  the  IDM  as  a  refinement  step  that  is  applied  to  the  last  Pareto  front  delivered  by  NSGA-II.
0	Human  activity  recognition  and  feature  selection  for  stroke  early  diagnosis.  Human  Activity  Recognition  (HAR)  refers  to  the  techniques  for  detecting  what  a  subject  is  currently  doing.  A  wide  variety  of  techniques  have  been  designed  and  applied  in  ambient  intelligence  -related  with  comfort  issues  in  home  automation-  and  in  Ambient  Assisted  Living  (AAL)  -related  with  the  health  care  of  elderly  people.  In  this  study,  we  focus  on  the  diagnosing  of  an  illness  that  requires  estimating  the  activity  of  the  subject.  In  a  previous  study,  we  adapted  a  well-known  HAR  technique  to  use  accelerometers  in  the  dominant  wrist.  This  study  goes  one  step  further,  firstly  analyzing  the  different  variables  that  have  been  reported  in  HAR,  then  evaluating  those  of  higher  relevance  and  finally  performing  a  wrapper  feature  selection  method.  The  main  contribution  of  this  study  is  the  best  adaptation  of  the  chosen  technique  for  estimating  the  current  activity  of  the  individual.  The  obtained  results  are  expected  to  be  included  in  a  specific  device  for  early  stroke  diagnosing.
0	Multi  dimensional  pattern  discovery  in  financial  time  series  using  sax  ga  with  extended  robustness.  This  paper  proposes  a  new  Multi-Dimensional  SAX-GA  approach  to  pattern  discovery  using  genetic  algorithms  (GA).  The  approach  is  capable  of  discovering  patterns  in  multi-dimensional  financial  time  series.  First,  the  several  dimensions  of  data  are  converted  to  a  Symbolic  Aggregate  approXimation  (SAX)  representation,  which  is,  then,  feed  to  a  GA  optimization  kernel.  The  GA  searches  for  profitable  patterns  occurring  simultaneously  in  the  multi-dimensional  time  series.  Based  on  the  patterns  found,  the  GA  produces  more  robust  investment  strategies,  since  the  simultaneity  of  patterns  on  different  dimensions  of  the  data,  reinforces  the  strength  of  the  trading  decisions  implemented.  The  proposed  approach  was  tested  using  stocks  from  S&P500  index,  and  is  compared  to  previous  reference  works  of  SAX-GA  and  to  the  Buy  &  Hold  (B&H)  classic  investment  strategy.
0	The  multi  objective  real  valued  gene  pool  optimal  mixing  evolutionary  algorithm.  The  recently  introduced  Multi-Objective  Gene-pool  Optimal  Mixing  Evolutionary  Algorithm  (MO-GOMEA)  exhibits  excellent  scalability  in  solving  a  wide  range  of  challenging  discrete  multi-objective  optimization  problems.  In  this  paper,  we  address  scalability  issues  in  solving  multi-objective  optimization  problems  with  continuous  variables  by  introducing  the  Multi-Objective  Real-Valued  GOMEA  (MO-RV-GOMEA),  which  combines  MO-GOMEA  with  aspects  of  the  multi-objective  estimation-of-distribution  algorithm  known  as  MAMaLGaM.  MO-RV-GOMEA  exploits  linkage  structure  in  optimization  problems  by  performing  distribution  estimation,  adaptation,  and  sampling  as  well  as  solution  mixing  based  on  an  explicitly-defined  linkage  model.  Such  a  linkage  model  can  be  defined  a  priori  when  some  problem-specific  knowledge  is  available,  or  it  can  be  learned  from  the  population.  The  scalability  of  MO-RV-GOMEA  using  different  linkage  models  is  compared  to  the  state-of-the-art  multi-objective  evolutionary  algorithms  NSGA-II  and  MAMaLGaM  on  a  wide  range  of  benchmark  problems.  MO-RV-GOMEA  is  found  to  retain  the  excellent  scalability  of  MO-GOMEA  through  the  successful  exploitation  of  linkage  structure,  scaling  substantially  better  than  NSGA-II  and  MAMaLGaM.  This  scalability  is  even  further  improved  when  partial  evaluations  are  possible,  achieving  strongly  sub-linear  scalability  in  terms  of  the  number  of  evaluations.
0	Designing  and  modeling  a  browser  based  distributedevolutionary  computation  system.  Web  browsers  have  scaled  from  simple  page-rendering  engines  to  operating  systems  that  include  most  services  the  lower  OS  layer  has,  with  the  added  facility  that  applications  can  be  run  by  just  visiting  a  web  page.  In  this  paper  we  will  describe  the  front  and  back  end  of  a  distributed  evolutionary  computation  system  that  uses  the  browser's  capabilities  of  running  programs  written  in  JavaScript.  We  will  focus  on  two  different  aspects  of  volunteer  computing:  first,  the  pragmatic:  where  to  find  those  resources,  which  ones  can  be  used,  what  kind  of  support  you  have  to  give  them;  and  then,  the  theoretical:  how  evolutionary  algorithms  can  be  adapted  to  an  environment  in  which  nodes  come  and  go,  have  different  computing  capabilities  and  operate  in  complete  asynchrony  of  each  other.  We  will  examine  the  setup  needed  to  create  a  simple  distributed  evolutionary  algorithm  using  JavaScript,  with  the  intention  of  eventually  finding  a  model  of  how  users  react  to  it  by  collecting  data  from  several  experiments  featuring  a  classical  benchmark  function.
0	A  gaussian  groundplan  projection  area  model  for  evolving  probabilistic  classifiers.  In  this  paper,  an  investigation  of  evolvable  probabilistic  classifiers  is  conducted,  along  with  a  thorough  comparison  between  a  classical  Gaussian  distance  model,  and  the  induction  of  Gaussian-to-circle  projection  model.  The  newly  introduced  model  refers  to  a  distance  fitness  measure,  based  on  the  projection  of  Gaussian  distributions  with  geometric  circles.  The  projection  architecture  aims  to  model  and  classify  physical  aggressive  behaviours,  by  using  biomechanical  primitives.  The  primitives  are  being  used  to  model  the  dynamics  of  the  aggressive  activities,  by  evolving  biomechanical  classifiers,  which  can  discriminate  between  three  behaviours  and  six  actions.  Both  evolutionary  models  have  shown  strong  discrimination  performances  on  recognising  the  individual  actions  of  each  behaviour.  From  the  comparison,  the  proposed  model  outperformed  the  classical  one  with  three  ensemble  programs.
0	Does  comma  selection  help  to  cope  with  local  optima.  One  hope  of  using  non-elitism  in  evolutionary  computation  is  that  it  aids  leaving  local  optima.  We  perform  a  rigorous  runtime  analysis  of  a  basic  non-elitist  evolutionary  algorithm  (EA),  the  (μ,  λ)  EA,  on  the  most  basic  benchmark  function  with  a  local  optimum,  the  jump  function.  We  prove  that  for  all  reasonable  values  of  the  parameters  and  the  problem,  the  expected  runtime  of  the  (μ,  λ)  EA  is,  apart  from  lower  order  terms,  at  least  as  large  as  the  expected  runtime  of  its  elitist  counterpart,  the  (μ  +  λ)  EA  (for  which  we  conduct  the  first  runtime  analysis  to  allow  this  comparison).  Consequently,  the  ability  of  the  (μ,  λ)  EA  to  leave  local  optima  to  inferior  solutions  does  not  lead  to  a  runtime  advantage.  We  complement  this  lower  bound  with  an  upper  bound  that,  for  broad  ranges  of  the  parameters,  is  identical  to  our  lower  bound  apart  from  lower  order  terms.  This  is  the  first  runtime  result  for  a  non-elitist  algorithm  on  a  multi-modal  problem  that  is  tight  apart  from  lower  order  terms.
0	A  guide  for  fitness  function  design.  Fitness  function  design  is  often  both  a  design  and  performance  bottleneck  for  evolutionary  algorithms.  The  fitness  function  for  a  given  problem  is  directly  related  to  the  specifications  for  that  problem.  This  paper  outlines  a  guide  for  transforming  problem  specifications  into  a  fitness  function.  The  target  audience  for  this  guide  are  both  non-expert  practitioners  and  those  interested  in  formalizing  fitness  function  design.  The  goal  is  to  investigate  and  formalize  the  fitness  function  generation  process  that  expert  developers  go  through  and  in  doing  so  make  fitness  function  design  less  of  a  bottleneck.  Solution  requirements  in  the  problem  specifications  are  identified  and  classified,  then  an  appropriate  fitness  function  component  is  generated  based  on  its  classifications,  and  finally  the  fitness  function  components  combined  to  yield  a  fitness  function  for  the  problem  in  question.  The  competitive  performance  of  a  guide  generated  fitness  function  is  demonstrated  by  comparing  it  to  that  of  an  expert  designed  fitness  function.
0	Impact  of  crossover  bias  in  genetic  programming.  In  tree-based  genetic  programming  (GP)  with  sub-tree  crossover,  the  parent  contributing  the  root  portion  of  the  tree  (the  root  parent)  often  contributes  more  to  the  semantics  of  the  resulting  child  than  the  non-root  parent.  Previous  research  demonstrated  that  when  the  root  parent  had  greater  fitness  than  the  non-root  parent,  the  fitness  of  the  child  tended  to  be  better  than  if  the  reverse  were  true.  Here  we  explore  the  significance  of  that  asymmetry  by  introducing  the  notion  of  crossover  bias,  where  we  bias  the  system  in  favor  of  having  the  more  fit  parent  as  the  root  parent.      In  this  paper  we  apply  crossover  bias  to  several  problems.  In  most  cases  we  found  that  crossover  bias  either  improved  performance  or  had  no  impact.  We  also  found  that  the  effectiveness  of  crossover  bias  is  dependent  on  the  problem,  and  significantly  dependent  on  other  parameter  choices.      While  this  work  focuses  specifically  on  sub-tree  crossover  in  tree-based  GP,  artificial  and  biological  evolutionary  systems  often  have  substantial  asymmetries,  many  of  which  remain  understudied.  This  work  suggests  that  there  is  value  in  further  exploration  of  the  impacts  of  these  asymmetries.
0	The  voice  prominence  hypothesis  the  interplay  of  f0  and  voice  source  features  in  accentuation.  This  paper  explores  the  interplay  of  source  correlates  of  accentuation,  examining  a  hypothesis  (the  Voice  Prominence  Hypothesis)  that  different  source  parameters  are  involved  and  may  serve  as  equivalent.  It  predicts  that  where  accentuation  is  not  marked  by  pitch  salience  there  will  be  more  extensive  changes  in  other  source  parameters.  This  follows  our  assumption  that  prosodic  entities  such  as  accentuation,  focus,  declination,  etc.  involve  adjustments  to  the  entire  voice  source  and  not  simply  to  F0.  Twelve  3-accent  sentences  of  Connemara  Irish  (declaratives,  WH  questions  and  Yes/No  questions)  were  analysed.  These  are  typically  produced  and  transcribed  as  H*  H*  H*L.  Of  particular  interest  were  the  second  accents:  although  they  are  heard  as  accented,  there  are  no  particular  pitch  excursions  that  would  account  for  their  salience.  Inverse  filtering  and  subsequent  source  parameterisation  was  carried  out  to  yield  measures  for  a  range  of  source  parameters.  Results  support  the  voice  prominence  hypothesis:  as  predicted,  the  most  striking  source  adjustments  were  found  in  the  second  accent.  Even  where  there  is  substantial  pitch  movement  (final  accent),  parameters  other  than  F0  appear  to  be  contributing  to  the  salience  of  the  accented  syllable.  The  precise  source  changes  associated  with  accentuation  varied  across  sentence  types  and  within  the  prosodic  phrase.  Index  Terms:  voice  source,  accentuation,  prominence.
0	A  multimodal  real  time  mri  articulatory  corpus  for  speech  research.  We  present  MRI-TIMIT:  a  large-scale  database  of  synchronized  audio  and  real-time  magnetic  resonance  imaging  (rtMRI)  data  for  speech  research.  The  database  currently  consists  of  speech  data  acquired  from  two  male  and  two  female  speakers  of  American  English.  Subjects’  upper  airways  were  imaged  in  the  midsagittal  plane  while  reading  the  same  460  sentence  corpus  used  in  the  MOCHA-TIMIT  corpus  [1].  Accompanying  acoustic  recordings  were  phonemically  transcribed  using  forced  alignment.  Vocal  tract  tissue  boundaries  were  automatically  identified  in  each  video  frame,  allowing  for  dynamic  quantification  of  each  speaker’s  midsagittal  articulation.  The  database  and  companion  toolset  provide  a  unique  resource  with  which  to  examine  articulatory-acoustic  relationships  in  speech  production.  Index  Terms:  speech  production,  speech  corpora,  real-time  MRI,  multi-modal  database,  large-scale  phonetic  tools
0	A  generalized  stein  s  estimation  approach  to  speech  enhancement  based  on  perceptual  criteria.  We  address  the  problem  of  speech  enhancement  using  a  risk-  estimation  approach.  In  particular,  we  propose  the  use  the  Stein’s  unbiased  risk  estimator  (SURE)  for  solving  the  problem.  The  need  for  a  suitable  finite-sample  risk  estimator  arises  because  the  actual  risks  invariably  depend  on  the  unknown  ground  truth.  We  consider  the  popular  mean-squared  error  (MSE)  criterion  first,  and  then  compare  it  against  the  perceptually-motivated  Itakura-Saito  (IS)  distortion,  by  deriving  unbiased  estimators  of  the  corresponding  risks.  We  use  a  generalized  SURE  (GSURE)  development,  recently  proposed  by  Eldar  for  MSE.  We  consider  dependent  observation  models  from  the  exponential  family  with  an  additive  noise  model,and  derive  an  unbiased  estimator  for  the  risk  corresponding  to  the  IS  distortion,  which  is  non-quadratic.  This  serves  to  address  the  speech  enhancement  problem  in  a  more  general  setting.  Experimental  results  illustrate  that  the  IS  metric  is  efficient  in  suppressing  musical  noise,  which  affects  the  MSE-enhanced  speech.  However,  in  terms  of  global  signal-to-noise  ratio  (SNR),  the  minimum  MSE  solution  gives  better  results.
0	Investigating  the  role  of  l1  in  automatic  pronunciation  evaluation  of  l2  speech.  Automatic  pronunciation  evaluation  plays  an  important  role  in  pronunciation  training  and  second  language  education.  This  field  draws  heavily  on  concepts  from  automatic  speech  recognition  (ASR)  to  quantify  how  close  the  pronunciation  of  non-native  speech  is  to  native-like  pronunciation.  However,  it  is  known  that  the  formation  of  accent  is  related  to  pronunciation  patterns  of  both  the  target  language  (L2)  and  the  speaker's  first  language  (L1).  In  this  paper,  we  propose  to  use  two  native  speech  acoustic  models,  one  trained  on  L2  speech  and  the  other  trained  on  L1  speech.  We  develop  two  sets  of  measurements  that  can  be  extracted  from  two  acoustic  models  given  accented  speech.  A  new  utterance-level  feature  extraction  scheme  is  used  to  convert  these  measurements  into  a  fixed-dimension  vector  which  is  used  as  an  input  to  a  statistical  model  to  predict  the  accentedness  of  a  speaker.  On  a  data  set  consisting  of  speakers  from  4  different  L1  backgrounds,  we  show  that  the  proposed  system  yields  improved  correlation  with  human  evaluators  compared  to  systems  only  using  the  L2  acoustic  model.
0	Bag  of  words  input  for  long  history  representation  in  neural  network  based  language  models  for  speech  recognition.  In  most  of  previous  works  on  neural  network  based  language  models  (NNLMs),  the  words  are  represented  as  1-of-N  encoded  feature  vectors.  In  this  paper  we  investigate  an  alternative  encoding  of  the  word  history,  known  as  bag-of-words  (BOW)  representation  of  a  word  sequence,  and  use  it  as  an  additional  input  feature  to  the  NNLM.  Both  the  feedforward  neural  network  (FFNN)  and  the  long  short-term  memory  recurrent  neural  network  (LSTM-RNN)  language  models  (LMs)  with  additional  BOW  input  are  evaluated  on  an  English  large  vocabulary  automatic  speech  recognition  (ASR)  task.  We  show  that  the  BOW  features  significantly  improve  both  the  perplexity  (PP)  and  the  word  error  rate  (WER)  of  a  standard  FFNN  LM.  In  contrast,  the  LSTM-RNN  LM  does  not  benefit  from  such  an  explicit  long  context  feature.  Therefore  the  performance  gap  between  feedforward  and  recurrent  architectures  for  language  modeling  is  reduced.  In  addition,  we  revisit  the  cache  based  LM,  a  seeming  analog  of  the  BOW  for  the  count  based  LM,  which  was  unsuccessful  for  ASR  in  the  past.  Although  the  cache  is  able  to  improve  the  perplexity,  we  only  observe  a  very  small  reduction  in  WER.  Index  Terms:  language  modeling,  speech  recognition,  bag-ofwords,  feedforward  neural  networks,  recurrent  neural  networks,  long  short-term  memory,  cache  language  model
0	Language  model  expansion  using  webdata  for  spoken  document  retrieval.  In  recent  years,  there  has  been  increasing  demand  for  ad  hoc  retrieval  of  spoken  documents.  We  can  use  existing  text  retrieval  methods  by  transcribing  spoken  documents  into  text  data  using  a  Large  Vocabulary  Continuous  Speech  Recognizer  (LVCSR).  However,  retrieval  performance  is  severely  deteriorated  by  recognition  errors  and  out-of-vocabulary  (OOV)  words.  To  solve  these  problems,  we  previously  proposed  an  expansion  method  that  compensates  the  transcription  by  using  text  data  downloaded  from  the  Web.  In  this  paper,  we  introduce  two  improvements  to  the  existing  document  expansion  framework.  First,  we  use  a  large-scale  sample  database  of  webdata  as  the  source  of  relevant  documents,  thus  avoiding  the  bias  introduced  by  choosing  keywords  in  the  existing  methods.  Next,  we  use  a  document  retrieval  method  based  on  a  statistical  language  model  (SLM),  which  is  a  popular  framework  in  information  retrieval,  and  also  propose  a  new  smoothing  method  considering  recognition  errors  and  missing  keywords.  Retrieval  experiments  show  that  the  proposed  methods  yield  a  good  results.  Index  Terms:  Spoken  document  retrieval,  statistical  language  models,  World  Wide  Web
0	Excitation  source  analysis  for  high  quality  speech  manipulation  systems  based  on  an  interference  free  representation  of  group  delay  with  minimum  phase  response  compensation.  A  group  delay-based  excitation  source  analysis  and  design  method  is  introduced  for  extension  of  TANDEM-STRAIGHT,  a  speech  analysis,  modification  and  synthesis  system.  This  introduction  makes  all  components  of  the  system  be  based  on  interference-free  representations.  They  are  power  spectrum,  instantaneous  frequency  and  group  delay  representations.  This  unification  has  potential  to  solve  the  major  weak  point  of  VOCODER  architecture  for  high-quality  speech  manipulation  applications.
0	Automatic  evaluation  of  speech  intelligibility  based  on  i  vectors  in  the  context  of  head  and  neck  cancers.  In  disordered  speech  context,  and  despite  its  well-known  sub-jectivity,  perceptual  evaluation  is  still  the  most  commonly  used  method  in  clinical  practice  to  evaluate  the  intelligibility  level  of  patients'  speech  productions.  However,  and  thanks  to  increasing  computing  power,  automatic  speech  processing  systems  have  witnessed  a  democratization  in  terms  of  users  and  application  areas  including  the  medical  practice.  In  this  paper,  we  evaluate  an  automatic  approach  for  the  prediction  of  cancer  patients'  speech  intelligibility  based  on  the  representation  of  the  speech  acoustics  in  the  total  variability  subspace  based  on  the  i-vector  paradigm.  Experimental  evaluations  of  the  proposed  predictive  approach  have  shown  a  very  high  correlation  rate  with  perceptual  intelligibility  when  applied  on  the  French  speech  corpora  C2SI  (r=0.84).  They  have  also  demonstrated  the  robustness  of  the  approach  when  using  a  limited  amount  of  disordered  speech  per  patient,  which  may  lead  to  the  redesign  and  alleviation  of  the  test  protocols  usually  used  in  disordered  speech  evaluation  context.
0	An  empirical  model  of  emphatic  word  detection.  Keywords:  probabilistic  amplitude  demodulation  ;  speech  emphasis  Reference  EPFL-REPORT-209098  Record  created  on  2015-06-19,  modified  on  2017-05-10
0	Prosody  contour  prediction  with  long  short  term  memory  bi  directional  deep  recurrent  neural  networks.  Deep  Neural  Networks  (DNNs)  have  been  shown  to  provide  state-of-the-art  performance  over  other  baseline  models  in  the  task  of  predicting  prosodic  targets  from  text  in  a  speechsynthesis  system.  However,  prosody  prediction  can  be  affected  by  an  interaction  of  short-  and  long-term  contextual  factors  that  a  static  model  that  depends  on  a  fixed-size  context  window  can  fail  to  properly  capture.  In  this  work,  we  look  at  a  recurrent  formulation  of  neural  networks  (RNNs)  that  are  deep  in  time  and  can  store  state  information  from  an  arbitrarily  large  input  history  when  making  a  prediction.  We  show  that  RNNs  provide  improved  performance  over  DNNs  of  comparable  size  in  terms  of  various  objective  metrics  for  a  variety  of  prosodic  streams  (notably,  a  relative  reduction  of  about  6%  in  F0  mean-square  error  accompanied  by  a  relative  increase  of  about  14%  in  F0  variance),  as  well  as  in  terms  of  perceptual  quality  assessed  through  mean-opinion-score  listening  tests.  Index  Terms:  speech  synthesis,text-to-speech,  prosody  prediction,  recurrent  neural  networks,  deep  learning
0	Seeing  voices  and  hearing  voices  learning  discriminative  embeddings  using  cross  modal  self  supervision.  The  goal  of  this  work  is  to  train  discriminative  cross-modal  embeddings  without  access  to  manually  annotated  data.  Recent  advances  in  self-supervised  learning  have  shown  that  effective  representations  can  be  learnt  from  natural  cross-modal  synchrony.  We  build  on  earlier  work  to  train  embeddings  that  are  more  discriminative  for  uni-modal  downstream  tasks.  To  this  end,  we  propose  a  novel  training  strategy  that  not  only  optimises  metrics  across  modalities,  but  also  enforces  intra-class  feature  separation  within  each  of  the  modalities.  The  effectiveness  of  the  method  is  demonstrated  on  two  downstream  tasks:  lip  reading  using  the  features  trained  on  audio-visual  synchronisation,  and  speaker  recognition  using  the  features  trained  for  cross-modal  biometric  matching.  The  proposed  method  outperforms  state-of-the-art  self-supervised  baselines  by  a  signficant  margin.
0	The  asynchronous  island  model  and  nsga  ii  study  of  a  new  migration  operator  and  its  performance.  This  work  presents  an  implementation  of  the  asynchronous  island  model  suitable  for  multi-objective  evolutionary  optimization  on  heterogeneous  and  large-scale  computing  platforms.  The  migration  of  individuals  is  regulated  by  the  crowding  comparison  operator  applied  to  the  originating  population  during  selection  and  to  the  receiving  population  augmented  by  all  migrants  during  replacement.  Experiments  using  this  method  combined  with  NSGA-II  show  its  scalability  up  to  128  islands  and  its  robustness.  Furthermore,  the  proposed  parallelization  technique  consistently  outperforms  a  multi-start  and  a  random  migration  approach  in  terms  of  convergence  speed,  while  maintaining  a  comparable  population  diversity.  Applied  to  a  real-world  problem  of  interplanetary  trajectory  design,  we  find  solutions  dominating  an  actual  NASA/ESA  mission  proposal  for  a  tour  from  Earth  to  Jupiter,  in  a  fraction  of  the  computational  time  that  would  be  needed  on  a  single  CPU.
0	Feature  selection  using  stochastic  diffusion  search.  The  method  introduced  in  this  paper  uses  stochastic  diffusion  search  (SDS)  to  select  the  most  relevant  feature  subset  for  the  classification  task.  In  this  algorithm,  SDS  is  adapted  to  find  a  suitable  feature  subset.  Moreover,  support  vector  machine  (SVM)  is  used  as  a  classifier  to  evaluate  the  predictive  accuracy  of  the  agent.  The  proposed  method  exhibits  a  statistically  significant  outperformance  when  compared  with  the  performance  of  the  classifier  without  the  SDS-powered  features  selections.  Additionally,  the  results  have  been  also  compared  with  other  methods  from  the  literature  over  nine  datasets.  It  is  shown  that  the  proposed  SDS  based  feature  selection  (SDS-FS)  offers  a  competitive  performance  with  other  methods  on  datasets  with  feature  size  greater  than  10.  The  behaviour  of  the  proposed  algorithm  has  been  investigated  in  the  context  of  global  exploration  and  local  exploitation.
0	Qr  mutations  improve  many  evolution  strategies  a  lot  on  highly  multimodal  problems.  Previous  studies  have  shown  the  efficiency  of  using  quasi-  random  mutations  on  the  well-know  CMA  evolution  strategy.  Quasi-random  mutations  have  many  advantages,  in  particular  their  application  is  stable,  efficient  and  easy  to  use.  In  this  article,  we  extend  this  principle  by  applying  quasi-random  mutations  on  several  well  known  continuous  evolutionary  algorithms  (SA,  CMSA,  CMA)  and  do  it  on  several  old  and  new  test  functions,  and  with  several  criteria.  The  results  point  out  a  clear  improvement  compared  to  the  baseline,  in  all  cases,  and  in  particular  for  moderate  computational  budget.
0	Genetic  algorithms  for  role  mining  in  critical  infrastructure  data  spaces.  In  the  paper,  a  Role  Mining  problem,  which  is  the  cornerstone  for  creating  Role-Based  Access  Control  (RBAC)  systems,  is  transferred  to  the  domain  of  data  spaces.  RBAC  is  the  most  widespread  model  of  access  control  in  different  multi-user  information  systems,  including  critical  infrastructures.  The  data  spaces  is  the  perspective  concept  of  creating  information  storage  systems,  which  transforms  the  concept  of  databases,  integrating  in  one  system  the  information  resources  from  other  systems,  and  allows  us  to  control  their  security  on  a  centralized  basis.  The  paper  considers  a  mathematical  statement  of  the  RBAC  design  problem  for  data  spaces  and  offers  the  approaches  to  its  solving  based  on  genetic  algorithms.  The  proposed  approaches  consider  requirements  of  compliance  with  role-based  security  policies  in  case  of  combining  all  users'  sets  and  all  permissions'  sets  in  the  data  space.  The  paper  considers  main  decisions  on  creation  and  enhancement  of  genetic  algorithms  which  implementation  increases  their  operational  speed.  The  experimental  assessment  of  the  offered  genetic  algorithms  shows  their  high  performance.
0	Multi  objective  routing  optimisation  for  battery  powered  wireless  sensor  mesh  networks.  Mesh  network  topologies  are  becoming  increasingly  popular  in  battery  powered  wireless  sensor  networks,  primarily  due  to  the  extension  of  network  range  and  resilience  against  routing  failures.  However,  multi-hop  mesh  networks  suffer  from  higher  energy  costs,  and  the  routing  strategy  directly  affects  the  lifetime  of  nodes  with  limited  energy  sources.  Hence  while  planning  routes  there  are  trade-offs  to  be  considered  between  individual  and  system-wide  battery  lifetimes.  We  present  a  novel  multi-objective  routing  optimisation  approach  using  evolutionary  algorithms  to  approximate  the  optimal  trade-off  between  minimum  lifetime  and  the  average  lifetime  of  nodes  in  the  network.  In  order  to  accomplish  this  combinatorial  optimisation  rapidly  and  thus  permit  dynamic  optimisation  for  self-healing  networks,  our  approach  uses  novel  $k$-shortest  paths  based  search  space  pruning  in  conjunction  with  a  new  edge  metric,  which  associates  the  energy  cost  at  a  pair  of  nodes  with  the  link  between  them.  We  demonstrate  our  solution  on  a  real  network,  deployed  in  the  Victoria  \&  Albert  Museum,  London.  We  show  that  this  approach  provides  better  trade-off  solutions  in  comparison  to  the  minimum  energy  option,  and  how  a  combination  of  solutions  over  the  lifetime  of  the  network  can  enhance  the  overall  minimum  lifetime.
0	Learning  oov  through  semantic  relatedness  in  spoken  dialog  systems.  •  Speech  recognition  and  language  understanding  performance  can  be  improved  through  an  OOV  expectand-learn  procedure.  •  A  limited  domain  vocabulary  can  be  utilized  to  effectively  acquire  OOVs  by  the  word  relatedness  theory  through  web  knowledge  bases.  •  With  data-driven  semantic  relatedness,  both  the  global  and  local  learning  procedures  are  able  to  successfully  harvest  more  than  50%  of  OOVs,  leading  to  better  recognition  and  understanding  performance.  •  This  work  demonstrates  that  o  OOV  learning  may  benefit  dialog  system  o  the  proposed  expect-and-learn  strategy  outperforms  the  traditional  detect-and-learn  in  both  higher  effectiveness  and  no  human  involvement.  1.  Linguistically  semantic  relatedness  o  Defined  by  linguistics,  e.g.,  WordNet  (WN),  Paraphrase  Database  (PPDB)  (Ganitkevitch  et  al.,  2013)  2.  Data-driven  semantic  relatedness  o  Distributional  semantics,  e.g.,  continuous  bag-ofword  embeddings  (CBOW)  (Mikolov  et  al.,  2013)    Detect-and-Learn  (Qin  et  al.,  2011;  2012):  o  Discover  OOV  words  during  the  conversation  o  Example:  S:  “I  heard  something  like  SELF,  can  you  repeat  it?”  U:  “It’s  SELFIE.”  o  Drawbacks  •  Limited  number  of  new  words  •  Required  human  efforts  to  correct  spellings  and  pronunciations    Expect-and-Learn  (proposed):  o  Use  semantic  relatedness  to  automatically  enrich  the  vocabulary  and  language  model  beforehand
0	Human  listening  and  live  captioning  multi  task  training  for  speech  enhancement.  With  the  surge  of  online  meetings,  it  has  become  more  critical  than  ever  to  provide  high-quality  speech  audio  and  live  captioning  under  various  noise  conditions.  However,  most  monaural  speech  enhancement  (SE)  models  introduce  processing  artifacts  and  thus  degrade  the  performance  of  downstream  tasks,  including  automatic  speech  recognition  (ASR).  This  paper  proposes  a  multi-task  training  framework  to  make  the  SE  models  unharmful  to  ASR.  Because  most  ASR  training  samples  do  not  have  corresponding  clean  signal  references,  we  alternately  perform  two  model  update  steps  called  SE-step  and  ASR-step.  The  SE-step  uses  clean  and  noisy  signal  pairs  and  a  signal-based  loss  function.  The  ASR-step  applies  a  pre-trained  ASR  model  to  training  signals  enhanced  with  the  SE  model.  A  cross-entropy  loss  between  the  ASR  output  and  reference  transcriptions  is  calculated  to  update  the  SE  model  parameters.  Experimental  results  with  realistic  large-scale  settings  using  ASR  models  trained  on  75,000-hour  data  show  that  the  proposed  framework  improves  the  word  error  rate  for  the  SE  output  by  11.82%  with  little  compromise  in  the  SE  quality.  Performance  analysis  is  also  carried  out  by  changing  the  ASR  model,  the  data  used  for  the  ASR-step,  and  the  schedule  of  the  two  update  steps.
0	Targeted  feature  dropout  for  robust  slot  filling  in  natural  language  understanding.  In  slot  filling  with  conditional  random  field  (CRF),  the  strong  current  word  and  dictionary  features  tend  to  swamp  the  effect  of  contextual  features,  a  phenomenon  also  known  as  feature  undertraining.  This  is  a  dangerous  tradeoff  especially  when  training  data  is  small  and  dictionaries  are  limited  in  its  coverage  of  the  entities  observed  during  testing.  In  this  paper,  we  propose  a  simple  and  effective  solution  that  extends  the  feature  dropout  algorithm,  directly  aiming  at  boosting  the  contribution  from  entity  context.  We  show  with  extensive  experiments  that  the  proposed  technique  can  significantly  improve  the  robustness  against  unseen  entities,  without  degrading  performance  on  entities  that  are  either  seen  or  exist  in  the  dictionary.
0	Practical  applicability  of  deep  neural  networks  for  overlapping  speaker  separation.  This  paper  examines  the  applicability  in  realistic  scenarios  of  two  deep  learning  based  solutions  to  the  overlapping  speaker  separation  problem.  Firstly,  we  present  experiments  that  show  that  these  methods  are  applicable  for  a  broad  range  of  languages.  Further  experimentation  indicates  limited  performance  loss  for  untrained  languages,  when  these  have  common  features  with  the  trained  language(s).  Secondly,  it  investigates  how  the  methods  deal  with  realistic  background  noise  and  proposes  some  modifications  to  better  cope  with  these  disturbances.  The  deep  learning  methods  that  will  be  examined  are  deep  clustering  and  deep  attractor  networks.
0	The  dku  system  for  the  speaker  recognition  task  of  the  2019  voices  from  a  distance  challenge.  In  this  paper,  we  present  the  DKU  system  for  the  speaker  recognition  task  of  the  VOiCES  from  a  distance  challenge  2019.  We  investigate  the  whole  system  pipeline  for  the  far-field  speaker  verification,  including  data  pre-processing,  short-term  spectral  feature  representation,  utterance-level  speaker  modeling,  back-end  scoring,  and  score  normalization.  Our  best  single  system  employs  a  residual  neural  network  trained  with  angular  softmax  loss.  Also,  the  weighted  prediction  error  algorithms  can  further  improve  performance.  It  achieves  0.3668  minDCF  and  5.58%  EER  on  the  evaluation  set  by  using  a  simple  cosine  similarity  scoring.  Finally,  the  submitted  primary  system  obtains  0.3532  minDCF  and  4.96%  EER  on  the  evaluation  set.
0	Speech  synthesis  in  various  communicative  situations  impact  of  pronunciation  variations.  While  current  research  in  speech  synthesis  focuses  on  the  generation  of  various  speaking  styles  or  emotions,  very  few  studies  have  addressed  the  possibility  of  including  phonetic  variations  according  to  the  communicative  situation  of  the  target  speech  (sports  commentaries,  TV  news,  etc.).  However,  significant  phonetic  variations  have  been  observed,  depending  on  various  communicative  factors  (e.g.  spontaneous/read  and  media  broadcast  or  not).  This  study  analyzes  whether  these  alternative  pronunciations  contribute  to  the  plausibility  of  the  message  and  should  therefore  be  considered  in  synthesis.  To  this  end,  subjective  tests  are  performed  on  synthesized  French  sports  commentaries.  They  aim  at  comparing  HMM-based  speech  synthesis  with  genuine  pronunciation  and  with  neutral  NLP-produced  phonetization.  Results  show  that  the  integration  of  the  phonetic  variations  significantly  improves  the  perceived  naturalness  of  the  generated  speech.  They  also  highlight  the  relative  importance  of  the  various  types  of  variations  and  show  that  schwa  elisions,  in  particular,  play  a  crucial  role  in  that  respect.
0	Incremental  learning  and  forgetting  in  stochastic  turn  taking  models.  We  present  a  computational  framework  for  stochastically  modeling  dyad  interaction  chronograms.  The  framework’s  most  novel  feature  is  the  capacity  for  incremental  learning  and  forgetting.  To  showcase  its  flexibility,  we  design  experiments  answering  four  concrete  questions  about  the  systematics  of  spoken  interaction.  The  results  show  that:  (1)  individuals  are  clearly  affected  by  one  another;  (2)  there  is  individual  variation  in  interaction  strategy;  (3)  strategies  wander  in  time  rather  than  converge;  and  (4)  individuals  exhibit  similarity  with  their  interlocutors.  We  expect  the  proposed  framework  to  be  capable  of  answering  many  such  questions  with  little  additional  effort.  Index  Terms:  interaction,  chronogram  modeling,  turn-taking,  incremental  learning.
0	Data  efficient  voice  cloning  from  noisy  samples  with  domain  adversarial  training.  Data  efficient  voice  cloning  aims  at  synthesizing  target  speaker's  voice  with  only  a  few  enrollment  samples  at  hand.  To  this  end,  speaker  adaptation  and  speaker  encoding  are  two  typical  methods  based  on  base  model  trained  from  multiple  speakers.  The  former  uses  a  small  set  of  target  speaker  data  to  transfer  the  multi-speaker  model  to  target  speaker's  voice  through  direct  model  update,  while  in  the  latter,  only  a  few  seconds  of  target  speaker's  audio  directly  goes  through  an  extra  speaker  encoding  model  along  with  the  multi-speaker  model  to  synthesize  target  speaker's  voice  without  model  update.  Nevertheless,  the  two  methods  need  clean  target  speaker  data.  However,  the  samples  provided  by  user  may  inevitably  contain  acoustic  noise  in  real  applications.  It's  still  challenging  to  generating  target  voice  with  noisy  data.  In  this  paper,  we  study  the  data  efficient  voice  cloning  problem  from  noisy  samples  under  the  sequence-to-sequence  based  TTS  paradigm.  Specifically,  we  introduce  domain  adversarial  training  (DAT)  to  speaker  adaptation  and  speaker  encoding,  which  aims  to  disentangle  noise  from  speech-noise  mixture.  Experiments  show  that  for  both  speaker  adaptation  and  encoding,  the  proposed  approaches  can  consistently  synthesize  clean  speech  from  noisy  speaker  samples,  apparently  outperforming  the  method  adopting  state-of-the-art  speech  enhancement  module.
0	Using  context  inference  to  improve  sentence  ordering  for  multi  document  summarization.  In  this  paper,  we  propose  a  novel  context  inference-based  approach  for  sentences  ordering  in  mult  i-document  summarization  application.  Our  method  first  detects  whether  or  not  two  summarizat  ion  sentences  should  be  adjacent  according  to  the  similarity  between  one  summarizat  ion  sentence  and  the  context  of  the  other  one,  and  then  it  co  mputes  the  reliability  of  the  adjacent  summarization  sentences  based  on  the  similarity  and  their  relative  position.  To  be  specific,  the  first  sentence  will  be  selected  according  to  features  of  sentence,  and  the  second  sentence  will  be  selected  if  and  only  if  it  has  the  maximu  m  reliability  with  previous  sentence.  Evaluation  result  shows  that  our  method  outperforms  the  state-of-the-art  ones  on  DUC2004  corpus.
0	Convolutional  neural  network  with  word  embeddings  for  chinese  word  segmentation.  Character-based  sequence  labeling  frame-  work  is  flexible  and  efficient  for  Chi-  nese  word  segmentation  (CWS).  Recently,  many  character-based  neural  models  have  been  applied  to  CWS.  While  they  obtain  good  performance,  they  have  two  obvious  weaknesses.  The  first  is  that  they  heav-  ily  rely  on  manually  designed  bigram  fea-  ture,  i.e.  they  are  not  good  at  captur-  ing  n-gram  features  automatically.  The  second  is  that  they  make  no  use  of  full  word  information.  For  the  first  weakness,  we  propose  a  convolutional  neural  model,  which  is  able  to  capture  rich  n-gram  fea-  tures  without  any  feature  engineering.  For  the  second  one,  we  propose  an  effective  approach  to  integrate  the  proposed  model  with  word  embeddings.  We  evaluate  the  model  on  two  benchmark  datasets:  PKU  and  MSR.  Without  any  feature  engineer-  ing,  the  model  obtains  competitive  per-  formance  —  95.7%  on  PKU  and  97.3%  on  MSR.  Armed  with  word  embeddings,  the  model  achieves  state-of-the-art  perfor-  mance  on  both  datasets  —  96.5%  on  PKU  and  98.0%  on  MSR,  without  using  any  ex-  ternal  labeled  resource.
0	Systematic  generalization  on  gscan  with  language  conditioned  embedding.  Systematic  Generalization  refers  to  a  learning  algorithm’s  ability  to  extrapolate  learned  behavior  to  unseen  situations  that  are  distinct  but  semantically  similar  to  its  training  data.  As  shown  in  recent  work,  state-of-the-art  deep  learning  models  fail  dramatically  even  on  tasks  for  which  they  are  designed  when  the  test  set  is  systematically  different  from  the  training  data.  We  hypothesize  that  explicitly  modeling  the  relations  between  objects  in  their  contexts  while  learning  their  representations  will  help  achieve  systematic  generalization.  Therefore,  we  propose  a  novel  method  that  learns  objects’  contextualized  embeddings  with  dynamic  message  passing  conditioned  on  the  input  natural  language  and  end-to-end  trainable  with  other  downstream  deep  learning  modules.  To  our  knowledge,  this  model  is  the  first  one  that  significantly  outperforms  the  provided  baseline  and  reaches  state-of-the-art  performance  on  grounded  SCAN  (gSCAN),  a  grounded  natural  language  navigation  dataset  designed  to  require  systematic  generalization  in  its  test  splits.
0	Entity  disambiguation  using  a  markov  logic  network.  *Entity  linking  (EL)  is  the  task  of  linking  a  textual  named  entity  mention  to  a  knowledge  base  entry.  It  is  a  difficult  task  involving  many  challenges,  but  the  most  crucial  problem  is  entity  ambiguity.  Traditional  EL  approaches  usually  employ  different  constraints  and  filtering  techniques  to  improve  performance.  However,  these  constraints  are  executed  in  several  different  stages  and  cannot  be  used  interactively.  In  this  paper,  we  propose  several  disambiguation  formulae/features  and  employ  a  Markov  logic  network  to  model  interweaved  constraints  found  in  one  type  of  EL,  gene  mention  linking.  To  assess  our  systems  effectiveness  in  different  applications,  we  adopt  two  evaluation  schemes:  article-wide  and  instance-based  precision/recall/F-measure.  Experimental  results  show  that  our  system  outperforms  the  baseline  systems  and  state-of-the-art  systems  under  both  evaluation  schemes.
0	End  to  end  learning  of  semantic  role  labeling  using  recurrent  neural  networks.  Semantic  role  labeling  (SRL)  is  one  of  the  basic  natural  language  processing  (NLP)  problems.  To  this  date,  most  of  the  successful  SRL  systems  were  built  on  top  of  some  form  of  parsing  results  (Koomen  et  al.,  2005;  Palmer  et  al.,  2010;  Pradhan  et  al.,  2013),  where  pre-defined  feature  templates  over  the  syntactic  structure  are  used.  The  attempts  of  building  an  end-to-end  SRL  learning  system  without  using  parsing  were  less  successful  (Collobert  et  al.,  2011).  In  this  work,  we  propose  to  use  deep  bi-directional  recurrent  network  as  an  end-to-end  system  for  SRL.  We  take  only  original  text  information  as  input  feature,  without  using  any  syntactic  knowledge.  The  proposed  algorithm  for  semantic  role  labeling  was  mainly  evaluated  on  CoNLL-2005  shared  task  and  achieved  F1  score  of  81.07.  This  result  outperforms  the  previous  state-of-the-art  system  from  the  combination  of  different  parsing  trees  or  models.  We  also  obtained  the  same  conclusion  with  F1  =  81.27  on  CoNLL2012  shared  task.  As  a  result  of  simplicity,  our  model  is  also  computationally  efficient  that  the  parsing  speed  is  6.7k  tokens  per  second.  Our  analysis  shows  that  our  model  is  better  at  handling  longer  sentences  than  traditional  models.  And  the  latent  variables  of  our  model  implicitly  capture  the  syntactic  structure  of  a  sentence.
0	Touch  editing  a  flexible  one  time  interaction  approach  for  translation.  We  propose  a  touch-based  editing  method  for  translation,  which  is  more  flexible  than  traditional  keyboard-mouse-based  translation  postediting.  This  approach  relies  on  touch  actions  that  users  perform  to  indicate  translation  errors.  We  present  a  dual-encoder  model  to  handle  the  actions  and  generate  refined  translations.  To  mimic  the  user  feedback,  we  adopt  the  TER  algorithm  comparing  between  draft  translations  and  references  to  automatically  extract  the  simulated  actions  for  training  data  construction.  Experiments  on  translation  datasets  with  simulated  editing  actions  show  that  our  method  significantly  improves  original  translation  of  Transformer  (up  to  25.31  BLEU)  and  outperforms  existing  interactive  translation  methods  (up  to  16.64  BLEU).  We  also  conduct  experiments  on  post-editing  dataset  to  further  prove  the  robustness  and  effectiveness  of  our  method.
0	Real  time  modeling  and  simulation  method  of  digital  twin  production  line.  In  view  of  the  low  efficiency  of  production  line  modeling  method  and  the  poor  quality  of  the  production  line  model,  this  paper  proposes  the  concept  of  digital  twin  production  line  by  analyzing  the  simulation  &  modeling  of  production  line  and  digital  twin,  and  analyzes  the  composition  of  digital  twin  production  line.  Then  the  real-time  modeling  and  simulation  method  of  digital  twin  production  line  is  proposed.  Finally,  the  effectiveness  of  the  proposed  method  is  verified  by  taking  a  product  assembly  line  as  an  example.
0	Equipment  acquisition  life  cycle  information  management  based  on  plm.  This  paper  firstly  analyzes  the  lifecycle  information  of  equipment  acquisition  between  the  military  and  the  contractors.  Then,  the  product  lifecycle  management  technology  adopted  in  the  modern  enterprises  is  introduced  in  the  information  management.  Finally,  SOA  architecture  for  the  information  management  of  equipment  acquisition  based  on  PLM  is  researched.
0	Multiobjective  firefly  algorithm  for  variable  selection  in  multivariate  calibration.  Firefly  Algorithm  is  a  newly  proposed  method  with  potential  application  on  several  real  world  problems,  such  as  variable  selection  problem.  This  paper  presents  a  Multiobjective  Firefly  Algorithm  (MOFA)  for  variable  selection  in  multivariate  calibration  models.  The  main  objective  is  to  propose  an  optimization  to  reduce  the  error  value  prediction  of  the  property  of  interest,  as  well  as  reducing  the  number  of  variables  selected.  Based  on  the  results  obtained,  it  is  possible  to  demonstrate  that  our  proposal  may  be  a  viable  alternative  in  order  to  deal  with  conflicting  objective-functions.  Additionally,  we  compare  MOFA  with  traditional  algorithms  for  variable  selection  and  show  that  it  is  a  more  relevant  contribution  for  the  variable  selection  problem.
0	Differential  evolution  with  a  two  stage  optimization  mechanism  for  numerical  optimization.  Differential  Evolution  (DE)  is  a  popular  paradigm  of  evolutionary  algorithms,  which  has  been  successfully  applied  to  solve  different  kinds  of  optimization  problems.  To  design  an  effective  DE,  it  is  necessary  to  consider  different  requirements  of  the  exploration  and  exploitation  at  different  evolutionary  stages.  Motivated  by  this  consideration,  a  new  DE  with  a  two-stage  optimization  mechanism,  called  TSDE,  has  been  proposed  in  this  paper.  In  TSDE,  based  on  the  number  of  fitness  evaluations,  the  whole  evolutionary  process  is  divided  into  two  stages,  namely  the  former  stage  and  the  latter  stage.  TSDE  focuses  on  improving  the  search  ability  in  the  former  stage  and  emphasizes  the  convergence  in  the  latter  stage.  Hence,  different  trial  vector  generation  strategies  have  been  utilized  at  different  stages.  TSDE  has  been  tested  on  25  benchmark  test  functions  from  IEEE  CEC2005  and  30  benchmark  test  functions  from  IEEE  CEC2014.  The  experimental  results  suggest  that  TSDE  performs  better  than  four  other  state-of-the-art  DE  variants.
0	A  clustering  based  approach  for  exploring  sequences  of  compiler  optimizations.  In  this  paper  we  present  a  clustering-based  selection  approach  for  reducing  the  number  of  compilation  passes  used  in  search  space  during  the  exploration  of  optimizations  aiming  at  increasing  the  performance  of  a  given  function  and/or  code  fragment.  The  basic  idea  is  to  identify  similarities  among  functions  and  to  use  the  passes  previously  explored  each  time  a  new  function  is  being  compiled.  This  subset  of  compiler  optimizations  is  then  used  by  a  Design  Space  Exploration  (DSE)  process.  The  identification  of  similarities  is  obtained  by  a  data  mining  method  which  is  applied  to  a  symbolic  code  representation  that  translates  the  main  structures  of  the  source  code  to  a  sequence  of  symbols  based  on  transformation  rules.  Experiments  were  performed  for  evaluating  the  effectiveness  of  the  proposed  approach.  The  selection  of  compiler  optimization  sequences  considering  a  set  of  49  compilation  passes  and  targeting  a  Xilinx  MicroBlaze  processor  was  performed  aiming  at  latency  improvements  for  41  functions  from  Texas  Instruments  benchmarks.  The  results  reveal  that  the  passes  selection  based  on  our  clustering  method  achieves  a  significant  gain  on  execution  time  over  the  full  search  space  still  achieving  important  performance  speedups.
0	A  mono  objective  evolutionary  algorithm  for  protein  structure  prediction  in  structural  and  energetic  contexts.  The  Protein  Structure  Prediction  (PSP)  problem  is  concerned  about  the  prediction  of  the  native  structure  of  a  protein  from  its  amino  acid  sequence.  PSP  is  a  challenging  and  computationally  open  problem.  Therefore,  several  researches  and  methodologies  have  been  developed  for  it.  This  paper  presents  the  application  of  protpred-GROMACS,  an  evolutionary  framework  for  PSP,  in  structural  and  energetic  contexts.  The  performance  of  mono-objective  algorithm  was  compared  with  other  methodologies,  such  as  multi-objective  evolutionary  algorithm,  coarse  grained  monte  carlo  and  replica  exchange  molecular  dynamics.
0	Asynchronous  evolutionary  multi  objective  algorithms  with  heterogeneous  evaluation  costs.  Master-slave  parallelization  of  Evolutionary  Algorithms  (EAs)  is  straightforward,  by  distributing  all  fitness  computations  to  slaves.  The  benefits  of  asynchronous  steady-state  approaches  are  well-known  when  facing  a  possible  heterogeneity  among  the  evaluation  costs  in  term  of  runtime,  be  they  due  to  heterogeneous  hardware  or  non-linear  numerical  simulations.  However,  when  this  heterogeneity  depends  on  some  characteristics  of  the  individuals  being  evaluated,  the  search  might  be  biased,  and  some  regions  of  the  search  space  poorly  explored.  Motivated  by  a  real-world  case  study  of  multi-objective  optimization  problem  —  the  optimization  of  the  combustion  in  a  Diesel  Engine  —  the  consequences  of  different  components  of  heterogeneity  in  the  evaluation  costs  on  the  convergence  of  two  Evolutionary  Multi-objective  Optimization  Algorithms  are  investigated  on  artificially-heterogeneous  benchmark  problems.  In  some  cases,  better  spread  of  the  population  on  the  Pareto  front  seem  to  result  from  the  interplay  between  the  heterogeneity  at  hand  and  the  evolutionary  search.
0	Novel  hybrid  compact  genetic  algorithm  for  simultaneous  structure  and  parameter  learning  of  neural  networks.  The  automatic  simultaneous  selection  of  structure  and  parameters  of  an  artificial  neural  networks  is  an  important  area  of  research.  Although  many  variants  of  evolutionary  algorithms  (EA)  have  been  successfully  applied  to  this  problem,  their  demanding  memory  requirements  have  restricted  their  application  to  real  world  problems,  especially  embedded  applications  with  memory  constraints.  In  this  paper,  structure  and  parameter  learning  of  a  neural  network  using  a  novel  hybrid  compact  genetic  algorithm  (HCGA)  is  proposed.  In  the  HCGA,  each  string  combines  real  and  binary  segments  together.  For  a  feedforward  neural  network,  the  real  segment  encodes  it  weights,  while  the  binary  segment  encodes  the  presence/absence  of  a  connection  of  the  network.  The  proposed  hybrid  compact  genetic  algorithm  (HCGA)  has  several  advantages:  low  computational  cost,  controllable  weight  regularization  leading  to  automatic  architecture  discovery.  The  HCGA  is  tested  on  two  benchmark  problems  of  Ripley's  synthetic  2-class  problem  and  Mackey  glass  time  series  prediction  problem.  Experimental  results  show  that  the  proposed  algorithm  exhibits  good  performance  with  low  computation  cost  and  controllable  network  structure.
0	A  novel  multi  objective  cultural  algorithm  embedding  five  element  cycle  optimization.  The  cultural  algorithm,  as  a  dual-inheritance  framework  designed  for  optimization  problems,  can  incorporate  any  population-adopted  evolutionary  computation  technique  in  its  population  space.  On  the  other  hand,  based  on  the  Five-Elements  Cycle  Model  derived  from  the  ancient  Chinese  Five  Elements  (metal,  wood,  water,  fire,  earth)  theory,  the  five-elements  cycle  optimization  algorithm  was  proved  to  be  effective  in  solving  continuous  function  optimization  problems.  In  this  work,  we  propose  a  multi-objective  cultural  algorithm  with  a  five-elements-cycle-optimization-based  population  space,  where  the  five-element  cycle  model  is  adopted  as  the  evolution  scheme  in  the  population  space  of  the  cultural  algorithm  framework.  Simulation  results  on  12  classic  benchmark  problems  show  that  the  proposed  algorithm  can  effectively  solve  continuous  optimization  functions  and  obtains  satisfactory  non-dominated  solutions  compared  with  8  representative  multi-objective  algorithms.
0	Handling  constraints  in  the  hp  model  for  protein  structure  prediction  by  multiobjective  optimization.  The  hydrophobic-polar  (TIP)  model  is  an  abstract  representation  of  the  protein  structure  prediction  problem,  where  hydrophobic  interactions  are  assumed  to  be  the  major  determinant  of  the  folded  state  of  proteins.  This  paper  inquires  into  the  constraint-handling  design  issue  of  metaheuristics,  which  is  crucial  when  dealing  with  such  a  challenging  and  highly  constrained  combinatorial  optimization  problem.  A  new  constraint-handling  strategy  for  the  TIP  model,  based  on  multiobjective  optimization  concepts,  is  here  proposed.  The  multiobjective  approach  for  handling  constraints  in  this  particular  problem  is  explored  for  the  first  time  in  this  study,  to  the  authors'  knowledge.  Using  a  basic  genetic  algorithm  and  a  large  set  of  test  instances  for  the  two-dimensional  TIP  model  (based  on  the  square  lattice),  the  proposed  multiobjective  strategy  was  evaluated  and  compared  with  respect  to  commonly  adopted  techniques  from  the  literature.  On  the  one  hand,  through  such  a  comparative  analysis  it  was  possible  to  demonstrate  the  suitability  of  the  proposed  multiobjective  strategy.  On  the  other  hand,  the  results  of  this  study  provide  further  insight  into  whether  infeasible  protein  conformations  should  be  allowed  or  prevented  during  the  metaheuristic  search  process,  which  has  been  a  subject  of  concern  in  the  specialized  literature.
0	Evolvable  free  form  deformation  control  volumes  for  evolutionary  design  optimization.  Evolutionary  design  optimization  for  improving  the  performance  of  real  world  objects,  like  e.g.  car  shapes  in  the  context  of  aerodynamic  efficiency,  usually  depends  on  a  well-balanced  combination  of  representation,  optimizer  and  design  evaluation  method.  Shape  representation  requires  a  fair  trade-off  between  minimum  number  of  design  parameters  and  design  flexibility  which  likewise  guarantees  a  good  optimization  convergence  while  allowing  manifold  design  variations.  Recently,  shape  morphing  methods  have  gained  increased  attention  because  of  their  capability  to  represent  complex  shapes  with  a  reasonable  number  of  parameters,  especially  powerful  if  coupled  with  numerical  simulations  for  measuring  design  performance.  Free-form  deformation,  as  prominent  shape  morphing  representative,  relies  on  an  initial  grid  of  control  points,  the  control  volume,  which  allows  the  modification  of  the  embedded  shape.  The  set-up  of  the  control  volume  is  a  crucial  process  which  in  practice  is  done  manually  based  on  the  experience  of  the  human  user.  Here,  a  method  for  the  automated  construction  of  control  volumes  is  suggested  based  on  a  proposed  measure  E  CV    which  relies  on  the  concept  of  evolvability  as  a  potential  capacity  of  representations  to  produce  successful  designs  in  a  reasonable  time.  It  is  shown  for  target  shape  matching  experiments  that  optimizations  based  on  E  CV  -tuned  control  volumes  provide  a  significantly  better  performance  in  design  optimization.
0	Feature  engineering  for  improving  financial  derivatives  based  rainfall  prediction.  Rainfall  is  one  of  the  most  challenging  variables  to  predict,  as  it  exhibits  very  unique  characteristics  that  do  not  exist  in  other  time  series  data.  Moreover,  rainfall  is  a  major  component  and  is  essential  for  applications  that  surround  water  resource  planning.  In  particular,  this  paper  is  interested  in  extending  previous  work  carried  out  on  the  prediction  of  rainfall  using  Genetic  Programming  (GP)  for  rainfall  derivatives.  Currently  in  the  rainfall  derivatives  literature,  the  process  of  predicting  rainfall  is  dominated  by  statistical  models,  namely  using  a  Markov-chain  extended  with  rainfall  prediction  (MCRP).  In  this  paper  we  further  extend  our  new  methodology  by  looking  at  the  effect  of  feature  engineering  on  the  rainfall  prediction  process.  Feature  engineering  will  allow  us  to  extract  additional  information  from  the  data  variables  created.  By  incorporating  feature  engineering  techniques  we  look  to  further  tailor  our  GP  to  the  problem  domain  and  we  compare  the  performance  of  the  previous  GP,  which  previously  statistically  outperformed  MCRP,  against  our  new  GP  using  feature  engineering  on  21  different  data  sets  of  cities  across  Europe  and  report  the  results.  The  goal  is  to  see  whether  GP  can  outperform  its  predecessor  without  extra  features,  which  acts  as  a  benchmark.  Results  indicate  that  in  general  GP  using  extra  features  significantly  outperforms  a  GP  without  the  use  of  extra  features.
0	Reference  point  based  multi  objective  optimization  through  decomposition.  In  this  paper  we  propose  a  user-preference  based  evolutionary  algorithm  that  relies  on  decomposition  strategies  to  convert  a  multi-objective  problem  into  a  set  of  single-objective  problems.  The  use  of  a  reference  point  allows  the  algorithm  to  focus  the  search  on  more  preferred  regions  which  can  potentially  save  considerable  amount  of  computational  resources.  The  algorithm  that  we  proposed,  dynamically  adapts  the  weight  vectors  and  is  able  to  converge  close  to  the  preferred  regions.  Combining  decomposition  strategies  with  reference  point  approaches  paves  the  way  for  more  effective  optimization  of  many-objective  problems.  The  use  of  a  decomposition  method  alleviates  the  selection  pressure  problem  associated  with  dominance-based  approaches  while  a  reference  point  allows  a  more  focused  search.  The  experimental  results  show  that  the  proposed  algorithm  is  capable  of  finding  solutions  close  to  the  reference  points  specified  by  a  decision  maker.  Moreover,  our  results  show  that  high  quality  solutions  can  be  obtained  using  less  computational  effort  as  compared  to  a  state-of-the-art  decomposition  based  evolutionary  multi-objective  algorithm.
0	Moea  d  with  an  improved  multi  dimensional  mapping  coding  scheme  for  constrained  multi  objective  portfolio  optimization.  Portfolio  optimization  is  an  important  financial  problem,  which  involves  an  optimal  allocation  of  finite  capital  to  a  series  of  assets  to  achieve  an  acceptable  trade-off  between  profit  and  risk  in  a  given  investment  period.  In  this  paper,  an  extended  Markowitz’s  mean-variance  portfolio  optimization  model,  which  is  converted  as  a  constrained  multi-objective  problem,  is  studied.  Since  this  model  involves  both  discrete  and  continuous  variables,  a  multi-dimensional  mapping  coding  scheme  (MDM)  has  been  adopted  to  convert  discrete  variables  to  continuous  ones.  Although  the  basic  MDM  is  effective  for  dealing  with  the  constrained  multi-objective  portfolio  optimization  problems,  it  sometimes  prefers  to  choose  some  balanced  investments,  in  which  the  allocation  of  funds  for  each  selected  asset  is  very  similar.  This  may  result  in  a  focus  on  the  low-risk  and  low-yield  solutions.  To  solve  this  problem,  an  improved  multi-dimensional  mapping  coding  scheme  is  proposed  in  this  paper.  This  new  coding  scheme  is  integrated  into  the  decomposition  based  multi-objective  evolutionary  algorithm  (MOEA/D).  The  algorithm  is  then  applied  to  some  test  data,  with  the  asset  size  ranging  from  31  to  255,  and  the  experimental  results  have  indicated  that  the  improved  MDM  coding  scheme  can  significantly  improve  the  performance  comparing  to  the  basic  MDM  coding  scheme.
0	Agent  based  modeling  of  migration  dynamics  in  the  mekong  delta  vietnam  automated  calibration  using  a  genetic  algorithm.  Migration  is  one  of  the  many  responses  humans  and  societies  make  to  ongoing  demographic,  economic,  societal  and  environmental  changes.  In  this  work,  we  use  agent-based  modeling  (ABM)  to  study  the  dynamics  of  migration  flows  across  provinces  and  cities  in  the  Mekong  Delta,  Vietnam.  The  strength  of  ABM  is  that  it  allows  a  bottom-up  approach  that  focuses  on  how  individuals  make  decisions  in  a  complex  system  comprising  various  factors.  Outputs  of  our  agent-based  model  are  automatically  calibrated  with  actual  data  using  a  genetic  algorithm.  This  automated  calibration  yields  some  significant  improvement  in  the  results,  with  all  observed  net-  and  out-migration  data  captured  within  the  95%  confidence  interval.  Sensitivity  analysis  carried  out  helps  to  further  understand  the  impact  of  critical  factors  on  the  final  migration  decision.
0	Automatic  design  of  heuristics  for  minimizing  the  makespan  in  permutation  flow  shops.  The  automatic  design  of  heuristic  search  methods  has  been  applied  successfully  to  many  optimization  problems.  In  this  paper  we  study  the  application  of  automatic  algorithm  configuration  to  the  permutation  flow  shop  scheduling  problem  with  makespan  minimization.  Our  approach  consists  in  using  a  grammar  to  determine  how  to  combine  individual  algorithmic  components  into  an  iterated  local  search,  coupled  with  a  parametric  representation  for  the  instantiations  of  such  a  grammar.  To  explore  the  algorithmic  search  space  we  employ  a  procedure  based  on  racing.  The  obtained  algorithms  are  evaluated  on  two  well-known  benchmarks  and  compared  to  state-of-the-art  heuristics.
0	Preference  learning  with  evolutionary  multivariate  adaptive  regression  spline  model.  This  paper  introduces  a  novel  approach  for  pairwise  preference  learning  through  combining  an  evolutionary  method  with  Multivariate  Adaptive  Regression  Spline  (MARS).  Collecting  users'  feedback  through  pairwise  preferences  is  recommended  over  other  ranking  approaches  as  this  method  is  more  appealing  for  human  decision  making.  Learning  models  from  pairwise  preference  data  is  however  an  NP-hard  problem.  Therefore,  constructing  models  that  can  effectively  learn  such  data  is  a  challenging  task.  Models  are  usually  constructed  with  accuracy  being  the  most  important  factor.  Another  vitally  important  aspect  that  is  usually  given  less  attention  is  expressiveness,  i.e.  how  easy  it  is  to  explain  the  relationship  between  the  model  input  and  output.  Most  machine  learning  techniques  are  focused  either  on  performance  or  on  expressiveness.  This  paper  employ  MARS  models  which  have  the  advantage  of  being  a  powerful  method  for  function  approximation  as  well  as  being  relatively  easy  to  interpret.  MARS  models  are  evolved  based  on  their  efficiency  in  learning  pairwise  data.  The  method  is  tested  on  two  datasets  that  collectively  provide  pairwise  preference  data  of  five  cognitive  states  expressed  by  users.  The  method  is  analysed  in  terms  of  the  performance,  expressiveness  and  complexity  and  showed  promising  results  in  all  aspects.
0	Competition  based  distributed  differential  evolution.  Differential  evolution  (DE)  is  a  simple  and  efficient  evolutionary  algorithm  for  global  optimization.  In  distributed  differential  evolution  (DDE),  the  population  is  divided  into  several  sub-populations  and  each  sub-population  evolves  independently  for  enhancing  algorithmic  performance.  Through  sharing  elite  individuals  between  sub-populations,  effective  information  is  spread.  However,  the  information  exchanged  through  individuals  is  still  too  limited.  To  address  this  issue,  a  competition-based  strategy  is  proposed  in  this  paper  to  achieve  comprehensive  interaction  between  sub-populations.  Two  operators  named  opposition-invasion  and  cross-invasion  are  designed  to  realize  the  invasion  from  good  performing  sub-populations  to  bad  performing  subpopulations.  By  utilizing  opposite  invading  sub-population,  the  search  efficiency  at  promising  regions  is  improved  by  opposition-invasion.  In  cross-invasion,  information  from  both  invading  and  invaded  sub-populations  is  combined  and  population  diversity  is  maintained.  Moreover,  the  proposed  algorithm  is  implemented  in  a  parallel  master-slave  manner.  Extensive  experiments  are  conducted  on  15  widely  used  large-scale  benchmark  functions.  Experimental  results  demonstrate  that  the  proposed  competition-based  DDE  (DDE-CB)  could  achieve  competitive  or  even  better  performance  compared  with  several  state-of-the-art  DDE  algorithms.  The  effect  of  proposed  competition-based  strategy  cooperation  with  well-known  DDE  variants  is  also  verified.
0	A  benchmark  of  population  based  metaheuristic  algorithms  for  high  dimensional  multi  level  image  thresholding.  Multi-level  image  thresholding  is  a  popular  approach  for  image  segmentation  where  the  image  is  divided  into  several  non-overlapping  regions  based  on  the  image  histogram.  Conventional  algorithms  for  multi-level  image  thresholding  are  time-consuming.  This  is  in  particular  so  when  the  number  of  thresholds  increases  due  to  the  curse  of  dimensionality  where  the  search  space  expands  exponentially  as  the  number  of  parameters  (thresholds)  increases.  One  approach  to  address  this  problem  is  to  employ  population-based  metaheuristic  algorithms.  Since  various  such  optimisation  algorithms  have  been  presented  in  the  literature,  in  this  paper,  we  benchmark  the  performance  of  13  population-based  algorithms  in  the  high-dimensional  search  spaces  of  the  multi-level  image  thresholding  problem.  The  algorithms  we  assess  include  the  whale  optimisation  algorithm  (WOA),  grey  wolf  optimiser  (GWO),  cuckoo  optimisation  algorithm  (COA),  biogeography-based  optimisation  (BBO),  teaching-learning-based  optimisation  (TLBO),  gravitational  search  algorithm  (GSA),  imperialist  competitive  algorithm  (ICA),  cuckoo  search  (CS),  firefly  algorithm  (FA),  bat  algorithm  (BA),  differential  evolution  (DE),  particle  swarm  optimisation  (PSO),  and  genetic  algorithm  (GA).  We  evaluate  these  on  different  images  with  regards  to  objective  function  value  as  well  as  peak  signal-to-noise  ratio  (PSNR)  and  also  employ  a  non-parametric  statistical  test,  the  Wilcoxon  signed  rank  test,  to  compare  the  algorithms  and  to  draw  conclusions  about  their  performance  for  multi-level  image  thresholding.
0	Evolution  of  cellular  automata  using  instruction  based  approach.  This  paper  introduces  a  method  of  encoding  cellular  automata  local  transition  function  using  an  instruction-based  approach  and  their  design  by  means  of  genetic  algorithms.  The  proposed  method  represents  an  indirect  mapping  between  the  input  combinations  of  states  in  the  cellular  neighborhood  and  the  next  states  of  the  cells  during  the  development  steps.  In  this  case  the  local  transition  function  is  described  by  a  program  (algorithm)  whose  execution  calculates  the  next  cell  states.  The  objective  of  the  program-based  representation  is  to  reduce  the  length  of  the  chromosome  in  case  of  the  evolutionary  design  of  cellular  automata.  It  will  be  shown  that  the  instruction-based  development  allows  us  to  design  complex  cellular  automata  with  higher  success  rate  than  the  conventional  table-based  method  especially  for  complex  cellular  automata  with  more  than  two  cell  states.  The  case  studies  include  the  replication  problem  and  the  problem  of  development  of  a  given  pattern  from  an  initial  seed.
0	Multi  dimensional  scaling  and  modeller  based  evolutionary  algorithms  for  protein  model  refinement.  Protein  structure  prediction,  i.e.,  computationally  predicting  the  three-dimensional  structure  of  a  protein  from  its  primary  sequence,  is  one  of  the  most  important  and  challenging  problems  in  bioinformatics.  Model  refinement  is  a  key  step  in  the  prediction  process,  where  improved  structures  are  constructed  based  on  a  pool  of  initially  generated  models.  Since  the  refinement  category  was  added  to  the  biennial  Critical  Assessment  of  Structure  Prediction  (CASP)  in  2008,  CASP  results  show  that  it  is  a  challenge  for  existing  model  refinement  methods  to  improve  model  quality  consistently.    This  paper  presents  three  evolutionary  algorithms  for  protein  model  refinement,  in  which  multidimensional  scaling(MDS),  the  MODELLER  software,  and  a  hybrid  of  both  are  used  as  crossover  operators,  respectively.  The  MDS-based  method  takes  a  purely  geometrical  approach  and  generates  a  child  model  by  combining  the  contact  maps  of  multiple  parents.  The  MODELLER-based  method  takes  a  statistical  and  energy  minimization  approach,  and  uses  the  remodeling  module  in  MODELLER  program  to  generate  new  models  from  multiple  parents.  The  hybrid  method  first  generates  models  using  the  MDS-based  method  and  then  run  them  through  the  MODELLER-based  method,  aiming  at  combining  the  strength  of  both.  Promising  results  have  been  obtained  in  experiments  using  CASP  datasets.  The  MDS-based  method  improved  the  best  of  a  pool  of  predicted  models  in  terms  of  the  global  distance  test  score  (GDT-TS)  in  9  out  of  16test  targets.
0	A  continuous  estimation  of  distribution  algorithm  by  evolving  graph  structures  using  reinforcement  learning.  A  novel  graph-based  Estimation  of  Distribution  Algorithm  (EDA)  named  Probabilistic  Model  Building  Genetic  Network  Programming  (PMBGNP)  has  been  proposed.  Inspired  by  classical  EDAs,  PMBGNP  memorizes  the  current  best  individuals  and  uses  them  to  estimate  a  distribution  for  the  generation  of  the  new  population.  However,  PMBGNP  can  evolve  compact  programs  by  representing  its  solutions  as  graph  structures.  Therefore,  it  can  solve  a  range  of  problems  different  from  conventional  ones  in  EDA  literature,  such  as  data  mining  and  Reinforcement  Learning  (RL)  problems.  This  paper  extends  PMBGNP  from  discrete  to  continuous  search  space,  which  is  named  PMBGNP-AC.  Besides  evolving  the  node  connections  to  determine  the  optimal  graph  structures  using  conventional  PMBGNP,  Gaussian  distribution  is  used  for  the  distribution  of  continuous  variables  of  nodes.  The  mean  value  μ  and  standard  deviation  σ  are  constructed  like  those  of  classical  continuous  Population-based  incremental  learning  (PBILc).  However,  a  RL  technique,  i.e.,  Actor-Critic  (AC),  is  designed  to  update  the  parameters  (μ  and  σ).  AC  allows  us  to  calculate  the  Temporal-Difference  (TD)  error  to  evaluate  whether  the  selection  of  the  continuous  value  is  better  or  worse  than  expected.  This  scalar  reinforcement  signal  can  decide  whether  the  tendency  to  select  this  continuous  value  should  be  strengthened  or  weakened,  allowing  us  to  determine  the  shape  of  the  probability  density  functions  of  the  Gaussian  distribution.  The  proposed  algorithm  is  applied  to  a  RL  problem,  i.e.,  autonomous  robot  control,  where  the  robot's  wheel  speeds  and  sensor  values  are  continuous.  The  experimental  results  show  the  superiority  of  PMBGNP-AC  comparing  with  the  conventional  algorithms.
0	Heuristic  optimization  for  software  project  management  with  impacts  of  team  efficiency.  Most  of  the  studies  on  project  scheduling  problems  assume  that  every  assigned  participant  or  every  team  of  the  same  number  of  participants,  completes  tasks  with  an  equal  efficiency,  but  this  is  usually  not  the  case  for  real  world  problems.  This  paper  presents  a  more  realistic  and  complex  model  with  extra  consideration  on  team  efficiency  which  are  quantitatively  measured  on  employee-task  assignment.  This  study  demonstrates  the  impacts  of  team  efficiency  in  a  well-studied  software  project  management  problem.  Moreover,  this  study  illustrates  how  a  heuristic  optimization  method,  population-based  incremental  learning,  copes  with  such  added  complexity.  The  experimental  results  show  that  the  resulting  near  optimal  solutions  not  only  satisfy  constraints,  but  also  reflect  the  impacts  of  team  efficiency.  The  findings  will  hopefully  motivate  future  studies  on  comprehensive  understandings  of  the  quality  and  efficiency  of  team  work.
0	Landscape  synergy  in  evolutionary  multitasking.  Over  the  years,  the  algorithms  of  evolutionary  computation  have  emerged  as  popular  tools  for  tackling  complex  real-world  optimization  problems.  A  common  feature  among  these  algorithms  is  that  they  focus  on  efficiently  solving  a  single  problem  at  a  time.  Despite  the  availability  of  a  population  of  individuals  navigating  the  search  space,  and  the  implicit  parallelism  of  their  collective  behavior,  seldom  has  an  effort  been  made  to  multitask.  Considering  the  power  of  implicit  parallelism,  we  are  drawn  to  the  idea  that  population-based  search  strategies  provide  an  idyllic  setting  for  leveraging  the  underlying  synergies  between  objective  function  landscapes  of  seemingly  distinct  optimization  tasks,  particularly  when  they  are  solved  together  with  a  single  population  of  evolving  individuals.  As  has  been  recently  demonstrated,  allowing  the  principles  of  evolution  to  autonomously  exploit  the  available  synergies  can  often  lead  to  accelerated  convergence  for  otherwise  complex  optimization  tasks.  With  the  aim  of  providing  deeper  insight  into  the  processes  of  evolutionary  multitasking,  we  present  in  this  paper  a  conceptualization  of  what,  in  our  opinion,  is  one  possible  interpretation  of  the  complementarity  between  optimization  tasks.  In  particular,  we  propose  a  synergy  metric  that  captures  the  correlation  between  objective  function  landscapes  of  distinct  tasks  placed  in  synthetic  multitasking  environments.  In  the  long  run,  it  is  contended  that  the  metric  will  serve  as  an  important  guide  toward  better  understanding  of  evolutionary  multitasking,  thereby  facilitating  the  design  of  improved  multitasking  engines.
0	Optimal  placement  of  distributed  generation  in  micro  grids  with  binary  and  integer  encoding  evolutionary  algorithms.  This  paper  discuses  the  performance  of  two  different  Evolutionary  Algorithms  (EAs)  in  a  problem  of  Optimal  Placement  of  Distributed  Power  Generation  (OPDPG)  in  Micro-Grids  (MGs).  Specifically,  the  problem  consists  of  choosing  the  node/nodes  to  locate  a  number  of  different  distributed  generators  with  different  technologies  (such  as  micro  wind  turbines,  photovoltaic  panels,  etc.),  in  such  a  way  that  the  electrical  power  losses  along  a  given  time  period  (T)  in  the  MG  are  minimized.  We  consider  a  situation  where  the  network  topology  is  already  defined  and  where  each  node  can  have  a  load  with  different  profiles  allocated.  The  consumption  profiles  are  real  measurements  of  different  types  (residential,  industrial,  etc.)  and  will  be  hourly  evaluated.  The  generations  profiles  are  also  real  measurement  data  from  different  generation  technologies.  We  consider  two  different  encodings  the  EAs:  first  a  binary-encoding  approach,  where  each  wind  generator  is  represented  by  2  bits  and  each  solar  generator  by  N  bits,  where  N  is  the  number  of  nodes  that  form  the  MG;  and  second,  an  integer-encoding  approach,  where  both  wind  and  PV  generators  are  represented  by  1  and  4  integer  elements,  respectively.  Experiments  are  performed  by  considering  three  different  MG  topologies,  with  different  number  of  nodes,  in  order  to  test  the  behavior  of  the  algorithms  with  search  spaces  of  increasing  size.  In  these  experimental  scenarios  we  show  how  the  binary  approach  attains  better  solutions  than  the  integer-encoding  approach,  tough  the  computational  time  of  the  former  is  higher.
0	Management  approaches  for  industry  4  0  a  human  resource  management  perspective.  Industry  4.0  is  characterized  by  smart  manufacturing,  implementation  of  Cyber  Physical  Systems  (CPS)  for  production,  i.e.,  embedded  actuators  and  sensors,  networks  of  microcomputers,  and  linking  the  machines  to  the  value  chain.  It  further  considers  the  digital  enhancement  and  reengineering  of  products.  It  is  also  characterized  by  highly  differentiated  customized  products,  and  well-coordinated  combination  of  products  and  services,  and  also  the  value  added  services  with  the  actual  product  or  service,  and  efficient  supply  chain.  All  these  challenges  require  continuous  innovation  and  learning,  which  is  dependent  on  people  and  enterprise's  capabilities.  Appropriate  management  approaches  can  play  a  vital  role  in  the  development  of  dynamic  capabilities,  and  effective  learning  and  innovation  climate.  This  paper  aims  at  offering  a  viewpoint  on  best  suitable  management  practices  which  can  promote  the  climate  of  innovation  and  learning  in  the  organization,  and  hence  facilitate  the  business  to  match  the  pace  of  industry  4.0.  This  paper  is  one  of  the  initial  attempts  to  draw  the  attention  towards  the  important  role  of  management  practices  in  industry  4.0,  as  most  of  the  recent  studies  are  discussing  the  technological  aspect.  This  paper  also  suggests  empirical  and  quantitative  investigation  on  these  management  approaches  in  the  context  of  industry  4.0.
0	Optimal  ensemble  classifiers  based  classification  for  automatic  vehicle  type  recognition.  In  this  work,  a  challenging  vehicle  type  classification  problem  for  automatic  toll  collection  task  is  considered,  which  is  currently  accomplished  with  an  Optical  Sensors  (OS)  and  corrected  manually.  Indeed,  the  human  operators  are  engaged  to  manually  correct  the  OS  misclassified  vehicles  by  observing  the  images  obtained  from  the  camera.  In  this  paper,  we  propose  a  novel  vehicle  classification  algorithm,  which  first  uses  the  camera  images  to  obtain  the  vehicle  class  probabilities  using  several  Convolutional  Neural  Networks  (CNNs)  models  and  then  uses  the  Gradient  Boosting  based  classifier  to  fuse  the  continuous  class  probabilities  with  the  discrete  class  labels  obtained  from  two  optical  sensors.  We  train  and  evaluate  our  method  using  a  challenging  dataset  collected  from  the  cameras  of  the  toll  collection  points.  Results  show  that  our  method  performs  significantly  (98.22%  compared  to  75.11%)  better  than  the  existing  automatic  toll  collection  system  and,  hence  will  vastly  reduce  the  workload  of  the  human  operators.  Moreover,  we  provide  an  in-depth  analysis  w.r.t.  the  learning  strategies:e.g.,  choice  of  the  optimization  algorithm  of  the  CNN  model.  Our  results  and  analysis  highlights  interesting  perspectives  and  challenges  for  the  future  work.
0	Genetic  algorithm  applied  in  uav  s  path  planning.  The  present  paper  introduces  a  hybrid  genetic  algorithm  for  path  planning  problem  with  obstacle  avoidance.  The  genetic  algorithm  is  combined  with  Ray  Casting  (RC)  algorithm,  where  RC  is  responsible  to  avoid  obstacles  and  to  find  safe  regions  for  emergency  landing.  Thus,  the  path  planning  system  must  deal  with  a  non-convex  environment  when  planning  and  re-planning  trajectories.  The  system  must  also  work  embedded  on  the  UAV  running  under  a  Raspberry  Pi  board.  The  hybrid  method  is  evaluated  over  50  benchmark  maps  from  literature  with  satisfactory  results  reported.
0	Topographic  analysis  of  correlated  components.  Independent  component  analysis  (ICA)  is  a  method  to  estimate  components  which  are  as  statistically  independent  as  possible.  However,  in  many  practical  applications,  the  estimated  components  are  not  independent.  Recent  variants  of  ICA  have  made  use  of  such  residual  dependencies  to  estimate  an  ordering  (topography)  of  the  components.  Like  in  ICA,  the  components  in  those  variants  are  assumed  to  be  uncorrelated,  which  might  be  a  rather  strict  condition.  In  this  paper,  we  address  this  shortcoming.  We  propose  a  generative  model  for  the  source  where  the  components  can  have  linear  and  higher  order  correlations,  which  generalizes  models  in  use  so  far.  Based  on  the  model,  we  derive  a  method  to  estimate  topographic  representations.  In  numerical  experiments  on  articial  data,  the  new  method  is  shown  to  be  more  widely  applicable  than  previously  proposed  extensions  of  ICA.  We  learn  topographic  representations  for  two  kinds  of  real  data  sets:  for  outputs  of  simulated  complex  cells  in  the  primary  visual  cortex  and  for  text  data.
0	Second  order  online  collaborative  filtering.  Collaborative  Filtering  (CF)  is  one  of  the  most  successful  learning  techniques  in  building  real-world  recommender  systems.  Traditional  CF  algorithms  are  often  based  on  batch  machine  learning  methods  which  suer  from  several  critical  drawbacks,  e.g.,  extremely  expensive  model  retraining  cost  whenever  new  samples  arrive,  unable  to  capture  the  latest  change  of  user  preferences  over  time,  and  high  cost  and  slow  reaction  to  new  users  or  products  extension.  Such  limitations  make  batch  learning  based  CF  methods  unsuitable  for  real-world  online  applications  where  data  often  arrives  sequentially  and  user  preferences  may  change  dynamically  and  rapidly.  To  address  these  limitations,  we  investigate  online  collaborative  ltering  techniques  for  building  live  recommender  systems  where  the  CF  model  can  evolve  on-the-y  over  time.  Unlike  the  regular  rst  order  CF  algorithms
0	Multi  label  connectionist  temporal  classification.  The  Connectionist  Temporal  Classification  (CTC)  loss  function  [1]  enables  end-to-end  training  of  a  neural  network  for  sequence-to-sequence  tasks  without  the  need  for  prior  alignments  between  the  input  and  output.  CTC  is  traditionally  used  for  training  sequential,  single-label  problems;  each  element  in  the  sequence  has  only  one  class.  In  this  work,  we  show  that  CTC  is  not  suitable  for  multi-label  tasks  and  we  present  a  novel  Multi-label  Connectionist  Temporal  Classification  (MCTC)  loss  function  for  multi-label,  sequence-to-sequence  classification.  Multi-label  classes  can  represent  meaningful  attributes  of  a  single  element;  for  example,  in  Optical  Music  Recognition  (OMR),  a  music  note  can  have  separate  duration  and  pitch  attributes.  Our  approach  achieves  state-of-the-art  results  on  Joint  Handwritten  Text  Recognition  and  Name  Entity  Recognition,  Asian  Character  Recognition,  and  OMR.
0	Content  extraction  from  lecture  video  via  speaker  action  classification  based  on  pose  information.  Online  lecture  videos  are  increasingly  important  e-learning  materials  for  students.  Automated  content  extraction  from  lecture  videos  facilitates  information  retrieval  applications  that  improve  access  to  the  lecture  material.  A  significant  number  of  lecture  videos  include  the  speaker  in  the  image.  Speakers  perform  various  semantically  meaningful  actions  during  the  process  of  teaching.  Among  all  the  movements  of  the  speaker,  key  actions  such  as  writing  or  erasing  potentially  indicate  important  features  directly  related  to  the  lecture  content.  In  this  paper,  we  present  a  methodology  for  lecture  video  content  extraction  using  the  speaker  actions.  Each  lecture  video  is  divided  into  small  temporal  units  called  action  segments.  Using  a  pose  estimator,  body  and  hands  skeleton  data  are  extracted  and  used  to  compute  motion-based  features  describing  each  action  segment.  Then,  the  dominant  speaker  action  of  each  of  these  segments  is  classified  using  Random  forests  and  the  motion-based  features.  With  the  temporal  and  spatial  range  of  these  actions,  we  implement  an  alternative  way  to  draw  key-frames  of  handwritten  content  from  the  video.  In  addition,  for  our  fixed  camera  videos,  we  also  use  the  skeleton  data  to  compute  a  mask  of  the  speaker  writing  locations  for  the  subtraction  of  the  background  noise  from  the  binarized  key-frames.  Our  method  has  been  tested  on  a  publicly  available  lecture  video  dataset,  and  it  shows  reasonable  recall  and  precision  results,  with  a  very  good  compression  ratio  which  is  better  than  previous  methods  based  on  content  analysis.
0	Towards  searchable  digital  urdu  libraries  a  word  spotting  based  retrieval  approach.  Libraries  in  South  Asia  hold  huge  collections  of  valuable  printed  documents  in  Urdu  and  it  is  of  interest  to  digitize  these  collections  to  make  them  more  accessible.  The  unavailability  of  an  OCR  for  Urdu  however  limits  the  concept  of  a  digital  Urdu  library  to  scanning  of  documents  only,  offering  very  limited  search  facility  based  on  manually  assigned  tags.  We  address  this  issue  by  proposing  a  word  spotting  based  keyword  search  method  for  information  retrieval  in  digitized  collections  of  printed  Urdu  documents.  The  proposed  method  is  based  on  segmentation  of  Urdu  text  in  to  partial  words  and  representing  each  partial  word  by  a  set  of  features.  To  search  a  specific  word  (or  phrase),  the  user  provides  a  query  in  the  form  of  an  image.  Comparing  the  features  of  the  partial  words  in  the  query  image  with  the  ones  already  indexed,  the  user  is  provided  with  a  list  of  documents  containing  occurrences  of  the  queried  word.  The  system  evaluated  on  50  Urdu  documents  exhibited  a  recall  of  95.17%  and  a  precision  of  94.3%.
0	Alif  a  dataset  for  arabic  embedded  text  recognition  in  tv  broadcast.  This  paper  proposes  a  dataset,  called  ALIF,  for  Arabic  embedded  text  recognition  in  TV  broadcast.  The  dataset  is  publicly  available  for  a  non-commercial  use.  It  is  composed  of  a  large  number  of  manually  annotated  text  images  that  were  extracted  from  Arabic  TV  broadcast.  It  is  the  first  public  dataset  dedicated  to  the  development  and  the  evaluation  of  video  Arabic  OCR  techniques.  Text  images  in  the  dataset  are  highly  variable  in  terms  of  text  characteristics  (fonts,  sizes,  colors…)  and  acquisition  conditions  (background  complexity,  low  resolution,  non-uniform  luminosity  and  contrast…).  Moreover,  an  important  part  of  the  dataset  is  finely  annotated,  i.e.  the  text  in  an  image  is  segmented  into  characters,  paws  and  words,  and  each  segment  is  labeled.  The  dataset  can  hence  be  used  for  both  segmentation-based  and  segmentation-free  text  recognition  techniques.  In  order  to  illustrate  how  the  ALIF  dataset  can  be  used,  the  results  of  an  evaluation  study  that  we  have  conducted  on  different  techniques  for  Arabic  text  recognition  are  also  presented.
0	Script  identification  of  pre  segmented  multi  font  characters  and  digits.  Character  recognition  problems  of  distinct  scripts  have  their  own  script  specific  characteristics.  The  state-of-art  optical  character  recognition  systems  use  different  methodolgies,  to  recognize  different  script  characters,  which  are  most  effective  for  the  corresponding  script.  The  identificaton  of  the  script  of  the  individual  character  has  not  brought  much  attention  between  researchers,  most  of  the  script  identification  work  is  on  document,  line  and  word  level.  In  this  multilingual/multiscript  world  presence  of  different  script  characters  in  a  single  document  is  very  common.  We  here  propose  a  system  to  encounter  such  adverse  situation  in  context  of  English  and  Gurumukhi  Script.  Experiments  on  multifont  and  multisized  characters  with  Gabor  features  based  on  directional  frequency  and  Gradient  features  based  on  gradient  information  of  an  individual  character  to  identify  it  as  Gurumukhi  or  English  and  also  as  character  or  numeral  are  reported  here.  Treating  it  as  four  class  classification  problem,  multi-class  Support  Vector  Machine(One  Vs  One)  has  been  used  for  classification.  We  got  promising  results  with  both  types  of  features.  The  average  identification  rates  obtained  with  Gabor  and  Gradient  features  are  98.9%  and  99.45%  respectively.
0	New  approach  for  symbol  recognition  combining  shape  context  of  interest  points  with  sparse  representation.  In  this  paper,  we  propose  a  new  approach  for  symbol  description.  Our  method  is  built  based  on  the  combination  of  shape  context  of  interest  points  descriptor  and  sparse  representation.  More  specifically,  we  first  learn  a  dictionary  describing  shape  context  of  interest  point  descriptors.  Then,  based  on  information  retrieval  techniques,  we  build  a  vector  model  for  each  symbol  based  on  its  sparse  representation  in  a  visual  vocabulary  whose  visual  words  are  columns  in  the  learned  dictionary.  The  retrieval  task  is  performed  by  ranking  symbols  based  on  similarity  between  vector  models.  The  evaluation  of  our  method,  using  benchmark  datasets,  demonstrates  the  validity  of  our  approach  and  shows  that  it  outperforms  related  state-of-the-art  methods.
0	Error  correction  with  in  domain  training  across  multiple  ocr  system  outputs.  Optical  character  recognition  (OCR)  systems  differ  in  the  types  of  errors  they  make,  particularly  in  recognizing  characters  from  degraded  or  poor  quality  documents.  The  problem  is  how  to  correct  these  OCR  errors,  which  is  the  first  step  toward  more  effective  use  of  the  documents  in  digital  libraries.  This  paper  demonstrates  the  degree  to  which  the  word  error  rate  (WER)  can  be  reduced  using  a  decision  list  on  a  combination  of  textual  features  across  the  aligned  output  of  multiple  OCR  engines  where  in-domain  training  data  is  available.  This  research  was  performed  on  a  data  set  for  which  the  mean  WER  across  the  three  OCR  engines  employed  is  33.5%,  and  the  lattice  word  error  rate  is  13.0%.  Our  correction  method  leads  to  a  52.2%  relative  decrease  in  the  mean  WER  and  a  19.5%  relative  improvement  over  the  best  single  OCR  engine,  as  well  as  an  improvement  over  our  previous  work.  Further,  our  method  yields  instances  where  the  document  WER  approaches  and  for  five  documents  matches  the  lattice  word  error  rate,  which  is  a  theoretical  lower  bound  given  the  evidence  found  in  the  OCR.
0	Are  multidimensional  recurrent  layers  really  necessary  for  handwritten  text  recognition.  Current  state-of-the-art  approaches  to  offline  Handwritten  Text  Recognition  extensively  rely  on  Multidimensional  Long  Short-Term  Memory  networks.  However,  these  architectures  come  with  quite  an  expensive  computational  cost,  and  we  observe  that  they  extract  features  visually  similar  to  those  of  convolutional  layers,  which  are  computationally  cheaper.  This  suggests  that  the  two-dimensional  long-term  dependencies,  which  are  potentially  modeled  by  multidimensional  recurrent  layers,  may  not  be  essential  to  achieve  a  good  recognition  accuracy,  at  least  in  the  lower  layers  of  the  architecture.  In  this  work,  an  alternative  model  is  explored  that  relies  only  on  convolutional  and  one-dimensional  recurrent  layers  that  achieves  better  or  equivalent  results  than  those  of  the  current  state-of-the-art  architecture,  and  runs  significantly  faster.  In  addition,  we  observe  that  using  random  distortions  during  training  as  synthetic  data  augmentation  dramatically  improves  the  accuracy  of  our  model.  Thus,  are  multidimensional  recurrent  layers  really  necessary  for  Handwritten  Text  Recognition?  Probably  not.
0	Classification  of  graphomotor  impressions  using  convolutional  neural  networks  an  application  to  automated  neuro  psychological  screening  tests.  Graphomotor  impressions  are  a  product  of  complex  cognitive,  perceptual  and  motor  skills  and  are  widely  used  as  psychometric  tools  for  the  diagnosis  of  a  variety  of  neuro-psychological  disorders.  Apparent  deformations  in  these  responses  are  quantified  as  errors  and  are  used  are  indicators  of  various  conditions.  Contrary  to  conventional  assessment  methods  where  manual  analysis  of  impressions  is  carried  out  by  trained  clinicians,  an  automated  scoring  system  is  marked  by  several  challenges.  Prior  to  analysis,  such  computerized  systems  need  to  extract  and  recognize  individual  shapes  drawn  by  subjects  on  a  sheet  of  paper  as  an  important  pre-processing  step.  The  aim  of  this  study  is  to  apply  deep  learning  methods  to  recognize  visual  structures  of  interest  produced  by  subjects.  Experiments  on  figures  of  Bender  Gestalt  Test  (BGT),  a  screening  test  for  visuo-spatial  and  visuo-constructive  disorders,  produced  by  120  subjects,  demonstrate  that  deep  feature  representation  brings  significant  improvements  over  classical  approaches.  The  study  is  intended  to  be  extended  to  discriminate  coherent  visual  structures  between  produced  figures  and  expected  prototypes.
0	Heask  robust  homography  estimation  based  on  appearance  similarity  and  keypoint  correspondences.  Abstract      Accurate  homography  estimation  is  a  classical  problem  with  high  industrial  value  and  has  been  investigated  extensively.  Most  previous  homography  estimation  methods  used  either  appearance  similarity  or  keypoint  correspondences  to  find  their  best  estimation.  In  this  paper,  a  novel  algorithm  is  proposed  which  integrates  the  advantages  of  the  pixel-based  and  the  feature-based  homography  estimation  approaches.  We  elegantly  combined  the  probability  models  of  appearance  similarity  and  keypoint  correspondences  in  a  Maximum  Likelihood  framework,  which  is  named  as  Homography  Estimation  based  on  Appearance  Similarity  and  Keypoint  correspondences  (HEASK).  In  the  model  of  keypoint  correspondences,  the  distribution  of  inlier  location  error  is  represented  by  a  Laplacian  distribution,  which  outperforms  the  previous  Gaussian  distribution  in  characterizing  heavy-tailed  distributions.  And  in  the  model  of  appearance  similarity,  the  enhanced  correlation  coefficient  (ECC)  is  adopted  for  describing  image  similarity,  and  the  distribution  of  ECC  is  studied  and  parametrically  formulated  using  a  truncated  exponential  distribution.  The  proposed  model  is  solved  based  on  an  improved  framework  of  random  sample  consensus  (RANSAC).  Several  simulations  summarize  the  performance  of  the  proposed  approach  in  objective  quality  measurement,  subjective  visual  quality,  and  computation  time.  The  experimental  results  demonstrate  that  the  proposed  approach  can  achieve  more  accurate  homography  estimation  under  different  image  transformation  degrees  and  with  different  ratios  of  inlier  keypoint  correspondences  as  compared  to  the  state-of-the-art  works.
0	Boundary  reconstruction  in  binary  images  using  splines.  In  image  analysis,  it  is  often  required  to  reconstruct  the  boundary  of  an  object  in  a  noisy  image.  This  paper  presents  a  new  method,  which  relies  on  flexibility  and  computational  simplicity  of  B-spline  curves,  to  reconstruct  a  smooth  connected  boundary  in  a  noisy  binary  image.  Boundary  inference  is  based  on  oriented  distance  functions  yielding  the  estimator  which  is  interpreted  as  a  posterior  expected  boundary  of  the  underlying  random  set.  The  performance  of  the  method  and  its  dependence  on  the  image  quality  and  model  specification  are  studied  on  simulated  data.  The  method  is  applied  to  reconstruct  the  skin-air  boundary  in  digitised  analogue  mammogram  images.  HighlightsWe  present  a  new  method  for  boundary  reconstruction  in  noisy  images  using  B-spline  curves.The  method  is  Bayesian  and  uses  oriented  distance  functions  for  inference.The  resulting  estimator  can  be  interpreted  as  a  posterior  expected  boundary.We  study  the  performance  of  the  method  on  simulated  data.We  apply  the  method  to  reconstruct  the  skin-air  boundary  in  mammogram  images.
0	An  adaptive  support  vector  regression  based  on  a  new  sequence  of  unified  orthogonal  polynomials.  In  practical  engineering,  small-scale  data  sets  are  usually  sparse  and  contaminated  by  noise.  In  this  paper,  we  propose  a  new  sequence  of  orthogonal  polynomials  varying  with  their  coefficient,  unified  Chebyshev  polynomials  (UCP),  which  has  two  important  properties,  namely,  orthogonality  and  adaptivity.  Based  on  these  new  polynomials,  a  new  kernel  function,  the  unified  Chebyshev  kernel  (UCK),  is  constructed,  which  has  been  proven  to  be  a  valid  SVM  kernel.  To  find  the  optimal  polynomial  coefficient  and  the  optimal  kernel,  we  propose  an  adaptive  algorithm  based  on  the  evaluation  criterion  for  adaptive  ability  of  UCK.  To  evaluate  the  performance  of  the  new  method,  we  applied  it  to  learning  some  benchmark  data  sets  for  regression,  and  compared  it  with  other  three  algorithms.  The  experiment  results  show  that  the  proposed  adaptive  algorithm  has  excellent  generalization  performance  and  prediction  accuracy,  and  does  not  cost  more  time  compared  with  other  SVMs.  Therefore,  this  method  is  suitable  for  practical  engineering  application.
0	Open  set  human  activity  recognition  based  on  micro  doppler  signatures.  Abstract  Open-set  activity  recognition  remains  as  a  challenging  problem  because  of  complex  activity  diversity.  In  previous  works,  extensive  efforts  have  been  paid  to  construct  a  negative  set  or  set  an  optimal  threshold  for  the  target  set.  In  this  paper,  a  model  based  on  Generative  Adversarial  Network  (GAN),  called  ‘OpenGAN’  is  proposed  to  address  the  open-set  recognition  without  manual  intervention  during  the  training  process.  The  generator  produces  fake  target  samples,  which  serve  as  an  automatic  negative  set,  and  the  discriminator  is  redesigned  to  output  multiple  categories  together  with  an  ‘unknown’  class.  We  evaluate  the  effectiveness  of  the  proposed  method  on  measured  micro-Doppler  radar  dataset  and  the  MOtion  CAPture  (MOCAP)  database  from  Carnegie  Mellon  University  (CMU).  The  comparison  results  with  several  state-of-the-art  methods  indicate  that  OpenGAN  provides  a  promising  open-set  solution  to  human  activity  recognition  even  under  the  circumstance  with  few  known  classes.  Ablation  studies  are  also  performed,  and  it  is  shown  that  the  proposed  architecture  outperforms  other  variants  and  is  robust  on  both  datasets.
0	Improving  classification  performance  of  breast  lesions  on  ultrasonography.  Several  morphological  and  texture  features  aiming  to  distinguish  between  benign  and  malignant  lesions  on  breast  ultrasound  (BUS)  have  been  proposed  in  the  literature.  Various  authors  also  claim  that  their  particular  feature  sets  are  capable  of  reaching  adequate  classification  rate.  However,  there  are  still  several  features  that  have  not  been  tested  together  for  determining  the  feature  set  that  effectively  improves  classification  performance.  Hence,  in  this  paper,  we  compiled  distinct  morphological  and  texture  features  widely  used  in  computer-aided  diagnosis  systems  for  BUS  images.  A  total  of  26  morphological  and  1465  texture  features  were  computed  from  641  BUS  images  (413  benign  and  228  malignant  lesions).  A  feature  selection  methodology,  based  on  mutual  information  and  statistical  tests,  was  used  to  evaluate  the  discrimination  power  of  distinct  feature  subsets.  The  .632+  bootstrap  method  was  used  to  estimate  the  classification  performance  of  each  feature  subset,  by  using  the  local  Fisher  discriminant  analysis  (LFDA),  with  linear  kernel,  as  classifier,  and  the  area  under  ROC  curve  (AUC)  as  performance  index.  The  experimental  results  indicated  that  the  best  classification  performance  is  AUC=0.942,  obtained  by  a  morphological  set  with  five  features.  In  addition,  this  morphological  set  outperformed  the  best  texture  set  with  four  features,  which  attained  AUC=0.897.  The  classification  performances  of  11  feature  sets  proposed  in  the  literature  were  also  surpassed  by  such  morphological  feature  set.  Highlights1491  features  are  evaluated  for  classifying  breast  lesions  on  ultrasound.Feature  selection  is  based  on  mutual  information  and  statistical  tests.5  morphological  and  4  texture  features  achieve  the  best  classification  performance.11  feature  sets  from  the  literature  are  surpassed  by  the  5  morphological  features.
0	Linear  reconstruction  measure  steered  nearest  neighbor  classification  framework.  The  linear  reconstruction  measure  (LRM),  which  determines  the  nearest  neighbors  of  the  query  sample  in  all  known  training  samples  by  sorting  the  minimum  L"2-norm  error  linear  reconstruction  coefficients,  is  introduced  in  this  paper.  The  intuitive  interpretation  and  mathematical  proofs  are  presented  to  reveal  the  efficient  working  mechanism  of  LRM.  Through  analyzing  the  physical  meaning  of  coefficients  and  regularization  items,  we  find  that  LRM  provides  more  useful  information  and  advantages  than  the  conventional  similarity  measure  model  which  calculates  the  distance  between  two  entities  (i.e.  conventional  point-to-point,  C-PtP).  Inspired  by  the  advantages  of  LRM,  the  linear  reconstruction  measure  steered  nearest  neighbor  classification  framework  (LRM-NNCF)  is  designed  with  eight  classifiers  according  to  different  decision  rules  and  models  of  LRM.  Evaluation  on  several  face  databases  and  the  experimental  results  demonstrate  that  these  proposed  classifiers  can  achieve  greater  performance  than  the  C-PtP  based  1-NNs  and  competitive  recognition  accuracy  and  robustness  compared  with  the  state-of-the-art  classifiers.
0	Semi  automatic  spline  fitting  of  planar  curvilinear  profiles  in  digital  images  using  the  hough  transform.  Abstract  We  develop  a  novel  method  for  the  recognition  of  curvilinear  profiles  in  digital  images.  The  proposed  method,  semi-automatic  for  both  closed  and  open  planar  profiles,  essentially  consists  of  a  preprocessing  step  exploiting  an  edge  detection  algorithm,  and  a  main  step  involving  the  Hough  transform  technique.  In  the  preprocessing  step,  a  Canny  edge  detection  algorithm  is  applied  in  order  to  obtain  a  reduced  point  set  describing  the  profile  curve  to  be  reconstructed.  Also,  to  identify  in  the  profile  possible  sharp  points  like  cusps,  we  additionally  use  an  algorithm  to  find  the  approximated  tangent  vector  of  every  edge  point.  In  the  subsequent  main  step,  we  then  use  a  piecewisely  defined  Hough  transform  to  locally  recognize  from  the  point  set  a  low-degree  piecewise  polynomial  curve.  The  final  outcome  of  the  algorithm  is  thus  a  spline  curve  approximating  the  underlined  profile  image.  The  output  curve  consists  of  polynomial  pieces  connected  G  1  continuously,  except  in  correspondence  of  the  identified  cusps,  where  the  order  of  continuity  is  only  C  0  ,  as  expected.  To  illustrate  effectiveness  and  efficiency  of  the  new  profile  detection  technique  we  present  several  numerical  results  dealing  with  detection  of  open  and  closed  profiles  in  images  of  different  type,  i.e.,  medical  and  photographic  images.
0	Coupling  and  decoupling  a  hierarchical  model  for  occlusion  free  object  detection.  Abstract      Handling  occlusion  is  a  very  challenging  problem  in  object  detection.  This  paper  presents  a  method  of  learning  a  hierarchical  model  for  X-to-X  occlusion-free  object  detection  (e.g.,  car-to-car  and  person-to-person  occlusions  in  our  experiments).  The  proposed  method  is  motivated  by  an  intuitive    coupling-and-decoupling    strategy.  In  the  learning  stage,  the  pair  of  occluding  X׳s  (e.g.,  car  pairs  or  person  pairs)  is  represented  directly  and  jointly  by  a  hierarchical  And–Or  directed  acyclic  graph  (AOG)  which  accounts  for  the  statistically  significant  co-occurrence  (i.e.,  coupling).  The  structure  and  the  parameters  of  the  AOG  are  learned  using  the  latent  structural  SVM  (LSSVM)  framework.  In  detection,  a  dynamic  programming  (DP)  algorithm  is  utilized  to  find  the  best  parse  trees  for  all  sliding  windows  with  detection  scores  being  greater  than  the  learned  threshold.  Then,  the  two  single  X׳s  are  decoupled  from  the  declared  detections  of  X-to-X  occluding  pairs  together  with  some  non-maximum  suppression  (NMS)  post-processing.  In  experiments,  our  method  is  tested  on  both  a  roadside-car  dataset  collected  by  ourselves  (which  will  be  released  with  this  paper)  and  two  public  person  datasets,  the  MPII-2Person  dataset  and  the  TUD-Crossing  dataset.  Our  method  is  compared  with  state-of-the-art  deformable  part-based  methods,  and  obtains  comparable  or  better  detection  performance.
0	Deep  multi  task  learning  with  relational  attention  for  business  success  prediction.  Abstract  Multi-task  learning  is  a  promising  machine  learning  branch,  which  aims  to  improve  the  generalization  of  the  prediction  models  by  sharing  knowledge  among  tasks.  Most  of  the  existing  multi-task  learning  methods  rely  on  predefined  task  relationships  and  guide  the  learning  process  of  models  by  linear  regularization  terms.  On  the  one  hand,  improper  setting  of  task  relationships  may  result  in  negative  knowledge  transfer;  on  the  other  hand,  these  methods  also  suffer  from  the  insufficiency  of  representation  ability.  To  overcome  these  problems,  this  paper  focuses  on  attention-based  deep  multi-task  learning  method,  and  provides  a  novel  deep  multi-task  learning  method,  namely,  Deep  Multi-task  Learning  with  Relational  Attention  (DMLRA).  In  particular,  we  first  provide  a  task-specific  attention  module  to  specify  features  for  different  learning  tasks,  because  different  prediction  tasks  may  rely  on  different  parts  of  the  shared  feature  set.  Then,  we  design  a  relational  attention  module  to  learn  relationships  among  multiple  tasks  automatically,  and  transfer  positive  and  negative  knowledge  among  multiple  tasks  accordingly.  Moreover,  we  provide  a  joint  deep  multi-task  learning  framework  to  combine  task-specific  module  and  relational  attention  module.  Finally,  we  apply  our  method  on  a  multi-criteria  business  success  assessment  problem,  both  classical  and  the  state-of-the-art  multi-task  learning  methods  are  employed  to  provide  baseline  performance.  The  experiments  are  conducted  on  real-world  datasets,  results  demonstrate  the  superiority  of  our  method  over  the  existing  methods.
0	An  adversarial  human  pose  estimation  network  injected  with  graph  structure.  Abstract  Because  of  the  invisible  human  keypoints  in  images  caused  by  illumination,  occlusion  and  overlap,  it  is  likely  to  produce  unreasonable  human  pose  prediction  for  most  of  the  current  human  pose  estimation  methods.  In  this  paper,  we  design  a  novel  generative  adversarial  network  (GAN)  to  improve  the  localization  accuracy  of  visible  joints  when  some  joints  are  invisible.  The  network  consists  of  two  simple  but  efficient  modules,  i.e.,  Cascade  Feature  Network  (CFN)  and  Graph  Structure  Network  (GSN).  First,  the  CFN  utilizes  the  prediction  maps  from  the  previous  stages  to  guide  the  prediction  maps  in  the  next  stage  to  produce  accurate  human  pose.  Second,  the  GSN  is  designed  to  contribute  to  the  localization  of  invisible  joints  by  passing  message  among  different  joints.  According  to  GAN,  if  the  prediction  pose  produced  by  the  generator  G  cannot  be  distinguished  by  the  discriminator  D  ,  the  generator  network  G  has  successfully  obtained  the  underlying  dependence  of  human  joints.  We  conduct  experiments  on  three  widely  used  human  pose  estimation  benchmark  datasets,  i.e.,  LSP,  MPII  and  COCO,  whose  results  show  the  effectiveness  of  our  proposed  framework.
0	A  novel  classification  selection  approach  for  the  self  updating  of  template  based  face  recognition  systems.  Abstract  The  boosting  on  the  need  of  security  notably  increased  the  amount  of  possible  facial  recognition  applications,  especially  due  to  the  success  of  the  Internet  of  Things  (IoT)  paradigm.  However,  although  handcrafted  and  deep  learning-inspired  facial  features  reached  a  significant  level  of  compactness  and  expressive  power,  the  facial  recognition  performance  still  suffers  from  intra-class  variations  such  as  ageing,  facial  expressions,  lighting  changes,  and  pose.  These  variations  cannot  be  captured  in  a  single  acquisition  and  require  multiple  acquisitions  of  long  duration,  which  are  expensive  and  need  a  high  level  of  collaboration  from  the  users.  Among  others,  self-update  algorithms  have  been  proposed  in  order  to  mitigate  these  problems.  Self-updating  aims  to  add  novel  templates  to  the  users’  gallery  among  the  inputs  submitted  during  system  operations.  Consequently,  computational  complexity  and  storage  space  tend  to  be  among  the  critical  requirements  of  these  algorithms.  The  present  paper  deals  with  the  above  problems  by  a  novel  template-based  self-update  algorithm,  able  to  keep  over  time  the  expressive  power  of  a  limited  set  of  templates  stored  in  the  system  database.  The  rationale  behind  the  proposed  approach  is  in  the  working  hypothesis  that  a  dominating  mode  characterises  the  features’  distribution  given  the  client.  Therefore,  the  key  point  is  to  select  the  best  templates  around  that  mode.  We  propose  two  methods,  which  are  tested  on  systems  based  on  handcrafted  features  and  deep-learning-inspired  autoencoders  at  the  state-of-the-art.  Three  benchmark  data  sets  are  used.  Experimental  results  confirm  that,  by  effective  and  compact  feature  sets  which  can  support  our  working  hypothesis,  the  proposed  classification-selection  approaches  overcome  the  problem  of  manual  updating  and,  in  case,  stringent  computational  requirements.
0	Mixture  of  grouped  regressors  and  its  application  to  visual  mapping.  Mixture  of  regressors  (MoR)  is  a  widely  used  regression  approach  for  approximating  nonlinear  mappings  between  input  and  target  outputs.  However,  existing  learning  procedures  for  MoR  are  prone  to  overfitting  when  only  limited  amounts  of  training  data  are  available.  To  address  this  problem,  we  propose  a  new  mixture  regression  model,  named  mixture  of  grouped  regressors  (MoGR).  It  partitions  the  individual  regressors  in  the  model  into  a  set  of  groups,  where  the  parameters  of  the  regressors  within  each  group  are  encouraged  to  take  on  similar  values.  As  the  parameters  for  each  local  regressor  are  learned  using  all  data  within  a  group,  they  tend  to  be  better  conditioned  and  more  robust  to  noise  in  the  training  data.  Extensive  experiments  on  real-world  head  pose  and  gaze  data  demonstrate  the  benefits  of  our  proposed  MoGR  model.  HighlightsMixture  of  grouped  regressors  (MoGR)  is  proposed  to  tackle  overfitting  problems  of  existing  Mixture  of  Regressors  methods.MoGR  partitions  the  individual  regressors  in  mixture  regression  model  into  a  number  of  groups.The  parameters  of  each  regressor  are  learned  using  all  data  within  a  group,  rather  than  a  cluster.MoGR  requires  a  small  number  of  training  data  and  is  robust  to  noise.It  has  shown  obviously  improved  performance  when  compared  with  state-of-the-art  nonlinear  visual  mapping  methods.
0	Memu  metric  correlation  siamese  network  and  multi  class  negative  sampling  for  visual  tracking.  Abstract  Despite  the  great  success  in  the  computer  vision  field,  visual  tracking  is  still  a  challenging  task.  The  main  obstacle  is  that  the  target  object  often  suffers  from  interference,  such  as  occlusion.  As  most  Siamese  network-based  trackers  mainly  sample  image  patches  of  target  objects  for  training,  the  tracking  algorithm  lacks  sufficient  information  about  the  surrounding  environment.  Besides,  many  Siamese  network-based  tracking  algorithms  build  a  regression  only  with  the  target  object  samples  without  considering  the  relationship  between  target  and  background,  which  may  deteriorate  the  performance  of  trackers.  In  this  paper,  we  propose  a  metric  correlation  Siamese  network  and  multi-class  negative  sampling  tracking  method.  For  the  first  time,  we  explore  a  sampling  approach  that  includes  three  different  kinds  of  negative  samples:  virtual  negative  samples  for  pre-learning  the  potential  occlusion  situation,  boundary  negative  samples  to  cope  with  potential  tracking  drift,  and  context  negative  samples  to  cope  with  potential  incorrect  positioning.  With  the  three  kinds  of  negative  samples,  we  also  propose  a  metric  correlation  method  to  train  a  correlation  filter  that  contains  metric  information  for  better  discrimination.  Furthermore,  we  design  a  Siamese  network-based  architecture  to  embed  the  metric  correlation  filter  module  mentioned  above  in  order  to  benefit  from  the  powerful  representation  ability  of  deep  learning.  Extensive  experiments  on  challenging  OTB100  and  VOT2017  datasets  demonstrate  the  competitive  performance  of  the  proposed  algorithm  performs  favorably  compared  with  state-of-the-art  approaches.
0	Treelet  kernel  incorporating  cyclic  stereo  and  inter  pattern  information  in  chemoinformatics.  Chemoinformatics  is  a  research  field  concerned  with  the  study  of  physical  or  biological  molecular  properties  through  computer  science's  research  fields  such  as  machine  learning  and  graph  theory.  From  this  point  of  view,  graph  kernels  provide  a  nice  framework  which  allows  to  naturally  combine  machine  learning  and  graph  theory  techniques.  Graph  kernels  based  on  bags  of  patterns  have  proven  their  efficiency  on  several  problems  both  in  terms  of  accuracy  and  computational  time.  Treelet  kernel  is  a  graph  kernel  based  on  a  bag  of  small  subtrees.  We  propose  in  this  paper  several  extensions  of  this  kernel  devoted  to  chemoinformatics  problems.  These  extensions  aim  to  weight  each  pattern  according  to  its  influence,  to  include  the  comparison  of  non-isomorphic  patterns,  to  include  stereo  information  and  finally  to  explicitly  encode  cyclic  information  into  kernel  computation.  HighlightsInclusion  of  chemical  information  into  treelet  kernel.Adaptation  of  multiple  kernel  learning  to  graph  kernels  based  on  bags  of  patterns.Two  new  molecular  representations  encoding  explicit  cyclic  information.New  relationship  between  maximum  structural  common  subgraph  and  graph  edit  distance.Stereoisomerism  is  encoded  in  treelet  kernel.
0	Hypergraph  based  image  retrieval  for  graph  based  representation.  In  this  paper,  we  introduce  a  novel  method  for  graph  indexing.  We  propose  a  hypergraph-based  model  for  graph  data  sets  by  allowing  cluster  overlapping.  More  precisely,  in  this  representation  one  graph  can  be  assigned  to  more  than  one  cluster.  Using  the  concept  of  the  graph  median  and  a  given  threshold,  the  proposed  algorithm  detects  automatically  the  number  of  classes  in  the  graph  database.  We  consider  clusters  as  hyperedges  in  our  hypergraph  model  and  we  index  the  graph  set  by  the  hyperedge  centroids.  This  model  is  interesting  to  traverse  the  data  set  and  efficient  to  retrieve  graphs.
0	Keyword  spotting  in  historical  handwritten  documents  based  on  graph  matching.  Abstract  In  the  last  decades  historical  handwritten  documents  have  become  increasingly  available  in  digital  form.  Yet,  the  accessibility  to  these  documents  with  respect  to  browsing  and  searching  remained  limited  as  full  automatic  transcription  is  often  not  possible  or  not  sufficiently  accurate.  This  paper  proposes  a  novel  reliable  approach  for  template-based  keyword  spotting  in  historical  handwritten  documents.  In  particular,  our  framework  makes  use  of  different  graph  representations  for  segmented  word  images  and  a  sophisticated  matching  procedure.  Moreover,  we  extend  our  method  to  a  spotting  ensemble.  In  an  exhaustive  experimental  evaluation  on  four  widely  used  benchmark  datasets  we  show  that  the  proposed  approach  is  able  to  keep  up  or  even  outperform  several  state-of-the-art  methods  for  template-  and  learning-based  keyword  spotting.
0	An  overview  of  ensemble  methods  for  binary  classifiers  in  multi  class  problems.  Classification  problems  involving  multiple  classes  can  be  addressed  in  different  ways.  One  of  the  most  popular  techniques  consists  in  dividing  the  original  data  set  into  two-class  subsets,  learning...
0	Improving  the  performance  of  lightweight  cnns  for  binary  classification  using  quadratic  mutual  information  regularization.  Abstract  In  this  paper,  we  propose  regularized  lightweight  deep  convolutional  neural  network  models,  capable  of  effectively  operating  in  real-time  on-drone  for  high-resolution  video  input.  Furthermore,  we  study  the  impact  of  hinge  loss  against  the  cross  entropy  loss  on  the  classification  performance,  mainly  in  binary  classification  problems.  Finally,  we  propose  a  novel  regularization  method  motivated  by  the  Quadratic  Mutual  Information,  in  order  to  improve  the  generalization  ability  of  the  utilized  models.  Extensive  experiments  on  various  binary  classification  problems  involved  in  autonomous  systems  are  performed,  indicating  the  effectiveness  of  the  proposed  models.  The  experimental  evaluation  on  four  datasets  indicates  that  hinge  loss  is  the  optimal  choice  for  binary  classification  problems,  considering  lightweight  deep  models.  Finally,  the  effectiveness  of  the  proposed  regularizer  in  enhancing  the  generalization  ability  of  the  proposed  models  is  also  validated.
0	A  dense  flow  based  framework  for  real  time  object  registration  under  compound  motion.  Abstract      A  moving  object  often  has  elastic  and  deformable  surfaces  (e.g.,  a  human  head).  Tracking  and  measuring  surface  deformation  while  the  object  itself  is  also  moving  is  a  challenging,  yet  important  problem  in  many  video  analysis  tasks.  For  example,  video-based  facial  expression  recognition  requires  tracking  non-rigid  motions  of  facial  features  without  being  affected  by  any  rigid  motions  of  the  head.  In  this  paper,  we  present  a  generic  video  alignment  framework  to  extract  and  characterize  surface  deformations  accompanied  by  rigid-body  motions  with  respect  to  a  fixed  reference  (a  canonical  form).  We  propose  a  generic  model  for  object  alignment  in  a  Bayesian  framework,  and  rigorously  show  that  a  special  case  of  the  model  results  in  a  SIFT  flow  and  optical  flow  based  least-square  problem.  We  demonstrate  that  dynamic  programming  can  be  used  to  speed  up  the  computation  of  our  algorithm.  The  proposed  algorithm  is  evaluated  on  three  applications,  including  the  analysis  of  subtle  facial  muscle  dynamics  in  spontaneous  expressions,  face  image  super-resolution,  and  generic  object  registration.  Experimental  results,  in  terms  of  both  qualitative  and  quantitative  measures,  demonstrate  the  efficacy  of  the  proposed  algorithm,  which  can  be  executed  in  real  time.
0	Blind  image  quality  assessment  via  learnable  attention  based  pooling.  Many  recent  algorithms  based  on  convolutional  neural  network  (CNN)  for  blind  image  quality  assessment  (BIQA)  share  a  common  two-stage  structure,  i.e.,  local  quality  measurement  followed  by  global  pooling.  In  this  paper,  we  mainly  focus  on  the  pooling  stage  and  propose  an  attention-based  pooling  network  (APNet)  for  BIQA.  The  core  idea  is  to  introduce  a  learnable  pooling  that  can  model  human  visual  attention  in  a  data-driven  manner.  Specifically,  the  APNet  is  built  by  incorporating  an  attention  module  and  allows  for  a  joint  learning  of  local  quality  and  local  weights.  It  can  automatically  learn  to  assign  visual  weights  while  generating  quality  estimations.  Moreover,  we  further  introduce  a  correlation  constraint  between  the  estimated  local  quality  and  attention  weight  in  the  network  to  regulate  the  training.  The  constraint  penalizes  the  case  in  which  the  local  quality  estimation  on  a  region  attracting  more  attention  differs  a  lot  from  the  overall  quality  score.  Experimental  results  on  benchmark  databases  demonstrate  that  our  APNet  achieves  state-of-the-art  prediction  accuracy.  By  yielding  an  attention  weight  map  as  by-product,  our  model  gives  a  better  interpretability  on  the  learned  pooling.  (C)  2019  Elsevier  Ltd.  All  rights  reserved.
0	A  novel  ensemble  method  for  classifying  imbalanced  data.  The  class  imbalance  problems  have  been  reported  to  severely  hinder  classification  performance  of  many  standard  learning  algorithms,  and  have  attracted  a  great  deal  of  attention  from  researchers  of  different  fields.  Therefore,  a  number  of  methods,  such  as  sampling  methods,  cost-sensitive  learning  methods,  and  bagging  and  boosting  based  ensemble  methods,  have  been  proposed  to  solve  these  problems.  However,  these  conventional  class  imbalance  handling  methods  might  suffer  from  the  loss  of  potentially  useful  information,  unexpected  mistakes  or  increasing  the  likelihood  of  overfitting  because  they  may  alter  the  original  data  distribution.  Thus  we  propose  a  novel  ensemble  method,  which  firstly  converts  an  imbalanced  data  set  into  multiple  balanced  ones  and  then  builds  a  number  of  classifiers  on  these  multiple  data  with  a  specific  classification  algorithm.  Finally,  the  classification  results  of  these  classifiers  for  new  data  are  combined  by  a  specific  ensemble  rule.  In  the  empirical  study,  different  class  imbalance  data  handling  methods  including  three  conventional  sampling  methods,  one  cost-sensitive  learning  method,  six  Bagging  and  Boosting  based  ensemble  methods,  our  previous  method  EM1vs1  and  two  fuzzy-rule  based  classification  methods  were  compared  with  our  method.  The  experimental  results  on  46  imbalanced  data  sets  show  that  our  proposed  method  is  usually  superior  to  the  conventional  imbalance  data  handling  methods  when  solving  the  highly  imbalanced  problems.  HighlightsWe  propose  a  novel  ensemble  method  to  handle  imbalanced  binary  data.The  method  turns  imbalanced  data  learning  into  multiple  balanced  data  learning.Our  method  usually  performs  better  than  the  conventional  methods  on  imbalanced  data.
0	A  survey  on  3d  mask  presentation  attack  detection  and  countermeasures.  Abstract  Despite  the  impressive  progress  in  face  recognition,  current  systems  are  vulnerable  to  presentation  attacks,  which  subvert  the  face  recognition  systems  by  presenting  a  face  artifact.  Several  techniques  have  been  developed  to  automatically  detect  different  presentation  attacks,  mostly  for  2D  photo  print  and  video  replay  attacks.  However,  with  the  development  of  3D  modeling  and  printing  technologies,  3D  mask  has  become  a  more  effective  way  to  attack  the  face  recognition  systems.  Over  the  last  decade,  various  detection  methods  for  3D  mask  attacks  have  been  proposed,  but  there  is  no  survey  yet  to  summarize  the  advances.  We  present  a  comprehensive  overview  of  the  state-of-the-art  approaches  in  3D  mask  spoofing  and  anti-spoofing,  including  existing  databases  and  countermeasures.  In  addition,  we  quantitatively  compare  the  performance  of  different  mask  spoofing  detection  methods  on  a  common  ground  (i.e.,  using  the  same  database  and  evaluation  metric).  The  effectiveness  of  several  2D  presentation  attack  detection  methods  is  also  evaluated  on  two  3D  mask  spoofing  databases  to  show  whether  they  are  applicable  or  not  for  3D  mask  attacks.  Finally,  we  present  some  insights  and  summarize  open  issues  to  address  in  the  future.
0	Image  decomposition  based  matrix  regression  with  applications  to  robust  face  recognition.  Abstract  The  previous  matrix  regression  based  methods  mainly  focus  on  designing  a  robust  error  term  to  characterize  the  occlusion  and  illumination  changes.  In  actually,  it  is  very  challenging  to  give  a  strong  model  for  solving  the  original  images  directly  since  the  images  contains  rich  and  complex  structure  information.  To  address  this  problem,  we  aim  to  simplify  the  complex  images  and  propose  a  simple  and  robust  matrix  regression  based  classification  model.  In  our  method,  we  firstly  employ  the  local  gradient  distribution  to  decompose  the  image  into  a  series  of  gradient  images  (LID  for  short).  Each  gradient  image  reveals  the  local  structure  information  in  different  gradient  orientations.  Subsequently,  we  consider  each  gradient  image  as  the  diagonal  block  element  and  construct  the  diagonal  block  matrix  for  image  representation.  Nuclear  norm  based  matrix  regression  model  (NMR)  is  then  applied  to  complete  the  classification  tasks.  The  proposed  model  can  be  called  ID-NMR  for  short.  We  further  design  a  fast  ADMM  optimization  algorithm  to  solve  the  proposed  ID-NMR  due  to  the  fact  that  the  big  diagonal  block  matrix  will  increase  the  computational  load.  Experimental  results  show  that  the  proposed  method  performs  favorably  compared  with  state-of-the-art  regression  based  classification  methods.
0	Examining  the  relationship  between  performance  feedback  and  emotions  in  diagnostic  reasoning  toward  a  predictive  framework  for  emotional  support.  The  purpose  of  this  research  is  to  understand  achievement  emotions  resulting  from  performance  feedback  in  a  medical  education  context  where  30  first  and  second  year  medical  students  learned  to  diagnose  virtual  patients  in  an  intelligent  tutoring  system  (ITS),  BioWorld.  We  found  that  students  could  be  organized  into  groups  using  cluster  analyses  based  on  the  emotions  they  reported  after  receiving  performance  feedback:  a  positive  emotion  cluster,  negative  emotion  cluster,  and  low  overall  emotion  cluster.  Medical  students  in  the  positive  achievement  emotion  cluster  had  the  highest  performance  on  the  diagnostic  reasoning  cases;  those  in  the  negative  achievement  emotion  cluster  had  the  lowest  performance;  and  students  categorized  as  belonging  to  the  low  overall  achievement  emotion  cluster  had  mean  performance  levels  that  fell  between  the  two.  From  the  results  we  propose  critical  performance  thresholds  that  can  be  used  to  predict  emotions  following  performance  feedback.
0	Exploring  the  impact  of  a  learning  dashboard  on  student  affect.  Research  highlights  that  many  students  experience  negative  emotions  during  learning  activities,  and  these  can  have  a  detrimental  impact  on  behaviors  and  outcomes.  Here,  we  investigate  the  impact  of  a  particular  kind  of  affective  intervention,  namely  a  learning  dashboard,  on  two  deactivating  emotions:  boredom  and  lack  of  excitement.  The  data  comes  from  a  study  we  conducted  with  over  200  middle  school  students  interacting  with  an  intelligent  tutor  that  provided  varying  levels  of  support  to  encourage  dashboard  use.  We  analyze  the  data  using  a  range  of  techniques  to  show  that  the  learning  dashboard  is  associated  with  reduced  deactivating  emotions,  but  that  its  utility  also  depends  on  the  way  its  use  is  promoted  and  on  students’  gender.
0	Exploring  through  simulation  an  instructional  planner  for  dynamic  open  ended  learning  environments.  Modern  online  courses  can  be  characterized  as  dynamic  open-ended  learning  environments  (DOELEs).  For  instructional  planning  to  work  in  DOELEs,  an  approach  is  needed  that  does  not  rely  on  data  structures  such  as  prerequisite  graphs  that  would  need  to  be  continually  rewired  as  the  LOs  change.  A  promising  approach  is  collaborative  filtering  based  on  learning  sequences  (CFLS)  using  the  ecological  approach  (EA)  architecture.  We  developed  a  CFLS  planner  that  compares  a  given  learner’s  most  recent  path  of  LOs  (of  length  \(b\))  to  other  learners  to  create  a  neighbourhood  of  similar  learners.  The  future  paths  (of  length  \(f\))  of  these  neighbours  are  checked  and  the  most  successful  path  ahead  is  recommended  to  the  target  learner,  who  then  follows  that  path  for  a  certain  length  (called  \(s\)).  An  experiment  with  simulated  learners  was  used  to  explore  what  are  the  best  values  of  \(b\),  \(f\)  and  \(s\).  Results  showed  that  the  CFLS  planner  should  avoid  sending  a  learner  any  further  ahead  (\(s\))  than  they  have  been  matched  in  the  past  (\(b\)),  a  prediction  that  can  be  applied  to  the  real  world.
0	Wire  watershed  based  iris  recognition.  Abstract      A  Watershed  transform  based  Iris  REcognition  system  (WIRE)  for  noisy  images  acquired  in  visible  wavelength  is  presented.  Key  points  of  the  system  are:  the  color/illumination  correction  pre-processing  step,  which  is  crucial  for  darkly  pigmented  irises  whose  albedo  would  be  dominated  by  corneal  specular  reflections;  the  criteria  used  for  the  binarization  of  the  watershed  transform,  leading  to  a  preliminary  segmentation  which  is  refined  by  taking  into  account  the  watershed  regions  at  least  partially  included  in  the  best  iris  fitting  circle;  the  introduction  of  a  new  cost  function  to  score  the  circles  detected  as  potentially  delimiting  limbus  and  pupil.  The  advantage  offered  by  the  high  precision  of  WIRE  in  iris  segmentation  has  a  positive  impact  as  regards  the  iris  code,  which  results  to  be  more  accurately  computed,  so  that  the  performance  of  iris  recognition  is  also  improved.  To  assess  the  performance  of  WIRE  and  to  compare  it  with  the  performance  of  other  available  methods,  two  well  known  databases  have  been  used,  specifically  UBIRIS  version  1  session  2  and  the  subset  of  UBIRIS  version  2  that  has  been  used  as  training  set  for  the  international  challenge  NICE  II.
0	Pseudo  low  rank  video  representation.  Abstract  Action  recognition  plays  a  fundamental  role  in  computer  vision  and  has  drawn  growing  attention  recently.  This  paper  addresses  this  issue  conditioned  on  extreme  Low  Resolution  (abbreviated  as  eLR).  Generally,  eLR  video  is  often  susceptible  to  noise,  thus  extracting  a  robust  representation  is  of  great  challenge.  Besides,  due  to  the  limitation  of  video  resolution,  eLR  video  cannot  be  cropped  or  resized  randomly,  then  it  is  inevitably  complicated  to  design  and  to  train  a  deep  network  for  eLR  video.  This  paper  proposes  a  novel  network  for  robust  video  representation  by  employing  pseudo  tensor  low  rank  regularization.  A  new  Video  Low  Rank  Representation  model  (named  VLRR)  is  first  proposed  to  recover  the  inherent  robust  component  of  a  given  video,  and  then  the  recovered  term  is  introduced  to  a  convolutional  Network  (denoted  pLRN)  as  an  auxiliary  pseudo  Low  Rank  guidance.  Benefitting  from  the  auxiliary  guidance,  pLRN  can  learn  an  approximate  low  rank  term  end-to-end.  Besides,  this  paper  presents  a  new  initialization  strategy  for  eLR  recognition  neTwork  based  on  Tensor  factorization  (dubbed  TenneT).  TenneT  is  data-driven  and  learns  the  convolutional  kernels  totally  from  the  video  distribution  while  without  any  back-propagation.  It  outperforms  random  initialization  both  in  speed  and  accuracy.  Experiments  on  benchmark  datasets  demonstrate  the  effectiveness  and  superiority  of  the  proposed  method.
0	Sql  superpixels  via  quaternary  labeling.  Abstract  This  paper  formulates  superpixel  segmentation  as  a  pixel  labeling  problem  and  proposes  a  quaternary  labeling  algorithm  to  generate  superpixel  lattice.  It  is  achieved  by  seaming  overlapped  patches  regularly  placed  on  the  image  plane.  Patch  seaming  is  formulated  as  a  pixel  labeling  problem,  where  each  label  indexes  one  patch.  Once  the  optimal  seaming  is  completed,  all  pixels  covered  by  one  retained  patch  constitute  one  superpixel.  Further,  four  kinds  of  patches  are  distinguished  and  assembled  into  four  layers  correspondingly,  and  the  patch  indexes  are  mapped  to  the  quaternary  layer  indexes.  It  significantly  reduces  the  number  of  labels  and  greatly  improves  labelling  efficiency.  Furthermore,  an  objective  function  is  developed  to  achieve  optimal  segmentation.  Lattice  structure  is  guaranteed  by  fixing  patch  centers  to  be  superpixel  centers,  compact  superpixels  are  assured  by  horizontal  and  vertical  constraints  enforced  on  the  smooth  terms,  and  coherent  superpixels  are  achieved  by  iteratively  refining  the  data  terms.  Extensive  experiments  on  BSDS  data  set  demonstrate  that  SQL  algorithm  significantly  improves  labeling  efficiency,  outperforms  the  other  superpixel  lattice  methods,  and  is  competitive  with  state-of-the-art  methods  without  lattice  guarantee.  Superpixel  lattice  allows  contextual  relationships  among  superpixels  to  be  easily  modeled  by  either  MRFs  or  CNN.
0	An  improved  online  writer  identification  framework  using  codebook  descriptors.  Abstract  This  work  proposes  a  text  independent  writer  identification  framework  for  online  handwritten  data.  We  derive  a  strategy  that  encodes  the  sequence  of  feature  vectors  extracted  at  sample  points  of  the  temporal  trace  with  descriptors  obtained  from  a  codebook.  The  derived  descriptors  take  into  account,  the  scores  of  each  of  the  attributes  in  a  feature  vector,  that  are  computed  with  regards  of  the  proximity  to  their  corresponding  values  in  the  assigned  codevector  of  the  codebook.  A  codebook  comprises  a  set  of  codevectors  that  are  pre-learnt  by  a  k-means  algorithm  applied  on  feature  vectors  of  handwritten  documents  pooled  from  several  writers.  In  addition,  for  constructing  the  codebook,  we  consider  features  that  are  derived  by  incorporating  a  so  called  ‘gap  parameter’  that  captures  characteristics  of  sample  points  in  the  neighborhood  of  the  point  under  consideration.  We  formulate  our  strategy  in  a  way  that,  for  a  given  codebook  size  k,  we  employ  the  descriptors  of  only  k  −  1  codevectors  to  construct  the  final  descriptor  by  concatenation.  The  usefulness  of  the  descriptor  is  demonstrated  by  several  experiments  that  are  reported  on  publicly  available  databases.
0	Fine  grained  analyses  of  interpersonal  processes  and  their  effect  on  learning.  Better  conversational  alignment  can  lead  to  shared  understanding,  changed  beliefs,  and  increased  rapport.  We  investigate  the  relationship  in  peer  tutoring  of  convergence,  interpersonal  rapport,  and  student  learning.  We  develop  an  approach  for  computational  modeling  of  convergence  by  accounting  for  the  horizontal  richness  and  time-based  dependencies  that  arise  in  non-stationary  and  noisy  longitudinal  interaction  streams.  Our  results,  which  illustrate  that  rapport  as  well  as  convergence  are  significantly  correlated  with  learning  gains,  provide  guidelines  for  development  of  peer  tutoring  agents  that  can  increase  learning  gains  through  subtle  changes  to  improve  tutor-tutee  alignment.
0	Student  agency  and  game  based  learning  a  study  comparing  low  and  high  agency.  A  key  feature  of  most  computer-based  games  is  agency:  the  capability  for  students  to  make  their  own  decisions  in  how  they  play.  Agency  is  assumed  to  lead  to  engagement  and  fun,  but  may  or  may  not  be  helpful  to  learning.  While  the  best  learners  are  often  good  self-regulated  learners,  many  students  are  not,  only  benefiting  from  instructional  choices  made  for  them.  In  the  study  presented  in  this  paper,  involving  a  total  of  158  fifth  and  sixth  grade  students,  children  played  a  mathematics  learning  game  called  Decimal  Point,  which  helps  middle-school  students  learn  decimals.  One  group  of  students  (79)  played  and  learned  with  a  low-agency  version  of  the  game,  in  which  they  were  guided  to  play  all  “mini-games”  in  a  prescribed  sequence.  The  other  group  of  students  (79)  played  and  learned  with  a  high-agency  version  of  the  game,  in  which  they  could  choose  how  many  and  in  what  order  they  would  play  the  mini-games.  The  results  show  there  were  no  significant  differences  in  learning  or  enjoyment  across  the  low  and  high-agency  conditions.  A  key  reason  for  this  may  be  that  students  across  conditions  did  not  substantially  vary  in  the  way  they  played,  perhaps  due  to  the  indirect  control  features  present  in  the  game.  It  may  also  be  the  case  that  the  young  students  who  participated  in  this  study  did  not  exercise  their  agency  or  self-regulated  learning.  This  work  is  relevant  to  the  AIED  community,  as  it  explores  how  game-based  learning  can  be  adapted.  In  general,  once  we  know  which  game  and  learning  features  lead  to  the  best  learning  outcomes,  as  well  as  the  circumstances  that  maximize  those  outcomes,  we  can  better  design  AI-powered,  adaptive  games  for  learning.
0	When  does  disengagement  correlate  with  learning  in  spoken  dialog  computer  tutoring.  We  investigate  whether  an  overall  student  disengagement  label  and  six  different  labels  of  disengagement  type  are  predictive  of  learning  in  a  spoken  dialog  computer  tutoring  corpus.  Our  results  show  first  that  although  students'  percentage  of  overall  disengaged  turns  negatively  correlates  with  the  amount  they  learn,  the  individual  types  of  disengagement  correlate  differently  with  learning:  some  negatively  correlate  with  learning,  while  others  don't  correlate  with  learning  at  all.  Second,  we  show  that  these  relationships  change  somewhat  depending  on  student  prerequisite  knowledge  level.  Third,  we  show  that  using  multiple  disengagement  types  to  predict  learning  improves  predictive  power.  Overall,  our  results  suggest  that  although  adapting  to  disengagement  should  improve  learning,  maximizing  learning  requires  different  system  interventions  depending  on  disengagement  type.
0	An  exploration  of  text  analysis  methods  to  identify  social  deliberative  skill.  We  report  on  text  processing  and  machine  learning  methods  with  the  goal  of  building  classifiers  for  social  deliberative  skill,  i.e.  the  capacity  to  deal  productively  with  heterogeneous  goals,  values,  or  perspectives.  Our  corpus  includes  online  deliberative  dialogue  from  three  diverse  domain  contexts.  We  use  the  LIWC  and  CohMetrix  linquistic  analysis  tools  to  generate  feature  sets  for  machine  learning.  We  report  on  our  evaluation  of  various  machine  learning  algorythms,  feature  selection  methods,  and  cross-domain  training  methods.
0	Higher  order  model  checking  from  theory  to  practice.  The  model  checking  of  higher-order  recursion  schemes  (higher-order  model  checking  for  short)  has  been  actively  studied  in  the  last  decade,  and  has  seen  significant  progress  in  both  theory  and  practice.  From  a  practical  perspective,  higher-order  model  checking  provides  a  foundation  for  software  model  checkers  for  functional  programming  languages  such  as  ML  and  Haskell.  This  short  article  aims  to  provide  an  overview  of  the  recent  progress  in  higher-order  model  checking  and  discuss  future  directions.
0	Approximate  span  liftings  compositional  semantics  for  relaxations  of  differential  privacy.  We  develop  new  abstractions  for  reasoning  about  three  relaxations  of  differential  privacy:  $R$  enyi  differential  privacy,  zero-concentrated  differential  privacy,  and  truncated  concentrated  differential  privacy,  which  express  bounds  on  statistical  divergences  between  two  output  probability  distributions.  In  order  to  reason  about  such  properties  compositionally,  we  introduce  approximate  span-lifting,  a  novel  construction  extending  the  approximate  relational  lifting  approaches  previously  developed  for  standard  differential  privacy  to  a  more  general  class  of  divergences,  and  also  to  continuous  distributions.  As  an  application,  we  develop  a  program  logic  based  on  approximate  span-liftings  capable  of  proving  relaxations  of  differential  privacy  and  other  statistical  divergence  properties.
0	Interaction  laws  of  monads  and  comonads.  We  introduce  and  study  functor-functor  and  monad-comonad  interaction  laws  as  mathematical  objects  to  describe  interaction  of  effectful  computations  with  behaviors  of  effect-performing  machines.  Monad-comonad  interaction  laws  are  monoid  objects  of  the  monoidal  category  of  functor-functor  interaction  laws.  We  show  that,  for  suitable  generalizations  of  the  concepts  of  dual  and  Sweedler  dual,  the  greatest  functor  resp.  monad  interacting  with  a  given  functor  or  comonad  is  its  dual  while  the  greatest  comonad  interacting  with  a  given  monad  is  its  Sweedler  dual.  We  relate  monad-comonad  interaction  laws  to  stateful  runners.  We  show  that  functor-functor  interaction  laws  are  Chu  spaces  over  the  category  of  endofunctors  taken  with  the  Day  convolution  monoidal  structure.  Hasegawa's  glueing  endows  the  category  of  these  Chu  spaces  with  a  monoidal  structure  whose  monoid  objects  are  monad-comonad  interaction  laws.
0	Tensor  decompositions  for  learning  latent  variable  models.  This  work  considers  a  computationally  and  statistically  efficient  parameter  estimation  method  for  a  wide  class  of  latent  variable  models--including  Gaussian  mixture  models,  hidden  Markov  models,  and  latent  Dirichlet  allocation--which  exploits  a  certain  tensor  structure  in  their  low-order  observable  moments  (typically,  of  second-  and  third-order).  Specifically,  parameter  estimation  is  reduced  to  the  problem  of  extracting  a  certain  (orthogonal)  decomposition  of  a  symmetric  tensor  derived  from  the  moments;  this  decomposition  can  be  viewed  as  a  natural  generalization  of  the  singular  value  decomposition  for  matrices.  Although  tensor  decompositions  are  generally  intractable  to  compute,  the  decomposition  of  these  specially  structured  tensors  can  be  efficiently  obtained  by  a  variety  of  approaches,  including  power  iterations  and  maximization  approaches  (similar  to  the  case  of  matrices).  A  detailed  analysis  of  a  robust  tensor  power  method  is  provided,  establishing  an  analogue  of  Wedin's  perturbation  theorem  for  the  singular  vectors  of  matrices.  This  implies  a  robust  and  computationally  tractable  estimation  approach  for  several  popular  latent  variable  models.
0	Kernel  bayes  rule  bayesian  inference  with  positive  definite  kernels.  A  kernel  method  for  realizing  Bayes'  rule  is  proposed,  based  on  representations  of  probabilities  in  reproducing  kernel  Hilbert  spaces.  Probabilities  are  uniquely  characterized  by  the  mean  of  the  canonical  map  to  the  RKHS.  The  prior  and  conditional  probabilities  are  expressed  in  terms  of  RKHS  functions  of  an  empirical  sample:  no  explicit  parametric  model  is  needed  for  these  quantities.  The  posterior  is  likewise  an  RKHS  mean  of  a  weighted  sample.  The  estimator  for  the  expectation  of  a  function  of  the  posterior  is  derived,  and  rates  of  consistency  are  shown.  Some  representative  applications  of  the  kernel  Bayes'  rule  are  presented,  including  Bayesian  computation  without  likelihood  and  filtering  with  a  nonparametric  state-space  model.
0	Fast  svm  training  using  approximate  extreme  points.  Applications  of  non-linear  kernel  support  vector  machines  (SVMs)  to  large  data  sets  is  seriously  hampered  by  its  excessive  training  time.  We  propose  a  modification,  called  the  approximate  extreme  points  support  vector  machine  (AESVM),  that  is  aimed  at  overcoming  this  burden.  Our  approach  relies  on  conducting  the  SVM  optimization  over  a  carefully  selected  subset,  called  the  representative  set,  of  the  training  data  set.  We  present  analytical  results  that  indicate  the  similarity  of  AESVM  and  SVM  solutions.  A  linear  time  algorithm  based  on  convex  hulls  and  extreme  points  is  used  to  compute  the  representative  set  in  kernel  space.  Extensive  computational  experiments  on  nine  data  sets  compared  AESVM  to  LIBSVM  (Chang  and  Lin,  2011),  CVM  (Tsang  et  al.,  2005),  BVM  (Tsang  et  al.,  2007),  LASVM  (Bordes  et  al.,  2005),  SVMperf  (Joachims  and  Yu,  2009),  and  the  random  features  method  (Rahimi  and  Recht,  2007).  Our  AESVM  implementation  was  found  to  train  much  faster  than  the  other  methods,  while  its  classification  accuracy  was  similar  to  that  of  LIBSVM  in  all  cases.  In  particular,  for  a  seizure  detection  data  set,  AESVM  training  was  almost  500  times  faster  than  LIBSVM  and  LASVM  and  20  times  faster  than  CVM  and  BVM.  Additionally,  AESVM  also  gave  competitively  fast  classification  times.
0	Embarrassingly  parallel  inference  for  gaussian  processes.  Training  Gaussian  process-based  models  typically  involves  an  $  O(N^3)$  computational  bottleneck  due  to  inverting  the  covariance  matrix.  Popular  methods  for  overcoming  this  matrix  inversion  problem  cannot  adequately  model  all  types  of  latent  functions,  and  are  often  not  parallelizable.  However,  judicious  choice  of  model  structure  can  ameliorate  this  problem.  A  mixture-of-experts  model  that  uses  a  mixture  of  $K$  Gaussian  processes  offers  modeling  flexibility  and  opportunities  for  scalable  inference.  Our  embarrassingly  parallel  algorithm  combines  low-dimensional  matrix  inversions  with  importance  sampling  to  yield  a  flexible,  scalable  mixture-of-experts  model  that  offers  comparable  performance  to  Gaussian  process  regression  at  a  much  lower  computational  cost.
0	A  group  theoretic  framework  for  data  augmentation.  Data  augmentation  is  a  widely  used  trick  when  training  deep  neural  networks:  in  addition  to  the  original  data,  properly  transformed  data  are  also  added  to  the  training  set.  However,  to  the  best  of  our  knowledge,  a  clear  mathematical  framework  to  explain  the  performance  benefits  of  data  augmentation  is  not  available.  In  this  paper,  we  develop  such  a  theoretical  framework.  We  show  data  augmentation  is  equivalent  to  an  averaging  operation  over  the  orbits  of  a  certain  group  that  keeps  the  data  distribution  approximately  invariant.  We  prove  that  it  leads  to  variance  reduction.  We  study  empirical  risk  minimization,  and  the  examples  of  exponential  families,  linear  regression,  and  certain  two-layer  neural  networks.  We  also  discuss  how  data  augmentation  could  be  used  in  problems  with  symmetry  where  other  approaches  are  prevalent,  such  as  in  cryo-electron  microscopy  (cryo-EM).
0	Gains  and  losses  are  fundamentally  different  in  regret  minimization  the  sparse  case.  We  demonstrate  that,  in  the  classical  non-stochastic  regret  minimization  problem  with  d  decisions,  gains  and  losses  to  be  respectively  maximized  or  minimized  are  fundamentally  different.  Indeed,  by  considering  the  additional  sparsity  assumption  (at  each  stage,  at  most  s  decisions  incur  a  nonzero  outcome),  we  derive  optimal  regret  bounds  of  different  orders.  Specifically,  with  gains,  we  obtain  an  optimal  regret  guarantee  after  T  stages  of  order  √T  log  s,  so  the  classical  dependency  in  the  dimension  is  replaced  by  the  sparsity  size.  With  losses,  we  provide  matching  upper  and  lower  bounds  of  order  √Ts  log(d)/d,  which  is  decreasing  in  d.  Eventually,  we  also  study  the  bandit  setting,  and  obtain  an  upper  bound  of  order  √Ts  log(d/s)  when  outcomes  are  losses.  This  bound  is  proven  to  be  optimal  up  to  the  logarithmic  factor  √log(d/s).
0	Locally  differentially  private  randomized  response  for  discrete  distribution  learning.  We  consider  a  setup  in  which  confidential  i.i.d.  samples  $X_1,\dotsc,X_n$  from  an  unknown  finite-support  distribution  $\boldsymbol{p}$  are  passed  through  $n$  copies  of  a  discrete  privatization  channel  (a.k.a.  mechanism)  producing  outputs  $Y_1,\dotsc,Y_n$.  The  channel  law  guarantees  a  local  differential  privacy  of  $\epsilon$.  Subject  to  a  prescribed  privacy  level  $\epsilon$,  the  optimal  channel  should  be  designed  such  that  an  estimate  of  the  source  distribution  based  on  the  channel  outputs  $Y_1,\dotsc,Y_n$  converges  as  fast  as  possible  to  the  exact  value  $\boldsymbol{p}$.  For  this  purpose  we  study  the  convergence  to  zero  of  three  distribution  distance  metrics:  $f$-divergence,  mean-squared  error  and  total  variation.  We  derive  the  respective  normalized  first-order  terms  of  convergence  (as  $n\to\infty$),  which  for  a  given  target  privacy  $\epsilon$  represent  a  rule-of-thumb  factor  by  which  the  sample  size  must  be  augmented  so  as  to  achieve  the  same  estimation  accuracy  as  that  of  a  non-randomizing  channel.  We  formulate  the  privacy-fidelity  trade-off  problem  as  being  that  of  minimizing  said  first-order  term  under  a  privacy  constraint  $\epsilon$.  We  further  identify  a  scalar  quantity  that  captures  the  essence  of  this  trade-off,  and  prove  bounds  and  data-processing  inequalities  on  this  quantity.  For  some  specific  instances  of  the  privacy-fidelity  trade-off  problem,  we  derive  inner  and  outer  bounds  on  the  optimal  trade-off  curve.
0	New  results  for  random  walk  learning.  In  a  very  strong  positive  result  for  passive  learning  algorithms,  Bshouty  et  al.  showed  that  DNF  expressions  are  efficiently  learnable  in  the  uniform  random  walk  model.  It  is  natural  to  ask  whether  the  more  expressive  class  of  thresholds  of  parities  (TOP)  can  also  be  learned  efficiently  in  this  model,  since  both  DNF  and  TOP  are  efficiently  uniform-learnable  from  queries.  However,  the  time  bounds  of  the  algorithms  of  Bshouty  et  al.  are  exponential  for  TOP.  We  present  a  new  approach  to  weak  parity  learning  that  leads  to  quasi-efficient  uniform  random  walk  learnability  of  TOP.  We  also  introduce  a  more  general  random  walk  model  and  give  two  positive  results  in  this  new  model:  DNF  is  efficiently  learnable  and  juntas  are  efficiently  agnostically  learnable.
0	Fast  rates  for  general  unbounded  loss  functions  from  erm  to  generalized  bayes.  We  present  new  excess  risk  bounds  for  general  unbounded  loss  functions  including  log  loss  and  squared  loss,  where  the  distribution  of  the  losses  may  be  heavy-tailed.  The  bounds  hold  for  general  estimators,  but  they  are  optimized  when  applied  to  η-generalized  Bayesian,  MDL,  and  empirical  risk  minimization  estimators.  In  the  case  of  log  loss,  the  bounds  imply  convergence  rates  for  generalized  Bayesian  inference  under  misspecification  in  terms  of  a  generalization  of  the  Hellinger  metric  as  long  as  the  learning  rate  η  is  set  correctly.  For  general  loss  functions,  our  bounds  rely  on  two  separate  conditions:  the  v-GRIP  (generalized  reversed  information  projection)  conditions,  which  control  the  lower  tail  of  the  excess  loss;  and  the  newly  introduced  witness  condition,  which  controls  the  upper  tail.  The  parameter  v  in  the  v-GRIP  conditions  determines  the  achievable  rate  and  is  akin  to  the  exponent  in  the  Tsybakov  margin  condition  and  the  Bernstein  condition  for  bounded  losses,  which  the  v-GRIP  conditions  generalize;  favorable  v  in  combination  with  small  model  complexity  leads  to  O(1/n)  rates.  The  witness  condition  allows  us  to  connect  the  excess  risk  to  an  “annealed”  version  thereof,  by  which  we  generalize  several  previous  results  connecting  Hellinger  and  Renyi  divergence  to  KL  divergence.
0	A  data  efficient  and  feasible  level  set  method  for  stochastic  convex  optimization  with  expectation  constraints.  Stochastic  convex  optimization  problems  with  expectation  constraints  (SOECs)  are  encountered  in  statistics  and  machine  learning,  business,  and  engineering.  In  data-rich  environments,  the  SOEC  objective  and  constraints  contain  expectations  defined  with  respect  to  large  datasets.  Therefore,  efficient  algorithms  for  solving  such  SOECs  need  to  limit  the  fraction  of  data  points  that  they  use,  which  we  refer  to  as  algorithmic  data  complexity.  Recent  stochastic  first  order  methods  exhibit  low  data  complexity  when  handling  SOECs  but  guarantee  near-feasibility  and  near-optimality  only  at  convergence.  These  methods  may  thus  return  highly  infeasible  solutions  when  heuristically  terminated,  as  is  often  the  case,  due  to  theoretical  convergence  criteria  being  highly  conservative.  This  issue  limits  the  use  of  first  order  methods  in  several  applications  where  the  SOEC  constraints  encode  implementation  requirements.  We  design  a  stochastic  feasible  level  set  method  (SFLS)  for  SOECs  that  has  low  data  complexity  and  emphasizes  feasibility  before  convergence.  Specifically,  our  level-set  method  solves  a  root-finding  problem  by  calling  a  novel  first  order  oracle  that  computes  a  stochastic  upper  bound  on  the  level-set  function  by  extending  mirror  descent  and  online  validation  techniques.  We  establish  that  SFLS  maintains  a  high-probability  feasible  solution  at  each  root-finding  iteration  and  exhibits  favorable  iteration  complexity  compared  to  state-of-the-art  deterministic  feasible  level  set  and  stochastic  subgradient  methods.  Numerical  experiments  on  three  diverse  applications  validate  the  low  data  complexity  of  SFLS  relative  to  the  former  approach  and  highlight  how  SFLS  finds  feasible  solutions  with  small  optimality  gaps  significantly  faster  than  the  latter  method.
0	Cyclic  steady  state  refinement.  The  paper  presents  a  new  modeling  framework  enabling  to  evaluate  the  cyclic  steady  state  of  a  given  system  of  concurrently  flowing  cyclic  processes  (SCCP)  on  the  base  of  the  assumed  topology  of  transportation  routes,  dispatching  rules  employed,  resources  and  operation  times  as  well  as  an  initial  processes  allocation.  The  objective  is  to  provide  the  rules  useful  in  the  course  of  routing  and  scheduling  executed  in  SCCP  where  local  cyclic  processes  interact  on  the  base  of  a  mutual  exclusion  protocol.
0	Shape  object  classification  using  echoes  inside  a  virtual  environment  application.  This  work  investigates  the  geometric  object-shape  classification  using  the  echoes  generated  by  various  kinds  of  obstacles  in  a  cellular  automata  based  virtual  environment  for  ultra-sound  propagation.  The  virtual  environment  is  implemented  as  a  JAVA  platform  [1]  capable  of  emulate  sound  propagation  in  a  controlled  2D  environment.  The  echoes  are  preprocessed  by  a  Feature  Processor  Vector  Unit  (FVPU)  and  then  classified  using  2  neural  networks:  Fast  Support  Vector  Classifier  (FSVC)  and  Extreme  Learning  Machine  (ELM).
0	Automatic  extraction  of  advice  revealing  sentences  foradvice  mining  from  online  forums.  Web  forums  often  contain  explicit  key  learnings  gleaned  from  people's  experiences  since  they  are  platforms  for  personal  communications  on  sharing  information  with  others.  One  of  the  key  learnings  contained  inWeb  forums  is  often  expressed  in  the  form  of  advice.  As  part  of  human  experience  mining  from  Web  resources,  we  aim  to  provide  a  methodology  to  extract  advice-revealing  sentences  from  Web  forums  due  to  its  usefulness,  especially  in  travel  domain.  Instead  of  viewing  the  problem  as  a  simple  classification,  we  define  it  as  a  sequence  labeling  problem  using  various  features.  We  identify  three  different  types  of  features  (i.e.,  syntactic  features,  context  features,  and  sentence  informativeness)  and  propose  a  new  way  of  using  Hidden  Markov  Model  (HMM)  for  labeling  sequential  sentences,  which  in  our  experiment  gave  the  best  performance  for  our  task.  Moreover,  the  sentence  informativeness  score  serves  as  an  important  feature  for  this  task.  It  is  worth  noting  that  this  work  is  the  first  attempt  to  extract  advice-revealing  sentences  from  Web  forums.
0	Bio  inspired  self  adaptive  agents  in  distributed  systems.  Normal      0                  21              false      false      false          EN-US      JA      X-NONE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /*  Style  Definitions  */  table.MsoNormalTable   	{mso-style-name:"Tabla  normal";   	mso-tstyle-rowband-size:0;   	mso-tstyle-colband-size:0;   	mso-style-noshow:yes;   	mso-style-priority:99;   	mso-style-parent:"";   	mso-padding-alt:0cm  5.4pt  0cm  5.4pt;   	mso-para-margin:0cm;   	mso-para-margin-bottom:.0001pt;   	mso-pagination:widow-orphan;   	font-size:12.0pt;   	font-family:Cambria;   	mso-ascii-font-family:Cambria;   	mso-ascii-theme-font:minor-latin;   	mso-hansi-font-family:Cambria;   	mso-hansi-theme-font:minor-latin;   	mso-ansi-language:EN-US;}                This  paper  proposes  a  bio-inspired  middleware  for  selfadaptive  software  agents  on  distributed  systems.  It  is  unique  to  other  existing  approaches  for  software  adaptation  because  it  introduces  the  notions  of  differentiation,  dedifferentiation,  and  cellular  division  in  cellular  slime  molds,  e.g.,  dictyostelium  discoideum,  into  real  distributed  systems.  When  an  agent  delegates  a  function  to  another  agent  coordinating  with  it,  if  the  former  has  the  function,  this  function  becomes  lessdeveloped  and  the  latter’s  function  becomes  welldeveloped.
0	Personalized  finance  advisory  through  case  based  recommender  systems  and  diversification  strategies.  Recommendation  of  financial  investment  strategies  is  a  complex  and  knowledge-intensive  task.  Typically,  financial  advisors  have  to  discuss  at  length  with  their  wealthy  clients  and  have  to  sift  through  several  investment  proposals  before  finding  one  able  to  completely  meet  investors'  needs  and  constraints.  As  a  consequence,  a  recent  trend  in  wealth  management  is  to  improve  the  advisory  process  by  exploiting  recommendation  technologies.  This  paper  proposes  a  framework  for  recommendation  of  asset  allocation  strategies  which  combines  case-based  reasoning  with  a  novel  diversification  strategy  to  support  financial  advisors  in  the  task  of  proposing  diverse  and  personalized  investment  portfolios.  The  performance  of  the  framework  has  been  evaluated  by  means  of  an  experimental  session  conducted  against  1172  real  users,  and  results  show  that  the  yield  obtained  by  recommended  portfolios  overcomes  that  of  portfolios  proposed  by  human  advisors  in  most  experimental  settings  while  meeting  the  preferred  risk  profile.  Furthermore,  our  diversification  strategy  shows  promising  results  in  terms  of  both  diversity  and  average  yield.  We  introduced  a  novel  framework  for  recommendation  of  asset  allocation  strategies.We  evaluated  the  effectiveness  of  CBRS  recommendation  strategies  in  a  special  and  not  yet  evaluated  domain.We  proposed  a  greedy  diversification  algorithm  able  to  diversify  the  investment  strategies  over  time.We  evaluated  the  effectiveness  of  the  framework  through  an  extensive  ex-post  evaluation.
0	Adoption  of  open  source  software  the  role  of  social  identification.  While  the  benefits  of  incorporating  Open  Source  Software  (OSS)  into  personal  and  organizational  systems  have  been  widely  touted,  OSS  must  be  adopted  and  used  by  end  users  before  these  benefits  can  be  realized.  Drawing  on  research  in  information  systems  and  sociology,  this  study  develops  and  evaluates  an  integrated  model  for  the  acceptance  of  OSS.  In  addition  to  the  traditional  technology  adoption  variables  the  findings  stress  the  importance  of  social  identification  as  a  key  driver  of  OSS  adoption.  The  proposed  model  provides  a  useful  decision  support  tool  for  assessing  and  proactively  designing  interventions  targeted  at  successful  OSS  adoption  and  diffusion.
0	An  experimental  study  of  software  agent  negotiations  with  humans.  Electronic  negotiations  allow  participants  to  negotiate  online  and  use  analytical  support  tools  in  making  their  decisions.  Software  agents  offer  the  possibility  of  automating  negotiation  process  using  these  tools.  This  paper  aims  at  investigating  the  prospects  of  agent-to-human  negotiations  using  experiments  with  human  subjects.  Various  types  of  agents  have  been  configured  using  the  following  tactics:  individualistic,  neutral,  yielding,  yielding-then-individualistic,  and  absolute  tit-for-tat.  These  agents  were  paired  up  with  human  counterparts  for  negotiating  product  sale.  A  set  of  hypotheses  has  been  proposed  involving  the  performance  of  agents,  as  well  as  humans  in  terms  of  objective,  as  well  as  subjective  measures.  Overall,  the  findings  speak  in  favor  of  agent-managed  negotiations.  An  experiment  in  software  agent  -  human  negotiations  is  described.Agents  employed  different  negotiation  tactics.The  negotiation  case  involving  purchase  of  a  computer  was  used.Subjects  were  university  students.Overall,  software  agents  outperformed  human  negotiators.
0	Ontology  supported  case  based  reasoning  approach  for  intelligent  m  government  emergency  response  services.  There  is  a  critical  need  to  develop  a  mobile-based  emergency  response  system  (MERS)  to  help  reduce  risks  in  emergency  situations.  Existing  systems  only  provide  short  message  service  (SMS)  notifications,  and  the  decision  support  is  weak,  especially  in  man-made  disaster  situations.  This  paper  presents  a  MERS  ontology-supported  case-based  reasoning  (OS-CBR)  method,  with  implementation,  to  support  emergency  decision  makers  to  effectively  respond  to  emergencies.  The  advantages  of  the  OS-CBR  approach  is  that  it  builds  a  case  retrieving  process,  which  provides  a  more  convenient  system  for  decision  support  based  on  knowledge  from,  and  solutions  provided  for  past  disaster  events.  The  OS-CBR  approach  includes  a  set  of  algorithms  that  have  been  successfully  implemented  in  four  components:  data  acquisition;  ontology;  knowledge  base;  and  reasoning;  as  a  sub-system  of  the  MERS  framework.  A  set  of  experiments  and  case  studies  validated  the  OS-CBR  approach  and  application,  and  demonstrate  its  efficiency.
0	Bankruptcy  prediction  for  smes  using  transactional  data  and  two  stage  multiobjective  feature  selection.  Abstract  Many  bankruptcy  prediction  models  for  small  and  medium-sized  enterprises  (SMEs)  are  built  using  accounting-based  financial  ratios.  This  study  proposes  a  bankruptcy  prediction  model  for  SMEs  that  uses  transactional  data  and  payment  network–based  variables  under  a  scenario  where  no  financial  (accounting)  data  are  required.  Offline  and  online  test  results  both  confirmed  the  predictive  capability  and  economic  benefit  of  transactional  data–based  variables.  However,  incorporating  those  features  in  predictive  models  produces  high  dimensional  problems,  which  deteriorates  model  interpretability  and  increases  feature  acquisition  costs.  Thus,  we  propose  a  two-stage  multiobjective  feature-selection  method  that  optimizes  the  number  of  features  as  well  as  model  classification  performance.  The  results  showed  that  the  proposed  model  achieved  similar  classification  performance  while  greatly  reducing  the  cardinality  of  the  feature  subset.  Finally,  the  feature  importance  evaluation  for  features  in  the  optimal  subset  confirmed  the  importance  of  transactional  data  and  payment  network-based  variables  for  bankruptcy  prediction.
0	Black  box  optimization  for  buildings  and  its  enhancement  by  advanced  communication  infrastructure.  The  solution  of  repeated  fixed-horizon  trajectory  optimization  problems  of  processes  that  are  either  too  difficult  or  too  complex  to  be  described  by  physics-based  models  can  pose  formidable  challenges.  Very  often,  soft-computing  methods    e.g.  black-box  modeling  and  evolutionary  optimization    are  used.  These  approaches  are  ineffective  or  even  computationally  intractable  for  searching  high-dimensional  parameter  spaces.  In  this  paper,  a  structured  iterative  process  is  described  for  addressing  such  problems:  the  starting  point  is  a  simple  parameterization  of  the  trajectory  starting  with  a  reduced  number  of  parameters;  after  selection  of  values  for  these  parameters  so  that  this  simpler  problem  is  covered  satisfactorily,  a  refinement  procedure  increases  the  number  of  parameters  and  the  optimization  is  repeated.  This  continuous  parameter  refinement  and  optimization  process  can  yield  effective  solutions  after  only  a  few  iterations.  To  illustrate  the  applicability  of  the  proposed  approach  we  investigate  the  problem  of  dynamic  optimization  of  the  operation  of  HVAC  (heating,  ventilation,  and  air  conditioning)  systems,  and  illustrative  simulation  results  are  presented.  Finally,  the  development  of  advanced  communication  and  interoperability  components  is  described,  addressing  the  problem  of  how  the  proposed  algorithm  could  be  deployed  in  realistic  contexts.
0	Intelligent  system  to  control  electric  power  distribution  networks.  The  use  of  high  voltage  power  lines  transport  involves  some  risks  that  may  be  avoided  with  periodic  reviews  as  imposed  by  law  in  most  countries.  The  objective  of  this  work  is  to  reduce  the  number  of  these  periodic  reviews  so  that  the  maintenance  cost  of  power  lines  is  also  reduced.  To  reduce  the  number  of  transmission  towers  (TT)  to  be  reviewed,  a  virtual  organization  (VO)  based  system  of  agents  is  proposed  in  conjunction  with  different  artificial  intelligence  methods  and  algorithms.  This  system  is  able  to  propose  a  sample  of  TT  from  a  selected  set  to  be  reviewed  and  to  ensure  that  the  whole  set  will  have  similar  values  without  needing  to  review  all  the  TT.  As  a  result,  the  system  provides  a  software  solution  to  manage  all  the  review  processes  and  all  the  TT  of  Spain,  allowing  the  review  companies  to  use  the  application  either  when  they  initiate  a  new  review  process  for  a  whole  line  or  area  of  TT,  or  when  they  want  to  place  an  entirely  new  set  of  TT,  in  which  case  the  system  would  recommend  the  best  place  and  the  best  type  of  structure  to  use.
0	A  new  intelligent  approach  to  aircrafts  take  off  landing  planning  at  congested  single  runway  airports.  Nowadays,  air  transportation  has  gained  a  significant  growth  due  to  its  advantages  in  transporting  goods  and  passengers.  The  rapid  growth  of  this  activity  and  some  limitations  in  different  parts  of  aviation  operation  often  cause  traffic  congestion  the  mismanagement  and  proper  planning  of  which  can  lead  to  a  lot  of  flight  delays;  accompanied  by  different  problems.  In  order  to  appropriately  systematize  air  traffic  congestion  various  researches  have  been  done  duringÂ   the  recent  two  decades  the  major  part  of  which  is  dealing  with  planning  of  aircrafts  taking-off  and  landing.  Thus,  in  the  current  study;  and  for  the  first  time,  the  two  algorithms  Biogeography-Based  Optimization  (BBO)  and  Particle  Swarm  Optimization  with  Constriction  Coefficient  (CPSO)  deal  with  a  feasible  planning  of  aircrafts  take-off  /landing,  taking  modern  conditions  and  limitations  into  account.  Simulations  prove  that  adding  rich  and  effective  knowledge  to  optimization  process  can,  to  a  large  extent,  undue  and  redundant  outcomes;  and  increase  convergence  rate  of  the  above  algorithms.  This  can  be  followed  by  over  50%  of  total  flight  delays  compared  with  First-Come/First-Serve  (FCFS)  plan.  Besides,  comparing  the  results  of  applying  the  two  new  optimization  algorithms  showed  that  BBO  can  be  more  effective  than  CPSO  because  of  its  better  research  domain.
0	Argumentation  based  negotiation  planning  for  autonomous  agents.  When  we  negotiate,  the  arguments  uttered  to  persuade  the  opponent  are  not  the  result  of  an  isolated  analysis,  but  of  an  integral  view  of  the  problem  that  we  want  to  agree  about.  Before  the  negotiation  starts,  we  have  in  mind  what  arguments  we  can  utter,  what  opponent  we  can  persuade,  which  negotiation  can  finish  successfully  and  which  cannot.  Thus,  we  plan  the  negotiation,  and  in  particular,  the  argumentation.  This  fact  allows  us  to  take  decisions  in  advance  and  to  start  the  negotiation  more  confidently.  With  this  in  mind,  we  claim  that  this  planning  can  be  exploited  by  an  autonomous  agent.  Agents  plan  the  actions  that  they  should  execute  to  achieve  their  goals.  In  these  plans,  some  actions  are  under  the  agent's  control,  while  some  others  are  not.  The  latter  must  be  negotiated  with  other  agents.  Negotiation  is  usually  carried  out  during  the  plan  execution.  In  our  opinion,  however,  negotiation  can  be  considered  during  the  planning  stage,  as  in  real  life.  In  this  paper,  we  present  a  novel  approach  to  integrate  argumentation-based  negotiation  planning  into  the  general  planning  process  of  an  autonomous  agent.  This  integration  allows  the  agent  to  take  key  decisions  in  advance.  We  evaluated  this  proposal  in  a  multiagent  scenario  by  comparing  the  performance  of  agents  that  plan  the  argumentation  and  agents  that  do  not.  These  evaluations  demonstrated  that  performance  improves  when  the  argumentation  is  planned,  specially,  when  the  negotiation  alternatives  increase.
0	Unsupervised  wrapper  induction  using  linked  data.  This  work  explores  the  usage  of  Linked  Data  for  Web  scale  Information  Extraction  and  shows  encouraging  results  on  the  task  of  Wrapper  Induction.  We  propose  a  simple  knowledge  based  method  which  is  (i)  highly  flexible  with  respect  to  different  domains  and  (ii)  does  not  require  any  training  material,  but  exploits  Linked  Data  as  background  knowledge  source  to  build  essential  learning  resources.  The  major  contribution  of  this  work  is  a  study  of  how  Linked  Data  -  an  imprecise,  redundant  and  large-scale  knowledge  resource  -  can  be  used  to  support  Web  scale  Information  Extraction  in  an  effective  and  efficient  way  and  identify  the  challenges  involved.  We  show  that,  for  domains  that  are  covered,  Linked  Data  serve  as  a  powerful  knowledge  resource  for  Information  Extraction.  Experiments  on  a  publicly  available  dataset  demonstrate  that,  under  certain  conditions,  this  simple  unsupervised  approach  can  achieve  competitive  results  against  some  complex  state  of  the  art  that  always  depends  on  training  data.
0	A  decision  support  system  for  patient  scheduling  in  travel  vaccine  administration.  The  administration  of  travel  vaccines  presents  a  number  of  operations  management  challenges.  The  interplay  between  shared  consumption  of  multi-dose  vaccine  packages,  rapid  spoilage  upon  opening,  the  high  cost  of  wastage,  and  the  unique  vaccination  needs  of  the  patients  makes  for  a  very  interesting  and  complex  scheduling  problem  that  could  benefit  from  computerized  decision  support.  We  compare  the  performance  of  a  novel  binary  integer  programming  model  and  a  genetic  algorithm  solution  technique  with  conventional  scheduling  approaches.  Computational  results  show  that  significant  cost  savings  can  be  achieved  with  the  DSS  while  simultaneously  considering  scheduling  preferences  of  patients  and  mitigating  scheduling  inconvenience.
0	Answering  queries  in  hybrid  bayesian  networks  using  importance  sampling.  In  this  paper  we  propose  an  algorithm  for  answering  queries  in  hybrid  Bayesian  networks  where  the  underlying  probability  distribution  is  of  class  MTE  (mixture  of  truncated  exponentials).  The  algorithm  is  based  on  importance  sampling  simulation.  We  show  how,  like  existing  importance  sampling  algorithms  for  discrete  networks,  it  is  able  to  provide  answers  to  multiple  queries  simultaneously  using  a  single  sample.  The  behaviour  of  the  new  algorithm  is  experimentally  tested  and  compared  with  previous  methods  existing  in  the  literature.
0	Hana  a  human  aware  negotiation  architecture.  In  this  paper  we  propose  HANA,  a  software  architecture  for  agents  that  need  to  bilaterally  negotiate  joint  plans  of  action  in  realistic  scenarios.  These  negotiations  may  involve  humans  and  are  repeated  along  time.  The  architecture  is  based  on  a  BDI  model  that  represents  the  uncertainty  on  the  environment  as  graded  beliefs,  desires  and  intentions.  The  architecture  is  modular  and  can  easily  be  extended  by  incorporating  different  models  (e.g.  trust,  intimacy,  personality,  normative...)  that  update  the  set  of  beliefs,  desires  or  intentions.  The  architecture  is  dynamic  as  it  monitors  the  environment  and  updates  the  beliefs  accordingly.  We  introduce  an  innovative  search&negotiation  method  that  facilitates  HANA  agents  to  cope  with  huge  spaces  of  joint  plans.  This  method  implements  an  anytime  search  algorithm  that  generates  partial  plans  to  feed  the  negotiation  process.  At  the  same  time  the  negotiation  guides  the  search  towards  joint  plans  that  are  more  likely  to  be  accepted.
0	A  decision  support  system  for  sustainable  energy  supply  combining  multi  objective  and  multi  attribute  analysis  an  australian  case  study.  A  framework  for  an  energy  supply  decision  support  system  (DSS)  for  sustainable  plant  design  and  production  is  presented  in  this  paper,  utilising  an  innovative  use  of  multi-objective  and  multi-attribute  decision-making  (MODM,  MADM)  modelling  together  with  impact  assessment  (IA)  of  the  emission  outputs.  The  mathematical  model  has  been  applied  within  an  eco-industrial  park  (EIP)  setting  and  includes  three  steps.  First,  an  assessment  of  the  total  EIP  emissions'  inventory  and  impacts  is  conducted;  the  second  step,  focusing  on  the  sustainability  benefits  of  combined  heating  and  power  (CHP)  plants  and  photovoltaic  technologies,  developed  a  multi-objective  mathematical  model  including  both  economic  and  environmental  objectives  in  a  Pareto-frontier  optimisation  analysis.  Four  different  scenarios  involving  combinations  of  CHP  plants  (internal  combustion  engine,  gas  turbine,  micro-turbines  and  fuel  cells)  and  two  types  of  PV  plant  (monocrystalline  and  polycrystalline)  were  evaluated.  The  third  step  utilises  a  MADM  methodology  -  the  analytic  hierarchy  process  (AHP)  -  for  selecting  the  best  alternative  among  the  Pareto-frontier  efficient  solutions.  This  model  has  been  applied  to  a  case  study  of  an  EIP  located  in  Perth  (Kwinana  Industrial  Area-KIA),  Western  Australia.
0	Detection  of  naming  convention  violations  in  process  models  for  different  languages.  Companies  increasingly  use  business  process  modeling  for  documenting  and  redesigning  their  operations.  However,  due  to  the  size  of  such  modeling  initiatives,  they  often  struggle  with  the  quality  assurance  of  their  model  collections.  While  many  model  properties  can  already  be  checked  automatically,  there  is  a  notable  gap  of  techniques  for  checking  linguistic  aspects  such  as  naming  conventions  of  process  model  elements.  In  this  paper,  we  address  this  problem  by  introducing  an  automatic  technique  for  detecting  violations  of  naming  conventions.  This  technique  is  based  on  text  corpora  and  independent  of  linguistic  resources  such  as  WordNet.  Therefore,  it  can  be  easily  adapted  to  the  broad  set  of  languages  for  which  corpora  exist.  We  demonstrate  the  applicability  of  the  technique  by  analyzing  nine  process  model  collections  from  practice,  including  over  27,000  labels  and  covering  three  different  languages.  The  results  of  the  evaluation  show  that  our  technique  yields  stable  results  and  can  reliably  deal  with  ambiguous  cases.  In  this  way,  this  paper  provides  an  important  contribution  to  the  field  of  automated  quality  assurance  of  conceptual  models.  We  present  an  automatic  technique  for  detecting  violations  of  naming  conventions.The  technique  is  based  on  text  corpora  and  independent  of  linguistic  resources.Because  of  its  design,  the  approach  can  be  easily  adapted  to  other  languages.The  evaluation  includes  27,000  labels  and  three  different  languages.
0	Simulation  of  fermentation  pathway  using  bees  algorithm.  Normal      0                  21              false      false      false          EN-US      JA      X-NONE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /*  Style  Definitions  */  table.MsoNormalTable   	{mso-style-name:"Tabla  normal";   	mso-tstyle-rowband-size:0;   	mso-tstyle-colband-size:0;   	mso-style-noshow:yes;   	mso-style-priority:99;   	mso-style-parent:"";   	mso-padding-alt:0cm  5.4pt  0cm  5.4pt;   	mso-para-margin:0cm;   	mso-para-margin-bottom:.0001pt;   	mso-pagination:widow-orphan;   	font-size:12.0pt;   	font-family:Cambria;   	mso-ascii-font-family:Cambria;   	mso-ascii-theme-font:minor-latin;   	mso-hansi-font-family:Cambria;   	mso-hansi-theme-font:minor-latin;   	mso-ansi-language:EN-US;}                In  this  paper,  we  propose  Bees  Algorithm  (BA)  to  enhance  the  performance  in  estimating  the  parameters  for  metabolic  pathway  data  to  simulate  fermentation  pathway  for  Saccharomyces  cerevisiae.  However,  the  parameter  estimation  of  biological  processes  has  always  been  a  challenging  task  due  to  the  complexity  and  nonlinear  equations.  Therefore,  we  present  this  algorithm  as  a  new  approach  for  parameter  estimation  for  biological  interactions  to  obtain  more  accurate  parameter  values.  The  result  shows  that  BA  outperforms  other  estimation  algorithms  as  it  produces  the  most  accurate  kinetic  parameters,  which  contributes  to  the  precision  of  simulated  kinetic  model.
0	A  control  liapunov  function  approach  to  generalized  and  regularized  descent  methods  for  zero  finding.  This  paper  revisits  a  class  of  recently  proposed  so-called  invariant  manifold  methods  for  zero  finding  of  ill-posed  problems,  showing  that  they  can  be  profitably  viewed  as  homotopy  methods,  in  which  the  homotopy  parameter  is  interpreted  as  a  learning  parameter.  Moreover,  it  is  shown  that  the  choice  of  this  learning  parameter  can  be  made  in  a  natural  manner  from  a  control  Liapunov  function  approach  CLF.  From  this  viewpoint,  maintaining  manifold  invariance  is  equivalent  to  ensuring  that  the  CLF  satisfies  a  certain  ordinary  differential  equation,  involving  the  learning  parameter,  that  allows  an  estimate  of  rate  of  convergence.  In  order  to  illustrate  this  approach,  algorithms  recently  proposed  using  the  invariant  manifold  approach,  are  rederived,  via  CLFs,  in  a  unified  manner.  Adaptive  regularization  parameters  for  solving  linear  algebraic  ill-posed  problems  were  also  proposed.  This  paper  also  shows  that  the  discretizations  of  the  ODEs  to  solve  the  zero  finding  problem,  as  well  as  the  different  adaptive  choices  of  the  regularization  parameter,  yield  iterative  methods  for  linear  systems,  which  are  also  derived  using  the  Liapunov  optimizing  control  LOC  method.
0	An  improved  satisfiable  sat  generator  based  on  random  subgraph  isomorphism.  We  introduce  Satisfiable  Random  High  Degree  Subgraph  Isomorphism  Generator(SRHD-SGI),  a  variation  of  the  Satisfiable  Random  Subgraph  Isomorphism  Generator  (SR-SGI).  We  use  the  direct  encoding  to  translate  the  SRHD-SGI  instances  into  Satisfiable  SAT  instances.  We  present  empirical  evidence  that  the  new  model  preserves  the  main  characteristics  of  SAT  encoded  SR-SGI:  easy-hard-easy  pattern  of  evolution  and  exponential  growth  of  empirical  hardness.  Our  experiments  indicate  that  SAT  encoded  SRHD-SGI  instances  are  empirically  harder  than  their  SR-SGI  counterparts.  Therefore  we  conclude  that  SRHD-SGI  is  an  improved  generator  of  satisfiable  SAT  instances.
0	Modelling  absence  epilepsy  seizure  data  in  the  neucube  evolving  spiking  neural  network  architecture.  Epilepsy  is  the  most  diffuse  brain  disorder  that  can  affect  people's  lives  even  on  its  early  stage.  In  this  paper,  we  used  for  the  first  time  the  spiking  neural  networks  (SNN)  framework  called  NeuCube  for  the  analysis  of  electroencephalography  (EEG)  data  recorded  from  a  person  affected  by  Absence  Epileptic  (AE),  using  permutation  entropy  (PE)  features.  Our  results  demonstrated  that  the  methodology  constitutes  a  valuable  tool  for  the  analysis  and  understanding  of  functional  changes  in  the  brain  in  term  of  its  spiking  activity  and  connectivity.  Future  applications  of  the  model  aim  at  personalised  modelling  of  epileptic  data  for  the  analysis  and  the  event  prediction.
0	Learning  minority  class  prior  to  minority  oversampling.  The  success  of  minority  oversampling  in  dealing  with  class-imbalanced  dataset  is  well  manifested  by  existing  approaches.  But  that  do  not  guarantee  the  true  class  of  the  synthetic  minority  points.  We  address  the  given  context  in  this  paper,  Learning  Minority  Class  prior  to  Minority  Oversampling  (LMCMO).  To  guarantee  the  class  information  of  synthetic  minority  points,  we  estimate  the  minority  spaces  before  generating  the  synthetic  minority  points.  The  performance  efficiency  of  the  LMCMO  oversampled  dataset  is  tested  on  C4.5  decision  tree  and  Linear  Support  Vector  Machine  (SVM)  classifier.  Empirical  evaluations  on  21  datasets  using  four  diversified  metrics  indicate  substantial  improvement  in  Linear  SVM  outcomes  of  the  proposed  method  over  four  competing  methods.  A  modest  but  still  significant  gain  is  achieved  by  our  method  over  other  methods  on  classification  using  C4.5  decision  tree.
0	Twitter  sentiment  classification  based  on  deep  random  vector  functional  link.  Social  networking  sites  like  Twitter,  Facebook,  Google  +  are  rapidly  gaining  popularity  as  they  allow  people  to  share  and  express  their  views  about  topics,  have  discussion  with  different  communities,  or  post  messages  across  the  world.  There  has  been  a  lot  of  work  in  the  field  of  sentiment  analysis  of  Twitter  data.  Randomization  based  methods  for  training  neural  networks  have  gained  increased  attention  in  recent  years  achieving  remarkable  performances  on  a  wide  variety  of  tasks.  The  idea  of  randomly  assigning  neural  network  parameters  is  shared  by  different  models  like  Random  Vector  Functional  Link  (RVFL)  networks,  the  Liquid  State  Machine  and  the  Feedforward  Neural  Network  with  Random  Weights.  We  propose  a  novel  non-iterative  deep  neural  network  using  RVFL  networks  called  Deep  RVFL  (D-RVFL).  We  evaluate  the  performance  of  D-RVFL  using  two  Twitter  datasets  (the  Catalan  referendum  of  2017  and  the  Chilean  earthquake  of  2010).  In  particular,  we  compare  the  classification  performance  of  D-RVFL  in  human  sentiment  with  Support  Vector  Machine  (SVM),  Random  Forest,  and  the  standard  RVFL.  The  results  confirm  the  advantages  of  using  the  proposed  method  for  sentiment  classification  in  Twitter  in  terms  of  the  $F_{1}$  score.
0	Estimating  vineyard  grape  yield  from  images.  Agricultural  yield  estimation  from  natural  images  is  a  challenging  problem  to  which  machine  learning  can  be  applied.  Convolutional  Neural  Networks  have  advanced  the  state  of  the  art  in  many  machine  learning  applications  such  as  computer  vision,  speech  recognition  and  natural  language  processing.  The  proposed  research  uses  convolution  neural  networks  to  develop  models  that  can  estimate  the  weight  of  grapes  on  a  vine  using  an  image.  Trained  and  tested  with  a  dataset  of  60  images  of  grape  vines,  the  system  manages  to  achieve  a  cross-validation  yield  estimation  accuracy  of  87%.
0	A  computer  simulation  approach  to  reduce  appointment  lead  time  in  outpatient  perinatology  departments  a  case  study  in  a  maternal  child  hospital.  A  significant  problem  in  outpatient  perinatology  departments  is  the  long  waiting  time  for  pregnant  women  to  receive  an  appointment.  In  this  respect,  appointment  delays  are  related  to  patient  dissatisfaction,  no  shows  and  sudden  infant  death  syndrome.  This  paper  aims  to  model  and  evaluate  improvement  proposals  to  outpatient  care  delivery  by  applying  computer  simulation  approaches.  First,  suitable  data  is  collected  and  analyzed.  Then,  a  discrete-event  simulation  (DES)  model  is  created  and  validated  to  determine  whether  it  is  statistically  equivalent  to  the  current  system.  Afterward,  the  average  appointment  lead-time  is  calculated  and  studied.  Finally,  improvement  proposals  are  designed  and  pretested  by  simulation  modelling  and  statistical  comparison  tests.  A  case  study  of  an  outpatient  perinatology  department  from  a  maternal-child  is  shown  to  validate  the  effectiveness  of  DES  to  fully  understand  and  manage  healthcare  systems.  The  results  evidenced  that  changes  to  care  delivery  can  be  effectively  assessed  and  appointment  lead-times  may  be  significantly  reduced  based  on  the  proposed  framework  within  this  paper.
0	A  space  frequency  localized  approach  of  spatial  filtering  for  motor  imagery  classification.  Classification  of  Motor  Imagery  (MI)  signals  is  the  heart  of  Brain-Computer  Interface  (BCI)  based  applications.  Spatial  filtering  is  an  important  step  in  this  process  that  produce  new  set  of  signals  for  better  discrimination  of  two  classes  of  EEG  signals.  In  this  work,  a  new  approach  of  spatial  filtering  called  Space-Frequency  Localized  Spatial  Filtering  (SFLSF)  is  proposed  to  enhance  the  performances  of  MI  classification.  The  SFLSF  method  initially  divides  the  scalp-EEG  channels  into  local  overlapping  spatial  windows.  Then  a  filter  bank  is  used  to  divide  the  signals  into  local  frequency  bands.  The  group  of  channels,  localized  in  space  and  frequency,  are  then  processed  with  spatial  filter,  and  features  are  subsequently  extracted  for  classification  task.  Experimental  results  corroborate  that  the  proposed  space  localization  helps  to  increase  the  classification  accuracy  when  compared  to  the  existing  methods  using  spatial  filters.  The  classification  performance  is  further  improved  when  frequency  localization  is  incorporated.  Thus,  the  proposed  space-frequency  localized  approach  of  spatial  filtering  helps  to  deliver  better  classification  result  which  is  consistently  3–5%  higher  than  traditional  methods.
0	A  time  window  neural  network  based  framework  for  remaining  useful  life  estimation.  This  paper  develops  a  framework  for  determining  the  Remaining  Useful  Life  (RUL)  of  aero-engines.  The  framework  includes  the  following  modular  components:  creating  a  moving  time  window,  a  suitable  feature  extraction  method  and  a  multi-layer  neural  network  as  the  main  machine  learning  algorithm.  The  proposed  framework  is  evaluated  on  the  publicly  available  C-MAPSS  dataset.  The  prognostic  accuracy  of  the  proposed  algorithm  is  also  compared  against  other  state-of-the-art  methods  available  in  the  literature  and  it  has  been  shown  that  the  proposed  framework  has  the  best  overall  performance.
0	Where  what  network  5  dealing  with  scales  for  objects  in  complex  backgrounds.  The  biologically-inspired  developmental  Where-What  Networks  (WWN)  are  general  purpose  visuomotor  networks  for  detecting  and  recognizing  objects  from  complex  backgrounds,  modeling  the  dorsal  and  ventral  streams  of  the  biological  visual  cortex.  The  networks  are  designed  for  the  attention  and  recognition  problem.  The  architecture  in  previous  versions  were  meant  for  a  single  scale  of  foreground.  This  paper  focuses  on  Where-What  Network-5  (WWN-5),  the  extension  for  multiple  scales.  WWN-5  can  learn  three  concepts  of  an  object:  type,  location  and  scale.
0	Bagging  adversarial  neural  networks  for  domain  adaptation  in  non  stationary  eeg.  A  major  issue  in  bringing  real-world  applications  of  machine  learning  outside  the  laboratory  is  the  difference  in  the  data  distributions  between  training  and  testing  stages  or  domains.  The  diverging  statistical  properties  in  different  domains  can  lead  to  decay  the  prediction  performance.  The  technical  term  for  a  change  in  the  distribution  of  features  is  covariate  shift,  which  also  happens  to  be  a  common  challenge  in  electroencephalogram  (EEG)  based  brain-computer  interface  (BCI);  this  is  due  to  the  presence  of  non-stationarities  in  the  EEG  signals.  It  is  also  the  case  that  collecting  and  labelling  samples  is  expensive,  resulting  in  small  datasets  that  are  not  in  tune  with  the  "big  data"  spirit  that  is  the  characteristic  of  the  era.  In  this  paper,  we  introduce  a  new  method  that  handles  domain  adaptation  in  small  datasets;  the  method  combines  elements  of  unsupervised  domain  adaptation  with  ensemble  methods.  We  evaluate  on  real-world  datasets  corresponding  to  motor-imagery  detection  (BCI  competition  2008  dataset  2A).  The  method  produces  state  of  the  art  results.
0	Weightless  neural  network  for  high  frequency  trading.  High  frequency  trading  depends  on  quick  reactions  to  meaningful  information.  In  order  to  identify  opportunities  in  intraday  negotiation  in  the  stock  markets,  we  propose  a  weightless  neural  network  autonomous  trader  agent  composed  by  forecasting  and  decision  modules.  The  forecasting  module  uses  ridge  regression,  which  compared  favorably  against  recursive  least  squares  with  exponential  forgetting.  The  decision  model  applies  the  predicted  prices  to  compute  technical  indicators  based  on  a  set  of  relative  strength  indicators  evaluated  by  back-testing,  which  are  then  used  to  train  the  weightless  neural  network  WiSARD  in  deciding  whether  to  buy  or  sell  stocks.  Experimental  results  on  a  real  dataset  from  the  Brazilian  stock  market  showed  that  it  is  feasible  encode  the  back-testing  in  WiSARD  in  order  to  improve  trading  rules  in  a  way  that  is  compatible  with  the  reaction  time  required  by  online  market  updates.
0	Bilinear  semi  tensor  product  attention  bstpa  model  for  visual  question  answering.  We  propose  a  semi-tensor  product  attention  network  model  as  a  visual  question  answering  tool  for  complex  interaction  over  image  features.  Proposed  model  performs  matrix  multiplication  of  two  arbitrary  dimensions,  which  is  used  to  overcome  possible  dimensional  limitations  and  improve  recognition  flexibility.  In  used  block-wise  operation  we  preserve  spatial  and  temporal  information  but  reduce  the  number  of  parameters  by  using  low-rank  pooling  scheme.  Applied  BERT  pre-train  model  is  tuned  to  recognize  question  features.  The  proposed  model  is  evaluated  on  the  VQA2.0  dataset.  Research  results  show  that  our  model  has  good  accuracy  and  easy  reconfiguration  for  future  research.
0	An  insect  brain  inspired  neural  model  for  object  representation  and  expectation.  In  spite  of  their  small  brain,  insects  show  a  complex  behavior  repertoire  and  are  becoming  a  reference  point  in  neuroscience  and  robotics.  In  particular,  it  is  very  interesting  to  analyze  how  biological  reaction-diffusion  systems  are  able  to  codify  sensorial  information  with  the  addition  of  learning  capabilities.  In  this  paper  we  propose  a  new  model  of  the  olfactory  system  of  the  fruit  fly  Drosophila  melanogaster.  The  architecture  is  a  multi-layer  spiking  neural  network,  inspired  by  the  structures  of  the  insect  brain  mainly  involved  in  the  olfactory  conditioning,  namely  the  Mushroom  Bodies,  the  Lateral  Horns  and  the  Antennal  Lobes.  The  Antennal  Lobes  model  is  based  on  a  competitive  topology  that  transduces  the  sensorial  information  into  a  pattern,  projecting  such  information  to  the  Mushroom  Bodies  model.  This  model  is  based  on  a  first  and  second  order  reaction-diffusion  paradigm  that  leads  to  a  spontaneous  emerging  of  clusters.  The  Lateral  Horns  have  been  modeled  as  an  input-triggered  resetting  system.  The  structure,  besides  showing  the  already  known  capabilities  of  associative  learning,  via  a  bottom-up  processing,  is  also  able  to  realize  a  top-down  modulation  at  the  input  level,  in  order  to  implement  an  expectation-based  filtering  of  the  sensorial  inputs.
0	Optimized  selection  of  training  samples  for  one  class  neural  network  classifier.  One-Class  Classification  (OCC)  based  on  the  Auto-Associative  Neural  Networks  (AANN)  has  been  widely  used  in  various  recognition  applications  for  its  effective  robustness.  Its  main  advantage  lies  in  the  description  of  samples  more  accurately  to  other  OCCs.  However,  it  is  considerably  sensitive  to  the  presence  of  outliers  or  noisy  data  contained  into  the  training  set,  which  may  affect  badly  the  representative  model.  Hence,  we  propose  in  this  paper  an  algorithm  that  uses  the  AANN  for  selecting  the  most  representative  training  samples.  The  same  AANN  is  retrained  to  reproduce  the  selected  samples  for  generating  an  optimal  representative  model.  The  experimental  evaluation  conducted  on  several  real-world  benchmarks  confirms  the  effective  use  of  the  Selected  Training  Samples  for  Associative  Neural  Network  (STS-AANN)  versus  the  training  on  the  entire  set.
0	Multidimensional  scaling  based  knowledge  provision  for  new  questions  in  community  question  answering  systems.  Community-based  Question  Answering  (CQA)  sites  have  become  popular  since  they  allow  users  to  get  answers  to  complex,  detailed  and  personal  question  from  other  users  directly.  However,  since  answering  a  question  depends  on  the  ability  and  willingness  of  other  users  to  address  the  askers'  real  needs,  a  significant  fraction  of  the  questions  remain  unanswered.  To  decrease  the  unanswered  question  rate  and  then  improve  the  user  experience,  in  this  paper,  a  multidimensional  scaling  (MDS)  based  data  reorganization  method  is  proposed.  By  using  this  method,  the  CQA  system  can  predict  the  askers'  intention  and  accordingly  provide  related  previous  question/answer  pairs  to  help  them  find  useful  information.  The  method  has  been  evaluated  on  an  off-line  dataset  extracted  from  Baidu  Zhidao  and  the  result  has  shown  its  promising  potential  in  knowledge  management  in  CQA  systems.
0	Connection  sparsity  versus  orbit  stability  in  dynamic  binary  neural  networks.  This  paper  considers  two  basic  problems  in  artificial  neural  networks  that  can  generate  various  binary  periodic  orbits.  The  first  problem  is  relation  between  sparsity  of  network  connection  and  stability  of  a  target  periodic  orbit.  The  second  problem  is  comparison  between  digital  circuits  and  artificial  neural  networks  in  the  orbit  stability.  We  consider  these  problems  in  dynamic  binary  neural  networks  characterized  by  the  signum  activation  function  and  ternary  connection  matrix.  Performing  basic  numerical  experiments,  we  give  conjectures  for  the  two  problems.  First,  as  the  connection  sparsity  increases,  the  orbit  stability  varies.  There  exists  suitable  sparsity  in  which  the  orbit  stability  is  very  strong.  Second,  as  the  connection  matrix  approaches  to  the  most  sparse  case,  the  dynamic  binary  neural  network  approaches  to  an  equivalent  system  to  the  shift  register  that  has  no  stable  periodic  orbit.
0	Multi  objective  cooperative  coevolution  of  neural  networks  for  time  series  prediction.  The  use  of  neural  networks  for  time  series  prediction  has  been  an  important  focus  of  recent  research.  Multi-objective  optimization  techniques  have  been  used  for  training  neural  networks  for  time  series  prediction.  Cooperative  coevolution  is  an  evolutionary  computation  method  that  decomposes  the  problem  into  subcomponents  and  has  shown  promising  results  for  training  neural  networks.  This  paper  presents  a  multi-objective  cooperative  coevolutionary  method  for  training  neural  networks  where  the  training  data  set  is  processed  to  obtain  the  different  objectives  for  multi-objective  evolutionary  training  of  the  neural  network.  We  use  different  time  lags  as  multi-objective  criterion.  The  trained  multi-objective  neural  network  can  give  prediction  of  the  original  time  series  for  preprocessed  data  sets  distinguished  by  their  time  lags.  The  proposed  method  is  able  to  outperform  the  conventional  cooperative  coevolutionary  methods  for  training  neural  networks  and  also  other  methods  from  the  literature  on  benchmark  problems.
0	A  generalized  asynchronous  digital  spiking  neuron  theoretical  analysis  and  compartmental  model.  The  most  generalized  version  of  asynchronous  sequential  logic  circuit  based  neuron  models  is  introduced,  where  the  dynamics  of  the  model  is  modeled  by  an  asynchronous  cellular  automaton.  In  this  paper,  a  new  theoretical  analysis  method  is  presented,  and  stabilities  of  neuron-like  orbits  and  occurrence  mechanisms  of  relational  neuron-like  bifurcations  are  clarified  theoretically.  A  synapse  unit  and  a  simple  compartmental  model  are  also  presented,  and  their  functions  are  confirmed  numerically.
0	Deep  process  neural  network  for  temporal  deep  learning.  Process  neural  network  is  widely  used  in  modeling  temporal  process  inputs  in  neural  networks.  Traditional  process  neural  network  is  usually  limited  in  structure  of  single  hidden  layer  due  to  the  unfavorable  training  strategies  of  neural  network  with  multiple  hidden  layers  and  complex  temporal  weights  in  process  neural  network.  Deep  learning  has  emerged  as  an  effective  pre-training  method  for  neural  network  with  multiple  hidden  layers.  Though  deep  learning  is  usually  limited  in  static  inputs,  it  provided  us  a  good  solution  for  training  neural  network  with  multiple  hidden  layers.  In  this  paper,  we  extended  process  neural  network  to  deep  process  neural  network.  Two  basic  structures  of  deep  process  neural  network  are  discussed.  One  is  the  accumulation  first  deep  process  neural  network  and  the  other  is  accumulation  last  deep  process  neural  network.  We  could  build  any  architecture  of  deep  process  neural  network  based  on  those  two  structures.  Temporal  process  inputs  are  represented  as  sequences  in  this  work  for  the  purpose  of  unsupervised  feature  learning  with  less  prior  knowledge.  Based  on  this,  we  proposed  learning  algorithms  for  two  basic  structures  inspired  by  the  numerical  learning  approach  for  process  neural  network  and  the  auto-encoder  in  deep  learning.  Finally,  extensive  experiments  demonstrated  that  deep  process  neural  network  is  effective  in  tasks  with  temporal  process  inputs.  Accuracy  of  deep  process  neural  network  is  higher  than  traditional  process  neural  network  while  time  complexity  is  near  in  the  task  of  traffic  flow  prediction  in  highway  system.
0	Significance  of  neural  phonotactic  models  for  large  scale  spoken  language  identification.  Language  identification  (LID)  is  vital  frontend  for  spoken  dialogue  systems  operating  in  diverse  linguistic  settings  to  reduce  recognition  and  understanding  errors.  Existing  LID  systems  which  use  low-level  signal  information  for  classification  do  not  scale  well  due  to  exponential  growth  of  parameters  as  the  classes  increase.  They  also  suffer  performance  degradation  due  to  the  inherent  variabilities  of  speech  signal.  In  the  proposed  approach,  we  model  the  language-specific  phonotactic  information  in  speech  using  recurrent  neural  network  for  developing  an  LID  system.  The  input  speech  signal  is  tokenized  to  phone  sequences  by  using  a  common  language-independent  phone  recognizer  with  varying  phonetic  coverage.  We  establish  a  causal  relationship  between  phonetic  coverage  and  LID  performance.  The  phonotactics  in  the  observed  phone  sequences  are  modeled  using  statistical  and  recurrent  neural  network  language  models  to  predict  language-specific  symbol  from  a  universal  phonetic  inventory.  Proposed  approach  is  robust,  computationally  light  weight  and  highly  scalable.  Experiments  show  that  the  convex  combination  of  statistical  and  recurrent  neural  network  language  model  (RNNLM)  based  phonotactic  models  significantly  outperform  a  strong  baseline  system  of  Deep  Neural  Network  (DNN)  which  is  shown  to  surpass  the  performance  of  i-vector  based  approach  for  LID.  The  proposed  approach  outperforms  the  baseline  models  in  terms  of  mean  F1  score  over  176  languages.  Further  we  provide  significant  information-theoretic  evidence  to  analyze  the  mechanism  of  the  proposed  approach.
0	Gated  recurrent  neural  tensor  network.  Recurrent  Neural  Networks  (RNNs),  which  are  a  powerful  scheme  for  modeling  temporal  and  sequential  data  need  to  capture  long-term  dependencies  on  datasets  and  represent  them  in  hidden  layers  with  a  powerful  model  to  capture  more  information  from  inputs.  For  modeling  long-term  dependencies  in  a  dataset,  the  gating  mechanism  concept  can  help  RNNs  remember  and  forget  previous  information.  Representing  the  hidden  layers  of  an  RNN  with  more  expressive  operations  (i.e.,  tensor  products)  helps  it  learn  a  more  complex  relationship  between  the  current  input  and  the  previous  hidden  layer  information.  These  ideas  can  generally  improve  RNN  performances.  In  this  paper,  we  proposed  a  novel  RNN  architecture  that  combine  the  concepts  of  gating  mechanism  and  the  tensor  product  into  a  single  model.  By  combining  these  two  concepts  into  a  single  RNN,  our  proposed  models  learn  long-term  dependencies  by  modeling  with  gating  units  and  obtain  more  expressive  and  direct  interaction  between  input  and  hidden  layers  using  a  tensor  product  on  3-dimensional  array  (tensor)  weight  parameters.  We  use  Long  Short  Term  Memory  (LSTM)  RNN  and  Gated  Recurrent  Unit  (GRU)  RNN  and  combine  them  with  a  tensor  product  inside  their  formulations.  Our  proposed  RNNs,  which  are  called  a  Long-Short  Term  Memory  Recurrent  Neural  Tensor  Network  (LSTMRNTN)  and  Gated  Recurrent  Unit  Recurrent  Neural  Tensor  Network  (GRURNTN),  are  made  by  combining  the  LSTM  and  GRU  RNN  models  with  the  tensor  product.  We  conducted  experiments  with  our  proposed  models  on  word-level  and  character-level  language  modeling  tasks  and  revealed  that  our  proposed  models  significantly  improved  their  performance  compared  to  our  baseline  models.
0	Gaining  extra  supervision  via  multi  task  learning  for  multi  modal  video  question  answering.  This  paper  proposes  a  method  to  gain  extra  supervision  via  multi-task  learning  for  multi-modal  video  question  answering.  Multi-modal  video  question  answering  is  an  important  task  that  aims  at  the  joint  understanding  of  vision  and  language.  However,  establishing  large  scale  dataset  for  multi-modal  video  question  answering  is  expensive  and  the  existing  benchmarks  are  relatively  small  to  provide  sufficient  supervision.  To  overcome  this  challenge,  this  paper  proposes  a  multi-task  learning  method  which  is  composed  of  three  main  components:  (1)  multi-modal  video  question  answering  network  that  answers  the  question  based  on  the  both  video  and  subtitle  feature,  (2)  temporal  retrieval  network  that  predicts  the  time  in  the  video  clip  where  the  question  was  generated  from  and  (3)  modality  alignment  network  that  solves  metric  learning  problem  to  find  correct  association  of  video  and  subtitle  modalities.  By  simultaneously  solving  related  auxiliary  tasks  with  hierarchically  shared  intermediate  layers,  the  extra  synergistic  supervisions  are  provided.  Motivated  by  curriculum  learning,  multi-task  ratio  scheduling  is  proposed  to  learn  easier  task  earlier  to  set  inductive  bias  at  the  beginning  of  the  training.  The  experiments  on  publicly  available  dataset  TVQA  shows  state-of-the-art  results,  and  ablation  studies  are  conducted  to  prove  the  statistical  validity.
0	Shift  invariant  convolutional  network  search.  The  development  of  Neural  Architecture  Search  (NAS)  makes  Convolutional  Neural  Networks  (CNN)  more  diverse  and  effective.  But  previous  NAS  approaches  don’t  pay  attention  to  the  shift-invariant  of  CNN.  Without  the  shift-invariant,  convolutional  network  is  not  robust  enough  when  input  data  is  disturbed  or  damaged.  Besides,  taking  accuracy  as  the  only  optimization  goal  of  NAS  cannot  meet  the  increasingly  diverse  needs.  In  this  paper,  we  propose  Shift-Invariant  Convolutional  Network  Search  (SICNS).  It  uses  one-shot  NAS  to  search  for  shift-invariant  convolutional  network  by  incorporating  the  low-pass  filter  into  the  one-shot  model.  Furthermore,  SICNS  optimizes  multiple  indicators  simultaneously  through  the  multi-objective  evolutionary  algorithm.  Through  training  one-shot  model  and  evolving  the  architecture,  we  obtain  convolutional  networks  which  are  robust  and  powerful  on  image  classification  task.  Especially,  our  work  can  achieve  4.52%  test  error  on  CIFAR-10  with  0.7M  parameters.  And  in  case  the  input  data  are  disturbed,  the  accuracy  of  searched  network  is  2.96%  higher  than  network  without  low-pass  filter.
0	Learning  optimization  for  decision  tree  classification  of  non  categorical  data  with  information  gain  impurity  criterion.  We  consider  the  problem  of  construction  of  decision  trees  in  cases  when  data  is  non-categorical  and  is  inherently  high-dimensional.  Using  conventional  tree  growing  algorithms  that  either  rely  on  univariate  splits  or  employ  direct  search  methods  for  determining  multivariate  splitting  conditions  is  computationally  prohibitive.  On  the  other  hand  application  of  standard  optimization  methods  for  finding  locally  optimal  splitting  conditions  is  obstructed  by  abundance  of  local  minima  and  discontinuities  of  classical  goodness  functions  such  as  e.g.  information  gain  or  Gini  impurity.  In  order  to  avoid  this  limitation  a  method  to  generate  smoothed  replacement  for  measuring  impurity  of  splits  is  proposed.  This  enables  to  use  vast  number  of  efficient  optimization  techniques  for  finding  locally  optimal  splits  and,  at  the  same  time,  decreases  the  number  of  local  minima.  The  approach  is  illustrated  with  examples.
0	Increasing  level  of  confidence  of  iris  biometric  matching.  A  key  requirement  of  biometric  matching  is  to  identify  people  with  high  level  of  confidence.  This  confidence  level  indicates  a  degree  that  the  biometric  system  can  reliably  decide  whether  a  query  biometric  template  belongs  to  a  registered  biometric  template  or  not.  This  paper  explores  the  use  of  a  template  transformation  and  the  longest  common  substring  expression  to  tackle  variability  during  biometric  acquisition  and  increase  the  level  of  confidence  of  iris  biometric  matching.  The  transformation  provided  with  a  derivative  of  the  registered  template  will  initially  convert  the  query  template  to  a  transformed  template  that  can  be  used  to  perform  an  exact  match.  When  the  transformation  has  been  performed,  the  longest  common  substring  between  the  registered  template  and  the  transformed  template  is  obtained.  The  proposed  transformation  will  cause  the  intra-class  distributions  more  homogeneous  and  push  the  distributions  to  a  large  similarity  while  the  proposed  longest  common  substring  will  push  the  inter-class  distributions  to  a  large  dissimilarity  thus  minimizing  the  chance  of  false  acceptances  and  increasing  the  separation  between  the  intra-class  and  inter-class  distributions.  We  extensively  tested  our  proposed  method  using  iris  images  from  the  commercial  Bath  dataset  and  found  that  the  decidability  index  (d′)  and  Fisher-ratio  can  be  increased  to  82.5  and  3401  respectively.  Moreover,  the  success  rate  of  the  transformation  to  produce  exact  matches  is  96.4%.
0	Predictive  coding  for  dynamic  vision  development  of  functional  hierarchy  in  a  multiple  spatio  temporal  scales  rnn  model.  The  current  paper  presents  a  novel  recurrent  neural  network  model,  predictive  multiple  spatio-temporal  scales  RNN  (P-MSTRNN),  which  can  generate  as  well  as  recognize  dynamic  visual  patterns  in  a  predictive  coding  framework.  The  model  is  characterized  by  multiple  spatio-temporal  scales  imposed  on  neural  unit  dynamics  through  which  an  adequate  spatio-temporal  hierarchy  develops  via  learning  from  exemplars.  The  model  was  evaluated  by  conducting  an  experiment  of  learning  a  set  of  whole  body  human  movement  patterns,  which  was  generated  by  following  a  hierarchically  defined  movement  syntax.  The  analysis  of  the  trained  model  clarifies  what  types  of  spatio-temporal  hierarchy  develops  in  dynamic  neural  activity  as  well  as  how  robust  generation  and  recognition  of  movement  patterns  can  be  achieved  by  using  the  error  minimization  principle.
0	Reservoir  computing  based  on  dynamics  of  pseudo  billiard  system  in  hypercube.  Reservoir  computing  (RC)  is  a  framework  for  constructing  recurrent  neural  networks  with  simple  training  rule  and  sparsely  and  randomly  connected  nonlinear  units.  The  network  (called  reservoir)  generates  complex  motion  that  can  be  used  for  many  tasks  including  time  series  generation  and  prediction.  We  construct  a  reservoir  based  on  the  dynamics  of  the  pseudo-billiard  system  that  produce  complex  motion  in  a  high-dimensional  hypercube.  In  particular,  we  use  the  chaotic  Boltzmann  machine  (CBM)  whose  units  exhibit  chaotic  behavior  in  the  hypercube.  The  units  interact  with  each  other  in  a  time-domain  manner  through  its  binary  state,  and  thus  an  efficient  hardware  implementation  of  the  system  is  expected.  In  order  to  utilize  the  CBM  as  the  reservoir,  it  is  necessary  to  control  its  chaotic  behavior  for  ensuring  the  echo  state  property  of  RC  and  establish  encoding  and  decoding  for  input  and  output  signal.  For  this  purpose,  we  introduce  a  reference  clock  and  analyze  effects  and  properties  of  the  reference  input.  We  evaluate  the  proposed  model  on  the  time  series  generation  tasks  and  show  that  the  model  works  properly  on  a  broad  range  of  parameter  values.  Our  approach  presents  a  novel  mechanism  for  time-domain  information  processing  and  a  fundamental  technology  for  a  brain  like  artificial  intelligence  system.
0	Mts  deepnet  for  lane  change  prediction.  Time  series  data  are  ubiquitous  and  are  of  importance  in  many  application  problems  in  engineering,  science,  medicine,  economics  and  entertainment.  Many  real  world  pattern  classification  problems  involve  the  processing  and  analysis  of  multiple  variables  in  the  temporal  domain.  These  types  of  problems  are  referred  to  as  Multivariate  Time  Series  (MTS)  problems.  In  many  real-world  applications,  an  MTS  problem  can  involve  a  large  number  of  signals,  and  require  algorithms  to  select  signals  and  extract  temporal  and  spatial  features  from  them.  In  this  paper,  we  present  an  innovative  convolutional  neural  network,  MTS-DeepNet  that  is  specially  designed  for  MTS  pattern  classification.  The  system  integrates  signal  and  feature  selections  with  MTS  pattern  classification  in  one  learning  framework.  MTS-DeepNet  is  applied  to  a  real-world  problem,  namely  predicting  driver  lane  departure  based  on  driver's  physiological  signals.  Our  experimental  results  showed  that,  in  comparison  to  a  multi-layer  neural  network  trained  with  the  backpropagation  algorithm,  MTS-DeepNet  gave  better  prediction  accuracy.
0	Transfer  learning  for  latin  and  chinese  characters  with  deep  neural  networks.  We  analyze  transfer  learning  with  Deep  Neural  Networks  (DNN)  on  various  character  recognition  tasks.  DNN  trained  on  digits  are  perfectly  capable  of  recognizing  uppercase  letters  with  minimal  retraining.  They  are  on  par  with  DNN  fully  trained  on  uppercase  letters,  but  train  much  faster.  DNN  trained  on  Chinese  characters  easily  recognize  uppercase  Latin  letters.  Learning  Chinese  characters  is  accelerated  by  first  pretraining  a  DNN  on  a  small  subset  of  all  classes  and  then  continuing  to  train  on  all  classes.  Furthermore,  pretrained  nets  consistently  outperform  randomly  initialized  nets  on  new  tasks  with  few  labeled  data.
0	Chaotic  quaternionic  associative  memory.  In  this  paper,  we  propose  a  chaotic  quaternionic  neuron  model  and  a  Chaotic  Quaternionic  Associative  Memory  (CQAM).  The  proposed  chaotic  quaternionic  neuron  model  is  based  on  the  chaotic  neuron  model  and  the  quaternionic  neuron  model.  In  the  chaotic  quaternionic  neuron  model,  if  the  parameters  are  set  appropriately,  chaotic  response  can  be  generated.  The  proposed  Chaotic  Quaternionic  Associative  Memory  is  composed  of  chaotic  quaternionic  neuron  models,  and  has  a  structure  which  is  similar  to  the  Hopfield  network.  In  the  proposed  Chaotic  Quaternionic  Associative  Memory,  plural  patterns  are  given  to  the  network  as  external  inputs  at  the  same  time,  each  pattern  can  be  recalled  separately.  The  proposed  Chaotic  Quaternionic  Associative  Memory  makes  use  of  the  dynamic  association  ability  of  the  chaotic  quaternionic  neuron  model  in  order  to  realize  pattern  separation.  We  carried  out  a  series  of  computer  experiments  and  confirmed  that  (1)  the  chaotic  quaternionic  neuron  can  generate  chaotic  response  when  the  parameters  are  set  appropriately  and  (2)  the  pattern  separation  can  be  realized  in  the  Chaotic  Quaternionic  Associative  Memory.
0	A  sandpile  model  for  reliable  actor  critic  reinforcement  learning.  Actor-Critic  algorithms  have  been  increasingly  researched  for  tackling  challenging  reinforcement  learning  problems.  These  algorithms  are  usually  composed  of  two  distinct  learning  processes,  namely  actor  (a.k.a,  policy)  learning  and  critic  (a.k.a,  value  function)  learning.  Actor  learning  is  heavily  dependent  on  critic  learning;  particularly  unreliable  critic  learning  due  to  its  divergence  can  significantly  affect  the  effectiveness  of  actor-critic  algorithms.  To  address  this  issue,  many  successful  algorithms  have  been  developed  recently  with  the  aim  of  improving  the  accuracy  of  value  function  approximation.  However,  these  algorithms  introduce  extra  complexities  to  the  learning  process  and  may  actually  increase  the  difficulty  for  effective  learning.  Thus,  in  this  research,  we  consider  a  simpler  approach  to  improving  the  critic  learning  reliability.  This  approach  requires  us  to  seamlessly  integrate  an  adapted  Sandpile  Model  with  the  critic  learning  process  so  as  to  achieve  desirable  self-organizing  property  for  reliable  critic  learning.  Following  this  approach,  we  propose  a  new  actor-critic  learning  algorithm.  Its  effectiveness  and  learning  reliability  have  been  further  evaluated  experimentally.  As  strongly  demonstrated  in  the  experiment  results,  our  new  algorithm  can  perform  much  better  than  traditional  actor-critic  algorithms.  Meanwhile,  correlation  analysis  further  suggests  that  a  strong  correlation  exists  in  between  learning  reliability  and  effectiveness.  This  finding  may  be  important  for  future  development  of  powerful  reinforcement  learning  algorithms.
0	Intruder  recognition  using  ecg  signal.  The  electrocardiogram  (ECG)  is  becoming  a  promising  technology  for  biometric  human  identification.  Usually  ECG  is  used  for  health  measurements  and  this  is  useful  for  biometric  applications  to  state  that  the  subject  under  analysis  is  live.  But  an  individual  identification  shouldn't  require  a  classical  ECG  clinical  analysis  where  several  contacts  are  applied  to  the  person  to  be  identified.  In  literature,  ECG  biometric  recognition  is  usually  studied  for  the  recognition  of  a  subject  within  a  group  of  known  subjects.  In  this  paper,  a  new  approach  is  considered.  The  aim  of  our  embedded  wearable  controller  is  to  authorize  a  subject  or  to  reject  him,  labeling  as  an  intruder  unknown  to  the  system.  The  study  used  40  healthy  subjects:  two  authorized  and  38  intruders.  A  one-lead  ECG  trace  has  been  recorded  from  the  wrists  of  subjects,  features  have  been  extracted  using  a  combination  of  Autocorrelation  and  Discrete  Cosine  Transform  (AC/DCT)  and  then  classified  using  a  Multilayer  Perceptron.  Results  show  that  intruder  recognition  can  be  performed  with  a  success  rate  equal  to  100%.
0	Dynamic  modeling  of  an  ostraciiform  robotic  fish  based  on  angle  of  attack  theory.  This  paper  focuses  on  the  dynamic  modeling  of  a  self-propelled,  multimodal  ostraciiform  robotic  fish,  whose  three  active  joints  (two  pectoral  fins  and  one  caudal  fin)  are  actuated  by  a  Central  Pattern  Generator  (CPG)  controller.  Compared  with  other  dynamic  modes  for  robotic  fish,  we  introduce  angle  of  attack  (AoA)  theory  on  the  fish  modeling,  which  can  be  used  to  further  explore  the  relationship  between  swimming  efficiency  and  AoA  of  robotic  fish.  First,  by  using  the  quasi-steady  wing  theory,  AoA  of  the  oscillatory  fins  are  explicitly  derived.  Then,  with  the  simplification  of  the  robot  as  a  multi-rigid-body  mechanism,  AoA-based  fluid  forces  acting  on  the  oscillatory  fins  of  the  robot  are  further  approximately  calculated  in  a  three-dimensional  context.  Next,  by  importing  the  driving  signals  (generated  by  CPG  control  law)  into  a  Lagrangian  function,  the  differential-algebraic  equations  are  employed  to  establish  a  hydrodynamic  model  for  steady  swimming  of  the  ostraciiform  robotic  fish  for  the  first  time.  Finally,  comparative  results  between  simulations  and  experiments  for  forward  and  turning  gaits  of  the  robot  are  systematically  conducted  to  show  the  effectiveness  of  the  built  AoA-based  dynamic  model.
0	Control  of  coffee  grinding  with  artificial  neural  networks.  Quality  assessment  and  standardization  of  the  property  of  the  final  product  is  fundamental  in  food  industry.  Coffee  particle  granulometry  and  density  are  continuously  monitored  during  coffee  beans  grinding  and  grinders  are  controlled  by  operators  in  order  to  keep  coffee  particle  granulometry  within  specific  thresholds.  In  this  work,  a  neural  system  is  used  to  learn  how  to  control  two  grinders  used  for  coffee  production  at  LAVAZZA  factory,  obtaining  average  control  error  of  the  order  of  a  few  µm.  The  results  appear  promising  for  the  future  development  of  an  automatic  decision  support  system.
0	Gpu  based  fast  parameter  optimization  for  phenomenological  spiking  neural  models.  A  significant  obstacle  in  using  phenomenological  models  of  spiking  neurons  for  large-scale  simulations  is  the  approximation  of  the  optimal  parameters  for  a  type  of  neuron,  given  the  available  experimental  data.  Here  we  show  a  method  for  optimizing  the  parameters  of  such  models,  based  on  a  combination  of  different  frequency-current  and  voltage-current  relations  of  a  neuron  as  well  as  known  physiological  properties.  We  also  present  a  python  toolbox  which  uses  NeMo  spiking  neural  network  simulator  and  provides  a  fast  GPU-based  implementation  of  our  method.  As  a  benchmark,  our  toolbox  was  used  to  fit  Izhikevich  equations  to  neurological  data  obtained  from  a  cat's  thalamic  relay  cell.  Our  resulting  model  was  able  to  predict  the  firing  patterns  of  known  membrane  potential  traces  of  this  neuron,  although  they  were  not  explicitly  defined  during  training.  A  further  comparison  between  this  neuron  model  and  a  previous  approach,  when  both  models  are  used  in  the  simulation  of  a  generic  thalamic  nucleus,  revealed  that  the  distribution  of  neuronal  avalanches  is  significantly  different  and  conforms  better  to  power  law-like  distributions,  thus  increasing  the  likelihood  of  a  critical  regime  and  the  biological  plausibility  of  the  simulation.
0	On  the  optimization  of  embedding  spaces  via  information  granulation  for  pattern  recognition.  Embedding  spaces  are  one  of  the  mainstream  approaches  when  dealing  with  structured  data.  Granular  Computing,  in  the  last  decade,  emerged  as  a  powerful  paradigm  for  the  automatic  synthesis  of  embedding  spaces  that,  at  the  same  time,  yield  an  interpretable  model  on  the  top  of  meaningful  entities  known  as  "information  granules".  Usually,  in  these  contexts,  one  aims  at  finding  the  smallest  set  of  information  granules  in  order  to  boost  the  model  interpretability  while  keeping  satisfactory  performances.  In  this  paper,  we  add  a  third  objective,  namely  the  structural  complexity  of  the  resulting  model  and  we  exploit  three  biology-related  case  studies  related  to  metabolic  networks  and  protein  networks  in  order  to  investigate  the  link  between  classification  performances,  embedding  space  dimensionality  and  structural  complexity  of  the  resulting  model.
0	Continuous  upper  confidence  trees  with  polynomial  exploration  consistency.  Upper  Confidence  Trees  (UCT)  are  now  a  well  known  algorithm  for  sequential  decision  making;  it  is  a  provably  consistent  variant  of  Monte-Carlo  Tree  Search.  However,  the  consistency  is  only  proved  in  a  the  case  where  the  action  space  is  finite.  We  here  propose  a  proof  in  the  case  of  fully  observable  Markov  Decision  Processes  with  bounded  horizon,  possibly  including  infinitely  many  states,  infinite  action  space  and  arbitrary  stochastic  transition  kernels.  We  illustrate  the  consistency  on  two  benchmark  problems,  one  being  a  legacy  toy  problem,  the  other  a  more  challenging  one,  the  famous  energy  unit  commitment  problem.
0	Context  modulation  of  sensor  data  applied  to  activity  recognition  in  smart  homes.  In  this  paper  we  present  a  method  of  modulating  the  context  of  data  captured  in  smart  homes.  We  show  that  we  can  dramatically  adapt  their  sensor  network  topology  and  that  this  approach  can  be  used  to  help  understand  various  aspects  of  such  sensor  environments.  We  demonstrate  how,  with  our  software,  we  can  discover  the  importance  of  individual  sensors,  clusters  of  sensors  and  sensor  categories  for  resident  identification  and  activity  recognition.  Finally,  we  validate  the  utility  of  context  modulation  in  a  number  of  experimental  scenarios  that  show  how  the  activity  recognition  is  affected  by  each  sensor  topology  elicited  by  these  scenarios.
0	A  game  theoretic  framework  for  data  privacy  preservation  in  recommender  systems.  We  address  the  fundamental  tradeoff  between  privacy  preservation  and  high-quality  recommendation  stemming  from  a  third  party.  Multiple  users  submit  their  ratings  to  a  third  party  about  items  they  have  viewed.  The  third  party  aggregates  the  ratings  and  generates  personalized  recommendations  for  each  user.  The  quality  of  recommendations  for  each  user  depends  on  submitted  rating  profiles  from  all  users,  including  the  user  to  which  the  recommendation  is  destined.  Each  user  would  like  to  declare  a  rating  profile  so  as  to  preserve  data  privacy  as  much  as  possible,  while  not  causing  deterioration  in  the  quality  of  the  recommendation  he  would  get,  compared  to  the  one  he  would  get  if  he  revealed  his  true  private  profile.    We  employ  game  theory  to  model  and  study  the  interaction  of  users  and  we  derive  conditions  and  expressions  for  the  Nash  Equilibrium  Point  (NEP).  This  consists  of  the  rating  strategy  of  each  user,  such  that  no  user  can  benefit  in  terms  of  improving  its  privacy  by  unilaterally  deviating  from  that  point.  User  strategies  converge  to  the  NEP  after  an  iterative  best-response  strategy  update.  For  a  hybrid  recommendation  system,  we  find  that  the  NEP  strategy  for  each  user  in  terms  of  privacy  preservation  is  to  declare  false  rating  only  for  one  item,  the  one  that  is  highly  ranked  in  his  private  profile  and  less  correlated  with  items  for  which  he  anticipates  recommendation.  We  also  present  various  modes  of  cooperation  by  which  users  can  mutually  benefit.
0	Learning  łukasiewicz  logic  fragments  by  quadratic  programming.  In  this  paper  we  provide  a  framework  to  embed  logical  constraints  into  the  classical  learning  scheme  of  kernel  machines,  that  gives  rise  to  a  learning  algorithm  based  on  a  quadratic  programming  problem.  In  particular,  we  show  that,  once  the  constraints  are  expressed  using  a  specific  fragment  from  the  Łukasiewicz  logic,  the  learning  objective  turns  out  to  be  convex.  We  formulate  the  primal  and  dual  forms  of  a  general  multi–task  learning  problem,  where  the  functions  to  be  determined  are  predicates  (of  any  arity)  defined  on  the  feature  space.  The  learning  set  contains  both  supervised  examples  for  each  predicate  and  unsupervised  examples  exploited  to  enforce  the  constraints.  We  give  some  properties  of  the  solutions  constructed  by  the  framework  along  with  promising  experimental  results.
0	Label  noise  robust  logistic  regression  and  its  applications.  The  classical  problem  of  learning  a  classifier  relies  on  a  set  of  labelled  examples,  without  ever  questioning  the  correctness  of  the  provided  label  assignments.  However,  there  is  an  increasing  realisation  that  labelling  errors  are  not  uncommon  in  real  situations.  In  this  paper  we  consider  a  label-noise  robust  version  of  the  logistic  regression  and  multinomial  logistic  regression  classifiers  and  develop  the  following  contributions:  (i)  We  derive  efficient  multiplicative  updates  to  estimate  the  label  flipping  probabilities,  and  we  give  a  proof  of  convergence  for  our  algorithm.  (ii)  We  develop  a  novel  sparsity-promoting  regularisation  approach  which  allows  us  to  tackle  challenging  high  dimensional  noisy  settings.  (iii)  Finally,  we  throughly  evaluate  the  performance  of  our  approach  in  synthetic  experiments  and  we  demonstrate  several  real  applications  including  gene  expression  analysis,  class  topology  discovery  and  learning  from  crowdsourcing  data.
0	The  spmf  open  source  data  mining  library  version  2.  SPMF  is  an  open-source  data  mining  library,  specialized  in  pattern  mining,  offering  implementations  of  more  than  120  data  mining  algorithms.  It  has  been  used  in  more  than  310  research  papers  to  solve  applied  problems  in  a  wide  range  of  domains  from  authorship  attribution  to  restaurant  recommendation.  Its  implementations  are  also  commonly  used  as  benchmarks  in  research  papers,  and  it  has  also  been  integrated  in  several  data  analysis  software  programs.  After  three  years  of  development,  this  paper  introduces  the  second  major  revision  of  the  library,  named  SPMF  2,  which  provides  (1)  more  than  60  new  algorithm  implementations  (including  novel  algorithms  for  sequence  prediction),  (2)  an  improved  user  interface  with  pattern  visualization  (3)  a  novel  plug-in  system,  (4)  improved  performance,  and  (5)  support  for  text  mining.
0	Harmonizing  maximum  likelihood  with  gans  for  multimodal  conditional  generation.  Recent  advances  in  conditional  image  generation  tasks,  such  as  image-to-image  translation  and  image  inpainting,  are  largely  accounted  to  the  success  of  conditional  GAN  models,  which  are  often  optimized  by  the  joint  use  of  the  GAN  loss  with  the  reconstruction  loss.  However,  we  reveal  that  this  training  recipe  shared  by  almost  all  existing  methods  causes  one  critical  side  effect:  lack  of  diversity  in  output  samples.  In  order  to  accomplish  both  training  stability  and  multimodal  output  generation,  we  propose  novel  training  schemes  with  a  new  set  of  losses  named  moment  reconstruction  losses  that  simply  replace  the  reconstruction  loss.  We  show  that  our  approach  is  applicable  to  any  conditional  generation  tasks  by  performing  thorough  experiments  on  image-to-image  translation,  super-resolution  and  image  inpainting  using  Cityscapes  and  CelebA  dataset.  Quantitative  evaluations  also  confirm  that  our  methods  achieve  a  great  diversity  in  outputs  while  retaining  or  even  improving  the  visual  fidelity  of  generated  samples.
0	Towards  an  integrated  approach  to  crowd  analysis  and  crowd  synthesis.  An  adaptive  model  for  the  simulation  of  groups  of  pedestrians  able  to  preserve  their  cohesion.An  observation  and  analysis  of  groups  of  pedestrians  in  a  real  world  situation.An  integrated  framework  for  crowd  analysis  and  synthesis.  Studies  related  to  crowds  of  pedestrians,  both  those  of  theoretical  nature  and  application  oriented  ones,  have  generally  focused  on  either  the  analysis  or  the  synthesis  of  the  phenomena  related  to  the  interplay  between  individual  pedestrians,  each  characterised  by  goals,  preferences  and  potentially  relevant  relationships  with  others,  and  the  environment  in  which  they  are  situated.  The  cases  in  which  these  activities  have  been  systematically  integrated  for  a  mutual  benefit  are  still  very  few  compared  to  the  corpus  of  crowd  related  literature.  This  paper  presents  a  case  study  of  an  integrated  approach  to  the  definition  of  an  innovative  model  for  pedestrian  and  crowd  simulation  (on  the  side  of  synthesis)  that  was  actually  motivated  and  supported  by  the  analyses  of  empirical  data  acquired  from  both  experimental  settings  and  observations  in  real  world  scenarios.  In  particular,  we  will  introduce  a  model  for  the  adaptive  behaviour  of  pedestrians  that  are  also  members  of  groups,  that  strive  to  maintain  their  cohesion  even  in  difficult  (e.g.,  high  density)  situations.  The  paper  will  show  how  the  synthesis  phase  also  provided  inputs  to  the  analysis  of  empirical  data,  in  a  virtuous  circle.
0	A  human  activity  recognition  framework  using  max  min  features  and  key  poses  with  differential  evolution  random  forests  classifier.  This  paper  presents  a  novel  framework  for  human  daily  activity  recognition  that  is  intended  to  rely  on  few  training  examples  evidencing  fast  training  times,  making  it  suitable  for  real-time  applications.  The  proposed  framework  starts  with  a  feature  extraction  stage,  where  the  division  of  each  activity  into  actions  of  variable-size,  based  on  key  poses,  is  performed.  Each  action  window  is  delimited  by  two  consecutive  and  automatically  identified  key  poses,  where  static  (i.e.  geometrical)  and  max-min  dynamic  (i.e.  temporal)  features  are  extracted.  These  features  are  first  used  to  train  a  random  forest  (RF)  classifier  which  was  tested  using  the  CAD-60  dataset,  obtaining  relevant  overall  average  results.  Then  in  a  second  stage,  an  extension  of  the  RF  is  proposed,  where  the  differential  evolution  meta-heuristic  algorithm  is  used,  as  splitting  node  methodology.  The  main  advantage  of  its  inclusion  is  the  fact  that  the  differential  evolution  random  forest  has  no  thresholds  to  tune,  but  rather  a  few  adjustable  parameters  with  well-defined  behavior.
0	A  novel  bacterial  foraging  technique  for  edge  detection.  A  new  approach  for  edge  detection  using  a  combination  of  bacterial  foraging  algorithm  (BFA)  and  probabilistic  derivative  technique  derived  from  Ant  Colony  Systems,  is  presented  in  this  paper.  The  foraging  behavior  of  some  species  of  bacteria  like  Escherichia  coli  can  be  hypothetically  modeled  as  an  optimization  process.  A  group  of  bacteria  search  for  nutrients  in  a  way  that  maximizes  the  energy  obtained  per  unit  time  spent  during  the  foraging.  The  proposed  approach  aims  at  driving  the  bacteria  through  the  edge  pixels.  The  direction  of  movement  of  the  bacteria  is  found  using  a  direction  probability  matrix,  computed  using  derivatives  along  the  possible  directions.  Rules  defining  the  derivatives  are  devised  to  ensure  that  the  variation  of  intensity  due  to  noise  is  discarded.  Quantitative  analysis  of  the  feasibility  of  the  proposed  approach  and  its  comparison  with  other  standard  edge  detection  operators  in  terms  of  kappa  and  entropy  are  given.  The  effect  of  initial  values  of  parameters  of  BFA  on  the  edge  detection  is  discussed.
0	Efficient  k  nearest  neighbors  search  in  graph  space.  Abstract  The  k-nearest  neighbors  classifier  has  been  widely  used  to  classify  graphs  in  pattern  recognition.  An  unknown  graph  is  classified  by  comparing  it  to  all  the  graphs  in  the  training  set  and  then  assigning  it  the  class  to  which  the  majority  of  the  nearest  neighbors  belong.  When  the  size  of  the  database  is  large,  the  search  of  k-nearest  neighbors  can  be  very  time  consuming.  On  this  basis,  researchers  proposed  optimization  techniques  to  speed  up  the  search  for  the  nearest  neighbors.  However,  to  the  best  of  our  knowledge,  all  the  existing  works  compared  the  unknown  graph  to  each  train  graph  separately  and  thus  none  of  them  considered  finding  the  k  nearest  graphs  from  a  query  as  a  single  problem.  In  this  paper,  we  define  a  new  problem  called  multi  graph  edit  distance  to  which  k-nearest  neighbor  belongs.  As  a  first  algorithm  to  solve  this  problem,  we  take  advantage  of  a  recent  exact  branch-and-bound  graph  edit  distance  approach  in  order  to  speed  up  the  classification  stage.  We  extend  this  algorithm  by  considering  all  the  search  spaces  needed  for  the  dissimilarity  computation  between  the  unknown  and  the  training  graphs  as  a  single  search  space.  Results  showed  that  this  approach  drastically  outperformed  the  original  approach  under  limited  time  constraints.  Moreover,  the  proposed  approach  outperformed  fast  graph  edit  distance  algorithms  in  terms  of  average  execution  time  especially  when  the  number  of  graphs  is  tremendous.
0	Modality  correlation  aware  sparse  representation  for  rgb  infrared  object  tracking.  Abstract  To  intelligently  analyze  and  understand  video  content,  a  key  step  is  to  accurately  perceive  the  motion  of  the  interested  objects  in  videos.  To  this  end,  the  task  of  object  tracking,  which  aims  to  determine  the  position  and  status  of  the  interested  object  in  consecutive  video  frames,  is  very  important,  and  has  received  great  research  interest  in  the  last  decade.  Although  numerous  algorithms  have  been  proposed  for  object  tracking  in  RGB  videos,  most  of  them  may  fail  to  track  the  object  when  the  information  from  the  RGB  video  is  not  reliable  (e.g.  in  dim  environment  or  large  illumination  change).  To  address  this  issue,  with  the  popularity  of  dual-camera  systems  for  capturing  RGB  and  infrared  videos,  this  paper  presents  a  feature  representation  and  fusion  model  to  combine  the  feature  representation  of  the  object  in  RGB  and  infrared  modalities  for  object  tracking.  Specifically,  this  proposed  model  is  able  to  (1)  perform  feature  representation  of  objects  in  different  modalities  by  employing  the  robustness  of  sparse  representation,  and  (2)  combine  the  representation  by  exploiting  the  modality  correlation.  Extensive  experiments  demonstrate  the  effectiveness  of  the  proposed  method.
0	Shape  based  object  matching  using  interesting  points  and  high  order  graphs.  The  introduction  of  a  novel  shape  descriptor  with  robust  shape  interesting  points  and  their  descriptors.The  implementation  of  a  high-order  graph  matching  algorithm  for  solving  the  shape  matching  problem.The  proposed  method  can  significantly  improve  the  traditional  correspondence-based  shape  matching  methods.The  proposed  method  is  very  robust  in  an  object  retrieval  scenario.  In  shape-based  object  matching,  it  is  important  how  to  fuse  similarities  between  points  on  a  shape  contour  and  the  ones  on  another  contour  into  the  overall  similarity.  However,  existing  methods  face  two  critical  problems.  Firstly,  since  most  contour  points  are  involved  for  possible  matchings  without  taking  into  account  the  usefulness  of  each  point,  it  causes  high  computational  costs  for  point  matching.  Secondly,  existing  methods  do  not  consider  geometrical  relations  characterised  by  multiple  points.  In  this  paper,  we  propose  a  shape-based  object  matching  method  which  is  able  to  overcome  these  problems.  To  counteract  the  first  problem  mentioned,  we  devise  a  shape  descriptor  using  a  small  number  of  interesting  points  which  are  generated  by  considering  both  curvatures  and  the  overall  shape  trend.  We  also  introduce  a  simple  and  highly  discriminative  point  descriptor,  namely  Point  Context,  which  represents  the  geometrical  and  topological  location  of  each  interesting  point.  For  the  second  problem,  we  employ  high-order  graph  matching  which  examines  similarities  for  singleton,  pairwise  and  triple  relations  of  points.  We  validate  the  robustness  and  accuracy  of  our  method  through  a  series  of  experiments  on  six  datasets.
0	Entropy  based  matrix  learning  machine  for  imbalanced  data  sets.  Adopt  entropy  to  evaluate  class  certainty  of  a  pattern.Determine  the  corresponding  fuzzy  membership  based  on  class  certainty.A  combination  of  FSVM-CIL  and  MatMHKS.  Imbalance  problem  occurs  when  negative  class  contains  many  more  patterns  than  that  of  positive  class.  Since  conventional  Support  Vector  Machine  (SVM)  and  Neural  Networks  (NN)  have  been  proven  not  to  effectively  handle  imbalanced  data,  some  improved  learning  machines  including  Fuzzy  SVM  (FSVM)  have  been  proposed.  FSVM  applies  a  fuzzy  membership  to  each  training  pattern  such  that  different  patterns  can  give  different  contributions  to  the  learning  machine.  However,  how  to  evaluate  fuzzy  membership  becomes  the  key  point  to  FSVM.  Moreover,  these  learning  machines  present  disadvantages  to  process  matrix  patterns.  In  order  to  process  matrix  patterns  and  to  tackle  the  imbalance  problem,  this  paper  proposes  an  entropy-based  matrix  learning  machine  for  imbalanced  data  sets,  adopting  the  Matrix-pattern-oriented  HoKashyap  learning  machine  with  regularization  learning  (MatMHKS)  as  the  base  classifier.  The  new  leaning  machine  is  named  EMatMHKS  and  its  contributions  are:  (1)  proposing  a  new  entropy-based  fuzzy  membership  evaluation  approach  which  enhances  the  importance  of  patterns,  (2)  guaranteeing  the  importance  of  positive  patterns  and  get  a  more  flexible  decision  surface.  Experiments  on  real-world  imbalanced  data  sets  validate  that  EMatMHKS  outperforms  compared  learning  machines.
0	Revealing  structure  in  large  graphs.  Provides  a  survey  of  the  regularity  lemma  and  its  algorithmic  aspectsProvides  a  survey  of  the  emerging  applications  of  the  regularity  lemma  in  structural  pattern  recognitionProvides  some  new  experimental  results  on  image  segmentation.  Introduced  in  the  mid-1970s  as  an  intermediate  step  in  proving  a  long-standing  conjecture  on  arithmetic  progressions,  Szemerdis  regularity  lemma  has  emerged  over  time  as  a  fundamental  tool  in  different  branches  of  graph  theory,  combinatorics  and  theoretical  computer  science.  Roughly,  it  states  that  every  graph  can  be  approximated  by  the  union  of  a  small  number  of  random-like  bipartite  graphs  called  regular  pairs.  In  other  words,  the  result  provides  us  a  way  to  obtain  a  good  description  of  a  large  graph  using  a  small  amount  of  data,  and  can  be  regarded  as  a  manifestation  of  the  all-pervading  dichotomy  between  structure  and  randomness.  In  this  paper  we  will  provide  an  overview  of  the  regularity  lemma  and  its  algorithmic  aspects,  and  will  discuss  its  relevance  in  the  context  of  pattern  recognition  research.
0	Perceiving  the  person  and  their  interactions  with  the  others  for  social  robotics  a  review.  Abstract  Social  robots  need  to  understand  human  activities,  dynamics,  and  the  intentions  behind  their  behaviors.  Most  of  the  time,  this  implies  the  modeling  of  the  whole  scene.  The  recognition  of  the  activities  and  intentions  of  a  person  are  inferred  from  the  perception  of  the  individual,  but  also  from  their  interactions  with  the  rest  of  the  environment  (i.e.,  objects  and/or  people).  Centering  on  the  social  nature  of  the  person,  robots  need  to  understand  human  social  cues,  which  include  verbal  but  also  nonverbal  behavioral  signals  such  as  actions,  gestures,  body  postures,  facial  emotions,  and  proxemics.  The  correct  understanding  of  these  signals  helps  these  robots  to  anticipate  the  needs  and  expectations  of  people.  It  also  avoids  abrupt  changes  on  the  human–robot  interaction,  as  the  temporal  dynamics  of  interactions  are  anchored  and  driven  by  a  major  repertoire  of social  landmarks.  Within  the  general  framework  of  interaction  of  robots  with  their  human  counterparts,  this  paper  reviews  recent  approaches  for  recognizing  human  activities,  but  also  for  perceiving  social  signals  emanated  from  a  person  or  a  group  of  people  during  an  interaction.  The  perception  of  visual  and/or  audio  signals  allow  them  to  correctly  localize  themselves  with  respect  to  humans  from  the  environment  while  also  navigating  and/or  interacting  with  a  person  or  a  group  of  people.
0	Robust  visible  infrared  image  matching  by  exploiting  dominant  edge  orientations.  Abstract  Finding  the  correspondences  between  visible  and  infrared  images  is  a  challenging  task  due  to  the  image  spectral  inconsistency  which  leads  to  large  differences  of  gradient  distributions  between  these  images.  To  alleviate  this  problem,  we  propose  a  novel  feature  descriptor  for  visible  and  infrared  image  matching  based  on  Log-Gabor  filters.  The  descriptor  employs  multi-orientation  and  multi-scale  Log-Gabor  filters  to  encode  the  edge  information  statistically.  Furthermore,  the  descriptor  provides  rotation  invariance  by  estimating  the  dominant  orientation  which  is  based  on  accumulated  edge  orientations.  The  experimental  results  demonstrate  the  effectiveness  of  the  proposed  rotation  invariant  descriptor  for  matching  visible  and  long  wave  infrared  images  as  compared  with  state-of-the-art  descriptors.
0	Learning  discrete  weights  using  the  local  reparameterization  trick.  Recent  breakthroughs  in  computer  vision  make  use  of  large  deep  neural  networks,  utilizing  the  substantial  speedup  offered  by  GPUs.  For  applications  running  on  limited  hardware,  however,  high  precision  real-time  processing  can  still  be  a  challenge.  One  approach  to  solving  this  problem  is  training  networks  with  binary  or  ternary  weights,  thus  removing  the  need  to  calculate  multiplications  and  significantly  reducing  memory  size.  In  this  work,  we  introduce  LR-nets  (Local  reparameterization  networks),  a  new  method  for  training  neural  networks  with  discrete  weights  using  stochastic  parameters.  We  show  how  a  simple  modification  to  the  local  reparameterization  trick,  previously  used  to  train  Gaussian  distributed  weights,  enables  the  training  of  discrete  weights.  Using  the  proposed  training  we  test  both  binary  and  ternary  models  on  MNIST,  CIFAR-10  and  ImageNet  benchmarks  and  reach  state-of-the-art  results  on  most  experiments.
0	Differentiable  learning  to  normalize  via  switchable  normalization.  We  address  a  learning-to-normalize  problem  by  proposing  Switchable  Normalization  (SN),  which  learns  to  select  different  normalizers  for  different  normalization  layers  of  a  deep  neural  network.  SN  employs  three  distinct  scopes  to  compute  statistics  (means  and  variances)  including  a  channel,  a  layer,  and  a  minibatch.  SN  switches  between  them  by  learning  their  importance  weights  in  an  end-to-end  manner.  It  has  several  good  properties.  First,  it  adapts  to  various  network  architectures  and  tasks  (see  Fig.1).  Second,  it  is  robust  to  a  wide  range  of  batch  sizes,  maintaining  high  performance  even  when  small  minibatch  is  presented  (e.g.  2  images/GPU).  Third,  SN  does  not  have  sensitive  hyper-parameter,  unlike  group  normalization  that  searches  the  number  of  groups  as  a  hyper-parameter.  Without  bells  and  whistles,  SN  outperforms  its  counterparts  on  various  challenging  benchmarks,  such  as  ImageNet,  COCO,  CityScapes,  ADE20K,  and  Kinetics.  Analyses  of  SN  are  also  presented.  We  hope  SN  will  help  ease  the  usage  and  understand  the  normalization  techniques  in  deep  learning.  The  code  of  SN  has  been  made  available  in  this  https  URL.
0	Iris  liveness  detection  for  mobile  devices  based  on  local  descriptors.  We  propose  to  use  local  binary  patterns  (LBP)  on  image  residual  for  iris  liveness  detection.Low-complexity  interpolation-free  implementation  enables  use  on  mobile  devices.Performance  is  promising  for  both  print-based  and  screen-based  attacks.  Iris  recognition  is  well  suited  to  authentication  on  mobile  devices,  due  to  its  intrinsic  security  and  non-intrusiveness.  However,  authentication  systems  can  be  easily  tricked  by  attacks  based  on  high-quality  printing.  A  liveness  detection  module  is  therefore  necessary.  Here,  we  propose  a  fast  and  accurate  technique  to  detect  printed-iris  attacks  based  on  the  local  binary  pattern  (LBP)  descriptor.  In  order  to  improve  the  discrimination  ability  of  LBP  and  better  explore  the  image  statistics,  LBP  is  performed  on  a  high-pass  version  of  the  image  with  3  i?  3  integer  kernel.  In  addition  a  simplified  interpolation-free  descriptor  is  considered  and  finally  a  linear  SVM  classification  scheme  is  used.  The  detection  performance,  measured  on  standard  databases,  is  extremely  promising,  despite  the  resulting  very  low  complexity,  which  makes  possible  the  implementation  for  the  relatively  small  CPU  processing  power  of  a  mobile  device.
0	Local  value  difference  metric.  Value  difference  metric  (VDM)  is  one  of  the  widely  used  distance  functions.We  propose  local  value  difference  metric  (LVDM).LVDM  uses  a  modified  decision  tree  algorithm  to  find  the  neighborhood  of  the  test  instance.The  experimental  results  on  36  UCI  datasets  validate  its  effectiveness.  Value  difference  metric  (VDM)  is  one  of  the  widely  used  distance  functions  designed  to  work  with  nominal  attributes.  Research  has  indicated  that  the  definition  of  VDM  follows  naturally  from  a  simple  probabilistic  model  called  a  naive  Bayes  (NB).  NB  assumes  that  all  the  attributes  are  independent  given  the  class.  To  further  improve  the  performance  of  NB,  several  techniques  have  been  proposed.  Among  these,  an  effective  technique  is  local  learning.  Because  VDM  has  a  close  relationship  with  NB,  in  this  paper,  we  propose  a  local  learning  method  for  VDM.  The  improved  distance  function  is  called  local  value  difference  metric  (LVDM).  When  LVDM  computes  the  distance  between  a  test  instance  and  each  training  instance,  the  conditional  probabilities  in  VDM  are  estimated  by  counting  from  the  neighborhood  of  the  test  instance  only  instead  of  from  all  the  training  data.  A  modified  decision  tree  algorithm  is  proposed  to  determine  the  neighborhood  of  the  test  instance.  The  experimental  results  on  43  datasets  downloaded  from  the  University  of  California  at  Irvine  (UCI)  show  that  the  proposed  LVDM  significantly  outperforms  VDM  in  terms  of  the  class-probability  estimation  performance  of  distance-based  learning  algorithms.
0	Speckle  noise  suppression  in  2d  ultrasound  kidney  images  using  local  pattern  based  topological  derivative.  Abstract  One  of  the  affordable  and  least  harmful  modalities  used  to  efficiently  detect  and  diagnose  the  kidney  diseases  is  the  Ultrasound  scans.  The  main  drawback  of  the  ultrasound  images  is  the  presence  of  speckle  noise  that  reduces  the  efficiency  of  image  processing  and  hinders  the  interpretation.  This  paper  proposes  a  novel  technique  named  Local  Binary  Pattern  based  Discrete  Topological  Derivative  and  its  variants  to  address  speckle  noise  reduction  problem  in  2D  ultrasound  kidney  images.  In  typical  Discrete  Topological  Derivative,  the  execution  time  is  higher  and  as  a  result  an  optimizer  is  incorporated  based  on  local  pattern  and  gradient  tolerance  value  resulting  in  20  times  reduction  in  execution  time  with  improved  results.  The  experimentation  is  carried  out  on  100  clinical  2D  ultrasound  images  and  moreover,  proposed  methods  are  compared  with  the  competing  Discrete  Topological  Derivative  and  some  commonly  used  speckle  noise  removal  filters.  The  results  are  promising  and  thereby  confirms  that  the  proposed  Local  Binary  Pattern  based  Discrete  Topological  Derivative  can  produce  a  better  speckle  noise  reduction  that  enables  the  doctors  to  detect  and  diagnose  kidney  diseases  in  2D  ultrasound  images  with  lesser  strain.
0	End  to  end  visual  speech  recognition  for  small  scale  datasets.  Abstract  Visual  speech  recognition  models  traditionally  consist  of  two  stages,  feature  extraction  and  classification.  Several  deep  learning  approaches  have  been  recently  presented  aiming  to  replace  the  feature  extraction  stage  by  automatically  extracting  features  from  mouth  images.  However,  research  on  simultaneously  learning  features  and  performing  classification  remains  limited.  In  addition,  most  of  the  existing  methods  require  large  amounts  of  data  in  order  to  achieve  state-of-the-art  performance,  otherwise  they  under-perform.  In  this  work,  an  end-to-end  lip-reading  system  for  isolated  word  recognition  is  presented  based  on  fully-connected  layers  and  Long-Short  Memory  (LSTM)  networks  which  is  suitable  for  small-scale  datasets.  The  model  consists  of  two  streams:  one  which  extracts  features  directly  from  the  mouth  images  and  one  which  extracts  features  from  the  difference  images.  A  Bidirectional  LSTM  (BLSTM)  is  used  for  modelling  the  temporal  dynamics  in  each  stream  which  are  then  fused  via  another  BLSTM.  An  absolute  improvement  in  classification  rate  of  0.6%,  3.4%,  3.9%,  11.4%  over  the  state-of-the-art  is  reported  on  the  OuluVS2,  CUAVE,  AVLetters  and  AVLetters2  databases,  respectively.
0	Exploring  stip  based  models  for  recognizing  human  interactions  in  tv  videos.  Human  motion  recognition  -  action  (HAR)  or  interaction  (HIR)  -  in  real  video  data  is  identified  as  a  very  challenging  task.  In  the  last  few  years  models  of  increasing  complexity  have  been  proposed  in  order  to  improve  the  performance  in  the  task.  However,  it  still  remains  unclear  whether  it  is  the  features  or  the  models  what  deserves  the  increase  in  complexity.  In  this  paper  an  evaluation  of  such  problem  is  carried  out  in  the  HIR  task.  For  that  purpose,  we  compare  the  results  obtained  in  our  experiments  -  by  using  STIP-based  features  and  BOW  models  as  basis  and  combined  with  a  standard  classifier  -  with  some  of  the  more  effective  and  recent  approaches  that  use  alternative  representation  models.  We  perform  a  comprehensive  experimental  study  on  two  state-of-the-art  databases  in  HIR:  TV  Human  interactions  and  UT-interactions.  We  compare  the  results  of  our  experiments  with  recent  results  published  on  these  datasets.  In  addition,  we  run  cross-data  experiments  on  Hollywood-2  dataset  in  order  to  study  the  capability  of  generalization  of  the  trained  models  through  different  datasets.  The  most  relevant  result  is  that  the  model  combining  STIP+BOW  is  competitive  in  the  HIR  task  in  comparison  with  the  most  complex  ones.  It  is  also  shown  that  the  vocabulary  learning  subtask  can  be  improved  by  using  compression  algorithms  on  large  enough  initial  set  of  features.  In  contrast  to  other  categorization  tasks  the  context  does  not  help,  the  results  show  that  dense  sampling  of  STIP  is  the  best  choice,  but  only  when  it  is  used  inside  the  region  of  interest  of  the  interaction.
0	Cascaded  regression  with  sparsified  feature  covariance  matrix  for  facial  landmark  detection.  We  analyse  the  value  of  context  in  the  Supervised  Descent  Method  (SDM).We  found  that  the  use  of  the  full  context  is  not  beneficial  in  general,  only  helping  in  certain  situations.We  highlight  the  relation  between  context  and  data  variance  context.We  show  that  optimal  performance  results  from  discarding  some  context  in  a  greedy  manner.  This  paper  explores  the  use  of  context  on  regression-based  methods  for  facial  landmarking.  Regression  based  methods  have  revolutionised  facial  landmarking  solutions.  In  particular  those  that  implicitly  infer  the  whole  shape  of  a  structured  object  have  quickly  become  the  state-of-the-art.  The  most  notable  exemplar  is  the  Supervised  Descent  Method  (SDM).  Its  main  characteristics  are  the  use  of  the  cascaded  regression  approach,  the  use  of  the  full  appearance  as  the  inference  input,  and  the  aforementioned  aim  to  directly  predict  the  full  shape.  In  this  article  we  argue  that  the  key  aspects  responsible  for  the  success  of  SDM  are  the  use  of  cascaded  regression  and  the  avoidance  of  the  constrained  optimisation  problem  that  characterised  most  of  the  previous  approaches.  We  show  that,  surprisingly,  it  is  possible  to  achieve  comparable  or  superior  performance  using  only  landmark-specific  predictors,  which  are  linearly  combined.  We  reason  that  augmenting  the  input  with  too  much  context  (of  which  using  the  full  appearance  is  the  extreme  case)  can  be  harmful.  In  fact,  we  experimentally  found  that  there  is  a  relation  between  the  data  variance  and  the  benefits  of  adding  context  to  the  input.  We  finally  devise  a  simple  greedy  procedure  that  makes  use  of  this  fact  to  obtain  superior  performance  to  the  SDM,  while  maintaining  the  simplicity  of  the  algorithm.  We  show  extensive  results  both  for  intermediate  stages  devised  to  prove  the  main  aspects  of  the  argumentative  line,  and  to  validate  the  overall  performance  of  two  models  constructed  based  on  these  considerations.
0	Fused  lasso  and  rotation  invariant  autoregressive  models  for  texture  classification.  Anisotropic,  rotation  invariant,  autoregressive  random  fields,  realised  by  considering  local  radial  sampling,  are  flexible  models  which  have  been  considered  for  texture  classification.  Unfortunately,  owing  to  the  strong  correlations  present  in  the  neighbourhood  covariate  matrix,  parameter  estimation  is  complicated  by  the  dichotomy  between  ill-conditionedness  and  rotation  invariance.  Exploiting  the  Fused  Lasso  framework,  we  here  propose  a  compromise  which  incorporates  two  regularisers.  The  @?"1-norm  induces  stability  and  performs  variable  selection  amongst  strongly  correlated  radial  samples;  the  total  variation  seminorm  encourages  clustering  and  promotes  parsimony.  Experiments  confirm  the  potential  utility.  Parallels  are  drawn  within  the  texture  classification  literature  and  beyond.
0	Invariants  of  distance  k  graphs  for  graph  embedding.  Graph  comparison  algorithms  based  on  metric  space  embedding  have  been  proven  robust  in  graph  clustering  and  classification.  In  this  paper  we  propose  graph  embedding  method  exploiting  ordered  invariants  of  distance  k-graphs,  which  encode  structure  of  shortest-paths.  We  study  degree  histograms  of  those  graphs  and  use  them  to  construct  permutation  invariant  graph  representation  called  vertex  B-matrix.  In  order  to  extract  more  information  from  structural  patterns  we  also  define  edge  distance  k-graphs  and  associated  edge  B-matrix.  Next,  several  new  graph  characteristics  obtained  by  condensing  information  stored  in  B-matrices  are  introduced.  We  demonstrate  that  our  approach  provides  stable  embedding,  which  captures  relevant  graph  features.  Experiments  on  classification  with  satellite  photo  and  mutagenicity  benchmark  datasets  revealed,  that  new  descriptors  allow  for  distinguishing  graphs  with  non  trivial  structural  differences.  Moreover,  they  appear  to  outperform  descriptors  based  on  heat  kernel  matrix,  being  at  the  same  time  more  effective  computationally.  In  the  end  we  test  feature  selection  on  B-matrices  showing  that  selecting  right  B-submatrix  can  improve  classification  rate  on  testing  datasets.
0	Single  and  multiple  outputs  decision  tree  classification  using  bi  level  discrete  continues  genetic  algorithm.  Abstract  Data  classification  with  decision  tree  models  is  an  attractive  method  in  data  analysis  and  data  mining.  However,  compared  to  other  classification  methods,  the  quality  of  prediction  of  these  models  is  lower  when  classic  heuristics  and  local  optimization  training  methods  are  employed.  To  improve  the  performance  of  these  models  for  single  output  and  multiple  outputs  data  sets,  an  optimal  tree  construction  method  based  on  the  genetic  algorithm  is  presented.  The  presented  bi-level  discrete-continues  genetic  algorithm  method  is  able  to  select  effective  features  as  well  as  construct  optimal  tree.  For  this  purpose,  new  operators  of  selection,  crossover,  and  mutation  are  designed  in  terms  of  continuous  and  discrete  variables.  Comparison  of  the  proposed  method  with  other  well-known  classification  methods  for  some  test  data  sets  and  real  world  data  shows  that  the  performance  of  the  decision  tree  models  has  been  upgraded  to  the  best  of  prediction  methods  level.
0	Iterated  learning  for  emergent  systematicity  in  vqa.  Although  neural  module  networks  have  an  architectural  bias  towards  compositionality,  they  require  gold  standard  layouts  to  generalize  systematically  in  practice.  When  instead  learning  layouts  and  modules  jointly,  compositionality  does  not  arise  automatically  and  an  explicit  pressure  is  necessary  for  the  emergence  of  layouts  exhibiting  the  right  structure.  We  propose  to  address  this  problem  using  iterated  learning,  a  cognitive  science  theory  of  the  emergence  of  compositional  languages  in  nature  that  has  primarily  been  applied  to  simple  referential  games  in  machine  learning.  Considering  the  layouts  of  module  networks  as  samples  from  an  emergent  language,  we  use  iterated  learning  to  encourage  the  development  of  structure  within  this  language.  We  show  that  the  resulting  layouts  support  systematic  generalization  in  neural  agents  solving  the  more  complex  task  of  visual  question-answering.  Our  regularized  iterated  learning  method  can  outperform  baselines  without  iterated  learning  on  SHAPES-SyGeT  (SHAPES  Systematic  Generalization  Test),  a  new  split  of  the  SHAPES  dataset  we  introduce  to  evaluate  systematic  generalization,  and  on  CLOSURE,  an  extension  of  CLEVR  also  designed  to  test  systematic  generalization.  We  demonstrate  superior  performance  in  recovering  ground-truth  compositional  program  structure  with  limited  supervision  on  both  SHAPES-SyGeT  and  CLEVR.
0	Using  latent  space  regression  to  analyze  and  leverage  compositionality  in  gans.  In  recent  years,  Generative  Adversarial  Networks  have  become  ubiquitous  in  both  research  and  public  perception,  but  how  GANs  convert  an  unstructured  latent  code  to  coherent,  high  quality  output  is  still  an  open  question.  In  this  work,  we  investigate  regression  into  the  latent  space  as  a  probe  to  understand  the  compositional  properties  of  GANs.  We  find  that  combining  the  regressor  and  a  pretrained  generator  provides  a  strong  image  prior,  allowing  us  to  create  composite  images  from  a  collage  of  random  image  parts  at  inference  time  and  leverage  the  generator  to  rectify  the  inconsistencies.  To  compare  compositional  properties  across  different  generators,  we  measure  the  trade-offs  between  reconstruction  of  the  unrealistic  input  and  image  quality  of  the  regenerated  samples.  We  find  that  the  regression  approach  enables  more  localized  editing  of  individual  image  parts  compared  to  direct  editing  in  the  latent  space,  and  we  conduct  experiments  to  quantify  this  independence  effect.  Our  method  is  agnostic  to  how  image  parts  are  extracted,  and  does  not  require  explicit  labelled  knowledge  of  concepts  during  training.  Because  this  approach  only  uses  a  single  feed-forward  pass,  it  can  operate  in  real-time  and  also  perform  a  number  of  related  applications,  such  as  image  inpainting  or  example-based  image  editing.  We  demonstrate  its  effectiveness  on  several  GANs  and  datasets.
0	Impact  of  representation  learning  in  linear  bandits.  We  study  how  representation  learning  can  improve  the  efficiency  of  bandit  problems.  We  study  the  setting  where  we  play  T  linear  bandits  with  dimension  d  concurrently,  and  these  T  bandit  tasks  share  a  common  k(≪d)  dimensional  linear  representation.  For  the  finite-action  setting,  we  present  a  new  algorithm  which  achieves  O~(TkN+dkNT)  regret,  where  N  is  the  number  of  rounds  we  play  for  each  bandit.  When  T  is  sufficiently  large,  our  algorithm  significantly  outperforms  the  naive  algorithm  (playing  T  bandits  independently)  that  achieves  O~(TdN)  regret.  We  also  provide  an  Ω(TkN+dkNT)  regret  lower  bound,  showing  that  our  algorithm  is  minimax-optimal  up  to  poly-logarithmic  factors.  Furthermore,  we  extend  our  algorithm  to  the  infinite-action  setting  and  obtain  a  corresponding  regret  bound  which  demonstrates  the  benefit  of  representation  learning  in  certain  regimes.  We  also  present  experiments  on  synthetic  and  real-world  data  to  illustrate  our  theoretical  findings  and  demonstrate  the  effectiveness  of  our  proposed  algorithms.
0	Composition  based  multi  relational  graph  convolutional  networks.  Graph  Convolutional  Networks  (GCNs)  have  recently  been  shown  to  be  quite  successful  in  modeling  graph-structured  data.  However,  the  primary  focus  has  been  on  handling  simple  undirected  graphs.  Multi-relational  graphs  are  a  more  general  and  prevalent  form  of  graphs  where  each  edge  has  a  label  and  direction  associated  with  it.  Most  of  the  existing  approaches  to  handle  such  graphs  suffer  from  over-parameterization  and  are  restricted  to  learning  representations  of  nodes  only.  In  this  paper,  we  propose  CompGCN,  a  novel  Graph  Convolutional  framework  which  jointly  embeds  both  nodes  and  relations  in  a  relational  graph.  CompGCN  leverages  a  variety  of  entity-relation  composition  operations  from  Knowledge  Graph  Embedding  techniques  and  scales  with  the  number  of  relations.  It  also  generalizes  several  of  the  existing  multi-relational  GCN  methods.  We  evaluate  our  proposed  method  on  multiple  tasks  such  as  node  classification,  link  prediction,  and  graph  classification,  and  achieve  demonstrably  superior  results.  We  make  the  source  code  of  CompGCN  available  to  foster  reproducible  research.
0	Slomo  improving  communication  efficient  distributed  sgd  with  slow  momentum.  Distributed  optimization  is  essential  for  training  large  models  on  large  datasets.  Multiple  approaches  have  been  proposed  to  reduce  the  communication  overhead  in  distributed  training,  such  as  synchronizing  only  after  performing  multiple  local  SGD  steps,  and  decentralized  methods  (e.g.,  using  gossip  algorithms)  to  decouple  communications  among  workers.  Although  these  methods  run  faster  than  AllReduce-based  methods,  which  use  blocking  communication  before  every  update,  the  resulting  models  may  be  less  accurate  after  the  same  number  of  updates.  Inspired  by  the  BMUF  method  of  Chen  &  Huo  (2016),  we  propose  a  slow  momentum  (SloMo)  framework,  where  workers  periodically  synchronize  and  perform  a  momentum  update,  after  multiple  iterations  of  a  base  optimization  algorithm.  Experiments  on  image  classification  and  machine  translation  tasks  demonstrate  that  SloMo  consistently  yields  improvements  in  optimization  and  generalization  performance  relative  to  the  base  optimizer,  even  when  the  additional  overhead  is  amortized  over  many  updates  so  that  the  SloMo  runtime  is  on  par  with  that  of  the  base  optimizer.  We  provide  theoretical  convergence  guarantees  showing  that  SloMo  converges  to  a  stationary  point  of  smooth  non-convex  losses.  Since  BMUF  is  a  particular  instance  of  the  SloMo  framework,  our  results  also  correspond  to  the  first  theoretical  convergence  guarantees  for  BMUF.
0	Gradientless  descent  high  dimensional  zeroth  order  optimization.  Zeroth-order  optimization  is  the  process  of  minimizing  an  objective  $f(x)$,  given  oracle  access  to  evaluations  at  adaptively  chosen  inputs  $x$.  In  this  paper,  we  present  two  simple  yet  powerful  GradientLess  Descent  (GLD)  algorithms  that  do  not  rely  on  an  underlying  gradient  estimate  and  are  numerically  stable.  We  analyze  our  algorithm  from  a  novel  geometric  perspective  and  we  show  that  for  {\it  any  monotone  transform}  of  a  smooth  and  strongly  convex  objective  with  latent  dimension  $k  \ge  n$,  we  present  a  novel  analysis  that  shows  convergence  within  an  $\epsilon$-ball  of  the  optimum  in  $O(kQ\log(n)\log(R/\epsilon))$  evaluations,  where  the  input  dimension  is  $n$,  $R$  is  the  diameter  of  the  input  space  and  $Q$  is  the  condition  number.  Our  rates  are  the  first  of  its  kind  to  be  both  1)  poly-logarithmically  dependent  on  dimensionality  and  2)  invariant  under  monotone  transformations.  We  further  leverage  our  geometric  perspective  to  show  that  our  analysis  is  optimal.  Both  monotone  invariance  and  its  ability  to  utilize  a  low  latent  dimensionality  are  key  to  the  empirical  success  of  our  algorithms,  as  demonstrated  on  synthetic  and  MuJoCo  benchmarks.
0	Apprentice  using  knowledge  distillation  techniques  to  improve  low  precision  network  accuracy.  Deep  learning  networks  have  achieved  state-of-the-art  accuracies  on  computer  vision  workloads  like  image  classification  and  object  detection.  The  performant  systems,  however,  typically  involve  big  models  with  numerous  parameters.  Once  trained,  a  challenging  aspect  for  such  top  performing  models  is  deployment  on  resource  constrained  inference  systems  --  the  models  (often  deep  networks  or  wide  networks  or  both)  are  compute  and  memory  intensive.  Low  precision  numerics  and  model  compression  using  knowledge  distillation  are  popular  techniques  to  lower  both  the  compute  requirements  and  memory  footprint  of  these  deployed  models.  In  this  paper,  we  study  the  combination  of  these  two  techniques  and  show  that  the  performance  of  low  precision  networks  can  be  significantly  improved  by  using  knowledge  distillation  techniques.  We  call  our  approach  Apprentice  and  show  state-of-the-art  accuracies  using  ternary  precision  and  4-bit  precision  for  many  variants  of  ResNet  architecture  on  ImageNet  dataset.  We  study  three  schemes  in  which  one  can  apply  knowledge  distillation  techniques  to  various  stages  of  the  train-and-deploy  pipeline.
0	Global  to  local  memory  pointer  networks  for  task  oriented  dialogue.  A  system  and  corresponding  method  are  provided  for  generating  responses  for  a  dialogue  between  a  user  and  a  computer.  The  system  includes  a  memory  storing  information  for  a  dialogue  history  and  a  knowledge  base.  An  encoder  may  receive  a  new  utterance  from  the  user  and  generate  a  global  memory  pointer  used  for  filtering  the  knowledge  base  information  in  the  memory.  A  decoder  may  generate  at  least  one  local  memory  pointer  and  a  sketch  response  for  the  new  utterance.  The  sketch  response  includes  at  least  one  sketch  tag  to  be  replaced  by  knowledge  base  information  from  the  memory.  The  system  generates  the  dialogue  computer  response  using  the  local  memory  pointer  to  select  a  word  from  the  filtered  knowledge  base  information  to  replace  the  at  least  one  sketch  tag  in  the  sketch  response.
0	Identifying  and  controlling  important  neurons  in  neural  machine  translation.  Neural  machine  translation  (NMT)  models  learn  representations  containing  substantial  linguistic  information.  However,  it  is  not  clear  if  such  information  is  fully  distributed  or  if  some  of  it  can  be  attributed  to  individual  neurons.  We  develop  unsupervised  methods  for  discovering  important  neurons  in  NMT  models.  Our  methods  rely  on  the  intuition  that  different  models  learn  similar  properties,  and  do  not  require  any  costly  external  supervision.  We  show  experimentally  that  translation  quality  depends  on  the  discovered  neurons,  and  find  that  many  of  them  capture  common  linguistic  phenomena.  Finally,  we  show  how  to  control  NMT  translations  in  predictable  ways,  by  modifying  activations  of  individual  neurons.
0	Randomized  ensembled  double  q  learning  learning  fast  without  a  model.  Using  a  high  Update-To-Data  (UTD)  ratio,  model-based  methods  have  recently  achieved  much  higher  sample  efficiency  than  previous  model-free  methods  for  continuous-action  DRL  benchmarks.  In  this  paper,  we  introduce  a  simple  model-free  algorithm,  Randomized  Ensembled  Double  Q-Learning  (REDQ),  and  show  that  its  performance  is  just  as  good  as,  if  not  better  than,  a  state-of-the-art  model-based  algorithm  for  the  MuJoCo  benchmark.  Moreover,  REDQ  can  achieve  this  performance  using  fewer  parameters  than  the  model-based  method,  and  with  less  wall-clock  run  time.  REDQ  has  three  carefully  integrated  ingredients  which  allow  it  to  achieve  its  high  performance:  (i)  a  UTD  ratio  ≫1;  (ii)  an  ensemble  of  Q  functions;  (iii)  in-target  minimization  across  a  random  subset  of  Q  functions  from  the  ensemble.  Through  carefully  designed  experiments,  we  provide  a  detailed  analysis  of  REDQ  and  related  model-free  algorithms.  To  our  knowledge,  REDQ  is  the  first  successful  model-free  DRL  algorithm  for  continuous-action  spaces  using  a  UTD  ratio  ≫1.
0	A  probabilistic  formulation  of  unsupervised  text  style  transfer.  We  present  a  deep  generative  model  for  unsupervised  text  style  transfer  that  unifies  previously  proposed  non-generative  techniques.  Our  probabilistic  approach  models  non-parallel  data  from  two  domains  as  a  partially  observed  parallel  corpus.  By  hypothesizing  a  parallel  latent  sequence  that  generates  each  observed  sequence,  our  model  learns  to  transform  sequences  from  one  domain  to  another  in  a  completely  unsupervised  fashion.  In  contrast  with  traditional  generative  sequence  models  (e.g.  the  HMM),  our  model  makes  few  assumptions  about  the  data  it  generates:  it  uses  a  recurrent  language  model  as  a  prior  and  an  encoder-decoder  as  a  transduction  distribution.  While  computation  of  marginal  data  likelihood  is  intractable  in  this  model  class,  we  show  that  amortized  variational  inference  admits  a  practical  surrogate.  Further,  by  drawing  connections  between  our  variational  objective  and  other  recent  unsupervised  style  transfer  and  machine  translation  techniques,  we  show  how  our  probabilistic  view  can  unify  some  known  non-generative  objectives  such  as  backtranslation  and  adversarial  loss.  Finally,  we  demonstrate  the  effectiveness  of  our  method  on  a  wide  range  of  unsupervised  style  transfer  tasks,  including  sentiment  transfer,  formality  transfer,  word  decipherment,  author  imitation,  and  related  language  translation.  Across  all  style  transfer  tasks,  our  approach  yields  substantial  gains  over  state-of-the-art  non-generative  baselines,  including  the  state-of-the-art  unsupervised  machine  translation  techniques  that  our  approach  generalizes.  Further,  we  conduct  experiments  on  a  standard  unsupervised  machine  translation  task  and  find  that  our  unified  approach  matches  the  current  state-of-the-art.
0	H_  infty  filtering  for  continuous  time  t  s  fuzzy  systems  with  partly  immeasurable  premise  variables.  This  paper  is  concerned  with  the        $H_{\infty  }$        filtering  problem  for  T–S  fuzzy  systems  with  partly  immeasurable  premise  variables.  By  using  measurable  premise  variables  of  fuzzy  models  as  the  premise  variables  of  fuzzy  filters,  a  new  fuzzy  filter  scheme  is  constructed.  Further  based  on  the  new  filter  scheme  and  a  class  of  new  line  integral  fuzzy  Lyapunov  functions,  a  convex  condition  for  designing        $H_{\infty  }$        filters  is  proposed.  In  contrast  to  the  existing  approaches,  the  new  condition  can  take  full  use  of  measurable  premise  variables  for  less  conservative  design.  A  numerical  example  is  given  to  illustrate  the  effectiveness  of  the  proposed  method.
0	Hand  movement  trajectory  reconstruction  from  eeg  for  brain  computer  interface  systems.  Decoding  hand  movement  parameters  (for  example  movement  trajectory,  speed  etc.)  from  scalp  recordings  such  as  Electroencephalography  (EEG)  is  a  challenging  and  less  explored  area  of  research  in  the  field  of  Brain  Computer  Interface  (BCI)  systems.  By  identifying  neural  features  underlying  movement  parameters,  a  detailed  and  well  defined  control  command  set  can  be  provided  to  the  BCI  output  device.  A  continuous  control  to  the  output  device  is  better  suited  for  practical  BCI  systems,  and  can  be  achieved  by  continuous  reconstruction  of  movement  trajectory  than  discrete  brain  activity  classifications.  In  this  study,  we  attempt  to  reconstruct/estimate  various  parameters  of  hand  movement  trajectory  from  multi  channel  EEG  recordings.  The  data  for  analysis  is  collected  by  performing  an  experiment  that  involved  centre-out  right  hand  movement  tasks  in  four  different  directions  at  two  different  speeds  in  random  order.  Multiple  linear  regression  (MLR)  strategy  that  fits  the  recorded  movement  parameters  to  a  set  of  spatial,  spectral  and  temporal  localized  neural  data  set  is  adopted.  We  propose  a  method  to  define  the  predictor  set  for  MLR,  using  wavelet  analysis,  to  decompose  the  signal  into  various  sub  bands.  The  correlation  between  recorded  and  estimated  parameters  are  calculated  and  an  average  correlation  coefficient  of  (0.56  ±  0.16)  is  obtained  over  estimating  six  movement  parameters.  The  promising  results  achieved  using  the  proposed  algorithm,  which  are  better  than  that  of  the  existing  algorithms,  indicate  the  applicability  of  EEG  for  continuous  motor  control.
0	Ssvep  based  bmi  for  a  meal  assistance  robot.  Meal  assistance  robots  provide  disabled  individuals  the  access  to  one  of  the  important  activities  in  daily  living,  self-feeding.  This  paper  proposes  a  Steady  State  Visually  Evoked  Potential  (SSVEP)  based  Brain  Machine  Interface  (BMI)  for  controlling  of  a  meal  assistance  robot.  In  the  proposed  system,  the  user  has  the  facility  to  select  any  solid  food  item  that  he  would  like  to  eat  from  3  different  bowls  just  by  looking  at  the  respective  LED  matrices  blinking  at  different  frequencies.  The  generated  SSVEP  signals  while  looking  at  the  LEDs  are  extracted  from  EEG  signals  acquired  using  OpenBCI  EEG  signal  acquisition  system.  Extracted  SSVEP  signals  are  used  to  identify  the  intention  of  the  user  and  subsequently  the  detected  intentions  are  used  to  operate  the  meal  assistant  robot.  Experiments  are  carried  out  to  validate  the  system  and  results  indicate  the  effectiveness  of  the  proposed  method.
0	User  authentication  based  on  representative  users.  User  authentication  based  on  username  and  password  is  the  most  common  means  to  enforce  access  control.  This  form  of  access  restriction  is  prone  to  hacking  since  stolen  usernames  and  passwords  can  be  exploited  to  impersonate  legitimate  users  in  order  to  commit  malicious  activity.  Biometric  authentication  incorporates  additional  user  characteristics  such  as  the  manner  by  which  the  keyboard  is  used  in  order  to  identify  users.  We  introduce  a  novel  approach  for  user  authentication  based  on  the  keystroke  dynamics  of  the  password  entry.  A  classifier  is  tailored  to  each  user  and  the  novelty  lies  in  the  manner  by  which  the  training  set  is  constructed.  Specifically,  only  the  keystroke  dynamics  of  a  small  subset  of  users,  which  we  refer  to  as  representatives,  is  used  along  with  the  password  entry  keystroke  dynamics  of  the  examined  user.  The  contribution  of  this  approach  is  twofold:  it  reduces  the  possibility  of  overfitting,  while  allowing  scalability  to  a  high  volume  of  users.  We  propose  two  strategies  to  construct  the  subset  for  each  user.  The  first  selects  the  users  whose  keystroke  profiles  govern  the  profiles  of  all  the  users,  while  the  second  strategy  chooses  the  users  whose  profiles  are  the  most  similar  to  the  profile  of  the  user  for  whom  the  classifier  is  constructed.  Results  are  promising  reaching  in  some  cases  90%  area  under  the  curve.  In  many  cases,  a  higher  number  of  representatives  deteriorate  the  accuracy  which  may  imply  overfitting.  An  extensive  evaluation  was  performed  using  a  dataset  containing  over  780  users.
0	Nonlinear  stabilizing  control  for  ship  mounted  cranes  with  ship  roll  and  heave  movements  design  analysis  and  experiments.  Presently,  ship-mounted  cranes  are  playing  more  and  more  important  roles  in  modern  ocean  transportation  and  logistics.  Different  from  traditional  land-fixed  crane  systems,  ship-mounted  cranes  present  much  more  complicated  nonlinear  dynamical  characteristics  and  they  are  persistently  influenced  by  different  mismatched  disturbances  due  to  harsh  sea  environments,  e.g.,  sea  waves,  ocean  currents,  sea  winds,  and  so  forth;  these  unfavorable  factors  bring  about  many  challenges  for  the  development  of  effective  control  schemes.  This  paper  presents  a  novel  nonlinear  stabilizing  control  strategy  for  underactuated  ship-mounted  crane  systems.  Specifically,  some  novel  coordinate  change  procedures  are  first  introduced  to  tackle  the  disturbing  terms  by  transforming  the  original  dynamics  into  a  new  form,  which  facilitates  both  controller  design  and  stability  analysis.  After  that,  a  nonlinear  control  law  is  constructed  to  regulate  the  cargo  position  to  the  desired  location  asymptotically,  in  the  presence  of  ship  roll  and  heave  movements.  The  boundedness  and  convergence  of  the  closed-loop  signals  are  proven  with  Lyapunov-based  analysis.  To  the  best  of  our  knowledge,  this  is  the  first  closed-loop  scheme  that  can  achieve  asymptotic  control  results,  without  linearizing/approximating  the  original  nonlinear  dynamics  when  performing  controller  design  and  stability  analysis,  for  underactuated  ship-mounted  cranes  with  ship  roll  and  heave  movements.  Hardware  experimental  results  are  included  to  show  that  the  proposed  control  method  can  achieve  satisfactory  control  performance  and  it  admits  strong  robustness  against  external  perturbations.
0	Securing  data  in  internet  of  things  iot  using  cryptography  and  steganography  techniques.  Internet  of  Things  (IoT)  is  a  domain  wherein  which  the  transfer  of  data  is  taking  place  every  single  second.  The  security  of  these  data  is  a  challenging  task;  however,  security  challenges  can  be  mitigated  with  cryptography  and  steganography  techniques.  These  techniques  are  crucial  when  dealing  with  user  authentication  and  data  privacy.  In  the  proposed  work,  the  elliptic  Galois  cryptography  protocol  is  introduced  and  discussed.  In  this  protocol,  a  cryptography  technique  is  used  to  encrypt  confidential  data  that  came  from  different  medical  sources.  Next,  a  Matrix  XOR  encoding  steganography  technique  is  used  to  embed  the  encrypted  data  into  a  low  complexity  image.  The  proposed  work  also  uses  an  optimization  algorithm  called  Adaptive  Firefly  to  optimize  the  selection  of  cover  blocks  within  the  image.  Based  on  the  results,  various  parameters  are  evaluated  and  compared  with  the  existing  techniques.  Finally,  the  data  that  is  hidden  in  the  image  is  recovered  and  is  then  decrypted.
0	Analysis  of  using  rls  in  neural  fuzzy  systems.  In  this  study,  we  continue  our  analysis  on  the  use  of  RLS  in  neural  fuzzy  systems.  The  recursive  least  square  (RLS)  algorithms  can  have  great  learning  performance  for  neural  fuzzy  networks.  From  our  previous  work,  it  can  be  observed  that  the  advantages  of  using  RLS  instead  of  using  BP  are  not  so  obvious.  For  the  use  of  forgetting  factor  in  RLS,  the  idea  is  to  account  for  the  effects  of  the  change  in  the  premise  part.  In  this  study,  we  have  observed  that  the  use  of  a  forgetting  factor  can  still  have  some  advantages  when  the  premise  part  is  fixed.  The  idea  is  similar  to  the  used  of  Widrow-Hoff  learning  concept  in  the  backpropagation  learning  algorithm.  From  our  experiments,  a  strong  forgetting  factor  (smaller  value)  can  let  the  consequent  part  trace  the  error  in  the  learning  phase.  But  the  testing  error  becomes  very  large.  When  the  system  capacity  is  sufficient,  a  forgetting  factor  will  improve  both  in  the  learning  phase  and  in  the  testing  phase.  Finally,  the  initial  value  of  the  covariance  matrix  is  considered.  The  learning  capacity  will  rise  when  the  initial  value  increases.  But  it  will  increase  the  error  tracing  phenomenon  in  the  consequent  part  too.  But  it  is  opposite  in  a  system  with  less  learning  capacity.
0	Systems  integration  technical  risks  assessment  model  sitram.  This  paper  presents  a  novel  system  integration  technical  risk  assessment  model  (SITRAM),  which  is  based  on  Bayesian  belief  networks  (BBN)  coupled  with  parametric  models  (PM).  This  model  provides  statistical  information  for  decision  makers,  improving  risk  management  of  complex  projects.  System  integration  technical  risks  (SITR)  represent  a  significant  part  of  project  risks  associated  with  the  development  of  large  software  intensive  systems  for  defense  and  commercial  applications.  We  propose  a  conceptual  modeling  framework  to  address  the  problem  of  SITR  assessment  in  the  early  stages  of  a  system  life  cycle.  Initial  risks'  taxonomy  and  risks'  interrelations  have  been  identified  using  a  hierarchical  holographic  modeling  (HHM)  approach.  The  framework  includes  a  set  of  BBN  models,  representing  relations  between  risk  contributing  factors,  and  complementing  PMs,  used  to  provide  input  data  to  the  BBN  models.  In  this  paper,  we  present  the  rationale  and  the  modeling  objectives,  and  describe  the  concepts  and  details  of  BBN  experimental  model  design  and  implementation.  To  address  practical  limitations,  heuristic  techniques  have  been  proposed  for  easing  the  generation  of  conditional  probability  tables.  PM  design  principles  are  described  and  examples  are  presented.  In  conclusion,  we  summarize  the  benefits  and  constraints  of  SITR  assessment  based  on  BBN  models.  Further  research  directions  and  model  improvements  are  also  presented.
0	Integrating  intelligent  agent  and  ontology  for  services  discovery  on  cloud  environment.  With  the  advance  of  cloud  computing,  cloud  service  providers  (CSP)  provide  increasingly  diversified  services  to  users.  To  utilize  general  search  engine,  such  as  Google,  is  not  an  effective  and  efficient  if  the  services  are  similar  but  with  different  attributes.  Therefore,  an  intelligent  service  discovery  platform  is  necessary  for  seeking  suitable  services  accurately  and  quickly.  In  the  paper1,  we  propose  a  framework  that  integrates  intelligent  agent  and  ontology  for  service  discovery  in  cloud  environment.  The  framework  contains  some  agents  and  mainly  assists  users  to  discovering  suitable  service  according  to  user  demand.  User  can  submit  his  flat-text  based  request  for  discovering  required  service.  We  implement  a  cloud  service  discovery  environment  to  demonstrate  the  concept  and  its  application.  We  also  utilize  the  Recall  and  Precision  to  evaluate  the  accuracy  of  the  system.
0	Models  methodologies  and  tools  supporting  establishment  and  management  of  second  generation  vbes.  Both  research  and  practice  have  shown  that  preexistence  of  long-term  associations/clusters  of  active  and  competitive  organizations,  the  so-called  virtual  organizations  breeding  environments  (VBEs),  can  greatly  enhance  dynamic  creation  of  virtual  organizations  (VOs).  During  the  past  decade,  a  number  of  VBEs  are  formed  worldwide,  mostly  among  organizations  located  in  a  common  region  that,  in  principle,  have  common  business  culture  and  other  similarities,  and  primarily  focus  on  static  lines  of  activities.  The  second-generation  VBEs  addressed  in  this  paper;  however,  aim  to  focus  on  associations/clusters  that  are  not  bound  to  geographical  regions  or  static  lines  of  activities,  and  wish  to  act  as  well  recognized  and  competitive  entities  in  the  society/market,  facilitating  time/cost  effective  and  fluid  configuration,  and  establishment  of  dynamic  VOs.  The  proposed  VBEs  apply  supporting  information  and  communication  technology  infrastructures,  tools,  and  services  that  provide  common  grounds  for  the  interaction  and  cooperation  among  their  member  organizations.  Furthermore,  these  multiregional  VBEs  assist  with  the  needed  evolution  of  VOs,  introducing  new  approaches  and  mechanisms  to  build  trust,  define  the  needed  collaboration  business  culture,  and  establish  the  common  value  systems  and  working/sharing  principles  among  their  independent  organizations,  among  others.  The  paper  introduces  a  number  of  models,  methodologies,  and  tools  that  are  designed  and  developed  supporting  both  the  management  as  well  as  successful  operation  of  the  second-generation  VBEs.
0	Tsk  fuzzy  function  approximators  design  and  accuracy  analysis.  Fuzzy  systems  are  excellent  approximators  of  known  functions  or  for  the  dynamic  response  of  a  physical  system.  We  propose  a  new  approach  to  approximate  any  known  function  by  a  Takagi-Sugeno-Kang  fuzzy  system  with  a  guaranteed  upper  bound  on  the  approximation  error.  The  new  approach  is  also  used  to  approximately  represent  the  behavior  of  a  dynamic  system  from  its  input-output  pairs  using  experimental  data  with  known  error  bounds.  We  provide  sufficient  conditions  for  this  class  of  fuzzy  systems  to  be  universal  approximators  with  specified  error  bounds.  The  new  conditions  require  a  smaller  number  of  membership  functions  than  all  previously  published  conditions.  We  illustrate  the  new  results  and  compare  them  to  published  error  bounds  through  numerical  examples.
0	K  complex  detection  using  a  hybrid  synergic  machine  learning  method.  Sleep  stage  identification  is  the  first  step  in  modern  sleep  disorder  diagnostics  process.  K-complex  is  an  indicator  for  the  sleep  stage  2.  However,  due  to  the  ambiguity  of  the  translation  of  the  medical  standards  into  a  computer-based  procedure,  reliability  of  automated  K-complex  detection  from  the  EEG  wave  is  still  far  from  expectation.  More  specifically,  there  are  some  significant  barriers  to  the  research  of  automatic  K-complex  detection.  First,  there  is  no  adequate  description  of  K-complex  that  makes  it  difficult  to  develop  automatic  detection  algorithm.  Second,  human  experts  only  provided  the  label  for  whether  a  whole  EEG  segment  contains  K-complex  or  not,  rather  than  individual  labels  for  each  subsegment.  These  barriers  render  most  pattern  recognition  algorithms  inapplicable  in  detecting  K-complex.  In  this  paper,  we  attempt  to  address  these  two  challenges,  by  designing  a  new  feature  extraction  method  that  can  transform  visual  features  of  the  EEG  wave  with  any  length  into  mathematical  representation  and  proposing  a  hybrid-synergic  machine  learning  method  to  build  a  K-complex  classifier.  The  tenfold  cross-validation  results  indicate  that  both  the  accuracy  and  the  precision  of  this  proposed  model  are  at  least  as  good  as  a  human  expert  in  K-complex  detection.
0	A  novel  line  scan  palmprint  acquisition  system.  Biometric  recognition  systems  have  been  widely  used  globally.  However,  one  effective  and  highly  accurate  biometric  authentication  method,  palmprint  recognition,  has  not  been  popularly  applied  as  it  should  have  been,  which  could  be  due  to  the  lack  of  small,  flexible  and  user-friendly  acquisition  systems.  To  expand  the  use  of  palmprint  biometrics,  we  propose  a  novel  palmprint  acquisition  system  based  on  the  line-scan  image  sensor.  The  proposed  system  consists  of  a  customized  and  highly  integrated  line-scan  sensor,  a  self-adaptive  synchronizing  unit,  and  a  field-programmable  gate  array  controller  with  a  cross-platform  interface.  The  volume  of  the  proposed  system  is  over  94%  smaller  than  the  volume  of  existing  palmprint  systems,  without  compromising  its  verification  performance.  The  verification  performance  of  the  proposed  system  was  tested  on  a  database  of  8000  samples  collected  from  250  people,  and  the  equal  error  rate  is  0.048%,  which  is  comparable  to  the  best  area  camera-based  systems.
0	Modeling  user  activity  preference  by  leveraging  user  spatial  temporal  characteristics  in  lbsns.  With  the  recent  surge  of  location  based  social  networks  (LBSNs),  activity  data  of  millions  of  users  has  become  attainable.  This  data  contains  not  only  spatial  and  temporal  stamps  of  user  activity,  but  also  its  semantic  information.  LBSNs  can  help  to  understand  mobile  users’  spatial  temporal  activity  preference  (STAP),  which  can  enable  a  wide  range  of  ubiquitous  applications,  such  as  personalized  context-aware  location  recommendation  and  group-oriented  advertisement.  However,  modeling  such  user-specific  STAP  needs  to  tackle  high-dimensional  data,  i.e.,  user-location-time-activity  quadruples,  which  is  complicated  and  usually  suffers  from  a  data  sparsity  problem.  In  order  to  address  this  problem,  we  propose  a  STAP  model.  It  first  models  the  spatial  and  temporal  activity  preference  separately,  and  then  uses  a  principle  way  to  combine  them  for  preference  inference.  In  order  to  characterize  the  impact  of  spatial  features  on  user  activity  preference,  we  propose  the  notion  of  personal  functional  region  and  related  parameters  to  model  and  infer  user  spatial  activity  preference.  In  order  to  model  the  user  temporal  activity  preference  with  sparse  user  activity  data  in  LBSNs,  we  propose  to  exploit  the  temporal  activity  similarity  among  different  users  and  apply  nonnegative  tensor  factorization  to  collaboratively  infer  temporal  activity  preference.  Finally,  we  put  forward  a  context-aware  fusion  framework  to  combine  the  spatial  and  temporal  activity  preference  models  for  preference  inference.  We  evaluate  our  proposed  approach  on  three  real-world  datasets  collected  from  New  York  and  Tokyo,  and  show  that  our  STAP  model  consistently  outperforms  the  baseline  approaches  in  various  settings.
0	On  nonexistence  of  a  maximally  permissive  liveness  enforcing  pure  net  supervisor.  Behavioral  permissiveness  is  one  of  the  well-accepted  criteria  for  evaluating  the  performance  of  a  liveness-enforcing  Petri  net  supervisor.  However,  a  maximally  permissive  liveness-enforcing  pure  net  supervisor  does  not  always  exist  for  any  Petri  net.  This  paper  focuses  on  the  nonexistence  of  maximally  permissive  liveness-enforcing  pure  net  supervisors  for  a  subclass  of  Petri  nets.  It  shows  that  if  a  net  contains  a  siphon  that  cannot  be  optimally  controlled  by  a  P-invariant,  the  net  does  not  have  a  maximally  permissive  liveness-enforcing  pure  net  supervisor.  For  a  class  of  Petri  nets  that  can  model  flexible  manufacturing  systems,  sufficient  conditions  are  proposed  to  decide  whether  there  exists  a  siphon  that  cannot  be  optimally  controlled.  This  paper  also  shows  that  a  Petri  net  system  does  not  have  a  maximally  permissive  liveness-enforcing  pure  net  supervisor  if  it  contains  some  special  reachable  markings.
0	Automated  problem  list  generation  from  electronic  medical  records  in  ibm  watson.  Identifying  a  patient's  important  medical  problems  requires  broad  and  deep  medical  expertise,  as  well  as  significant  time  to  gather  all  the  relevant  facts  from  the  patient's  medical  record  and  assess  the  clinical  importance  of  the  facts  in  reaching  the  final  conclusion.  A  patient's  medical  problem  list  is  by  far  the  most  critical  information  that  a  physician  uses  in  treatment  and  care  of  a  patient.  In  spite  of  its  critical  role,  its  curation,  manual  or  automated,  has  been  an  unmet  need  in  clinical  practice.  We  developed  a  machine  learning  technique  in  IBM  Watson  to  automatically  generate  a  patient's  medical  problem  list.  The  machine  learning  model  uses  lexical  and  medical  features  extracted  from  a  patient's  record  using  NLP  techniques.  We  show  that  the  automated  method  achieves  70%  recall  and  67%  precision  based  on  the  gold  standard  that  medical  experts  created  on  a  set  of  de-identified  patient  records  from  a  major  hospital  system  in  the  US.  To  the  best  of  our  knowledge  this  is  the  first  successful  machine  learning/NLP  method  of  extracting  an  open-ended  patient's  medical  problems  from  an  Electronic  Medical  Record  (EMR).  This  paper  also  contributes  a  methodology  for  assessing  accuracy  of  a  medical  problem  list  generation  technique.
0	Action  translation  in  extensive  form  games  with  large  action  spaces  axioms  paradoxes  and  the  pseudo  harmonic  mapping.  When  solving  extensive-form  games  with  large  action  spaces,  typically  significant  abstraction  is  needed  to  make  the  problem  manageable  from  a  modeling  or  computational  perspective.  When  this  occurs,  a  procedure  is  needed  to  interpret  actions  of  the  opponent  that  fall  outside  of  our  abstraction  (by  mapping  them  to  actions  in  our  abstraction).  This  is  called  an  action  translation  mapping.  Prior  action  translation  mappings  have  been  based  on  heuristics  without  theoretical  justification.  We  show  that  the  prior  mappings  are  highly  exploitable  and  that  most  of  them  violate  certain  natural  desiderata.  We  present  a  new  mapping  that  satisfies  these  desiderata  and  has  significantly  lower  exploitability  than  the  prior  mappings.  Furthermore,  we  observe  that  the  cost  of  this  worst-case  performance  benefit  (low  exploitability)  is  not  high  in  practice;  our  mapping  performs  competitively  with  the  prior  mappings  against  no-limit  Texas  Hold'em  agents  submitted  to  the  2012  Annual  Computer  Poker  Competition.  We  also  observe  several  paradoxes  that  can  arise  when  performing  action  abstraction  and  translation;  for  example,  we  show  that  it  is  possible  to  improve  performance  by  including  suboptimal  actions  in  our  abstraction  and  excluding  optimal  actions.
0	Scram  scalable  collision  avoiding  role  assignment  with  minimal  makespan  for  formational  positioning.  Teams  of  mobile  robots  often  need  to  divide  up  subtasks  efficiently.  In  spatial  domains,  a  key  criterion  for  doing  so  may  depend  on  distances  between  robots  and  the  subtasks'  locations.  This  paper  considers  a  specific  such  criterion,  namely  how  to  assign  interchangeable  robots,  represented  as  point  masses,  to  a  set  of  target  goal  locations  within  an  open  two  dimensional  space  such  that  the  makespan  (time  for  all  robots  to  reach  their  target  locations)  is  minimized  while  also  preventing  collisions  among  robots.  We  present  scaleable  (computable  in  polynomial  time)  role  assignment  algorithms  that  we  classify  as  being  SCRAM  (Scalable  Collision-avoiding  Role  Assignment  with  Minimal-makespan).  SCRAM  role  assignment  algorithms  use  a  graph  theoretic  approach  to  map  agents  to  target  goal  locations  such  that  our  objectives  for  both  minimizing  the  makespan  and  avoiding  agent  collisions  are  met.  A  system  using  SCRAM  role  assignment  was  originally  designed  to  allow  for  decentralized  coordination  among  physically  realistic  simulated  humanoid  soccer  playing  robots  in  the  partially  observable,  non-deterministic,  noisy,  dynamic,  and  limited  communication  setting  of  the  RoboCup  3D  simulation  league.  In  its  current  form,  SCRAM  role  assignment  generalizes  well  to  many  realistic  and  real-world  multiagent  systems,  and  scales  to  thousands  of  agents.
0	Awareness  in  mixed  initiative  planning.  For  tasks  that  need  to  be  accomplished  in  unconstrained  environments,  as  in  the  case  of  Urban  Search  and  Rescue  (USAR),  human-robot  collaboration  is  considered  as  an  indispensable  component.  Collaboration  is  based  on  accurate  models  of  robot  and  human  perception  consistent  with  one  another,  so  that  exchange  of  information  critical  to  the  accomplishment  of  a  task  is  performed  efficiently  and  in  a  simplified  fashion  to  minimize  the  interaction  overhead.  In  this  paper,  we  highlight  the  features  of  a  human-robot  team,  i.e.  how  robot  perception  may  be  combined  with  human  perception  based  on  a  task-driven  direction  for  USAR.  We  elaborate  on  the  design  of  the  components  of  a  mixed-initiative  system  wherein  a  task  assigned  to  the  robot  is  planned  and  executed  jointly  with  the  human  operator  as  a  result  of  their  interaction.  Our  description  is  solidified  by  demonstrating  the  application  of  mixed-initiative  planning  in  a  number  of  examples  related  to  the  morphological  adaptation  of  the  rescue  robot.
0	Spectral  bisection  tree  guided  deep  adaptive  exemplar  autoencoder  for  unsupervised  domain  adaptation.  Learning  with  limited  labeled  data  is  always  a  challenge  in  AI  problems,  and  one  of  promising  ways  is  transferring  well-established  source  domain  knowledge  to  the  target  domain,  i.e.,  domain  adaptation.  In  this  paper,  we  extend  the  deep  representation  learning  to  domain  adaptation  scenario,  and  propose  a  novel  deep  model  called  "Deep  Adaptive  Exemplar  AutoEncoder  (DAE2)".  Different  from  conventional  de-noising  autoencoders  using  corrupted  inputs,  we  assign  semantics  to  the  input-output  pairs  of  the  autoencoders,  which  allow  us  to  gradually  extract  discriminant  features  layer  by  layer.  To  this  end,  first,  we  build  a  spectral  bisection  tree  to  generate  source-target  data  compositions  as  the  training  pairs  fed  to  autoencoders.  Second,  a  low-rank  coding  regularizer  is  imposed  to  ensure  the  transferability  of  the  learned  hidden  layer.  Finally,  a  supervised  layer  is  added  on  top  to  transform  learned  representations  into  discriminant  features.  The  problem  above  can  be  solved  iteratively  in  an  EM  fashion  of  learning.  Extensive  experiments  on  domain  adaptation  tasks  including  object,  handwritten  digits,  and  text  data  classifications  demonstrate  the  effectiveness  of  the  proposed  method.
0	A  theoretically  guaranteed  deep  optimization  framework  for  robust  compressive  sensing  mri.  Magnetic  Resonance  Imaging  (MRI)  is  one  of  the  most  dynamic  and  safe  imaging  techniques  available  for  clinical  applications.  However,  the  rather  slow  speed  of  MRI  acquisitions  limits  the  patient  throughput  and  potential  indications.  Compressive  Sensing  (CS)  has  proven  to  be  an  efficient  technique  for  accelerating  MRI  acquisition.  The  most  widely  used  CS-MRI  model,  founded  on  the  premise  of  reconstructing  an  image  from  an  incompletely  filled  k-space,  leads  to  an  ill-posed  inverse  problem.  In  the  past  years,  lots  of  efforts  have  been  made  to  efficiently  optimize  the  CS-MRI  model.  Inspired  by  deep  learning  techniques,  some  preliminary  works  have  tried  to  incorporate  deep  architectures  into  CS-MRI  process.  Unfortunately,  the  convergence  issues  (due  to  the  experience-based  networks)  and  the  robustness  (i.e.,  lack  real-world  noise  modeling)  of  these  deeply  trained  optimization  methods  are  still  missing.  In  this  work,  we  develop  a  new  paradigm  to  integrate  designed  numerical  solvers  and  the  data-driven  architectures  for  CS-MRI.  By  introducing  an  optimal  condition  checking  mechanism,  we  can  successfully  prove  the  convergence  of  our  established  deep  CS-MRI  optimization  scheme.  Furthermore,  we  explicitly  formulate  the  Rician  noise  distributions  within  our  framework  and  obtain  an  extended  CS-MRI  network  to  handle  the  real-world  nosies  in  the  MRI  process.  Extensive  experimental  results  verify  that  the  proposed  paradigm  outperforms  the  existing  state-of-theart  techniques  both  in  reconstruction  accuracy  and  efficiency  as  well  as  robustness  to  noises  in  real  scene.
0	Deep  tamer  interactive  agent  shaping  in  high  dimensional  state  spaces.  While  recent  advances  in  deep  reinforcement  learning  have  allowed  autonomous  learning  agents  to  succeed  at  a  variety  of  complex  tasks,  existing  algorithms  generally  require  a  lot  of  training  data.  One  way  to  increase  the  speed  at  which  agents  are  able  to  learn  to  perform  tasks  is  by  leveraging  the  input  of  human  trainers.  Although  such  input  can  take  many  forms,  real-time,  scalar-valued  feedback  is  especially  useful  in  situations  where  it  proves  difficult  or  impossible  for  humans  to  provide  expert  demonstrations.  Previous  approaches  have  shown  the  usefulness  of  human  input  provided  in  this  fashion  (e.g.,  the  TAMER  framework),  but  they  have  thus  far  not  considered  high-dimensional  state  spaces  or  employed  the  use  of  deep  learning.  In  this  paper,  we  do  both:  we  propose  Deep  TAMER,  an  extension  of  the  TAMER  framework  that  leverages  the  representational  power  of  deep  neural  networks  in  order  to  learn  complex  tasks  in  just  a  short  amount  of  time  with  a  human  trainer.  We  demonstrate  Deep  TAMER's  success  by  using  it  and  just  15  minutes  of  human-provided  feedback  to  train  an  agent  that  performs  better  than  humans  on  the  Atari  game  of  Bowling  -  a  task  that  has  proven  difficult  for  even  state-of-the-art  reinforcement  learning  methods.
0	Crash  to  not  crash  learn  to  identify  dangerous  vehicles  using  a  simulator.  Developing  a  computer  vision-based  algorithm  for  identifying  dangerous  vehicles  requires  a  large  amount  of  labeled  accident  data,  which  is  difficult  to  collect  in  the  real  world.  To  tackle  this  challenge,  we  first  develop  a  synthetic  data  generator  built  on  top  of  a  driving  simulator.  We  then  observe  that  the  synthetic  labels  that  are  generated  based  on  simulation  results  are  very  noisy,  resulting  in  poor  classification  performance.  In  order  to  improve  the  quality  of  synthetic  labels,  we  propose  a  new  label  adaptation  technique  that  first  extracts  internal  states  of  vehicles  from  the  underlying  driving  simulator,  and  then  refines  labels  by  predicting  future  paths  of  vehicles  based  on  a  well-studied  motion  model.  Via  real-data  experiments,  we  show  that  our  dangerous  vehicle  classifier  can  reduce  the  missed  detection  rate  by  at  least  18.5%  compared  with  those  trained  with  real  data  when  time-to-collision  is  between  1.6s  and  1.8s.
0	From  label  smoothing  to  label  relaxation.  Regularization  of  (deep)  learning  models  can  be  realized  at  the  model,  loss,  or  data  level.  As  a  technique  somewhere  in-between  loss  and  data,  label  smoothing  turns  deterministic  class  labels  into  probability  distributions,  for  example  by  uniformly  distributing  a  certain  part  of  the  probability  mass  over  all  classes.  A  predictive  model  is  then  trained  on  these  distributions  as  targets,  using  cross-entropy  as  loss  function.  While  this  method  has  shown  improved  performance  compared  to  non-smoothed  cross-entropy,  we  argue  that  the  use  of  a  smoothed  though  still  precise  probability  distribution  as  a  target  can  be  questioned  from  a  theoretical  perspective.  As  an  alternative,  we  propose  a  generalized  technique  called  label  relaxation,  in  which  the  target  is  a  set  of  probabilities  represented  in  terms  of  an  upper  probability  distribution.  This  leads  to  a  genuine  relaxation  of  the  target  instead  of  a  distortion,  thereby  reducing  the  risk  of  incorporating  an  undesirable  bias  in  the  learning  process.  Methodically,  label  relaxation  leads  to  the  minimization  of  a  novel  type  of  loss  function,  for  which  we  propose  a  suitable  closed-form  expression  for  model  optimization.  The  effectiveness  of  the  approach  is  demonstrated  in  an  empirical  study  on  image  data.
0	Deep  time  stream  framework  for  click  through  rate  prediction  by  tracking  interest  evolution.  Click-through  rate  (CTR)  prediction  is  an  essential  task  in  industrial  applications  such  as  video  recommendation.  Recently,  deep  learning  models  have  been  proposed  to  learn  the  representation  of  users'  overall  interests,  while  ignoring  the  fact  that  interests  may  dynamically  change  over  time.  We  argue  that  it  is  necessary  to  consider  the  continuous-time  information  in  CTR  models  to  track  user  interest  trend  from  rich  historical  behaviors.  In  this  paper,  we  propose  a  novel  Deep  Time-Stream  framework  (DTS)  which  introduces  the  time  information  by  an  ordinary  differential  equations  (ODE).  DTS  continuously  models  the  evolution  of  interests  using  a  neural  network,  and  thus  is  able  to  tackle  the  challenge  of  dynamically  representing  users'  interests  based  on  their  historical  behaviors.  In  addition,  our  framework  can  be  seamlessly  applied  to  any  existing  deep  CTR  models  by  leveraging  the  additional  Time-Stream  Module,  while  no  changes  are  made  to  the  original  CTR  models.  Experiments  on  public  dataset  as  well  as  real  industry  dataset  with  billions  of  samples  demonstrate  the  effectiveness  of  proposed  approaches,  which  achieve  superior  performance  compared  with  existing  methods.
0	Repetitive  reprediction  deep  decipher  for  semi  supervised  learning.  Most  recent  semi-supervised  deep  learning  (deep  SSL)  methods  used  a  similar  paradigm:  use  network  predictions  to  update  pseudo-labels  and  use  pseudo-labels  to  update  network  parameters  iteratively.  However,  they  lack  theoretical  support  and  cannot  explain  why  predictions  are  good  candidates  for  pseudo-labels.  In  this  paper,  we  propose  a  principled  end-to-end  framework  named  deep  decipher  (D2)  for  SSL.  Within  the  D2  framework,  we  prove  that  pseudo-labels  are  related  to  network  predictions  by  an  exponential  link  function,  which  gives  a  theoretical  support  for  using  predictions  as  pseudo-labels.  Furthermore,  we  demonstrate  that  updating  pseudo-labels  by  network  predictions  will  make  them  uncertain.  To  mitigate  this  problem,  we  propose  a  training  strategy  called  repetitive  reprediction  (R2).  Finally,  the  proposed  R2-D2  method  is  tested  on  the  large-scale  ImageNet  dataset  and  outperforms  state-of-the-art  methods  by  5  percentage  points.
0	Adversarial  domain  adaptation  with  domain  mixup.  Recent  works  on  domain  adaptation  reveal  the  effectiveness  of  adversarial  learning  on  filling  the  discrepancy  between  source  and  target  domains.  However,  two  common  limitations  exist  in  current  adversarial-learning-based  methods.  First,  samples  from  two  domains  alone  are  not  sufficient  to  ensure  domain-invariance  at  most  part  of  latent  space.  Second,  the  domain  discriminator  involved  in  these  methods  can  only  judge  real  or  fake  with  the  guidance  of  hard  label,  while  it  is  more  reasonable  to  use  soft  scores  to  evaluate  the  generated  images  or  features,  i.e.,  to  fully  utilize  the  inter-domain  information.  In  this  paper,  we  present  adversarial  domain  adaptation  with  domain  mixup  (DM-ADA),  which  guarantees  domain-invariance  in  a  more  continuous  latent  space  and  guides  the  domain  discriminator  in  judging  samples'  difference  relative  to  source  and  target  domains.  Domain  mixup  is  jointly  conducted  on  pixel  and  feature  level  to  improve  the  robustness  of  models.  Extensive  experiments  prove  that  the  proposed  approach  can  achieve  superior  performance  on  tasks  with  various  degrees  of  domain  shift  and  data  complexity.
0	Constructing  models  of  user  and  task  characteristics  from  eye  gaze  data  for  user  adaptive  information  highlighting.  A  user-adaptive  information  visualization  system  capable  of  learning  models  of  users  and  the  visualization  tasks  they  perform  could  provide  interventions  optimized  for  helping  specific  users  in  specific  task  contexts.  In  this  paper,  we  investigate  the  accuracy  of  predicting  visualization  tasks,  user  performance  on  tasks,  and  user  traits  from  gaze  data.  We  show  that  predictions  made  with  a  logistic  regression  model  are  significantly  better  than  a  baseline  classifier,  with  particularly  strong  results  for  predicting  task  type  and  user  performance.  Furthermore,  we  compare  classifiers  built  with  interface-independent  and  interface-dependent  features,  and  show  that  the  interface-independent  features  are  comparable  or  superior  to  interface-dependent  ones.  Finally,  we  discuss  how  the  accuracy  of  predictive  models  is  affected  if  they  are  trained  with  data  from  trials  that  had  highlighting  interventions  added  to  the  visualization.
0	Learning  greedy  policies  for  the  easy  first  framework.  Easy-first,  a  search-based  structured  prediction  approach,  has  been  applied  to  many  NLP  tasks  including  dependency  parsing  and  coreference  resolution.  This  approach  employs  a  learned  greedy  policy  (action  scoring  function)  to  make  easy  decisions  first,  which  constrains  the  remaining  decisions  and  makes  them  easier.  We  formulate  greedy  policy  learning  in  the  Easy-first  approach  as  a  novel  non-convex  optimization  problem  and  solve  it  via  an  efficient  Majorization  Minimization  (MM)  algorithm.  Results  on  within-document  coreference  and  cross-document  joint  entity  and  event  coreference  tasks  demonstrate  that  the  proposed  approach  achieves  statistically  significant  performance  improvement  over  existing  training  regimes  for  Easy-first  and  is  less  susceptible  to  overfitting.
0	Multi  level  deep  cascade  trees  for  conversion  rate  prediction  in  recommendation  system.  Developing  effective  and  efficient  recommendation  methods  is  very  challenging  for  modern  e-commerce  platforms.  Generally  speaking,  two  essential  modules  named  “ClickThrough  Rate  Prediction”  (CTR)  and  “Conversion  Rate  Prediction”  (CVR)  are  included,  where  CVR  module  is  a  crucial  factor  that  affects  the  final  purchasing  volume  directly.  However,  it  is  indeed  very  challenging  due  to  its  sparseness  nature.  In  this  paper,  we  tackle  this  problem  by  proposing  multiLevel  Deep  Cascade  Trees  (ldcTree),  which  is  a  novel  decision  tree  ensemble  approach.  It  leverages  deep  cascade  structures  by  stacking  Gradient  Boosting  Decision  Trees  (GBDT)  to  effectively  learn  feature  representation.  In  addition,  we  propose  to  utilize  the  cross-entropy  in  each  tree  of  the  preceding  GBDT  as  the  input  feature  representation  for  next  level  GBDT,  which  has  a  clear  explanation,  i.e.,  a  traversal  from  root  to  leaf  nodes  in  the  next  level  GBDT  corresponds  to  the  combination  of  certain  traversals  in  the  preceding  GBDT.  The  deep  cascade  structure  and  the  combination  rule  enable  the  proposed  ldcTree  to  have  a  stronger  distributed  feature  representation  ability.  Moreover,  inspired  by  ensemble  learning,  we  propose  an  Ensemble  ldcTree  (E-ldcTree)  to  encourage  the  model’s  diversity  and  enhance  the  representation  ability  further.  Finally,  we  propose  an  improved  Feature  learning  method  based  on  EldcTree  (F-EldcTree)  for  taking  adequate  use  of  weak  and  strong  correlation  features  identified  by  pretrained  GBDT  models.  Experimental  results  on  off-line  data  set  and  online  deployment  demonstrate  the  effectiveness  of  the  proposed  methods.
0	Thou  shalt  not  hate  countering  online  hate  speech.  Hate  content  in  social  media  is  ever  increasing.  While  Facebook,  Twitter,  Google  have  attempted  to  take  several  steps  to  tackle  the  hateful  content,  they  have  mostly  been  unsuccessful.  Counterspeech  is  seen  as  an  effective  way  of  tackling  the  online  hate  without  any  harm  to  the  freedom  of  speech.  Thus,  an  alternative  strategy  for  these  platforms  could  be  to  promote  counterspeech  as  a  defense  against  hate  content.  However,  in  order  to  have  a  successful  promotion  of  such  counterspeech,  one  has  to  have  a  deep  understanding  of  its  dynamics  in  the  online  world.  Lack  of  carefully  curated  data  largely  inhibits  such  understanding.  In  this  paper,  we  create  and  release  the  first  ever  dataset  for  counterspeech  using  comments  from  YouTube.  The  data  contains  13,924  manually  annotated  comments  where  the  labels  indicate  whether  a  comment  is  a  counterspeech  or  not.  This  data  allows  us  to  perform  a  rigorous  measurement  study  characterizing  the  linguistic  structure  of  counterspeech  for  the  first  time.  This  analysis  results  in  various  interesting  insights  such  as:  the  counterspeech  comments  receive  much  more  likes  as  compared  to  the  noncounterspeech  comments,  for  certain  communities  majority  of  the  non-counterspeech  comments  tend  to  be  hate  speech,  the  different  types  of  counterspeech  are  not  all  equally  effective  and  the  language  choice  of  users  posting  counterspeech  is  largely  different  from  those  posting  non-counterspeech  as  revealed  by  a  detailed  psycholinguistic  analysis.  Finally,  we  build  a  set  of  machine  learning  models  that  are  able  to  automatically  detect  counterspeech  in  YouTube  videos  with  an  F1-score  of  0.71.  We  also  build  multilabel  models  that  can  detect  different  types  of  counterspeech  in  a  comment  with  an  F1-score  of  0.60.
0	Video  based  sign  language  recognition  without  temporal  segmentation.  Millions  of  hearing  impaired  people  around  the  world  routinely  use  some  variants  of  sign  languages  to  communicate,  thus  the  automatic  translation  of  a  sign  language  is  meaningful  and  important.  Currently,  there  are  two  sub-problems  in  Sign  Language  Recognition  (SLR),  i.e.,  isolated  SLR  that  recognizes  word  by  word  and  continuous  SLR  that  translates  entire  sentences.  Existing  continuous  SLR  methods  typically  utilize  isolated  SLRs  as  building  blocks,  with  an  extra  layer  of  preprocessing  (temporal  segmentation)  and  another  layer  of  post-processing  (sentence  synthesis).  Unfortunately,  temporal  segmentation  itself  is  non-trivial  and  inevitably  propagates  errors  into  subsequent  steps.  Worse  still,  isolated  SLR  methods  typically  require  strenuous  labeling  of  each  word  separately  in  a  sentence,  severely  limiting  the  amount  of  attainable  training  data.  To  address  these  challenges,  we  propose  a  novel  continuous  sign  recognition  framework,  the  Hierarchical  Attention  Network  with  Latent  Space  (LS-HAN),  which  eliminates  the  preprocessing  of  temporal  segmentation.  The  proposed  LS-HAN  consists  of  three  components:  a  two-stream  Convolutional  Neural  Network  (CNN)  for  video  feature  representation  generation,  a  Latent  Space  (LS)  for  semantic  gap  bridging,  and  a  Hierarchical  Attention  Network  (HAN)  for  latent  space  based  recognition.  Experiments  are  carried  out  on  two  large  scale  datasets.  Experimental  results  demonstrate  the  effectiveness  of  the  proposed  framework.
0	Decentralized  policy  gradient  descent  ascent  for  safe  multi  agent  reinforcement  learning.  This  paper  deals  with  distributed  reinforcement  learning  problems  with  safety  constraints.  In  particular,  we  consider  that  a  team  of  agents  cooperate  in  a  shared  environment,  where  each  agent  has  its  individual  reward  function  and  safety  constraints  that  involve  all  agents'  joint  actions.  As  such,  the  agents  aim  to  maximize  the  team-average  long-term  return,  subject  to  all  the  safety  constraints.  More  intriguingly,  no  central  controller  is  assumed  to  coordinate  the  agents,  and  both  the  rewards  and  constraints  are  only  known  to  each  agent  locally/privately.  Instead,  the  agents  are  connected  by  a  peer-to-peer  communication  network  to  share  information  with  their  neighbors.  In  this  work,  we  first  formulate  this  problem  as  a  distributed  constrained  Markov  decision  process  (D-CMDP)  with  networked  agents.  Then,  we  propose  a  decentralized  policy  gradient  (PG)  method,  Safe  Dec-PG,  to  perform  policy  optimization  based  on  this  D-CMDP  model  over  a  network.  Convergence  guarantees,  together  with  numerical  results,  showcase  the  superiority  of  the  proposed  algorithm.  To  the  best  of  our  knowledge,  this  is  the  first  decentralized  PG  algorithm  that  accounts  for  the  coupled  safety  constraints  with  a  quantifiable  convergence  rate  in  multi-agent  reinforcement  learning.  Finally,  we  emphasize  that  our  algorithm  is  also  novel  in  solving  a  class  of  decentralized  stochastic  nonconvex-concave  minimax  optimization  problems,  where  both  the  algorithm  design  and  corresponding  theoretical  analysis  are  of  independent  interest.
0	Deep  feature  space  trojan  attack  of  neural  networks  by  controlled  detoxification.  Trojan  (backdoor)  attack  is  a  form  of  adversarial  attack  on  deep  neural  networks  where  the  attacker  provides  victims  with  a  model  trained/retrained  on  malicious  data.  The  backdoor  can  be  activated  when  a  normal  input  is  stamped  with  a  certain  pattern  called  trigger,  causing  misclassification.  Many  existing  trojan  attacks  have  their  triggers  being  input  space  patches/objects  (e.g.,  a  polygon  with  solid  color)  or  simple  input  transformations  such  as  Instagram  filters.  These  simple  triggers  are  susceptible  to  recent  backdoor  detection  algorithms.  We  propose  a  novel  deep  feature  space  trojan  attack  with  five  characteristics:  effectiveness,  stealthiness,  controllability,  robustness  and  reliance  on  deep  features.  We  conduct  extensive  experiments  on  9  image  classifiers  on  various  datasets  including  ImageNet  to  demonstrate  these  properties  and  show  that  our  attack  can  evade  state-of-the-art  defense.
0	Exploiting  spatial  invariance  for  scalable  unsupervised  object  tracking.  The  ability  to  detect  and  track  objects  in  the  visual  world  is  a  crucial  skill  for  any  intelligent  agent,  as  it  is  a  necessary  precursor  to  any  object-level  reasoning  process.  Moreover,  it  is  important  that  agents  learn  to  track  objects  without  supervision  (i.e.  without  access  to  annotated  training  videos)  since  this  will  allow  agents  to  begin  operating  in  new  environments  with  minimal  human  assistance.  The  task  of  learning  to  discover  and  track  objects  in  videos,  which  we  call  unsupervised  object  tracking,  has  grown  in  prominence  in  recent  years;  however,  most  architectures  that  address  it  still  struggle  to  deal  with  large  scenes  containing  many  objects.  In  the  current  work,  we  propose  an  architecture  that  scales  well  to  the  large-scene,  many-object  setting  by  employing  spatially  invariant  computations  (convolutions  and  spatial  attention)  and  representations  (a  spatially  local  object  specification  scheme).  In  a  series  of  experiments,  we  demonstrate  a  number  of  attractive  features  of  our  architecture;  most  notably,  that  it  outperforms  competing  methods  at  tracking  objects  in  cluttered  scenes  with  many  objects,  and  that  it  can  generalize  well  to  videos  that  are  larger  and/or  contain  more  objects  than  videos  encountered  during  training.
0	Effective  aer  object  classification  using  segmented  probability  maximization  learning  in  spiking  neural  networks.  Address  event  representation  (AER)  cameras  have  recently  attracted  more  attention  due  to  the  advantages  of  high  temporal  resolution  and  low  power  consumption,  compared  with  traditional  frame-based  cameras.  Since  AER  cameras  record  the  visual  input  as  asynchronous  discrete  events,  they  are  inherently  suitable  to  coordinate  with  the  spiking  neural  network  (SNN),  which  is  biologically  plausible  and  energy-efficient  on  neuromorphic  hardware.  However,  using  SNN  to  perform  the  AER  object  classification  is  still  challenging,  due  to  the  lack  of  effective  learning  algorithms  for  this  new  representation.  To  tackle  this  issue,  we  propose  an  AER  object  classification  model  using  a  novel  segmented  probability-maximization  (SPA)  learning  algorithm.  Technically,  1)  the  SPA  learning  algorithm  iteratively  maximizes  the  probability  of  the  classes  that  samples  belong  to,  in  order  to  improve  the  reliability  of  neuron  responses  and  effectiveness  of  learning;  2)  a  peak  detection  (PD)  mechanism  is  introduced  in  SPA  to  locate  informative  time  points  segment  by  segment,  based  on  which  information  within  the  whole  event  stream  can  be  fully  utilized  by  the  learning.  Extensive  experimental  results  show  that,  compared  to  state-of-the-art  methods,  not  only  our  model  is  more  effective,  but  also  it  requires  less  information  to  reach  a  certain  level  of  accuracy.
0	Diagnosis  using  labeled  petri  nets  with  silent  or  undistinguishable  fault  events.  A  commonplace  assumption  in  the  fault  diagnosis  of  discrete  event  systems  (DESs)  is  that  of  modeling  faulty  events  with  unobservable  transitions,  i.e.,  transitions  whose  occurrence  does  not  produce  any  observable  label.  The  diagnostic  system  must  thus  infer  the  occurrence  of  a  fault  from  the  observed  behavior  corresponding  to  the  firing  of  nonfaulty  transitions.  The  presence  of  nonfaulty  unobservable  transitions  is  a  source  of  additional  complexity  in  the  diagnostic  procedure.  In  this  paper,  we  assume  that  fault  events  can  also  be  modeled  by  observable  transitions,  i.e.,  transitions  whose  occurrence  produces  an  observable  label.  This  does  not  mean,  however,  that  the  occurrence  of  such  a  transition  can  be  unambiguously  detected:  In  fact,  the  same  label  may  be  shared  with  other  fault  transitions  (e.g.,  belonging  to  different  fault  classes)  or  with  other  nonfaulty  transitions.  We  generalize  to  this  new  setting  our  previous  results  on  the  diagnosis  of  DESs  using  Petri  nets  based  on  the  notions  of  minimal  explanations  and  basis  markings.  The  presented  procedure  does  not  require  the  enumeration  of  the  complete  reachability  set  but  only  of  the  subset  of  basis  markings,  thus  reducing  the  computational  complexity  of  solving  a  diagnosis  problem.
0	Finite  time  cluster  synchronization  of  lur  e  networks  a  nonsmooth  approach.  This  paper  is  devoted  to  the  finite-time  cluster  synchronization  issue  of  nonlinearly  coupled  complex  networks  which  consist  of  discontinuous  Lur’e  systems.  On  the  basis  of  the  definition  of  Filippov  regularization  process  and  the  measurable  selection  theorem,  the  discontinuously  nonlinear  function  is  mapped  into  a  function-valued  set,  then  a  measurable  function  is  accordingly  selected  from  the  Filippov  set  to  ensure  the  existence  of  the  solution  for  the  discontinuous  system.  By  designing  the  finite-time  pinning  controller,  some  sufficient  conditions  are  obtained  for  cluster  synchronization  of  the  identical  and  nonidentical  Lur’e  networks,  respectively.  In  addition,  the  settling  time  for  achieving  the  cluster  synchronization  is  estimated  by  applying  the  finite-time  stability  theory.  And  finally,  a  numerical  example  is  presented  to  illustrate  the  validity  of  theoretical  analysis.
0	A  smart  hmi  for  driving  safety  using  emotion  prediction  of  eeg  signals.  This  paper  provides  an  overview  on  the  past  pieces  of  literature  on  emotion  prediction  systems  and  the  different  machine  learning  algorithms  used  to  classify  emotions.  We  propose  a  system  which  incorporates  the  emotion  prediction  system  with  a  custom  Smart  Human  Machine  Interface  (SHMI)  for  vehicle  drivers  to  improve  drive  safety.  This  is  achieved  based  on  EEG  signals  and  basic  vehicle  information's  obtained  from  an  OBD  (On-Board  Diagnostics)  data.  EEG  signals  are  classified  into  four  emotional  states:  happy,  sad,  relaxed  and  angry.  In  this  paper,  we  present  an  initial  development  of  the  Smart  Human  Machine  Interface  (SHMI)  for  emotion  detection  for  vehicle  applications.  To  evaluate  the  classification  of  the  EEG  signals  we  use  Russell's  circumflex  model,  Higuchi  Fractal  Dimension  (HFD),  PSD  (Power  Spectral  Density)  for  feature  extraction  and  Support  Vector  Machines  (SVM)  for  classification.
0	New  criteria  for  stability  of  generalized  neural  networks  including  markov  jump  parameters  and  additive  time  delays.  This  paper  examines  the  problem  of  asymptotic  stability  criteria  for  Markovian  jump  generalized  neural  networks  with  successive  time-varying  delay  components.  Generalized  neural  networks  consist  of  a  finite  number  of  modes,  which  may  jump  from  one  mode  to  another  according  to  a  Markovian  chain  with  known  transition  probability.  By  constructing  novel  augmented  Lyapunov–Krasovskii  functionals  (LKFs)  with  triple  integral  terms  that  contain  more  and  more  information  on  the  state  vectors  of  the  NNs,  the  upper  bound  of  the  successive  time-varying  delays  is  formulated.  By  employing  a  new  integral  inequality  technique,  free-weighting  matrix-based  integral  inequality  approach,  and  Wirtinger  double  integral  inequality  technique  and  that  is  combined  with  the  reciprocally  convex  combination  approach  to  estimate  the  single  and  double  integral  terms  in  the  time  derivative  of  the  LKFs,  a  new  set  of  delay-dependent  conditions  for  the  asymptotic  stability  of  the  considered  NNs  are  represented  in  the  form  of  linear  matrix  inequalities.  Finally,  five  numerical  examples  are  given  to  verify  the  effectiveness  of  the  proposed  approach  with  a  four-tank  benchmark  real-world  problem.
0	Adaptive  neural  backstepping  for  a  class  of  switched  nonlinear  system  without  strict  feedback  form.  This  paper  focuses  on  backstepping-based  adaptive  neural  control  for  switched  nonlinear  systems  in  nonstrict-feedback  form.  A  structural  characteristic  of  radial  basis  function  neural  networks  is  first  developed.  With  this  structural  characteristic,  adaptive  neural  backstepping  has  been  extended  to  the  switched  nonlinear  systems  with  nonstrict-feedback  structure.  By  using  a  common  Lyapunov  function  method,  an  adaptive  neural  controller  is  constructed  by  backstepping  technique.  It  is  shown  that  under  the  action  of  the  suggested  controller,  all  the  closed-loop  signals  are  bounded  and  meanwhile  the  system  output  follows  the  desired  reference  signal  well.  Finally,  a  numerical  simulation  example  is  used  to  illustrate  the  effectiveness  of  our  results.
0	An  artificial  bee  colony  algorithm  with  a  modified  choice  function  for  the  traveling  salesman  problem.  The  Artificial  Bee  Colony  (ABC)  algorithm  is  a  swarm  intelligence  approach  which  has  initially  been  proposed  to  solve  optimization  of  mathematical  test  functions  with  a  unique  neighbourhood  search  mechanism.  However,  this  neighbourhood  search  mechanism  could  not  be  directly  applied  to  combinatorial  discrete  optimization  problems.  The  employed  and  onlooker  bees  need  to  be  equipped  with  problem-specific  perturbative  heuristics  in  order  to  tackle  combinatorial  discrete  optimization  problems.  However,  there  is  a  large  variety  of  available  problem-specific  heuristics.  In  this  paper,  a  hyper-heuristic  method,  namely  a  Modified  Choice  Function  (MCF),  is  applied  such  that  it  can  regulate  the  selection  of  the  neighbourhood  search  heuristics  adopted  by  the  employed  and  onlooker  bees  automatically.  The  proposed  MCF-based  ABC  model  is  implemented  using  the  Hyper-heuristic  Flexible  Framework  (HyFlex).  To  demonstrate  the  effectiveness  of  the  proposed  model,  ten  Traveling  Salesman  Problem  (TSP)  instances  available  in  HyFlex  have  been  evaluated.  The  empirical  results  show  that  the  proposed  model  is  able  to  statistically  outperform  four  out  of  five  ABC  variants  throughout  the  optimization  process.
0	Pupdroid  personalized  user  privacy  mechanism  for  android.  The  technological  progress  of  mobile  devices,  the  low  cost  of  this  device  and  the  facility  of  use  are  increasing  the  usability  of  mobile  devices.  Since  these  devices  provide  access  to  network,  its  owner  has  access  to  almost  limitless  amount  of  data  and  can  store  more  information  than  in  his  personal  computer  (like  phone  call  record,  list  of  contact  and  calendar).  A  point  of  attention  is  that  these  devices  have  some  resources  that,  if  misused,  can  be  dangerous  to  the  owner.  The  major  number  of  existing  mobile  devices  Operational  System  does  not  allow  the  owner  to  manage  how  the  installed  application  installed  is  using  hardware  device  and  personal  information.  In  this  paper  we  will  present  a  mechanism  to  transfer  the  control  of  access  permission  to  the  owner's  device  with  personalized  roles.  The  idea  behind  the  mechanism  is  to  provide  to  the  device  owner  a  way  to  control  the  access  permission  by  installed  applications  providing  flexibility  to  personalize  the  application  privacy.  Since  the  platform  does  not  allow  the  user  to  change  the  application  permission  after  it  is  installed,  Android  was  chosen  to  develop  the  proof  of  concept  of  the  mechanism  because  it  is  an  open  source  mobile  operational  system.  The  results  show  that  the  mechanism  will  allow  Android  user  to  improve  his  device  privacy  and  control  how  it  is  used  since  the  roles  of  access  for  each  application  can  be  personalized.  The  mechanism  will  also  provide  a  way  for  enterprises  to  build  an  application  to  manage  the  employee's  device  privacy.
0	Development  of  a  robotic  manipulator  system  for  congenital  diaphragmatic  hernia.  The  clinical  target  of  this  study  is  intratracheal  balloon  occlusion  from  minimally  invasive  fetal  surgery  for  Congenital  Diaphragmatic  Hernia  (CDH).  The  target  for  the  surgery  is  a  prenatal  temporary  tracheal  occlusion,  which  enlarges  the  fetal  lungs,  and  is  a  promising  procedure  for  severe  cases.  A  balloon  is  supposed  to  be  inserted  for  tracheal  occlusion  using  the  manipulator.  We  propose  a  novel  robotic  manipulator  for  intrauterine  fetal  surgery  for  tracheal  occlusion.  In  this  study,  the  3-unit  robotic  manipulator  with  ball  joint-shaped  arthroses  and  a  shaft  diameter  of  2.4  mm  using  a  wire-driven  mechanism  has  been  developed  to  approach  the  fetal  trachea  without  the  risk  of  damage  to  fetal  tissue.  The  robotic  manipulator  has  a  thin  structure  and  7  degrees  of  freedom  to  avoid  damage  to  the  fragile  fetal  cells.  However,  the  positioning  control  of  the  robotic  manipulator  is  very  difficult.  In  this  paper,  the  mechanism  of  the  robotic  manipulator  positioning  control  was  confirmed  by  measuring  the  motion  of  each  unit  through  the  experiment.  Additionally,  a  simulation  was  developed  to  estimate  the  mechanism  of  the  robotic  manipulator  positioning  control,  and  an  approach  method  was  considered.  The  results  of  the  evaluation  experiment  suggested  that  the  robotic  manipulator  could  approach  the  fetal  trachea  through  the  mouth,  and  that  the  contact  force  was  controlled  to  under  0.05  [N].
0	Filtering  of  t  s  fuzzy  systems  with  nonuniform  sampling.  The  problem  of  asynchronous  filtering  of  nonlinear  systems  is  investigated  in  this  paper.  To  facilitate  analysis,  a  discrete-time  Takagi-Sugeno  fuzzy  model  with  a  fast  and  uniform  period  is  introduced  to  approximate  continuous  nonlinear  systems.  A  slow  and  nonuniform  sampler  between  system  and  filter  is  proposed  to  overcome  the  contradictions  between  fast  sampling  period  and  limited  bandwidth.  The  nonuniform  sampling  interval  of  sampler  is  subject  to  a  Markov  chain.  To  make  full  use  of  the  partial  information  of  sampling  interval  which  is  accessible  to  the  filter,  the  asynchronous  filter  which  is  described  by  the  hidden  Markov  model  is  introduced  to  reduce  conservatism.  By  resorting  to  the  augmentation  approach  and  the  Lyapunov  functional  method  which  depends  on  nonuniform  sampling  interval,  some  sufficient  conditions  for  the  existence  of  asynchronous  filer  are  given  to  guarantee  the  stability  and  dissipativity  of  the  augmented  system.  The  gains  of  asynchronous  filter  are  given  via  solving  a  set  of  linear  matrix  inequalities.  Two  examples  are  utilized  to  illustrate  the  validity  of  the  developed  filter  design  technique  where  the  relationships  between  optimal  dissipative  performance  indices  and  the  mode  synchronization  rate  are  given.
0	Symmetric  non  negative  matrix  factorization  based  link  partition  method  for  overlapping  community  detection.  Partitioning  links  rather  than  nodes  is  effective  in  overlapping  community  detection  (OCD)  on  complex  networks.  However,  it  consumes  high  CPU  and  memory  overheads  because  the  volume  of  links  is  huge  especially  when  the  network  is  rather  complex.  In  this  paper,  we  proposes  a  symmetric  non-negative  matrix  factorization  (SNMF)  based  link  partition  method  called  SNMF-Link  to  overcome  this  deficiency.  In  particular,  SNMFLink  represents  data  in  a  lower-dimensional  space  spanned  by  the  node-link  incidence  matrix.  By  solving  a  lighter  SNMF  problem,  SNMF-Link  learns  the  clustering  indicators  of  each  links.  Since  traditional  multiplicative  update  rule  (MUR)  based  optimization  algorithm  for  SNMF  suffers  from  slow  convergence,  we  applied  the  augmented  Lagrangian  method  (ALM)  to  efficiently  optimize  SNMF.  Experimental  results  show  that  SNMF-Link  is  much  more  efficient  than  the  representative  clustering  algorithms  without  reducing  the  OCD  performance.
0	Shadowed  c  means  for  image  segmentation  using  local  and  non  local  spatial  information.  This  paper  introduces  some  new  image  segmentation  methods  in  the  framework  of  shadowed  c-means  clustering.  By  implanting  the  local  and  non-local  spatial  information  in  the  membership  value  estimation  procedure,  we  propose  the  Local  Spatial  Shadowed  C-Means  (LSSCM)  algorithm,  Non-local  Spatial  Shadowed  C-Means  (NLSSCM)  algorithm  and  their  combination  -  L+NLSSCM.  Compared  to  traditional  fuzzy  c-means  and  shadowed  c-means  based  approaches,  the  proposed  image  segmentation  algorithms  can  obtain  better  segmentation  results  on  test  images.  It  is  observed  the  proposed  algorithms  can  effectively  tackle  the  overlapping  among  segments  and  the  noise  problem  in  images.
0	Development  of  automatic  filtering  system  for  individually  unpleasant  data  detected  by  pupil  size  change.  We  proposed  an  automatic  filtering  system  to  classify  individual  unpleasant  emotions  represented  by  pupil-size  change  and  to  remove  similar  images  from  a  multimedia  database.  The  support  vector  machines  classifier  was  applied  to  single-trial  data  of  the  pupil  size  and  indicated  the  possibility  of  the  accurate  judgment  of  individually  unpleasant  states  immediately  after  looking  at  emotional  pictures.  We  then  constructed  the  framework  to  automatically  filter  such  unpleasant  information  from  a  picture  database,  using  the  bag  of  features  scheme  to  search  for  similar  images.
0	Using  link  structure  to  infer  opinions  in  social  networks.  The  emergence  of  online  social  networks  in  the  past  few  years  has  generated  an  enormous  amount  of  information  about  potentially  any  subject.  Valuable  data  containing  users'  opinions  and  thoughts  are  available  on  those  repositories  and  several  sentiment  analysis  techniques  have  been  proposed  that  address  the  problem  of  understanding  the  opinion  orientation  of  the  users'  postings.  In  this  paper,  we  take  a  different  perspective  to  the  problem  through  a  user  centric  approach,  which  uses  a  graph  to  model  users  (and  their  postings)  and  applies  link  mining  techniques  to  infer  opinions  of  users.  Preliminary  experiments  on  a  Twitter  corpus  have  shown  promising  results.
0	Neural  kalman  filter  for  estimating  dynamic  velocity  and  headway  distance  in  vehicle  platoon  system.  A  dynamic  feedback  system  is  developed  for  estimating  the  velocity  and  headway  distance  in  a  longitudinal  three-vehicle  platoon.  The  estimation  system  is  modeled  using  an  extended  Kalman  filter  (EKF)  and  a  Neural  Kalman  filter  (NKF)  that  estimate  the  velocity  and  headway  distance  by  measuring  the  acceleration  rate  of  some  selected  vehicles  in  the  platoon.  State  equations  of  the  EKF  are  analytically  defined  by  a  discrete  conservation  equation  of  vehicle  speed  and  headway  distance,  whereas  the  measurement  equation  is  based  on  a  conventional  car-following  model.  The  NKF,  however,  defines  both  equations  using  artificial  neural  network  models  (ANNs)  that  enable  both  equations  to  be  defined  without  using  any  analytical  equations.  Numerical  analysis  showed  that  the  NKF  reduces  the  estimation  errors  in  most  cases  even  under  unexpected  car-following  situations  as  the  ANNs  have  the  capability  of  describing  nonlinear  car-following  phenomena.  However,  some  difficulties  still  remain  unsolved  in  optimizing  NKF  parameters.  It  was  found  that  alternate  approaches  may  be  required  to  yield  more  accurate  estimates  instead  of  using  NKF.
0	An  investigation  into  thumb  rotation  using  high  density  surface  electromyography  of  extrinsic  hand  muscles.  Improving  our  understanding  of  mechanisms  driving  thumb  rotation  could  potentially  advance  the  design  of  current  upper  limb  prosthesis  to  incorporate  powered  thumb  control  increasing  the  dexterity  of  the  device.  This  study  investigates  thumb  rotation  using  high  density  surface  electromyography  (HD-sEMG)  of  extrinsic  hand  muscles.  An  apparatus  was  designed  and  developed  to  test  thumb  rotation.  Relevant  data  were  acquired  during  isometric  voluntary  contractions  with  a  20-40%  of  maximum  voluntary  contraction  (MVC)  performed  by  7  healthy  right-handed  volunteers.  Data  were  processed  and  analyzed.  Analysis  showed  that,  thumb  position  could  be  extracted  using  features  extracted  from  the  HD-sEMG  data  recorded  with  high  statistical  significance.
0	Design  and  realization  of  competence  profiling  tool  for  effective  selection  of  professionals  in  maintenance  management.  Enterprises  and  industrial  companies  strive  to  improve  their  functional  performance  by  identification  of  core  competencies  in  order  to  utilize  human  resources  and  optimise  the  knowledge  integration  processes  of  the  company.  In  addition,  maintenance  operations  are  one  of  the  most  important  sections  in  industrial  companies  which  consist  of  key  personnel  and  also  explicit  and  implicit  knowledge  resources  that  have  direct  effects  on  product  quality  and  return  on  investment.  In  the  context  of  implicit  knowledge  resources,  the  principal  objective  is  firstly  to  identify  knowledge  holders  who  are  mainly  domain  experts  (e.g.  Chief  Maintenance  Officer-CMO)  and  maintenance  practitioners  (e.g.  engineers,  technicians,  etc.),  and  secondly  to  measure  their  domain  expertise.  This  paper  presents  the  concept  and  implementation  results  of  an  interdisciplinary  research  which  aims  at  improving  the  knowledge  measuring  of  maintenance  practitioners.  In  this  way,  the  companies  are  enabled  to  deduce  rate  of  human  failures  in  maintenance  operations  by  allocating  the  right  professionals  in  the  right  positions  using  competency  profiling  of  employees.  The  implementation  results  in  developing  a  competence  profiling  tool  as  an  add-on  for  Computerized  Maintenance  Management  Information  Systems  (CMMIS),  which  is  previously  developed  in  the  Institute  of  Knowledge  Based  Systems  and  Knowledge  Management  (KBS&KM).
0	Granular  association  rules  with  four  subtypes.  Relational  data  mining  approaches  look  for  patterns  that  involve  multiple  tables;  therefore  they  become  popular  in  recent  years.  In  this  paper,  we  introduce  granular  association  rules  to  reveal  connections  between  concepts  in  two  universes.  An  example  of  such  an  association  might  be  “men  like  alcohol.”  We  present  four  meaningful  explanations  corresponding  to  four  subtypes  of  granular  association  rules.  We  also  define  five  measures  to  evaluate  the  quality  of  rules.  Based  on  these  measures,  the  relationships  among  different  subtypes  are  revealed.  This  work  opens  a  new  research  trend  concerning  granular  computing  and  associate  rule  mining.
0	Three  way  clustering  method  for  incomplete  information  system  based  on  set  pair  analysis.  Traditional  clustering  algorithms  clearly  assign  uncertain  information  into  a  single  cluster,  which  does  not  fully  indicate  that  a  cluster  may  not  have  a  clear  boundary.  For  a  large  number  of  missing  data,  the  traditional  clustering  method  cannot  achieve  a  good  clustering  effect  on  these  datasets.  Therefore,  the  idea  of  three-way  decision  is  introduced  into  the  traditional  k-means  clustering,  as  a  result,  the  knowledge  of  set-pair  information  granule  be  combined.  This  paper  presents  a  three-way  clustering  method  which  can  process  missing  values  effectively.  First,  for  missing  values,  the  granularity  corresponding  to  missing  values  are  recorded  as  the  degree  of  difference.  Next,  the  algorithm  is  going  to  establish  the  distance  between  the  samples  and  the  clustering  centers  according  to  the  set-pair  theory.  All  samples  are  assigned  into  clusters  according  to  the  size  of  the  distance,  and  the  clustering  results  with  three-way  are  formed,  which  are  positive  region,  boundary  region  and  negative  region,  which  improves  the  structure  of  clustering  results.  The  samples  of  positive  region  certainly  belong  to  this  cluster;  the  samples  of  boundary  region  may  belong  to  this  cluster;  the  samples  of  negative  region  don’t  belong  to  this  cluster;  and  the  clustering  results  are  represented  by  the  three  regions  together.  Finally,  the  validity  of  the  algorithm  is  verified  by  UCI  dataset  great  work.
0	How  important  is  weight  symmetry  in  backpropagation.  Gradient  backpropagation  (BP)  requires  symmetric  feedforward  and  feedback  connections—the  same  weights  must  be  used  for  forward  and  backward  passes.  This  "weight  transport  problem"  (Grossberg  1987)  is  thought  to  be  one  of  the  main  reasons  to  doubt  BP's  biologically  plausibility.  Using  15  different  classification  datasets,  we  systematically  investigate  to  what  extent  BP  really  depends  on  weight  symmetry.  In  a  study  that  turned  out  to  be  surprisingly  similar  in  spirit  to  Lillicrap  et  al.'s  demonstration  (Lillicrap  et  al.  2014)  but  orthogonal  in  its  results,  our  experiments  indicate  that:  (1)  the  magnitudes  of  feedback  weights  do  not  matter  to  performance  (2)  the  signs  of  feedback  weights  do  matter—the  more  concordant  signs  between  feedforward  and  their  corresponding  feedback  connections,  the  better  (3)  with  feedback  weights  having  random  magnitudes  and  100%  concordant  signs,  we  were  able  to  achieve  the  same  or  even  better  performance  than  SGD.  (4)  some  normalizations/stabilizations  are  indispensable  for  such  asymmetric  BP  to  work,  namely  Batch  Normalization  (BN)  (Ioffe  and  Szegedy  2015)  and/or  a  "Batch  Manhattan"  (BM)  update  rule.
0	An  interior  point  approach  to  large  games  of  incomplete  information.  Since  their  discovery  30  years  ago,  interior  point  methods  deliver  the  most  competitive  algorithms  for  large  scale  optimization.  Surprisingly,  even  when  games  of  incomplete  information  can  be  formulated  as  a  linear  program,  interior  point  methods  have  been  discarded  in  favor  of  usually  less  attractive  methods.  This  paper  describes  how  specialized  interior  point  methods  can  also  scale  to  large  games.
0	A  unified  approach  to  online  matching  with  conflict  aware  constraints.  Online  bipartite  matching  and  allocation  models  are  widely  used  to  analyze  and  design  markets  such  as  Internet  advertising,  online  labor,  and  crowdsourcing.  Traditionally,  vertices  on  one  side  of  the  market  are  fixed  and  known  a  priori,  while  vertices  on  the  other  side  arrive  online  and  are  matched  by  a  central  agent  to  the  offline  side.  The  issue  of  possible  conflicts  among  offline  agents  emerges  in  various  real  scenarios  when  we  need  to  match  each  online  agent  with  a  set  of  offline  agents.For  example,  in  event-based  social  networks  (e.g.,  Meetup),  offline  events  conflict  for  some  users  since  they  will  be  unable  to  attend  mutually-distant  events  at  proximate  times;  in  advertising  markets,  two  competing  firms  may  prefer  not  to  be  shown  to  one  user  simultaneously;  and  in  online  recommendation  systems  (e.g.,  Amazon  Books),  books  of  the  same  type  “conflict”  with  each  other  in  some  sense  due  to  the  diversity  requirement  for  each  online  buyer.The  conflict  nature  inherent  among  certain  offline  agents  raises  significant  challenges  in  both  modeling  and  online  algorithm  design.  In  this  paper,  we  propose  a  unifying  model,  generalizing  the  conflict  models  proposed  in  (She  et  al.,  TKDE  2016)  and  (Chen  et  al.,  TKDE  16).  Our  model  can  capture  not  only  a  broad  class  of  conflict  constraints  on  the  offline  side  (which  is  even  allowed  to  be  sensitive  to  each  online  agent),  but  also  allows  a  general  arrival  pattern  for  the  online  side  (which  is  allowed  to  change  over  the  online  phase).  We  propose  an  efficient  linear  programming  (LP)  based  online  algorithm  and  prove  theoretically  that  it  has  nearly-optimal  online  performance.  Additionally,  we  propose  two  LP-based  heuristics  and  test  them  against  two  natural  baselines  on  both  real  and  synthetic  datasets.  Our  LP-based  heuristics  experimentally  dominate  the  baseline  algorithms,  aligning  with  our  theoretical  predictions  and  supporting  our  unified  approach.
0	Optimizing  resilience  in  large  scale  networks.  We  propose  a  decision  making  framework  to  optimize  the  resilience  of  road  networks  to  natural  disasters  such  as  floods.  Our  model  generalizes  an  existing  one  for  this  problem  by  allowing  roads  with  a  broad  class  of  stochastic  delay  models.  We  then  present  a  fast  algorithm  based  on  the  sample  average  approximation  (SAA)  method  and  network  design  techniques  to  solve  this  problem  approximately.  On  a  small  existing  benchmark,  our  algorithm  produces  near-optimal  solutions  and  the  SAA  method  converges  quickly  with  a  small  number  of  samples.  We  then  apply  our  algorithm  to  a  large  real-world  problem  to  optimize  the  resilience  of  a  road  network  to  failures  of  stream  crossing  structures  to  minimize  travel  times  of  emergency  medical  service  vehicles.  On  medium-sized  networks,  our  algorithm  obtains  solutions  of  comparable  quality  to  a  greedy  baseline  method  but  is  30-60  times  faster.  Our  algorithm  is  the  only  existing  algorithm  that  can  scale  to  the  full  network,  which  has  many  thousands  of  edges.
0	Lifting  model  sampling  for  general  game  playing  to  incomplete  information  models.  General  Game  Playing  is  the  design  of  AI  systems  able  to  understand  the  rules  of  new  games  and  to  use  such  descriptions  to  play  those  games  effectively.  Games  with  incomplete  information  have  recently  been  added  as  a  new  challenge  for  general  game-playing  systems.  The  only  published  solutions  to  this  challenge  are  based  on  sampling  complete  information  models.  In  doing  so  they  ground  all  of  the  unknown  information,  thereby  making  information  gathering  moves  of  no  value;  a  well-known  criticism  of  such  sampling  based  systems.  We  present  and  analyse  a  method  for  escalating  reasoning  from  complete  information  models  to  incomplete  information  models  and  show  how  this  enables  a  general  game  player  to  correctly  value  information  in  incomplete  information  games.  Experimental  results  demonstrate  the  success  of  this  technique  over  standard  model  sampling.
0	Integrating  clustering  and  multi  document  summarization  by  bi  mixture  probabilistic  latent  semantic  analysis  plsa  with  sentence  bases.  Probabilistic  Latent  Semantic  Analysis  (PLSA)  has  been  popularly  used  in  document  analysis.  However,  as  it  is  currently  formulated,  PLSA  strictly  requires  the  number  of  word  latent  classes  to  be  equal  to  the  number  of  document  latent  classes.  In  this  paper,  we  propose  Bi-mixture  PLSA,  a  new  formulation  of  PLSA  that  allows  the  number  of  latent  word  classes  to  be  different  from  the  number  of  latent  document  classes.  We  further  extend  Bi-mixture  PLSA  to  incorporate  the  sentence  information,  and  propose  Bi-mixture  PLSA  with  sentence  bases  (Bi-PLSAS)  to  simultaneously  cluster  and  summarize  the  documents  utilizing  the  mutual  influence  of  the  document  clustering  and  summarization  procedures.  Experiments  on  real-world  datasets  demonstrate  the  effectiveness  of  our  proposed  methods.
0	Spin  structure  preserving  inner  offset  network  for  scene  text  recognition.  Arbitrary  text  appearance  poses  a  great  challenge  in  scene  text  recognition  tasks.  Existing  works  mostly  handle  with  the  problem  in  consideration  of  the  shape  distortion,  including  perspective  distortions,  line  curvature  or  other  style  variations.  Rectification  (i.e.,  spatial  transformers)  as  the  preprocessing  stage  is  one  popular  approach  and  extensively  studied.  However,  chromatic  difficulties  in  complex  scenes  have  not  been  paid  much  attention  on.  In  this  work,  we  introduce  a  new  learnable  geometric-unrelated  rectification,  Structure-Preserving  Inner  Offset  Network  (SPIN),  which  allows  the  color  manipulation  of  source  data  within  the  network.  This  differentiable  module  can  be  inserted  before  any  recognition  architecture  to  ease  the  downstream  tasks,  giving  neural  networks  the  ability  to  actively  transform  input  intensity  rather  than  only  the  spatial  rectification.  It  can  also  serve  as  a  complementary  module  to  known  spatial  transformations  and  work  in  both  independent  and  collaborative  ways  with  them.  Extensive  experiments  show  the  proposed  transformation  outperforms  existing  rectification  networks  and  has  comparable  performance  among  the  state-of-the-arts.
0	Memcap  memorizing  style  knowledge  for  image  captioning.  Generating  stylized  captions  for  images  is  a  challenging  task  since  it  requires  not  only  describing  the  content  of  the  image  accurately  but  also  expressing  the  desired  linguistic  style  appropriately.  In  this  paper,  we  propose  MemCap,  a  novel  stylized  image  captioning  method  that  explicitly  encodes  the  knowledge  about  linguistic  styles  with  memory  mechanism.  Rather  than  relying  heavily  on  a  language  model  to  capture  style  factors  in  existing  methods,  our  method  resorts  to  memorizing  stylized  elements  learned  from  training  corpus.  Particularly,  we  design  a  memory  module  that  comprises  a  set  of  embedding  vectors  for  encoding  style-related  phrases  in  training  corpus.  To  acquire  the  style-related  phrases,  we  develop  a  sentence  decomposing  algorithm  that  splits  a  stylized  sentence  into  a  style-related  part  that  reflects  the  linguistic  style  and  a  content-related  part  that  contains  the  visual  content.  When  generating  captions,  our  MemCap  first  extracts  content-relevant  style  knowledge  from  the  memory  module  via  an  attention  mechanism  and  then  incorporates  the  extracted  knowledge  into  a  language  model.  Extensive  experiments  on  two  stylized  image  captioning  datasets  (SentiCap  and  FlickrStyle10K)  demonstrate  the  effectiveness  of  our  method.
0	Web  supervised  network  with  softly  update  drop  training  for  fine  grained  visual  classification.  Labeling  objects  at  the  subordinate  level  typically  requires  expert  knowledge,  which  is  not  always  available  from  a  random  annotator.  Accordingly,  learning  directly  from  web  images  for  fine-grained  visual  classification  (FGVC)  has  attracted  broad  attention.  However,  the  existence  of  noise  in  web  images  is  a  huge  obstacle  for  training  robust  deep  neural  networks.  In  this  paper,  we  propose  a  novel  approach  to  remove  irrelevant  samples  from  the  real-world  web  images  during  training,  and  only  utilize  useful  images  for  updating  the  networks.  Thus,  our  network  can  alleviate  the  harmful  effects  caused  by  irrelevant  noisy  web  images  to  achieve  better  performance.  Extensive  experiments  on  three  commonly  used  fine-grained  datasets  demonstrate  that  our  approach  is  much  superior  to  state-of-the-art  webly  supervised  methods.  The  data  and  source  code  of  this  work  have  been  made  anonymously  available  at:  https://github.com/z337-408/WSNFGVC.
0	Mining  biomedical  knowledge  using  mutual  information  abc.  The  novel  connection  between  Raynaud  disease  and  fish  oils  was  uncovered  from  two  disjointed  biomedical  literature  sets  by  Swanson  in  1986.  Since  then,  there  have  been  many  approaches  to  uncover  novel  connections  by  mining  the  biomedical  literatures.  This  paper  presents  a  Mining  Biomedical  Knowledge  method  Using  Mutual  information  ABC.  For  a  given  starting  medical  concept,  it  discovers  new,  potentially  meaningful  relations/connection  with  other  concepts  that  have  not  been  published  in  the  medical  literature  before.  The  discovered  relations/connections  are  novel  and  can  be  useful  for  domain  expert  to  conduct  new  experiment  and  try  new  treatment.
0	Single  stage  fuzzy  supply  chain  model  with  weibull  distributed  demand  for  milk  commodities.  The  newsvendor  model  is  very  simple,  but  it  is  very  important  to  study  problems  of  supply  chain  coordination.  This  model  is  also  called  a  single-cycle  inventory  model.  In  a  short  cycle,  it  is  very  difficult  to  forecast  the  demand.  The  effect  of  good  prediction  reduces  inventory  shortage,  increases  the  profit  and  enhances  competitiveness.  This  type  of  product  is  characterized  by  strong  uncertainty,  that  means  demand  always  changes.  This  creates  challenges  for  decision  makers.  For  newly  developed  products,  it  is  very  difficult  to  obtain  the  statistical  demand  distribution  of  a  product.  Here,  we  consider  the  demand  of  the  milk  products,  which  are  fluctuating  in  nature.  Hence,  demand  is  a  random  variable.  It  is  assumed  that  demand  follows  Weibull  distribution.  Also,  there  is  some  uncertainty  in  demand.  It  leads  to  the  over  estimation  or  under  estimation  of  the  parameters  of  the  distribution.  Fuzzy  triangular  numbers  are  used  to  obtain  the  uncertain  demand.  The  lifetime  of  the  items  are  very  short.  Thus,  unsold  items  are  sent  back  to  the  manufacturers.  The  retailers  and  manufacturers  profit  is  obtained  under  decentralized  and  centralized  supply  chain.  Also,the  proposed  single  period  (newsboy)  inventory  model  is  used  to  obtain  an  optimal  order  quantity.  The  proposed  methodology  is  illustrated  for  milk  product  from  Aundh  market  Pune,  India.
0	Concept  of  a  data  thread  based  parking  space  occupancy  prediction  in  a  berlin  pilot  region.  In  the  presented  research  project,  a  software  and  hardware  infrastructure  for  parking  space  focussed  inter-modal  route  planning  in  a  public  pilot  region  in  Berlin  is  developed.  One  central  topic  is  the  development  of  a  prediction  system  which  gives  an  estimated  occupancy  for  the  parking  spaces  in  the  pilot  region  for  a  given  date  and  time  in  the  future.  Occupancy  data  will  be  collected  online  by  roadside  parking  sensors  developed  within  the  project.  The  occupancy  prediction  will  be  implemented  using  “Neural  Gas”  machine  learning  in  combination  with  a  proposed  method  which  uses  data  threads  to  improve  the  prediction  quality.  In  this  paper,  a  short  overview  of  the  whole  research  project  is  given.  Furthermore,  the  concept  of  the  software  framework  and  the  learning  methods  are  presented  and  first  collected  data  is  shown.  The  prediction  method  using  data  threads  is  explained  in  more  detail.
0	On  the  axiomatic  characterization  of  runoff  voting  rules.  Runoff  voting  rules  such  as  single  transferable  vote  (STV)  and  Baldwin's  rule  are  of  particular  interest  in  computational  social  choice  due  to  their  recursive  nature  and  hardness  of  manipulation,  as  well  as  in  (human)  practice  because  they  are  relatively  easy  to  understand.  However,  they  are  not  known  for  their  compliance  with  desirable  axiomatic  properties,  which  we  attempt  to  rectify  here.  We  characterize  runoff  rules  that  are  based  on  scoring  rules  using  two  axioms:  a  weakening  of  local  independence  of  irrelevant  alternatives  and  a  variant  of  population-consistency.  We  then  show,  as  our  main  technical  result,  that  STV  is  the  only  runoff  scoring  rule  satisfying  an  independence-of-clones  property.  Furthermore,  we  provide  axiomatizations  of  Baldwin's  rule  and  Coombs'  rule.
0	Text  simplification  using  neural  machine  translation.  Text  simplification  (TS)  is  the  technique  of  reducing  the  lexical,  syntactical  complexity  of  text.  Existing  automatic  TS  systems  can  simplify  text  only  by  lexical  simplification  or  by  manually  defined  rules.  Neural  Machine  Translation  (NMT)  is  a  recently  proposed  approach  for  Machine  Translation  (MT)  that  is  receiving  a  lot  of  research  interest.  In  this  paper,  we  regard  original  English  and  simplified  English  as  two  languages,  and  apply  a  NMT  model–Recurrent  Neural  Network  (RNN)  encoder-decoder  on  TS  to  make  the  neural  network  to  learn  text  simplification  rules  by  itself.  Then  we  discuss  challenges  and  strategies  about  how  to  apply  a  NMT  model  to  the  task  of  text  simplification.
0	Optimal  planning  strategy  for  ambush  avoidance.  Operating  vehicles  in  adversarial  environments  between  a  recurring  origin-destination  pair  requires  new  planning  techniques.  Such  a  technique,  presented  in  this  paper,  is  a  game  inspired  by  Ruckle’s  original  contribution.  The  goal  of  the  first  player  is  to  minimize  the  expected  casualties  undergone  by  a  moving  agent.  The  goal  of  the  second  player  is  to  maximize  this  damage.  The  outcome  of  the  game  is  obtained  via  a  linear  program  that  solves  the  corresponding  minmax  optimization  problem  over  this  outcome.  The  formulation  originally  proposed  by  Feron  and  Joseph  is  extended  to  different  environment  models  in  order  to  compute  routing  strategies  over  unstructured  environments.  To  compare  these  methods  for  increasingly  accurate  representations  of  the  environment,  a  grid-based  model  is  chosen  to  represent  the  environment  and  the  existence  of  a  sufficient  network  size  is  highlighted.  A  global  framework  for  the  generation  of  realistic  routing  strategies  between  any  two  points  is  described.  Finally  the  practicality  of  the  proposed  framework  is  illustrated  on  real  world  environments.
0	Gazetteer  independent  toponym  resolution  using  geographic  word  profiles.  Toponym  resolution,  or  grounding  names  of  places  to  their  actual  locations,  is  an  important  problem  in  analysis  of  both  historical  corpora  and  present-day  news  and  web  content.  Recent  approaches  have  shifted  from  rule-based  spatial  minimization  methods  to  machine  learned  classifiers  that  use  features  of  the  text  surrounding  a  toponym.  Such  methods  have  been  shown  to  be  highly  effective,  but  they  crucially  rely  on  gazetteers  and  are  unable  to  handle  unknown  place  names  or  locations.  We  address  this  limitation  by  modeling  the  geographic  distributions  of  words  over  the  earth's  surface:  we  calculate  the  geographic  profile  of  each  word  based  on  local  spatial  statistics  over  a  set  of  geo-referenced  language  models.  These  geo-profiles  can  be  further  refined  by  combining  in-domain  data  with  background  statistics  from  Wikipedia.  Our  resolver  computes  the  overlap  of  all  geo-profiles  in  a  given  text  span;  without  using  a  gazetteer,  it  performs  on  par  with  existing  classifiers.  When  combined  with  a  gazetteer,  it  achieves  state-of-the-art  performance  for  two  standard  toponym  resolution  corpora  (TR-CoNLL  and  Civil  War).  Furthermore,  it  dramatically  improves  recall  when  toponyms  are  identified  by  named  entity  recognizers,  which  often  (correctly)  find  non-standard  variants  of  toponyms.
0	Efficient  subspace  segmentation  via  quadratic  programming.  We  explore  in  this  paper  efficient  algorithmic  solutions  to  robust  subspace  segmentation.  We  propose  the  SSQP,  namely  Subspace  Segmentation  via  Quadratic  Programming,  to  partition  data  drawn  from  multiple  subspaces  into  multiple  clusters.  The  basic  idea  of  SSQP  is  to  express  each  datum  as  the  linear  combination  of  other  data  regularized  by  an  overall  term  targeting  zero  reconstruction  coefficients  over  vectors  from  different  subspaces.  The  derived  coefficient  matrix  by  solving  a  quadratic  programming  problem  is  taken  as  an  affinity  matrix,  upon  which  spectral  clustering  is  applied  to  obtain  the  ultimate  segmentation  result.  Similar  to  sparse  subspace  clustering  (SCC)  and  low-rank  representation  (LRR),  SSQP  is  robust  to  data  noises  as  validated  by  experiments  on  toy  data.  Experiments  on  Hopkins  155  database  show  that  SSQP  can  achieve  competitive  accuracy  as  SCC  and  LRR  in  segmenting  affine  subspaces,  while  experimental  results  on  the  Extended  Yale  Face  Database  B  demonstrate  SSQP's  superiority  over  SCC  and  LRR.  Beyond  segmentation  accuracy,  all  experiments  show  that  SSQP  is  much  faster  than  both  SSC  and  LRR  in  the  practice  of  subspace  segmentation.
0	Using  crowdsourcing  to  improve  profanity  detection.  Profanity  detection  is  often  thought  to  be  an  easy  task.    However,  past  work  has  shown  that  current,  list-based  systems  are  performing  poorly.  They  fail  to  adapt  to  evolving  profane  slang,  identify  profane  terms  that  have  been  disguised  or  only  partially  censored  (e.g.,  @ss,  f$#%)  or  intentionally  or  unintentionally  misspelled  (e.g.,  biatch,  shiiiit).  For  these  reasons,  they  are  easy  to  circumvent  and  have  very  poor  recall.  Secondly,  they  are  a  one-size  fits  all  solution  –  making  assumptions  that  the  definition,  use  and  perceptions  of  profane  or  inappropriate  holds  across  all  contexts.  In  this  article,  we  present  work  that  attempts  to  move  beyond  list-based  profanity  detection  systems  by  identifying  the  context  in  which  profanity  occurs.    The  proposed  system  uses  a  set  of  comments  from  a  social  news  site  labeled  by  Amazon  Mechanical  Turk  workers  for  the  presence  of  profanity.  This  system  far  surpasses  the  performance  of  list-based  profanity  detection  techniques.  The  use  of  crowdsourcing  in  this  task  suggests  an  opportunity  to  build  profanity  detection  systems  tailored  to  sites  and  communities.
0	On  machine  learning  towards  predictive  sales  pipeline  analytics.  Sales  pipeline  win-propensity  prediction  is  fundamental  to  effective  sales  management.  In  contrast  to  using  subjective  human  rating,  we  propose  a  modern  machine  learning  paradigm  to  estimate  the  win-propensity  of  sales  leads  over  time.  A  profile-specific  two-dimensional  Hawkes  processes  model  is  developed  to  capture  the  influence  from  seller's  activities  on  their  leads  to  the  win  outcome,  coupled  with  lead's  personalized  profiles.  It  is  motivated  by  two  observations:  i)  sellers  tend  to  frequently  focus  their  selling  activities  and  efforts  on  a  few  leads  during  a  relatively  short  time.  This  is  evidenced  and  reflected  by  their  concentrated  interactions  with  the  pipeline,  including  login,  browsing  and  updating  the  sales  leads  which  are  logged  by  the  system;  ii)  the  pending  opportunity  is  prone  to  reach  its  win  outcome  shortly  after  such  temporally  concentrated  interactions.  Our  model  is  deployed  and  in  continual  use  to  a  large,  global,  B2B  multinational  technology  enter-prize  (Fortune  500)  with  a  case  study.  Due  to  the  generality  and  flexibility  of  the  model,  it  also  enjoys  the  potential  applicability  to  other  real-world  problems.
0	Safe  policy  improvement  with  baseline  bootstrapping  in  factored  environments.  We  present  a  novel  safe  reinforcement  learning  algorithm  that  exploits  the  factored  dynamics  of  the  environment  to  become  less  conservative.  We  focus  on  problem  settings  in  which  a  policy  is  already  running  and  the  interaction  with  the  environment  is  limited.  In  order  to  safely  deploy  an  updated  policy,  it  is  necessary  to  provide  a  confidence  level  regarding  its  expected  performance.  However,  algorithms  for  safe  policy  improvement  might  require  a  large  number  of  past  experiences  to  become  confident  enough  to  change  the  agent’s  behavior.  Factored  reinforcement  learning,  on  the  other  hand,  is  known  to  make  good  use  of  the  data  provided.  It  can  achieve  a  better  sample  complexity  by  exploiting  independence  between  features  of  the  environment,  but  it  lacks  a  confidence  level.  We  study  how  to  improve  the  sample  efficiency  of  the  safe  policy  improvement  with  baseline  bootstrapping  algorithm  by  exploiting  the  factored  structure  of  the  environment.  Our  main  result  is  a  theoretical  bound  that  is  linear  in  the  number  of  parameters  of  the  factored  representation  instead  of  the  number  of  states.  The  empirical  analysis  shows  that  our  method  can  improve  the  policy  using  a  number  of  samples  potentially  one  order  of  magnitude  smaller  than  the  flat  algorithm.
0	When  do  words  matter  understanding  the  impact  of  lexical  choice  on  audience  perception  using  individual  treatment  effect  estimation.  Studies  across  many  disciplines  have  shown  that  lexical  choice  can  affect  audience  perception.  For  example,  how  users  describe  themselves  in  a  social  media  profile  can  affect  their  perceived  socio-economic  status.  However,  we  lack  general  methods  for  estimating  the  causal  effect  of  lexical  choice  on  the  perception  of  a  specific  sentence.  While  randomized  controlled  trials  may  provide  good  estimates,  they  do  not  scale  to  the  potentially  millions  of  comparisons  necessary  to  consider  all  lexical  choices.  Instead,  in  this  paper,  we  first  offer  two  classes  of  methods  to  estimate  the  effect  on  perception  of  changing  one  word  to  another  in  a  given  sentence.  The  first  class  of  algorithms  builds  upon  quasi-experimental  designs  to  estimate  individual  treatment  effects  from  observational  data.  The  second  class  treats  treatment  effect  estimation  as  a  classification  problem.  We  conduct  experiments  with  three  data  sources  (Yelp,  Twitter,  and  Airbnb),  finding  that  the  algorithmic  estimates  align  well  with  those  produced  by  randomized-control  trials.  Additionally,  we  find  that  it  is  possible  to  transfer  treatment  effect  classifiers  across  domains  and  still  maintain  high  accuracy.
0	Infusing  multi  source  knowledge  with  heterogeneous  graph  neural  network  for  emotional  conversation  generation.  The  success  of  emotional  conversation  systems  depends  on  sufficient  perception  and  appropriate  expression  of  emotions.  In  a  real-world  conversation,  we  firstly  instinctively  perceive  emotions  from  multi-source  information,  including  the  emotion  flow  of  dialogue  history,  facial  expressions,  and  personalities  of  speakers,  and  then  express  suitable  emotions  according  to  our  personalities,  but  these  multiple  types  of  information  are  insufficiently  exploited  in  emotional  conversation  fields.  To  address  this  issue,  we  propose  a  heterogeneous  graph-based  model  for  emotional  conversation  generation.  Specifically,  we  design  a  Heterogeneous  Graph-Based  Encoder  to  represent  the  conversation  content  (i.e.,  the  dialogue  history,  its  emotion  flow,  facial  expressions,  and  speakers'  personalities)  with  a  heterogeneous  graph  neural  network,  and  then  predict  suitable  emotions  for  feedback.  After  that,  we  employ  an  Emotion-Personality-Aware  Decoder  to  generate  a  response  not  only  relevant  to  the  conversation  context  but  also  with  appropriate  emotions,  by  taking  the  encoded  graph  representations,  the  predicted  emotions  from  the  encoder  and  the  personality  of  the  current  speaker  as  inputs.  Experimental  results  show  that  our  model  can  effectively  perceive  emotions  from  multi-source  knowledge  and  generate  a  satisfactory  response,  which  significantly  outperforms  previous  state-of-the-art  models.
0	Bayesian  optimization  for  categorical  and  category  specific  continuous  inputs.  Many  real-world  functions  are  defined  over  both  categorical  and  category-specific  continuous  variables  and  thus  cannot  be  optimized  by  traditional  Bayesian  optimization  (BO)  methods.  To  optimize  such  functions,  we  propose  a  new  method  that  formulates  the  problem  as  a  multi-armed  bandit  problem,  wherein  each  category  corresponds  to  an  arm  with  its  reward  distribution  centered  around  the  optimum  of  the  objective  function  in  continuous  variables.  Our  goal  is  to  identify  the  best  arm  and  the  maximizer  of  the  corresponding  continuous  function  simultaneously.  Our  algorithm  uses  a  Thompson  sampling  scheme  that  helps  connecting  both  multi-arm  bandit  and  BO  in  a  unified  framework.  We  extend  our  method  to  batch  BO  to  allow  parallel  optimization  when  multiple  resources  are  available.  We  theoretically  analyze  our  method  for  convergence  and  prove  sub-linear  regret  bounds.  We  perform  a  variety  of  experiments:  optimization  of  several  benchmark  functions,  hyper-parameter  tuning  of  a  neural  network,  and  automatic  selection  of  the  best  machine  learning  model  along  with  its  optimal  hyper-parameters  (a.k.a  automated  machine  learning).  Comparisons  with  other  methods  demonstrate  the  effectiveness  of  our  proposed  method.
0	5  knowledge  graph  embeddings  with  projective  transformations.  Performing  link  prediction  using  knowledge  graph  embedding  models  has  become  a  popular  approach  for  knowledge  graph  completion.  Such  models  employ  a  transformation  function  that  maps  nodes  via  edges  into  a  vector  space  in  order  to  measure  the  likelihood  of  the  links.  While  mapping  the  individual  nodes,  the  structure  of  subgraphs  is  also  transformed.  Most  of  the  embedding  models  designed  in  Euclidean  geometry  usually  support  a  single  transformation  type  --  often  translation  or  rotation,  which  is  suitable  for  learning  on  graphs  with  small  differences  in  neighboring  subgraphs.  However,  multi-relational  knowledge  graphs  often  include  multiple  subgraph  structures  in  a  neighborhood  (e.g.~combinations  of  path  and  loop  structures),  which  current  embedding  models  do  not  capture  well.  To  tackle  this  problem,  we  propose  a  novel  KGE  model  5*E  in  projective  geometry,  which  supports  multiple  simultaneous  transformations  --  specifically  inversion,  reflection,  translation,  rotation,  and  homothety.  The  model  has  several  favorable  theoretical  properties  and  subsumes  the  existing  approaches.  It  outperforms  them  on  most  widely  used  link  prediction  benchmarks
0	On  fairness  in  decision  making  under  uncertainty  definitions  computation  and  comparison.  The  utilitarian  solution  criterion,  which  has  been  extensively  studied  in  multi-agent  decision  making  under  uncertainty,  aims  to  maximize  the  sum  of  individual  utilities.  However,  as  the  utilitarian  solution  often  discriminates  against  some  agents,  it  is  not  desirable  for  many  practical  applications  where  agents  have  their  own  interests  and  fairness  is  expected.  To  address  this  issue,  this  paper  introduces  egalitarian  solution  criteria  for  sequential  decision-making  under  uncertainty,  which  are  based  on  the  maximin  principle.  Motivated  by  different  application  domains,  we  propose  four  maximin  fairness  criteria  and  develop  corresponding  algorithms  for  computing  their  optimal  policies.  Furthermore,  we  analyze  the  connections  between  these  criteria  and  discuss  and  compare  their  characteristics.
0	Constrained  sampling  and  counting  universal  hashing  meets  sat  solving.  Constrained  sampling  and  counting  are  two  fundamental  problems  in  artificial  intelligence  with  a  diverse  range  of  applications,  spanning  probabilistic  reasoning  and  planning  to  constrained-random  verification.  While  the  theory  of  these  problems  was  thoroughly  investigated  in  the  1980s,  prior  work  either  did  not  scale  to  industrial  size  instances  or  gave  up  correctness  guarantees  to  achieve  scalability.  Recently,  we  proposed  a  novel  approach  that  combines  universal  hashing  and  SAT  solving  and  scales  to  formulas  with  hundreds  of  thousands  of  variables  without  giving  up  correctness  guarantees.  This  paper  provides  an  overview  of  the  key  ingredients  of  the  approach  and  discusses  challenges  that  need  to  be  overcome  to  handle  larger  real-world  instances.
0	Operational  representation  a  unifying  representation  for  activity  learning  and  problem  solving.  A  typical  AI  system  engages  many  levels  of  cognitive  processing  from  learning  to  problem  solving.  The  issue  we  would  like  to  address  in  this  paper  is:  Can  a  unified  representational  scheme  be  used  in  learning  processes  as  well  as  the  various  levels  of  cognitive  processing  from  concept  representation  to  problem  solving  including  the  generation  of  action  plans?  In  a  previous  paper  we  defined  a  set  of  representations  called  “atomic  operational  representations”  that  employs  an  explicit  representation  of  the  temporal  dimension  and  that  can  be  used  to  ground  concepts  in  the  physical  world,  such  as  concepts  that  involve  various  activities  and  interactions.  In  this  paper  we  apply  operational  representations  in  a  unified  manner  to  the  following  cognitive  processes:  1)  the  unsupervised  learning  and  encoding  of  causal  rules  of  actions  and  their  consequences;  and  2)  the  application  of  the  learned  causal  rules  to  problem  solving  processes  that  produce  desired  action  plans.  The  unique  and  explicit  temporal  characteristic  of  operational  representations  is  the  key  feature  that  allows  the  encoded  concepts  to  be  used  in  a  unified  manner  across  the  various  levels  of  cognitive  processing.  Hence,  abstractions  in  the  form  of  operational  representations  have  an  important  role  toplay  in  AI.
0	Fully  proportional  representation  with  approval  ballots  approximating  the  maxcover  problem  with  bounded  frequencies  in  fpt  time.  We  consider  the  problem  of  winner  determination  under  Chamberlin-Courant's  multiwinner  voting  rule  with  approval  utilities.  This  problem  is  equivalent  to  the  well-known  NP-complete  MaxCover  problem  (i.e.,  a  version  of  the  SetCover  problem  where  we  aim  to  cover  as  many  elements  as  possible)  and,  so,  the  best  polynomial-time  approximation  algorithm  for  it  has  approximation  ratio  1  -  1/e.  We  show  exponential-time/FPT  approximation  algorithms  that,  on  one  hand,  achieve  arbitrarily  good  approximation  ratios  and,  on  the  other  hand,  have  running  times  much  better  than  known  exact  algorithms.  We  focus  on  the  cases  where  the  voters  have  to  approve  of  at  most/at  least  a  given  number  of  candidates.
0	Joint  extraction  of  entities  and  overlapping  relations  using  position  attentive  sequence  labeling.  Joint  entity  and  relation  extraction  is  to  detect  entity  and  relation  using  a  single  model.  In  this  paper,  we  present  a  novel  unified  joint  extraction  model  which  directly  tags  entity  and  relation  labels  according  to  a  query  word  position  p,  i.e.,  detecting  entity  at  p,  and  identifying  entities  at  other  positions  that  have  relationship  with  the  former.  To  this  end,  we  first  design  a  tagging  scheme  to  generate  n  tag  sequences  for  an  n-word  sentence.  Then  a  position-attention  mechanism  is  introduced  to  produce  different  sentence  representations  for  every  query  position  to  model  these  n  tag  sequences.  In  this  way,  our  method  can  simultaneously  extract  all  entities  and  their  type,  as  well  as  all  overlapping  relations.  Experiment  results  show  that  our  framework  performances  significantly  better  on  extracting  overlapping  relations  as  well  as  detecting  long-range  relation,  and  thus  we  achieve  state-of-the-art  performance  on  two  public  datasets.
0	Representational  issues  in  the  debate  on  the  standard  model  of  the  mind.  In  this  paper  we  discuss  some  of  the  issues  concerning  the  Memory  and  Content  aspects  in  the  recent  debate  on  the  identification  of  a  Standard  Model  of  the  Mind  (Laird,  Lebiere,  and  Rosenbloom  in  press).  In  particular,  we  focus  on  the  representational  models  concerning  the  Declarative  Memories  of  current  Cognitive  Architectures  (CAs).  In  doing  so  we  outline  some  of  the  main  problems  affecting  the  current  CAs  and  suggest  that  the  Conceptual  Spaces,  a  representational  framework  developed  by  Gardenfors,  is  worth-considering  to  address  such  problems.  Finally,  we  briefly  analyze  the  alternative  representational  assumptions  employed  in  the  three  CAs  constituting  the  current  baseline  for  the  Standard  Model  (i.e.  SOAR,  ACT-R  and  Sigma).  In  doing  so,  we  point  out  the  respective  differences  and  discuss  their  implications  in  the  light  of  the  analyzed  problems.
0	Analysing  the  noise  model  error  for  realistic  noisy  label  data.  Distant  and  weak  supervision  allow  to  obtain  large  amounts  of  labeled  training  data  quickly  and  cheaply,  but  these  automatic  annotations  tend  to  contain  a  high  amount  of  errors.  A  popular  technique  to  overcome  the  negative  effects  of  these  noisy  labels  is  noise  modelling  where  the  underlying  noise  process  is  modelled.  In  this  work,  we  study  the  quality  of  these  estimated  noise  models  from  the  theoretical  side  by  deriving  the  expected  error  of  the  noise  model.  Apart  from  evaluating  the  theoretical  results  on  commonly  used  synthetic  noise,  we  also  publish  NoisyNER,  a  new  noisy  label  dataset  from  the  NLP  domain  that  was  obtained  through  a  realistic  distant  supervision  technique.  It  provides  seven  sets  of  labels  with  differing  noise  patterns  to  evaluate  different  noise  levels  on  the  same  instances.  Parallel,  clean  labels  are  available  making  it  possible  to  study  scenarios  where  a  small  amount  of  gold-standard  data  can  be  leveraged.  Our  theoretical  results  and  the  corresponding  experiments  give  insights  into  the  factors  that  influence  the  noise  model  estimation  like  the  noise  distribution  and  the  sampling  technique.
0	Vsql  variational  shadow  quantum  learning  for  classification.  Classification  of  quantum  data  is  essential  for  quantum  machine  learning  and  near-term  quantum  technologies.  In  this  paper,  we  propose  a  new  hybrid  quantum-classical  framework  for  supervised  quantum  learning,  which  we  call  Variational  Shadow  Quantum  Learning  (VSQL).  Our  method  in  particular  utilizes  the  classical  shadows  of  quantum  data,  which  fundamentally  represent  the  side  information  of  quantum  data  with  respect  to  certain  physical  observables.  Specifically,  we  first  use  variational  shadow  quantum  circuits  to  extract  classical  features  in  a  convolution  way  and  then  utilize  a  fully-connected  neural  network  to  complete  the  classification  task.  We  show  that  this  method  could  sharply  reduce  the  number  of  parameters  and  thus  better  facilitate  quantum  circuit  training.  Simultaneously,  less  noise  will  be  introduced  since  fewer  quantum  gates  are  employed  in  such  shadow  circuits.  Moreover,  we  show  that  the  Barren  Plateau  issue,  a  significant  gradient  vanishing  problem  in  quantum  machine  learning,  could  be  avoided  in  VSQL.  Finally,  we  demonstrate  the  efficiency  of  VSQL  in  quantum  classification  via  numerical  experiments  on  the  classification  of  quantum  states  and  the  recognition  of  multi-labeled  handwritten  digits.  In  particular,  our  VSQL  approach  outperforms  existing  variational  quantum  classifiers  in  the  test  accuracy  in  the  binary  case  of  handwritten  digit  recognition  and  notably  requires  much  fewer  parameters.
0	Wind  power  resource  estimation  with  deep  neural  networks.  The  measure-correlate-predict  technique  is  state-of-the-art  for  assessing  the  quality  of  a  wind  power  resource  based  on  long  term  numerical  weather  prediction  systems.  On-site  wind  speed  measurements  are  correlated  to  meteorological  reanalysis  data,  which  represent  the  best  historical  estimate  available  for  the  atmospheric  state.  The  different  variants  of  MCP  more  or  less  correct  the  statistical  main  attributes  by  making  the  meteorological  reanalyses  bias  and  scaling  free  using  the  on-site  measurements.  However,  by  neglecting  the  higher  order  correlations  none  of  the  variants  utilize  the  full  potential  of  the  measurements.  We  show  that  deep  neural  networks  make  use  of  these  higher  order  correlations.  Our  implementation  is  tailored  to  the  requirements  of  MCP  in  the  context  of  wind  resource  assessment.  We  show  the  application  of  this  method  to  a  set  of  different  locations  and  compare  the  results  to  a  simple  linear  fit  to  the  wind  speed  frequency  distribution  as  well  as  to  a  standard  linear  regression  MCP,  that  represents  the  state-of-the-art  in  industrial  aerodynamics.  The  neural  network  based  MCP  outperforms  both  other  methods  with  respect  to  correlation,  root-mean-square  error  and  the  distance  in  the  wind  speed  frequency  distribution.  Site  assessment  can  be  considered  one  of  the  most  important  steps  developing  a  wind  energy  project.  To  this  end,  the  approach  described  can  be  regarded  as  a  novel,  high-quality  tool  for  reducing  uncertainties  in  the  long-term  reference  problem  of  on-site  measurements.
0	A  comparative  study  of  stock  scoring  using  regression  and  genetic  based  linear  models.  Stock  selection  has  long  been  a  challenging  and  important  task  in  investment  and  finance.  Researchers  and  practitioners  in  this  area  often  use  regression  models  to  tackle  this  problem  due  to  their  simplicity  and  effectiveness.  Recent  advances  in  machine  learning  (ML)  are  leading  to  significant  opportunities  to  solve  these  problems  more  effectively.  In  this  paper,  we  present  a  comparative  study  between  the  traditional  regression-based  and  ML-based  linear  models  for  stock  scoring,  which  is  crucial  to  the  success  of  stock  selection.  In  ML-based  models,  Genetic  Algorithms  (GA),  a  class  of  well-known  search  algorithms  in  the  area  of  ML,  is  used  for  optimization  of  model  parameters  and  selection  of  input  variables  to  the  stock  scoring  model.  We  will  show  that  our  proposed  genetic-based  method  significantly  outperforms  the  traditional  regression-based  method  as  well  as  the  benchmark.  As  a  result,  we  expect  this  genetic-based  methodology  to  advance  the  research  in  machine  learning  for  finance  and  provide  an  attractive  alternative  to  stock  selection  over  the  regression-based  approach.
0	Semi  supervised  model  for  emotion  recognition  in  speech.  To  recognize  emotional  traits  on  speech  is  a  challenging  task  which  became  very  popular  in  the  past  years,  especially  due  to  the  recent  advances  in  deep  neural  networks.  Although  very  successful,  these  models  inherited  a  common  problem  from  strongly  supervised  deep  neural  networks:  a  large  number  of  strongly  labeled  samples  demands  necessary,  so  the  model  learns  a  general  emotion  representation.  This  paper  proposes  a  solution  for  this  problem  with  the  development  of  a  semi-supervised  neural  network  which  can  learn  speech  representation  from  unlabeled  samples  and  used  them  in  different  emotion  recognition  in  speech  scenarios.  We  provide  experiments  with  different  datasets,  representing  natural  and  controlled  scenarios.  Our  results  show  that  our  model  is  competitive  with  state-of-the-art  solutions  in  all  these  scenarios  while  sharing  the  same  learned  representations,  which  were  learned  without  the  necessity  of  strong  labeled  data.
0	A  unified  method  of  defuzzification  for  type  2  fuzzy  numbers  with  its  application  to  multiobjective  decision  making.  This  paper  introduces  a  process  of  defuzzification  for  ranking  of  type-2  trapezoidal  fuzzy  numbers.  A  two-phase  defuzzification  method  has  been  developed  using  probability  density  function  of  the  random  variables  associated  with  the  fuzzy  numbers.  This  method  finds  an  equivalent  defuzzified  value  of  type-2  fuzzy  numbers  through  phasewise  reduction.  The  process  reduces  the  computational  complexities  for  using  type-2  fuzzy  numbers  significantly  and  it  is  applicable  to  not  only  type-2  fuzzy  numbers  but  also  to  any  types  of  fuzzy  numbers  for  ranking  them  properly.  To  illustrate  the  proposed  defuzzification  process,  the  method  is  applied  on  a  set  of  type-2  trapezoidal  fuzzy  numbers  and  ranked  them  according  to  their  defuzzified  values.  The  achieved  results  are  compared  with  other  existing  ranking  methods.  Furthermore,  a  multiobjective  linear  programming  model  having  type-2  fuzzy  numbers  as  parameters  is  solved  using  the  proposed  defuzzification  process.  Fuzzy  goal  programming  technique  is  used  for  achieving  the  highest  degree  of  each  of  the  defined  membership  goals  to  the  extent  possible  in  the  decision-making  context.  A  numerical  example  is  provided  to  demonstrate  the  efficiency  of  the  proposed  methodology.
0	Contingent  features  for  reinforcement  learning.  Applying  reinforcement  learning  algorithms  in  real-world  domains  is  challenging  because  relevant  state  information  is  often  embedded  in  a  stream  of  high-dimensional  sensor  data.  This  paper  describes  a  novel  algorithm  for  learning  task-relevant  features  through  interactions  with  the  environment.  The  key  idea  is  that  a  feature  is  likely  to  be  useful  to  the  degree  that  its  dynamics  can  be  controlled  by  the  actions  of  the  agent.  We  describe  an  algorithm  that  can  find  such  features  and  we  demonstrate  its  effectiveness  in  an  artificial  domain.
0	Strips  existential  quantification  in  planning  and  constraint  satisfaction.  Existentially  quantified  variables  in  goals  and  action  preconditions  are  part  of  the  standard  PDDL  planning  language,  yet  few  planners  support  them,  while  those  that  do  compile  them  away  at  an  exponential  cost.  In  this  work,  we  argue  that  existential  variables  are  an  essential  feature  for  representing  and  reasoning  with  constraints  in  planning,  and  that  it  is  harmful  to  compile  them  away  or  avoid  them  altogether,  since  this  hides  part  of  the  problem  structure  that  can  be  exploited  computationally.  We  show  how  to  do  this  by  formulating  an  extension  of  the  standard  delete-relaxation  heuristics  that  handles  existential  variables.  While  this  extension  is  simple,  the  consequences  for  both  modeling  and  computation  are  important.  Furthermore,  by  allowing  existential  variables  in  STRIPS  and  treating  them  properly,  CSPs  can  be  represented  and  solved  in  a  direct  manner  as  action-less,  fluent-less  STRIPS  planning  problems,  something  important  for  problems  involving  restrictions.  In  addition,  functional  fluents  in  Functional  STRIPS  can  be  compiled  away  with  no  effect  on  the  structure  and  informativeness  of  the  resulting  heuristic.  Experiments  are  reported  comparing  our  native  ∃-STRIPS  planner  with  state-of-the-art  STRIPS  planners  over  compiled  and  propositional  encodings,  and  with  a  Functional  STRIPS  planner.
0	How  to  keep  a  knowledge  base  synchronized  with  its  encyclopedia  source.  Knowledge  bases  are  playing  an  increasingly  important  role  in  many  real-world  applications.  However,  most  of  these  knowledge  bases  tend  to  be  outdated,  which  limits  the  utility  of  these  knowledge  bases.  In  this  paper,  we  investigate  how  to  keep  the  freshness  of  the  knowledge  base  by  synchronizing  it  with  its  data  source  (usually  encyclopedia  websites).  A  direct  solution  is  revisiting  the  whole  encyclopedia  periodically  and  rerun  the  entire  pipeline  of  the  construction  of  knowledge  base  like  most  existing  methods.  However,  this  solution  is  wasteful  and  incurs  massive  overload  of  the  network,  which  limits  the  update  frequency  and  leads  to  knowledge  obsolescence.  To  overcome  the  weakness,  we  propose  a  set  of  synchronization  principles  upon  which  we  build  an  Update  System  for  knowledge  Base  (USB)  with  an  update  frequency  predictor  of  entities  as  the  core  component.  We  also  design  a  set  of  effective  features  and  realize  the  predictor.  We  conduct  extensive  experiments  to  justify  the  effectiveness  of  the  proposed  system,  model,  as  well  as  the  underlying  principles.  Finally,  we  deploy  USB  on  a  Chinese  knowledge  base  to  improve  its  freshness.
0	Online  interactive  user  guidance  for  high  dimensional  constrained  motion  planning.  We  consider  the  problem  of  planning  a  collision-free  path  for  a  high-dimensional  robot.  Specifically,  we  suggest  a  planning  framework  where  a  motion-planning  algorithm  can  obtain  guidance  from  a  user.  In  contrast  to  existing  approaches  that  try  to  speed  up  planning  by  incorporating  experiences  or  demonstrations  ahead  of  planning,  we  suggest  to  seek  user  guidance  only  when  the  planner  identifies  that  it  ceases  to  make  significant  progress  towards  the  goal.  Guidance  is  provided  in  the  form  of  an  intermediate  configuration  $\hat{q}$,  which  is  used  to  bias  the  planner  to  go  through  $\hat{q}$.  We  demonstrate  our  approach  for  the  case  where  the  planning  algorithm  is  Multi-Heuristic  A*  (MHA*)  and  the  robot  is  a  34-DOF  humanoid.  We  show  that  our  approach  allows  to  compute  highly-constrained  paths  with  little  domain  knowledge.  Without  our  approach,  solving  such  problems  requires  carefully-crafting  domain-dependent  heuristics.
0	Medical  image  segmentation  using  squeeze  and  expansion  transformers.  Medical  image  segmentation  is  important  for  computer-aided  diagnosis.  Good  segmentation  demands  the  model  to  see  the  big  picture  and  fine  details  simultaneously,  i.e.,  to  learn  image  features  that  incorporate  large  context  while  keep  high  spatial  resolutions.  To  approach  this  goal,  the  most  widely  used  methods  --  U-Net  and  variants,  extract  and  fuse  multi-scale  features.  However,  the  fused  features  still  have  small  "effective  receptive  fields"  with  a  focus  on  local  image  cues,  limiting  their  performance.  In  this  work,  we  propose  Segtran,  an  alternative  segmentation  framework  based  on  transformers,  which  have  unlimited  "effective  receptive  fields"  even  at  high  feature  resolutions.  The  core  of  Segtran  is  a  novel  Squeeze-and-Expansion  transformer:  a  squeezed  attention  block  regularizes  the  self  attention  of  transformers,  and  an  expansion  block  learns  diversified  representations.  Additionally,  we  propose  a  new  positional  encoding  scheme  for  transformers,  imposing  a  continuity  inductive  bias  for  images.  Experiments  were  performed  on  2D  and  3D  medical  image  segmentation  tasks:  optic  disc/cup  segmentation  in  fundus  images  (REFUGE'20  challenge),  polyp  segmentation  in  colonoscopy  images,  and  brain  tumor  segmentation  in  MRI  scans  (BraTS'19  challenge).  Compared  with  representative  existing  methods,  Segtran  consistently  achieved  the  highest  segmentation  accuracy,  and  exhibited  good  cross-domain  generalization  capabilities.  The  source  code  of  Segtran  is  released  at  this  https  URL.
0	Sobolev  training  for  neural  networks.  At  the  heart  of  deep  learning  we  aim  to  use  neural  networks  as  function  approximators  -  training  them  to  produce  outputs  from  inputs  in  emulation  of  a  ground  truth  function  or  data  creation  process.  In  many  cases  we  only  have  access  to  input-output  pairs  from  the  ground  truth,  however  it  is  becoming  more  common  to  have  access  to  derivatives  of  the  target  output  with  respect  to  the  input  --  for  example  when  the  ground  truth  function  is  itself  a  neural  network  such  as  in  network  compression  or  distillation.  Generally  these  target  derivatives  are  not  computed,  or  are  ignored.  This  paper  introduces  Sobolev  Training  for  neural  networks,  which  is  a  method  for  incorporating  these  target  derivatives  in  addition  the  to  target  values  while  training.  By  optimising  neural  networks  to  not  only  approximate  the  function’s  outputs  but  also  the  function’s  derivatives  we  encode  additional  information  about  the  target  function  within  the  parameters  of  the  neural  network.  Thereby  we  can  improve  the  quality  of  our  predictors,  as  well  as  the  data-efficiency  and  generalization  capabilities  of  our  learned  function  approximation.  We  provide  theoretical  justifications  for  such  an  approach  as  well  as  examples  of  empirical  evidence  on  three  distinct  domains:  regression  on  classical  optimisation  datasets,  distilling  policies  of  an  agent  playing  Atari,  and  on  large-scale  applications  of  synthetic  gradients.  In  all  three  domains  the  use  of  Sobolev  Training,  employing  target  derivatives  in  addition  to  target  values,  results  in  models  with  higher  accuracy  and  stronger  generalisation.
0	Fast  algorithms  for  robust  pca  via  gradient  descent.  We  consider  the  problem  of  Robust  PCA  in  the  fully  and  partially  observed  settings.  Without  corruptions,  this  is  the  well-known  matrix  completion  problem.  From  a  statistical  standpoint  this  problem  has  been  recently  well-studied,  and  conditions  on  when  recovery  is  possible  (how  many  observations  do  we  need,  how  many  corruptions  can  we  tolerate)  via  polynomial-time  algorithms  is  by  now  understood.  This  paper  presents  and  analyzes  a  non-convex  optimization  approach  that  greatly  reduces  the  computational  complexity  of  the  above  problems,  compared  to  the  best  available  algorithms.  In  particular,  in  the  fully  observed  case,  with  $r$  denoting  rank  and  $d$  dimension,  we  reduce  the  complexity  from  $O(r^2d^2\log(1/\epsilon))$  to  $O(rd^2\log(1/\epsilon))$  --  a  big  savings  when  the  rank  is  big.  For  the  partially  observed  case,  we  show  the  complexity  of  our  algorithm  is  no  more  than  $O(r^4d\log(d)\log(1/\epsilon))$.  Not  only  is  this  the  best-known  run-time  for  a  provable  algorithm  under  partial  observation,  but  in  the  setting  where  $r$  is  small  compared  to  $d$,  it  also  allows  for  near-linear-in-$d$  run-time  that  can  be  exploited  in  the  fully-observed  case  as  well,  by  simply  running  our  algorithm  on  a  subset  of  the  observations.
0	Infogail  interpretable  imitation  learning  from  visual  demonstrations.  The  goal  of  imitation  learning  is  to  mimic  expert  behavior  without  access  to  an  explicit  reward  signal.  Expert  demonstrations  provided  by  humans,  however,  often  show  significant  variability  due  to  latent  factors  that  are  typically  not  explicitly  modeled.  In  this  paper,  we  propose  a  new  algorithm  that  can  infer  the  latent  structure  of  expert  demonstrations  in  an  unsupervised  way.  Our  method,  built  on  top  of  Generative  Adversarial  Imitation  Learning,  can  not  only  imitate  complex  behaviors,  but  also  learn  interpretable  and  meaningful  representations  of  complex  behavioral  data,  including  visual  demonstrations.  In  the  driving  domain,  we  show  that  a  model  learned  from  human  demonstrations  is  able  to  both  accurately  reproduce  a  variety  of  behaviors  and  accurately  anticipate  human  actions  using  raw  visual  inputs.  Compared  with  various  baselines,  our  method  can  better  capture  the  latent  structure  underlying  expert  demonstrations,  often  recovering  semantically  meaningful  factors  of  variation  in  the  data.
0	Neural  variational  inference  and  learning  in  undirected  graphical  models.  Many  problems  in  machine  learning  are  naturally  expressed  in  the  language  of  undirected  graphical  models.  Here,  we  propose  black-box  learning  and  inference  algorithms  for  undirected  models  that  optimize  a  variational  approximation  to  the  log-likelihood  of  the  model.  Central  to  our  approach  is  an  upper  bound  on  the  log-partition  function  parametrized  by  a  function  q  that  we  express  as  a  flexible  neural  network.  Our  bound  makes  it  possible  to  track  the  partition  function  during  learning,  to  speed-up  sampling,  and  to  train  a  broad  class  of  hybrid  directed/undirected  models  via  a  unified  variational  inference  framework.  We  empirically  demonstrate  the  effectiveness  of  our  method  on  several  popular  generative  modeling  datasets.
0	Optimizing  generalized  pagerank  methods  for  seed  expansion  community  detection.  Landing  probabilities  (LP)  of  random  walks  (RW)  over  graphs  encode  rich  information  regarding  graph  topology.  Generalized  PageRanks  (GPR),  which  represent  weighted  sums  of  LPs  of  RWs,  utilize  the  discriminative  power  of  LP  features  to  enable  many  graph-based  learning  studies.  Previous  work  in  the  area  has  mostly  focused  on  evaluating  suitable  weights  for  GPRs,  and  only  a  few  studies  so  far  have  attempted  to  derive  the  optimal  weights  of  GPRs  for  a  given  application.  We  take  a  fundamental  step  forward  in  this  direction  by  using  random  graph  models  to  better  our  understanding  of  the  behavior  of  GPRs.  In  this  context,  we  provide  a  rigorous  non-asymptotic  analysis  for  the  convergence  of  LPs  and  GPRs  to  their  mean-field  values  on  edge-independent  random  graphs.  Although  our  theoretical  results  apply  to  many  problem  settings,  we  focus  on  the  task  of  seed-expansion  community  detection  over  stochastic  block  models.  There,  we  find  that  the  predictive  power  of  LPs  decreases  significantly  slower  than  previously  reported  based  on  asymptotic  findings.  Given  this  result,  we  propose  a  new  GPR,  termed  Inverse  PR  (IPR),  with  LP  weights  that  increase  for  the  initial  few  steps  of  the  walks.  Extensive  experiments  on  both  synthetic  and  real,  large-scale  networks  illustrate  the  superiority  of  IPR  compared  to  other  GPRs  for  seeded  community  detection.
0	Opponent  models  with  uncertainty  for  strategic  argumentation.  This  paper  deals  with  the  issue  of  strategic  argumentation  in  the  setting  of  Dung-style  abstract  argumentation  theory.  Such  reasoning  takes  place  through  the  use  of  opponent  models--recursive  representations  of  an  agent's  knowledge  and  beliefs  regarding  the  opponent's  knowledge.  Using  such  models,  we  present  three  approaches  to  reasoning.  The  first  directly  utilises  the  opponent  model  to  identify  the  best  move  to  advance  in  a  dialogue.  The  second  extends  our  basic  approach  through  the  use  of  quantitative  uncertainty  over  the  opponent's  model.  The  final  extension  introduces  virtual  arguments  into  the  opponent's  reasoning  process.  Such  arguments  are  unknown  to  the  agent,  but  presumed  to  exist  and  interact  with  known  arguments.  They  are  therefore  used  to  add  a  primitive  notion  of  risk  to  the  agent's  reasoning.  We  have  implemented  our  models  and  we  have  performed  an  empirical  analysis  that  shows  that  this  added  expressivity  improves  the  performance  of  an  agent  in  a  dialogue.
0	Learning  finite  beta  liouville  mixture  models  via  variational  bayes  for  proportional  data  clustering.  During  the  past  decade,  finite  mixture  modeling  has  become  a  well-established  technique  in  data  analysis  and  clustering.  This  paper  focus  on  developing  a  variational  inference  framework  to  learn  finite  Beta-Liouville  mixture  models  that  have  been  proposed  recently  as  an  efficient  way  for  proportional  data  clustering.  In  contrast  to  the  conventional  expectation  maximization  (EM)  algorithm,  commonly  used  for  learning  finite  mixture  models,  the  proposed  algorithm  has  the  advantages  that  it  is  more  efficient  from  a  computational  point  of  view  and  by  preventing  over-and  under-fitting  problems.  Moreover,  the  complexity  of  the  mixture  model  (i.e.  the  number  of  components)  can  be  determined  automatically  and  simultaneously  with  the  parameters  estimation  in  a  closed  form  as  part  of  the  Bayesian  inference  procedure.  The  merits  of  the  proposed  approach  are  shown  using  both  artificial  data  sets  and  two  interesting  and  challenging  real  applications  namely  dynamic  textures  clustering  and  facial  expression  recognition.
0	Radar  residual  analysis  for  anomaly  detection  in  attributed  networks.  Attributed  networks  are  pervasive  in  different  domains,  ranging  from  social  networks,  gene  regulatory  networks  to  financial  transaction  networks.  This  kind  of  rich  network  representation  presents  challenges  for  anomaly  detection  due  to  the  heterogeneity  of  two  data  representations.  A  vast  majority  of  existing  algorithms  assume  certain  properties  of  anomalies  are  given  a  prior.  Since  various  types  of  anomalies  in  real-world  attributed  networks  coexist,  the  assumption  that  priori  knowledge  regarding  anomalies  is  available  does  not  hold.  In  this  paper,  we  investigate  the  problem  of  anomaly  detection  in  attributed  networks  generally  from  a  residual  analysis  perspective,  which  has  been  shown  to  be  effective  in  traditional  anomaly  detection  problems.  However,  it  is  a  non-trivial  task  in  attributed  networks  as  interactions  among  instances  complicate  the  residual  modeling  process.  Methodologically,  we  propose  a  learning  framework  to  characterize  the  residuals  of  attribute  information  and  its  coherence  with  network  information  for  anomaly  detection.  By  learning  and  analyzing  the  residuals,  we  detect  anomalies  whose  behaviors  are  singularly  different  from  the  majority.  Experiments  on  real  datasets  show  the  effectiveness  and  generality  of  the  proposed  framework.
0	An  approach  to  minimal  belief  via  objective  belief.  As  a  doxastic  counterpart  to  epistemic  logic  based  on  S5  we  study  the  modal  logic  KSD  that  can  be  viewed  as  an  approach  to  modelling  a  kind  of  objective  and  fair  belief.  We  apply  KSD  to  the  problem  of  minimal  belief  and  develop  an  alternative  approach  to  nonmonotonic  modal  logic  using  a  weaker  concept  of  expansion.  This  corresponds  to  a  certain  minimal  kind  of  KSD  model  and  yields  a  new  type  of  nonmonotonic  doxastic  reasoning.
0	On  computing  minimal  correction  subsets.  A  set  of  constraints  that  cannot  be  simultaneously  satisfied  is  over-constrained.  Minimal  relaxations  and  minimal  explanations  for  over-constrained  problems  find  many  practical  uses.  For  Boolean  formulas,  minimal  relaxations  of  over-constrained  problems  are  referred  to  as  Minimal  Correction  Subsets  (MCSes).  MCSes  find  many  applications,  including  the  enumeration  of  MUSes.  Existing  approaches  for  computing  MCSes  either  use  a  Maximum  Satisfiability  (MaxSAT)  solver  or  iterative  calls  to  a  Boolean  Satisfiability  (SAT)  solver.  This  paper  shows  that  existing  algorithms  for  MCS  computation  can  be  inefficient,  and  so  inadequate,  in  certain  practical  settings.  To  address  this  problem,  this  paper  develops  a  number  of  novel  techniques  for  improving  the  performance  of  existing  MCS  computation  algorithms.  More  importantly,  the  paper  proposes  a  novel  algorithm  for  computing  MCSes.  Both  the  techniques  and  the  algorithm  are  evaluated  empirically  on  representative  problem  instances,  and  are  shown  to  yield  the  most  efficient  and  robust  solutions  for  MCS  computation.
0	Neurons  merging  layer  towards  progressive  redundancy  reduction  for  deep  supervised  hashing.  Deep  supervised  hashing  has  become  an  active  topic  in  information  retrieval.  It  generates  hashing  bits  by  the  output  neurons  of  a  deep  hashing  network.  During  binary  discretization,  there  often  exists  much  redundancy  between  hashing  bits  that  degenerates  retrieval  performance  in  terms  of  both  storage  and  accuracy.  This  paper  proposes  a  simple  yet  effective  Neurons  Merging  Layer  (NMLayer)  for  deep  supervised  hashing.  A  graph  is  constructed  to  represent  the  redundancy  relationship  between  hashing  bits  that  is  used  to  guide  the  learning  of  a  hashing  network.  Specifically,  it  is  dynamically  learned  by  a  novel  mechanism  defined  in  our  active  and  frozen  phases.  According  to  the  learned  relationship,  the  NMLayer  merges  the  redundant  neurons  together  to  balance  the  importance  of  each  output  neuron.  Moreover,  multiple  NMLayers  are  progressively  trained  for  a  deep  hashing  network  to  learn  a  more  compact  hashing  code  from  a  long  redundant  code.  Extensive  experiments  on  four  datasets  demonstrate  that  our  proposed  method  outperforms  state-of-the-art  hashing  methods.
0	A  document  grounded  matching  network  for  response  selection  in  retrieval  based  chatbots.  We  present  a  document-grounded  matching  network  (DGMN)  for  response  selection  that  can  power  a  knowledge-aware  retrieval-based  chatbot  system.  The  challenges  of  building  such  a  model  lie  in  how  to  ground  conversation  contexts  with  background  documents  and  how  to  recognize  important  information  in  the  documents  for  matching.  To  overcome  the  challenges,  DGMN  fuses  information  in  a  document  and  a  context  into  representations  of  each  other,  and  dynamically  determines  if  grounding  is  necessary  and  importance  of  different  parts  of  the  document  and  the  context  through  hierarchical  interaction  with  a  response  at  the  matching  step.  Empirical  studies  on  two  public  data  sets  indicate  that  DGMN  can  significantly  improve  upon  state-of-the-art  methods  and  at  the  same  time  enjoys  good  interpretability.
0	Extracting  hot  spots  of  topics  from  time  stamped  documents.  Identifying  time  periods  with  a  burst  of  activities  related  to  a  topic  has  been  an  important  problem  in  analyzing  time-stamped  documents.  In  this  paper,  we  propose  an  approach  to  extract  a  hot  spot  of  a  given  topic  in  a  time-stamped  document  set.  Topics  can  be  basic,  containing  a  simple  list  of  keywords,  or  complex.  Logical  relationships  such  as  and,  or,  and  not  are  used  to  build  complex  topics  from  basic  topics.  A  concept  of  presence  measure  of  a  topic  based  on  fuzzy  set  theory  is  introduced  to  compute  the  amount  of  information  related  to  the  topic  in  the  document  set.  Each  interval  in  the  time  period  of  the  document  set  is  associated  with  a  numeric  value  which  we  call  the  discrepancy  score.  A  high  discrepancy  score  indicates  that  the  documents  in  the  time  interval  are  more  focused  on  the  topic  than  those  outside  of  the  time  interval.  A  hot  spot  of  a  given  topic  is  defined  as  a  time  interval  with  the  highest  discrepancy  score.  We  first  describe  a  naive  implementation  for  extracting  hot  spots.  We  then  construct  an  algorithm  called  EHE  (Efficient  Hot  Spot  Extraction)  using  several  efficient  strategies  to  improve  performance.  We  also  introduce  the  notion  of  a  topic  DAG  to  facilitate  an  efficient  computation  of  presence  measures  of  complex  topics.  The  proposed  approach  is  illustrated  by  several  experiments  on  a  subset  of  the  TDT-Pilot  Corpus  and  DBLP  conference  data  set.  The  experiments  show  that  the  proposed  EHE  algorithm  significantly  outperforms  the  naive  one,  and  the  extracted  hot  spots  of  given  topics  are  meaningful.
0	Interpretable  anomaly  prediction  predicting  anomalous  behavior  in  industry  4  0  settings  via  regularized  logistic  regression  tools.  Abstract  Prediction  of  anomalous  behavior  in  industrial  assets  based  on  sensor  reading  represents  a  key  focus  in  modern  business  practice.  As  a  matter  of  fact,  forecast  of  forthcoming  faults  is  crucial  to  implement  predictive  maintenance,  i.e.  maintenance  decision  making  based  on  real  time  information  from  components  and  systems,  which  allows,  among  other  benefits,  to  reduce  maintenance  cost,  minimize  downtime,  increase  safety,  enhance  product  quality  and  productivity.  However,  building  a  model  able  to  predict  the  future  occurrence  of  a  failure  is  challenging  for  various  reasons.  First,  data  are  usually  highly  imbalanced,  meaning  that  patterns  describing  a  faulty  regime  are  much  less  numerous  than  normal  behavior  instances,  which  makes  model  design  difficult.  Second,  model  predictions  should  be  not  only  accurate  (to  avoid  false  alarms  and  missed  detections)  but  also  explainable  to  operators  responsible  for  scheduling  maintenance  or  control  actions.  In  this  paper  we  introduce  a  method  called  Interpretable  Anomaly  Prediction  (IAP)  allowing  to  handle  these  issues  by  using  regularized  logistic  regression  as  core  prediction  model.  In  particular,  in  contrast  to  anomaly  detection  algorithms  which  permit  to  identify  if  the  current  data  are  anomalous  or  not,  the  proposed  technique  is  able  to  predict  the  probability  that  future  data  will  be  abnormal.  Furthermore,  feature  extraction  and  selection  mechanisms  give  insights  on  the  possible  root  causes  leading  to  failures.  The  proposed  strategy  is  validated  with  a  large  imbalanced  multivariate  time-series  dataset  consisting  of  measurements  of  several  process  variables  surrounding  an  high  pressure  plunger  pump  situated  in  a  complex  chemical  plant.
0	Variational  dropout  and  the  local  reparameterization  trick.  We  investigate  a  local  reparameterizaton  technique  for  greatly  reducing  the  variance  of  stochastic  gradients  for  variational  Bayesian  inference  (SGVB)  of  a  posterior  over  model  parameters,  while  retaining  parallelizability.  This  local  reparameterization  translates  uncertainty  about  global  parameters  into  local  noise  that  is  independent  across  datapoints  in  the  minibatch.  Such  parameterizations  can  be  trivially  parallelized  and  have  variance  that  is  inversely  proportional  to  the  mini-batch  size,  generally  leading  to  much  faster  convergence.  Additionally,  we  explore  a  connection  with  dropout:  Gaussian  dropout  objectives  correspond  to  SGVB  with  local  reparameterization,  a  scale-invariant  prior  and  proportionally  fixed  posterior  variance.  Our  method  allows  inference  of  more  flexibly  parameterized  posteriors;  specifically,  we  propose  variational  dropout,  a  generalization  of  Gaussian  dropout  where  the  dropout  rates  are  learned,  often  leading  to  better  models.  The  method  is  demonstrated  through  several  experiments.
0	Real  time  reinforcement  learning.  Markov  Decision  Processes  (MDPs),  the  mathematical  framework  underlying  most  algorithms  in  Reinforcement  Learning  (RL),  are  often  used  in  a  way  that  wrongfully  assumes  that  the  state  of  an  agent's  environment  does  not  change  during  action  selection.  As  RL  systems  based  on  MDPs  begin  to  find  application  in  real-world  safety  critical  situations,  this  mismatch  between  the  assumptions  underlying  classical  MDPs  and  the  reality  of  real-time  computation  may  lead  to  undesirable  outcomes.  In  this  paper,  we  introduce  a  new  framework,  in  which  states  and  actions  evolve  simultaneously  and  show  how  it  is  related  to  the  classical  MDP  formulation.  We  analyze  existing  algorithms  under  the  new  real-time  formulation  and  show  why  they  are  suboptimal  when  used  in  real-time.  We  then  use  those  insights  to  create  a  new  algorithm  Real-Time  Actor  Critic  (RTAC)  that  outperforms  the  existing  state-of-the-art  continuous  control  algorithm  Soft  Actor  Critic  both  in  real-time  and  non-real-time  settings.
0	Predictive  state  recurrent  neural  networks.  We  present  a  new  model,  Predictive  State  Recurrent  Neural  Networks  (PSRNNs),  for  filtering  and  prediction  in  dynamical  systems.  PSRNNs  draw  on  insights  from  both  Recurrent  Neural  Networks  (RNNs)  and  Predictive  State  Representations  (PSRs),  and  inherit  advantages  from  both  types  of  models.  Like  many  successful  RNN  architectures,  PSRNNs  use  (potentially  deeply  composed)  bilinear  transfer  functions  to  combine  information  from  multiple  sources.  We  show  that  such  bilinear  functions  arise  naturally  from  state  updates  in  Bayes  filters  like  PSRs,  in  which  observations  can  be  viewed  as  gating  belief  states.  We  also  show  that  PSRNNs  can  be  learned  effectively  by  combining  Backpropogation  Through  Time  (BPTT)  with  an  initialization  derived  from  a  statistically  consistent  learning  algorithm  for  PSRs  called  two-stage  regression  (2SR).  Finally,  we  show  that  PSRNNs  can  be  factorized  using  tensor  decomposition,  reducing  model  size  and  suggesting  interesting  connections  to  existing  multiplicative  architectures  such  as  LSTMs  and  GRUs.  We  apply  PSRNNs  to  4  datasets,  and  show  that  we  outperform  several  popular  alternative  approaches  to  modeling  dynamical  systems  in  all  cases.
0	Homeostatic  plasticity  in  bayesian  spiking  networks  as  expectation  maximization  with  posterior  constraints.  Recent  spiking  network  models  of  Bayesian  inference  and  unsupervised  learning  frequently  assume  either  inputs  to  arrive  in  a  special  format  or  employ  complex  computations  in  neuronal  activation  functions  and  synaptic  plasticity  rules.  Here  we  show  in  a  rigorous  mathematical  treatment  how  homeostatic  processes,  which  have  previously  received  little  attention  in  this  context,  can  overcome  common  theoretical  limitations  and  facilitate  the  neural  implementation  and  performance  of  existing  models.  In  particular,  we  show  that  homeostatic  plasticity  can  be  understood  as  the  enforcement  of  a  'balancing'  posterior  constraint  during  probabilistic  inference  and  learning  with  Expectation  Maximization.  We  link  homeostatic  dynamics  to  the  theory  of  variational  inference,  and  show  that  nontrivial  terms,  which  typically  appear  during  probabilistic  inference  in  a  large  class  of  models,  drop  out.  We  demonstrate  the  feasibility  of  our  approach  in  a  spiking  Winner-Take-All  architecture  of  Bayesian  inference  and  learning.  Finally,  we  sketch  how  the  mathematical  framework  can  be  extended  to  richer  recurrent  network  architectures.  Altogether,  our  theory  provides  a  novel  perspective  on  the  interplay  of  homeostatic  processes  and  synaptic  plasticity  in  cortical  microcircuits,  and  points  to  an  essential  role  of  homeostasis  during  inference  and  learning  in  spiking  networks.
0	Bellman  error  based  feature  generation  using  random  projections  on  sparse  spaces.  This  paper  addresses  the  problem  of  automatic  generation  of  features  for  value  function  approximation  in  reinforcement  learning.  Bellman  Error  Basis  Functions  (BEBFs)  have  been  shown  to  improve  policy  evaluation,  with  a  convergence  rate  similar  to  that  of  value  iteration.  We  propose  a  simple,  fast  and  robust  algorithm  based  on  random  projections,  which  generates  BEBFs  for  sparse  feature  spaces.  We  provide  a  finite  sample  analysis  of  the  proposed  method,  and  prove  that  projections  logarithmic  in  the  dimension  of  the  original  space  guarantee  a  contraction  in  the  error.  Empirical  results  demonstrate  the  strength  of  this  method  in  domains  in  which  choosing  a  good  state  representation  is  challenging.
0	Breaking  the  glass  ceiling  for  embedding  based  classifiers  for  large  output  spaces.  In  extreme  classification  settings,  embedding-based  neural  network  models  are  currently  not  competitive  with  sparse  linear  and  tree-based  methods  in  terms  of  accuracy.  Most  prior  works  attribute  this  poor  performance  to  the  low-dimensional  bottleneck  in  embedding-based  methods.  In  this  paper,  we  demonstrate  that  theoretically  there  is  no  limitation  to  using  low-dimensional  embedding-based  methods,  and  provide  experimental  evidence  that  overfitting  is  the  root  cause  of  the  poor  performance  of  embedding-based  methods.  These  findings  motivate  us  to  investigate  novel  data  augmentation  and  regularization  techniques  to  mitigate  overfitting.  To  this  end,  we  propose  GLaS,  a  new  regularizer  for  embedding-based  neural  network  approaches.  It  is  a  natural  generalization  from  the  graph  Laplacian  and  spread-out  regularizers,  and  empirically  it  addresses  the  drawback  of  each  regularizer  alone  when  applied  to  the  extreme  classification  setup.  With  the  proposed  techniques,  we  attain  or  improve  upon  the  state-of-the-art  on  most  widely  tested  public  extreme  classification  datasets  with  hundreds  of  thousands  of  labels.
0	Prnet  self  supervised  learning  for  partial  to  partial  registration.  We  present  a  simple,  flexible,  and  general  framework  titled  Partial  Registration  Network  (PRNet),  for  partial-to-partial  point  cloud  registration.  Inspired  by  recently-proposed  learning-based  methods  for  registration,  we  use  deep  networks  to  tackle  non-convexity  of  the  alignment  and  partial  correspondence  problem.  While  previous  learning-based  methods  assume  the  entire  shape  is  visible,  PRNet  is  suitable  for  partial-to-partial  registration,  outperforming  PointNetLK,  DCP,  and  non-learning  methods  on  synthetic  data.  PRNet  is  self-supervised,  jointly  learning  an  appropriate  geometric  representation,  a  keypoint  detector  that  finds  points  in  common  between  partial  views,  and  keypoint-to-keypoint  correspondences.  We  show  PRNet  predicts  keypoints  and  correspondences  consistently  across  views  and  objects.  Furthermore,  the  learned  representation  is  transferable  to  classification.
0	Learning  without  the  phase  regularized  phasemax  achieves  optimal  sample  complexity.  The  problem  of  estimating  an  unknown  signal,  $\mathbf  x_0\in  \mathbb  R^n$,  from  a  vector  $\mathbf  y\in  \mathbb  R^m$  consisting  of  $m$  magnitude-only  measurements  of  the  form  $y_i=|\mathbf  a_i\mathbf  x_0|$,  where  $\mathbf  a_i$'s  are  the  rows  of  a  known  measurement  matrix  $\mathbf  A$  is  a  classical  problem  known  as  phase  retrieval.  This  problem  arises  when  measuring  the  phase  is  costly  or  altogether  infeasible.  In  many  applications  in  machine  learning,  signal  processing,  statistics,  etc.,  the  underlying  signal  has  certain  structure  (sparse,  low-rank,  finite  alphabet,  etc.),  opening  of  up  the  possibility  of  recovering  $\mathbf  x_0$  from  a  number  of  measurements  smaller  than  the  ambient  dimension,  i.e.,  $m<n$.  Ideally,  one  would  like  to  recover  the  signal  from  a  number  of  phaseless  measurements  that  is  on  the  order  of  the  "degrees  of  freedom"  of  the  structured  $\mathbf  x_0$.  To  this  end,  inspired  by  the  PhaseMax  algorithm,  we  formulate  a  convex  optimization  problem,  where  the  objective  function  relies  on  an  initial  estimate  of  the  true  signal  and  also  includes  an  additive  regularization  term  to  encourage  structure.  The  new  formulation  is  referred  to  as  {\textbf{regularized  PhaseMax}}.  We  analyze  the  performance  of  regularized  PhaseMax  to  find  the  minimum  number  of  phaseless  measurements  required  for  perfect  signal  recovery.  The  results  are  asymptotic  and  are  in  terms  of  the  geometrical  properties  (such  as  the  Gaussian  width)  of  certain  convex  cones.  When  the  measurement  matrix  has  i.i.d.  Gaussian  entries,  we  show  that  our  proposed  method  is  indeed  order-wise  optimal,  allowing  perfect  recovery  from  a  number  of  phaseless  measurements  that  is  only  a  constant  factor  away  from  the  degrees  of  freedom.  We  explicitly  compute  this  constant  factor,  in  terms  of  the  quality  of  the  initial  estimate,  by  deriving  the  exact  phase  transition.  The  theory  well  matches  empirical  results  from  numerical  simulations.
0	Learning  others  intentional  models  in  multi  agent  settings  using  interactive  pomdps.  Interactive  partially  observable  Markov  decision  processes  (I-POMDPs)  provide  a  principled  framework  for  planning  and  acting  in  a  partially  observable,  stochastic  and  multi-agent  environment.  It  extends  POMDPs  to  multi-agent  settings  by  including  models  of  other  agents  in  the  state  space  and  forming  a  hierarchical  belief  structure.  In  order  to  predict  other  agents'  actions  using  I-POMDPs,  we  propose  an  approach  that  effectively  uses  Bayesian  inference  and  sequential  Monte  Carlo  sampling  to  learn  others'  intentional  models  which  ascribe  to  them  beliefs,  preferences  and  rationality  in  action  selection.  Empirical  results  show  that  our  algorithm  accurately  learns  models  of  the  other  agent  and  has  superior  performance  than  methods  that  use  subintentional  models.  Our  approach  serves  as  a  generalized  Bayesian  learning  algorithm  that  learns  other  agents'  beliefs,  strategy  levels,  and  transition,  observation  and  reward  functions.  It  also  effectively  mitigates  the  belief  space  complexity  due  to  the  nested  belief  hierarchy.
0	Regularized  linear  autoencoders  recover  the  principal  components  eventually.  Our  understanding  of  learning  input-output  relationships  with  neural  nets  has  improved  rapidly  in  recent  years,  but  little  is  known  about  the  convergence  of  the  underlying  representations,  even  in  the  simple  case  of  linear  autoencoders  (LAEs).  We  show  that  when  trained  with  proper  regularization,  LAEs  can  directly  learn  the  optimal  representation  --  ordered,  axis-aligned  principal  components.  We  analyze  two  such  regularization  schemes:  non-uniform  $\ell_2$  regularization  and  a  deterministic  variant  of  nested  dropout  [Rippel  et  al,  ICML'  2014].  Though  both  regularization  schemes  converge  to  the  optimal  representation,  we  show  that  this  convergence  is  slow  due  to  ill-conditioning  that  worsens  with  increasing  latent  dimension.  We  show  that  the  inefficiency  of  learning  the  optimal  representation  is  not  inevitable  --  we  present  a  simple  modification  to  the  gradient  descent  update  that  greatly  speeds  up  convergence  empirically.
0	Self  adaptive  training  beyond  empirical  risk  minimization.  We  propose  self-adaptive  training---a  new  training  algorithm  that  dynamically  corrects  problematic  training  labels  by  model  predictions  without  incurring  extra  computational  cost---to  improve  generalization  of  deep  learning  for  potentially  corrupted  training  data.  This  problem  is  crucial  towards  robustly  learning  from  data  that  are  corrupted  by,  e.g.,  label  noises  and  out-of-distribution  samples.  The  standard  empirical  risk  minimization  (ERM)  for  such  data,  however,  may  easily  overfit  noises  and  thus  suffers  from  sub-optimal  performance.  In  this  paper,  we  observe  that  model  predictions  can  substantially  benefit  the  training  process:  self-adaptive  training  significantly  improves  generalization  over  ERM  under  various  levels  of  noises,  and  mitigates  the  overfitting  issue  in  both  natural  and  adversarial  training.  We  evaluate  the  error-capacity  curve  of  self-adaptive  training:  the  test  error  is  monotonously  decreasing  w.r.t.  model  capacity.  This  is  in  sharp  contrast  to  the  recently-discovered  double-descent  phenomenon  in  ERM  which  might  be  a  result  of  overfitting  of  noises.  Experiments  on  CIFAR  and  ImageNet  datasets  verify  the  effectiveness  of  our  approach  in  two  applications:  classification  with  label  noise  and  selective  classification.  We  release  our  code  at  this  https  URL.
0	Unsupervised  sound  separation  using  mixture  invariant  training.  In  recent  years,  rapid  progress  has  been  made  on  the  problem  of  single-channel  sound  separation  using  supervised  training  of  deep  neural  networks.  In  such  supervised  approaches,  a  model  is  trained  to  predict  the  component  sources  from  synthetic  mixtures  created  by  adding  up  isolated  ground-truth  sources.  Reliance  on  this  synthetic  training  data  is  problematic  because  good  performance  depends  upon  the  degree  of  match  between  the  training  data  and  real-world  audio,  especially  in  terms  of  the  acoustic  conditions  and  distribution  of  sources.  The  acoustic  properties  can  be  challenging  to  accurately  simulate,  and  the  distribution  of  sound  types  may  be  hard  to  replicate.  In  this  paper,  we  propose  a  completely  unsupervised  method,  mixture  invariant  training  (MixIT),  that  requires  only  single-channel  acoustic  mixtures.  In  MixIT,  training  examples  are  constructed  by  mixing  together  existing  mixtures,  and  the  model  separates  them  into  a  variable  number  of  latent  sources,  such  that  the  separated  sources  can  be  remixed  to  approximate  the  original  mixtures.  We  show  that  MixIT  can  achieve  competitive  performance  compared  to  supervised  methods  on  speech  separation.  Using  MixIT  in  a  semi-supervised  learning  setting  enables  unsupervised  domain  adaptation  and  learning  from  large  amounts  of  real  world  data  without  ground-truth  source  waveforms.  In  particular,  we  significantly  improve  reverberant  speech  separation  performance  by  incorporating  reverberant  mixtures,  train  a  speech  enhancement  system  from  noisy  mixtures,  and  improve  universal  sound  separation  by  incorporating  a  large  amount  of  in-the-wild  data.
0	Transfer  hashing  with  privileged  information.  Most  existing  learning  to  hash  methods  assume  that  there  are  sufficient  data,  either  labeled  or  unlabeled,  on  the  domain  of  interest  (i.e.,  the  target  domain)  for  training.  However,  this  assumption  cannot  be  satisfied  in  some  real-world  applications.  To  address  this  data  sparsity  issue  in  hashing,  inspired  by  transfer  learning,  we  propose  a  new  framework  named  Transfer  Hashing  with  Privileged  Information  (THPI).  Specifically,  we  extend  the  standard  learning  to  hash  method,  Iterative  Quantization  (ITQ),  in  a  transfer  learning  manner,  namely  ITQ+.  In  ITQ+,  a  new  slack  function  is  learned  from  auxiliary  data  to  approximate  the  quantization  error  in  ITQ.  We  developed  an  alternating  optimization  approach  to  solve  the  resultant  optimization  problem  for  ITQ+.  We  further  extend  ITQ+  to  LapITQ+  by  utilizing  the  geometry  structure  among  the  auxiliary  data  for  learning  more  precise  binary  codes  in  the  target  domain.  Extensive  experiments  on  several  benchmark  datasets  verify  the  effectiveness  of  our  proposed  approaches  through  comparisons  with  several  state-of-the-art  baselines.
0	Symmetries  and  lazy  clause  generation.  Lazy  clause  generation  is  a  powerful  approach  to  reducing  search  in  constraint  programming.  This  is  achieved  by  recording  sets  of  domain  restrictions  that  previously  led  to  failure  as  new  clausal  propagators.  Symmetry  breaking  approaches  are  also  powerful  methods  for  reducing  search  by  recognizing  that  parts  of  the  search  tree  are  symmetric  and  do  not  need  to  be  explored.  In  this  paper  we  show  how  we  can  successfully  combine  symmetry  breaking  methods  with  lazy  clause  generation.  Further,  we  show  that  the  more  precise  nogoods  generated  by  a  lazy  clause  solver  allow  our  combined  approach  to  exploit  redundancies  that  cannot  be  exploited  via  any  previous  symmetry  breaking  method,  be  it  static  or  dynamic.
0	Fostering  cooperation  in  structured  populations  through  local  and  global  interference  strategies.  We  study  the  situation  of  an  exogenous  decision-maker  aiming  to  encourage  a  population  of  autonomous,  self-regarding  agents  to  follow  a  desired  behaviour  at  a  minimal  cost.  The  primary  goal  is  therefore  to  reach  an  efficient  trade-off  between  pushing  the  agents  to  achieve  the  desired  configuration  while  minimising  the  total  investment.  To  this  end,  we  test  several  interference  paradigms  resorting  to  simulations  of  agents  facing  a  cooperative  dilemma  in  a  spatial  arrangement.  We  systematically  analyse  and  compare  interference  strategies  rewarding  local  or  global  behavioural  patterns.  Our  results  show  that  taking  into  account  the  neighbourhood's  local  properties,  such  as  its  level  of  cooperativeness,  can  lead  to  a  significant  improvement  regarding  cost  efficiency  while  guaranteeing  high  levels  of  cooperation.  As  such,  we  argue  that  local  interference  strategies  are  more  efficient  than  global  ones  in  fostering  cooperation  in  a  population  of  autonomous  agents.
0	Extracting  action  sequences  from  texts  based  on  deep  reinforcement  learning.  Extracting  action  sequences  from  natural  language  texts  is  challenging,  as  it  requires  commonsense  inferences  based  on  world  knowledge.  Although  there  has  been  work  on  extracting  action  scripts,  instructions,  navigation  actions,  etc.,  they  require  that  either  the  set  of  candidate  actions  be  provided  in  advance,  or  that  action  descriptions  are  restricted  to  a  specific  form,  e.g.,  description  templates.  In  this  paper,  we  aim  to  extract  action  sequences  from  texts  in  free  natural  language,  i.e.,  without  any  restricted  templates,  provided  the  candidate  set  of  actions  is  unknown.  We  propose  to  extract  action  sequences  from  texts  based  on  the  deep  reinforcement  learning  framework.  Specifically,  we  view  "selecting"  or  "eliminating"  words  from  texts  as  "actions",  and  the  texts  associated  with  actions  as  "states".  We  then  build  Q-networks  to  learn  the  policy  of  extracting  actions  and  extract  plans  from  the  labeled  texts.  We  demonstrate  the  effectiveness  of  our  approach  on  several  datasets  with  comparison  to  state-of-the-art  approaches,  including  online  experiments  interacting  with  humans.
0	Collaborative  filtering  as  a  multi  armed  bandit.  Recommender  Systems  (RS)  aim  at  suggesting  to  users  one  or  several  items  in  which  they  might  have  interest.  Following  the  feedback  they  receive  from  the  user,  these  systems  have  to  adapt  their  model  in  order  to  improve  future  recommendations.  The  repetition  of  these  steps  defines  the  RS  as  a  sequential  process.  This  sequential  aspect  raises  an  exploration-exploitation  dilemma,  which  is  surprisingly  rarely  taken  into  account  for  RS  without  contextual  information.  In  this  paper  we  present  an  explore-exploit  collaborative  filtering  RS,  based  on  Matrix  Factor-ization  and  Bandits  algorithms.  Using  experiments  on  artificial  and  real  datasets,  we  show  the  importance  and  practicability  of  using  sequential  approaches  to  perform  recommendation.  We  also  study  the  impact  of  the  model  update  on  both  the  quality  and  the  computation  time  of  the  recommendation  procedure.
0	Confidence  sets  for  network  structure.  Latent  variable  models  are  frequently  used  to  identify  structure  in  dichotomous  network  data,  in  part  because  they  give  rise  to  a  Bernoulli  product  likelihood  that  is  both  well  understood  and  consistent  with  the  notion  of  exchangeable  random  graphs.  In  this  article  we  propose  conservative  confidence  sets  that  hold  with  respect  to  these  underlying  Bernoulli  parameters  as  a  function  of  any  given  partition  of  network  nodes,  enabling  us  to  assess  estimates  of  residual  network  structure,  that  is,  structure  that  cannot  be  explained  by  known  covariates  and  thus  cannot  be  easily  verified  by  manual  inspection.  We  demonstrate  the  proposed  methodology  by  analyzing  student  friendship  networks  from  the  National  Longitudinal  Survey  of  Adolescent  Health  that  include  race,  gender,  and  school  year  as  covariates.  We  employ  a  stochastic  expectation-maximization  algorithm  to  fit  a  logistic  regression  model  that  includes  these  explanatory  variables  as  well  as  a  latent  stochastic  blockmodel  component  and  additional  node-specific  effects.  Although  maximum-likelihood  estimates  do  not  appear  consistent  in  this  context,  we  are  able  to  evaluate  confidence  sets  as  a  function  of  different  blockmodel  partitions,  which  enables  us  to  qualitatively  assess  the  significance  of  estimated  residual  network  structure  relative  to  a  baseline,  which  models  covariates  but  lacks  block  structure.
0	Integration  methods  and  optimization  algorithms.  We  show  that  accelerated  optimization  methods  can  be  seen  as  particular  instances  of  multi-step  integration  schemes  from  numerical  analysis,  applied  to  the  gradient  flow  equation.  Compared  with  recent  advances  in  this  vein,  the  differential  equation  considered  here  is  the  basic  gradient  flow,  and  we  derive  a  class  of  multi-step  schemes  which  includes  accelerated  algorithms,  using  classical  conditions  from  numerical  analysis.  Multi-step  schemes  integrate  the  differential  equation  using  larger  step  sizes,  which  intuitively  explains  the  acceleration  phenomenon.
0	Dip  means  an  incremental  clustering  method  for  estimating  the  number  of  clusters.  Learning  the  number  of  clusters  is  a  key  problem  in  data  clustering.  We  present  dip-means,  a  novel  robust  incremental  method  to  learn  the  number  of  data  clusters  that  can  be  used  as  a  wrapper  around  any  iterative  clustering  algorithm  of  k-means  family.  In  contrast  to  many  popular  methods  which  make  assumptions  about  the  underlying  cluster  distributions,  dip-means  only  assumes  a  fundamental  cluster  property:  each  cluster  to  admit  a  unimodal  distribution.  The  proposed  algorithm  considers  each  cluster  member  as  an  individual  'viewer'  and  applies  a  univariate  statistic  hypothesis  test  for  unimodality  (dip-test)  on  the  distribution  of  distances  between  the  viewer  and  the  cluster  members.  Important  advantages  are:  i)  the  unimodality  test  is  applied  on  univariate  distance  vectors,  ii)  it  can  be  directly  applied  with  kernel-based  methods,  since  only  the  pairwise  distances  are  involved  in  the  computations.  Experimental  results  on  artificial  and  real  datasets  indicate  the  effectiveness  of  our  method  and  its  superiority  over  analogous  approaches.
0	Phoneme  classification  using  constrained  variational  gaussian  process  dynamical  system.  For  phoneme  classification,  this  paper  describes  an  acoustic  model  based  on  the  variational  Gaussian  process  dynamical  system  (VGPDS).  The  nonlinear  and  non-parametric  acoustic  model  is  adopted  to  overcome  the  limitations  of  classical  hidden  Markov  models  (HMMs)  in  modeling  speech.  The  Gaussian  process  prior  on  the  dynamics  and  emission  functions  respectively  enable  the  complex  dynamic  structure  and  long-range  dependency  of  speech  to  be  better  represented  than  that  by  an  HMM.  In  addition,  a  variance  constraint  in  the  VGPDS  is  introduced  to  eliminate  the  sparse  approximation  error  in  the  kernel  matrix.  The  effectiveness  of  the  proposed  model  is  demonstrated  with  three  experimental  results,  including  parameter  estimation  and  classification  performance,  on  the  synthetic  and  benchmark  datasets.
0	Private  graphon  estimation  for  sparse  graphs.  We  design  algorithms  for  fitting  a  high-dimensional  statistical  model  to  a  large,  sparse  network  without  revealing  sensitive  information  of  individual  members.  Given  a  sparse  input  graph  G,  our  algorithms  output  a  node-differentially  private  nonparametric  block  model  approximation.  By  node-differentially  private,  we  mean  that  our  output  hides  the  insertion  or  removal  of  a  vertex  and  all  its  adjacent  edges.  If  G  is  an  instance  of  the  network  obtained  from  a  generative  nonparametric  model  defined  in  terms  of  a  graphon  W,  our  model  guarantees  consistency:  as  the  number  of  vertices  tends  to  infinity,  the  output  of  our  algorithm  converges  to  W  in  an  appropriate  version  of  the  L2  norm.  In  particular,  this  means  we  can  estimate  the  sizes  of  all  multi-way  cuts  in  G.    Our  results  hold  as  long  as  W  is  bounded,  the  average  degree  of  G  grows  at  least  like  the  log  of  the  number  of  vertices,  and  the  number  of  blocks  goes  to  infinity  at  an  appropriate  rate.  We  give  explicit  error  bounds  in  terms  of  the  parameters  of  the  model;  in  several  settings,  our  bounds  improve  on  or  match  known  nonprivate  results.
0	Calculating  optimistic  likelihoods  using  geodesically  convex  optimization.  A  fundamental  problem  arising  in  many  areas  of  machine  learning  is  the  evaluation  of  the  likelihood  of  a  given  observation  under  different  nominal  distributions.  Frequently,  these  nominal  distributions  are  themselves  estimated  from  data,  which  makes  them  susceptible  to  estimation  errors.  We  thus  propose  to  replace  each  nominal  distribution  with  an  ambiguity  set  containing  all  distributions  in  its  vicinity  and  to  evaluate  an  optimistic  likelihood,  that  is,  the  maximum  of  the  likelihood  over  all  distributions  in  the  ambiguity  set.  When  the  proximity  of  distributions  is  quantified  by  the  Fisher-Rao  distance  or  the  Kullback-Leibler  divergence,  the  emerging  optimistic  likelihoods  can  be  computed  efficiently  using  either  geodesic  or  standard  convex  optimization  techniques.  We  showcase  the  advantages  of  working  with  optimistic  likelihoods  on  a  classification  problem  using  synthetic  as  well  as  empirical  data.
0	Doubly  robust  lasso  bandit.  Contextual  multi-armed  bandit  algorithms  are  widely  used  in  sequential  decision  tasks  such  as  news  article  recommendation  systems,  web  page  ad  placement  algorithms,  and  mobile  health.  Most  of  the  existing  algorithms  have  regret  proportional  to  a  polynomial  function  of  the  context  dimension,  $d$.  In  many  applications  however,  it  is  often  the  case  that  contexts  are  high-dimensional  with  only  a  sparse  subset  of  size  $s_0  (\ll  d)$  being  correlated  with  the  reward.  We  consider  the  stochastic  linear  contextual  bandit  problem  and  propose  a  novel  algorithm,  namely  the  Doubly-Robust  Lasso  Bandit  algorithm,  which  exploits  the  sparse  structure  of  the  regression  parameter  as  in  Lasso,  while  blending  the  doubly-robust  technique  used  in  missing  data  literature.  The  high-probability  upper  bound  of  the  regret  incurred  by  the  proposed  algorithm  does  not  depend  on  the  number  of  arms  and  scales  with  $\mathrm{log}(d)$  instead  of  a  polynomial  function  of  $d$.  The  proposed  algorithm  shows  good  performance  when  contexts  of  different  arms  are  correlated  and  requires  less  tuning  parameters  than  existing  methods.
0	Scaling  the  poisson  glm  to  massive  neural  datasets  through  polynomial  approximations.  Recent  advances  in  recording  technologies  have  allowed  neuroscientists  to  record  simultaneous  spiking  activity  from  hundreds  to  thousands  of  neurons  in  multiple  brain  regions.  Such  large-scale  recordings  pose  a  major  challenge  to  existing  statistical  methods  for  neural  data  analysis.  Here  we  develop  highly  scalable  approximate  inference  methods  for  Poisson  generalized  linear  models  (GLMs)  that  require  only  a  single  pass  over  the  data.  Our  approach  relies  on  a  recently  proposed  method  for  obtaining  approximate  sufficient  statistics  for  GLMs  using  polynomial  approximations  [Huggins  et  al.,  2017],  which  we  adapt  to  the  Poisson  GLM  setting.  We  focus  on  inference  using  quadratic  approximations  to  nonlinear  terms  in  the  Poisson  GLM  log-likelihood  with  Gaussian  priors,  for  which  we  derive  closed-form  solutions  to  the  approximate  maximum  likelihood  and  MAP  estimates,  posterior  distribution,  and  marginal  likelihood.  We  introduce  an  adaptive  procedure  to  select  the  polynomial  approximation  interval  and  show  that  the  resulting  method  allows  for  efficient  and  accurate  inference  and  regularization  of  high-dimensional  parameters.  We  use  the  quadratic  estimator  to  fit  a  fully-coupled  Poisson  GLM  to  spike  train  data  recorded  from  831  neurons  across  five  regions  of  the  mouse  brain  for  a  duration  of  41  minutes,  binned  at  1  ms  resolution.  Across  all  neurons,  this  model  is  fit  to  over  2  billion  spike  count  bins  and  identifies  fine-timescale  statistical  dependencies  between  neurons  within  and  across  cortical  and  subcortical  areas.
0	Why  do  deep  residual  networks  generalize  better  than  deep  feedforward  networks  a  neural  tangent  kernel  perspective.  Deep  residual  networks  (ResNets)  have  demonstrated  better  generalization  performance  than  deep  feedforward  networks  (FFNets).  However,  the  theory  behind  such  a  phenomenon  is  still  largely  unknown.  This  paper  studies  this  fundamental  problem  in  deep  learning  from  a  so-called  "neural  tangent  kernel"  perspective.  Specifically,  we  first  show  that  under  proper  conditions,  as  the  width  goes  to  infinity,  training  deep  ResNets  can  be  viewed  as  learning  reproducing  kernel  functions  with  some  kernel  function.  We  then  compare  the  kernel  of  deep  ResNets  with  that  of  deep  FFNets  and  discover  that  the  class  of  functions  induced  by  the  kernel  of  FFNets  is  asymptotically  not  learnable,  as  the  depth  goes  to  infinity.  In  contrast,  the  class  of  functions  induced  by  the  kernel  of  ResNets  does  not  exhibit  such  degeneracy.  Our  discovery  partially  justifies  the  advantages  of  deep  ResNets  over  deep  FFNets  in  generalization  abilities.  Numerical  results  are  provided  to  support  our  claim.
0	Exactly  computing  the  local  lipschitz  constant  of  relu  networks.  The  local  Lipschitz  constant  of  a  neural  network  is  a  useful  metric  with  applications  in  robustness,  generalization,  and  fairness  evaluation.  We  provide  novel  analytic  results  relating  the  local  Lipschitz  constant  of  nonsmooth  vector-valued  functions  to  a  maximization  over  the  norm  of  the  generalized  Jacobian.  We  present  a  sufficient  condition  for  which  backpropagation  always  returns  an  element  of  the  generalized  Jacobian,  and  reframe  the  problem  over  this  broad  class  of  functions.  We  show  strong  inapproximability  results  for  estimating  Lipschitz  constants  of  ReLU  networks,  and  then  formulate  an  algorithm  to  compute  these  quantities  exactly.  We  leverage  this  algorithm  to  evaluate  the  tightness  of  competing  Lipschitz  estimators  and  the  effects  of  regularized  training  on  the  Lipschitz  constant.
0	Graph  geometry  interaction  learning.  While  numerous  approaches  have  been  developed  to  embed  graphs  into  either  Euclidean  or  hyperbolic  spaces,  they  do  not  fully  utilize  the  information  available  in  graphs,  or  lack  the  flexibility  to  model  intrinsic  complex  graph  geometry.  To  utilize  the  strength  of  both  Euclidean  and  hyperbolic  geometries,  we  develop  a  novel  Geometry  Interaction  Learning  (GIL)  method  for  graphs,  a  well-suited  and  efficient  alternative  for  learning  abundant  geometric  properties  in  graph.  GIL  captures  a  more  informative  internal  structural  features  with  low  dimensions  while  maintaining  conformal  invariance  of  each  space.  Furthermore,  our  method  endows  each  node  the  freedom  to  determine  the  importance  of  each  geometry  space  via  a  flexible  dual  feature  interaction  learning  and  probability  assembling  mechanism.  Promising  experimental  results  are  presented  for  five  benchmark  datasets  on  node  classification  and  link  prediction  tasks.
0	Editorial  compendium  a  text  summarization  system  for  generating  abstracts  of  research  papers.  This  article  analyzes  the  appropriateness  of  a  text  summarization  system,  COMPENDIUM,  for  generating  abstracts  of  biomedical  papers.  Two  approaches  are  suggested:  an  extractive  (COMPENDIUM"E),  which  only  selects  and  extracts  the  most  relevant  sentences  of  the  documents,  and  an  abstractive-oriented  one  (COMPENDIUM"E"-"A),  thus  facing  also  the  challenge  of  abstractive  summarization.  This  novel  strategy  combines  extractive  information,  with  some  pieces  of  information  of  the  article  that  have  been  previously  compressed  or  fused.  Specifically,  in  this  article,  we  want  to  study:  i)  whether  COMPENDIUM  produces  good  summaries  in  the  biomedical  domain;  ii)  which  summarization  approach  is  more  suitable;  and  iii)  the  opinion  of  real  users  towards  automatic  summaries.  Therefore,  two  types  of  evaluation  were  performed:  quantitative  and  qualitative,  for  evaluating  both  the  information  contained  in  the  summaries,  as  well  as  the  user  satisfaction.  Results  show  that  extractive  and  abstractive-oriented  summaries  perform  similarly  as  far  as  the  information  they  contain,  so  both  approaches  are  able  to  keep  the  relevant  information  of  the  source  documents,  but  the  latter  is  more  appropriate  from  a  human  perspective,  when  a  user  satisfaction  assessment  is  carried  out.  This  also  confirms  the  suitability  of  our  suggested  approach  for  generating  summaries  following  an  abstractive-oriented  paradigm.
0	Answer  set  solving  with  bounded  treewidth  revisited.  Parameterized  algorithms  are  a  way  to  solve  hard  problems  more  efficiently,  given  that  a  specific  parameter  of  the  input  is  small.  In  this  paper,  we  apply  this  idea  to  the  field  of  answer  set  programming  (ASP).  To  this  end,  we  propose  two  kinds  of  graph  representations  of  programs  to  exploit  their  treewidth  as  a  parameter.  Treewidth  roughly  measures  to  which  extent  the  internal  structure  of  a  program  resembles  a  tree.  Our  main  contribution  is  the  design  of  parameterized  dynamic  programming  algorithms,  which  run  in  linear  time  if  the  treewidth  and  weights  of  the  given  program  are  bounded.  Compared  to  previous  work,  our  algorithms  handle  the  full  syntax  of  ASP.  Finally,  we  report  on  an  empirical  evaluation  that  shows  good  runtime  behaviour  for  benchmark  instances  of  low  treewidth,  especially  for  counting  answer  sets.
0	Combining  multiple  classifiers  using  vote  based  classifier  ensemble  technique  for  named  entity  recognition.  In  this  paper,  we  pose  the  classifier  ensemble  problem  under  single  and  multiobjective  optimization  frameworks,  and  evaluate  it  for  Named  Entity  Recognition  (NER),  an  important  step  in  almost  all  Natural  Language  Processing  (NLP)  application  areas.  We  propose  the  solutions  to  two  different  versions  of  the  ensemble  problem  for  each  of  the  optimization  frameworks.  We  hypothesize  that  the  reliability  of  predictions  of  each  classifier  differs  among  the  various  output  classes.  Thus,  in  an  ensemble  system  it  is  necessary  to  find  out  either  the  eligible  classes  for  which  a  classifier  is  most  suitable  to  vote  (i.e.,  binary  vote  based  ensemble)  or  to  quantify  the  amount  of  voting  for  each  class  in  a  particular  classifier  (i.e.,  real  vote  based  ensemble).  We  use  seven  diverse  classifiers,  namely  Naive  Bayes,  Decision  Tree  (DT),  Memory  Based  Learner  (MBL),  Hidden  Markov  Model  (HMM),  Maximum  Entropy  (ME),  Conditional  Random  Field  (CRF)  and  Support  Vector  Machine  (SVM)  to  build  a  number  of  models  depending  upon  the  various  representations  of  the  available  features  that  are  identified  and  selected  mostly  without  using  any  domain  knowledge  and/or  language  specific  resources.  The  proposed  technique  is  evaluated  for  three  resource-constrained  languages,  namely  Bengali,  Hindi  and  Telugu.  Results  using  multiobjective  optimization  (MOO)  based  technique  yield  the  overall  recall,  precision  and  F-measure  values  of  94.21%,  94.72%  and  94.74%,  respectively  for  Bengali,  99.07%,  90.63%  and  94.66%,  respectively  for  Hindi  and  82.79%,  95.18%  and  88.55%,  respectively  for  Telugu.  Results  for  all  the  languages  show  that  the  proposed  MOO  based  classifier  ensemble  with  real  voting  attains  the  performance  level  which  is  superior  to  all  the  individual  classifiers,  three  baseline  ensembles  and  the  corresponding  single  objective  based  ensemble.
0	Who  to  invite  next  predicting  invitees  of  social  groups.  Social  instant  messaging  services  (SMS)  such  as  WhatsApp,  Snapchat  and  WeChat,  have  significantly  changed  the  way  people  work,  live,  and  communicate,  attracting  increasing  attention  from  multiple  disciplinary  including  computer  science,  sociology,  psychology,  and  physics.  In  SMS,  social  groups  play  a  very  important  role  in  supporting  communication  among  multiple  users.  An  interesting  question  arises:  what  are  the  dynamic  mechanisms  underlying  the  group  evolution?  Or  more  specifically,  in  an  existing  group,  who  should  be  invited  to  join?  In  this  paper,  we  formalize  a  novel  problem  of  predicting  potential  invitees  of  groups.  Employing  WeChat,  the  largest  social  messaging  service  in  China,  as  the  source  for  our  experimental  data,  we  develop  a  probabilistic  graph  model  to  capture  the  fundamental  factors  that  determine  the  probability  of  a  user  to  be  invited  to  a  specific  social  group.  Our  results  show  that  the  proposed  model  indeed  lead  to  statistically  significant  prediction  improvements  over  several  state-of-the-art  baseline  methods.
0	Approximate  probabilistic  inference  with  bounded  error  for  hybrid  probabilistic  logic  programming.  Probabilistic  logics,  especially  those  based  on  logic  programming  (LP),  are  gaining  popularity  as  modelling  and  reasoning  tools,  since  they  combine  the  power  of  logic  to  represent  knowledge  with  the  ability  of  probability  theory  to  deal  with  uncertainty.  In  this  paper,  we  propose  a  hybrid  extension  for  probabilistic  logic  programming,  which  allows  for  exact  inference  for  a  much  wider  class  of  continuous  distributions  than  existing  extensions.  At  the  same  time,  our  extension  allows  one  to  compute  approximations  with  bounded  and  arbitrarily  small  error.  We  propose  a  novel  anytime  algorithm  exploiting  the  logical  and  continuous  structure  of  distributions  and  experimentally  show  that  our  algorithm  is,  for  typical  relational  problems,  competitive  with  state-of-the-art  sampling  algorithms  and  outperforms  them  by  far  if  rare  events  with  deterministic  structure  are  provided  as  evidence,  despite  the  fact  that  it  provides  much  stronger  guarantees.
0	Cost  aware  pre  training  for  multiclass  cost  sensitive  deep  learning.  Deep  learning  has  been  one  of  the  most  prominent  machine  learning  techniques  nowadays,  being  the  state-of-the-art  on  a  broad  range  of  applications  where  automatic  feature  extraction  is  needed.  Many  such  applications  also  demand  varying  costs  for  different  types  of  mis-classification  errors,  but  it  is  not  clear  whether  or  how  such  cost  information  can  be  incorporated  into  deep  learning  to  improve  performance.  In  this  work,  we  first  design  a  novel  loss  function  that  embeds  the  cost  information  for  the  training  stage  of  cost-sensitive  deep  learning.  We  then  show  that  the  loss  function  can  also  be  integrated  into  the  pre-training  stage  to  conduct  cost-aware  feature  extraction  more  effectively.  Extensive  experimental  results  justify  the  validity  of  the  novel  loss  function  for  making  existing  deep  learning  models  cost-sensitive,  and  demonstrate  that  our  proposed  model  with  cost-aware  pre-training  and  training  outperforms  non-deep  models  and  other  deep  models  that  digest  the  cost  information  in  other  stages.
0	Rebalancing  expanding  ev  sharing  systems  with  deep  reinforcement  learning.  Electric  Vehicle  (EV)  sharing  systems  have  recently  experienced  unprecedented  growth  across  the  world.  One  of  the  key  challenges  in  their  operation  is  vehicle  rebalancing,  i.e.,  repositioning  the  EVs  across  stations  to  better  satisfy  future  user  demand.  This  is  particularly  challenging  in  the  shared  EV  context,  because  i)  the  range  of  EVs  is  limited  while  charging  time  is  substantial,  which  constrains  the  rebalancing  options;  and  ii)  as  a  new  mobility  trend,  most  of  the  current  EV  sharing  systems  are  still  continuously  expanding  their  station  networks,  i.e.,  the  targets  for  rebalancing  can  change  over  time.  To  tackle  these  challenges,  in  this  paper  we  model  the  rebalancing  task  as  a  Multi-Agent  Reinforcement  Learning  (MARL)  problem,  which  directly  takes  the  range  and  charging  properties  of  the  EVs  into  account.  We  propose  a  novel  approach  of  policy  optimization  with  action  cascading,  which  isolates  the  non-stationarity  locally,  and  use  two  connected  networks  to  solve  the  formulated  MARL.  We  evaluate  the  proposed  approach  using  a  simulator  calibrated  with  1-year  operation  data  from  a  real  EV  sharing  system.  Results  show  that  our  approach  significantly  outperforms  the  state-of-the-art,  offering  up  to  14%  gain  in  order  satisfied  rate  and  12%  increase  in  net  revenue.
0	Formal  query  building  with  query  structure  prediction  for  complex  question  answering  over  knowledge  base.  Formal  query  building  is  an  important  part  of  complex  question  answering  over  knowledge  bases.  It  aims  to  build  correct  executable  queries  for  questions.  Recent  methods  try  to  rank  candidate  queries  generated  by  a  state-transition  strategy.  However,  this  candidate  generation  strategy  ignores  the  structure  of  queries,  resulting  in  a  considerable  number  of  noisy  queries.  In  this  paper,  we  propose  a  new  formal  query  building  approach  that  consists  of  two  stages.  In  the  first  stage,  we  predict  the  query  structure  of  the  question  and  leverage  the  structure  to  constrain  the  generation  of  the  candidate  queries.  We  propose  a  novel  graph  generation  framework  to  handle  the  structure  prediction  task  and  design  an  encoder-decoder  model  to  predict  the  argument  of  the  predetermined  operation  in  each  generative  step.  In  the  second  stage,  we  follow  the  previous  methods  to  rank  the  candidate  queries.  The  experimental  results  show  that  our  formal  query  building  approach  outperforms  existing  methods  on  complex  questions  while  staying  competitive  on  simple  questions.
0	Load  aware  inter  co  processor  parallelism  in  database  query  processing.  For  a  decade,  the  database  community  has  been  exploring  graphics  processing  units  and  other  co-processors  to  accelerate  query  processing.  While  the  developed  algorithms  often  outperform  their  CPU  counterparts,  it  is  not  beneficial  to  keep  processing  devices  idle  while  overutilizing  others.  Therefore,  an  approach  is  needed  that  efficiently  distributes  a  workload  on  available  (co-)processors  while  providing  accurate  performance  estimates  for  the  query  optimizer.  In  this  paper,  we  contribute  heuristics  that  optimize  query  processing  for  response  time  and  throughput  simultaneously  via  inter-device  parallelism.  Our  empirical  evaluation  reveals  that  the  new  approach  achieves  speedups  up  to  1.85  compared  to  state-of-the-art  approaches  while  preserving  accurate  performance  estimations.  In  a  further  series  of  experiments,  we  evaluate  our  approach  on  two  new  use  cases:  joining  and  sorting.  Furthermore,  we  use  a  simulation  to  assess  the  performance  of  our  approach  for  systems  with  multiple  co-processors  and  derive  some  general  rules  that  impact  performance  in  those  systems.  Contribute  heuristics  to  enhance  performance  by  exploiting  inter-device  parallelismHeuristics  consider  load  and  speed  on  (co-)processors.Extensive  evaluation  on  four  use  cases:  aggregation,  selection,  sort,  and  joinAssess  the  performance  of  best  heuristic  for  systems  with  multiple  co-processorsDiscuss  how  operator-stream-based  scheduling  can  be  used  in  a  query  processor
0	Investigating  the  effects  of  psychological  empowerment  and  interpersonal  conflicts  on  employees  knowledge  sharing  intentions.  The  purpose  of  this  paper  is  to  investigate  how  different  forms  of  interpersonal  conflicts  and  employees’  psychological  empowerment  may  affect  knowledge  sharing  intentions  directly  or  indirectly  via  interpersonal  trust  in  the  workplace.,Survey  data  collected  from  249  employees  of  37  of  the  top  500  corporations  in  the  manufacturing  industry  in  Taiwan  were  used  for  the  data  analysis.  The  research  model  was  analyzed  using  the  component-based  structural  equation  modeling  technique,  namely,  the  partial  least  squares  (PLS)  approach.,The  results  indicate  that  both  relationship  and  task  conflicts  have  significant  indirect  effects  on  employees’  knowledge  sharing  intentions  via  psychological  empowerment  and  trust.  Additionally,  psychological  empowerment  significantly  influences  employees’  knowledge  sharing  intentions  both  directly  and  indirectly  via  trust.,The  primary  theoretical  implication  is  an  advancement  in  the  understanding  of  the  critical  antecedents  of  and  their  different  effects  on  employees’  knowledge  sharing  intentions  from  the  perspectives  of  conflict  management  and  individual  psychological  empowerment.  Future  research  may  concentrate  on  investigating  the  bidirectional  interactions  among  trust,  relationship  conflicts  and  task  conflicts  in  different  knowledge-sharing  contexts.,This  study  provides  practical  insights  into  conflict  resolution  intended  to  facilitate  psychological  empowerment  and  interpersonal  trust  that  encourage  knowledge  sharing  in  the  workplace.,To  the  best  of  the  authors’  knowledge,  this  study  is  the  first  knowledge  sharing  study  that  empirically  examines  how  task  and  relationship  conflicts  affect  employees’  knowledge  sharing  intentions  differently  via  the  mediation  of  their  perceived  psychological  empowerment  and  interpersonal  trust  in  one  another  in  the  workplace.
0	A  generalization  of  submodular  cover  via  the  diminishing  return  property  on  the  integer  lattice.  We  consider  a  generalization  of  the  submodular  cover  problem  based  on  the  concept  of  diminishing  return  property  on  the  integer  lattice.  We  are  motivated  by  real  scenarios  in  machine  learning  that  cannot  be  captured  by  (traditional)  sub-modular  set  functions.  We  show  that  the  generalized  submodular  cover  problem  can  be  applied  to  various  problems  and  devise  a  bicriteria  approximation  algorithm.  Our  algorithm  is  guaranteed  to  output  a  log-factor  approximate  solution  that  satisfies  the  constraints  with  the  desired  accuracy.  The  running  time  of  our  algorithm  is  roughly  O(n  log(nr)  log  r),  where  n  is  the  size  of  the  ground  set  and  r  is  the  maximum  value  of  a  coordinate.  The  dependency  on  r  is  exponentially  better  than  the  naive  reduction  algorithms.  Several  experiments  on  real  and  artificial  datasets  demonstrate  that  the  solution  quality  of  our  algorithm  is  comparable  to  naive  algorithms,  while  the  running  time  is  several  orders  of  magnitude  faster.
0	Lower  bounds  for  passive  and  active  learning.  We  develop  unified  information-theoretic  machinery  for  deriving  lower  bounds  for  passive  and  active  learning  schemes.  Our  bounds  involve  the  so-called  Alexander's  capacity  function.  The  supremum  of  this  function  has  been  recently  rediscovered  by  Hanneke  in  the  context  of  active  learning  under  the  name  of  "disagreement  coefficient."  For  passive  learning,  our  lower  bounds  match  the  upper  bounds  of  Gine  and  Koltchinskii  up  to  constants  and  generalize  analogous  results  of  Massart  and  Nedelec.  For  active  learning,  we  provide  first  known  lower  bounds  based  on  the  capacity  function  rather  than  the  disagreement  coefficient.
0	Positive  unlabeled  learning  with  non  negative  risk  estimator.  From  only  positive  (P)  and  unlabeled  (U)  data,  a  binary  classifier  could  be  trained  with  PU  learning,  in  which  the  state  of  the  art  is  unbiased  PU  learning.  However,  if  its  model  is  very  flexible,  empirical  risks  on  training  data  will  go  negative,  and  we  will  suffer  from  serious  overfitting.  In  this  paper,  we  propose  a  non-negative  risk  estimator  for  PU  learning:  when  getting  minimized,  it  is  more  robust  against  overfitting,  and  thus  we  are  able  to  use  very  flexible  models  (such  as  deep  neural  networks)  given  limited  P  data.  Moreover,  we  analyze  the  bias,  consistency,  and  mean-squared-error  reduction  of  the  proposed  risk  estimator,  and  bound  the  estimation  error  of  the  resulting  empirical  risk  minimizer.  Experiments  demonstrate  that  our  risk  estimator  fixes  the  overfitting  problem  of  its  unbiased  counterparts.
0	Learning  multi  channel  deep  feature  representations  for  face  recognition.  Deep  learning  provides  a  natural  way  to  obtain  feature  representations  from  data  without  relying  on  hand-crafted  descriptors.  In  this  paper,  we  propose  to  learn  deep  feature  representations  using  unsupervised  and  supervised  learning  in  a  cascaded  fashion  to  produce  generically  descriptive  yet  class  specic  features.  The  proposed  method  can  take  full  advantage  of  the  availability  of  large-scale  unlabeled  data  and  learn  discriminative  features  (supervised)  from  generic  features  (unsupervised).  It  is  then  applied  to  multiple  essential  facial  regions  to  obtain  multi-channel  deep  facial  representations  for  face  recognition.  The  ecacy  of  the  proposed  feature  representations  is  validated  on  both  controlled  (i.e.,  extended  Yale-B,  Yale,  and  AR)  and  uncontrolled  (PubFig)  benchmark  face  databases.  Experimental  results  show  its  eectiveness.
0	Practical  and  consistent  estimation  of  f  divergences.  The  estimation  of  an  f-divergence  between  two  probability  distributions  based  on  samples  is  a  fundamental  problem  in  statistics  and  machine  learning.  Most  works  study  this  problem  under  very  weak  assumptions,  in  which  case  it  is  provably  hard.  We  consider  the  case  of  stronger  structural  assumptions  that  are  commonly  satisfied  in  modern  machine  learning,  including  representation  learning  and  generative  modelling  with  autoencoder  architectures.  Under  these  assumptions  we  propose  and  study  an  estimator  that  can  be  easily  implemented,  works  well  in  high  dimensions,  and  enjoys  faster  rates  of  convergence.  We  verify  the  behavior  of  our  estimator  empirically  in  both  synthetic  and  real-data  experiments,  and  discuss  its  direct  implications  for  total  correlation,  entropy,  and  mutual  information  estimation.
0	A  direct  tilde  o  1  epsilon  iteration  parallel  algorithm  for  optimal  transport.  Optimal  transportation,  or  computing  the  Wasserstein  or  ``earth  mover's''  distance  between  two  $n$-dimensional  distributions,  is  a  fundamental  primitive  which  arises  in  many  learning  and  statistical  settings.  We  give  an  algorithm  which  solves  the  problem  to  additive  $\epsilon$  accuracy  with  $\tilde{O}(1/\epsilon)$  parallel  depth  and  $\tilde{O}\left(n^2/\epsilon\right)$  work.  \cite{BlanchetJKS18,  Quanrud19}  obtained  this  runtime  through  reductions  to  positive  linear  programming  and  matrix  scaling.  However,  these  reduction-based  algorithms  use  subroutines  which  may  be  impractical  due  to  requiring  solvers  for  second-order  iterations  (matrix  scaling)  or  non-parallelizability  (positive  LP).  Our  methods  match  the  previous-best  work  bounds  by  \cite{BlanchetJKS18,  Quanrud19}  while  either  improving  parallelization  or  removing  the  need  for  linear  system  solves,  and  improve  upon  the  previous  best  first-order  methods  running  in  time  $\tilde{O}(\min(n^2  /  \epsilon^2,  n^{2.5}  /  \epsilon))$  \cite{DvurechenskyGK18,  LinHJ19}.  We  obtain  our  results  by  a  primal-dual  extragradient  method,  motivated  by  recent  theoretical  improvements  to  maximum  flow  \cite{Sherman17}.
0	The  lottery  ticket  hypothesis  for  pre  trained  bert  networks.  In  natural  language  processing  (NLP),  enormous  pre-trained  models  like  BERT  have  become  the  standard  starting  point  for  training  on  a  range  of  downstream  tasks,  and  similar  trends  are  emerging  in  other  areas  of  deep  learning.  In  parallel,  work  on  the  lottery  ticket  hypothesis  has  shown  that  models  for  NLP  and  computer  vision  contain  smaller  matching  subnetworks  capable  of  training  in  isolation  to  full  accuracy  and  transferring  to  other  tasks.  In  this  work,  we  combine  these  observations  to  assess  whether  such  trainable,  transferrable  subnetworks  exist  in  pre-trained  BERT  models.  For  a  range  of  downstream  tasks,  we  indeed  find  matching  subnetworks  at  40%  to  90%  sparsity.  We  find  these  subnetworks  at  (pre-trained)  initialization,  a  deviation  from  prior  NLP  research  where  they  emerge  only  after  some  amount  of  training.  Subnetworks  found  on  the  masked  language  modeling  task  (the  same  task  used  to  pre-train  the  model)  transfer  universally;  those  found  on  other  tasks  transfer  in  a  limited  fashion  if  at  all.  As  large-scale  pre-training  becomes  an  increasingly  central  paradigm  in  deep  learning,  our  results  demonstrate  that  the  main  lottery  ticket  observations  remain  relevant  in  this  context.  Codes  available  at  this  https  URL.
0	Domain  generalization  for  medical  imaging  classification  with  linear  dependency  regularization.  Recently,  we  have  witnessed  great  progress  in  the  field  of  medical  imaging  classification  by  adopting  deep  neural  networks.  However,  the  recent  advanced  models  still  require  accessing  sufficiently  large  and  representative  datasets  for  training,  which  is  often  unfeasible  in  clinically  realistic  environments.  When  trained  on  limited  datasets,  the  deep  neural  network  is  lack  of  generalization  capability,  as  the  trained  deep  neural  network  on  data  within  a  certain  distribution  (e.g.  the  data  captured  by  a  certain  device  vendor  or  patient  population)  may  not  be  able  to  generalize  to  the  data  with  another  distribution.  In  this  paper,  we  introduce  a  simple  but  effective  approach  to  improve  the  generalization  capability  of  deep  neural  networks  in  the  field  of  medical  imaging  classification.  Motivated  by  the  observation  that  the  domain  variability  of  the  medical  images  is  to  some  extent  compact,  we  propose  to  learn  a  representative  feature  space  through  variational  encoding  with  a  novel  linear-dependency  regularization  term  to  capture  the  shareable  information  among  medical  data  collected  from  different  domains.  As  a  result,  the  trained  neural  network  is  expected  to  equip  with  better  generalization  capability  to  the  "unseen"  medical  data.  Experimental  results  on  two  challenging  medical  imaging  classification  tasks  indicate  that  our  method  can  achieve  better  cross-domain  generalization  capability  compared  with  state-of-the-art  baselines.
0	Node  embeddings  and  exact  low  rank  representations  of  complex  networks.  Low-dimensional  embeddings,  from  classical  spectral  embeddings  to  modern  neural-net-inspired  methods,  are  a  cornerstone  in  the  modeling  and  analysis  of  complex  networks.  Recent  work  by  Seshadhri  et  al.  (PNAS  2020)  suggests  that  such  embeddings  cannot  capture  local  structure  arising  in  complex  networks.  In  particular,  they  show  that  any  network  generated  from  a  natural  low-dimensional  model  cannot  be  both  sparse  and  have  high  triangle  density  (high  clustering  coefficient),  two  hallmark  properties  of  many  real-world  networks.    In  this  work  we  show  that  the  results  of  Seshadhri  et  al.  are  intimately  connected  to  the  model  they  use  rather  than  the  low-dimensional  structure  of  complex  networks.  Specifically,  we  prove  that  a  minor  relaxation  of  their  model  can  generate  sparse  graphs  with  high  triangle  density.  Surprisingly,  we  show  that  this  same  model  leads  to  exact  low-dimensional  factorizations  of  many  real-world  networks.  We  give  a  simple  algorithm  based  on  logistic  principal  component  analysis  (LPCA)  that  succeeds  in  finding  such  exact  embeddings.  Finally,  we  perform  a  large  number  of  experiments  that  verify  the  ability  of  very  low-dimensional  embeddings  to  capture  local  structure  in  real-world  networks.
0	Toward  a  theoretical  model  of  learning  organization  and  knowledge  management  processes.  This  article  puts  forward  a  conceptual  model  for  understanding  the  influence  of  the  learning  organization's  five  disciplines  on  knowledge  management  processes.  It  proposes  that  the  learning  process  should  be  measured  as  a  multi-disciplinary  construct  consisting  of  personal  mastery,  mental  models,  team  learning,  shared  vision,  and  systems  thinking.  Potential  implications  of  learning  organisation  disciplines  for  the  acquisition,  sharing,  and  application  of  knowledge  are  illustrated.  A  number  of  possible  relationships  between  learning  organization  disciplines,  and  knowledge  management  processes  were  employed  to  propose  such  relationships.  This  article  develops  a  measure  that  shows  the  connection  between  the  learning  organization's  disciplines  and  knowledge  management  processes.  Through  a  conceptualization  of  the  relationships  between  the  learning  organization  discipline  and  knowledge  management  processes  the  study  provides  practical  guidance  for  practitioners  during  the  implementation  of  knowledge  management  processes.
0	A  practical  approach  to  generalized  hierarchical  task  specification  for  indirect  force  controlled  robots.  The  main  contribution  of  this  paper  is  the  general  formulation  of  force  and  positioning  tasks  on  joint  and  Cartesian  level  for  indirect  force  controlled  robots  and  combining  them  in  a  strict  hierarchical  way.  As  a  secondary  contribution,  we  provide  a  simple  and  intuitive  programming  paradigm,  using  the  developed  formulation.  By  building  on  the  well-established  indirect  force  control  scheme,  which  is  often  already  provided  for  commercial  robots,  we  provide  application  programmers  with  a  useful  tool  for  specifying  tasks,  involving  positioning  and  force  components.  Different  physical  interaction  tasks  have  been  implemented  to  show  the  potential  of  the  proposed  method  and  discuss  the  general  advantages  and  drawbacks.
0	Scheduling  operator  attention  for  multi  robot  control.  A  wide  class  of  multirobot  control  tasks  involves  operator  interactions  with  individual  robots.  Where  the  robots'  actions  are  independent,  as  for  example  in  some  foraging  tasks,  the  operator  can  interact  with  robots  sequentially  in  a  round  robin  fashion.  If  the  need  for  interaction  can  be  detected  by  the  robot  through  self-reflection,  the  robot  could  communicate  its  need  for  interaction  to  the  operator.  The  resulting  human-robot  system  would  form  a  queuing  system  in  which  the  operator  is  the  server  and  the  queue  of  robots  requesting  interaction,  the  jobs.  As  a  queuing  system,  performance  could  be  optimized  using  standard  techniques,  providing  the  operator's  attention  could  be  appropriately  directed.  An  earlier  study  found  that  Human-Robot  Interaction  (HRI)  performance  was  improved  by  communicating  requests  for  interaction  to  the  operator,  however,  a  first-in-first-out  (FIFO)  aid  showing  a  single  request  at  a  time  led  to  poorer  performance  than  one  showing  the  entire  (Open)  queue.  The  current  experiment  compared  Open-queue  and  FIFO  conditions  from  the  first  experiment  with  a  Priority-queue  using  a  shortest  job  first  (SJF)  discipline  known  to  maximize  throughput.  Performance  in  the  Priority-queue  condition  was  statistically  indistinguishable  from  the  best  performance  for  all  measures  except  those  for  missed  victims  where  it  was  intermediate  between  FIFO  (best)  and  Open-queue.  Both  of  the  other  conditions  produced  poorest  performance  on  some  measures.  The  results  suggest  that  operator  attention  can  be  effectively  scheduled  allowing  the  use  of  scheduling  algorithms  to  improve  the  efficiency  of  HRI.
0	Act  to  see  and  see  to  act  pomdp  planning  for  objects  search  in  clutter.  We  study  the  problem  of  objects  search  in  clutter.  In  cluttered  environments,  partial  occlusion  among  objects  prevents  vision  systems  from  correctly  recognizing  objects.  Hence,  the  agent  needs  to  move  objects  around  to  gather  information,  which  helps  reduce  uncertainty  in  perception.  At  the  same  time,  the  agent  needs  to  minimize  the  efforts  of  moving  objects  to  reduce  the  time  required  to  complete  the  task.  We  model  the  problem  as  a  Partially  Observable  Markov  Decision  Process  (POMDP),  formulating  it  as  a  problem  of  optimal  decision  making  under  uncertainty.  By  exploiting  spatial  constraints,  we  are  able  to  adapt  online  POMDP  planners  to  handle  objects  search  problems  with  large  state  space  and  action  space.  Experiments  show  that  the  POMDP  solution  outperforms  greedy  approaches,  especially  in  cases  where  multi-step  manipulation  is  required.
0	Generalized  information  filtering  for  mav  parameter  estimation.  In  this  paper  we  present  a  new  estimation  algorithm  that  allows  for  the  combination  of  information  from  any  number  of  process  and  measurement  models.  This  adds  more  flexibility  to  the  design  of  the  estimator  and  in  our  case  avoids  the  need  for  state  augmentation.  We  achieve  this  by  adapting  the  maximum  likelihood  formulation  of  the  Kalman  Filter,  and  thereby  represent  all  measurement  models  as  residuals.  Posing  the  problem  in  this  form  allows  for  the  straightforward  integration  of  any  number  of  (nonlinear)  constraints  between  two  subsequent  states.  To  solve  the  optimization  we  present  a  closed  form  recursive  set  of  equations  that  directly  marginalizes  out  information  that  is  not  required,  this  leads  to  an  efficient  and  generic  implementation.  The  new  algorithm  is  applied  to  parameter  estimation  on  MAVs  which  have  two  dynamic  models,  the  MAV  dynamic  model  and  the  IMU-driven  model.  We  show  the  benefits  and  limitations  of  the  new  filtering  approach  on  a  simplified  simulation  example  and  on  a  real  MAV  system.
0	Automatic  force  compliant  robotic  ultrasound  screening  of  abdominal  aortic  aneurysms.  Ultrasound  (US)  imaging  is  commonly  employed  for  the  diagnosis  and  staging  of  abdominal  aortic  aneurysms  (AAA),  mainly  due  to  its  non-invasiveness  and  high  availability.  High  inter-operator  variability  and  a  lack  of  repeatability  of  current  US  image  acquisition  impair  the  implementation  of  extensive  screening  programs  for  affected  patient  populations.  However,  this  opens  the  way  to  a  possible  automation  of  the  procedure,  and  recent  works  have  exploited  the  use  of  robotic  platforms  for  US  applications,  both  in  diagnostic  and  interventional  scenarios.  In  this  work,  we  propose  a  system  for  autonomous  robotic  US  acquisitions  aimed  at  the  quantitative  assessment  of  patients'  vessel  diameter  for  abdominal  aortic  aneurysm  screening.  Using  a  probabilistic  measure  of  the  US  quality,  we  introduce  an  automatic  estimation  of  the  optimal  pressure  to  be  applied  during  the  acquisition,  and  an  online  optimization  of  the  out-of-plane  rotation  of  the  US  probe  to  maximize  the  visibility  of  the  aorta.  We  evaluate  our  method  on  healthy  volunteers  and  compare  the  results  to  manual  acquisitions  performed  by  a  clinical  expert,  demonstrating  the  feasibility  of  the  presented  system  for  AAA  screening.
0	Reconfigurable  intelligent  space  r  ispace  and  mobile  module  momo.  In  this  paper,  the  concept  of  R+iSpace  and  the  mechanical  architecture  of  mobile  module  MoMo  for  the  R+iSpace  is  introduced.  The  R+iSpace  denotes  ‘Reconfigurable  Intelligent  Space’.  The  R+iSpace  is  able  to  rearrange  every  devices  in  the  space  according  to  situation  of  the  space,  and  it  reconfigures  the  system  itself.  For  rearranging  devices,  which  includes  sensors,  projectors,  etc,  the  mobile  module  MoMo  is  proposed.  With  the  MoMo  and  a  little  modification  of  environment,  the  R+iSpace  can  be  achieved.
0	Obstacle  detection  for  usvs  by  joint  stereo  view  semantic  segmentation.  We  propose  a  stereo-based  obstacle  detection  approach  for  unmanned  surface  vehicles.  Obstacle  detection  is  cast  as  a  scene  semantic  segmentation  problem  in  which  pixels  are  assigned  a  probability  of  belonging  to  water  or  non-water  regions.  We  extend  a  single-view  model  to  a  stereo  system  by  adding  a  constraint  which  prefers  consistent  class  labels  assignment  to  pixels  in  the  left  and  right  camera  images  corresponding  to  the  same  parts  of  a  3D  scene.  Our  approach  jointly  fits  a  semantic  model  to  both  images,  leading  to  an  improved  class-label  posterior  map  from  which  obstacles  and  water  edge  are  extracted.  In  overall  F-measure,  our  approach  outperforms  the  current  state-of-the-art  monocular  approach  by  0.495,  a  monocular  CNN  by  0.798  and  their  stereo  extensions  by  0.059  and  0.515,  respectively  on  the  task  of  obstacle  detection  while  running  real-time  on  a  single  CPU.
0	A  method  for  robot  motor  fatigue  management  in  physical  interaction  and  human  robot  collaboration  tasks.  Collaborative  robots  are  often  designed  with  limited  power  and  force  capacity,  with  the  aim  to  provide  affordable  solutions  and  ensure  human  safety  in  case  of  accidental  collisions  and  impacts.  If  a  task  requires  a  power  beyond  this  capacity,  or  is  performed  repeatedly  over  long  periods,  such  limits  may  be  exceeded,  which  can  cause  inevitable  robot  damage  and  contribute  to  the  lost  productivity.  In  such  cases,  where  hardware  solutions  and  improvements  are  not  applicable,  effective  software  frameworks  can  prolong  robot  productivity  and  lifetime.  To  this  end,  in  this  paper  we  propose  a  novel  technique  for  the  monitoring  and  management  of  robot  fatigue  in  repetitive  or  high-effort  task  execution  scenarios.  The  robot  fatigue  is  estimated  by  the  measured  temperature  of  motors  in  the  joints.  The  proposed  fatigue  management  system  is  composed  of  two-stage  reaction  process  that  is  triggered  by  different  levels  of  the  estimated  fatigue.  The  first  stage  exploits  the  kinematic  redundancy  of  robot  structure  in  attempt  to  minimise  the  load  in  the  specific  joints  that  under  fatigue  by  reconfiguration  in  the  joint  space  through  the  null  space  of  the  Cartesian  task  production.  If  the  first  stage  is  not  successful  in  reducing  the  fatigue,  the  second  stage  is  activated  that  gradually  reduces  the  forces  of  hybrid  controller.  At  that  point,  the  human  co-worker  can  temporarily  take  over  the  task  execution  until  the  robot  will  be  recovered  from  the  excessive  fatigue.  To  validate  the  proposed  approach  we  conducted  experiments  on  KUKA  Lightweight  Robot  performing  two  interaction  tasks:  autonomous  surface  wiping  and  collaborative  human-robot  surface  polishing.
0	Clothoid  based  moving  formation  control  using  virtual  structures.  Formation  control  is  a  canonical  problem  in  multi-agent  systems  as  many  multi-agent  problems  require  agents  to  travel  in  coordination  at  some  point  during  execution.  This  paper  develops  a  method  for  coordinated  moving  formation  control  by  building  upon  existing  virtual  structures  approaches  to  define  the  relative  vehicle  positions  and  orientations  and  building  upon  clothoid-based  motion  planning  to  create  the  desired  motion  of  the  structure.  The  result  is  a  coordinated  formation  control  method  that  respects  individual  curvature  constraints  of  each  agent  while  allowing  agents  to  track  their  desired  positions  within  the  formation  with  asymptotic  convergence.
0	Smoothed  gradients  for  stochastic  variational  inference.  Stochastic  variational  inference  (SVI)  lets  us  scale  up  Bayesian  computation  to  massive  data.  It  uses  stochastic  optimization  to  fit  a  variational  distribution,  following  easy-to-compute  noisy  natural  gradients.  As  with  most  traditional  stochastic  optimization  methods,  SVI  takes  precautions  to  use  unbiased  stochastic  gradients  whose  expectations  are  equal  to  the  true  gradients.  In  this  paper,  we  explore  the  idea  of  following  biased  stochastic  gradients  in  SVI.  Our  method  replaces  the  natural  gradient  with  a  similarly  constructed  vector  that  uses  a  fixed-window  moving  average  of  some  of  its  previous  terms.  We  will  demonstrate  the  many  advantages  of  this  technique.  First,  its  computational  cost  is  the  same  as  for  SVI  and  storage  requirements  only  multiply  by  a  constant  factor.  Second,  it  enjoys  significant  variance  reduction  over  the  unbiased  estimates,  smaller  bias  than  averaged  gradients,  and  leads  to  smaller  mean-squared  error  against  the  full  gradient.  We  test  our  method  on  latent  Dirichlet  allocation  with  three  large  corpora.
0	Stochastic  gradient  descent  weighted  sampling  and  the  randomized  kaczmarz  algorithm.  We  obtain  an  improved  finite-sample  guarantee  on  the  linear  convergence  of  stochastic  gradient  descent  for  smooth  and  strongly  convex  objectives,  improving  from  a  quadratic  dependence  on  the  conditioning  $$(L/\mu  )^2$$(L/μ)2  (where  $$L$$L  is  a  bound  on  the  smoothness  and  $$\mu  $$μ  on  the  strong  convexity)  to  a  linear  dependence  on  $$L/\mu  $$L/μ.  Furthermore,  we  show  how  reweighting  the  sampling  distribution  (i.e.  importance  sampling)  is  necessary  in  order  to  further  improve  convergence,  and  obtain  a  linear  dependence  in  the  average  smoothness,  dominating  previous  results.  We  also  discuss  importance  sampling  for  SGD  more  broadly  and  show  how  it  can  improve  convergence  also  in  other  scenarios.  Our  results  are  based  on  a  connection  we  make  between  SGD  and  the  randomized  Kaczmarz  algorithm,  which  allows  us  to  transfer  ideas  between  the  separate  bodies  of  literature  studying  each  of  the  two  methods.  In  particular,  we  recast  the  randomized  Kaczmarz  algorithm  as  an  instance  of  SGD,  and  apply  our  results  to  prove  its  exponential  convergence,  but  to  the  solution  of  a  weighted  least  squares  problem  rather  than  the  original  least  squares  problem.  We  then  present  a  modified  Kaczmarz  algorithm  with  partially  biased  sampling  which  does  converge  to  the  original  least  squares  solution  with  the  same  exponential  convergence  rate.
0	A  survey  of  modern  questions  and  challenges  in  feature  extraction.  The  problem  of  extracting  features  from  given  input  data  is  of  critical  importance  for  the  successful  application  of  machine  learning.  Feature  extraction,  as  usually  understood,  seeks  an  optimal  transformation  from  input  data  into  a  (typically  real-valued)  feature  vector  that  can  be  used  as  an  input  for  a  learning  algorithm.  Over  time,  this  problem  has  been  attacked  using  a  growing  number  of  diverse  techniques  that  originated  in  separate  research  communities,  including  feature  selection,  dimensionality  reduction,  manifold  learning,  distance  metric  learning  and  representation  learning.  The  goal  of  this  paper  is  to  contrast  and  compare  feature  extraction  techniques  coming  from  different  machine  learning  areas,  discuss  the  modern  challenges  and  open  problems  in  feature  extraction  and  suggest  novel  solutions  to  some  of  them.
0	A  memory  frontier  for  complex  synapses.  An  incredible  gulf  separates  theoretical  models  of  synapses,  often  described  solely  by  a  single  scalar  value  denoting  the  size  of  a  postsynaptic  potential,  from  the  immense  complexity  of  molecular  signaling  pathways  underlying  real  synapses.  To  understand  the  functional  contribution  of  such  molecular  complexity  to  learning  and  memory,  it  is  essential  to  expand  our  theoretical  conception  of  a  synapse  from  a  single  scalar  to  an  entire  dynamical  system  with  many  internal  molecular  functional  states.  Moreover,  theoretical  considerations  alone  demand  such  an  expansion;  network  models  with  scalar  synapses  assuming  finite  numbers  of  distinguishable  synaptic  strengths  have  strikingly  limited  memory  capacity.  This  raises  the  fundamental  question,  how  does  synaptic  complexity  give  rise  to  memory?  To  address  this,  we  develop  new  mathematical  theorems  elucidating  the  relationship  between  the  structural  organization  and  memory  properties  of  complex  synapses  that  are  themselves  molecular  networks.  Moreover,  in  proving  such  theorems,  we  uncover  a  framework,  based  on  first  passage  time  theory,  to  impose  an  order  on  the  internal  states  of  complex  synaptic  models,  thereby  simplifying  the  relationship  between  synaptic  structure  and  function.
0	On  a  theory  of  nonparametric  pairwise  similarity  for  clustering  connecting  clustering  to  classification.  Pairwise  clustering  methods  partition  the  data  space  into  clusters  by  the  pairwise  similarity  between  data  points.  The  success  of  pairwise  clustering  largely  depends  on  the  pairwise  similarity  function  defined  over  the  data  points,  where  kernel  similarity  is  broadly  used.  In  this  paper,  we  present  a  novel  pairwise  clustering  framework  by  bridging  the  gap  between  clustering  and  multi-class  classification.  This  pairwise  clustering  framework  learns  an  unsupervised  nonparametric  classifier  from  each  data  partition,  and  search  for  the  optimal  partition  of  the  data  by  minimizing  the  generalization  error  of  the  learned  classifiers  associated  with  the  data  partitions.  We  consider  two  nonparametric  classifiers  in  this  framework,  i.e.  the  nearest  neighbor  classifier  and  the  plug-in  classifier.  Modeling  the  underlying  data  distribution  by  nonparametric  kernel  density  estimation,  the  generalization  error  bounds  for  both  unsupervised  nonparametric  classifiers  are  the  sum  of  nonparametric  pairwise  similarity  terms  between  the  data  points  for  the  purpose  of  clustering.  Under  uniform  distribution,  the  nonparametric  similarity  terms  induced  by  both  unsupervised  classifiers  exhibit  a  well  known  form  of  kernel  similarity.  We  also  prove  that  the  generalization  error  bound  for  the  unsupervised  plug-in  classifier  is  asymptotically  equal  to  the  weighted  volume  of  cluster  boundary  [1]  for  Low  Density  Separation,  a  widely  used  criteria  for  semi-supervised  learning  and  clustering.  Based  on  the  derived  nonparametric  pairwise  similarity  using  the  plug-in  classifier,  we  propose  a  new  nonparametric  exemplar-based  clustering  method  with  enhanced  discriminative  capability,  whose  superiority  is  evidenced  by  the  experimental  results.
0	Error  minimizing  estimates  and  universal  entry  wise  error  bounds  for  low  rank  matrix  completion.  We  propose  a  general  framework  for  reconstructing  and  denoising  single  entries  of  incomplete  and  noisy  entries.  We  describe:  effective  algorithms  for  deciding  if  and  entry  can  be  reconstructed  and,  if  so,  for  reconstructing  and  denoising  it;  and  a  priori  bounds  on  the  error  of  each  entry,  individually.  In  the  noiseless  case  our  algorithm  is  exact.  For  rank-one  matrices,  the  new  algorithm  is  fast,  admits  a  highly-parallel  implementation,  and  produces  an  error  minimizing  estimate  that  is  qualitatively  close  to  our  theoretical  and  the  state-of-the-art  Nuclear  Norm  and  OptSpace  methods.
0	Learning  by  abstraction  the  neural  state  machine.  We  introduce  the  Neural  State  Machine,  seeking  to  bridge  the  gap  between  the  neural  and  symbolic  views  of  AI  and  integrate  their  complementary  strengths  for  the  task  of  visual  reasoning.  Given  an  image,  we  first  predict  a  probabilistic  graph  that  represents  its  underlying  semantics  and  serves  as  a  structured  world  model.  Then,  we  perform  sequential  reasoning  over  the  graph,  iteratively  traversing  its  nodes  to  answer  a  given  question  or  draw  a  new  inference.  In  contrast  to  most  neural  architectures  that  are  designed  to  closely  interact  with  the  raw  sensory  data,  our  model  operates  instead  in  an  abstract  latent  space,  by  transforming  both  the  visual  and  linguistic  modalities  into  semantic  concept-based  representations,  thereby  achieving  enhanced  transparency  and  modularity.  We  evaluate  our  model  on  VQA-CP  and  GQA,  two  recent  VQA  datasets  that  involve  compositionality,  multi-step  inference  and  diverse  reasoning  skills,  achieving  state-of-the-art  results  in  both  cases.  We  provide  further  experiments  that  illustrate  the  model's  strong  generalization  capacity  across  multiple  dimensions,  including  novel  compositions  of  concepts,  changes  in  the  answer  distribution,  and  unseen  linguistic  structures,  demonstrating  the  qualities  and  efficacy  of  our  approach.
0	Task  driven  convolutional  recurrent  models  of  the  visual  system.  Feed-forward  convolutional  neural  networks  (CNNs)  are  currently  state-of-the-art  for  object  classification  tasks  such  as  ImageNet.  Further,  they  are  quantitatively  accurate  models  of  temporally-averaged  responses  of  neurons  in  the  primate  brain's  visual  system.  However,  biological  visual  systems  have  two  ubiquitous  architectural  features  not  shared  with  typical  CNNs:  local  recurrence  within  cortical  areas,  and  long-range  feedback  from  downstream  areas  to  upstream  areas.  Here  we  explored  the  role  of  recurrence  in  improving  classification  performance.  We  found  that  standard  forms  of  recurrence  (vanilla  RNNs  and  LSTMs)  do  not  perform  well  within  deep  CNNs  on  the  ImageNet  task.  In  contrast,  novel  cells  that  incorporated  two  structural  features,  bypassing  and  gating,  were  able  to  boost  task  accuracy  substantially.  We  extended  these  design  principles  in  an  automated  search  over  thousands  of  model  architectures,  which  identified  novel  local  recurrent  cells  and  long-range  feedback  connections  useful  for  object  recognition.  Moreover,  these  task-optimized  ConvRNNs  matched  the  dynamics  of  neural  activity  in  the  primate  visual  system  better  than  feedforward  networks,  suggesting  a  role  for  the  brain's  recurrent  connections  in  performing  difficult  visual  behaviors.
0	Multiparameter  persistence  image  for  topological  machine  learning.  In  the  last  decade,  there  has  been  increasing  interest  in  topological  data  analysis,  a  new  methodology  for  using  geometric  structures  in  data  for  inference  and  learning.  A  central  theme  in  the  area  is  the  idea  of  persistence,  which  in  its  most  basic  form  studies  how  measures  of  shape  change  as  a  scale  parameter  varies.  There  are  now  a  number  of  frameworks  that  support  statistics  and  machine  learning  in  this  context.  However,  in  many  applications  there  are  several  different  parameters  one  might  wish  to  vary:  for  example,  scale  and  density.  In  contrast  to  the  one-parameter  setting,  techniques  for  applying  statistics  and  machine  learning  in  the  setting  of  multiparameter  persistence  are  not  well  understood  due  to  the  lack  of  a  concise  representation  of  the  results.  We  introduce  a  new  descriptor  for  multiparameter  persistence,  which  we  call  the  Multiparameter  Persistence  Image,  that  is  suitable  for  machine  learning  and  statistical  frameworks,  is  robust  to  perturbations  in  the  data,  has  finer  resolution  than  existing  descriptors  based  on  slicing,  and  can  be  efficiently  computed  on  data  sets  of  realistic  size.  Moreover,  we  demonstrate  its  efficacy  by  comparing  its  performance  to  other  multiparameter  descriptors  on  several  classification  tasks.
0	Learning  to  detect  objects  with  a  1  megapixel  event  camera.  Event  cameras  encode  visual  information  with  high  temporal  precision,  low  data-rate,  and  high-dynamic  range.  Thanks  to  these  characteristics,  event  cameras  are  particularly  suited  for  scenarios  with  high  motion,  challenging  lighting  conditions  and  requiring  low  latency.  However,  due  to  the  novelty  of  the  field,  the  performance  of  event-based  systems  on  many  vision  tasks  is  still  lower  compared  to  conventional  frame-based  solutions.  The  main  reasons  for  this  performance  gap  are:  the  lower  spatial  resolution  of  event  sensors,  compared  to  frame  cameras;  the  lack  of  large-scale  training  datasets;  the  absence  of  well  established  deep  learning  architectures  for  event-based  processing.  In  this  paper,  we  address  all  these  problems  in  the  context  of  an  event-based  object  detection  task.  First,  we  publicly  release  the  first  high-resolution  large-scale  dataset  for  object  detection.  The  dataset  contains  more  than  14  hours  recordings  of  a  1  megapixel  event  camera,  in  automotive  scenarios,  together  with  25M  bounding  boxes  of  cars,  pedestrians,  and  two-wheelers,  labeled  at  high  frequency.  Second,  we  introduce  a  novel  recurrent  architecture  for  event-based  detection  and  a  temporal  consistency  loss  for  better-behaved  training.  The  ability  to  compactly  represent  the  sequence  of  events  into  the  internal  memory  of  the  model  is  essential  to  achieve  high  accuracy.  Our  model  outperforms  by  a  large  margin  feed-forward  event-based  architectures.  Moreover,  our  method  does  not  require  any  reconstruction  of  intensity  images  from  events,  showing  that  training  directly  from  raw  events  is  possible,  more  efficient,  and  more  accurate  than  passing  through  an  intermediate  intensity  image.  Experiments  on  the  dataset  introduced  in  this  work,  for  which  events  and  gray  level  images  are  available,  show  performance  on  par  with  that  of  highly  tuned  and  studied  frame-based  detectors.
0	A  haptic  shared  control  algorithm  for  flexible  human  assistance  to  semi  autonomous  robots.  Autonomous  as  well  as  teleoperated  robots  find  wide  applications  in  various  environments.  Their  capability  to  accomplish  complex  and  dynamic  operations  can  be  significantly  improved  by  fusing  human  intelligence  with  autonomous  algorithms.  In  this  paper,  we  propose  a  haptic  shared  control  algorithm  to  provide  flexible  human  assistance  for  semi-autonomous  mobile  robots.  Through  the  admittance  and  impedance  models,  the  haptic  shared  controller  smoothly  puts  together  human  operator  inputs  with  robot  autonomy.  Further,  the  level  of  autonomy  is  fully  determined  by  the  operator  with  the  grasp  motion.  A  decomposed  design  has  been  taken  for  the  autonomous  controller  of  the  mobile  robot.  The  algorithm  was  implemented  on  the  haptic  interface  omega.7  together  with  a  QBot  mobile  robot,  and  its  feasibility  and  efficacy  have  been  validated  by  experiments.
0	Bilateral  physical  interaction  with  a  robot  manipulator  through  a  weighted  combination  of  flow  fields.  When  collaboration  between  human  users  and  robots  involves  physical  interaction,  the  importance  of  the  safety  issue  arises.  We  propose  a  method  to  transfer  to  robots  several  tasks  demonstrated  by  the  user  through  kinesthetic  teaching  and  subsequently  learned  using  a  weighted  combination  of  dynamical  systems  (DS).  The  approach  used  to  encode  the  desired  skills  ensures  a  safe  robot  behavior  during  the  task  reproduction,  allowing  physical  interaction  with  the  user  who  can  employ  the  manipulator  as  a  tangible  interface.  By  using  a  force  sensor-less  impedance  controller  with  a  back-drivable  robot,  this  concept  is  exploited  in  two  physical  human-robot  interaction  (pHRI)  scenarios.  The  first  considers  an  emergency  situation  in  which  the  user  can  stop  or  pause  a  task  execution  by  grasping  and  moving  the  robot  away  from  the  region  of  space  associated  to  the  skill.  The  second  studies  the  possibility  to  select  one  among  several  learned  tasks  and  switch  to  its  execution  by  physically  guiding  the  robot  towards  the  task  region.
0	Kinematic  and  dynamic  analysis  of  a  novel  6  dof  serial  manipulator  for  underground  distribution  power  lines.  This  paper  presents  a  new  6-DOF  serial  manipulator  with  five  revolute  joints  and  one  prismatic  joint,  designed  to  operate  equipment  inside  Hydro-Quebec  underground  distribution  vaults.  The  manipulator  is  an  upgraded  model  developed  after  field  testing  a  6-DOF  wrist-partitioned  serial  manipulator  prototype.  The  new  manipulator  architecture  is  intended  to  solve  space  constraint  problems  in  some  vaults.  This  paper  covers  the  geometrical  model  of  the  new  manipulator,  an  analytical  solution  for  the  inverse  kinematic  equations,  the  dynamic  model  used  to  compute  torques/forces  at  the  actuators,  and  simulations  performed  with  MATLAB  and  CATIA.  The  manipulator  is  the  only  robot  application  in  the  world  designed  to  operate  equipment  on  underground  distribution  power  lines.
0	Hawkeye  open  source  framework  for  field  surveillance.  This  paper  introduces  a  generic  framework  for  field  surveillance  using  consumer  rotorcrafts  and  ground  vehicles.  Building  such  an  autonomous  system  comes  with  two  key  challenges  in  persistent  perception  and  obstacle  avoidance.  We  begin  with  explaining  two  core  algorithms  to  solve  the  challenges:  an  auto-landing  algorithm  that  enables  a  quadrotor  to  land  on  a  moving  ground  vehicle  at  a  speed  of  6.00  m/s,  and  an  obstacle  avoidance  algorithm  that  ensures  the  safety  of  the  quadrotor  during  searching  process.  On  the  basis  of  these  algorithms,  the  architecture  and  infrastructure  of  Hawkeye  framework  are  presented  as  well.  Hawkeye  is  designed  to  be  a  generic  platform  with  extensibility  that  allows  integration  of  other  domain  applications.  We  demonstrate  the  potential  of  Hawkeye  framework  in  a  simulated  agriculture  monitoring  mission  and  report  its  performance  at  the  end  of  the  paper.
0	Modeling  structure  and  aerosol  concentration  with  fused  radar  and  lidar  data  in  environments  with  changing  visibility.  LiDAR  scanners  are  commonly  used  for  mapping  and  localization  with  mobile  robots.  But,  they  cannot  see  through  occlusions,  as  it  occurs  in  harsh  environments,  containing  smoke,  fog  or  dust.  Radar  scanners  can  overcome  this  problem,  but  they  have  lower  range  and  angular  resolution,  and  cannot  represent  an  environment  in  the  same  quality.  In  the  following  article,  we  present  the  integration  of  fused  LiDAR  and  radar  data  into  a  SLAM  cycle  and  continue  our  work  from  [1],  where  we  presented  first  results  regarding  a  feature  based  and  a  scan  matching-based  approach  for  SLAM  in  environments  with  changing  visibility  using  LiDAR  and  radar  sensors.  New  content  in  this  article,  the  data  fusion  takes  place  on  scan  level  as  well  as  on  map  level  and  aims  to  result  in  an  optimum  map  quality  considering  the  visibility  situation.  Additionally,  we  collected  more  data  during  an  indoor  experiment  involving  real  fog  (see  Fig.  1).  Besides  the  structure  of  the  environment,  we  can  model  aerosol  concentration  with  fused  LiDAR  and  Radar  data  in  parallel  to  the  mapping  process  with  a  finite  difference  model  without  involving  a  smoke  or  gas  sensor.  Overall,  our  method  allows  the  modeling  of  the  structure  of  an  environment  including  dynamic  distribution  of  aerosol  concentration.
0	On  impact  decoupling  properties  of  elastic  robots  and  time  optimal  velocity  maximization  on  joint  level.  Designing  intrinsically  elastic  robot  systems,  making  systematic  use  of  their  properties  in  terms  of  impact  decoupling,  and  exploiting  temporary  energy  storage  and  release  during  excitative  motions  is  becoming  an  important  topic  in  nowadays  robot  design  and  control.  In  this  paper  we  treat  two  distinct  questions  that  are  of  primary  interest  in  this  context.  First,  we  elaborate  an  accurate  estimation  of  the  maximum  contact  force  during  simplified  human/obstacle-robot  collisions  and  how  the  relation  between  reflected  joint  stiffness,  link  inertia,  human/obstacle  stiffness,  and  human/obstacle  inertia  affect  it.  Overall,  our  analysis  provides  a  safety  oriented  methodology  for  designing  intrinsically  elastic  joints  and  clearly  defines  how  its  basic  mechanical  properties  influence  the  overall  collision  behavior.  This  can  be  used  for  designing  safer  and  more  robust  robots.  Secondly,  we  provide  a  closed  form  solution  of  reaching  maximum  link  side  velocity  in  minimum  time  with  an  intrinsically  elastic  joint,  while  keeping  the  maximum  deflection  constraint.  This  gives  an  analytical  tool  for  determining  suitable  stiffness  and  maximum  deflection  values  in  order  to  be  able  to  execute  desired  optimal  excitation  trajectories  for  explosive  motions.
0	Zero  moment  point  based  balance  control  of  leg  wheel  hybrid  structures  with  inequality  constraints  of  kinodynamic  behavior.  This  paper  discusses  a  balance  control  method  with  kinodynamic  constraints  for  leg-wheel  hybrid  structures  in  an  effort  to  improve  the  mobility  of  locomotion  over  hard,  flat  surfaces.  Preliminarily,  we  defined  a  prioritized  Jacobian  and  a  prioritized  inverse  of  Jacobian  to  formulate  the  dynamically  decoupled  model  in  the  task  space  for  the  constrained  multi-contact  multi-rigid-body  system  with  a  floating  base.  Our  strategy  has  two  tracks  to  accommodate  the  uncertainty  and  the  complexity  of  the  system  dynamics.  1)  The  time-delay  estimation  and  control  are  combined  with  the  nonlinear  programming.  2)  Whole  kinodynamic  constraints  are  derived  as  functions  of  the  control  input.  The  proposed  balance  control  algorithm  allows  the  system  to  traverse  desired  trajectories  satisfying  the  kinodynamic  constraints  and  improves  the  mobility  of  locomotion.  The  effectiveness  of  the  algorithm  is  tested  with  the  dynamic  simulations.
0	Real  time  convolutional  networks  for  depth  based  human  pose  estimation.  We  propose  to  combine  recent  Convolutional  Neural  Networks  (CNN)  models  with  depth  imaging  to  obtain  a  reliable  and  fast  multi-person  pose  estimation  algorithm  applicable  to  Human  Robot  Interaction  (HRI)  scenarios.  Our  hypothesis  is  that  depth  images  contain  less  structures  and  are  easier  to  process  than  RGB  images  while  keeping  the  required  information  for  human  detection  and  pose  inference,  thus  allowing  the  use  of  simpler  networks  for  the  task.  Our  contributions  are  threefold.  (i)  we  propose  a  fast  and  efficient  network  based  on  residual  blocks  (called  RPM)  for  body  landmark  localization  from  depth  images;  (ii)  we  created  a  public  dataset  DIH  comprising  more  than  170k  synthetic  images  of  human  bodies  with  various  shapes  and  viewpoints  as  well  as  real  (annotated)  data  for  evaluation;  (iii)  we  show  that  our  model  trained  on  synthetic  data  from  scratch  can  perform  well  on  real  data,  obtaining  similar  results  to  larger  models  initialized  with  pre-trained  networks.  It  thus  provides  a  good  trade-off  between  performance  and  computation.  Experiments  on  real  data  demonstrate  the  validity  of  our  approach.
0	A  4  point  algorithm  for  relative  pose  estimation  of  a  calibrated  camera  with  a  known  relative  rotation  angle.  We  propose  an  algorithm  to  estimate  the  relative  camera  pose  using  four  feature  correspondences  and  one  relative  rotation  angle  measurement.  The  algorithm  can  be  used  for  relative  pose  estimation  of  a  rigid  body  equipped  with  a  camera  and  a  relative  rotation  angle  sensor  which  can  be  either  an  odometer,  an  IMU  or  a  GPS/INS  system.  This  algorithm  exploits  the  fact  that  the  relative  rotation  angles  of  both  the  camera  and  relative  rotation  angle  sensor  are  the  same  as  the  camera  and  sensor  are  rigidly  mounted  to  a  rigid  body.  Therefore,  knowledge  of  the  extrinsic  calibration  between  the  camera  and  sensor  is  not  required.  We  carry  out  a  quantitative  comparison  of  our  algorithm  with  the  well-known  5-point  and  1-point  algorithms,  and  show  that  our  algorithm  exhibits  the  highest  level  of  accuracy.
0	Sustainable  robot  foraging  adaptive  fine  grained  multi  robot  task  allocation  for  maximum  sustainable  yield  of  biological  resources.  We  introduce  the  concept  of  Maximum  Sustainable  Yield  (MSY)  to  the  context  of  autonomous  robot  foraging.  MSY  is  an  optimal  approach  to  the  problem  of  maximizing  sustainable  foraging  where  the  resources  harvested  are  replenished  by  logistic  growth,  e.g.  living  things.  Over-harvesting  reduces  both  the  instantaneous  resource  availability  and  growth  rate,  and  above  some  threshold  will  permanently  deplete  resources.  Under-harvesting  is  sustainable,  but  fails  to  maximally  exploit  the  resources.  We  describe  a  system  model  and  use  it  to  determine  the  optimal  allocation  of  robot  work  to  resource-producing  `patches'.  We  give  a  practical  illustration  of  a  troublesome  feature  of  MSY:  it  is  too  sensitive  for  a  fixed  allocation  to  be  sustainable  in  practice.  We  show  how  to  centrally  allocate  a  number  of  robots  to  each  patch,  and  then  locally  adapt  the  work  rate  of  each  robot  to  achieve  sustainable  and  near-optimal  foraging.  This  is  the  first  study  of  robot  foraging  where  the  robots'  activity  modifies  the  productivity  and  sustainability  of  the  environment.
0	Towards  robotic  calligraphy.  Although  thousands  of  Chinese  characters  exist,  they  can  be  constructed  from  a  limited  number  of  single  strokes.  In  Chinese  calligraphy  these  strokes  are  combined  into  a  full  character  in  a  fluid  way.  Therefore  Chinese  calligraphy  provides  an  interesting  problem  to  study  learning  mechanisms  such  as  how  to  automatically  construct  complex  tasks  (full  characters)  from  previously  learned  simpler  ones  (single  strokes)  (Fig.  1).  The  goal  of  this  project  is  that  a  robot  should  be  able  to  decide  which  previously  learned  strokes  or  characters  to  use  for  drawing  a  newly  presented  character  and  to  improve  its  drawing  over  several  iterations.
0	Sensing  the  motion  of  bellows  through  changes  in  mutual  inductance.  Bellows-like  actuators  are  popular  in  soft  robotic  systems.  Sensing  the  movement  of  these  actuators  with  traditional  sensors  is  challenging.  This  work  proposes  and  tests  a  sensing  system  based  on  the  changing  mutual  inductance  of  wire  coils  on  bellows.  A  method  for  modeling  the  changes  in  mutual  inductance  between  coils  of  tightly-packed  wires  is  introduced.  Changes  in  mutual  inductance  are  measured  through  the  self-inductance  of  a  circuit  made  up  of  the  coils  in  series.  The  self-inductance  of  the  circuit  measures  the  bellows  bend-angle.  The  experiments  show  an  approximately  quadratic  relationship  between  the  bend  angle  and  the  measured  inductance.  From  a  bend  angle  of  121  °  to  −67  °  the  inductance  of  the  circuit  increases  by  19  %.  The  bias-inducing  effects  of  shear  strain,  torsional  strain,  non-uniform  bending,  and  nearby  metal  are  also  explored.
0	Design  modeling  and  control  of  a  novel  amphibious  robot  with  dual  swing  legs  propulsion  mechanism.  This  paper  describes  a  novel  amphibious  robot,  which  adopts  a  dual-swing-legs  propulsion  mechanism,  proposing  a  new  locomotion  mode.  The  robot  is  called  FroBot,  since  its  structure  and  locomotion  are  similar  to  frogs.  Our  inspiration  comes  from  the  frog  scooter  and  breaststroke.  Based  on  its  swing  leg  mechanism,  an  unusual  universal  wheel  structure  is  used  to  generate  propulsion  on  land,  while  a  pair  of  flexible  caudal  fins  functions  like  the  foot  flippers  of  a  frog  to  generate  similar  propulsion  underwater.  On  the  basis  of  the  prototype  design  and  the  dynamic  model  of  the  robot,  some  locomotion  control  simulations  and  experiments  were  conducted  for  the  purpose  of  adjusting  the  parameters  that  affect  the  propulsion  of  the  robot.  Finally,  a  series  of  underwater  experiments  were  performed  to  verify  the  design  feasibility  of  FroBot  and  the  rationality  of  the  control  algorithm.
0	Multi  resolution  h  cost  motion  planning  a  new  framework  for  hierarchical  motion  planning  for  autonomous  mobile  vehicles.  This  paper  summarizes  some  recent  developments  on  a  new  motion  planning  framework  for  autonomous  vehicles.  The  main  novelties  of  the  current  work  include:  a  provably  complete  multi-resolution  path  planning  scheme  using  wavelet-based  workspace  cell  decompositions;  a  general  technique  for  incorporating  vehicle  dynamic  constraints  in  the  geometric  path  planner;  and  a  local  trajectory  generation  scheme  based  on  model  predictive  control.
0	Full  steam  ahead  exactly  sparse  gaussian  process  regression  for  batch  continuous  time  trajectory  estimation  on  se  3.  This  paper  shows  how  to  carry  out  batch  continuous-time  trajectory  estimation  for  bodies  translating  and  rotating  in  three-dimensional  (3D)  space,  using  a  very  efficient  form  of  Gaussian-process  (GP)  regression.  The  method  is  fast,  singularity-free,  uses  a  physically  motivated  prior  (the  mean  is  constant  body-centric  velocity),  and  permits  trajectory  queries  at  arbitrary  times  through  GP  interpolation.  Landmark  estimation  can  be  folded  in  to  allow  for  simultaneous  trajectory  estimation  and  mapping  (STEAM),  a  variant  of  SLAM.
0	The  effect  of  spine  morphology  on  rapid  acceleration  in  quadruped  robots.  An  actuated  spine  appears  to  be  a  critical  component  for  maneuverability  in  quadruped  animals.  However,  robotic  systems  have  yet  to  capitalize  on  this  mechanism.  This  research  compares  three  different  spine  morphologies  in  the  planar  case,  namely  the  rigid,  revolute  and  prismatic  spine.  Using  a  wide  range  of  robots  sampled  from  the  design  space  (200  robots  sampled  at  random),  large-scale  trajectory  optimization  (60  seed  points  per  robot  per  spine  morphology)  was  used  to  determine  the  best  spine  morphology  in  terms  of  stride  averaged  acceleration.  Bootstrapping  was  performed  on  the  results  to  achieve  a  better  statistical  representation  and  this  revealed  that  for  75%  of  the  robots,  a  prismatic  spine  design  is  the  most  effective  at  rapid  acceleration,  followed  by  the  revolute  spine  at  6%  and  rigid  spine  at  18%.
0	Impedance  based  contact  control  of  a  free  flying  space  robot  with  a  compliant  wrist  for  non  cooperative  satellite  capture.  This  paper  presents  the  impedance-based  contact  control  of  a  free-flying  space  robot  utilizing  a  compliant  wrist  for  non-cooperative  satellite  capture  operation.  An  open  loop  impedance  control  law  based  on  contact  dynamics  model  is  introduced  to  realize  a  desired  coefficient  of  restitution  defined  between  a  manipulator  hand  of  a  space  robot  and  a  contact  point  on  a  free-flying  target.  The  coefficient  of  restitution  and  the  damping  ratio  are  expressed  as  a  function  of  contact  and  impedance  parameters;  and  hence,  the  impedance  parameters  are  tuned  by  setting  a  desired  coefficient  of  restitution  and  a  desired  damping  ratio.  The  collision  experiment  using  twodimensional  microgravity  emulator,  called  air-floating  test  bed,  verifies  that  the  proposed  open  loop  control  law  is  capable  of  realizing  a  desired  coefficient  of  restitution  with  fairly  small  errors.
0	Partially  shared  deep  neural  network  in  sound  source  separation  and  identification  using  a  uav  embedded  microphone  array.  This  paper  addresses  sound  source  separation  and  identification  for  noise-contaminated  acoustic  signals  recorded  with  a  microphone  array  embedded  in  an  Unmanned  Aerial  Vehicle  (UAV),  aiming  at  people's  voice  detection  quickly  and  widely  in  a  disaster  situation.  The  key  approach  to  achieve  this  is  Deep  Neural  Network  (DNN),  but  it  is  well  known  that  training  a  DNN  needs  a  huge  dataset  to  improve  its  performance.  In  a  practical  application,  building  such  a  dataset  is  not  often  realistic  owing  to  the  cost  of  manual  data  annotation.  Therefore,  we  propose  a  Partially-Shared  Deep  Neural  Network  (PS-DNN)  which  can  learn  multiple  tasks  at  the  same  time  with  a  small  amount  of  annotated  data.  Preliminary  results  show  that  the  PS-DNN  outperforms  conventional  DNN-based  approaches  which  require  fully-annotated  data  in  training  in  terms  of  identification  accuracy.  In  addition,  it  maintains  performance  even  when  noise-suppressed  signals  are  used  for  sound  source  separation  training,  and  partially  annotated  data  is  used  for  sound  source  identification  training.
0	Fast  and  robust  3  d  sound  source  localization  with  dsvd  phat.  This  paper  introduces  a  variant  of  the  Singular  Value  Decomposition  with  Phase  Transform  (SVD-PHAT),  named  Difference  SVD-PHAT  (DSVD-PHAT),  to  achieve  robust  Sound  Source  Localization  (SSL)  in  noisy  conditions.  Experiments  are  performed  on  a  Baxter  robot  with  a  four-microphone  planar  array  mounted  on  its  head.  Results  show  that  this  method  offers  similar  robustness  to  noise  as  the  state-of-the-art  Multiple  Signal  Classification  based  on  Generalized  Singular  Value  Decomposition  (GSVD-MUSIC)  method,  and  considerably  reduces  the  computational  load  by  a  factor  of  250.  This  performance  gain  thus  makes  DSVD-PHAT  appealing  for  real-time  application  on  robots  with  limited  on-board  computing  power.
0	Optimizing  decisions  using  knowledge  risk  strategy.  Purpose          The  paper  aims  to  focus  on  a  strategic  approach  for  making  trade-offs  between  knowledge  and  risk.          Design/methodology/approach          Knowledge  and  risk  are  viewed  as  organizational  resources  that  have  an  inherent  trade-off  between  them,  so  that  optimal  firm  performance  does  not  necessarily  arise  through  greater  accumulation  of  knowledge  nor  from  reduced  risk.  This  trade-off  is  represented  as  an  efficient  knowledge-risk  frontier.  The  paper  examines  the  dynamics  of  this  frontier  on  organizational  performance.          Findings          The  concept  of  knowledge-risk  strategy  is  presented  which  contends  that  non-probabilistic  risk  or  uncertainty  originates  from  gaps  in  knowledge.          Research  limitations  implications          The  paper  proposes  a  new  line  of  research  to  understand  decision-making  in  organizations,  particularly  those  which  focus  on  knowledge  intensive  products  and  services.          Practical  implications          The  paper  proposes  managerial  approaches  to  improve  organizational  positioning  relative  to  the  efficient  knowledge-risk  frontier  through  greater  awareness  of  contributors  to  knowledge  gaps  and  risk  in  decision  situations,  as  well  as  traditional  strategic  tools  such  as  outsourcing.          Originality/value          The  postulated  link  between  risk  and  knowledge  gaps  establishes  a  knowledge-based  view  of  firm  risk  and  recognizes  trade-offs  for  decisions  regarding  knowledge  accumulation.
0	Experience  as  a  source  of  knowledge  in  divestiture  decisions  emerging  issues  and  knowledge  management  implications.  Purpose          This  paper  aims  to  analyse  the  idea  that  experience  acts  as  an  antecedent  in  divestiture  and  triggers  an  organisational  learning  process  that  enables  the  divesting  firm  to  convert  experience  into  knowledge,  increasing  the  probability  that  a  firm  will  undertake  subsequent  divestitures.          Design/methodology/approach          The  approach  is  quantitative.  The  research  project  used  a  case–control  design,  with  a  sample  consisting  of  274  divesting  and  non-divesting  firms.  Given  the  dichotomous  nature  of  the  dependent  variable,  the  relations  of  the  research  model  are  tested  using  logistic  regression.          Findings          The  likelihood  of  a  divestiture  increases  when  firms  have  already  had  past  experience  of  divestitures.  Firm  performance  and  firm  size  act  as  moderating  variables,  that  is,  the  learning  effects  are  weaker  in  firms  with  better  past  performance  and  also  in  larger  firms.          Research  limitations/implications          The  study  contributes  to  the  literature  on  organisational  learning  and  divestiture.  In  particular,  the  knowledge  obtained  from  previous  divestitures  is  positively  related  to  subsequent  ones.  The  results  on  firm  size  and  performance  as  contingency  factors  make  it  possible  to  distinguish  between  the  different  learning  mechanisms  in  proactive  and  reactive  divestitures,  as  well  as  in  larger  and  smaller  firms.  Accordingly,  a  two-level  framework  of  experience  and  knowledge  is  proposed.          Practical  implications          The  results  are  of  interest  for  practitioners  who  need  a  better  understanding  of  the  antecedents  of  their  strategic  actions  in  terms  of  past  experience  and  knowledge.  The  study  also  offers  insights  into  the  knowledge  management  practices  that  fit  into  the  proposed  two-level  framework  of  knowledge  accumulation.          Originality/value          The  originality  of  the  study  consists  in  the  strong  evidence  of  learning  effects  in  divestitures  that  it  finds.  This  study  augments  a  promising  line  of  research  on  the  effect  of  experience  in  rare  strategic  decisions,  enriching  our  understanding  of  the  learning  mechanisms  associated  with  complex  experiences.
0	Classification  of  supply  chain  knowledge  a  morphological  approach.  Purpose  –  The  purpose  of  the  article  is  to  create  a  knowledge  classification  model  that  can  be  used  by  knowledge  management  (KM)  practitioners  for  establishing  a  knowledge  management  framework  (KMF)  in  a  supply  chain  (SC)  network.  Epistemological  and  ontological  aspects  of  knowledge  have  been  examined.  SC  networks  provide  a  more  generic  setting  for  managing  knowledge  due  to  the  additional  issues  concerning  flow  of  knowledge  across  the  boundaries  of  organizations.  Design/methodology/approach  –  Morphological  analysis  has  been  used  to  build  the  knowledge  classification  model.  Morphological  approach  is  particularly  useful  in  exploratory  research  on  concepts/entities  having  multiple  dimensions.  Knowledge  itself  has  been  shown  in  literature  to  have  many  characteristics,  and  the  methodology  used  has  enabled  a  comprehensive  classification  scheme  based  on  such  characteristics.  Findings  –  A  single  comprehensive  classification  model  for  knowledge  that  exists  in  SC  networks  has  been  proposed.  Nine  characteristics,  ea...
0	Active  localization  with  dynamic  obstacles.  This  paper  addresses  the  problem  of  robot  global  localization  in  a  known  environment,  in  the  presence  of  many  dynamic  obstacles.  Deploying  a  robot  in  crowded  spaces  such  as  museums,  shopping  malls,  department  stores,  or  university  campuses  is  especially  challenging  because  the  moving  people  occlude  the  static  parts  of  the  environment,  such  as  walls  and  doorways,  making  the  robot  essentially  blind.  A  new  weighting  function  is  proposed  for  a  particle  filter  state  estimation  algorithm  that  accounts  for  the  presence  of  dynamic  obstacles  and  avoids  population  depletion.  An  active  localization  strategy  is  employed  which  guides  the  robot  to  locations  that  resolve  ambiguities  and  eliminate  hypotheses  in  a  systematic  manner.  Experimental  results  from  multiple  simulations  and  from  real  robot  deployments  validate  the  localization  improvements  achieved  by  the  proposed  method.
0	Anticipating  human  activities  for  reactive  robotic  response.  An  important  aspect  of  human  perception  is  anticipation,  which  we  use  extensively  in  our  day-to-day  activities  when  interacting  with  other  humans  as  well  as  with  our  surroundings.  Anticipating  which  activities  will  a  human  do  next  (and  how  to  do  them)  can  enable  an  assistive  robot  to  plan  ahead  for  reactive  responses  in  the  human  environments.  In  this  work,  our  goal  is  to  enable  robots  to  predict  the  future  activities  as  well  as  the  details  of  how  a  human  is  going  to  perform  them  in  short-term  (e.g.,  1-10  seconds).  For  example,  if  a  robot  has  seen  a  person  move  his  hand  to  a  coffee  mug,  it  is  possible  he  would  move  the  coffee  mug  to  a  few  potential  places  such  as  his  mouth,  to  a  kitchen  sink  or  just  move  it  to  a  different  location  on  the  table.  If  a  robot  can  anticipate  this,  then  it  would  rather  not  start  pouring  milk  into  the  coffee  when  the  person  is  moving  his  hand  towards  the  mug,  thus  avoiding  a  spill.  We  represent  each  possible  future  using  an  anticipatory  temporal  conditional  random  field  (ATCRF)  that  models  the  rich  spatial-temporal  relations  through  object  affordances.  We  then  consider  each  ATCRF  as  a  particle  and  represent  the  distribution  over  the  potential  futures  using  a  set  of  particles.  We  evaluate  our  anticipation  approach  extensively  on  CAD-120  human  activity  dataset,  which  contains  120  RGB-D  videos  of  daily  human  activities,  such  as  microwaving  food,  taking  medicine,  etc.  For  robotic  evaluation,  we  measure  how  many  times  the  robot  anticipates  and  performs  the  correct  reactive  response.  The  accompanying  video  shows  a  PR2  robot  performing  assistive  tasks  based  on  the  anticipations  generated  by  our  proposed  method.
0	Why  did  the  robot  cross  the  road  learning  from  multi  modal  sensor  data  for  autonomous  road  crossing.  We  consider  the  problem  of  developing  robots  that  navigate  like  pedestrians  on  sidewalks  through  city  centers  for  performing  various  tasks  including  delivery  and  surveillance.  One  particular  challenge  for  such  robots  is  crossing  streets  without  pedestrian  traffic  lights.  To  solve  this  task  the  robot  has  to  decide  based  on  its  sensory  input  if  the  road  is  clear.  In  this  work,  we  propose  a  novel  multi-modal  learning  approach  for  the  problem  of  autonomous  street  crossing.  Our  approach  solely  relies  on  laser  and  radar  data  and  learns  a  classifier  based  on  Random  Forests  to  predict  when  it  is  safe  to  cross  the  road.  We  present  extensive  experimental  evaluations  using  real-world  data  collected  from  multiple  street  crossing  situations  which  demonstrate  that  our  approach  yields  a  safe  and  accurate  street  crossing  behavior  and  generalizes  well  over  different  types  of  situations.  A  comparison  to  alternative  methods  demonstrates  the  advantages  of  our  approach.
0	Autonomous  question  answering  with  mobile  robots  in  human  populated  environments.  Autonomous  mobile  robots  will  soon  become  ubiquitous  in  human-populated  environments.  Besides  their  typical  applications  in  fetching,  delivery,  or  escorting,  such  robots  present  the  opportunity  to  assist  human  users  in  their  daily  tasks  by  gathering  and  reporting  up-to-date  knowledge  about  the  environment.  In  this  paper,  we  explore  this  use  case  and  present  an  end-to-end  framework  that  enables  a  mobile  robot  to  answer  natural  language  questions  about  the  state  of  a  large-scale,  dynamic  environment  asked  by  the  inhabitants  of  that  environment.  The  system  parses  the  question  and  estimates  an  initial  viewpoint  that  is  likely  to  contain  information  for  answering  the  question  based  on  prior  environment  knowledge.  Then,  it  autonomously  navigates  towards  the  viewpoint  while  dynamically  adapting  to  changes  and  new  information.  The  output  of  the  system  is  an  image  of  the  most  relevant  part  of  the  environment  that  allows  the  user  to  obtain  an  answer  to  their  question.  We  additionally  demonstrate  the  benefits  of  a  continuously  operating  information  gathering  robot  by  showing  how  the  system  can  answer  retrospective  questions  about  the  past  state  of  the  world  using  incidentally  recorded  sensory  data.  We  evaluate  our  approach  with  a  custom  mobile  robot  deployed  in  a  university  building,  with  questions  collected  from  occupants  of  the  building.  We  demonstrate  our  system's  ability  to  respond  to  these  questions  in  different  environmental  conditions.
0	Medical  applicability  of  a  low  cost  industrial  robot  arm  guided  with  an  optical  tracking  system.  Robot  systems  used  in  surgical  procedures  can  autonomously  position  tools  at  points  correlated  with  preoperative  imaging  techniques  such  as  magnetic  resonance  (MR)  and  computed  tomography  (CT).  The  aim  of  this  paper  is  to  measure  and  assess  medical  applicability  of  a  low-cost,  lightweight  industrial  robot  arm  (Universal  robot  UR5)  guided  with  the  medically  certified  optical  tracking  system  (Polaris  Vicra)  to  positions  registered  from  a  CT  scan.  Technical  setup,  measurement  equipment,  device  communication  and  robot  control  based  on  OTS  feedback  are  described.  Robot  intrinsic  accuracy,  CT  scan  accuracy  and  two  methods  of  robot  tool  positioning  with  aid  of  the  optical  tracking  system  (OTS)  are  measured.  Measurements  show  RMS  error  of  the  robot  (0.669  mm)  is  decreased  55.4%  when  guided  with  OTS  using  a  single  marker  probe  (0.29  mm)  and  40.5%  when  using  OTS  with  relative  referencing  (0.39  mm).  RMS  error  of  the  CT  scan  readings  is  0.46  mm.
0	Sensor  fusion  based  human  detection  and  tracking  system  for  human  robot  interaction.  Service  robot  has  received  enormous  attention  with  rapid  development  of  advanced  technology  in  recent  years,  and  it  is  endowed  with  the  capabilities  of  performing  human-robot  interaction  (HRI).  We  construct  a  sensor  fusion  based  system  to  integrate  the  information  from  both  sensors  by  using  a  data  association  approach  -  Covariance  Intersection  (CI).  It  will  be  used  to  increase  the  robustness  and  reliability  of  HRI  in  the  real  world  environment.  In  this  paper,  we  propose  a  Behavior  System  for  analyzing  human  features  and  classifying  the  behavior  by  the  crucial  information  from  sensor  fusion  system.  The  system  is  used  to  infer  the  human  behavioral  intentions,  and  also  allow  the  robot  to  perform  more  natural  and  intelligent  interaction.  We  apply  a  spatial  model  based  on  proxemics  rules  to  our  robot,  and  design  a  behavioral  intention  inference  strategy.  Furthermore,  the  robot  will  make  the  corresponding  reaction  in  accordance  with  the  identified  behavioral  intention.
0	Augmenting  physical  simulators  with  stochastic  neural  networks  case  study  of  planar  pushing  and  bouncing.  An  efficient,  generalizable  physical  simulator  with  universal  uncertainty  estimates  has  wide  applications  in  robot  state  estimation,  planning,  and  control.  In  this  paper,  we  build  such  a  simulator  for  two  scenarios,  planar  pushing  and  ball  bouncing,  by  augmenting  an  analytical  rigid-body  simulator  with  a  neural  network  that  learns  to  model  uncertainty  as  residuals.  Combining  symbolic,  deterministic  simulators  with  learnable,  stochastic  neural  nets  provides  us  with  expressiveness,  efficiency,  and  generalizability  simultaneously.  Our  model  outperforms  both  purely  analytical  and  purely  learned  simulators  consistently  on  real,  standard  benchmarks.  Compared  with  methods  that  model  uncertainty  using  Gaussian  processes,  our  model  runs  much  faster,  generalizes  better  to  new  object  shapes,  and  is  able  to  characterize  the  complex  distribution  of  object  trajectories.
0	Characterization  of  active  passive  pneumatic  actuators  for  assistive  devices.  Assistive  devices  have  been  developed  for  power  augmentation  and  task-oriented  assistance  such  as  loaded  walking.  The  effective  joint  dynamics  of  the  user  can  be  altered  using  a  wearable  system,  providing  assistance  when  a  task  is  performed.  The  authors  have  investigated  an  Active/Passive  Pneumatic  Actuator  (AP2A)  for  an  assistive  device,  which  has  a  simple  structure  and  responds  as  a  passive  nonlinear  spring  with  controllable  stiffness.  This  paper  introduces  a  novel  controller  for  the  AP2  $A$  and  validates  the  performance  through  experiments.  The  developed  controller  is  found  to  stabilize  at  the  desired  stiffness  response  within  1  second,  confirming  the  ability  of  the  AP2  $A$  to  act  as  an  adjustable  passive  nonlinear  spring.
0	Intuitive  gaze  control  of  a  robotized  flexible  endoscope.  Flexible  endoscopy  is  a  routinely  performed  procedure  that  has  predominantly  remained  unchanged  for  decades  despite  its  many  challenges.  This  paper  introduces  a  novel,  more  intuitive  and  ergonomic  platform  that  can  be  used  with  any  flexible  endoscope,  allowing  easier  navigation  and  manipulation.  A  standard  endoscope  is  robotized  and  a  gaze  control  system  based  on  eye-tracking  is  developed  and  implemented,  allowing  hands-free  manipulation.  The  system  characteristics  and  step  response  has  been  evaluated  using  visual  servoing.  Further,  the  robotized  system  has  been  compared  with  a  manually  controlled  endoscope  during  a  user  study.  The  users  $(\mathbf{n}=11)$  showed  a  preference  for  the  gaze  controlled  endoscope  and  a  lower  task  load  when  the  task  was  performed  with  the  gaze  control.  In  addition,  gaze  control  was  related  to  a  higher  success  rate  and  a  lower  time  to  perform  the  task.  The  results  presented  validate  the  system's  technical  performance  and  demonstrate  the  intuitiveness  of  hands-free  gaze  control  in  flexible  endoscopy.
0	Proactive  kinodynamic  planning  using  the  extended  social  force  model  and  human  motion  prediction  in  urban  environments.  This  paper  presents  a  novel  approach  for  robot  navigation  in  crowded  urban  environments  where  people  and  objects  are  moving  simultaneously  while  a  robot  is  navigating.  Avoiding  moving  obstacles  at  their  corresponding  precise  moment  motivates  the  use  of  a  robotic  planner  satisfying  both  dynamic  and  nonholonomic  constraints,  also  referred  as  kynodynamic  constraints.  We  present  a  proactive  navigation  approach  with  respect  its  environment,  in  the  sense  that  the  robot  calculates  the  reaction  produced  by  its  actions  and  provides  the  minimum  impact  on  nearby  pedestrians.  As  a  consequence,  the  proposed  planner  integrates  seamlessly  planning  and  prediction  and  calculates  a  complete  motion  prediction  of  the  scene  for  each  robot  propagation.  Making  use  of  the  Extended  Social  Force  Model  (ESFM)  allows  an  enormous  simplification  for  both  the  prediction  model  and  the  planning  system  under  differential  constraints.  Simulations  and  real  experiments  have  been  carried  out  to  demonstrate  the  success  of  the  proactive  kinodynamic  planner.
0	Cinemairsim  a  camera  realistic  robotics  simulator  for  cinematographic  purposes.  Unmanned  Aerial  Vehicles  (UAVs)  are  becoming  increasingly  popular  in  the  film  and  entertainment  industries,  in  part  because  of  their  maneuverability  and  perspectives  they  enable.  While  there  exists  methods  for  controlling  the  position  and  orientation  of  the  drones  for  visibility,  other  artistic  elements  of  the  filming  process,  such  as  focal  blur,  remain  unexplored  in  the  robotics  community.  The  lack  of  cinematographic  robotics  solutions  is  partly  due  to  the  cost  associated  with  the  cameras  and  devices  used  in  the  filming  industry,  but  also  because  state-of-the-art  photo-realistic  robotics  simulators  only  utilize  a  full  in-focus  pinhole  camera  model  which  does  not  incorporate  these  desired  artistic  attributes.  To  overcome  this,  the  main  contribution  of  this  work  is  to  endow  the  well-known  drone  simulator,  AirSim,  with  a  cinematic  camera  as  well  as  extend  its  API  to  control  all  of  its  parameters  in  real  time,  including  various  filming  lenses  and  common  cinematographic  properties.  In  this  paper,  we  detail  the  implementation  of  our  AirSim  modification,  CinemAirSim,  present  examples  that  illustrate  the  potential  of  the  new  tool,  and  highlight  the  new  research  opportunities  that  the  use  of  cinematic  cameras  can  bring  to  research  in  robotics  and  control.
0	A  fast  motion  planning  algorithm  for  car  parking  based  on  static  optimization.  This  paper  presents  a  fast  optimization  based  algorithm  for  car  parking.  The  challenge  arises  from  the  non-holonomic  characteristics  of  the  car  and  the  close  distance  to  the  obstacles.  The  presented  approach  utilizes  the  Minkowski  sum  to  account  for  obstacle  avoidance.  The  geometric  path  planning  problem  is  decoupled  from  the  kinematic  problem  and  discretized  with  respect  to  the  path  parameter  by  means  of  a  Runge-Kutta  discretization.  For  the  discrete  path  segments,  an  optimization  problem  is  formulated  to  calculate  the  path  independent  of  the  parking  scenario.  This  static  optimization  problem  can  be  solved  numerically  in  a  very  efficient  way.  The  performance  of  the  algorithm  is  evaluated  in  several  simulation  scenarios.
0	Biologically  inspired  dead  beat  controller  for  bipedal  running  in  3d.  This  paper  introduces  a  Biologically  Inspired  Dead-beat  (BID)  controller  for  bipedal  running  in  3D.  The  controller  runs  in  real-time,  is  extremely  robust  against  perturbations  and  allows  for  versatile  running  patterns.  It  is  based  on  the  encoding  of  leg  forces  and  CoM  trajectories  during  stance  as  polynomial  splines,  allowing  for  intuitive  and  primarily  analytical  controller  design.  The  performance  of  the  control  framework  is  tested  in  various  simulations  for  a  bipedal  point-mass  model.
0	Adaptive  look  ahead  for  robotic  navigation  in  unknown  environments.  Receding  horizon  control  strategies  have  proven  effective  in  many  control  and  robotic  applications.  These  methods  simulate  the  state  a  certain  time  horizon  into  the  future  to  choose  the  optimal  trajectory.  However,  in  many  cases,  such  as  in  mobile  robot  navigation,  the  selection  of  an  appropriate  time  horizon  is  important  as  too  long  of  a  time  horizon  can  amplify  the  detrimental  effects  caused  by  environmental  uncertainties  in  the  prediction  of  the  future  state  while  too  of  a  short  horizon  will  lead  to  reduced  performance.  In  this  paper  we  strike  a  balance  between  these  two  conflicting  objectives  by  first  introducing  a  receding  horizon  method  for  navigation  founded  on  schema-based  behaviors.  We  then  suggest  a  method  of  adapting  the  time  horizon  by  minimizing  a  cost  function  which  balances  the  performance  of  the  underlying  control  problem  (which  prefers  longer  horizons)  with  the  performance  of  our  state  prediction  (which  prefers  shorter  time  horizons).  We  illustrate  the  operation  with  an  example  which  shows  the  usefulness  of  our  navigation  scheme  with  an  adaptive  time  horizon.
0	Target  localization  and  circumnavigation  by  a  non  holonomic  robot.  This  paper  addresses  a  surveillance  problem  in  which  the  goal  is  to  achieve  a  circular  motion  around  a  target  by  a  non-holonomic  agent.  The  agent  only  knows  its  own  position  with  respect  to  its  initial  frame,  and  the  bearing  angle  of  the  target  in  that  frame.  It  is  assumed  that  the  position  of  the  target  is  unknown.  An  estimator  and  a  controller  are  proposed  to  estimate  the  position  of  the  target  and  make  the  agent  move  on  a  circular  trajectory  with  a  desired  radius  around  it.  The  performance  of  the  proposed  algorithm  is  verified  both  through  simulations  and  experiments.  Robustness  is  also  established  in  the  face  of  noise  and  target  motion.
0	Magnetic  maps  of  indoor  environments  for  precise  localization  of  legged  and  non  legged  locomotion.  The  magnetic  field  in  indoor  environments  is  rich  in  features  and  exceptionally  easy  to  sense.  In  conjunction  with  a  suitable  form  of  odometry,  such  as  signals  produced  from  inertial  sensors  or  wheel  encoders,  a  map  of  this  field  can  be  used  to  precisely  localize  a  human  or  robot  in  an  indoor  environment.  We  show  how  the  use  of  this  field  yields  significant  improvements  in  terms  of  localization  accuracy  for  both  legged  and  non-legged  locomotion.  We  suggest  various  likelihood  functions  for  sequential  Monte  Carlo  localization  and  evaluate  their  performance  based  on  magnetic  maps  of  different  resolutions.  Specifically,  we  investigate  the  influence  that  measurement  representation  (e.g.,  intensity-based,  vector-based)  and  map  resolution  have  on  localization  accuracy,  robustness,  and  complexity.  Compared  to  other  localization  approaches  (e.g.,  camera-based,  LIDAR-based),  there  exist  far  fever  privacy  concerns  when  sensing  the  indoor  environment's  magnetic  field.  Furthermore,  the  required  sensors  are  less  costly,  compact,  and  have  a  lower  raw  data  rate  and  power  consumption.  The  combination  of  technical  and  privacy-related  advantages  makes  the  use  of  the  magnetic  field  a  very  viable  solution  to  indoor  navigation  for  both  humans  and  robots.
0	Visual  and  physical  segmentation  of  novel  objects.  The  ability  to  recognize  novel  items  in  complex  environment  is  fundamental  to  robot  intelligence  and  manipulation.  In  this  work,  we  consider  the  issue  of  segmenting  visual  and  physical  aspects  of  novel  objects  of  interest,  specifically  ones  that  are  being  seen  for  the  first  time  and  located  in  the  proximity  of  unidentified  obstacles.  Illumination  at  different  wavelengths  and  angles  are  projected  by  a  robot,  thus  acquiring  additional  information  about  the  scene  and  exploiting  it  for  successful  novel  objects  segmentation.  By  analyzing  shades  and  reflections  of  a  scene's  objects,  we  were  able  to  form  and  identify  true  edges  and  visually  separate  items  of  interest.  In  order  to  recognize  the  physical  characteristics  of  segmented  objects,  we  introduce  a  novel  material  classification  technique  which  utilizes  static  electricity  charge  sensing.  Our  proposed  methods  do  not  require  predefined  models  of  target  objects  and  assume  no  previously  assigned  targets'  pose.  Using  extracted  object  visual  and  physical  aspects,  an  autonomous  robot  manipulator  successfully  performed  a  trash  separation  task.
0	Path  planning  for  clothes  climbing  robots  on  deformable  clothes  surface.  This  paper  proposes  a  novel  path  planning  method  for  a  robot  to  climb  on  the  deformable  clothes  surface.  Based  on  the  deformable  characteristic  of  the  clothes,  the  tension  force  of  clothes  is  analyzed  and  the  model  of  tension  degree  is  established.  A  clothes  climbing  robot  called  Clothbot  is  composed  of  a  two-wheeled  gripper  and  a  2  Degrees  of  Freedom  (DOF)  tail.  Based  on  the  locomotion  of  this  robot,  the  weights  of  tension  degree  and  the  locomotion  characteristic  are  added  into  the  A*  algorithm.  Combined  with  the  two  weights  applied,  the  optimal  path  to  the  target  for  the  Clothbot  is  obtained.  The  Clothbot  has  been  developed  to  evaluate  the  algorithm.  The  simulation  and  the  experiments  have  verified  the  feasibility  of  this  method.  In  addition,  The  error  state  of  the  movement  of  the  robot  which  is  called  side  tumbling  has  been  corrected  by  the  motion  of  the  2-DOF  tail.
0	Learning  coordinated  vehicle  maneuver  motion  primitives  from  human  demonstration.  High-fidelity  computational  human  models  provide  a  safe  and  cost-efficient  method  for  studying  driver  experience  in  vehicle  maneuvers  and  for  validation  of  vehicle  design.  Compared  to  passive  human  models,  active  human  models  capable  of  reproducing  the  decision-making,  as  well  as  vehicle  maneuver  motion  planning  and  control,  will  be  able  to  support  realistic  simulation  of  human-vehicle  interaction.  In  this  paper,  we  propose  an  integrated  human-vehicle  interaction  simulation  framework  which  learns  vehicle  maneuver  motion  primitives  from  human  drivers,  and  uses  them  to  compose  natural  and  contextual  driving  motions.  Specifically,  we  recruited  six  experienced  drivers  and  recorded  their  vehicle  maneuver  motions  on  a  fixed-base  driving  simulation  testbed.  We  further  segmented  and  classified  the  collected  data  based  on  their  similarity  in  joint  coordination.  Using  a  combination  of  imitation  learning  methods,  we  extracted  the  regularity  and  variability  of  vehicle  maneuver  motions  across  subjects,  and  learned  the  dynamic  motion  primitives  to  be  used  for  motion  reproduction  in  simulation.  We  present  an  implementation  of  the  framework  on  lower-extremity  joint  coordination  in  pedal  activation  for  longitudinal  vehicle  control.  Our  research  efforts  lead  to  a  motion  primitive  library  which  enables  planning  natural  driver  motions,  and  will  be  integrated  with  the  driving  decision-making,  motion  control,  and  vehicle  dynamics  in  the  proposed  framework  for  simulating  human-vehicle  interaction.
0	Multicamera  3d  reconstruction  of  dynamic  surgical  cavities  non  rigid  registration  and  point  classification.  Deformable  objects  and  surfaces  are  ubiquitous  in  the  daily  lives  of  humans  –  from  the  garments  in  fashion  to  soft  tissues  within  the  body.  Because  of  this  routine  interaction  with  soft  materials,  humans  are  adept  and  trained  in  manipulation  of  deformable  objects  while  avoiding  irreversible  damage.  The  dexterity  and  care  involved  is  largely  facilitated  through  a  combination  of  the  human  haptic  sense  of  touch  and  visual  observations  of  object  deformation  [1].  While  this  scenario  presents  itself  as  a  trivially  intuitive  task,  it  becomes  significantly  more  difficult  and  complex  with  the  deprivation  of  both  3D  depth  perception  and  haptic  senses.  This  deprived  state  is  not  dissimilar  to  the  scenarios  encountered  in  many  robot-assisted  minimally  invasive  surgeries.  As  a  result,  unintentional  tissue  damage  can  occur  due  to  lack  of  force  feedback  and  fine  3D  visibility  [2].  One  approach  to  remediate  these  issues  combines  real-time  dynamic  3D  reconstruction  and  vision-based  force  estimation  for  haptic  feedback.  Toward  that  end,  this  work  continues  research  in  a  series  of  studies  focusing  on  multicamera  3D  reconstruction  of  dynamic  surgical  cavities.  Previous  work  introduced  a  novel  approach  of  camera  grouping  and  pair  sequencing  [3].  This  paper  builds  upon  that  work  by  introducing  a  method  for  non-rigid,  sparse  point  cloud  registration  and  subsequent  point  classification.  In  particular,  to  enable  deformation  and  force  analyses,  surfaces  are  locally  classified  into  three  categories:  static,  shifting  and  deforming.  The  topics  addressed  in  this  paper  present  open  challenges  and  ongoing  research  directions  for  researchers  to  this  day  [4],  and  provide  a  step  towards  real-time  3D  reconstruction  and  force  feedback  in  robot-assisted  surgery.
0	Meta  learning  for  multi  objective  reinforcement  learning.  Multi-objective  reinforcement  learning  (MORL)  is  the  generalization  of  standard  reinforcement  learning  (RL)  approaches  to  solve  sequential  decision  making  problems  that  consist  of  several,  possibly  conflicting,  objectives.  Generally,  in  such  formulations,  there  is  no  single  optimal  policy  which  optimizes  all  the  objectives  simultaneously,  and  instead,  a  number  of  policies  has  to  be  found  each  optimizing  a  preference  of  the  objectives.  In  this  paper,  we  introduce  a  novel  MORL  approach  by  training  a  meta-policy,  a  policy  simultaneously  trained  with  multiple  tasks  sampled  from  a  task  distribution,  for  a  number  of  randomly  sampled  Markov  decision  processes  (MDPs).  In  other  words,  the  MORL  is  framed  as  a  meta-learning  problem,  with  the  task  distribution  given  by  a  distribution  over  the  preferences.  We  demonstrate  that  such  a  formulation  results  in  a  better  approximation  of  the  Pareto  optimal  solutions  in  terms  of  both  the  optimality  and  the  computational  efficiency.  We  evaluated  our  method  on  obtaining  Pareto  optimal  policies  using  a  number  of  continuous  control  problems  with  high  degrees  of  freedom.
0	Structured  reward  shaping  using  signal  temporal  logic  specifications.  Deep  reinforcement  learning  has  become  a  popular  technique  to  train  autonomous  agents  to  learn  control  policies  that  enable  them  to  accomplish  complex  tasks  in  uncertain  environments.  A  key  component  of  an  RL  algorithm  is  the  definition  of  a  reward  function  that  maps  each  state  and  an  action  that  can  be  taken  in  that  state  to  some  real-valued  reward.  Typically,  reward  functions  informally  capture  an  implicit  (albeit  vague)  specification  on  the  desired  behavior  of  the  agent.  In  this  paper,  we  propose  the  use  of  the  logical  formalism  of  Signal  Temporal  Logic(STL)  as  a  formal  specification  for  the  desired  behaviors  of  the  agent.  Furthermore,  we  propose  algorithms  to  locally  shape  rewards  in  each  state  with  the  goal  of  satisfying  the  high-level  STL  specification.  We  demonstrate  our  technique  on  two  case  studies,  a  cart-pole  balancing  problem  with  a  discrete  action  space,  and  controlling  the  actuation  of  a  simulated  quadrotor  for  point-to-point  movement.The  proposed  framework  is  agnostic  to  any  specific  RL  algorithm,  as  locally  shaped  rewards  can  be  easily  used  in  concert  with  any  deep  RL  algorithm.
0	Dr  spaam  a  spatial  attention  and  auto  regressive  model  for  person  detection  in  2d  range  data.  Detecting  persons  using  a  2D  LiDAR  is  a  challenging  task  due  to  the  low  information  content  of  2D  range  data.  To  alleviate  the  problem  caused  by  the  sparsity  of  the  LiDAR  points,  current  state-of-the-art  methods  fuse  multiple  previous  scans  and  perform  detection  using  the  combined  scans.  The  downside  of  such  a  backward  looking  fusion  is  that  all  the  scans  need  to  be  aligned  explicitly,  and  the  necessary  alignment  operation  makes  the  whole  pipeline  more  expensive  –  often  too  expensive  for  real-world  applications.  In  this  paper,  we  propose  a  person  detection  network  which  uses  an  alternative  strategy  to  combine  scans  obtained  at  different  times.  Our  method,  Distance  Robust  SPatial  Attention  and  Auto-regressive  Model  (DR-SPAAM),  follows  a  forward  looking  paradigm.  It  keeps  the  intermediate  features  from  the  backbone  network  as  a  template  and  recurrently  updates  the  template  when  a  new  scan  becomes  available.  The  updated  feature  template  is  in  turn  used  for  detecting  persons  currently  in  the  scene.  On  the  DROW  dataset,  our  method  outperforms  the  existing  state-of-the-art,  while  being  approximately  four  times  faster,  running  at  87.2  FPS  on  a  laptop  with  a  dedicated  GPU  and  at  22.6  FPS  on  an  NVIDIA  Jetson  AGX  embedded  GPU.  We  release  our  code  in  PyTorch  and  a  ROS  node  including  pre-trained  models.
0	Multi  robot  hunting  based  on  swarm  intelligence.  This  paper  reports  studies  carried  out  on  a  multi-robot  system  assigned  with  the  task  of  hunting  down  a  rogue  element  in  its  vicinity  whose  location  is  not  known.  Swarm  Intelligence  (SI)  has  been  used  in  the  studies  for  organizing  the  multi-robot  coordinated  strategy  to  capture/enclose  the  target/invader.  MATLAB  based  simulation  has  been  used  to  ascertain  the  Flexibility,  Robustness  and  Self-organization  capability  of  the  SI  based  strategy.
0	Dsp  based  embedded  fingerprint  recognition  system.  Along  with  the  rapid  development  of  biometric  recognition  techniques,  the  fingerprint  recognition  became  a  significant  subject.  Fingerprint  recognition  is  a  method  to  identify  a  person,  based  on  the  physiological  characteristics  of  the  finger.  Many  of  today's  recognition  systems  are  realized  on  the  PC-based  platform,  which  has  high  power  consumption  and  cannot  be  used  with  ease.  But,  the  recently  developed  market  for  civil  applications  has  more  demand  for  system's  portability  and  power  consumption.  So,  the  trend  of  miniaturizing  and  embedding  the  system  for  automatic  fingerprint  identification  became  more  distinct.  In  this  paper,  a  convenient,  fast,  and  inexpensive  low  power  Digital  Signal  Processor  (DSP)  based  embedded  fingerprint  recognition  system  is  introduced.  Reliable  extraction  of  features  from  input  fingerprint  image  is  the  most  challenging  problem  faced  in  the  area  of  fingerprint  recognition.  However  the  performance  of  the  Minutiae  extraction  algorithm  relies  heavily  on  the  quality  of  the  input  fingerprint  image.  In  order  to  ensure  to  extract  the  true  minutiae  points  it  is  essential  to  incorporate  a  good  enhancement  algorithm.  The  earlier  fingerprint  image  enhancement  methods  are  FFT  based  and  has  a  drawback  of  poor  image  quality  which  results  in  unreliable  minutiae  extraction  and  thereby  reduces  the  accuracy  of  recognition  result.  In  this  paper,  at  the  algorithm  level  a  new  approach  for  fingerprint  image  enhancement  based  on  the  Gabor  filter  is  introduced.  Comparatively  this  algorithm  produced  the  good  results  in  the  view  of  image  quality  and  accuracy.
0	Applying  weighted  queries  on  probabilistic  databases.  Relational  queries  applied  on  probabilistic  databases  have  been  established  as  a  powerful  tool  for  accessing  huge  data  sets  of  uncertain  data.  Often  various  parts  of  such  queries  have  different  significances  for  a  specific  user.  Thus,  a  query  language  should  allow  us  to  give  subqueries  different  weights  to  quantify  the  individual  user  preferences.  In  this  work  we  introduce  a  theoretical  foundation  for  weighted  algebra  operators  on  probabilistic  databases  within  a  SQL-like  query  language.
0	Profiling  pedestrian  distribution  and  anomaly  detection  in  a  dynamic  environment.  Pedestrians  movements  have  a  major  impact  on  the  dynamics  of  cities  and  provide  valuable  guidance  to  city  planners.  In  this  paper  we  model  the  normal  behaviours  of  pedestrian  flows  and  detect  anomalous  events  from  pedestrian  counting  data  of  the  City  of  Melbourne.  Since  the  data  spans  an  extended  period,  and  pedestrian  activities  can  change  intermittently  (e.g.,  activities  in  winter  vs.  summer),  we  applied  an  Ensemble  Switching  Model,  which  is  a  dynamic  anomaly  detection  technique  that  can  accommodate  systems  that  switch  between  different  states.  The  results  are  compared  with  those  produced  by  a  static  clustering  model  (HyCARCE)  and  also  cross-validated  with  known  events.  We  found  that  the  results  from  the  Ensemble  Switching  Model  are  valid  and  more  accurate  than  HyCARCE.
0	Skipping  word  a  character  sequential  representation  based  framework  for  question  answering.  Recent  works  using  artificial  neural  networks  based  on  word  distributed  representation  greatly  boost  the  performance  of  various  natural  language  learning  tasks,  especially  question  answering.  Though,  they  also  carry  along  with  some  attendant  problems,  such  as  corpus  selection  for  embedding  learning,  dictionary  transformation  for  different  learning  tasks,  etc.  In  this  paper,  we  propose  to  straightforwardly  model  sentences  by  means  of  character  sequences,  and  then  utilize  convolutional  neural  networks  to  integrate  character  embedding  learning  together  with  point-wise  answer  selection  training.  Compared  with  deep  models  pre-trained  on  word  embedding  (WE)  strategy,  our  character-sequential  representation  (CSR)  based  method  shows  a  much  simpler  procedure  and  more  stable  performance  across  different  benchmarks.  Extensive  experiments  on  two  benchmark  answer  selection  datasets  exhibit  the  competitive  performance  compared  with  the  state-of-the-art  methods.
0	On  sampling  the  wisdom  of  crowds  random  vs  expert  sampling  of  the  twitter  stream.  Several  applications  today  rely  upon  content  streams  crowd-sourced  from  online  social  networks.  Since  real-time  processing  of  large  amounts  of  data  generated  on  these  sites  is  difficult,  analytics  companies  and  researchers  are  increasingly  resorting  to  sampling.  In  this  paper,  we  investigate  the  crucial  question  of  how  to  sample  the  data  generated  by  users  in  social  networks.  The  traditional  method  is  to  randomly  sample  all  the  data.  We  analyze  a  different  sampling  methodology,  where  content  is  gathered  only  from  a  relatively  small  subset  (
0	Multimedia  summarization  for  trending  topics  in  microblogs.  Microblogging  services  have  revolutionized  the  way  people  exchange  information.  Confronted  with  the  ever-increasing  numbers  of  microblogs  with  multimedia  contents  and  trending  topics,  it  is  desirable  to  provide  visualized  summarization  to  help  users  to  quickly  grasp  the  essence  of  topics.  While  existing  works  mostly  focus  on  text-based  methods  only,  summarization  of  multiple  media  types  (e.g.,  text  and  image)  are  scarcely  explored.  In  this  paper,  we  propose  a  multimedia  microblog  summarization  framework  to  automatically  generate  visualized  summaries  for  trending  topics.  Specifically,  a  novel  generative  probabilistic  model,  termed  multimodal-LDA  (MMLDA),  is  proposed  to  discover  subtopics  from  microblogs  by  exploring  the  correlations  among  different  media  types.  Based  on  the  information  achieved  from  MMLDA,  a  multimedia  summarizer  is  designed  to  separately  identify  representative  textual  and  visual  samples  and  then  form  a  comprehensive  visualized  summary.  We  conduct  extensive  experiments  on  a  real-world  Sina  Weibo  microblog  dataset  to  demonstrate  the  superiority  of  our  proposed  method  against  the  state-of-the-art  approaches.
0	Point  of  interest  recommendation  exploiting  self  attentive  autoencoders  with  neighbor  aware  influence.  The  rapid  growth  of  Location-based  Social  Networks  (LBSNs)  provides  a  great  opportunity  to  satisfy  the  strong  demand  for  personalized  Point-of-Interest  (POI)  recommendation  services.  However,  with  the  tremendous  increase  of  users  and  POIs,  POI  recommender  systems  still  face  several  challenging  problems:  (1)  the  hardness  of  modeling  complex  user-POI  interactions  from  sparse  implicit  feedback;  (2)  the  difficulty  of  incorporating  the  geographical  context  information.  To  cope  with  these  challenges,  we  propose  a  novel  autoencoder-based  model  to  learn  the  complex  user-POI  relations,  namely  SAE-NAD,  which  consists  of  a  self-attentive  encoder  (SAE)  and  a  neighbor-aware  decoder  (NAD).  In  particular,  unlike  previous  works  equally  treat  users'  checked-in  POIs,  our  self-attentive  encoder  adaptively  differentiates  the  user  preference  degrees  in  multiple  aspects,  by  adopting  a  multi-dimensional  attention  mechanism.  To  incorporate  the  geographical  context  information,  we  propose  a  neighbor-aware  decoder  to  make  users'  reachability  higher  on  the  similar  and  nearby  neighbors  of  checked-in  POIs,  which  is  achieved  by  the  inner  product  of  POI  embeddings  together  with  the  radial  basis  function  (RBF)  kernel.  To  evaluate  the  proposed  model,  we  conduct  extensive  experiments  on  three  real-world  datasets  with  many  state-of-the-art  methods  and  evaluation  metrics.  The  experimental  results  demonstrate  the  effectiveness  of  our  model.
0	Mining  coherent  anomaly  collections  on  web  data.  The  recent  boom  of  weblogs  and  social  media  has  attached  increasing  importance  to  the  identification  of  suspicious  users  with  unusual  behavior,  such  as  spammers  or  fraudulent  reviewers.  A  typical  spamming  strategy  is  to  employ  multiple  dummy  accounts  to  collectively  promote  a  target,  be  it  a  URL  or  a  product.  Consequently,  these  suspicious  accounts  exhibit  certain  coherent  anomalous  behavior  identifiable  as  a  collection.  In  this  paper,  we  propose  the  concept  of  Coherent  Anomaly  Collection  (CAC)  to  capture  this  kind  of  collections,  and  put  forward  an  efficient  algorithm  to  simultaneously  find  the  top-K  disjoint  CACs  together  with  their  anomalous  behavior  patterns.  Compared  with  existing  approaches,  our  new  algorithm  can  find  disjoint  anomaly  collections  with  coherent  extreme  behavior  without  having  to  specify  either  their  number  or  sizes.  Results  on  real  Twitter  data  show  that  our  approach  discovers  meaningful  and  informative  hashtag  spammer  groups  of  various  sizes  which  are  hard  to  detect  by  clustering-based  methods.
0	Health  forum  thread  recommendation  using  an  interest  aware  topic  model.  We  introduce  a  general,  interest-aware  topic  model  (IATM),  in  which  known  higher-level  interests  on  topics  expressed  by  each  user  can  be  modeled.  We  then  specialize  the  IATM  for  use  in  consumer  health  forum  thread  recommendation  by  equating  each  user's  self-reported  medical  conditions  as  interests  and  topics  as  symptoms  of  treatments  for  recommendation.  The  IATM  additionally  models  the  implicit  interests  embodied  by  users'  textual  descriptions  in  their  profiles.  To  further  enhance  the  personalized  nature  of  the  recommendations,  we  introduce  jointly  normalized  collaborative  topic  regression  (JNCTR)  which  captures  how  users  interact  with  the  various  symptoms  belonging  to  the  same  clinical  condition.          In  our  experiments  on  two  real-world  consumer  health  forums,  our  proposed  model  significantly  outperforms  competitive  state-of-the-art  baselines  by  over  10%  in  recall.  Importantly,  we  show  that  our  IATM+JNCTR  pipeline  also  imbues  the  recommendation  process  with  added  transparency,  allowing  a  recommendation  system  to  justify  its  recommendation  with  respect  to  each  user's  interest  in  certain  health  conditions.
0	Augment  to  prevent  short  text  data  augmentation  in  deep  learning  for  hate  speech  classification.  In  this  paper,  we  address  the  issue  of  augmenting  text  data  in  supervised  Natural  Language  Processing  problems,  exemplified  by  deep  online  hate  speech  classification.  A  great  challenge  in  this  domain  is  that  although  the  presence  of  hate  speech  can  be  deleterious  to  the  quality  of  service  provided  by  social  platforms,  it  still  comprises  only  a  tiny  fraction  of  the  content  that  can  be  found  online,  which  can  lead  to  performance  deterioration  due  to  majority  class  overfitting.  To  this  end,  we  perform  a  thorough  study  on  the  application  of  deep  learning  to  the  hate  speech  detection  problem:  a)  we  propose  three  text-based  data  augmentation  techniques  aimed  at  reducing  the  degree  of  class  imbalance  and  to  maximise  the  amount  of  information  we  can  extract  from  our  limited  resources  and  b)  we  apply  them  on  a  selection  of  top-performing  deep  architectures  and  hate  speech  databases  in  order  to  showcase  their  generalisation  properties.  The  data  augmentation  techniques  are  based  on  a)  synonym  replacement  based  on  word  embedding  vector  closeness,  b)  warping  of  the  word  tokens  along  the  padded  sequence  or  c)  class-conditional,  recurrent  neural  language  generation.  Our  proposed  framework  yields  a  significant  increase  in  multi-class  hate  speech  detection,  outperforming  the  baseline  in  the  largest  online  hate  speech  database  by  an  absolute  5.7%  increase  in  Macro-F1  score  and  30%  in  hate  speech  class  recall.
0	Prediction  of  retweet  cascade  size  over  time.  Retweet  cascades  play  an  essential  role  in  information  diffusion  in  Twitter.  Popular  tweets  reflect  the  current  trends  in  Twitter,  while  Twitter  itself  is  one  of  the  most  important  online  media.  Thus,  understanding  the  reasons  why  a  tweet  becomes  popular  is  of  great  interest  for  sociologists,  marketers  and  social  media  researches.  What  is  even  more  important  is  the  possibility  to  make  a  prognosis  of  a  tweet's  future  popularity.  Besides  the  scientific  significance  of  such  possibility,  this  sort  of  prediction  has  lots  of  practical  applications  such  as  breaking  news  detection,  viral  marketing  etc.  In  this  paper  we  try  to  forecast  how  many  retweets  a  given  tweet  will  gain  during  a  fixed  time  period.  We  train  an  algorithm  that  predicts  the  number  of  retweets  during  time  T  since  the  initial  moment.  In  addition  to  a  standard  set  of  features  we  utilize  several  new  ones.  One  of  the  most  important  features  is  the  flow  of  the  cascade.  Another  one  is  PageRank  on  the  retweet  graph,  which  can  be  considered  as  the  measure  of  influence  of  users.
0	Providing  grades  and  feedback  for  student  summaries  by  ontology  based  information  extraction.  Automatic  grading  systems  for  summaries  and  essays  have  been  studied  for  years.  Most  commercial  and  research  implementations  are  based  in  statistical  methods,  such  as  Latent  Semantic  Analysis  (LSA),  which  can  provide  high  accuracy  on  similarity  between  the  essay  and  the  graded  or  standard  essays,  but  they  can  offer  very  limited  feedback.  In  the  present  work,  we  propose  a  novel  method  to  provide  both  grades  and  meaningful  feedback  for  student  summaries  by  Ontology-based  Information  Extraction  (OBIE).  We  use  ontological  concepts  and  relationships  to  create  extraction  rules  to  identify  correct  statements.  Based  on  ontology  constraints  (e.g.,  disjointness  between  concepts),  we  define  patterns  that  are  logically  inconsistent  with  the  ontology  to  create  rules  to  extract  incorrect  statements.  Experiments  show  that  the  grades  given  to  18  student  summaries  on  Ecosystems  by  OBIE  are  correlated  to  human  gradings.  OBIE  also  provide  meaningful  feedback  on  the  errors  those  students  made  in  their  summaries.
0	Keyword  search  on  rdf  graphs  a  query  graph  assembly  approach.  Keyword  search  provides  ordinary  users  an  easy-to-use  interface  for  querying  RDF  data.  Given  the  input  keywords,  in  this  paper,  we  study  how  to  assemble  a  query  graph  that  is  to  represent  user's  query  intention  accurately  and  efficiently.  Based  on  the  input  keywords,  we  first  obtain  the  elementary  query  graph  building  blocks,  such  as  entity/class  vertices  and  predicate  edges.  Then,  we  formally  define  the  query  graph  assembly  (QGA)  problem.  Unfortunately,  we  prove  theoretically  that  QGA  is  a  NP-complete  problem.  In  order  to  solve  that,  we  design  some  heuristic  lower  bounds  and  propose  a  bipartite  graph  matching-based  best-first  search  algorithm.  The  algorithm's  time  complexity  is  O(k2l  ...  l3l),  where  l  is  the  number  of  the  keywords  and  k  is  a  tunable  parameter,  i.e.,  the  maximum  number  of  candidate  entity/class  vertices  and  predicate  edges  allowed  to  match  each  keyword.  Although  QGA  is  intractable,  both  l  and  k  are  small  in  practice.  Furthermore,  the  algorithm's  time  complexity  does  not  depend  on  the  RDF  graph  size,  which  guarantees  the  good  scalability  of  our  system  in  large  RDF  graphs.  Experiments  on  DBpedia  and  Freebase  confirm  the  superiority  of  our  system  on  both  effectiveness  and  efficiency.
0	Words  are  malleable  computing  semantic  shifts  in  political  and  media  discourse.  Recently,  researchers  started  to  pay  attention  to  the  detection  of  temporal  shifts  in  the  meaning  of  words.  However,  most  (if  not  all)  of  these  approaches  restricted  their  efforts  to  uncovering  change  over  time,  thus  neglecting  other  valuable  dimensions  such  as  social  or  political  variability.  We  propose  an  approach  for  detecting  semantic  shifts  between  different  viewpoints---broadly  defined  as  a  set  of  texts  that  share  a  specific  metadata  feature,  which  can  be  a  time-period,  but  also  a  social  entity  such  as  a  political  party.  For  each  viewpoint,  we  learn  a  semantic  space  in  which  each  word  is  represented  as  a  low  dimensional  neural  embedded  vector.  The  challenge  is  to  compare  the  meaning  of  a  word  in  one  space  to  its  meaning  in  another  space  and  measure  the  size  of  the  semantic  shifts.  We  compare  the  effectiveness  of  a  measure  based  on  optimal  transformations  between  the  two  spaces  with  a  measure  based  on  the  similarity  of  the  neighbors  of  the  word  in  the  respective  spaces.  Our  experiments  demonstrate  that  the  combination  of  these  two  performs  best.  We  show  that  the  semantic  shifts  not  only  occur  over  time  but  also  along  different  viewpoints  in  a  short  period  of  time.  For  evaluation,  we  demonstrate  how  this  approach  captures  meaningful  semantic  shifts  and  can  help  improve  other  tasks  such  as  the  contrastive  viewpoint  summarization  and  ideology  detection  (measured  as  classification  accuracy)  in  political  texts.  We  also  show  that  the  two  laws  of  semantic  change  which  were  empirically  shown  to  hold  for  temporal  shifts  also  hold  for  shifts  across  viewpoints.  These  laws  state  that  frequent  words  are  less  likely  to  shift  meaning  while  words  with  many  senses  are  more  likely  to  do  so.
0	Learning  latent  vector  spaces  for  product  search.  We  introduce  a  novel  latent  vector  space  model  that  jointly  learns  the  latent  representations  of  words,  e-commerce  products  and  a  mapping  between  the  two  without  the  need  for  explicit  annotations.  The  power  of  the  model  lies  in  its  ability  to  directly  model  the  discriminative  relation  between  products  and  a  particular  word.  We  compare  our  method  to  existing  latent  vector  space  models  (LSI,  LDA  and  word2vec)  and  evaluate  it  as  a  feature  in  a  learning  to  rank  setting.  Our  latent  vector  space  model  achieves  its  enhanced  performance  as  it  learns  better  product  representations.  Furthermore,  the  mapping  from  words  to  products  and  the  representations  of  words  benefit  directly  from  the  errors  propagated  back  from  the  product  representations  during  parameter  estimation.  We  provide  an  in-depth  analysis  of  the  performance  of  our  model  and  analyze  the  structure  of  the  learned  representations.
0	Investigating  and  mitigating  degree  related  biases  in  graph  convoltuional  networks.  Graph  Convolutional  Networks  (GCNs)  show  promising  results  for  semi-supervised  learning  tasks  on  graphs,  thus  become  favorable  comparing  with  other  approaches.  Despite  the  remarkable  success  of  GCNs,  it  is  difficult  to  train  GCNs  with  insufficient  supervision.  When  labeled  data  are  limited,  the  performance  of  GCNs  becomes  unsatisfying  for  low-degree  nodes.  While  some  prior  work  analyze  successes  and  failures  of  GCNs  on  the  entire  model  level,  profiling  GCNs  on  individual  node  level  is  still  underexplored.  In  this  paper,  we  analyze  GCNs  in  regard  to  the  node  degree  distribution.  From  empirical  observation  to  theoretical  proof,  we  confirm  that  GCNs  are  biased  towards  nodes  with  larger  degrees  with  higher  accuracy  on  them,  even  if  high-degree  nodes  are  underrepresented  in  most  graphs.  We  further  develop  a  novel  Self-Supervised-Learning  Degree-Specific  GCN  (SL-DSGCN)  that  mitigate  the  degree-related  biases  of  GCNs  from  model  and  data  aspects.  Firstly,  we  propose  a  degree-specific  GCN  layer  that  captures  both  discrepancies  and  similarities  of  nodes  with  different  degrees,  which  reduces  the  inner  model-aspect  biases  of  GCNs  caused  by  sharing  the  same  parameters  with  all  nodes.  Secondly,  we  design  a  self-supervised-learning  algorithm  that  creates  pseudo  labels  with  uncertainty  scores  on  unlabeled  nodes  with  a  Bayesian  neural  network.  Pseudo  labels  increase  the  chance  of  connecting  to  labeled  neighbors  for  low-degree  nodes,  thus  reducing  the  biases  of  GCNs  from  the  data  perspective.  Uncertainty  scores  are  further  exploited  to  weight  pseudo  labels  dynamically  in  the  stochastic  gradient  descent  for  SL-DSGCN.  Experiments  on  three  benchmark  datasets  show  SL-DSGCN  not  only  outperforms  state-of-the-art  self-training/self-supervised-learning  GCN  methods,  but  also  improves  GCN  accuracy  dramatically  for  low-degree  nodes.
0	A  framework  for  personalized  and  collaborative  clustering  of  search  results.  How  to  organize  and  present  search  results  plays  a  critical  role  in  the  utility  of  search  engines.  Due  to  the  unprecedented  scale  of  the  Web  and  diversity  of  search  results,  the  common  strategy  of  ranked  lists  has  become  increasingly  inadequate,  and  clustering  has  been  considered  as  a  promising  alternative.  Clustering  divides  a  long  list  of  disparate  search  results  into  a  few  topic-coherent  clusters,  allowing  the  user  to  quickly  locate  relevant  results  by  topic  navigation.  While  many  clustering  algorithms  have  been  proposed  that  innovate  on  the  automatic  clustering  procedure,  we  introduce  ClusteringWiki,  the  first  prototype  and  framework  for  personalized  clustering  that  allows  direct  user  editing  of  the  clustering  results.  Through  a  Wiki  interface,  the  user  can  edit  and  annotate  the  membership,  structure  and  labels  of  clusters  for  a  personalized  presentation.  In  addition,  the  edits  and  annotations  can  be  shared  among  users  as  a  mass-collaborative  way  of  improving  search  result  organization  and  search  engine  utility.
0	Heterogeneous  graph  neural  networks  for  malicious  account  detection.  We  present,  GEM,  the  first  heterogeneous  graph  neural  network  approach  for  detecting  malicious  accounts  at  Alipay,  one  of  the  world's  leading  mobile  cashless  payment  platform.  Our  approach,  inspired  from  a  connected  subgraph  approach,  adaptively  learns  discriminative  embeddings  from  heterogeneous  account-device  graphs  based  on  two  fundamental  weaknesses  of  attackers,  i.e.  device  aggregation  and  activity  aggregation.  For  the  heterogeneous  graph  consists  of  various  types  of  nodes,  we  propose  an  attention  mechanism  to  learn  the  importance  of  different  types  of  nodes,  while  using  the  sum  operator  for  modeling  the  aggregation  patterns  of  nodes  in  each  type.  Experiments  show  that  our  approaches  consistently  perform  promising  results  compared  with  competitive  methods  over  time.
0	Transductive  visual  semantic  embedding  for  zero  shot  learning.  Zero-shot  learning  (ZSL)  aims  to  bridge  the  knowledge  transfer  via  available  semantic  representations  (e.g.,  attributes)  between  labeled  source  instances  of  seen  classes  and  unlabelled  target  instances  of  unseen  classes.  Most  existing  ZSL  approaches  achieve  this  by  learning  a  projection  from  the  visual  feature  space  to  the  semantic  representation  space  based  on  the  source  instances,  and  directly  applying  it  to  the  target  instances.  However,  the  intrinsic  manifold  structures  residing  in  both  semantic  representations  and  visual  features  are  not  effectively  incorporated  into  the  learned  projection  function.  Moreover,  these  methods  may  suffer  from  the  inherent  projection  shift  problem,  due  to  the  disjointness  between  seen  and  unseen  classes.  To  overcome  these  drawbacks,  we  propose  a  novel  framework  termed  transductive  visual-semantic  embedding  (TVSE)  for  ZSL.  In  specific,  TVSE  first  learns  a  latent  embedding  space  to  incorporate  the  manifold  structures  in  both  labeled  source  instances  and  unlabeled  target  instances  under  the  transductive  setting.  In  the  learned  space,  each  instance  is  viewed  as  a  mixture  of  seen  class  scores.  TVSE  then  effectively  constructs  the  relational  mapping  between  seen  and  unseen  classes  using  the  available  semantic  representations,  and  applies  it  to  map  the  seen  class  scores  of  the  target  instances  to  their  predictions  of  unseen  classes.  Extensive  experiments  on  four  benchmark  datasets  demonstrate  that  the  proposed  TVSE  achieves  competitive  performance  compared  with  the  state-of-the-arts  for  zero-shot  recognition  and  retrieval  tasks.
0	Indexing  the  signature  quadratic  form  distance  for  efficient  content  based  multimedia  retrieval.  The  Signature  Quadratic  Form  Distance  has  been  introduced  as  an  adaptive  similarity  measure  coping  with  flexible  content  representations  of  various  multimedia  data.  Although  the  Signature  Quadratic  Form  Distance  has  shown  good  retrieval  performance  with  respect  to  their  qualities  of  effectiveness  and  efficiency,  its  applicability  to  index  structures  remains  a  challenging  issue  due  to  its  dynamic  nature.  In  this  paper,  we  investigate  the  indexability  of  the  Signature  Quadratic  Form  Distance  regarding  metric  access  methods.  We  show  how  the  distance's  inherent  parameters  determine  the  indexability  and  analyze  the  relationship  between  effectiveness  and  efficiency  on  numerous  image  databases.
0	Leveraging  multi  modal  prior  knowledge  for  large  scale  concept  learning  in  noisy  web  data.  Learning  video  concept  detectors  automatically  from  the  big  but  noisy  web  data  with  no  additional  manual  annotations  is  a  novel  but  challenging  area  in  the  multimedia  and  the  machine  learning  community.  A  considerable  amount  of  videos  on  the  web  is  associated  with  rich  but  noisy  contextual  information,  such  as  the  title  and  other  multi-modal  information,  which  provides  weak  annotations  or  labels  about  the  video  content.  To  tackle  the  problem  of  large-scale  noisy  learning,  We  propose  a  novel  method  called  Multi-modal  WEbly-Labeled  Learning  (WELL-MM),  which  is  established  on  the  state-of-the-art  machine  learning  algorithm  inspired  by  the  learning  process  of  human.  WELL-MM  introduces  a  novel  multi-modal  approach  to  incorporate  meaningful  prior  knowledge  called  curriculum  from  the  noisy  web  videos.  We  empirically  study  the  curriculum  constructed  from  the  multi-modal  features  of  the  Internet  videos  and  images.  The  comprehensive  experimental  results  on  FCVID  and  YFCC100M  demonstrate  that  WELL-MM  outperforms  state-of-the-art  studies  by  a  statically  significant  margin  on  learning  concepts  from  noisy  web  video  data.  In  addition,  the  results  also  verify  that  WELL-MM  is  robust  to  the  level  of  noisiness  in  the  video  data.  Notably,  WELL-MM  trained  on  sufficient  noisy  web  labels  is  able  to  achieve  a  better  accuracy  to  supervised  learning  methods  trained  on  the  clean  manually  labeled  data.
0	Adaptive  dichotomous  image  segmentation  toolkit.  The  article  deals  with  the  adaptive  hierarchical  segmentation  of  the  digital  image  divided  into  segments  of  the  calculated  form.  The  dichotomous  segmentation  is  examined,  in  which  case  the  vast  majority  of  segments  are  divided  into  two  nested  segments.  A  method  is  described  for  the  rapid  construction  of  a  dichotomous  hierarchy  by  a  given  criterion  for  the  proximity  of  segments  by  the  iterative  merging  of  adjacent  segments  of  an  initial  image  decomposition.  The  numerical  characteristic  of  the  regularity  of  the  dichotomous  hierarchy  of  segments  is  introduced.  Variants  are  constructed  of  the  hierarchical  approximation  of  the  image  by  nested  partitions  (levels  of  the  hierarchy)  formed  from  the  segments  with  repetitions.  Three  basic  types  of  transformations  are  determined  on  the  set  of  hierarchical  decompositions.  In  order  to  optimize  the  approximation  of  visible  objects  by  image  segments,  22  algorithms  are  studied  of  its  partition  to  successively  increase  the  number  of  segments.  Depending  on  the  number  of  segments,  the  required  standard  deviation  is  estimated.  A  comparison  with  similar  solutions  is  given.
0	Facial  recognition  system  for  suspect  identification  using  a  surveillance  camera.  Nowadays,  finding  and  Tracking  a  person  in  the  world  of  technology  is  becoming  a  necessary  task  for  various  security  purposes.  Since  the  advent  of  technology,  the  development  in  the  field  of  Facial  Recognition  plays  an  important  role  and  has  been  exponentially  increasing  in  today’s  world.  In  this,  a  model  is  proposed  for  facial  recognition  to  identify  and  alert  the  system  when  a  person  in  search  has  been  found  at  a  specific  location  under  the  surveillance  of  a  CCTV  camera.  The  CCTV  cameras  are  connected  to  a  centralized  server  to  which  the  live  streaming  feed  is  uploaded  by  cameras  at  each  location.  The  server  contains  a  database  of  all  persons  to  be  found.  Based  on  the  video  feed  from  each  camera,  if  a  particular  person  in  search  is  found  in  a  certain  feed,  then  the  location  of  that  person  will  be  tracked  and  also  a  signal  is  passed  to  the  system  responsible.  This  model  is  based  on  image  processing  concepts  to  match  live  images  with  the  existing  trained  images  of  the  person  in  search.  Since  this  model  recognizes  a  person  based  on  the  first  and  foremost  primary  unique  feature  of  a  human,  that  is,  only  the  person’s  face  image  is  required  and  will  be  found  to  be  stored  in  the  database.  Hence  the  task  of  finding  a  person  reduces  to  the  task  of  detecting  human  faces  in  the  video  feed  and  matching  with  the  existing  images  from  the  database.
0	Correlation  analysis  and  synthesis  of  random  field  wave  models.  The  solution  of  many  scientific  and  technical  problems  involves  the  extraction  of  useful  information  from  multidimensional  data  (images).  Mathematical  formalization  of  the  problems  that  arise  includes  creating  models  of  these  images.  Despite  the  large  number  of  works  on  image  models,  significant  difficulties  arise  when  an  image  model  with  preset  properties  is  constructed.  This  work  proposes  the  use  of  random  field  wave  models  for  correlation  analysis  and  the  synthesis  of  multidimensional  image  models  and  their  sequences.  Essentially,  these  models  also  make  it  possible  to  imitate  images  with  small  computational  expenditures,  is  required  for  testing  of  various  image  processing  algorithms.
0	Variance  function  estimation  in  high  dimensions.  We  consider  the  high-dimensional  heteroscedastic  regression  model,  where  the  mean  and  the  log  variance  are  modeled  as  a  linear  combination  of  input  variables.  Existing  literature  on  high-dimensional  linear  regression  models  has  largely  ignored  non-constant  error  variances,  even  though  they  commonly  occur  in  a  variety  of  applications  ranging  from  biostatistics  to  finance.  In  this  paper  we  study  a  class  of  nonconvex  penalized  pseudolikelihood  estimators  for  both  the  mean  and  variance  parameters.  We  show  that  the  Heteroscedastic  Iterative  Penalized  Pseudolikelihood  Optimizer  (HIPPO)  achieves  the  oracle  property,  that  is,  we  prove  that  the  rates  of  convergence  are  the  same  as  if  the  true  model  was  known.  We  demonstrate  numerical  properties  of  the  procedure  on  a  simulation  study  and  real  world  data.
0	Hard  margin  active  linear  regression.  We  consider  the  fundamental  problem  of  linear  regression  in  which  the  designer  can  actively  choose  observations.  This  model  naturally  captures  various  experiment  design  settings  in  medical  experiments,  ad  placement  problems  and  more.  Whereas  previous  literature  addresses  the  soft-margin  or  mean-square-error  variants  of  the  problem,  we  consider  a  natural  machine  learning  hard-margin  criterion.  In  this  setting,  we  show  that  active  learning  admits  significantly  better  sample  complexity  bounds  than  the  passive  learning  counterpart,  and  give  efficient  algorithms  that  attain  near-optimal  bounds.
0	Parsing  epileptic  events  using  a  markov  switching  process  model  for  correlated  time  series.  Patients  with  epilepsy  can  manifest  short,  sub-clinical  epileptic  \bursts"  in  addition  to  full-blown  clinical  seizures.  We  believe  the  relationship  between  these  two  classes  of  events|something  not  previously  studied  quantitatively|could  yield  important  insights  into  the  nature  and  intrinsic  dynamics  of  seizures.  A  goal  of  our  work  is  to  parse  these  complex  epileptic  events  into  distinct  dynamic  regimes.  A  challenge  posed  by  the  intracranial  EEG  (iEEG)  data  we  study  is  the  fact  that  the  number  and  placement  of  electrodes  can  vary  between  patients.  We  develop  a  Bayesian  nonparametric  Markov  switching  process  that  allows  for  (i)  shared  dynamic  regimes  between  a  variable  numbers  of  channels,  (ii)  asynchronous  regimeswitching,  and  (iii)  an  unknown  dictionary  of  dynamic  regimes.  We  encode  a  sparse  and  changing  set  of  dependencies  between  the  channels  using  a  Markov-switching  Gaussian  graphical  model  for  the  innovations  process  driving  the  channel  dynamics.  We  demonstrate  the  importance  of  this  model  in  parsing  and  out-of-sample  predictions  of  iEEG  data.  We  show  that  our  model  produces  intuitive  state  assignments  that  can  help  automate  clinical  analysis  of  seizures  and  enable  the  comparison  of  sub-clinical  bursts  and  full  clinical  seizures.
0	Optical  character  recognition  system  for  nepali  language  using  convnet.  This  paper  describes  the  implementation  of  CNN  (Convolution  Neural  Network)  based  Optical  Character  Recognition  System  for  Nepali  Language,  a  commonly  spoken  language  in  Nepal.  The  system  has  been  developed  in  python  using  Keras[1]  library  on  top  of  Theano[2]  and  numpy[3].  The  system  has  been  trained  using  a  set  of  real  world[4]  and  synthesized  data  sets  considering  various  noise  conditions.  The  tests  have  also  been  carried  out  in  a  similar  setup.  This  paper  details  the  experiment  by  discussing  the  concept,  implementation  details  and  overall  interpretation  of  the  system.
0	Generic  exploration  and  k  armed  voting  bandits.  We  study  a  stochastic  online  learning  scheme  with  partial  feedback  where  the  utility  of  decisions  is  only  observable  through  an  estimation  of  the  environment  parameters.  We  propose  a  generic  pure-exploration  algorithm,  able  to  cope  with  various  utility  functions  from  multi-armed  bandits  settings  to  dueling  bandits.  The  primary  application  of  this  setting  is  to  offer  a  natural  generalization  of  dueling  bandits  for  situations  where  the  environment  parameters  reflect  the  idiosyncratic  preferences  of  a  mixed  crowd.
0	Causal  inference  by  identification  of  vector  autoregressive  processes  with  hidden  components.  A  widely  applied  approach  to  causal  inference  from  a  time  series  X,  often  referred  to  as  "(linear)  Granger  causal  analysis",  is  to  simply  regress  present  on  past  and  interpret  the  regression  matrix  B  causally.  However,  if  there  is  an  unmeasured  time  series  Z  that  influences  X,  then  this  approach  can  lead  to  wrong  causal  conclusions,  i.e.,  distinct  from  those  one  would  draw  if  one  had  additional  information  such  as  Z.  In  this  paper  we  take  a  different  approach:  We  assume  that  X  together  with  some  hidden  Z  forms  a  first  order  vector  autoregressive  (VAR)  process  with  transition  matrix  A,  and  argue  why  it  is  more  valid  to  interpret  A  causally  instead  of  B.  Then  we  examine  under  which  conditions  the  most  important  parts  of  A  are  identifiable  or  almost  identifiable  from  only  X.  Essentially,  sufficient  conditions  are  (1)  non-Gaussian,  independent  noise  or  (2)  no  influence  from  X  to  Z.  We  present  two  estimation  algorithms  that  are  tailored  towards  conditions  (1)  and  (2),  respectively,  and  evaluate  them  on  synthetic  and  real-world  data.  We  discuss  how  to  check  the  model  using  X.
0	Predictor  corrector  policy  optimization.  We  present  a  predictor-corrector  framework,  called  PicCoLO,  that  can  transform  a  first-order  model-free  reinforcement  or  imitation  learning  algorithm  into  a  new  hybrid  method  that  leverages  predictive  models  to  accelerate  policy  learning.  The  new  "PicCoLOed"  algorithm  optimizes  a  policy  by  recursively  repeating  two  steps:  In  the  Prediction  Step,  the  learner  uses  a  model  to  predict  the  unseen  future  gradient  and  then  applies  the  predicted  estimate  to  update  the  policy;  in  the  Correction  Step,  the  learner  runs  the  updated  policy  in  the  environment,  receives  the  true  gradient,  and  then  corrects  the  policy  using  the  gradient  error.  Unlike  previous  algorithms,  PicCoLO  corrects  for  the  mistakes  of  using  imperfect  predicted  gradients  and  hence  does  not  suffer  from  model  bias.  The  development  of  PicCoLO  is  made  possible  by  a  novel  reduction  from  predictable  online  learning  to  adversarial  online  learning,  which  provides  a  systematic  way  to  modify  existing  first-order  algorithms  to  achieve  the  optimal  regret  with  respect  to  predictable  information.  We  show,  in  both  theory  and  simulation,  that  the  convergence  rate  of  several  first-order  model-free  algorithms  can  be  improved  by  PicCoLO.
0	Efficient  algorithms  for  adversarial  contextual  learning.  We  provide  the  first  oracle  efficient  sublinear  regret  algorithms  for  adversarial  versions  of  the  contextual  bandit  problem.  In  this  problem,  the  learner  repeatedly  makes  an  action  on  the  basis  of  a  context  and  receives  reward  for  the  chosen  action,  with  the  goal  of  achieving  reward  competitive  with  a  large  class  of  policies.  We  analyze  two  settings:  i)  in  the  transductive  setting  the  learner  knows  the  set  of  contexts  a  priori,  ii)  in  the  small  separator  setting,  there  exists  a  small  set  of  contexts  such  that  any  two  policies  behave  differently  on  one  of  the  contexts  in  the  set.  Our  algorithms  fall  into  the  Follow-The-Perturbed-Leader  family  (Kalai  &  Vempala,  2005)  and  achieve  regret  O(T3/4√K  log(N))  in  the  transductive  setting  and  O(T2/3d3/4K√log(N))  in  the  separator  setting,  where  T  is  the  number  of  rounds,  K  is  the  number  of  actions,  N  is  the  number  of  base-line  policies,  and  d  is  the  size  of  the  separator.  We  actually  solve  the  more  general  adversarial  contextual  semi-bandit  linear  optimization  problem,  whilst  in  the  full  information  setting  we  address  the  even  more  general  contextual  combinatorial  optimization.  We  provide  several  extensions  and  implications  of  our  algorithms,  such  as  switching  regret  and  efficient  learning  with  predictable  sequences.
0	Efficient  gradient  free  variational  inference  using  policy  search.  Inference  from  complex  distributions  is  a  common  problem  in  machine  learning  needed  for  many  Bayesian  methods.  We  propose  an  efficient,  gradient-free  method  for  learning  general  GMM  approximations  of  multimodal  distributions  based  on  recent  insights  from  stochastic  search  methods.  Our  method  establishes  information-geometric  trust  regions  to  ensure  efficient  exploration  of  the  sampling  space  and  stability  of  the  GMM  updates,  allowing  for  efficient  estimation  of  multi-variate  Gaussian  variational  distributions.  For  GMMs,  we  apply  a  variational  lower  bound  to  decompose  the  learning  objective  into  sub-problems  given  by  learning  the  individual  mixture  components  and  the  coefficients.  The  number  of  mixture  components  is  adapted  online  in  order  to  allow  for  arbitrary  exact  approximations.  We  demonstrate  on  several  domains  that  we  can  learn  significantly  better  approximations  than  competing  variational  inference  methods  and  that  the  quality  of  samples  drawn  from  our  approximations  is  on  par  with  samples  created  by  state-of-the-art  MCMC  samplers  that  require  significantly  more  computational  resources.
0	Elementary  theory  of  similarities  and  its  use  in  biology  and  geography.  A  system  of  axioms  of  similarity  measures,  which  is  defined  based  on  families  of  finite  descriptive  sets,  is  considered.  The  concepts  of  the  equivalence  and  coequivalence  of  similarity  measures  and  dissimilarity  measures  are  presented.  Diversity  measures  of  one  or  more  descriptive  sets  are  introduced  using  similarity  measures.  Illustrative  examples  of  similarity  and  dissimilarity  measures  are  taken  from  biology  and  geography.
0	Temporal  video  segmentation  by  event  detection  a  novelty  detection  approach.  Temporal  segmentation  of  videos  into  meaningful  image  sequences  containing  some  particular  activities  is  an  interesting  problem  in  computer  vision.  We  present  a  novel  algorithm  to  achieve  this  semantic  video  segmentation.  The  segmentation  task  is  accomplished  through  event  detection  in  a  frame-by-frame  processing  setup.  We  propose  using  one-class  classification  (OCC)  techniques  to  detect  events  that  indicate  a  new  segment,  since  they  have  been  proved  to  be  successful  in  object  classification  and  they  allow  for  unsupervised  event  detection  in  a  natural  way.  Various  OCC  schemes  have  been  tested  and  compared,  and  additionally,  an  approach  based  on  the  temporal  self-similarity  maps  (TSSMs)  is  also  presented.  The  testing  was  done  on  a  challenging  publicly  available  thermal  video  dataset.  The  results  are  promising  and  show  the  suitability  of  our  approaches  for  the  task  of  temporal  video  segmentation.
0	Revisiting  k  means  new  algorithms  via  bayesian  nonparametrics.  Bayesian  models  offer  great  flexibility  for  clustering  applications--Bayesian  nonparametrics  can  be  used  for  modeling  infinite  mixtures,  and  hierarchical  Bayesian  models  can  be  utilized  for  sharing  clusters  across  multiple  data  sets.  For  the  most  part,  such  flexibility  is  lacking  in  classical  clustering  methods  such  as  k-means.  In  this  paper,  we  revisit  the  k-means  clustering  algorithm  from  a  Bayesian  nonparametric  viewpoint.  Inspired  by  the  asymptotic  connection  between  k-means  and  mixtures  of  Gaussians,  we  show  that  a  Gibbs  sampling  algorithm  for  the  Dirichlet  process  mixture  approaches  a  hard  clustering  algorithm  in  the  limit,  and  further  that  the  resulting  algorithm  monotonically  minimizes  an  elegant  underlying  k-means-like  clustering  objective  that  includes  a  penalty  for  the  number  of  clusters.  We  generalize  this  analysis  to  the  case  of  clustering  multiple  data  sets  through  a  similar  asymptotic  argument  with  the  hierarchical  Dirichlet  process.  We  also  discuss  further  extensions  that  highlight  the  benefits  of  our  analysis:  i)  a  spectral  relaxation  involving  thresholded  eigenvectors,  and  ii)  a  normalized  cut  graph  clustering  algorithm  that  does  not  fix  the  number  of  clusters  in  the  graph.
0	A  unified  robust  regression  model  for  lasso  like  algorithms.  We  develop  a  unified  robust  linear  regression  model  and  show  that  it  is  equivalent  to  a  general  regularization  framework  to  encourage  sparse-like  structure  that  contains  group  Lasso  and  fused  Lasso  as  specific  examples.  This  provides  a  robustness  interpretation  of  these  widely  applied  Lasso-like  algorithms,  and  allows  us  to  construct  novel  generalizations  of  Lasso-like  algorithms  by  considering  different  uncertainty  sets.  Using  this  robustness  interpretation,  we  present  new  sparsity  results,  and  establish  the  statistical  consistency  of  the  proposed  regularized  linear  regression.  This  work  extends  a  classical  result  from  Xu  et  al.  (2010)  that  relates  standard  Lasso  with  robust  linear  regression  to  learning  problems  with  more  general  sparse-like  structures,  and  provides  new  robustness-based  tools  to  to  understand  learning  problems  with  sparse-like  structures.
0	Projection  free  online  optimization  with  stochastic  gradient  from  convexity  to  submodularity.  Online  optimization  has  been  a  successful  framework  for  solving  large-scale  problems  under  computational  constraints  and  partial  information.  Current  methods  for  online  convex  optimization  require  either  a  projection  or  exact  gradient  computation  at  each  step,  both  of  which  can  be  prohibitively  expensive  for  large-scale  applications.  At  the  same  time,  there  is  a  growing  trend  of  non-convex  optimization  in  machine  learning  community  and  a  need  for  online  methods.  Continuous  DR-submodular  functions,  which  exhibit  a  natural  diminishing  returns  condition,  have  recently  been  proposed  as  a  broad  class  of  non-convex  functions  which  may  be  efficiently  optimized.  Although  online  methods  have  been  introduced,  they  suffer  from  similar  problems.  In  this  work,  we  propose  Meta-Frank-Wolfe,  the  first  online  projection-free  algorithm  that  uses  stochastic  gradient  estimates.  The  algorithm  relies  on  a  careful  sampling  of  gradients  in  each  round  and  achieves  the  optimal  $O(  \sqrt{T})$  adversarial  regret  bounds  for  convex  and  continuous  submodular  optimization.  We  also  propose  One-Shot  Frank-Wolfe,  a  simpler  algorithm  which  requires  only  a  single  stochastic  gradient  estimate  in  each  round  and  achieves  an  $O(T^{2/3})$  stochastic  regret  bound  for  convex  and  continuous  submodular  optimization.  We  apply  our  methods  to  develop  a  novel  "lifting"  framework  for  the  online  discrete  submodular  maximization  and  also  see  that  they  outperform  current  state-of-the-art  techniques  on  various  experiments.
0	Learning  optimal  linear  regularizers.  We  present  algorithms  for  efficiently  learning  regularizers  that  improve  generalization.  Our  approach  is  based  on  the  insight  that  regularizers  can  be  viewed  as  upper  bounds  on  the  generalization  gap,  and  that  reducing  the  slack  in  the  bound  can  improve  performance  on  test  data.  For  a  broad  class  of  regularizers,  the  hyperparameters  that  give  the  best  upper  bound  can  be  computed  using  linear  programming.  Under  certain  Bayesian  assumptions,  solving  the  LP  lets  us  "jump"  to  the  optimal  hyperparameters  given  very  limited  data.  This  suggests  a  natural  algorithm  for  tuning  regularization  hyperparameters,  which  we  show  to  be  effective  on  both  real  and  synthetic  data.
0	Reinforcement  learning  as  one  big  sequence  modeling  problem.  Reinforcement  learning  (RL)  is  typically  concerned  with  estimating  single-step  policies  or  single-step  models,  leveraging  the  Markov  property  to  factorize  the  problem  in  time.  However,  we  can  also  view  RL  as  a  sequence  modeling  problem,  with  the  goal  being  to  predict  a  sequence  of  actions  that  leads  to  a  sequence  of  high  rewards.  Viewed  in  this  way,  it  is  tempting  to  consider  whether  powerful,  high-capacity  sequence  prediction  models  that  work  well  in  other  domains,  such  as  natural-language  processing,  can  also  provide  simple  and  effective  solutions  to  the  RL  problem.  To  this  end,  we  explore  how  RL  can  be  reframed  as  "one  big  sequence  modeling"  problem,  using  state-of-the-art  Transformer  architectures  to  model  distributions  over  sequences  of  states,  actions,  and  rewards.  Addressing  RL  as  a  sequence  modeling  problem  significantly  simplifies  a  range  of  design  decisions:  we  no  longer  require  separate  behavior  policy  constraints,  as  is  common  in  prior  work  on  offline  model-free  RL,  and  we  no  longer  require  ensembles  or  other  epistemic  uncertainty  estimators,  as  is  common  in  prior  work  on  model-based  RL.  All  of  these  roles  are  filled  by  the  same  Transformer  sequence  model.  In  our  experiments,  we  demonstrate  the  flexibility  of  this  approach  across  long-horizon  dynamics  prediction,  imitation  learning,  goal-conditioned  RL,  and  offline  RL.
0	Hypothesis  testing  using  pairwise  distances  and  associated  kernels.  We  provide  a  unifying  framework  linking  two  classes  of  statistics  used  in  two-sample  and  independence  testing:  on  the  one  hand,  the  energy  distances  and  distance  covariances  from  the  statistics  literature;  on  the  other,  distances  between  embeddings  of  distributions  to  reproducing  kernel  Hilbert  spaces  (RKHS),  as  established  in  machine  learning.  The  equivalence  holds  when  energy  distances  are  computed  with  semimetrics  of  negative  type,  in  which  case  a  kernel  may  be  defined  such  that  the  RKHS  distance  between  distributions  corresponds  exactly  to  the  energy  distance.  We  determine  the  class  of  probability  distributions  for  which  kernels  induced  by  semimetrics  are  characteristic  (that  is,  for  which  embeddings  of  the  distributions  to  an  RKHS  are  injective).  Finally,  we  investigate  the  performance  of  this  family  of  kernels  in  two-sample  and  independence  tests:  we  show  in  particular  that  the  energy  distance  most  commonly  employed  in  statistics  is  just  one  member  of  a  parametric  family  of  kernels,  and  that  other  choices  from  this  family  can  yield  more  powerful  tests.
0	Evaluation  of  open  information  extraction  methods  using  reuters  21578  database.  The  following  article  shows  the  precision,  the  recall  and  the  F1-measure  for  three  knowledge  extraction  methods  under  Open  Information  Extraction  paradigm.  These  methods  are:  ReVerb,  OLLIE  and  ClausIE.  For  the  calculation  of  these  three  measures,  a  representative  sample  of  Reuters-21578  was  used;  103  newswire  texts  were  taken  randomly  from  that  database.  A  big  discrepancy  was  observed,  after  analyzing  the  obtained  results,  between  the  expected  and  the  observed  precision  for  ClausIE.  In  order  to  save  the  observed  gap  in  ClausIE  precision,  a  simple  improvement  is  proposed  for  the  method.  Although  the  correction  improved  the  precision  of  Clausie,  ReVerb  turned  out  to  be  the  most  precise  method;  however  ClausIE  is  the  one  with  the  better  F1-measure.
0	Learning  optimal  tree  models  under  beam  search.  Retrieving  relevant  targets  from  an  extremely  large  target  set  under  computational  limits  is  a  common  challenge  for  information  retrieval  and  recommendation  systems.  Tree  models,  which  formulate  targets  as  leaves  of  a  tree  with  trainable  node-wise  scorers,  have  attracted  a  lot  of  interests  in  tackling  this  challenge  due  to  their  logarithmic  computational  complexity  in  both  training  and  testing.  Tree-based  deep  models  (TDMs)  and  probabilistic  label  trees  (PLTs)  are  two  representative  kinds  of  them.  Though  achieving  many  practical  successes,  existing  tree  models  suffer  from  the  training-testing  discrepancy,  where  the  retrieval  performance  deterioration  caused  by  beam  search  in  testing  is  not  considered  in  training.  This  leads  to  an  intrinsic  gap  between  the  most  relevant  targets  and  those  retrieved  by  beam  search  with  even  the  optimally  trained  node-wise  scorers.  We  take  a  first  step  towards  understanding  and  analyzing  this  problem  theoretically,  and  develop  the  concept  of  Bayes  optimality  under  beam  search  and  calibration  under  beam  search  as  general  analyzing  tools  for  this  purpose.  Moreover,  to  eliminate  the  discrepancy,  we  propose  a  novel  algorithm  for  learning  optimal  tree  models  under  beam  search.  Experiments  on  both  synthetic  and  real  data  verify  the  rationality  of  our  theoretical  analysis  and  demonstrate  the  superiority  of  our  algorithm  compared  to  state-of-the-art  methods.
0	Rethinking  the  value  of  transformer  components.  Transformer  becomes  the  state-of-the-art  translation  model,  while  it  is  not  well  studied  how  each  intermediate  component  contributes  to  the  model  performance,  which  poses  significant  challenges  for  designing  optimal  architectures.  In  this  work,  we  bridge  this  gap  by  evaluating  the  impact  of  individual  component  (sub-layer)  in  trained  Transformer  models  from  different  perspectives.  Experimental  results  across  language  pairs,  training  strategies,  and  model  capacities  show  that  certain  components  are  consistently  more  important  than  the  others.  We  also  report  a  number  of  interesting  findings  that  might  help  humans  better  analyze,  understand  and  improve  Transformer  models.  Based  on  these  observations,  we  further  propose  a  new  training  strategy  that  can  improves  translation  performance  by  distinguishing  the  unimportant  components  in  training.
0	Salamnet  at  semeval  2020  task  12  deep  learning  approach  for  arabic  offensive  language  detection.  This  paper  describes  SalamNET,  an  Arabic  offensive  language  detection  system  that  has  been  submitted  to  SemEval  2020  shared  task  12:  Multilingual  Offensive  Language  Identification  in  Social  Media.  Our  approach  focuses  on  applying  multiple  deep  learning  models  and  conducting  in  depth  error  analysis  of  results  to  provide  system  implications  for  future  development  considerations.  To  pursue  our  goal,  a  Recurrent  Neural  Network  (RNN),  a  Gated  Recurrent  Unit  (GRU),  and  Long-Short  Term  Memory  (LSTM)  models  with  different  design  architectures  have  been  developed  and  evaluated.  The  SalamNET,  a  Bi-directional  Gated  Recurrent  Unit  (Bi-GRU)  based  model,  reports  a  macro-F1  score  of  0.83%
0	Joint  learning  of  local  and  global  features  for  entity  linking  via  neural  networks.  A  system,  method  and  computer  program  product  for  disambiguating  one  or  more  entity  mentions  in  one  or  more  documents.  The  method  facilitates  the  simultaneous  linking  entity  mentions  in  a  document  based  on  convolution  neural  networks  and  recurrent  neural  networks  that  model  both  the  local  and  global  features  for  entity  linking.  The  framework  uses  the  capacity  of  convolution  neural  networks  to  induce  the  underlying  representations  for  local  contexts  and  the  advantage  of  recurrent  neural  networks  to  adaptively  compress  variable  length  sequences  of  predictions  for  global  constraints.  The  RNN  functions  to  accumulate  information  about  the  previous  entity  mentions  and/or  target  entities,  and  provide  them  as  the  global  constraints  for  the  linking  process  of  a  current  entity  mention.
0	Multisem  at  semeval  2020  task  3  fine  tuning  bert  for  lexical  meaning.  We  present  the  MULTISEM  systems  submitted  to  SemEval  2020  Task  3:  Graded  Word  Similarity  in  Context  (GWSC).  We  experiment  with  injecting  semantic  knowledge  into  pre-trained  BERT  models  through  fine-tuning  on  lexical  semantic  tasks  related  to  GWSC.  We  use  existing  semantically  annotated  datasets,  and  propose  to  approximate  similarity  through  automatically  generated  lexical  substitutes  in  context.  We  participate  in  both  GWSC  subtasks  and  address  two  languages,  English  and  Finnish.  Our  best  English  models  occupy  the  third  and  fourth  positions  in  the  ranking  for  the  two  subtasks.  Performance  is  lower  for  the  Finnish  models  which  are  mid-ranked  in  the  respective  subtasks,  highlighting  the  important  role  of  data  availability  for  fine-tuning.
0	Learning  to  few  shot  learn  across  diverse  natural  language  classification  tasks.  Pre-trained  transformer  models  have  shown  enormous  success  in  improving  performance  on  several  downstream  tasks.  However,  fine-tuning  on  a  new  task  still  requires  large  amounts  of  task-specific  labeled  data  to  achieve  good  performance.  We  consider  this  problem  of  learning  to  generalize  to  new  tasks,  with  a  few  examples,  as  a  meta-learning  problem.  While  meta-learning  has  shown  tremendous  progress  in  recent  years,  its  application  is  still  limited  to  simulated  problems  or  problems  with  limited  diversity  across  tasks.  We  develop  a  novel  method,  LEOPARD,  which  enables  optimization-based  meta-learning  across  tasks  with  a  different  number  of  classes,  and  evaluate  different  methods  on  generalization  to  diverse  NLP  classification  tasks.  LEOPARD  is  trained  with  the  state-of-the-art  transformer  architecture  and  shows  better  generalization  to  tasks  not  seen  at  all  during  training,  with  as  few  as  4  examples  per  label.  Across  17  NLP  tasks,  including  diverse  domains  of  entity  typing,  natural  language  inference,  sentiment  analysis,  and  several  other  text  classification  tasks,  we  show  that  LEOPARD  learns  better  initial  parameters  for  few-shot  learning  than  self-supervised  pre-training  or  multi-task  training,  outperforming  many  strong  baselines,  for  example,  yielding  14.6%  average  relative  gain  in  accuracy  on  unseen  tasks  with  only  4  examples  per  label.
0	Refining  source  representations  with  relation  networks  for  neural  machine  translation.  Although  neural  machine  translation  with  the  encoder-decoder  framework  has  achieved  great  success  recently,  it  still  suffers  drawbacks  of  forgetting  distant  information,  which  is  an  inherent  disadvantage  of  recurrent  neural  network  structure,  and  disregarding  relationship  between  source  words  during  encoding  step.  Whereas  in  practice,  the  former  information  and  relationship  are  often  useful  in  current  step.  We  target  on  solving  these  problems  and  thus  introduce  relation  networks  to  learn  better  representations  of  the  source.  The  relation  networks  are  able  to  facilitate  memorization  capability  of  recurrent  neural  network  via  associating  source  words  with  each  other,  this  would  also  help  retain  their  relationships.  Then  the  source  representations  and  all  the  relations  are  fed  into  the  attention  component  together  while  decoding,  with  the  main  encoder-decoder  framework  unchanged.  Experiments  on  several  datasets  show  that  our  method  can  improve  the  translation  performance  significantly  over  the  conventional  encoder-decoder  model  and  even  outperform  the  approach  involving  supervised  syntactic  knowledge.
0	A  corpus  based  study  of  edit  categories  in  featured  and  non  featured  wikipedia  articles.  In  this  paper,  we  present  a  study  of  the  collaborative  writing  process  in  Wikipedia.  Our  work  is  based  on  a  corpus  of  1,995  edits  obtained  from  891  article  revisions  in  the  English  Wikipedia.  We  propose  a  21-category  classification  scheme  for  edits  based  on  Faigley  and  Witte’s  (1981)  model.  Example  edit  categories  include  spelling  error  corrections  and  vandalism.  In  a  manual  multi-label  annotation  study  with  3  annotators,  we  obtain  an  inter-annotator  agreement  of  =  0.67.  We  further  analyze  the  distribution  of  edit  categories  for  distinct  stages  in  the  revision  history  of  10  featured  and  10  non-featured  articles.  Our  results  show  that  the  information  content  in  featured  articles  tends  to  become  more  stable  after  their  promotion.  On  the  opposite,  this  is  not  true  for  non-featured  articles.  We  make  the  resulting  corpus  and  the  annotation  guidelines  freely  available.  1
0	A  novel  distributional  approach  to  multilingual  conceptual  metaphor  recognition.  We  present  a  novel  approach  to  the  problem  of  multilingual  conceptual  metaphor  recognition.  Our  approach  extends  recent  work  in  conceptual  metaphor  discovery  by  combining  a  complex  methodology  for  facet-based  concept  induction  with  a  distributional  vector  space  model  of  linguistic  and  conceptual  metaphor.  In  the  evaluation  of  our  system  in  English,  Spanish,  Russian,  and  Farsi,  we  experiment  with  several  state-of-the-art  vector  space  models  and  demonstrate  a  clear  benefit  to  the  fine-grained  concept  representation  that  forms  the  basis  of  our  methodology  for  conceptual  metaphor  recognition.
0	A  supervised  learning  approach  towards  profiling  the  preservation  of  authorial  style  in  literary  translations.  Recently  there  has  been  growing  interest  in  the  application  of  approaches  from  the  text  classification  literature  to  fine-grained  problems  of  textual  stylometry.  This  paper  seeks  to  answer  a  question  which  has  concerned  the  translation  studies  community:  how  does  a  literary  translator’s  style  vary  across  their  translations  of  different  authors?  This  study  focuses  on  the  works  of  Constance  Garnett,  one  of  the  most  prolific  English-language  translators  of  Russian  literature,  and  uses  supervised  learning  approaches  to  analyse  her  translations  of  three  well-known  Russian  authors,  Ivan  Turgenev,  Fyodor  Dosteyevsky  and  Anton  Chekhov.  This  analysis  seeks  to  identify  common  linguistic  patterns  which  hold  for  all  of  the  translations  from  the  same  author.  Based  on  the  experimental  results,  it  is  ascertained  that  both  document-level  metrics  and  n-gram  features  prove  useful  for  distinguishing  between  authorial  contributions  in  our  translation  corpus  and  their  individual  efficacy  increases  further  when  these  two  feature  types  are  combined,  resulting  in  classification  accuracy  of  greater  than  90  %  on  the  task  of  predicting  the  original  author  of  a  textual  segment  using  a  Support  Vector  Machine  classifier.  The  ratio  of  nouns  and  pronouns  to  total  tokens  are  identified  as  distinguishing  features  in  the  document  metrics  space,  along  with  occurrences  of  common  adverbs  and  reporting  verbs  from  the  collection  of  n-gram  features.
0	Detecting  players  personality  behavior  with  any  effort  of  concealment.  We  introduce  a  novel  natural  language  processing  component  using  machine  learning  techniques  for  prediction  of  personality  behaviors  of  players  in  a  serious  game,  Land  Science,  where  players  act  as  interns  in  an  urban  planning  firm  and  discuss  in  groups  their  ideas  about  urban  planning  and  environmental  science  in  written  natural  language.  Our  model  learns  vector  space  representations  for  various  features  extraction.  In  order  to  apply  this  framework,  input  excerpts  must  be  classified  into  one  of  six  possible  personality  classes.  We  applied  this  personality  classification  task  using  several  machine  learning  algorithms,  such  as:  Naive  Bayes,  Support  Vector  Machines,  and  Decision  Tree.  Training  is  performed  on  a  relatively  dataset  of  manually  annotated  excerpts.  By  combining  these  features  spaces  from  psychology  and  computational  linguistics,  we  perform  and  evaluate  our  approaches  to  detecting  personality,  and  eventually  develop  a  classifier  that  is  nearly  83%  accurate  on  our  dataset.  Based  on  the  feature  analysis  of  our  models,  we  add  several  theoretical  contributions,  including  revealing  a  relationship  between  different  personality  behaviors  in  players'  writing.
0	Ssn  nlp  at  semeval  2020  task  4  text  classification  and  generation  on  common  sense  context  using  neural  networks.  Common  sense  validation  deals  with  testing  whether  a  system  can  differentiate  natural  language  statements  that  make  sense  from  those  that  do  not  make  sense.  This  paper  describes  the  our  approach  to  solve  this  challenge.  For  common  sense  validation  with  multi  choice,  we  propose  a  stacking  based  approach  to  classify  sentences  that  are  more  favourable  in  terms  of  common  sense  to  the  particular  statement.  We  have  used  majority  voting  classifier  methodology  amongst  three  models  such  as  Bidirectional  Encoder  Representations  from  Transformers  (BERT),  Micro  Text  Classification  (Micro  TC)  and  XLNet.  For  sentence  generation,  we  used  Neural  Machine  Translation  (NMT)  model  to  generate  explanatory  sentences.
0	Legal  yes  no  question  answering  system  using  case  role  analysis.  A  central  issue  of  yes/no  question  answering  is  the  usage  of  knowledge  source  given  a  question.  While  yes/no  question  answering  has  been  studied  for  a  long  time,  legal  yes/no  question  answering  largely  differs  from  other  domains.  The  most  distinguishing  characteristic  is  that  legal  issues  require  precise  analysis  of  roles  and  relationships  of  agents  named  in  sentences.  We  have  developed  a  yes/no  question  answering  system  for  answering  questions  about  a  statute  legal  domain.  Our  system  uses  case-role  analysis,  in  order  to  find  correspondences  of  roles  and  relationships  between  given  problem  sentences  and  knowledge  source  sentences.  We  applied  our  system  to  the  JURISIN’s  COLIEE  (Competition  on  Legal  Information  Extraction/Entailment)  2016  task.  Our  system  performance  was  better  than  systems  of  previous  task  participants  and  shared  first  place  in  current  year’s  task  in  Phase  Two.  This  result  shows  the  importance  of  the  points  described  above,  while  revealing  opportunities  to  continue  further  work  on  improving  our  system’s  accuracy.
0	Counterattack  detection  in  broadcast  soccer  videos  using  camera  motion  estimation.  This  paper  presents  a  new  method  for  counterattack  detection  using  estimated  camera  motion  and  evaluates  some  classification  methods  to  detect  this  event.  To  this  end,  video  is  partitioned  to  shots  and  view  type  of  each  shot  is  recognized  first.  Then,  relative  pan  of  the  camera  during  far-view  and  medium-view  shots  is  estimated.  After  weighting  of  pan  value  of  each  frame  according  to  the  type  of  shots,  the  video  is  partitioned  to  motion  segments.  Then,  motion  segments  are  refined  to  achieve  better  results.  Finally,  the  features  extracted  from  consecutive  motion  segments  are  investigated  for  counterattack  detection.  We  propose  two  methods  for  counterattack  detection:  (1)  rule-based  (heuristic  rules)  and  (2)  SVM-based.  Experiments  show  that  the  SVM  classifier  with  linear  or  RBF  kernel  results  in  the  best  results.
0	Linguistic  truth  valued  intuitionistic  fuzzy  algebra.  Based  on  18-element  linguistic  truth-valued  lattice  implication  algebra  L-v(9x2),  we  establish  a  45-element  linguistic  truth-valued  intuitionistic  fuzzy  algebra  LI18  and  obtain  its  properties  for  logic  reasoning.  The  method  not  only  can  better  express  both  the  comparable  information  and  incomparable  information  but  also  can  deal  with  the  problem  which  has  both  positive  evidence  and  negative  evidence  at  the  same  time.
0	A  solution  to  a  problem  of  d  lau  complete  classification  of  intervals  in  the  lattice  of  partial  boolean  clones.  The  following  natural  problem,  first  considered  by  D.  Lau,  has  been  tackled  by  several  authors  recently:  Let  C  be  a  total  clone  on  2  :=  {0,  1}.  Describe  the  interval  I(C)  of  all  partial  clones  on  2  whose  total  component  is  C.  We  establish  some  results  in  this  direction  and  combine  them  with  previous  ones  to  show  the  following  dichotomy  result:  For  every  total  clone  C  on  2,  the  set  I(C)  is  either  finite  or  of  continuum  cardinality.  1.  Preliminaries  Let  k  ≥  2  be  an  integer  and  let  k  be  a  k-element  set.  Without  loss  of  generality  we  assume  that  k  :=  {0,.  ..  ,  k  −  1}.  For  a  positive  integer  n,  an  n-ary  partial  function  on  k  is  a  map  f  :  dom  (f)  →  k  where  dom  (f)  is  a  subset  of  k  n  ,  called  the  domain  of  f.  Let  Par  (n)  (k)  denote  the  set  of  all  n-ary  partial  functions  on  k  and  let  Par(k)  :=  n≥1
0	Genetic  programming  with  one  point  crossover  and  subtree  mutation  for  effective  problem  solving  and  bloat  control.  Genetic  programming  (GP)  is  one  of  the  most  widely  used  paradigms  of  evolutionary  computation  due  to  its  ability  to  automatically  synthesize  computer  programs  and  mathematical  expressions.  However,  because  GP  uses  a  variable  length  representation,  the  individuals  within  the  evolving  population  tend  to  grow  rapidly  without  a  corresponding  return  in  fitness  improvement,  a  phenomenon  known  as  bloat.  In  this  paper,  we  present  a  simple  bloat  control  strategy  for  standard  tree-based  GP  that  achieves  a  one  order  of  magnitude  reduction  in  bloat  when  compared  with  standard  GP  on  benchmark  tests,  and  practically  eliminates  bloat  on  two  real-world  problems.  Our  proposal  is  to  substitute  standard  subtree  crossover  with  the  one-point  crossover  (OPX)  developed  by  Poli  and  Langdon  (Second  online  world  conference  on  soft  computing  in  engineering  design  and  manufacturing,  Springer,  Berlin  (1997)),  while  maintaining  all  other  GP  aspects  standard,  particularly  subtree  mutation.  OPX  was  proposed  for  theoretical  purposes  related  to  GP  schema  theorems,  however  since  it  curtails  exploration  during  the  search  it  has  never  achieved  widespread  use.  In  our  results,  on  the  other  hand,  we  are  able  to  show  that  OPX  can  indeed  perform  an  effective  search  if  it  is  coupled  with  subtree  mutation,  thus  combining  the  bloat  control  capabilities  of  OPX  with  the  exploration  provided  by  standard  mutation.
0	Adaptive  fuzzy  sliding  mode  controller  for  a  class  of  siso  nonlinear  time  delay  systems.  In  this  paper,  we  propose  an  adaptive  fuzzy  controller  for  a  class  of  nonlinear  SISO  time-delay  systems.  The  plant  model  structure  is  represented  by  a  Takagi---Sugeno  (T---S)  type  fuzzy  system.  The  T---S  fuzzy  model  parameters  are  adjusted  online.  The  proposed  algorithm  utilizes  the  sliding  surface  to  adjust  online  the  parameters  of  T---S  fuzzy  model.  The  controller  is  based  on  adjustable  T---S  fuzzy  parameters  model  and  sliding  mode  theory.  The  stability  analysis  of  the  closed-loop  system  is  based  on  the  Lyapunov  approach.  The  plant  state  follows  asymptotically  any  bounded  reference  signal.  Two  examples  have  been  used  to  check  performances  of  the  proposed  fuzzy  adaptive  control  scheme.
0	A  formal  framework  for  the  formalization  of  informal  requirements.  Systems?  requirements  are  usually  written  in  a  natural  language  since  it  generally  means  a  greater  understanding  among  the  various  stakeholders.  However,  using  an  informal  language  potentially  gives  rise  to  interpretation  problems,  which  are  to  be  resolved  prior  to  using  (automated)  verification  techniques.  This  article  tackles  an  important  issue  pertaining  to  requirement  engineering:  how  to  guide  and  help  requirements?  formalization?  In  order  to  support  the  formalization  process,  we  propose  a  methodology  based  on  a  formal  structure,  which  is  the  corner  stone  of  the  refinement  process.  The  operating  mode  of  the  refinement  process  is  highly  iterative:  the  aforementioned  structure  is  constructed  incrementally  until  its  validity  is  formally  obtained.  Although  this  process  is  formally  backed  up,  it  is  a  fundamentally  subjective  one,  which  means  that  interpretation  errors  can  still  occur.  In  case  of  errors,  it  is  essential  to  be  able  to  backtrack  refinements  until  an  interpretation  error  is  found.  This  is  why  we  require  that  each  refinement  be  associated  with  a  justification  which  may  subsequently  be  analyzed  in  case  an  error  occurred  during  the  verification  phase.  This  formalization  process  was  designed  to  be  used  alongside  an  (unspecific)  engineering  process,  in  charge  of  the  implementation.  Once  the  formalization  is  complete,  it  is  checked  against  the  implementation  using  testing  techniques,  or  directly  against  an  implementation  model  via  model-checking.
0	Some  new  shapley  2  tuple  linguistic  choquet  aggregation  operators  and  their  applications  to  multiple  attribute  group  decision  making.  In  this  paper,  we  investigate  the  multiple  attribute  group  decision  making  (MAGDM)  problems  with  2-tuple  linguistic  information.  Firstly,  motivated  by  the  ideas  of  Choquet  integral  and  Shapley  index,  we  propose  three  2-tuple  linguistic  aggregation  operators  called  Shapley  2-tuple  linguistic  Choquet  averaging  operator,  Shapley  2-tuple  linguistic  Choquet  geometric  operator  and  generalized  Shapley  2-tuple  linguistic  Choquet  averaging  operator.  Then  we  discuss  some  properties  of  these  operators,  such  as  idempotency,  monotonicity,  boundary  and  commutativity.  Secondly,  if  the  information  about  the  weights  of  decision  makers  (DMs)  and  attributes  is  incompletely  known,  we  build  two  models  to  determine  the  optimal  fuzzy  measures  on  DM  set  and  attribute  set,  respectively.  Furthermore,  we  develop  a  new  method  for  multiple  attribute  group  decision  making  under  2-tuple  linguistic  environment  based  on  the  proposed  operators.  Finally,  we  apply  the  developed  MAGDM  method  to  select  the  most  desirable  emergency  alternative  and  the  validity  of  the  developed  method  is  verified  by  comparing  the  evaluation  results  with  those  obtained  from  the  existing  2-tuple  correlated  aggregation  operators.
0	Fuzzy  wall  following  control  of  a  wheelchair.  This  paper  proposes  a  fuzzy  wall  following  control  approach  to  guide  the  wheelchair  robot  to  the  target  without  colliding  with  walls  and  obstacles  in  an  unknown  environment.  First,  the  control  problem  and  the  model  of  the  wheeled-chair  robot  are  described.  Then,  the  ultrasonic  sensors  are  used  to  detect  the  environmental  walls  and  obstacles.  Based  on  the  feedback  distance  to  the  environment,  the  control  action  is  separated  into  the  right  and  left  wall  following  control  regions  to  simplify  the  complexity  of  the  controller.  Afterward,  the  fuzzy  logic  control  rule  bases  are  presented  by  involving  human's  intelligent  operating  rules  of  the  wheelchairs.  Compared  with  model-based  control,  the  fuzzy  logic  control  has  more  implement  flexibility  and  high  integration  with  multiple  sensors.  Finally,  simulations  and  experiments  are  performed  to  demonstrate  the  feasibility  and  effectiveness  of  the  proposed  control  method.
0	A  time  weighted  average  based  paa  representation  for  time  series  symbolization.  The  operation  of  time  series  analysis  to  effectively  manage  the  large  amounts  of  data  with  high  dimensional  became  an  important  research  problem.  Choose  effective  and  scalable  algorithms  for  appropriate  representation  of  data  is  another  challenge.  A  lot  of  high-level  representations  of  the  time  series  have  been  proposed  for  data  extraction,  such  as  spectral  transfers,  wavelets,  piecewise  polynomial,  symbolic  models,  etc.  One  of  the  methods  is  Piecewise  Aggregate  Approximation  (PAA)  which  minimizes  dimensionality  by  the  mean  values  of  equal-sized  frames,  but  this  focus  on  mean  value  takes  into  consideration  only  the  central  tendency  and  not  the  dispersion  present  in  each  segment,  which  may  lead  to  some  important  patterns  being  missed  in  some  time  series  data  sets.  We  propose  method  based  on  Time-Weighted  Average  for  Symbolic  Aggregate  approximation  method  (TWA_SAX)  compare  its  performance  with  some  current  methods.  TWA_SAX  is  enables  raw  data  to  be  specifically  compared  to  the  minimized  representation  and,  at  the  same  time,  ensures  reduced  limits  to  Euclidean  distance.  It  can  be  utilized  to  generate  quicker,  more  precise  algorithms  for  similarity  searches,  which  improves  the  preciseness  of  time  series  representation  through  enabling  better  tightness  of  the  lower  bound.
0	How  much  and  where  to  use  manual  guidance  in  the  computational  detection  of  contours  for  histopathological  images.  There  have  been  important  developments  in  microscopy  hardware  in  the  last  decades  and  the  number  of  hospitals  to  deploy  such  tools  has  increased,  naturally  leading  to  a  very  high  number  of  microscopic  images  that  need  to  be  processed.  The  interest  in  automated  image  analysis  has  accordingly  grown  and  calls  for  a  close  collaboration  between  physicians  and  computer  scientists.  A  first  step  in  analyzing  histopathological  images  with  highly  irregularly  shaped  areas  of  interest,  for  instance,  is  represented  by  image  segmentation,  i.e.,  the  accurate  selection  of  the  glands  or  nuclei.  In  the  current  study,  manual  segmentation  is  applied  to  several  histopathological  images  and  the  resulting  files  are  next  used  for  measuring  how  close  automatic  contour  detection  methods  can  get  to  the  annotations  of  the  specialist.  More,  it  is  subsequently  investigated  whether  annotations  on  a  small  part  of  the  initial  file  can  help  in  automatically  marking  the  entire  image,  and  how  small  such  a  section  can  be  in  order  to  achieve  reasonable  results.  The  histogram  resemblance  between  the  cropped  image  and  the  complete  one  appears  to  have  an  important  impact  over  the  results.
0	Fractional  order  pid  control  of  a  chopper  fed  dc  motor  drive  using  a  novel  firefly  algorithm  with  dynamic  control  mechanism.  In  this  paper,  a  dynamic  control  mechanism  is  proposed  to  improve  the  firefly  algorithm’s  (FA)  rate  of  convergence  and  minimization  of  the  fitness  function.  The  dynamic  FA  (DFA)  dynamically  selects  the  best-performing  combinations  of  the  step  size  scaling  factor,  the  attractiveness  coefficient,  the  absorption  coefficient  and  the  population  size  along  the  complete  evolution  process  of  the  algorithm.  A  fractional-order  PID  (FOPID)  controller  based  on  DFA  is  proposed  to  improve  the  performance  of  a  chopper-fed  direct  current  motor  drive.  The  proposed  controller  is  used  in  speed  control  loop  to  improve  the  response.  To  illustrate  the  efficacy  of  the  DFA-based  FOPID,  we  compare  its  performance  with  those  based  on  the  conventional  FA,  genetic  algorithm,  particle  swarm  optimization,  artificial  bee  colony  algorithm  and  differential  evolution  algorithm.  The  simulation  results  and  analyses  show  the  effectiveness  of  the  proposed  method.
0	Fuzzy  topsis  and  fuzzy  vikor  in  selecting  green  suppliers  for  sponge  iron  and  steel  manufacturing.  Green  factors  toward  an  environmental  performance  of  suppliers  have  impeccable  importance  and  contributions.  The  research  study  is  carried  out  by  employing  fuzzy-based  multi-criteria  decision-making  processes  in  order  to  evaluate  the  environmental  performance  of  suppliers.  Firstly,  the  criteria  are  identified  for  assessing  the  environmental  performance  of  suppliers  and  secondly,  the  rating  of  selected  criteria  and  its  alternatives  (suppliers)  are  framed  as  per  the  expert’s  opinion,  by  combining  the  qualitative  criteria  through  fuzzy  technique  for  order  preference  by  similarity  to  ideal  solution  (TOPSIS)  and  fuzzy  VlseKriterijumska  Optimizacija  I  Kompromisno  Resenje  (VIKOR)  to  generate  an  overall  score  for  each  alternative.  The  supplier  obtained  the  highest  score  which  is  ranked  one.  It  means  that  the  supplier  has  the  highest  environmental  performance.  Fuzzy  TOPSIS  differentiates  between  benefit  (higher  the  better)  and  the  cost  (lower  the  better)  criteria.  VIKOR  method  selects  and  ranks  the  best  supplier  among  the  alternatives  in  supply  chain,  whereas  TOPSIS  method  chooses  the  best  alternatives  which  are  closest  to  the  positive  ideal  solution  and  the  worst  alternative  is  farthest  to  the  negative  ideal  solution.  Sensitivity  analysis  is  performed  to  analyze  the  effect  of  criteria  weights  on  environmental  performance.  Radar  diagrams  of  fuzzy  TOPSIS  and  fuzzy  VIKOR  are  developed  from  sensitivity  analysis  for  varied  criteria  weights.  A  case  study  has  been  adopted  on  a  small-scale  sponge  iron  and  steel  industry,  located  in  the  eastern  part  of  India.
0	Evaluation  of  classification  algorithms  for  intrusion  detection  system  a  review.  Intrusion  detection  is  one  of  the  most  critical  network  security  problems  in  the  technology  world.  Machine  learning  techniques  are  being  implemented  to  improve  the  Intrusion  Detection  System  (IDS).  In  order  to  enhance  the  performance  of  IDS,  different  classification  algorithms  are  applied  to  detect  various  types  of  attacks.  Choosing  a  suitable  classification  algorithm  for  building  IDS  is  not  an  easy  task.  The  best  method  is  to  test  the  performance  of  the  different  classification  algorithms.  This  paper  aims  to  present  the  result  of  evaluating  different  classification  algorithms  to  build  an  IDS  model  in  terms  of  confusion  matrix,  accuracy,  recall,  precision,  f-score,  specificity  and  sensitivity.  Nevertheless,  most  researchers  have  focused  on  the  confusion  matrix  and  accuracy  metric  as  measurements  of  classification  performance.  It  also  provides  a  detailed  comparison  with  the  dataset,  data  preprocessing,  number  of  features  selected,  feature  selection  technique,  classification  algorithms,  and  evaluation  performance  of  algorithms  described  in  the  intrusion  detection  system.
0	Development  and  evaluation  of  the  cascade  correlation  neural  network  and  the  random  forest  models  for  river  stage  and  river  flow  prediction  in  australia.  Accurately  predicting  river  flows  over  daily  timescales  is  considered  as  an  important  task  for  sustainable  management  of  freshwater  ecosystems,  agricultural  applications,  and  water  resources  management.  In  this  research  paper,  artificial  intelligence  (AI)  techniques,  namely  the  cascade  correlation  neural  networks  (CCNN)  and  the  random  forest  (RF)  models,  were  employed  in  daily  river  stage  and  river  flow  prediction  for  two  river  systems  (i.e.,  Dulhunty  River  and  Herbert  River)  in  Australia.  To  develop  the  CCNN  and  RF  models,  a  significant  3-day  antecedent  river  stage  and  river  flow  time  series  were  used.  80%  of  the  whole  data  were  used  for  model  training  and  the  remaining  20%  for  model  testing.  A  total  of  ten  different  model  structures  with  different  input  combinations  were  used  to  evaluate  the  optimal  model  in  the  training  phase,  and  the  results  were  analyzed  using  statistical  metrics  including  the  root  mean  square  error  (RMSE),  Nash–Sutcliffe  coefficient  (NS),  Willmott’s  index  of  agreement  (WI),  and  Legate  and  McCabe’s  index  (ELM)  in  the  testing  phase.  The  inter-comparison  of  CCNN  and  RF  models  for  both  river  systems  showed  that  the  CCNN  model  was  able  to  generate  a  more  accurate  prediction  of  the  river  stage  and  river  flow  compared  to  the  RF  model.  Due  to  hydro-geographic  differences  leading  to  a  different  underlying  historical  data  characteristics,  the  optimal  CCNN’s  performance  for  the  Dulhunty  River  was  found  to  be  most  accurate,  in  terms  of  ELM = 0.779,  WI = 0.964,  and  ENS = 0.862  versus  0.775,  0.968,  and  0.885  for  the  Herbert  River.  Following  the  performance  accuracies,  the  authors  ascertained  that  the  CCNN  model  can  be  taken  as  a  preferred  data  intelligent  tool  for  river  stage  and  river  flow  prediction.
0	Averaging  aggregation  functions  based  on  inclusion  exclusion  integrals.  The  inclusion-exclusion  integral,  defined  with  respect  to  a  fuzzy  measure  and  interaction  operator,  generalizes  the  Choquet  integral.  Here  we  look  at  some  of  its  interesting  properties  and  investigate  the  conditions  on  the  interaction  operator  which  ensure  the  integral  is  averaging.  We  present  some  illustrative  examples.
0	Application  of  optimization  technique  in  she  controlled  multilevel  inverter.  Application  of  Particle  Swarm  Optimization  algorithm  for  finding  the  optimal  solution  of  switching  angles  in  a  3-ph  seven  level  Cascaded  Multilevel  Inverter  (CMLI)  with  unequal  DC  sources  for  elimination  of  lower  order  harmonics  is  presented  in  this  paper.  For  Cascaded  Multilevel  Inverter,  the  common  switching  technologies  such  as  low  frequency  Selective  Harmonic  Elimination-Pulse  Width  Modulation  (SHE-PWM)  technique  and  Optimal  Minimization  of  Total  Harmonic  Distortion  (OMTHD)  techniques  are  applied  to  obtain  staircase  voltage  waveform.  SHE-PWM  based  approach  is  used  to  maintain  desired  fundamental  component  of  output  voltage  and  to  reduce  specific  lower  order  odd  harmonics.  Whereas  OMTHD  is  used  to  minimize  the  overall  THD.  The  lower  order  odd  harmonics  cannot  be  eliminated  easily  as  it  contains  non  linear  transcendental  equations  derived  from  Fourier  series  expansion.  Solutions  to  these  equations  are  complex  and  time  consuming.  In  this  paper,  to  solve  these  nonlinear  equations  PSO  algorithm  has  been  implemented.  Simulation  of  seven  level  CMLI  with  unequal  DC  sources  with  proposed  algorithm  in  MATLAB/Simulink  environment  is  performed  to  validate  the  results.
0	A  genetic  based  effective  approach  to  path  planning  of  autonomous  underwater  glider  with  upstream  current  avoidance  in  variable  oceans.  In  this  work,  an  exponential  effective  function  (EEF)  is  developed  as  fitness  function  applied  in  a  hybrid-Genetic  Algorithm  (hybrid-GA)  to  propose  a  genetic-based  effective  approach  to  the  glider  path-planning  of  ocean-sampling  mission  in  variable  oceans.  The  proposed  EEF  is  such  an  objective  function  that  is  able  to  be  implemented  in  optimization  algorithm  such  as  Genetic  Algorithm  (GA)  for  evaluation  of  the  fittest  path.  In  consideration  of  the  glider  path-planning  problem  (GPP),  two  motivations  are  driven  by  the  proposed  approach  to  the  glider  path-planning  in  discovery  of:  (1)  a  reachable  path  with  the  upstream-current  avoidance  (UCA)  in  variable  oceans;  (2)  an  efficient  path  for  the  glider  ocean-sampling  mission.  The  exponential  combination  of  the  glider  motion  and  current  effects  as  well  as  the  cruising  distance  benefits  the  path  in  terms  of  reachability  and  efficiency.  The  reachability  is  the  first  motivation  to  discover  a  reachable  path  implemented  by  the  scheme  of  UCA,  while  the  efficiency  is  the  second  motivation  to  shorten  the  cruising  distance.  Meanwhile,  the  stabilized  path  solution  is  accomplished  by  hybrid-GA.  In  variable  oceans,  currents  severely  impact  the  path  solution  and  lead  the  global  optimum  to  absence.  Therefore,  alternative  is  to  discover  an  optimal  path  with  the  minimum  upstream-current  sub-paths  to  approximate  the  minimal  cruising  distance  in  the  condition  that  the  discovered  cruising  distance  should  be  less  than  the  glider  cruising  range.  To  deeply  analyze  the  path  reachability,  two  theorems  are  developed  to  verify  the  conditions  of  the  downstream-current  angle  (DCA).  To  evaluate  the  path-planning  performances,  6  state-of-the-art  fitness  functions  are  studied  and  used  to  make  a  fair  comparison  with  the  EEF  in  hybrid-GA.  First  of  all,  112  scenarios  are  created  in  the  restricted  random  current  variations  (RRCV).  Secondly,  21  scenarios  are  created  in  the  near-real  Kuroshio  Current  of  east  Taiwan  (KCET)  introducing  from  an  ocean  prediction  model.  These  scenarios  are  designed  to  evaluate  fairly  the  EEF  in  hybrid-GA.  Numeric  results  show  that  whether  the  RRCV  or  the  KCET,  the  proposed  EEF  indeed  is  able  to  discover  the  optimal  path  with  the  benefits  of  reachability  and  efficiency.  Therefore,  the  proposed  genetic-based  effective  approach  is  well  developed  to  solve  the  GPP  in  variable  oceans.
0	Event  correlation  in  the  integrated  cyber  physical  security  system.  The  paper  considers  approaches  to  the  integration  of  heterogeneous  data  sources  and  correlations  of  disparate  security  events  for  protection  against  cyber-physical  attacks.  The  architecture  of  the  proposed  integrated  security  system,  milestones  and  methods  of  data  correlation,  as  well  as  examples  of  the  application  of  the  system  are  suggested.
0	A  ea  and  aca  based  qos  multicast  routing  algorithm  with  multiple  constraints  for  ad  hoc  networks.  With  the  rapid  development  of  communication  networks,  the  quality  of  service  (QoS)  on  such  networks  has  become  an  important  research  topic.  With  regard  to  ad  hoc  networks,  this  paper  presents  an  evolutionary  algorithm  (EA)  and  an  ant  colony  algorithm  (ACA)  to  serve  as  the  basis  for  a  QoS  multicast  routing  algorithm  (EA-ACA-QMRA).  This  algorithm  combines  the  rapid  global  search  capability  and  robustness  of  EAs  with  the  pheromone  feedback  factors  of  ACAs  while  accounting  for  multiple  constraints,  including  constraints  related  to  delay,  delay  jitter,  packet  delivery  ratio,  bandwidth  and  cost.  For  the  case  of  self-adapting  ad  hoc  networks  in  particular,  our  new  algorithm  is  far  superior  to  traditional  ACAs.  Our  experimental  results  show  that  the  EA-ACA-QMRA  can  address  multiple  constraints  in  the  QoS  multicast  routing  problem  and  can  achieve  higher  accuracy  and  faster  convergence  than  can  traditional  ACAs  in  terms  of  the  end-to-end  delay  and  packet  delivery  ratio.  The  proposed  algorithm  provides  an  effective  means  of  solving  the  QoS  multicast  routing  problem  for  ad  hoc  networks,  and  it  is  better  than  the  traditional  methods  at  avoiding  network  congestion.
0	Difference  co  occurrence  matrix  using  bp  neural  network  for  fingerprint  liveness  detection.  With  the  growing  use  of  fingerprint  identification  systems  in  recent  years,  preventing  fingerprint  identification  systems  from  being  spoofed  by  artificial  fake  fingerprints  has  become  a  critical  problem.  In  this  paper,  we  put  forward  a  novel  method  to  detect  fingerprint  liveness  based  on  BP  neural  network,  which  is  used  for  the  first  time  in  the  fingerprint  liveness  detection.  Moreover,  different  from  traditional  detection  methods,  we  propose  a  scheme  to  construct  the  input  data  and  corresponding  category  labels.  More  effective  and  efficient  texture  features  of  fingerprints,  which  are  used  as  the  input  data  of  the  BP  neural  network,  are  computed  to  improve  classification  performance  and  obtain  a  better  pre-trained  network  model.  After  a  variety  of  preprocessing  operations  and  image  compression  operations,  gradient  values  in  the  horizontal  and  vertical  directions  are  computed  by  using  Laplacian  operator,  and  difference  co-occurrence  matrices  are  constructed  from  the  obtained  gradient  values.  Then,  the  input  data  of  neural  network  model  are  built  based  on  two  DCMs.  The  pre-trained  neural  network  models  with  diverse  neuron  nodes  are  learnt.  Different  experiments  based  on  different  parameters  for  the  BP  neural  network  have  been  conducted.  Finally,  classification  accuracy  of  testing  fingerprints  is  predicted  based  on  the  pre-trained  networks.  Experimental  results  on  the  LivDet  2013  show  that  the  classification  performance  of  our  proposed  method  is  effective  and  meanwhile  provides  a  better  detection  accuracy  compared  with  the  majority  of  previously  published  results.
0	Circular  convolution  parallel  extreme  learning  machine  for  modeling  boiler  efficiency  for  a  300  mw  cfbb.  Aiming  at  the  accuracy  prediction  of  combustion  efficiency  for  a  300 MW  circulating  fluidized  bed  boiler  (CFBB),  a  circular  convolution  parallel  extreme  learning  machine  (CCPELM)  which  is  a  double  parallel  forward  neural  network  is  proposed.  In  CCPELM,  the  circular  convolution  theory  is  introduced  to  map  the  hidden  layer  information  into  higher-dimension  information;  in  addition,  the  input  layer  information  is  directly  transmitted  to  its  output  layer,  which  makes  the  whole  network  into  a  double  parallel  construction.  In  this  paper,  CCPELM  is  applied  to  establish  a  model  for  boiler  efficiency  though  data  samples  collected  from  a  300 MW  CFBB.  Some  comparative  simulation  results  with  other  neural  network  models  show  that  CCPELM  owns  very  high  prediction  accuracy  with  fast  learning  speed  and  very  good  repeatability  in  learning  ability.
0	On  soft  computing  with  random  fuzzy  sets  in  econometrics  and  machine  learning.  Several  typical  econometric  analyses,  namely  fuzzy  coalitional  games,  regression  for  causal  inference,  statistical  quality  control,  and  prediction  in  machine  learning,  are  examined  in  this  paper  to  point  out  that  soft  computing  components  such  as  fuzzy  theory  and  random  set  theory  have  a  major  role  to  play.  The  examination  of  these  statistical  analyses  suggests  an  extended  use  of  random  fuzzy  sets,  based  upon  theory  of  random  fuzzy  sets,  to  improve  empirical  research.
0	Optimization  of  milling  operation  using  genetic  and  pso  algorithm.  Metal  cutting  is  one  of  the  important  and  widely  used  manufacturing  processes  in  engineering  industries.  Optimizing  the  machining  parameters  has  become  an  essential  one  in  order  to  be  competitive  and  to  meet  customer  demands  quickly.  For  this  purpose  several  optimization  techniques  are  used.  Among  those  techniques  Particle  Swarm  Optimization  and  Genetic  Algorithm  is  used  in  this  paper  because  of  its  better  ability.  A  genetic  algorithm  (GA)  is  a  search  heuristic  that  mimics  the  process  of  natural  evolution.  This  heuristic  is  routinely  used  to  generate  useful  solutions  to  optimization  and  search  problems.  Genetic  algorithms  belong  to  the  larger  class  of  Evolutionary  Algorithms  (EA),  which  generate  solutions  to  optimization  problems  using  techniques  inspired  by  natural  evolution,  such  as  inheritance,  mutation,  selection,  and  crossover.  Particle  Swarm  Optimization  (PSO)  is  a  computational  method  that  optimizes  a  problem  by  iteratively  trying  to  improve  a  candidate  solution  with  regard  to  a  given  measure  of  quality.  Such  methods  are  commonly  known  as  metaheuristics  as  they  make  few  or  no  assumptions  about  the  problem  being  optimized  and  can  search  very  large  spaces  of  candidate  solutions.  These  techniques  are  used  to  optimize  the  machining  parameters  like  depth  of  cut,  feed  rate  and  cutting  speed.  This  will  help  in  better  optimization  of  milling  operation.  The  developed  techniques  are  evaluated  with  a  case  study
0	Evaluation  of  the  impact  of  state  s  administrative  efforts  on  tax  potential  using  sugeno  type  fuzzy  inference  method.  Evaluation  of  the  impact  of  state’s  administrative  efforts  on  tax  potential  via  Sugeno-type  fuzzy  inference  method  has  been  investigated  in  the  article.  For  this  purpose,  input  data  of  the  model  has  been  fuzzified  on  the  base  of  expert  knowledge  via  different  membership  functions,  and  the  output  function  has  been  evaluated  on  the  base  of  the  determined  rules.  Effective  model-specific  parameters  have  been  selected  in  order  to  calculate  the  output  function.  The  results  obtained  by  Sugeno-type  fuzzy  inference  method  have  been  compared  with  the  results  evaluated  via  the  Mamdani-type  fuzzy  inference  method.
0	Survey  on  soft  computing.  Soft  computing  triggers  a  revolutionary  change  in  the  field  of  computer  science  and  technology.  How  did  soft  computing  evolve?  What  is  soft  computing  application  situation?  Which  fields  are  soft  computing  widely  used?  Which  fields  does  soft  computing  still  need  popularizing?  All  of  these  problems  resort  to  the  survey  on  the  development  of  soft  computing.
0	Feature  extraction  based  on  graph  discriminant  embedding  and  its  applications  to  face  recognition.  Graph  embedding-based  learning  methods  have  been  widely  employed  to  reduce  the  dimensionality  of  high-dimensional  data,  while  how  to  construct  adjacency  graphs  to  discover  the  essential  structure  of  the  data  is  the  key  problem  in  these  methods.  In  this  paper,  we  present  a  novel  algorithm  called  graph  discriminant  embedding  (GDE)  for  feature  extraction  and  recognition.  GDE  combines  local  information  and  label  information  of  data  points  to  construct  two  neighbor  graphs,  which  help  to  pull  the  same-class  samples  nearer  and  nearer  and  repel  the  not-same-class  samples  farther  and  farther  when  they  are  projected  onto  a  feature  subspace.  Significantly  differing  from  most  of  the  other  graph  embedding  methods,  GDE  does  not  only  emphasize  the  importance  of  the  nearby  points  but  also  enhance  the  importance  of  the  distant  points  which  may  have  potential  advantages  for  classification.  Experimental  results  on  the  AR,  CMU  PIE  and  FERET  face  databases  demonstrate  the  effectiveness  of  the  proposed  algorithm.
0	Multi  objective  performance  optimization  of  a  probabilistic  similarity  dissimilarity  based  broadcasting  scheme  for  mobile  ad  hoc  networks  in  disaster  response  scenarios.  Communications  among  crewmembers  in  rescue  teams  and  among  victims  are  crucial  to  relieve  the  consequences  and  damages  of  a  disaster  situation.  A  common  communication  system  for  establishing  real  time  communications  between  the  elements  (victims,  crewmembers,  people  living  in  the  vicinity  of  the  disaster  scenario,  among  others)  involved  in  a  disaster  scenario  is  required.  Ad  hoc  networks  have  been  envisioned  for  years  as  a  possible  solution.  They  allow  users  to  establish  decentralized  communications  quickly  and  using  common  devices  like  mobile  phones.  Broadcasting  is  the  main  mechanism  used  to  disseminate  information  in  all-to-all  fashion  in  ad  hoc  networks.  The  objective  of  this  paper  is  to  optimize  a  broadcasting  scheme  based  on  similarity/dissimilarity  coefficient  designed  for  disaster  response  scenarios  through  a  multi-objective  optimization  problem  in  which  several  performance  metrics  such  as  reachability,  number  of  retransmissions  and  delay  are  optimized  simultaneously.
0	A  comparative  analysis  of  evolutionary  and  memetic  algorithms  for  community  detection  from  signed  social  networks.  To  detect  communities  in  signed  networks  consisting  of  both  positive  and  negative  links,  two  new  evolutionary  algorithms  (EAs)  and  two  new  memetic  algorithms  (MAs)  are  proposed  and  compared.  Furthermore,  two  measures,  namely  the  improved  modularity  Q  and  the  improved  modularity  density  D-value,  are  used  as  the  objective  functions.  The  improved  measures  not  only  preserve  all  properties  of  the  original  ones,  but  also  have  the  ability  of  dealing  with  negative  links.  Moreover,  D-value  can  also  control  the  partition  to  different  resolutions.  To  fully  investigate  the  performance  of  these  four  algorithms  and  the  two  objective  functions,  benchmark  social  networks  and  various  large-scale  randomly  generated  signed  networks  are  used  in  the  experiments.  The  experimental  results  not  only  show  the  capability  and  high  efficiency  of  the  four  algorithms  in  successfully  detecting  communities  from  signed  networks,  but  also  indicate  that  the  two  MAs  outperform  the  two  EAs  in  terms  of  the  solution  quality  and  the  computational  cost.  Moreover,  by  tuning  the  parameter  in  D-value,  the  four  algorithms  have  the  multi-resolution  ability.
0	Using  fractional  order  accumulation  to  reduce  errors  from  inverse  accumulated  generating  operator  of  grey  model.  To  smooth  the  randomness,  a  grey  forecasting  model  is  formulated  using  the  data  of  accumulating  generation  operator  (AGO)  rather  than  original  data.  Then  the  inverse  accumulating  generation  operator  (IAGO)  is  applied  to  find  the  predicted  values  of  original  data.  It  is  proved  that  the  errors  from  IAGO  are  affected  by  the  order  number  of  AGO.  To  achieve  an  accurate  prediction,  GM(2,1),  which  stands  for  one-variable  and  second-order  differential  equation,  has  been  improved  by  means  of  fractional  order  AGO.  Finally,  four  real  data  sets  are  imported  for  comparing  the  performance  of  the  developed  GM(2,1)  with  several  other  grey  models,  such  as  traditional  GM(2,1)  and  GM(1,1).  The  simulation  results  show  that  optimized  GM(2,1)  has  higher  performances  not  only  on  model  fitting  but  also  on  forecasting.
0	Consensus  via  penalty  functions  for  decision  making  in  ensembles  in  fuzzy  rule  based  classification  systems.  Display  Omitted  A  consensus  method  via  penalty  functions  for  decision  making  in  ensembles  of  fuzzy  rule-based  classification  systems  is  introduced.Overlap  indices  are  built  using  overlap  functions.A  method  for  constructing  confidence  and  support  measures  from  overlap  indices  is  presented.A  new  fuzzy  rule  mechanism  is  proposed,  considering  different  overlap  indices,  which  generalizes  the  classical  methods.An  example  of  a  generation  of  fuzzy  rule-based  ensembles  and  the  decision  making  by  consensus  via  penalty  functions  is  presented.  The  aim  of  this  paper  is  to  propose  a  consensus  method  via  penalty  functions  for  decision  making  in  ensembles  of  fuzzy  rule-based  classification  systems  (FRBCSs).  For  that,  we  first  introduce  a  method  based  on  overlap  indices  for  building  confidence  and  support  measures,  which  are  usually  used  to  evaluate  the  degree  of  certainty  or  interest  of  a  certain  association  rule.  Those  overlap  indices  (a  generalizations  of  the  Zadeh's  consistency  index  between  two  fuzzy  sets)  are  built  using  overlap  functions,  which  are  a  special  kind  of  non  necessarily  associative  aggregation  functions  proposed  for  applications  related  to  the  overlap  problem  and/or  when  the  associativity  property  is  not  demanded.  Then,  we  introduce  a  new  FRM  for  the  FRBCS,  considering  different  overlap  indices,  which  generalizes  the  classical  methods.  By  considering  several  overlap  indices  and  aggregation  functions,  we  generate  fuzzy  rule-based  ensembles,  providing  different  results.  For  the  decision  making  related  to  the  selection  of  the  best  class,  we  introduce  a  consensus  method  for  classification,  based  on  penalty  functions.  We  also  present  theoretical  results  related  to  the  developed  methods.  A  detailed  example  of  a  generation  of  fuzzy  rule-based  ensembles  based  on  the  proposed  approach,  and  the  decision  making  by  consensus  via  penalty  functions,  is  presented.
0	Fuzzy  integral  based  elm  ensemble  for  imbalanced  big  data  classification.  Big  data  are  data  too  big  to  be  handled  and  analyzed  by  traditional  software  tools,  big  data  can  be  characterized  by  five  V’s  features:  volume,  velocity,  variety,  value  and  veracity.  However,  in  the  real  world,  some  big  data  have  another  feature,  i.e.,  class  imbalanced,  such  as  e-health  big  data,  credit  card  fraud  detection  big  data  and  extreme  weather  forecast  big  data  are  all  class  imbalanced.  In  order  to  deal  with  the  problem  of  classifying  binary  imbalanced  big  data,  based  on  MapReduce,  non-iterative  learning,  ensemble  learning  and  oversampling,  this  paper  proposed  an  promising  algorithm  which  includes  three  stages.  Firstly,  for  each  positive  instance,  its  enemy  nearest  neighbor  is  found  with  MapReduce,  and  p  positive  instances  are  randomly  generated  with  uniform  distribution  in  its  enemy  nearest  neighbor  hypersphere,  i.e.,  oversampling  p  positive  instances  within  the  hypersphere.  Secondly,  l  balanced  data  subsets  are  constructed  and  l  classifiers  are  trained  on  the  constructed  data  subsets  with  an  non-iterative  learning  approach.  Finally,  the  trained  classifiers  are  integrated  by  fuzzy  integral  to  classify  unseen  instances.  We  experimentally  compared  the  proposed  algorithm  with  three  related  algorithms:  SMOTE,  SMOTE+RF-BigData  and  MR-V-ELM,  and  conducted  a  statistical  analysis  on  the  experimental  results.  The  experimental  results  and  the  statistical  analysis  demonstrate  that  the  proposed  algorithm  outperforms  the  other  three  methods.
0	High  quality  correspondence  and  segmentation  estimation  for  dual  lens  smart  phone  portraits.  Estimating  correspondence  between  two  images  and  extracting  the  foreground  object  are  two  challenges  in  computer  vision.  With  dual-lens  smart  phones,  such  as  iPhone  7Plus  and  Huawei  P9,  coming  into  the  market,  two  images  of  slightly  different  views  provide  us  new  information  to  unify  the  two  topics.  We  propose  a  joint  method  to  tackle  them  simultaneously  via  a  joint  fully  connected  conditional  random  field  (CRF)  framework.  The  regional  correspondence  is  used  to  handle  textureless  regions  in  matching  and  make  our  CRF  system  computationally  efficient.  Our  method  is  evaluated  over  2,000  new  image  pairs,  and  produces  promising  results  on  challenging  portrait  images.
0	Asist  automatic  semantically  invariant  scene  transformation.  We  present  ASIST,  a  technique  for  transforming  point  clouds  by  replacing  objects  with  their  semantically  equivalent  counterparts.  Transformations  of  this  kind  have  applications  in  virtual  reality,  repair  of  fused  scans,  and  robotics.  ASIST  is  based  on  a  unified  formulation  of  semantic  labeling  and  object  replacement;  both  result  from  minimizing  a  single  objective.  We  present  numerical  tools  for  the  efficient  solution  of  this  optimization  problem.  The  method  is  experimentally  assessed  on  new  datasets  of  both  synthetic  and  real  point  clouds,  and  is  additionally  compared  to  two  recent  works  on  object  replacement  on  data  from  the  corresponding  papers.
0	Ava  a  video  dataset  of  spatio  temporally  localized  atomic  visual  actions.  This  paper  introduces  a  video  dataset  of  spatio-temporally  localized  Atomic  Visual  Actions  (AVA).  The  AVA  dataset  densely  annotates  80  atomic  visual  actions  in  430  15-minute  video  clips,  where  actions  are  localized  in  space  and  time,  resulting  in  1.58M  action  labels  with  multiple  labels  per  person  occurring  frequently.  The  key  characteristics  of  our  dataset  are:  (1)  the  definition  of  atomic  visual  actions,  rather  than  composite  actions;  (2)  precise  spatio-temporal  annotations  with  possibly  multiple  annotations  for  each  person;  (3)  exhaustive  annotation  of  these  atomic  actions  over  15-minute  video  clips;  (4)  people  temporally  linked  across  consecutive  segments;  and  (5)  using  movies  to  gather  a  varied  set  of  action  representations.  This  departs  from  existing  datasets  for  spatio-temporal  action  recognition,  which  typically  provide  sparse  annotations  for  composite  actions  in  short  video  clips.  We  will  release  the  dataset  publicly.    AVA,  with  its  realistic  scene  and  action  complexity,  exposes  the  intrinsic  difficulty  of  action  recognition.  To  benchmark  this,  we  present  a  novel  approach  for  action  localization  that  builds  upon  the  current  state-of-the-art  methods,  and  demonstrates  better  performance  on  JHMDB  and  UCF101-24  categories.  While  setting  a  new  state  of  the  art  on  existing  datasets,  the  overall  results  on  AVA  are  low  at  15.6%  mAP,  underscoring  the  need  for  developing  new  approaches  for  video  understanding.
0	Large  scale  digital  prostate  pathology  image  analysis  combining  feature  extraction  and  deep  neural  network.  Histopathological  assessments,  including  surgical  resection  and  core  needle  biopsy,  are  the  standard  procedures  in  the  diagnosis  of  the  prostate  cancer.  Current  interpretation  of  the  histopathology  images  includes  the  determination  of  the  tumor  area,  Gleason  grading,  and  identification  of  certain  prognosis-critical  features.  Such  a  process  is  not  only  tedious,  but  also  prune  to  intra/inter-observe  variabilities.  Recently,  FDA  cleared  the  marketing  of  the  first  whole  slide  imaging  system  for  digital  pathology.  This  opens  a  new  era  for  the  computer  aided  prostate  image  analysis  and  feature  extraction  based  on  the  digital  histopathology  images.  In  this  work,  we  present  an  analysis  pipeline  that  includes  localization  of  the  cancer  region,  grading,  area  ratio  of  different  Gleason  grades,  and  cytological/architectural  feature  extraction.  The  proposed  algorithm  combines  the  human  engineered  feature  extraction  as  well  as  those  learned  by  the  deep  neural  network.  Moreover,  the  entire  pipeline  is  implemented  to  directly  operate  on  the  whole  slide  images  produced  by  the  digital  scanners  and  is  therefore  potentially  easy  to  translate  into  clinical  practices.  The  algorithm  is  tested  on  368  whole  slide  images  from  the  TCGA  data  set  and  achieves  an  overall  accuracy  of  75%  in  differentiating  Gleason  3+4  with  4+3  slides.
0	Make  your  bone  great  again  a  study  on  osteoporosis  classification.  Osteoporosis  can  be  identified  by  looking  at  2D  x-ray  images  of  the  bone.  The  high  degree  of  similarity  between  images  of  a  healthy  bone  and  a  diseased  one  makes  classification  a  challenge.  A  good  bone  texture  characterization  technique  is  essential  for  identifying  osteoporosis  cases.  Standard  texture  feature  extraction  techniques  like  Local  Binary  Pattern  (LBP),  Gray  Level  Co-occurrence  Matrix  (GLCM)  have  been  used  for  this  purpose.  In  this  paper,  we  draw  a  comparison  between  deep  features  extracted  from  convolution  neural  network  against  these  traditional  features.  Our  results  show  that  deep  features  have  more  discriminative  power  as  classifiers  trained  on  them  always  outperform  the  ones  trained  on  traditional  features.
0	Delving  deep  into  rectifiers  surpassing  human  level  performance  on  imagenet  classification.  Rectified  activation  units  (rectifiers)  are  essential  for  state-of-the-art  neural  networks.  In  this  work,  we  study  rectifier  neural  networks  for  image  classification  from  two  aspects.  First,  we  propose  a  Parametric  Rectified  Linear  Unit  (PReLU)  that  generalizes  the  traditional  rectified  unit.  PReLU  improves  model  fitting  with  nearly  zero  extra  computational  cost  and  little  overfitting  risk.  Second,  we  derive  a  robust  initialization  method  that  particularly  considers  the  rectifier  nonlinearities.  This  method  enables  us  to  train  extremely  deep  rectified  models  directly  from  scratch  and  to  investigate  deeper  or  wider  network  architectures.  Based  on  our  PReLU  networks  (PReLU-nets),  we  achieve  4.94%  top-5  test  error  on  the  ImageNet  2012  classification  dataset.  This  is  a  26%  relative  improvement  over  the  ILSVRC  2014  winner  (GoogLeNet,  6.66%).  To  our  knowledge,  our  result  is  the  first  to  surpass  human-level  performance  (5.1%,  Russakovsky  et  al.)  on  this  visual  recognition  challenge.
0	Visual  recognition  by  counting  instances  a  multi  instance  cardinality  potential  kernel.  Many  visual  recognition  problems  can  be  approached  by  counting  instances.  To  determine  whether  an  event  is  present  in  a  long  internet  video,  one  could  count  how  many  frames  seem  to  contain  the  activity.  Classifying  the  activity  of  a  group  of  people  can  be  done  by  counting  the  actions  of  individual  people.  Encoding  these  cardinality  relationships  can  reduce  sensitivity  to  clutter,  in  the  form  of  irrelevant  frames  or  individuals  not  involved  in  a  group  activity.  Learned  parameters  can  encode  how  many  instances  tend  to  occur  in  a  class  of  interest.  To  this  end,  this  paper  develops  a  powerful  and  flexible  framework  to  infer  any  cardinality  relation  between  latent  labels  in  a  multi-instance  model.  Hard  or  soft  cardinality  relations  can  be  encoded  to  tackle  diverse  levels  of  ambiguity.  Experiments  on  tasks  such  as  human  activity  recognition,  video  event  detection,  and  video  summarization  demonstrate  the  effectiveness  of  using  cardinality  relations  for  improving  recognition  results.
0	A  coarse  to  fine  pyramidal  model  for  person  re  identification  via  multi  loss  dynamic  training.  Most  existing  Re-IDentification  (Re-ID)  methods  are  highly  dependent  on  precise  bounding  boxes  that  enable  images  to  be  aligned  with  each  other.  However,  due  to  the  inevitable  challenging  scenarios,  current  detection  models  often  output  inaccurate  bounding  boxes  yet,  which  inevitably  worsen  the  performance  of  these  Re-ID  algorithms.  In  this  paper,  to  relax  the  requirement,  we  propose  a  novel  coarse-to-fine  pyramid  model  that  not  only  incorporates  local  and  global  information,  but  also  integrates  the  gradual  cues  between  them.  The  pyramid  model  is  able  to  match  the  cues  at  different  scales  and  then  search  for  the  correct  image  of  the  same  identity  even  when  the  image  pair  are  not  aligned.  In  addition,  in  order  to  learn  discriminative  identity  representation,  we  explore  a  dynamic  training  scheme  to  seamlessly  unify  two  losses  and  extract  appropriate  shared  information  between  them.  Experimental  results  clearly  demonstrate  that  the  proposed  method  achieves  the  state-of-the-art  results  on  three  datasets  and  it  is  worth  noting  that  our  approach  exceeds  the  current  best  method  by  9.5%  on  the  most  challenging  dataset  CUHK03.
0	An  efficient  supervised  dictionary  learning  method  for  audio  signal  recognition.  Machine  hearing  or  listening  represents  an  emerging  area.  Conventional  approaches  rely  on  the  design  of  handcrafted  features  specialized  to  a  specific  audio  task  and  that  can  hardly  generalized  to  other  audio  fields.  For  example,  Mel-Frequency  Cepstral  Coefficients  (MFCCs)  and  its  variants  were  successfully  applied  to  computational  auditory  scene  recognition  while  Chroma  vectors  are  good  at  music  chord  recognition.  Unfortunately,  these  predefined  features  may  be  of  variable  discrimination  power  while  extended  to  other  tasks  or  even  within  the  same  task  due  to  different  nature  of  clips.  Motivated  by  this  need  of  a  principled  framework  across  domain  applications  for  machine  listening,  we  propose  a  generic  and  data-driven  representation  learning  approach.  For  this  sake,  a  novel  and  efficient  supervised  dictionary  learning  method  is  presented.  The  method  learns  dissimilar  dictionaries,  one  per  each  class,  in  order  to  extract  heterogeneous  information  for  classification.  In  other  words,  we  are  seeking  to  minimize  the  intra-class  homogeneity  and  maximize  class  separability.  This  is  made  possible  by  promoting  pairwise  orthogonality  between  class  specific  dictionaries  and  controlling  the  sparsity  structure  of  the  audio  clip's  decomposition  over  these  dictionaries.  The  resulting  optimization  problem  is  non-convex  and  solved  using  a  proximal  gradient  descent  method.  Experiments  are  performed  on  both  computational  auditory  scene  (East  Anglia  and  Rouen)  and  synthetic  music  chord  recognition  datasets.  Obtained  results  show  that  our  method  is  capable  to  reach  state-of-the-art  hand-crafted  features  for  both  applications.
0	Kercnns  biologically  inspired  lateral  connections  for  classification  of  corrupted  images.  The  state  of  the  art  in  many  computer  vision  tasks  is  represented  by  Convolutional  Neural  Networks  (CNNs).  Although  their  hierarchical  organization  and  local  feature  extraction  are  inspired  by  the  structure  of  primate  visual  systems,  the  lack  of  lateral  connections  in  such  architectures  critically  distinguishes  their  analysis  from  biological  object  processing.  The  idea  of  enriching  CNNs  with  recurrent  lateral  connections  of  convolutional  type  has  been  put  into  practice  in  recent  years,  in  the  form  of  learned  recurrent  kernels  with  no  geometrical  constraints.  In  the  present  work,  we  introduce  biologically  plausible  lateral  kernels  encoding  a  notion  of  correlation  between  the  feedforward  filters  of  a  CNN:  at  each  layer,  the  associated  kernel  acts  as  a  transition  kernel  on  the  space  of  activations.  The  lateral  kernels  are  defined  in  terms  of  the  filters,  thus  providing  a  parameter-free  approach  to  assess  the  geometry  of  horizontal  connections  based  on  the  feedforward  structure.  We  then  test  this  new  architecture,  which  we  call  KerCNN,  on  a  generalization  task  related  to  global  shape  analysis  and  pattern  completion:  once  trained  for  performing  basic  image  classification,  the  network  is  evaluated  on  corrupted  testing  images.  The  image  perturbations  examined  are  designed  to  undermine  the  recognition  of  the  images  via  local  features,  thus  requiring  an  integration  of  context  information  -  which  in  biological  vision  is  critically  linked  to  lateral  connectivity.  Our  KerCNNs  turn  out  to  be  far  more  stable  than  CNNs  and  recurrent  CNNs  to  such  degradations,  thus  validating  this  biologically  inspired  approach  to  reinforce  object  recognition  under  challenging  conditions.
0	Pva  pixel  aligned  volumetric  avatars.  Acquisition  and  rendering  of  photo-realistic  human  heads  is  a  highly  challenging  research  problem  of  particular  importance  for  virtual  telepresence.  Currently,  the  highest  quality  is  achieved  by  volumetric  approaches  trained  in  a  person  specific  manner  on  multi-view  data.  These  models  better  represent  fine  structure,  such  as  hair,  compared  to  simpler  mesh-based  models.  Volumetric  models  typically  employ  a  global  code  to  represent  facial  expressions,  such  that  they  can  be  driven  by  a  small  set  of  animation  parameters.  While  such  architectures  achieve  impressive  rendering  quality,  they  can  not  easily  be  extended  to  the  multi-identity  setting.  In  this  paper,  we  devise  a  novel  approach  for  predicting  volumetric  avatars  of  the  human  head  given  just  a  small  number  of  inputs.  We  enable  generalization  across  identities  by  a  novel  parameterization  that  combines  neural  radiance  fields  with  local,  pixel-aligned  features  extracted  directly  from  the  inputs,  thus  sidestepping  the  need  for  very  deep  or  complex  networks.  Our  approach  is  trained  in  an  end-to-end  manner  solely  based  on  a  photometric  re-rendering  loss  without  requiring  explicit  3D  supervision.We  demonstrate  that  our  approach  outperforms  the  existing  state  of  the  art  in  terms  of  quality  and  is  able  to  generate  faithful  facial  expressions  in  a  multi-identity  setting.
0	Dual  resolution  correspondence  networks.  We  tackle  the  problem  of  establishing  dense  pixel-wise  correspondences  between  a  pair  of  images.  In  this  work,  we  introduce  Dual-Resolution  Correspondence  Networks  (DualRC-Net),  to  obtain  pixel-wise  correspondences  in  a  coarse-to-fine  manner.  DualRC-Net  extracts  both  coarse-  and  fine-  resolution  feature  maps.  The  coarse  maps  are  used  to  produce  a  full  but  coarse  4D  correlation  tensor,  which  is  then  refined  by  a  learnable  neighbourhood  consensus  module.  The  fine-resolution  feature  maps  are  used  to  obtain  the  final  dense  correspondences  guided  by  the  refined  coarse  4D  correlation  tensor.  The  selected  coarse-resolution  matching  scores  allow  the  fine-resolution  features  to  focus  only  on  a  limited  number  of  possible  matches  with  high  confidence.  In  this  way,  DualRC-Net  dramatically  increases  matching  reliability  and  localisation  accuracy,  while  avoiding  to  apply  the  expensive  4D  convolution  kernels  on  fine-resolution  feature  maps.  We  comprehensively  evaluate  our  method  on  large-scale  public  benchmarks  including  HPatches,  InLoc,  and  Aachen  Day-Night.  It  achieves  the  state-of-the-art  results  on  all  of  them.
0	Music  gesture  for  visual  sound  separation.  Recent  deep  learning  approaches  have  achieved  impressive  performance  on  visual  sound  separation  tasks.  However,  these  approaches  are  mostly  built  on  appearance  and  optical  flow  like  motion  feature  representations,  which  exhibit  limited  abilities  to  find  the  correlations  between  audio  signals  and  visual  points,  especially  when  separating  multiple  instruments  of  the  same  types,  such  as  multiple  violins  in  a  scene.  To  address  this,  we  propose  "Music  Gesture,"  a  keypoint-based  structured  representation  to  explicitly  model  the  body  and  finger  movements  of  musicians  when  they  perform  music.  We  first  adopt  a  context-aware  graph  network  to  integrate  visual  semantic  context  with  body  dynamics,  and  then  apply  an  audio-visual  fusion  model  to  associate  body  movements  with  the  corresponding  audio  signals.  Experimental  results  on  three  music  performance  datasets  show:  1)  strong  improvements  upon  benchmark  metrics  for  hetero-musical  separation  tasks  (i.e.  different  instruments);  2)  new  ability  for  effective  homo-musical  separation  for  piano,  flute,  and  trumpet  duets,  which  to  our  best  knowledge  has  never  been  achieved  with  alternative  methods.  Project  page:  this  http  URL.
0	Weakly  supervised  dcnn  for  rgb  d  object  recognition  in  real  world  applications  which  lack  large  scale  annotated  training  data.  This  paper  addresses  the  problem  of  RGBD  object  recognition  in  real-world  applications,  where  large  amounts  of  annotated  training  data  are  typically  unavailable.  To  overcome  this  problem,  we  propose  a  novel,  weakly-supervised  learning  architecture  (DCNN-GPC)  which  combines  parametric  models  (a  pair  of  Deep  Convolutional  Neural  Networks  (DCNN)  for  RGB  and  D  modalities)  with  non-parametric  models  (Gaussian  Process  Classification).  Our  system  is  initially  trained  using  a  small  amount  of  labeled  data,  and  then  automatically  prop-  agates  labels  to  large-scale  unlabeled  data.  We  first  run  3D-  based  objectness  detection  on  RGBD  videos  to  acquire  many  unlabeled  object  proposals,  and  then  employ  DCNN-GPC  to  label  them.  As  a  result,  our  multi-modal  DCNN  can  be  trained  end-to-end  using  only  a  small  amount  of  human  annotation.  Finally,  our  3D-based  objectness  detection  and  multi-modal  DCNN  are  integrated  into  a  real-time  detection  and  recognition  pipeline.  In  our  approach,  bounding-box  annotations  are  not  required  and  boundary-aware  detection  is  achieved.  We  also  propose  a  novel  way  to  pretrain  a  DCNN  for  the  depth  modality,  by  training  on  virtual  depth  images  projected  from  CAD  models.  We  pretrain  our  multi-modal  DCNN  on  public  3D  datasets,  achieving  performance  comparable  to  state-of-the-art  methods  on  Washington  RGBS  Dataset.  We  then  finetune  the  network  by  further  training  on  a  small  amount  of  annotated  data  from  our  novel  dataset  of  industrial  objects  (nuclear  waste  simulants).  Our  weakly  supervised  approach  has  demonstrated  to  be  highly  effective  in  solving  a  novel  RGBD  object  recognition  application  which  lacks  of  human  annotations.
0	Interpretable  transformations  with  encoder  decoder  networks.  Deep  feature  spaces  have  the  capacity  to  encode  complex  transformations  of  their  input  data.  However,  understanding  the  relative  feature-space  relationship  between  two  transformed  encoded  images  is  difficult.  For  instance,  what  is  the  relative  feature  space  relationship  between  two  rotated  images?  What  is  decoded  when  we  interpolate  in  feature  space?  Ideally,  we  want  to  disentangle  confounding  factors,  such  as  pose,  appearance,  and  illumination,  from  object  identity.  Disentangling  these  is  difficult  because  they  interact  in  very  nonlinear  ways.  We  propose  a  simple  method  to  construct  a  deep  feature  space,  with  explicitly  disentangled  representations  of  several  known  transformations.  A  person  or  algorithm  can  then  manipulate  the  disentangled  representation,  for  example,  to  re-render  an  image  with  explicit  control  over  parameterized  degrees  of  freedom.  The  feature  space  is  constructed  using  a  transforming  encoder-decoder  network  with  a  custom  feature  transform  layer,  acting  on  the  hidden  representations.  We  demonstrate  the  advantages  of  explicit  disentangling  on  a  variety  of  datasets  and  transformations,  and  as  an  aid  for  traditional  tasks,  such  as  classification.
0	Learning  discriminative  motion  features  through  detection.  Despite  huge  success  in  the  image  domain,  modern  detection  models  such  as  Faster  R-CNN  have  not  been  used  nearly  as  much  for  video  analysis.  This  is  arguably  due  to  the  fact  that  detection  models  are  designed  to  operate  on  single  frames  and  as  a  result  do  not  have  a  mechanism  for  learning  motion  representations  directly  from  video.  We  propose  a  learning  procedure  that  allows  detection  models  such  as  Faster  R-CNN  to  learn  motion  features  directly  from  the  RGB  video  data  while  being  optimized  with  respect  to  a  pose  estimation  task.  Given  a  pair  of  video  frames---Frame  A  and  Frame  B---we  force  our  model  to  predict  human  pose  in  Frame  A  using  the  features  from  Frame  B.  We  do  so  by  leveraging  deformable  convolutions  across  space  and  time.  Our  network  learns  to  spatially  sample  features  from  Frame  B  in  order  to  maximize  pose  detection  accuracy  in  Frame  A.  This  naturally  encourages  our  network  to  learn  motion  offsets  encoding  the  spatial  correspondences  between  the  two  frames.  We  refer  to  these  motion  offsets  as  DiMoFs  (Discriminative  Motion  Features).    In  our  experiments  we  show  that  our  training  scheme  helps  learn  effective  motion  cues,  which  can  be  used  to  estimate  and  localize  salient  human  motion.  Furthermore,  we  demonstrate  that  as  a  byproduct,  our  model  also  learns  features  that  lead  to  improved  pose  detection  in  still-images,  and  better  keypoint  tracking.  Finally,  we  show  how  to  leverage  our  learned  model  for  the  tasks  of  spatiotemporal  action  localization  and  fine-grained  action  recognition.
0	Crowd  density  estimation  using  novel  feature  descriptor.  Crowd  density  estimation  is  an  important  task  for  crowd  monitoring.  Many  efforts  have  been  done  to  automate  the  process  of  estimating  crowd  density  from  images  and  videos.  Despite  series  of  efforts,  it  remains  a  challenging  task.  In  this  paper,  we  proposes  a  new  texture  feature-based  approach  for  the  estimation  of  crowd  density  based  on  Completed  Local  Binary  Pattern  (CLBP).  We  first  divide  the  image  into  blocks  and  then  re-divide  the  blocks  into  cells.  For  each  cell,  we  compute  CLBP  and  then  concatenate  them  to  describe  the  texture  of  the  corresponding  block.  We  then  train  a  multi-class  Support  Vector  Machine  (SVM)  classifier,  which  classifies  each  block  of  image  into  one  of  four  categories,  i.e.  Very  Low,  Low,  Medium,  and  High.  We  evaluate  our  technique  on  the  PETS  2009  dataset,  and  from  the  experiments,  we  show  to  achieve  95%  accuracy  for  the  proposed  descriptor.  We  also  compare  other  state-of-the-art  texture  descriptors  and  from  the  experimental  results,  we  show  that  our  proposed  method  outperforms  other  state-of-the-art  methods.
0	Online  object  representations  with  contrastive  learning.  We  propose  a  self-supervised  approach  for  learning  representations  of  objects  from  monocular  videos  and  demonstrate  it  is  particularly  useful  in  situated  settings  such  as  robotics.  The  main  contributions  of  this  paper  are:  1)  a  self-supervising  objective  trained  with  contrastive  learning  that  can  discover  and  disentangle  object  attributes  from  video  without  using  any  labels;  2)  we  leverage  object  self-supervision  for  online  adaptation:  the  longer  our  online  model  looks  at  objects  in  a  video,  the  lower  the  object  identification  error,  while  the  offline  baseline  remains  with  a  large  fixed  error;  3)  to  explore  the  possibilities  of  a  system  entirely  free  of  human  supervision,  we  let  a  robot  collect  its  own  data,  train  on  this  data  with  our  self-supervise  scheme,  and  then  show  the  robot  can  point  to  objects  similar  to  the  one  presented  in  front  of  it,  demonstrating  generalization  of  object  attributes.  An  interesting  and  perhaps  surprising  finding  of  this  approach  is  that  given  a  limited  set  of  objects,  object  correspondences  will  naturally  emerge  when  using  contrastive  learning  without  requiring  explicit  positive  pairs.  Videos  illustrating  online  object  adaptation  and  robotic  pointing  are  available  at:  this  https  URL.
0	Technical  report  image  captioning  with  semantically  similar  images.  This  report  presents  our  submission  to  the  MS  COCO  Captioning  Challenge  2015.  The  method  uses  Convolutional  Neural  Network  activations  as  an  embedding  to  find  semantically  similar  images.  From  these  images,  the  most  typical  caption  is  selected  based  on  unigram  frequencies.  Although  the  method  received  low  scores  with  automated  evaluation  metrics  and  in  human  assessed  average  correctness,  it  is  competitive  in  the  ratio  of  captions  which  pass  the  Turing  test  and  which  are  assessed  as  better  or  equal  to  human  captions.
0	Grip  enhanced  graph  based  interaction  aware  trajectory  prediction  for  autonomous  driving.  Despite  the  advancement  in  the  technology  of  autonomous  driving  cars,  the  safety  of  a  self-driving  car  is  still  a  challenging  problem  that  has  not  been  well  studied.  Motion  prediction  is  one  of  the  core  functions  of  an  autonomous  driving  car.  Previously,  we  propose  a  novel  scheme  called  GRIP  which  is  designed  to  predict  trajectories  for  traffic  agents  around  an  autonomous  car  efficiently.  GRIP  uses  a  graph  to  represent  the  interactions  of  close  objects,  applies  several  graph  convolutional  blocks  to  extract  features,  and  subsequently  uses  an  encoder-decoder  long  short-term  memory  (LSTM)  model  to  make  predictions.  Even  though  our  experimental  results  show  that  GRIP  improves  the  prediction  accuracy  of  the  state-of-the-art  solution  by  30%,  GRIP  still  has  some  limitations.  GRIP  uses  a  fixed  graph  to  describe  the  relationships  between  different  traffic  agents  and  hence  may  suffer  some  performance  degradations  when  it  is  being  used  in  urban  traffic  scenarios.  Hence,  in  this  paper,  we  describe  an  improved  scheme  called  GRIP++  where  we  use  both  fixed  and  dynamic  graphs  for  trajectory  predictions  of  different  types  of  traffic  agents.  Such  an  improvement  can  help  autonomous  driving  cars  avoid  many  traffic  accidents.  Our  evaluations  using  a  recently  released  urban  traffic  dataset,  namely  ApolloScape  showed  that  GRIP++  achieves  better  prediction  accuracy  than  state-of-the-art  schemes.  GRIP++  ranked  #1  on  the  leaderboard  of  the  ApolloScape  trajectory  competition  in  October  2019.  In  addition,  GRIP++  runs  21.7  times  faster  than  a  state-of-the-art  scheme,  CS-LSTM.
0	Robustness  meets  deep  learning  an  end  to  end  hybrid  pipeline  for  unsupervised  learning  of  egomotion.  In  this  work,  we  propose  a  method  that  combines  unsupervised  deep  learning  predictions  for  optical  flow  and  monocular  disparity  with  a  model  based  optimization  procedure  for  instantaneous  camera  pose.  Given  the  flow  and  disparity  predictions  from  the  network,  we  apply  a  RANSAC  outlier  rejection  scheme  to  find  an  inlier  set  of  flows  and  disparities,  which  we  use  to  solve  for  the  relative  camera  pose  in  a  least  squares  fashion.  We  show  that  this  pipeline  is  fully  differentiable,  allowing  us  to  combine  the  pose  with  the  network  outputs  as  an  additional  unsupervised  training  loss  to  further  refine  the  predicted  flows  and  disparities.  This  method  not  only  allows  us  to  directly  regress  relative  pose  from  the  network  outputs,  but  also  automatically  segments  away  pixels  that  do  not  fit  the  rigid  scene  assumptions  that  many  unsupervised  structure  from  motion  methods  apply,  such  as  on  independently  moving  objects.  We  evaluate  our  method  on  the  KITTI  dataset,  and  demonstrate  state  of  the  art  results,  even  in  the  presence  of  challenging  independently  moving  objects.
0	Human  action  recognition  and  prediction  a  survey.  Derived  from  rapid  advances  in  computer  vision  and  machine  learning,  video  analysis  tasks  have  been  moving  from  inferring  the  present  state  to  predicting  the  future  state.  Vision-based  action  recognition  and  prediction  from  videos  are  such  tasks,  where  action  recognition  is  to  infer  human  actions  (present  state)  based  upon  complete  action  executions,  and  action  prediction  to  predict  human  actions  (future  state)  based  upon  incomplete  action  executions.  These  two  tasks  have  become  particularly  prevalent  topics  recently  because  of  their  explosively  emerging  real-world  applications,  such  as  visual  surveillance,  autonomous  driving  vehicle,  entertainment,  and  video  retrieval,  etc.  Many  attempts  have  been  devoted  in  the  last  a  few  decades  in  order  to  build  a  robust  and  effective  framework  for  action  recognition  and  prediction.  In  this  paper,  we  survey  the  complete  state-of-the-art  techniques  in  the  action  recognition  and  prediction.  Existing  models,  popular  algorithms,  technical  difficulties,  popular  action  databases,  evaluation  protocols,  and  promising  future  directions  are  also  provided  with  systematic  discussions.
0	Towards  automated  post  earthquake  inspections  with  deep  learning  based  condition  aware  models.  In  the  aftermath  of  an  earthquake,  rapid  structural  inspections  are  required  to  get  citizens  back  in  to  their  homes  and  offices  in  a  safe  and  timely  manner.  These  inspections  gfare  typically  conducted  by  municipal  authorities  through  structural  engineer  volunteers.  As  manual  inspec-tions  can  be  time  consuming,  laborious  and  dangerous,  research  has  been  underway  to  develop  methods  to  help  speed  up  and  increase  the  automation  of  the  entire  process.  Researchers  typi-cally  envisage  the  use  of  unmanned  aerial  vehicles  (UAV)  for  data  acquisition  and  computer  vision  for  data  processing  to  extract  actionable  information.  In  this  work  we  propose  a  new  framework  to  generate  vision-based  condition-aware  models  that  can  serve  as  the  basis  for  speeding  up  or  automating  higher  level  inspection  decisions.  The  condition-aware  models  are  generated  by  projecting  the  inference  of  trained  deep-learning  models  on  a  set  of  images  of  a  structure  onto  a  3D  mesh  model  generated  through  multi-view  stereo  from  the  same  image  set.  Deep  fully  convolutional  residual  networks  are  used  for  semantic  segmentation  of  images  of  buildings  to  provide  (i)  damage  information  such  as  cracks  and  spalling  (ii)  contextual  infor-mation  such  as  the  presence  of  a  building  and  visually  identifiable  components  like  windows  and  doors.  The  proposed  methodology  was  implemented  on  a  damaged  building  that  was  sur-veyed  by  the  authors  after  the  Central  Mexico  Earthquake  in  September  2017  and  qualitative-ly  evaluated.  Results  demonstrate  the  promise  of  the  proposed  method  towards  the  ultimate  goal  of  rapid  and  automated  post-earthquake  inspections.
0	Hypergraph  p  laplacian  regularization  for  remote  sensing  image  recognition.  It  is  of  great  importance  to  preserve  locality  and  similarity  information  in  semi-supervised  learning  (SSL)  based  applications.  Graph  based  SSL  and  manifold  regularization  based  SSL  including  Laplacian  regularization  (LapR)  and  Hypergraph  Laplacian  regularization  (HLapR)  are  representative  SSL  methods  and  have  achieved  prominent  performance  by  exploiting  the  relationship  of  sample  distribution.  However,  it  is  still  a  great  challenge  to  exactly  explore  and  exploit  the  local  structure  of  the  data  distribution.  In  this  paper,  we  present  an  effect  and  effective  approximation  algorithm  of  Hypergraph  p-Laplacian  and  then  propose  Hypergraph  p-Laplacian  regularization  (HpLapR)  to  preserve  the  geometry  of  the  probability  distribution.  In  particular,  p-Laplacian  is  a  nonlinear  generalization  of  the  standard  graph  Laplacian  and  Hypergraph  is  a  generalization  of  a  standard  graph.  Therefore,  the  proposed  HpLapR  provides  more  potential  to  exploiting  the  local  structure  preserving.  We  apply  HpLapR  to  logistic  regression  and  conduct  the  implementations  for  remote  sensing  image  recognition.  We  compare  the  proposed  HpLapR  to  several  popular  manifold  regularization  based  SSL  methods  including  LapR,  HLapR  and  HpLapR  on  UC-Merced  dataset.  The  experimental  results  demonstrate  the  superiority  of  the  proposed  HpLapR.
0	Latent  ransac.  We  present  a  method  that  can  evaluate  a  RANSAC  hypothesis  in  constant  time,  i.e.  independent  of  the  size  of  the  data.  A  key  observation  here  is  that  correct  hypotheses  are  tightly  clustered  together  in  the  latent  parameter  domain.  In  a  manner  similar  to  the  generalized  Hough  transform  we  seek  to  find  this  cluster,  only  that  we  need  as  few  as  two  votes  for  a  successful  detection.  Rapidly  locating  such  pairs  of  similar  hypotheses  is  made  possible  by  adapting  the  recent  "Random  Grids"  range-search  technique.  We  only  perform  the  usual  (costly)  hypothesis  verification  stage  upon  the  discovery  of  a  close  pair  of  hypotheses.  We  show  that  this  event  rarely  happens  for  incorrect  hypotheses,  enabling  a  significant  speedup  of  the  RANSAC  pipeline.  The  suggested  approach  is  applied  and  tested  on  three  robust  estimation  problems:  camera  localization,  3D  rigid  alignment  and  2D-homography  estimation.  We  perform  rigorous  testing  on  both  synthetic  and  real  datasets,  demonstrating  an  improvement  in  efficiency  without  a  compromise  in  accuracy.  Furthermore,  we  achieve  state-of-the-art  3D  alignment  results  on  the  challenging  "Redwood"  loop-closure  challenge.
0	Deep  directional  statistics  pose  estimation  with  uncertainty  quantification.  Modern  deep  learning  systems  successfully  solve  many  perception  tasks  such  as  object  pose  estimation  when  the  input  image  is  of  high  quality.  However,  in  challenging  imaging  conditions  such  as  on  low-resolution  images  or  when  the  image  is  corrupted  by  imaging  artifacts,  current  systems  degrade  considerably  in  accuracy.  While  a  loss  in  performance  is  unavoidable,  we  would  like  our  models  to  quantify  their  uncertainty  in  order  to  achieve  robustness  against  images  of  varying  quality.  Probabilistic  deep  learning  models  combine  the  expressive  power  of  deep  learning  with  uncertainty  quantification.  In  this  paper,  we  propose  a  novel  probabilistic  deep  learning  model  for  the  task  of  angular  regression.  Our  model  uses  von  Mises  distributions  to  predict  a  distribution  over  object  pose  angle.  Whereas  a  single  von  Mises  distribution  is  making  strong  assumptions  about  the  shape  of  the  distribution,  we  extend  the  basic  model  to  predict  a  mixture  of  von  Mises  distributions.  We  show  how  to  learn  a  mixture  model  using  a  finite  and  infinite  number  of  mixture  components.  Our  model  allows  for  likelihood-based  training  and  efficient  inference  at  test  time.  We  demonstrate  on  a  number  of  challenging  pose  estimation  datasets  that  our  model  produces  calibrated  probability  predictions  and  competitive  or  superior  point  estimates  compared  to  the  current  state-of-the-art.
0	Iterative  transformer  network  for  3d  point  cloud.  3D  point  cloud  is  an  efficient  and  flexible  representation  of  3D  structures.  Recently,  neural  networks  operating  on  point  clouds  have  shown  superior  performance  on  3D  understanding  tasks  such  as  shape  classification  and  part  segmentation.  However,  performance  on  such  tasks  is  evaluated  on  complete  shapes  aligned  in  a  canonical  frame,  while  real  world  3D  data  are  partial  and  unaligned.  A  key  challenge  in  learning  from  partial,  unaligned  point  cloud  data  is  to  learn  features  that  are  invariant  or  equivariant  with  respect  to  geometric  transformations.  To  address  this  challenge,  we  propose  the  Iterative  Transformer  Network  (IT-Net),  a  network  module  that  canonicalizes  the  pose  of  a  partial  object  with  a  series  of  3D  rigid  transformations  predicted  in  an  iterative  fashion.  We  demonstrate  the  efficacy  of  IT-Net  as  an  anytime  pose  estimator  from  partial  point  clouds  without  using  complete  object  models.  Further,  we  show  that  IT-Net  achieves  superior  performance  over  alternative  3D  transformer  networks  on  various  tasks,  such  as  partial  shape  classification  and  object  part  segmentation.
0	Fully  convolutional  adaptation  networks  for  semantic  segmentation.  The  recent  advances  in  deep  neural  networks  have  convincingly  demonstrated  high  capability  in  learning  vision  models  on  large  datasets.  Nevertheless,  collecting  expert  labeled  datasets  especially  with  pixel-level  annotations  is  an  extremely  expensive  process.  An  appealing  alternative  is  to  render  synthetic  data  (e.g.,  computer  games)  and  generate  ground  truth  automatically.  However,  simply  applying  the  models  learnt  on  synthetic  images  may  lead  to  high  generalization  error  on  real  images  due  to  domain  shift.  In  this  paper,  we  facilitate  this  issue  from  the  perspectives  of  both  visual  appearance-level  and  representation-level  domain  adaptation.  The  former  adapts  source-domain  images  to  appear  as  if  drawn  from  the  "style"  in  the  target  domain  and  the  latter  attempts  to  learn  domain-invariant  representations.  Specifically,  we  present  Fully  Convolutional  Adaptation  Networks  (FCAN),  a  novel  deep  architecture  for  semantic  segmentation  which  combines  Appearance  Adaptation  Networks  (AAN)  and  Representation  Adaptation  Networks  (RAN).  AAN  learns  a  transformation  from  one  domain  to  the  other  in  the  pixel  space  and  RAN  is  optimized  in  an  adversarial  learning  manner  to  maximally  fool  the  domain  discriminator  with  the  learnt  source  and  target  representations.  Extensive  experiments  are  conducted  on  the  transfer  from  GTA5  (game  videos)  to  Cityscapes  (urban  street  scenes)  on  semantic  segmentation  and  our  proposal  achieves  superior  results  when  comparing  to  state-of-the-art  unsupervised  adaptation  techniques.  More  remarkably,  we  obtain  a  new  record:  mIoU  of  47.5%  on  BDDS  (drive-cam  videos)  in  an  unsupervised  setting.
0	Real  time  prediction  of  segmentation  quality.  Recent  advances  in  deep  learning  based  image  segmentation  methods  have  enabled  real-time  performance  with  human-level  accuracy.  However,  occasionally  even  the  best  method  fails  due  to  low  image  quality,  artifacts  or  unexpected  behaviour  of  black  box  algorithms.  Being  able  to  predict  segmentation  quality  in  the  absence  of  ground  truth  is  of  paramount  importance  in  clinical  practice,  but  also  in  large-scale  studies  to  avoid  the  inclusion  of  invalid  data  in  subsequent  analysis.    In  this  work,  we  propose  two  approaches  of  real-time  automated  quality  control  for  cardiovascular  MR  segmentations  using  deep  learning.  First,  we  train  a  neural  network  on  12,880  samples  to  predict  Dice  Similarity  Coefficients  (DSC)  on  a  per-case  basis.  We  report  a  mean  average  error  (MAE)  of  0.03  on  1,610  test  samples  and  97%  binary  classification  accuracy  for  separating  low  and  high  quality  segmentations.  Secondly,  in  the  scenario  where  no  manually  annotated  data  is  available,  we  train  a  network  to  predict  DSC  scores  from  estimated  quality  obtained  via  a  reverse  testing  strategy.  We  report  an  MAE=0.14  and  91%  binary  classification  accuracy  for  this  case.  Predictions  are  obtained  in  real-time  which,  when  combined  with  real-time  segmentation  methods,  enables  instant  feedback  on  whether  an  acquired  scan  is  analysable  while  the  patient  is  still  in  the  scanner.  This  further  enables  new  applications  of  optimising  image  acquisition  towards  best  possible  analysis  results.
0	Stereo  camera  system  calibration  the  need  of  two  sets  of  parameters.  The  reconstruction  of  a  scene  via  a  stereo-camera  system  is  a  two-steps  process,  where  at  first  images  from  different  cameras  are  matched  to  identify  the  set  of  point-to-point  correspondences  that  then  will  actually  be  reconstructed  in  the  three  dimensional  real  world.  The  performance  of  the  system  strongly  relies  of  the  calibration  procedure,  which  has  to  be  carefully  designed  to  guarantee  optimal  results.  We  implemented  three  different  calibration  methods  and  we  compared  their  performance  over  19  datasets.  We  present  the  experimental  evidence  that,  due  to  the  image  noise,  a  single  set  of  parameters  is  not  sufficient  to  achieve  high  accuracy  in  the  identification  of  the  correspondences  and  in  the  3D  reconstruction  at  the  same  time.  We  propose  to  calibrate  the  system  twice  to  estimate  two  different  sets  of  parameters:  the  one  obtained  by  minimizing  the  reprojection  error  that  will  be  used  when  dealing  with  quantities  defined  in  the  2D  space  of  the  cameras,  and  the  one  obtained  by  minimizing  the  reconstruction  error  that  will  be  used  when  dealing  with  quantities  defined  in  the  real  3D  world.
0	Going  deeper  into  face  detection  a  survey.  Face  detection  is  a  crucial  first  step  in  many  facial  recognition  and  face  analysis  systems.  Early  approaches  for  face  detection  were  mainly  based  on  classifiers  built  on  top  of  hand-crafted  features  extracted  from  local  image  regions,  such  as  Haar  Cascades  and  Histogram  of  Oriented  Gradients.  However,  these  approaches  were  not  powerful  enough  to  achieve  a  high  accuracy  on  images  of  from  uncontrolled  environments.  With  the  breakthrough  work  in  image  classification  using  deep  neural  networks  in  2012,  there  has  been  a  huge  paradigm  shift  in  face  detection.  Inspired  by  the  rapid  progress  of  deep  learning  in  computer  vision,  many  deep  learning  based  frameworks  have  been  proposed  for  face  detection  over  the  past  few  years,  achieving  significant  improvements  in  accuracy.  In  this  work,  we  provide  a  detailed  overview  of  some  of  the  most  representative  deep  learning  based  face  detection  methods  by  grouping  them  into  a  few  major  categories,  and  present  their  core  architectural  designs  and  accuracies  on  popular  benchmarks.  We  also  describe  some  of  the  most  popular  face  detection  datasets.  Finally,  we  discuss  some  current  challenges  in  the  field,  and  suggest  potential  future  research  directions.
0	Far  gan  for  one  shot  face  reenactment.  Animating  a  static  face  image  with  target  facial  expressions  and  movements  is  important  in  the  area  of  image  editing  and  movie  production.  This  face  reenactment  process  is  challenging  due  to  the  complex  geometry  and  movement  of  human  faces.  Previous  work  usually  requires  a  large  set  of  images  from  the  same  person  to  model  the  appearance.  In  this  paper,  we  present  a  one-shot  face  reenactment  model,  FaR-GAN,  that  takes  only  one  face  image  of  any  given  source  identity  and  a  target  expression  as  input,  and  then  produces  a  face  image  of  the  same  source  identity  but  with  the  target  expression.  The  proposed  method  makes  no  assumptions  about  the  source  identity,  facial  expression,  head  pose,  or  even  image  background.  We  evaluate  our  method  on  the  VoxCeleb1  dataset  and  show  that  our  method  is  able  to  generate  a  higher  quality  face  image  than  the  compared  methods.
0	A  comprehensive  study  of  deep  video  action  recognition.  Video  action  recognition  is  one  of  the  representative  tasks  for  video  understanding.  Over  the  last  decade,  we  have  witnessed  great  advancements  in  video  action  recognition  thanks  to  the  emergence  of  deep  learning.  But  we  also  encountered  new  challenges,  including  modeling  long-range  temporal  information  in  videos,  high  computation  costs,  and  incomparable  results  due  to  datasets  and  evaluation  protocol  variances.  In  this  paper,  we  provide  a  comprehensive  survey  of  over  200  existing  papers  on  deep  learning  for  video  action  recognition.  We  first  introduce  the  17  video  action  recognition  datasets  that  influenced  the  design  of  models.  Then  we  present  video  action  recognition  models  in  chronological  order:  starting  with  early  attempts  at  adapting  deep  learning,  then  to  the  two-stream  networks,  followed  by  the  adoption  of  3D  convolutional  kernels,  and  finally  to  the  recent  compute-efficient  models.  In  addition,  we  benchmark  popular  methods  on  several  representative  datasets  and  release  code  for  reproducibility.  In  the  end,  we  discuss  open  problems  and  shed  light  on  opportunities  for  video  action  recognition  to  facilitate  new  research  ideas.
0	Fast  feature  extraction  with  cnns  with  pooling  layers.  In  recent  years,  many  publications  showed  that  convolutional  neural  network  based  features  can  have  a  superior  performance  to  engineered  features.  However,  not  much  effort  was  taken  so  far  to  extract  local  features  efficiently  for  a  whole  image.  In  this  paper,  we  present  an  approach  to  compute  patch-based  local  feature  descriptors  efficiently  in  presence  of  pooling  and  striding  layers  for  whole  images  at  once.  Our  approach  is  generic  and  can  be  applied  to  nearly  all  existing  network  architectures.  This  includes  networks  for  all  local  feature  extraction  tasks  like  camera  calibration,  Patchmatching,  optical  flow  estimation  and  stereo  matching.  In  addition,  our  approach  can  be  applied  to  other  patch-based  approaches  like  sliding  window  object  detection  and  recognition.  We  complete  our  paper  with  a  speed  benchmark  of  popular  CNN  based  feature  extraction  approaches  applied  on  a  whole  image,  with  and  without  our  speedup,  and  example  code  (for  Torch)  that  shows  how  an  arbitrary  CNN  architecture  can  be  easily  converted  by  our  approach.
0	Fully  automated  packaging  structure  recognition  in  logistics  environments.  Within  a  logistics  supply  chain,  a  large  variety  of  transported  goods  need  to  be  handled,  recognized  and  checked  at  many  different  network  points.  Often,  huge  manual  effort  is  involved  in  recognizing  or  verifying  packet  identity  or  packaging  structure,  for  instance  to  check  the  delivery  for  completeness.  We  propose  a  method  for  complete  automation  of  packaging  structure  recognition:  Based  on  a  single  image,  one  or  multiple  transport  units  are  localized  and,  for  each  of  these  transport  units,  the  characteristics,  the  total  number  and  the  arrangement  of  its  packaging  units  is  recognized.  Our  algorithm  is  based  on  deep  learning  models,  more  precisely  convolutional  neural  networks  for  instance  segmentation  in  images,  as  well  as  computer  vision  methods  and  heuristic  components.  We  use  a  custom  data  set  of  realistic  logistics  images  for  training  and  evaluation  of  our  method.  We  show  that  the  solution  is  capable  of  correctly  recognizing  the  packaging  structure  in  approximately  85%  of  our  test  cases,  and  even  more  (91%)  when  focusing  on  most  common  package  types.
0	Adv  makeup  a  new  imperceptible  and  transferable  attack  on  face  recognition.  Deep  neural  networks,  particularly  face  recognition  models,  have  been  shown  to  be  vulnerable  to  both  digital  and  physical  adversarial  examples.  However,  existing  adversarial  examples  against  face  recognition  systems  either  lack  transferability  to  black-box  models,  or  fail  to  be  implemented  in  practice.  In  this  paper,  we  propose  a  unified  adversarial  face  generation  method  -  Adv-Makeup,  which  can  realize  imperceptible  and  transferable  attack  under  black-box  setting.  Adv-Makeup  develops  a  task-driven  makeup  generation  method  with  the  blending  module  to  synthesize  imperceptible  eye  shadow  over  the  orbital  region  on  faces.  And  to  achieve  transferability,  Adv-Makeup  implements  a  fine-grained  meta-learning  adversarial  attack  strategy  to  learn  more  general  attack  features  from  various  models.  Compared  to  existing  techniques,  sufficient  visualization  results  demonstrate  that  Adv-Makeup  is  capable  to  generate  much  more  imperceptible  attacks  under  both  digital  and  physical  scenarios.  Meanwhile,  extensive  quantitative  experiments  show  that  Adv-Makeup  can  significantly  improve  the  attack  success  rate  under  black-box  setting,  even  attacking  commercial  systems.
0	Fast  convolution  based  methods  for  computing  the  signed  distance  function  and  its  derivatives.  We  present  a  fast  convolution-based  technique  for  computing  an  approximate,  signed  Euclidean  distance  function  at  a  set  of  2D  and  3D  grid  locations.  Instead  of  solving  the  non-linear  static  Hamilton-Jacobi  equation  (|\nabla  S|=1),  our  solution  stems  from  solving  for  a  scalar  field  \phi  in  a  linear  differential  equation  and  then  deriving  the  solution  for  S  from  its  exponent.  In  other  words,  when  S  and  \phi  are  related  by  \phi  =  \exp(-S/\tau)  and  \phi  satisfies  a  specific  linear  differential  equation  corresponding  to  the  extremum  of  a  variational  problem,  we  obtain  the  Euclidean  distance  function  S  =  -\tau  \log(\phi)in  the  limit  as  \tau-->0.  This  is  in  sharp  contrast  to  solvers  such  as  the  fast  marching  and  fast  sweeping  methods  which  directly  solve  the  Hamilton-Jacobi  equation  by  the  Godunov  upwind  discretization  scheme.  Our  linear  formulation  provides  us  with  a  closed-form  solution  to  the  approximate  Euclidean  distance  function  expressed  as  a  discrete  convolution,  and  hence  efficiently  computed  by  the  Fast  Fourier  Transform  (FFT).  Moreover,  the  solution  circumvents  the  need  for  spatial  discretization  of  the  derivative  operator  thereby  providing  highly  accurate  results.  As  \tau-->0,  we  show  the  convergence  of  our  results  to  the  true  solution  and  also  bound  the  error  for  a  given  value  of  \tau.  The  differentiability  of  our  solution  allows  us  to  compute  -  using  a  set  of  convolutions  -  the  first  and  second  derivatives  of  the  approximate  distance  function.  In  order  to  determine  the  sign  of  the  distance  function  (defined  to  be  positive  inside  a  closed  region  and  negative  outside),  we  compute  the  winding  number  in  2D  and  the  topological  degree  in  3D  and  again  explicitly  show  that  these  computations  can  be  performed  via  fast  convolutions.
0	Human  head  pose  estimation  by  facial  features  location.  We  describe  a  method  for  estimating  human  head  pose  in  a  color  image  that  contains  enough  of  information  to  locate  the  head  silhouette  and  detect  non-trivial  color  edges  of  individual  facial  features.  The  method  works  by  spotting  the  human  head  on  an  arbitrary  background,  extracting  the  head  outline,  and  locating  facial  features  necessary  to  describe  the  head  orientation  in  the  3D  space.  It  is  robust  enough  to  work  with  both  color  and  gray-level  images  featuring  quasi-frontal  views  of  a  human  head  under  variable  lighting  conditions.
0	The  open  world  of  micro  videos.  Micro-videos  are  six-second  videos  popular  on  social  media  networks  with  several  unique  properties.  Firstly,  because  of  the  authoring  process,  they  contain  significantly  more  diversity  and  narrative  structure  than  existing  collections  of  video  "snippets".  Secondly,  because  they  are  often  captured  by  hand-held  mobile  cameras,  they  contain  specialized  viewpoints  including  third-person,  egocentric,  and  self-facing  views  seldom  seen  in  traditional  produced  video.  Thirdly,  due  to  to  their  continuous  production  and  publication  on  social  networks,  aggregate  micro-video  content  contains  interesting  open-world  dynamics  that  reflects  the  temporal  evolution  of  tag  topics.  These  aspects  make  micro-videos  an  appealing  well  of  visual  data  for  developing  large-scale  models  for  video  understanding.  We  analyze  a  novel  dataset  of  micro-videos  labeled  with  58  thousand  tags.  To  analyze  this  data,  we  introduce  viewpoint-specific  and  temporally-evolving  models  for  video  understanding,  defined  over  state-of-the-art  motion  and  deep  visual  features.  We  conclude  that  our  dataset  opens  up  new  research  opportunities  for  large-scale  video  analysis,  novel  viewpoints,  and  open-world  dynamics.
0	Learning  models  for  actions  and  person  object  interactions  with  transfer  to  question  answering.  This  paper  proposes  deep  convolutional  network  models  that  utilize  local  and  global  context  to  make  human  activity  label  predictions  in  still  images,  achieving  state-of-the-art  performance  on  two  recent  datasets  with  hundreds  of  labels  each.  We  use  multiple  instance  learning  to  handle  the  lack  of  supervision  on  the  level  of  individual  person  instances,  and  weighted  loss  to  handle  unbalanced  training  data.  Further,  we  show  how  specialized  features  trained  on  these  datasets  can  be  used  to  improve  accuracy  on  the  Visual  Question  Answering  (VQA)  task,  in  the  form  of  multiple  choice  fill-in-the-blank  questions  (Visual  Madlibs).  Specifically,  we  tackle  two  types  of  questions  on  person  activity  and  person-object  relationship  and  show  improvements  over  generic  features  trained  on  the  ImageNet  classification  task.
0	Bayesian  inference  of  bijective  non  rigid  shape  correspondence.  Many  algorithms  for  the  computation  of  correspondences  between  deformable  shapes  rely  on  some  variant  of  nearest  neighbor  matching  in  a  descriptor  space.  Such  are,  for  example,  various  point-wise  correspondence  recovery  algorithms  used  as  a  postprocessing  stage  in  the  functional  correspondence  framework.  In  this  paper,  we  show  that  such  frequently  used  techniques  in  practice  suffer  from  lack  of  accuracy  and  result  in  poor  surjectivity.  We  propose  an  alternative  recovery  technique  guaranteeing  a  bijective  correspondence  and  producing  significantly  higher  accuracy.  We  derive  the  proposed  method  from  a  statistical  framework  of  Bayesian  inference  and  demonstrate  its  performance  on  several  challenging  deformable  3D  shape  matching  datasets.
0	Learning  based  animation  of  clothing  for  virtual  try  on.  This  paper  presents  a  learning-based  clothing  animation  method  for  highly  efficient  virtual  try-on  simulation.  Given  a  garment,  we  preprocess  a  rich  database  of  physically-based  dressed  character  simulations,  for  multiple  body  shapes  and  animations.  Then,  using  this  database,  we  train  a  learning-based  model  of  cloth  drape  and  wrinkles,  as  a  function  of  body  shape  and  dynamics.  We  propose  a  model  that  separates  global  garment  fit,  due  to  body  shape,  from  local  garment  wrinkles,  due  to  both  pose  dynamics  and  body  shape.  We  use  a  recurrent  neural  network  to  regress  garment  wrinkles,  and  we  achieve  highly  plausible  nonlinear  effects,  in  contrast  to  the  blending  artifacts  suffered  by  previous  methods.  At  runtime,  dynamic  virtual  try-on  animations  are  produced  in  just  a  few  milliseconds  for  garments  with  thousands  of  triangles.  We  show  qualitative  and  quantitative  analysis  of  results
0	Hybrid  retrieval  generation  reinforced  agent  for  medical  image  report  generation.  Generating  long  and  coherent  reports  to  describe  medical  images  poses  challenges  to  bridging  visual  patterns  with  informative  human  linguistic  descriptions.  We  propose  a  novel  Hybrid  Retrieval-Generation  Reinforced  Agent  (HRGR-Agent)  which  reconciles  traditional  retrieval-based  approaches  populated  with  human  prior  knowledge,  with  modern  learning-based  approaches  to  achieve  structured,  robust,  and  diverse  report  generation.  HRGR-Agent  employs  a  hierarchical  decision-making  procedure.  For  each  sentence,  a  high-level  retrieval  policy  module  chooses  to  either  retrieve  a  template  sentence  from  an  off-the-shelf  template  database,  or  invoke  a  low-level  generation  module  to  generate  a  new  sentence.  HRGR-Agent  is  updated  via  reinforcement  learning,  guided  by  sentence-level  and  word-level  rewards.  Experiments  show  that  our  approach  achieves  the  state-of-the-art  results  on  two  medical  report  datasets,  generating  well-balanced  structured  sentences  with  robust  coverage  of  heterogeneous  medical  report  contents.  In  addition,  our  model  achieves  the  highest  detection  accuracy  of  medical  terminologies,  and  improved  human  evaluation  performance.
0	Real  time  deep  registration  with  geodesic  loss.  With  an  aim  to  increase  the  capture  range  and  accelerate  the  performance  of  state-of-the-art  inter-subject  and  subject-to-template  3D  registration,  we  propose  deep  learning-based  methods  that  are  trained  to  find  the  3D  position  of  arbitrarily  oriented  subjects  or  anatomy  based  on  slices  or  volumes  of  medical  images.  For  this,  we  propose  regression  CNNs  that  learn  to  predict  the  angle-axis  representation  of  3D  rotations  and  translations  using  image  features.  We  use  and  compare  mean  square  error  and  geodesic  loss  for  training  regression  CNNs  in  two  different  scenarios:  3D  pose  estimation  from  slices  and  3D  to  3D  registration.  As  an  exemplary  application,  we  applied  the  proposed  methods  to  register  arbitrarily  oriented  reconstructed  images  of  fetuses  scanned  in-utero  at  a  wide  gestational  age  range  to  a  standard  atlas  space.  Our  results  show  that  in  such  registration  applications  that  are  amendable  to  learning,  the  proposed  deep  learning  methods  with  geodesic  loss  minimization  can  achieve  accurate  results  with  a  wide  capture  range  in  real-time  (<100ms).  We  tested  the  generalization  capability  of  the  trained  CNNs  on  an  expanded  age  range  and  on  images  of  newborn  subjects  with  similar  and  different  MR  image  contrasts.  We  trained  our  models  on  T2-weighted  fetal  brain  MRI  scans  and  used  them  to  predict  the  3D  position  of  newborn  brains  based  on  T1-weighted  MRI  scans.  We  showed  that  trained  models  generalized  well  for  the  new  domain  when  we  performed  image  contrast  transfer  through  a  conditional  generative  adversarial  network.  This  indicates  that  the  domain  of  application  of  the  trained  deep  regression  CNNs  can  be  further  expanded  to  image  modalities  and  contrasts  other  than  those  used  in  training.  A  combination  of  our  proposed  methods  with  optimization-based  registration  algorithms  can  dramatically  enhance  the  performance  of  automatic  imaging  devices  and  image  processing  methods  of  the  future.
0	Csgnet  neural  shape  parser  for  constructive  solid  geometry.  We  present  a  neural  architecture  that  takes  as  input  a  2D  or  3D  shape  and  induces  a  program  to  generate  it.  The  in-  structions  in  our  program  are  based  on  constructive  solid  geometry  principles,  i.e.,  a  set  of  boolean  operations  on  shape  primitives  defined  recursively.  Bottom-up  techniques  for  this  task  that  rely  on  primitive  detection  are  inherently  slow  since  the  search  space  over  possible  primitive  combi-  nations  is  large.  In  contrast,  our  model  uses  a  recurrent  neural  network  conditioned  on  the  input  shape  to  produce  a  sequence  of  instructions  in  a  top-down  manner  and  is  sig-  nificantly  faster.  It  is  also  more  effective  as  a  shape  detec-  tor  than  existing  state-of-the-art  detection  techniques.  We  also  demonstrate  that  our  network  can  be  trained  on  novel  datasets  without  ground-truth  program  annotations  through  policy  gradient  techniques.
0	Learning  dynamic  hierarchical  models  for  anytime  scene  labeling.  With  increasing  demand  for  efficient  image  and  video  analysis,  test-time  cost  of  scene  parsing  becomes  critical  for  many  large-scale  or  time-sensitive  vision  applications.  We  propose  a  dynamic  hierarchical  model  for  anytime  scene  labeling  that  allows  us  to  achieve  flexible  trade-offs  between  efficiency  and  accuracy  in  pixel-level  prediction.  In  particular,  our  approach  incorporates  the  cost  of  feature  computation  and  model  inference,  and  optimizes  the  model  performance  for  any  given  test-time  budget  by  learning  a  sequence  of  image-adaptive  hierarchical  models.  We  formulate  this  anytime  representation  learning  as  a  Markov  Decision  Process  with  a  discrete-continuous  state-action  space.  A  high-quality  policy  of  feature  and  model  selection  is  learned  based  on  an  approximate  policy  iteration  method  with  action  proposal  mechanism.  We  demonstrate  the  advantages  of  our  dynamic  non-myopic  anytime  scene  parsing  on  three  semantic  segmentation  datasets,  which  achieves  $90\%$  of  the  state-of-the-art  performances  by  using  $15\%$  of  their  overall  costs.
0	A  noise  sensitivity  analysis  based  test  prioritization  technique  for  deep  neural  networks.  Deep  neural  networks  (DNNs)  have  been  widely  used  in  the  fields  such  as  natural  language  processing,  computer  vision  and  image  recognition.  But  several  studies  have  been  shown  that  deep  neural  networks  can  be  easily  fooled  by  artificial  examples  with  some  perturbations,  which  are  widely  known  as  adversarial  examples.  Adversarial  examples  can  be  used  to  attack  deep  neural  networks  or  to  improve  the  robustness  of  deep  neural  networks.  A  common  way  of  generating  adversarial  examples  is  to  first  generate  some  noises  and  then  add  them  into  original  examples.  In  practice,  different  examples  have  different  noise-sensitive.  To  generate  an  effective  adversarial  example,  it  may  be  necessary  to  add  a  lot  of  noise  to  low  noise-sensitive  example,  which  may  make  the  adversarial  example  meaningless.  In  this  paper,  we  propose  a  noise-sensitivity-analysis-based  test  prioritization  technique  to  pick  out  examples  by  their  noise  sensitivity.  We  construct  an  experiment  to  validate  our  approach  on  four  image  sets  and  two  DNN  models,  which  shows  that  examples  are  sensitive  to  noise  and  our  method  can  effectively  pick  out  examples  by  their  noise  sensitivity.
0	Semantic  segmentation  from  remote  sensor  data  and  the  exploitation  of  latent  learning  for  classification  of  auxiliary  tasks.  In  this  paper  we  address  three  different  aspects  of  semantic  segmentation  from  remote  sensor  data  using  deep  neural  networks.  Firstly,  we  focus  on  the  semantic  segmentation  of  buildings  from  remote  sensor  data  and  propose  ICT-Net.  The  proposed  network  has  been  tested  on  the  INRIA  and  AIRS  benchmark  datasets  and  is  shown  to  outperform  all  other  state  of  the  art  by  more  than  1.5%  and  1.8%  on  the  Jaccard  index,  respectively.    Secondly,  as  the  building  classification  is  typically  the  first  step  of  the  reconstruction  process,  we  investigate  the  relationship  of  the  classification  accuracy  to  the  reconstruction  accuracy.    Finally,  we  present  the  simple  yet  compelling  concept  of  latent  learning  and  the  implications  it  carries  within  the  context  of  deep  learning.  We  posit  that  a  network  trained  on  a  primary  task  (i.e.  building  classification)  is  unintentionally  learning  about  auxiliary  tasks  (e.g.  the  classification  of  road,  tree,  etc)  which  are  complementary  to  the  primary  task.  We  extensively  tested  the  proposed  technique  on  the  ISPRS  benchmark  dataset  which  contains  multi-label  ground  truth,  and  report  an  average  classification  accuracy  (F1  score)  of  54.29%  (SD=17.03)  for  roads,  10.15%  (SD=2.54)  for  cars,  24.11%  (SD=5.25)  for  trees,  42.74%  (SD=6.62)  for  low  vegetation,  and  18.30%  (SD=16.08)  for  clutter.    The  source  code  and  supplemental  material  is  publicly  available  at  this  http  URL.
0	Single  view  place  recognition  under  seasonal  changes.  Single-view  place  recognition,  that  we  can  define  as  finding  an  image  that  corresponds  to  the  same  place  as  a  given  query  image,  is  a  key  capability  for  autonomous  navigation  and  mapping.  Although  there  has  been  a  considerable  amount  of  research  in  the  topic,  the  high  degree  of  image  variability  (with  viewpoint,  illumination  or  occlusions  for  example)  makes  it  a  research  challenge.    One  of  the  particular  challenges,  that  we  address  in  this  work,  is  weather  variation.  Seasonal  changes  can  produce  drastic  appearance  changes,  that  classic  low-level  features  do  not  model  properly.  Our  contributions  in  this  paper  are  twofold.  First  we  pre-process  and  propose  a  partition  for  the  Nordland  dataset,  frequently  used  for  place  recognition  research  without  consensus  on  the  partitions.  And  second,  we  evaluate  several  neural  network  architectures  such  as  pre-trained,  siamese  and  triplet  for  this  problem.  Our  best  results  outperform  the  state  of  the  art  of  the  field.  A  video  showing  our  results  can  be  found  in  this  https  URL.  The  partitioned  version  of  the  Nordland  dataset  at  this  http  URL.
0	A  structural  causal  model  for  mr  images  of  multiple  sclerosis.  Precision  medicine  involves  answering  counterfactual  questions  such  as  "Would  this  patient  respond  better  to  treatment  A  or  treatment  B?"  These  types  of  questions  are  causal  in  nature  and  require  the  tools  of  causal  inference  to  be  answered,  e.g.,  with  a  structural  causal  model  (SCM).  In  this  work,  we  develop  an  SCM  that  models  the  interaction  between  demographic  information,  disease  covariates,  and  magnetic  resonance  (MR)  images  of  the  brain  for  people  with  multiple  sclerosis  (MS).  Inference  in  the  SCM  generates  counterfactual  images  that  show  what  an  MR  image  of  the  brain  would  look  like  when  demographic  or  disease  covariates  are  changed.  These  images  can  be  used  for  modeling  disease  progression  or  used  for  downstream  image  processing  tasks  where  controlling  for  confounders  is  necessary.
0	Relgan  multi  domain  image  to  image  translation  via  relative  attributes.  Multi-domain  image-to-image  translation  has  gained  increasing  attention  recently.  Previous  methods  take  an  image  and  some  target  attributes  as  inputs  and  generate  an  output  image  with  the  desired  attributes.  However,  such  methods  have  two  limitations.  First,  these  methods  assume  binary-valued  attributes  and  thus  cannot  yield  satisfactory  results  for  fine-grained  control.  Second,  these  methods  require  specifying  the  entire  set  of  target  attributes,  even  if  most  of  the  attributes  would  not  be  changed.  To  address  these  limitations,  we  propose  RelGAN,  a  new  method  for  multi-domain  image-to-image  translation.  The  key  idea  is  to  use  relative  attributes,  which  describes  the  desired  change  on  selected  attributes.  Our  method  is  capable  of  modifying  images  by  changing  particular  attributes  of  interest  in  a  continuous  manner  while  preserving  the  other  attributes.  Experimental  results  demonstrate  both  the  quantitative  and  qualitative  effectiveness  of  our  method  on  the  tasks  of  facial  attribute  transfer  and  interpolation.
0	Outlier  robust  estimation  hardness  minimally  tuned  algorithms  and  applications.  Nonlinear  estimation  in  robotics  and  vision  is  typically  plagued  with  outliers  due  to  wrong  data  association,  or  to  incorrect  detections  from  signal  processing  and  machine  learning  methods.  This  paper  introduces  two  unifying  formulations  for  outlier-robust  estimation,  Generalized  Maximum  Consensus  (G-MC)  and  Generalized  Truncated  Least  Squares  (G-TLS),  and  investigates  fundamental  limits,  practical  algorithms,  and  applications.  Our  first  contribution  is  a  proof  that  outlier-robust  estimation  is  inapproximable:  in  the  worst  case,  it  is  impossible  to  (even  approximately)  find  the  set  of  outliers,  even  with  slower-than-polynomial-time  algorithms  (particularly,  algorithms  running  in  quasi-polynomial  time).  As  a  second  contribution,  we  review  and  extend  two  general-purpose  algorithms.  The  first,  Adaptive  Trimming  (ADAPT),  is  combinatorial,  and  is  suitable  for  G-MC;  the  second,  Graduated  Non-Convexity  (GNC),  is  based  on  homotopy  methods,  and  is  suitable  for  G-TLS.  We  extend  ADAPT  and  GNC  to  the  case  where  the  user  does  not  have  prior  knowledge  of  the  inlier-noise  statistics  (or  the  statistics  may  vary  over  time)  and  is  unable  to  guess  a  reasonable  threshold  to  separate  inliers  from  outliers  (as  the  one  commonly  used  in  RANSAC).  We  propose  the  first  minimally  tuned  algorithms  for  outlier  rejection,  that  dynamically  decide  how  to  separate  inliers  from  outliers.  Our  third  contribution  is  an  evaluation  of  the  proposed  algorithms  on  robot  perception  problems:  mesh  registration,  image-based  object  detection  (shape  alignment),  and  pose  graph  optimization.  ADAPT  and  GNC  execute  in  real-time,  are  deterministic,  outperform  RANSAC,  and  are  robust  up  to  80-90%  outliers.  Their  minimally  tuned  versions  also  compare  favorably  with  the  state  of  the  art,  even  though  they  do  not  rely  on  a  noise  bound  for  the  inliers.
0	Audio  visual  waypoints  for  navigation.  In  audio-visual  navigation,  an  agent  intelligently  travels  through  a  complex,  unmapped  3D  environment  using  both  sights  and  sounds  to  find  a  sound  source  (e.g.,  a  phone  ringing  in  another  room).  Existing  models  learn  to  act  at  a  fixed  granularity  of  agent  motion  and  rely  on  simple  recurrent  aggregations  of  the  audio  observations.  We  introduce  a  reinforcement  learning  approach  to  audio-visual  navigation  with  two  key  novel  elements  1)  audio-visual  waypoints  that  are  dynamically  set  and  learned  end-to-end  within  the  navigation  policy,  and  2)  an  acoustic  memory  that  provides  a  structured,  spatially  grounded  record  of  what  the  agent  has  heard  as  it  moves.  Both  new  ideas  capitalize  on  the  synergy  of  audio  and  visual  data  for  revealing  the  geometry  of  an  unmapped  space.  We  demonstrate  our  approach  on  the  challenging  Replica  environments  of  real-world  3D  scenes.  Our  model  improves  the  state  of  the  art  by  a  substantial  margin,  and  our  experiments  reveal  that  learning  the  links  between  sights,  sounds,  and  space  is  essential  for  audio-visual  navigation.
0	Recycle  gan  unsupervised  video  retargeting.  We  introduce  a  data-driven  approach  for  unsupervised  video  retargeting  that  translates  content  from  one  domain  to  another  while  preserving  the  style  native  to  a  domain,  i.e.,  if  contents  of  John  Oliver's  speech  were  to  be  transferred  to  Stephen  Colbert,  then  the  generated  content/speech  should  be  in  Stephen  Colbert's  style.  Our  approach  combines  both  spatial  and  temporal  information  along  with  adversarial  losses  for  content  translation  and  style  preservation.  In  this  work,  we  first  study  the  advantages  of  using  spatiotemporal  constraints  over  spatial  constraints  for  effective  retargeting.  We  then  demonstrate  the  proposed  approach  for  the  problems  where  information  in  both  space  and  time  matters  such  as  face-to-face  translation,  flower-to-flower,  wind  and  cloud  synthesis,  sunrise  and  sunset.
0	Play  and  prune  adaptive  filter  pruning  for  deep  model  compression.  While  convolutional  neural  networks  (CNN)  have  achieved  impressive  performance  on  various  classification/recognition  tasks,  they  typically  consist  of  a  massive  number  of  parameters.  This  results  in  significant  memory  requirement  as  well  as  computational  overheads.  Consequently,  there  is  a  growing  need  for  filter-level  pruning  approaches  for  compressing  CNN  based  models  that  not  only  reduce  the  total  number  of  parameters  but  reduce  the  overall  computation  as  well.  We  present  a  new  min-max  framework  for  filter-level  pruning  of  CNNs.  Our  framework,  called  Play  and  Prune  (PP),  jointly  prunes  and  fine-tunes  CNN  model  parameters,  with  an  adaptive  pruning  rate,  while  maintaining  the  model's  predictive  performance.  Our  framework  consists  of  two  modules:  (1)  An  adaptive  filter  pruning  (AFP)  module,  which  minimizes  the  number  of  filters  in  the  model;  and  (2)  A  pruning  rate  controller  (PRC)  module,  which  maximizes  the  accuracy  during  pruning.  Moreover,  unlike  most  previous  approaches,  our  approach  allows  directly  specifying  the  desired  error  tolerance  instead  of  pruning  level.  Our  compressed  models  can  be  deployed  at  run-time,  without  requiring  any  special  libraries  or  hardware.  Our  approach  reduces  the  number  of  parameters  of  VGG-16  by  an  impressive  factor  of  17.5X,  and  number  of  FLOPS  by  6.43X,  with  no  loss  of  accuracy,  significantly  outperforming  other  state-of-the-art  filter  pruning  methods.
0	End  to  end  deep  learning  for  person  search.  Existing  person  re-identification  (re-id)  benchmarks  and  algorithms  mainly  focus  on  matching  cropped  pedestrian  images  between  queries  and  candidates.  However,  it  is  different  from  real-world  scenarios  where  the  annotations  of  pedestrian  bounding  boxes  are  unavailable  and  the  target  person  needs  to  be  found  from  whole  images.  To  close  the  gap,  we  investigate  how  to  localize  and  match  query  persons  from  the  scene  images  without  relying  on  the  annotations  of  candidate  boxes.  Instead  of  breaking  it  down  into  two  separate  tasks—pedestrian  detection  and  person  re-id,  we  propose  an  end-to-end  deep  learning  framework  to  jointly  handle  both  tasks.  A  random  sampling  softmax  loss  is  proposed  to  effectively  train  the  model  under  the  supervision  of  sparse  and  unbalanced  labels.  On  the  other  hand,  existing  benchmarks  are  small  in  scale  and  the  samples  are  collected  from  a  few  fixed  camera  views  with  low  scene  diversities.  To  address  this  issue,  we  collect  a  largescale  and  scene-diversified  person  search  dataset,  which  contains  18,184  images,  8,432  persons,  and  99,809  annotated  bounding  boxes1.  We  evaluate  our  approach  and  other  baselines  on  the  proposed  dataset,  and  study  the  influence  of  various  factors.  Experiments  show  that  our  method  achieves  the  best  result.
0	Attention  aware  generative  adversarial  networks  ata  gans.  In  this  work,  we  present  a  novel  approach  for  training  Generative  Adversarial  Networks  (GANs).  Using  the  attention  maps  produced  by  a  Teacher-  Network  we  are  able  to  improve  the  quality  of  the  generated  images  as  well  as  perform  weakly  object  localization  on  the  generated  images.  To  this  end,  we  generate  images  of  HEp-2  cells  captured  with  Indirect  Imunofluoresence  (IIF)  and  study  the  ability  of  our  network  to  perform  a  weakly  localization  of  the  cell.  Firstly,  we  demonstrate  that  whilst  GANs  can  learn  the  mapping  between  the  input  domain  and  the  target  distribution  efficiently,  the  discriminator  network  is  not  able  to  detect  the  regions  of  interest.  Secondly,  we  present  a  novel  attention  transfer  mechanism  which  allows  us  to  enforce  the  discriminator  to  put  emphasis  on  the  regions  of  interest  via  transfer  learning.  Thirdly,  we  show  that  this  leads  to  more  realistic  images,  as  the  discriminator  learns  to  put  emphasis  on  the  area  of  interest.  Fourthly,  the  proposed  method  allows  one  to  generate  both  images  as  well  as  attention  maps  which  can  be  useful  for  data  annotation  e.g  in  object  detection.
0	Physical  primitive  decomposition.  Objects  are  made  of  parts,  each  with  distinct  geometry,  physics,  functionality,  and  affordances.  Developing  such  a  distributed,  physical,  interpretable  representation  of  objects  will  facilitate  intelligent  agents  to  better  explore  and  interact  with  the  world.  In  this  paper,  we  study  physical  primitive  decomposition---understanding  an  object  through  its  components,  each  with  physical  and  geometric  attributes.  As  annotated  data  for  object  parts  and  physics  are  rare,  we  propose  a  novel  formulation  that  learns  physical  primitives  by  explaining  both  an  object's  appearance  and  its  behaviors  in  physical  events.  Our  model  performs  well  on  block  towers  and  tools  in  both  synthetic  and  real  scenarios;  we  also  demonstrate  that  visual  and  physical  observations  often  provide  complementary  signals.  We  further  present  ablation  and  behavioral  studies  to  better  understand  our  model  and  contrast  it  with  human  performance.
0	Learning  about  objects  by  learning  to  interact  with  them.  Much  of  the  remarkable  progress  in  computer  vision  has  been  focused  around  fully  supervised  learning  mechanisms  relying  on  highly  curated  datasets  for  a  variety  of  tasks.  In  contrast,  humans  often  learn  about  their  world  with  little  to  no  external  supervision.  Taking  inspiration  from  infants  learning  from  their  environment  through  play  and  interaction,  we  present  a  computational  framework  to  discover  objects  and  learn  their  physical  properties  along  this  paradigm  of  Learning  from  Interaction.  Our  agent,  when  placed  within  the  near  photo-realistic  and  physics-enabled  AI2-THOR  environment,  interacts  with  its  world  and  learns  about  objects,  their  geometric  extents  and  relative  masses,  without  any  external  guidance.  Our  experiments  reveal  that  this  agent  learns  efficiently  and  effectively;  not  just  for  objects  it  has  interacted  with  before,  but  also  for  novel  instances  from  seen  categories  as  well  as  novel  object  categories.
0	Deep  learning  for  deepfakes  creation  and  detection  a  survey.  Deep  learning  has  been  successfully  applied  to  solve  various  complex  problems  ranging  from  big  data  analytics  to  computer  vision  and  human-level  control.  Deep  learning  advances  however  have  also  been  employed  to  create  software  that  can  cause  threats  to  privacy,  democracy  and  national  security.  One  of  those  deep  learning-powered  applications  recently  emerged  is  "deepfake".  Deepfake  algorithms  can  create  fake  images  and  videos  that  humans  cannot  distinguish  them  from  authentic  ones.  The  proposal  of  technologies  that  can  automatically  detect  and  assess  the  integrity  of  digital  visual  media  is  therefore  indispensable.  This  paper  presents  a  survey  of  algorithms  used  to  create  deepfakes  and,  more  importantly,  methods  proposed  to  detect  deepfakes  in  the  literature  to  date.  We  present  extensive  discussions  on  challenges,  research  trends  and  directions  related  to  deepfake  technologies.  By  reviewing  the  background  of  deepfakes  and  state-of-the-art  deepfake  detection  methods,  this  study  provides  a  comprehensive  overview  of  deepfake  techniques  and  facilitates  the  development  of  new  and  more  robust  methods  to  deal  with  the  increasingly  challenging  deepfakes.
0	Reliable  probabilistic  face  embeddings  in  the  wild.  Probabilistic  Face  Embeddings  (PFE)  can  improve  face  recognition  performance  in  unconstrained  scenarios  by  integrating  data  uncertainty  into  the  feature  representation.  However,  existing  PFE  methods  tend  to  be  over-confident  in  estimating  uncertainty  and  is  too  slow  to  apply  to  large-scale  face  matching.  This  paper  proposes  a  regularized  probabilistic  face  embedding  method  to  improve  the  robustness  and  speed  of  PFE.  Specifically,  the  mutual  likelihood  score  (MLS)  metric  used  in  PFE  is  simplified  to  speedup  the  matching  of  face  feature  pairs.  Then,  an  output-constraint  loss  is  proposed  to  penalize  the  variance  of  the  uncertainty  output,  which  can  regularize  the  output  of  the  neural  network.  In  addition,  an  identification  preserving  loss  is  proposed  to  improve  the  discriminative  of  the  MLS  metric,  and  a  multi-layer  feature  fusion  module  is  proposed  to  improve  the  neural  network's  uncertainty  estimation  ability.  Comprehensive  experiments  show  that  the  proposed  method  can  achieve  comparable  or  better  results  in  8  benchmarks  than  the  state-of-the-art  methods,  and  can  improve  the  performance  of  risk-controlled  face  recognition.  The  code  of  ProbFace  is  publicly  available  in  GitHub  (this  https  URL).
0	Centerhmr  a  bottom  up  single  shot  method  for  multi  person  3d  mesh  recovery  from  a  single  image.  In  this  paper,  we  propose  a  method  to  recover  multi-person  3D  mesh  from  a  single  image.  Existing  methods  follow  a  multi-stage  detection-based  pipeline,  where  the  3D  mesh  of  each  person  is  regressed  from  the  cropped  image  patch.  They  have  to  suffer  from  the  high  complexity  of  the  multi-stage  process  and  the  ambiguity  of  the  image-level  features.  For  example,  it  is  hard  for  them  to  estimate  multi-person  3D  mesh  from  the  inseparable  crowded  cases.  Instead,  in  this  paper,  we  present  a  novel  bottom-up  single-shot  method,  Center-based  Human  Mesh  Recovery  network  (CenterHMR).  The  model  is  trained  to  simultaneously  predict  two  maps,  which  represent  the  location  of  each  human  body  center  and  the  corresponding  parameter  vector  of  3D  human  mesh  at  each  center.  This  explicit  center-based  representation  guarantees  the  pixel-level  feature  encoding.  Besides,  the  3D  mesh  result  of  each  person  is  estimated  from  the  features  centered  at  the  visible  body  parts,  which  improves  the  robustness  under  occlusion.  CenterHMR  surpasses  previous  methods  on  multi-person  in-the-wild  benchmark  3DPW  and  occlusion  dataset  3DOH50K.  Besides,  CenterHMR  has  achieved  a  2-nd  place  on  ECCV  2020  3DPW  Challenge.  The  code  is  released  on  https://github.com/Arthur151/CenterHMR.
0	Online  invariance  selection  for  local  feature  descriptors.  To  be  invariant,  or  not  to  be  invariant:  that  is  the  question  formulated  in  this  work  about  local  descriptors.  A  limitation  of  current  feature  descriptors  is  the  trade-off  between  generalization  and  discriminative  power:  more  invariance  means  less  informative  descriptors.  We  propose  to  overcome  this  limitation  with  a  disentanglement  of  invariance  in  local  descriptors  and  with  an  online  selection  of  the  most  appropriate  invariance  given  the  context.  Our  framework  consists  in  a  joint  learning  of  multiple  local  descriptors  with  different  levels  of  invariance  and  of  meta  descriptors  encoding  the  regional  variations  of  an  image.  The  similarity  of  these  meta  descriptors  across  images  is  used  to  select  the  right  invariance  when  matching  the  local  descriptors.  Our  approach,  named  Local  Invariance  Selection  at  Runtime  for  Descriptors  (LISRD),  enables  descriptors  to  adapt  to  adverse  changes  in  images,  while  remaining  discriminative  when  invariance  is  not  required.  We  demonstrate  that  our  method  can  boost  the  performance  of  current  descriptors  and  outperforms  state-of-the-art  descriptors  in  several  matching  tasks,  when  evaluated  on  challenging  datasets  with  day-night  illumination  as  well  as  viewpoint  changes.
0	Haa500  human  centric  atomic  action  dataset  with  curated  videos.  We  contribute  HAA500,  a  manually  annotated  human-centric  atomic  action  dataset  for  action  recognition  on  500  classes  with  over  591k  labeled  frames.  Unlike  existing  atomic  action  datasets,  where  coarse-grained  atomic  actions  were  labeled  with  action-verbs,  e.g.,  "Throw",  HAA500  contains  fine-grained  atomic  actions  where  only  consistent  actions  fall  under  the  same  label,  e.g.,  "Baseball  Pitching"  vs  "Free  Throw  in  Basketball",  to  minimize  ambiguities  in  action  classification.  HAA500  has  been  carefully  curated  to  capture  the  movement  of  human  figures  with  less  spatio-temporal  label  noises  to  greatly  enhance  the  training  of  deep  neural  networks.  The  advantages  of  HAA500  include:  1)  human-centric  actions  with  a  high  average  of  69.7%  detectable  joints  for  the  relevant  human  poses;  2)  each  video  captures  the  essential  elements  of  an  atomic  action  without  irrelevant  frames;  3)  fine-grained  atomic  action  classes.  Our  extensive  experiments  validate  the  benefits  of  human-centric  and  atomic  characteristics  of  HAA,  which  enables  the  trained  model  to  improve  prediction  by  attending  to  atomic  human  poses.  We  detail  the  HAA500  dataset  statistics  and  collection  methodology,  and  compare  quantitatively  with  existing  action  recognition  datasets.
0	Trained  trajectory  based  automated  parking  system  using  visual  slam  on  surround  view  cameras.  Automated  Parking  is  becoming  a  standard  feature  in  modern  vehicles.  Existing  parking  systems  build  a  local  map  to  be  able  to  plan  for  maneuvering  towards  a  detected  slot.  Next  generation  parking  systems  have  an  use  case  where  they  build  a  persistent  map  of  the  environment  where  the  car  is  frequently  parked,  say  for  example,  home  parking  or  office  parking.  The  pre-built  map  helps  in  re-localizing  the  vehicle  better  when  its  trying  to  park  the  next  time.  This  is  achieved  by  augmenting  the  parking  system  with  a  Visual  SLAM  pipeline  and  the  feature  is  called  trained  trajectory  parking  in  the  automotive  industry.  In  this  paper,  we  discuss  the  use  cases,  design  and  implementation  of  a  trained  trajectory  automated  parking  system.  The  proposed  system  is  deployed  on  commercial  vehicles  and  the  consumer  application  is  illustrated  in  \url{this  https  URL}.  The  focus  of  this  paper  is  on  the  application  and  the  details  of  vision  algorithms  are  kept  at  high  level.
0	How  incomplete  is  contrastive  learning  an  inter  intra  variant  dual  representation  method  for  self  supervised  video  recognition.  Contrastive  learning  applied  to  self-supervised  representation  learning  has  seen  a  resurgence  in  deep  models.  In  this  paper,  we  find  that  existing  contrastive  learning  based  solutions  for  self-supervised  video  recognition  focus  on  inter-variance  encoding  but  ignore  the  intra-variance  existing  in  clips  within  the  same  video.  We  thus  propose  to  learn  dual  representations  for  each  clip  which  (\romannumeral  1)  encode  intra-variance  through  a  shuffle-rank  pretext  task;  (\romannumeral  2)  encode  inter-variance  through  a  temporal  coherent  contrastive  loss.  Experiment  results  show  that  our  method  plays  an  essential  role  in  balancing  inter  and  intra  variances  and  brings  consistent  performance  gains  on  multiple  backbones  and  contrastive  learning  frameworks.  Integrated  with  SimCLR  and  pretrained  on  Kinetics-400,  our  method  achieves  $\textbf{82.0\%}$  and  $\textbf{51.2\%}$  downstream  classification  accuracy  on  UCF101  and  HMDB51  test  sets  respectively  and  $\textbf{46.1\%}$  video  retrieval  accuracy  on  UCF101,  outperforming  both  pretext-task  based  and  contrastive  learning  based  counterparts.
0	Cordial  labeling  for  the  splitting  graph  of  some  standard  graphs.  In  this  paper  we  prove  that  the  splitting  graph  of  path  Pn,  cycle  Cn,  complete  bipartite  graph  Km,n,  matching  Mn,  wheel  Wn  and  are  cordial.
0	Parallel  computations  for  real  time  implementation  of  adaptive  sparse  approximation  methods.  The  article  is  devoted  to  a  new  approach  to  the  analysis  and  modeling  of  short  geoacoustic  signals.  A  parallel  implementation  of  the  applied  methods  is  described.
0	Constructing  knowledge  using  exploratory  text  mining.  Our  goal  is  to  support  users  who  want  to  discover  or  create  knowledge  from  a  large  amount  of  text  data.  Text  mining  is  a  process  that  extracts  novel  knowledge  from  unstructured  data.  A  variety  of  applications  for  text  mining,  such  as  Total  Environment  for  Text  Data  Mining,  have  been  proposed.  However,  to  the  best  of  our  knowledge,  these  methods  are  not  effective  at  conducting  text  mining  tasks  aimed  at  finding  novel  knowledge.  In  this  paper,  we  discuss  characteristics  of  the  text  mining  process  and  propose  a  design  principle  for  building  text  mining  applications  based  on  two  concepts:  (1)  text  mining  is  an  exploratory  search  task,  and  (2)  text  mining  is  a  process  for  creative  knowledge  work.
0	Automatic  traffic  control  system  using  pca  based  approach.  In  recent  years,  communication  in  wireless  is  well  improved  technology  for  moving  the  data  like  text,  image,  video,  and  audio.  But  in  WN  (wireless  network),  overload  is  a  important  issues  of  packet  loss  and  traffic,  and  high  excellence  video  and  bandwidth  limit  are  important  network  surplus  thread.  To  defeat  these  issues,  Shih-Chia  Huang  Bo-Hao  suggested  PCA-related  RBF  network  method  for  moving  object  identification  (motion  detection).  This  method  first  changes  the  video  bit  rate  as  per  the  level  of  bandwidth.  If  bandwidth  level  is  more,  then  the  video  bit  rate  would  be  more,  if  bandwidth  level  is  minimum  then  the  moving  bit  rate  of  video  would  be  more.  Also  this  method  identifies  the  moving  object  in  low  video  bit  rate.  We  apply  this  method  on  automatic  traffic  control  system.  In  existing  there  is  no  automatic  system  for  monitor  the  traffic  system.  These  above  approaches  are  useful  in  identifying  the  traffic  level  and  to  control  the  traffic  automatically
0	A  new  entropy  based  approach  for  fuzzy  c  means  clustering  and  its  application  to  brain  mr  image  segmentation.  Automated  segmentation  of  different  tissue  regions  from  brain  magnetic  resonance  (MR)  imaging  has  a  substantial  impact  on  many  computer-assisted  neuro-imaging  studies.  Major  challenges  to  accomplish  this  task  emerge  from  limited  spatial  resolution,  signal-to-noise  ratio,  and  RF  coil  inhomogeneity.  These  imaging  artifacts  lead  to  fuzziness  of  tissue  boundaries  and  uncertainty  in  MR  intensity-based  tissue  characterization  at  individual  image  voxels.  The  conventional  fuzzy  c-means  (FCM)  algorithm  fails  to  produce  satisfactory  results  for  noisy  image.  In  this  paper,  we  present  an  entropy-based  FCM  segmentation  method  that  incorporates  the  uncertainty  of  classification  of  individual  pixels  within  the  classical  framework  of  FCM.  Furthermore,  instead  of  Euclidean  distance,  we  have  defined  the  non-Euclidean  distance  based  on  Gaussian  probability  density  function.  The  new  segmentation  method  was  applied  to  Brainweb  brain  MR  database  at  varying  noise  and  inhomogeneity,  and  its  performance  was  compared  with  existing  FCM-based  algorithms.  The  proposed  method  yields  superior  performance  over  some  classical  state-of-the-art  methods.  In  addition  to  this,  we  also  have  performed  the  proposed  method  on  some  in  vivo  human  brain  MR  data  to  demonstrate  its  performance.
0	Real  time  human  activity  recognition  from  accelerometer  data  using  convolutional  neural  networks.  Abstract  With  a  widespread  of  various  sensors  embedded  in  mobile  devices,  the  analysis  of  human  daily  activities  becomes  more  common  and  straightforward.  This  task  now  arises  in  a  range  of  applications  such  as  healthcare  monitoring,  fitness  tracking  or  user-adaptive  systems,  where  a  general  model  capable  of  instantaneous  activity  recognition  of  an  arbitrary  user  is  needed.  In  this  paper,  we  present  a  user-independent  deep  learning-based  approach  for  online  human  activity  classification.  We  propose  using  Convolutional  Neural  Networks  for  local  feature  extraction  together  with  simple  statistical  features  that  preserve  information  about  the  global  form  of  time  series.  Furthermore,  we  investigate  the  impact  of  time  series  length  on  the  recognition  accuracy  and  limit  it  up  to  1 s  that  makes  possible  continuous  real-time  activity  classification.  The  accuracy  of  the  proposed  approach  is  evaluated  on  two  commonly  used  WISDM  and  UCI  datasets  that  contain  labeled  accelerometer  data  from  36  and  30  users  respectively,  and  in  cross-dataset  experiment.  The  results  show  that  the  proposed  model  demonstrates  state-of-the-art  performance  while  requiring  low  computational  cost  and  no  manual  feature  engineering.
0	A  fuzzy  decision  tree  algorithm  based  on  c4  5.  Decision  trees  have  been  successfully  applied  to  many  areas  for  tasks  such  as  classification,  regression,  and  feature  subset  selection.  Decision  trees  are  popular  models  in  machine  learning  due  to  the  fact  that  they  produce  graphical  models,  as  well  as  text  rules,  that  end  users  can  easily  understand.  Moreover,  their  induction  process  is  usually  fast,  requiring  low  computational  resources.  Fuzzy  systems,  on  the  other  hand,  provide  mechanisms  to  handle  imprecision  and  uncertainty  in  data,  based  on  the  fuzzy  logic  and  fuzzy  sets  theory.  The  combination  of  fuzzy  systems  and  decision  trees  has  produced  fuzzy  decision  tree  models,  which  benefit  from  both  techniques  to  provide  simple,  accurate,  and  highly  interpretable  models  at  low  computational  costs.  In  this  paper,  we  expand  previous  experiments  and  present  more  details  of  the  FuzzyDT  algorithm,  a  fuzzy  decision  tree  based  on  the  classic  C4.5  decision  tree  algorithm.  Experiments  were  carried  out  using  16  datasets  comparing  FuzzyDT  with  C4.5.  This  paper  also  includes  a  comparison  of  some  relevant  issues  regarding  the  classic  and  fuzzy  models.
0	Using  data  mining  techniques  to  improve  replica  management  in  cloud  environment.  Effective  data  management  is  a  crucial  problem  in  distributed  systems  such  as  data  grid  and  cloud.  This  can  be  achieved  by  replicating  file  in  a  wise  manner,  which  reduces  data  access  time,  increases  data  availability,  reliability  and  system  load  balancing.  Determining  a  reasonable  number  and  appropriate  location  of  replicas  is  essential  decision  in  cloud  computing.  In  this  paper,  a  new  dynamic  replication  strategy  called  Data  Mining-based  Data  Replication  (DMDR)  is  proposed,  which  determines  the  correlation  of  the  data  files  accessed  using  the  file  access  history.  We  focus  particularly  on  how  extracted  knowledge  with  maximal  frequent  correlated  pattern  mining  improves  data  replication.  We  can  group  files  with  high  dependency  in  the  same  replica  set.  Through  the  DMDR  strategy,  replicas  can  be  stored  in  the  suitable  locations,  with  reduced  access  latency  according  to  the  centrality  factor.  In  addition,  due  to  the  finite  storage  space  of  each  node,  replicas  that  are  useful  for  future  tasks  can  be  wastefully  deleted  and  replaced  with  less  beneficial  ones.  Results  of  simulation  using  CloudSim  indicate  that  DMDR  strategy  has  a  relative  advantage  in  effective  network  usage,  average  response  time,  hit  ratio  in  comparison  with  current  methods.  It  can  be  concluded  from  this  investigation  that  data  mining  technique  is  effective  and  helpful  in  the  finding  of  users’  future  access  behavior  in  cloud  environment.
0	Fluid  annotation  a  human  machine  collaboration  interface  for  full  image  annotation.  We  introduce  Fluid  Annotation,  an  intuitive  human-machine  collaboration  interface  for  annotating  the  class  label  and  outline  of  every  object  and  background  region  in  an  image.  Fluid  annotation  is  based  on  three  principles:  (I)  Strong  Machine-Learning  aid.  We  start  from  the  output  of  a  strong  neural  network  model,  which  the  annotator  can  edit  by  correcting  the  labels  of  existing  regions,  adding  new  regions  to  cover  missing  objects,  and  removing  incorrect  regions.  The  edit  operations  are  also  assisted  by  the  model.  (II)  Full  image  annotation  in  a  single  pass.  As  opposed  to  performing  a  series  of  small  annotation  tasks  in  isolation,  we  propose  a  unified  interface  for  full  image  annotation  in  a  single  pass.  (III)  Empower  the  annotator.  We  empower  the  annotator  to  choose  what  to  annotate  and  in  which  order.  This  enables  concentrating  on  what  the  machine  does  not  already  know,  i.e.  putting  human  effort  only  on  the  errors  it  made.  This  helps  using  the  annotation  budget  effectively.  Through  extensive  experiments  on  the  COCO+Stuff  dataset,  we  demonstrate  that  Fluid  Annotation  leads  to  accurate  annotations  very  efficiently,  taking  three  times  less  annotation  time  than  the  popular  LabelMe  interface.
0	Learning  a  hierarchical  compositional  shape  vocabulary  for  multi  class  object  representation.  Hierarchies  allow  feature  sharing  between  objects  at  multiple  levels  of  representation,  can  code  exponential  variability  in  a  very  compact  way  and  enable  fast  inference.  This  makes  them  potentially  suitable  for  learning  and  recognizing  a  higher  number  of  object  classes.  However,  the  success  of  the  hierarchical  approaches  so  far  has  been  hindered  by  the  use  of  hand-crafted  features  or  predetermined  grouping  rules.  This  paper  presents  a  novel  framework  for  learning  a  hierarchical  compositional  shape  vocabulary  for  representing  multiple  object  classes.  The  approach  takes  simple  contour  fragments  and  learns  their  frequent  spatial  configurations.  These  are  recursively  combined  into  increasingly  more  complex  and  class-specific  shape  compositions,  each  exerting  a  high  degree  of  shape  variability.  At  the  top-level  of  the  vocabulary,  the  compositions  are  sufficiently  large  and  complex  to  represent  the  whole  shapes  of  the  objects.  We  learn  the  vocabulary  layer  after  layer,  by  gradually  increasing  the  size  of  the  window  of  analysis  and  reducing  the  spatial  resolution  at  which  the  shape  configurations  are  learned.  The  lower  layers  are  learned  jointly  on  images  of  all  classes,  whereas  the  higher  layers  of  the  vocabulary  are  learned  incrementally,  by  presenting  the  algorithm  with  one  object  class  after  another.  The  experimental  results  show  that  the  learned  multi-class  object  representation  scales  favorably  with  the  number  of  object  classes  and  achieves  a  state-of-the-art  detection  performance  at  both,  faster  inference  as  well  as  shorter  training  times.
0	Building  a  large  scale  multimodal  knowledge  base  system  for  answering  visual  queries.  The  complexity  of  the  visual  world  creates  significant  challenges  for  comprehensive  visual  understanding.  In  spite  of  recent  successes  in  visual  recognition,  today's  vision  systems  would  still  struggle  to  deal  with  visual  queries  that  require  a  deeper  reasoning.  We  propose  a  knowledge  base  (KB)  framework  to  handle  an  assortment  of  visual  queries,  without  the  need  to  train  new  classifiers  for  new  tasks.  Building  such  a  large-scale  multimodal  KB  presents  a  major  challenge  of  scalability.  We  cast  a  large-scale  MRF  into  a  KB  representation,  incorporating  visual,  textual  and  structured  data,  as  well  as  their  diverse  relations.  We  introduce  a  scalable  knowledge  base  construction  system  that  is  capable  of  building  a  KB  with  half  billion  variables  and  millions  of  parameters  in  a  few  hours.  Our  system  achieves  competitive  results  compared  to  purpose-built  models  on  standard  recognition  and  retrieval  tasks,  while  exhibiting  greater  flexibility  in  answering  richer  visual  queries.
0	Submodular  trajectory  optimization  for  aerial  3d  scanning.  Drones  equipped  with  cameras  are  emerging  as  a  powerful  tool  for  large-scale  aerial  3D  scanning,  but  existing  automatic  flight  planners  do  not  exploit  all  available  information  about  the  scene,  and  can  therefore  produce  inaccurate  and  incomplete  3D  models.  We  present  an  automatic  method  to  generate  drone  trajectories,  such  that  the  imagery  acquired  during  the  flight  will  later  produce  a  high-fidelity  3D  model.  Our  method  uses  a  coarse  estimate  of  the  scene  geometry  to  plan  camera  trajectories  that:  (1)  cover  the  scene  as  thoroughly  as  possible;  (2)  encourage  observations  of  scene  geometry  from  a  diverse  set  of  viewing  angles;  (3)  avoid  obstacles;  and  (4)  respect  a  user-specified  flight  time  budget.  Our  method  relies  on  a  mathematical  model  of  scene  coverage  that  exhibits  an  intuitive  diminishing  returns  property  known  as  submodularity.  We  leverage  this  property  extensively  to  design  a  trajectory  planning  algorithm  that  reasons  globally  about  the  non-additive  coverage  reward  obtained  across  a  trajectory,  jointly  with  the  cost  of  traveling  between  views.  We  evaluate  our  method  by  using  it  to  scan  three  large  outdoor  scenes,  and  we  perform  a  quantitative  evaluation  using  a  photorealistic  video  game  simulator.
0	Sparse  3d  convolutional  neural  networks  for  large  scale  shape  retrieval.  In  this  paper  we  present  preliminary  results  of  performance  evaluation  of  S3DCNN  -  a  Sparse  3D  Convolutional  Neural  Network  -  on  a  large-scale  3D  Shape  benchmark  ModelNet40,  and  measure  how  it  is  impacted  by  voxel  resolution  of  input  shape.  We  demonstrate  comparable  classification  and  retrieval  performance  to  state-of-the-art  models,  but  with  much  less  computational  costs  in  training  and  inference  phases.  We  also  notice  that  benefits  of  higher  input  resolution  can  be  limited  by  an  ability  of  a  neural  network  to  generalize  high  level  features.
0	Predicting  the  category  and  attributes  of  visual  search  targets  using  deep  gaze  pooling.  Predicting  the  target  of  visual  search  from  eye  fixation  (gaze)  data  is  a  challenging  problem  with  many  applications  in  human-computer  interaction.  In  contrast  to  previous  work  that  has  focused  on  individual  instances  as  a  search  target,  we  propose  the  first  approach  to  predict  categories  and  attributes  of  search  targets  based  on  gaze  data.  However,  state  of  the  art  models  for  categorical  recognition,  in  general,  require  large  amounts  of  training  data,  which  is  prohibitive  for  gaze  data.  To  address  this  challenge,  we  propose  a  novel  Gaze  Pooling  Layer  that  integrates  gaze  information  into  CNN-based  architectures  as  an  attention  mechanism  -  incorporating  both  spatial  and  temporal  aspects  of  human  gaze  behavior.  We  show  that  our  approach  is  effective  even  when  the  gaze  pooling  layer  is  added  to  an  already  trained  CNN,  thus  eliminating  the  need  for  expensive  joint  data  collection  of  visual  and  gaze  data.  We  propose  an  experimental  setup  and  data  set  and  demonstrate  the  effectiveness  of  our  method  for  search  target  prediction  based  on  gaze  behavior.  We  further  study  how  to  integrate  temporal  and  spatial  gaze  information  most  effectively,  and  indicate  directions  for  future  research  in  the  gaze-based  prediction  of  mental  states.
0	Lsda  large  scale  detection  through  adaptation.  A  major  challenge  in  scaling  object  detection  is  the  difficulty  of  obtaining  labeled  images  for  large  numbers  of  categories.  Recently,  deep  convolutional  neural  networks  (CNNs)  have  emerged  as  clear  winners  on  object  classification  benchmarks,  in  part  due  to  training  with  1.2M+  labeled  classification  images.  Unfortunately,  only  a  small  fraction  of  those  labels  are  available  for  the  detection  task.  It  is  much  cheaper  and  easier  to  collect  large  quantities  of  image-level  labels  from  search  engines  than  it  is  to  collect  detection  data  and  label  it  with  precise  bounding  boxes.  In  this  paper,  we  propose  Large  Scale  Detection  through  Adaptation  (LSDA),  an  algorithm  which  learns  the  difference  between  the  two  tasks  and  transfers  this  knowledge  to  classifiers  for  categories  without  bounding  box  annotated  data,  turning  them  into  detectors.  Our  method  has  the  potential  to  enable  detection  for  the  tens  of  thousands  of  categories  that  lack  bounding  box  annotations,  yet  have  plenty  of  classification  data.  Evaluation  on  the  ImageNet  LSVRC-2013  detection  challenge  demonstrates  the  efficacy  of  our  approach.  This  algorithm  enables  us  to  produce  a  >7.6K  detector  by  using  available  classification  data  from  leaf  nodes  in  the  ImageNet  tree.  We  additionally  demonstrate  how  to  modify  our  architecture  to  produce  a  fast  detector  (running  at  2fps  for  the  7.6K  detector).  Models  and  software  are  available  at
0	Deepcut  object  segmentation  from  bounding  box  annotations  using  convolutional  neural  networks.  In  this  paper,  we  propose  DeepCut,  a  method  to  obtain  pixelwise  object  segmentations  given  an  image  dataset  labelled  with  bounding  box  annotations.  It  extends  the  approach  of  the  well-known  GrabCut  method  to  include  machine  learning  by  training  a  neural  network  classifier  from  bounding  box  annotations.  We  formulate  the  problem  as  an  energy  minimisation  problem  over  a  densely-connected  conditional  random  field  and  iteratively  update  the  training  targets  to  obtain  pixelwise  object  segmentations.  Additionally,  we  propose  variants  of  the  DeepCut  method  and  compare  those  to  a  naive  approach  to  CNN  training  under  weak  supervision.  We  test  its  applicability  to  solve  brain  and  lung  segmentation  problems  on  a  challenging  fetal  magnetic  resonance  dataset  and  obtain  encouraging  results  in  terms  of  accuracy.
0	Depth  from  a  single  image  by  harmonizing  overcomplete  local  network  predictions.  A  single  color  image  can  contain  many  cues  informative  towards  different  aspects  of  local  geometric  structure.  We  approach  the  problem  of  monocular  depth  estimation  by  using  a  neural  network  to  produce  a  mid-level  representation  that  summarizes  these  cues.  This  network  is  trained  to  characterize  local  scene  geometry  by  predicting,  at  every  image  location,  depth  derivatives  of  different  orders,  orientations  and  scales.  However,  instead  of  a  single  estimate  for  each  derivative,  the  network  outputs  probability  distributions  that  allow  it  to  express  confidence  about  some  coefficients,  and  ambiguity  about  others.  Scene  depth  is  then  estimated  by  harmonizing  this  overcomplete  set  of  network  predictions,  using  a  globalization  procedure  that  finds  a  single  consistent  depth  map  that  best  matches  all  the  local  derivative  distributions.  We  demonstrate  the  efficacy  of  this  approach  through  evaluation  on  the  NYU  v2  depth  data  set.
0	Highly  efficient  forward  and  backward  propagation  of  convolutional  neural  networks  for  pixelwise  classification.  We  present  highly  efficient  algorithms  for  performing  forward  and  backward  propagation  of  Convolutional  Neural  Network  (CNN)  for  pixelwise  classification  on  images.  For  pixelwise  classification  tasks,  such  as  image  segmentation  and  object  detection,  surrounding  image  patches  are  fed  into  CNN  for  predicting  the  classes  of  centered  pixels  via  forward  propagation  and  for  updating  CNN  parameters  via  backward  propagation.  However,  forward  and  backward  propagation  was  originally  designed  for  whole-image  classification.  Directly  applying  it  to  pixelwise  classification  in  a  patch-by-patch  scanning  manner  is  extremely  inefficient,  because  surrounding  patches  of  pixels  have  large  overlaps,  which  lead  to  a  lot  of  redundant  computation.    The  proposed  algorithms  eliminate  all  the  redundant  computation  in  convolution  and  pooling  on  images  by  introducing  novel  d-regularly  sparse  kernels.  It  generates  exactly  the  same  results  as  those  by  patch-by-patch  scanning.  Convolution  and  pooling  operations  with  such  kernels  are  able  to  continuously  access  memory  and  can  run  efficiently  on  GPUs.  A  fraction  of  patches  of  interest  can  be  chosen  from  each  training  image  for  backward  propagation  by  applying  a  mask  to  the  error  map  at  the  last  CNN  layer.  Its  computation  complexity  is  constant  with  respect  to  the  number  of  patches  sampled  from  the  image.  Experiments  have  shown  that  our  proposed  algorithms  speed  up  commonly  used  patch-by-patch  scanning  over  1500  times  in  both  forward  and  backward  propagation.  The  speedup  increases  with  the  sizes  of  images  and  patches.
0	Interleaved  text  image  deep  mining  on  a  large  scale  radiology  database  for  automated  image  interpretation.  Despite  tremendous  progress  in  computer  vision,  there  has  not  been  an  attempt  for  machine  learning  on  very  large-scale  medical  image  databases.  We  present  an  interleaved  text/image  deep  learning  system  to  extract  and  mine  the  semantic  interactions  of  radiology  images  and  reports  from  a  national  research  hospital's  Picture  Archiving  and  Communication  System.  With  natural  language  processing,  we  mine  a  collection  of  representative  ~216K  two-dimensional  key  images  selected  by  clinicians  for  diagnostic  reference,  and  match  the  images  with  their  descriptions  in  an  automated  manner.  Our  system  interleaves  between  unsupervised  learning  and  supervised  learning  on  document-  and  sentence-level  text  collections,  to  generate  semantic  labels  and  to  predict  them  given  an  image.  Given  an  image  of  a  patient  scan,  semantic  topics  in  radiology  levels  are  predicted,  and  associated  key-words  are  generated.  Also,  a  number  of  frequent  disease  types  are  detected  as  present  or  absent,  to  provide  more  specific  interpretation  of  a  patient  scan.  This  shows  the  potential  of  large-scale  learning  and  prediction  in  electronic  patient  records  available  in  most  modern  clinical  institutions.
0	Unsupervised  feature  learning  with  c  svddnet.  In  this  paper,  we  investigate  the  problem  of  learning  feature  representation  from  unlabeled  data  using  a  single-layer  K-means  network.  A  K-means  network  maps  the  input  data  into  a  feature  representation  by  finding  the  nearest  centroid  for  each  input  point,  which  has  attracted  researchers'  great  attention  recently  due  to  its  simplicity,  effectiveness,  and  scalability.  However,  one  drawback  of  this  feature  mapping  is  that  it  tends  to  be  unreliable  when  the  training  data  contains  noise.  To  address  this  issue,  we  propose  a  SVDD  based  feature  learning  algorithm  that  describes  the  density  and  distribution  of  each  cluster  from  K-means  with  an  SVDD  ball  for  more  robust  feature  representation.  For  this  purpose,  we  present  a  new  SVDD  algorithm  called  C-SVDD  that  centers  the  SVDD  ball  towards  the  mode  of  local  density  of  each  cluster,  and  we  show  that  the  objective  of  C-SVDD  can  be  solved  very  efficiently  as  a  linear  programming  problem.  Additionally,  traditional  unsupervised  feature  learning  methods  usually  take  an  average  or  sum  of  local  representations  to  obtain  global  representation  which  ignore  spatial  relationship  among  them.  To  use  spatial  information  we  propose  a  global  representation  with  a  variant  of  SIFT  descriptor.  The  architecture  is  also  extended  with  multiple  receptive  field  scales  and  multiple  pooling  sizes.  Extensive  experiments  on  several  popular  object  recognition  benchmarks,  such  as  STL-10,  MINST,  Holiday  and  Copydays  shows  that  the  proposed  C-SVDDNet  method  yields  comparable  or  better  performance  than  that  of  the  previous  state  of  the  art  methods.
0	Shufflenet  v2  practical  guidelines  for  efficient  cnn  architecture  design.  Currently,  the  neural  network  architecture  design  is  mostly  guided  by  the  \emph{indirect}  metric  of  computation  complexity,  i.e.,  FLOPs.  However,  the  \emph{direct}  metric,  e.g.,  speed,  also  depends  on  the  other  factors  such  as  memory  access  cost  and  platform  characterics.  Thus,  this  work  proposes  to  evaluate  the  direct  metric  on  the  target  platform,  beyond  only  considering  FLOPs.  Based  on  a  series  of  controlled  experiments,  this  work  derives  several  practical  \emph{guidelines}  for  efficient  network  design.  Accordingly,  a  new  architecture  is  presented,  called  \emph{ShuffleNet  V2}.  Comprehensive  ablation  experiments  verify  that  our  model  is  the  state-of-the-art  in  terms  of  speed  and  accuracy  tradeoff.
0	Explain  your  move  understanding  agent  actions  using  focused  feature  saliency.  As  deep  reinforcement  learning  (RL)  is  applied  to  more  tasks,  there  is  a  need  to  visualize  and  understand  the  behavior  of  learned  agents.  Saliency  maps  explain  agent  behavior  by  highlighting  the  features  of  the  input  state  that  are  most  relevant  for  the  agent  in  taking  an  action.  Existing  perturbation-based  approaches  to  compute  saliency  often  highlight  regions  of  the  input  that  are  not  relevant  to  the  action  taken  by  the  agent.  Our  approach  generates  more  focused  saliency  maps  by  balancing  two  aspects  (specificity  and  relevance)  that  capture  different  desiderata  of  saliency.  The  first  captures  the  impact  of  perturbation  on  the  relative  expected  reward  of  the  action  to  be  explained.  The  second  downweights  irrelevant  features  that  alter  the  relative  expected  rewards  of  actions  other  than  the  action  to  be  explained.  We  compare  our  approach  with  existing  approaches  on  agents  trained  to  play  board  games  (Chess  and  Go)  and  Atari  games  (Breakout,  Pong  and  Space  Invaders).  We  show  through  illustrative  examples  (Chess,  Atari,  Go),  human  studies  (Chess),  and  automated  evaluation  methods  (Chess)  that  our  approach  generates  saliency  maps  that  are  more  interpretable  for  humans  than  existing  approaches.
0	Subsampled  turbulence  removal  network.  We  present  a  deep-learning  approach  to  restore  a  sequence  of  turbulence-distorted  video  frames  from  turbulent  deformations  and  space-time  varying  blurs.  Instead  of  requiring  a  massive  training  sample  size  in  deep  networks,  we  purpose  a  training  strategy  that  is  based  on  a  new  data  augmentation  method  to  model  turbulence  from  a  relatively  small  dataset.  Then  we  introduce  a  subsampled  method  to  enhance  the  restoration  performance  of  the  presented  GAN  model.  The  contributions  of  the  paper  is  threefold:  first,  we  introduce  a  simple  but  effective  data  augmentation  algorithm  to  model  the  turbulence  in  real  life  for  training  in  the  deep  network;  Second,  we  firstly  purpose  the  Wasserstein  GAN  combined  with  $\ell_1$  cost  for  successful  restoration  of  turbulence-corrupted  video  sequence;  Third,  we  combine  the  subsampling  algorithm  to  filter  out  strongly  corrupted  frames  to  generate  a  video  sequence  with  better  quality.
0	Deep  adaptive  proposal  network  for  object  detection  in  optical  remote  sensing  images.  Object  detection  is  a  fundamental  and  challenging  problem  in  aerial  and  satellite  image  analysis.  More  recently,  a  two-stage  detector  Faster  R-CNN  is  proposed  and  demonstrated  to  be  a  promising  tool  for  object  detection  in  optical  remote  sensing  images,  while  the  sparse  and  dense  characteristic  of  objects  in  remote  sensing  images  is  complexity.  It  is  unreasonable  to  treat  all  images  with  the  same  region  proposal  strategy,  and  this  treatment  limits  the  performance  of  two-stage  detectors.  In  this  paper,  we  propose  a  novel  and  effective  approach,  named  deep  adaptive  proposal  network  (DAPNet),  address  this  complexity  characteristic  of  object  by  learning  a  new  category  prior  network  (CPN)  on  the  basis  of  the  existing  Faster  R-CNN  architecture.  Moreover,  the  candidate  regions  produced  by  DAPNet  model  are  different  from  the  traditional  region  proposal  network  (RPN),  DAPNet  predicts  the  detail  category  of  each  candidate  region.  And  these  candidate  regions  combine  the  object  number,  which  generated  by  the  category  prior  network  to  achieve  a  suitable  number  of  candidate  boxes  for  each  image.  These  candidate  boxes  can  satisfy  detection  tasks  in  sparse  and  dense  scenes.  The  performance  of  the  proposed  framework  has  been  evaluated  on  the  challenging  NWPU  VHR-10  data  set.  Experimental  results  demonstrate  the  superiority  of  the  proposed  framework  to  the  state-of-the-art.
0	Conditional  adversarial  generative  flow  for  controllable  image  synthesis.  Flow-based  generative  models  show  great  potential  in  image  synthesis  due  to  its  reversible  pipeline  and  exact  log-likelihood  target,  yet  it  suffers  from  weak  ability  for  conditional  image  synthesis,  especially  for  multi-label  or  unaware  conditions.  This  is  because  the  potential  distribution  of  image  conditions  is  hard  to  measure  precisely  from  its  latent  variable  $z$.  In  this  paper,  based  on  modeling  a  joint  probabilistic  density  of  an  image  and  its  conditions,  we  propose  a  novel  flow-based  generative  model  named  conditional  adversarial  generative  flow  (CAGlow).  Instead  of  disentangling  attributes  from  latent  space,  we  blaze  a  new  trail  for  learning  an  encoder  to  estimate  the  mapping  from  condition  space  to  latent  space  in  an  adversarial  manner.  Given  a  specific  condition  $c$,  CAGlow  can  encode  it  to  a  sampled  $z$,  and  then  enable  robust  conditional  image  synthesis  in  complex  situations  like  combining  person  identity  with  multiple  attributes.  The  proposed  CAGlow  can  be  implemented  in  both  supervised  and  unsupervised  manners,  thus  can  synthesize  images  with  conditional  information  like  categories,  attributes,  and  even  some  unknown  properties.  Extensive  experiments  show  that  CAGlow  ensures  the  independence  of  different  conditions  and  outperforms  regular  Glow  to  a  significant  extent.
0	Learning  to  dress  3d  people  in  generative  clothing.  Three-dimensional  human  body  models  are  widely  used  in  the  analysis  of  human  pose  and  motion.  Existing  models,  however,  are  learned  from  minimally-clothed  3D  scans  and  thus  do  not  generalize  to  the  complexity  of  dressed  people  in  common  images  and  videos.  Additionally,  current  models  lack  the  expressive  power  needed  to  represent  the  complex  non-linear  geometry  of  pose-dependent  clothing  shapes.  To  address  this,  we  learn  a  generative  3D  mesh  model  of  clothed  people  from  3D  scans  with  varying  pose  and  clothing.  Specifically,  we  train  a  conditional  Mesh-VAE-GAN  to  learn  the  clothing  deformation  from  the  SMPL  body  model,  making  clothing  an  additional  term  in  SMPL.  Our  model  is  conditioned  on  both  pose  and  clothing  type,  giving  the  ability  to  draw  samples  of  clothing  to  dress  different  body  shapes  in  a  variety  of  styles  and  poses.  To  preserve  wrinkle  detail,  our  Mesh-VAE-GAN  extends  patchwise  discriminators  to  3D  meshes.  Our  model,  named  CAPE,  represents  global  shape  and  fine  local  structure,  effectively  extending  the  SMPL  body  model  to  clothing.  To  our  knowledge,  this  is  the  first  generative  model  that  directly  dresses  3D  human  body  meshes  and  generalizes  to  different  poses.  The  model,  code  and  data  are  available  for  research  purposes  at  this  https  URL.
0	Random  vector  functional  link  neural  network  based  ensemble  deep  learning.  In  this  paper,  we  propose  a  deep  learning  framework  based  on  randomized  neural  network.  In  particular,  inspired  by  the  principles  of  Random  Vector  Functional  Link  (RVFL)  network,  we  present  a  deep  RVFL  network  (dRVFL)  with  stacked  layers.  The  parameters  of  the  hidden  layers  of  the  dRVFL  are  randomly  generated  within  a  suitable  range  and  kept  fixed  while  the  output  weights  are  computed  using  the  closed  form  solution  as  in  a  standard  RVFL  network.  We  also  propose  an  ensemble  deep  network  (edRVFL)  that  can  be  regarded  as  a  marriage  of  ensemble  learning  with  deep  learning.  Unlike  traditional  ensembling  approaches  that  require  training  several  models  independently  from  scratch,  edRVFL  is  obtained  by  training  a  single  dRVFL  network  once.  Both  dRVFL  and  edRVFL  frameworks  are  generic  and  can  be  used  with  any  RVFL  variant.  To  illustrate  this,  we  integrate  the  deep  learning  networks  with  a  recently  proposed  sparse-pretrained  RVFL  (SP-RVFL).  Extensive  experiments  on  benchmark  datasets  from  diverse  domains  show  the  superior  performance  of  our  proposed  deep  RVFL  networks.
0	A  geometry  sensitive  approach  for  photographic  style  classification.  Photographs  are  characterized  by  different  compositional  attributes  like  the  Rule  of  Thirds,  depth  of  field,  vanishing-lines  etc.  The  presence  or  absence  of  one  or  more  of  these  attributes  contributes  to  the  overall  artistic  value  of  an  image.  In  this  work,  we  analyze  the  ability  of  deep  learning  based  methods  to  learn  such  photographic  style  attributes.  We  observe  that  although  a  standard  CNN  learns  the  texture  and  appearance  based  features  reasonably  well,  its  understanding  of  global  and  geometric  features  is  limited  by  two  factors.  First,  the  data-augmentation  strategies  (cropping,  warping,  etc.)  distort  the  composition  of  a  photograph  and  affect  the  performance.  Secondly,  the  CNN  features,  in  principle,  are  translation-invariant  and  appearance-dependent.  But  some  geometric  properties  important  for  aesthetics,  e.g.  the  Rule  of  Thirds  (RoT),  are  position-dependent  and  appearance-invariant.  Therefore,  we  propose  a  novel  input  representation  which  is  geometry-sensitive,  position-cognizant  and  appearance-invariant.  We  further  introduce  a  two-column  CNN  architecture  that  performs  better  than  the  state-of-the-art  (SoA)  in  photographic  style  classification.  From  our  results,  we  observe  that  the  proposed  network  learns  both  the  geometric  and  appearance-based  attributes  better  than  the  SoA.
0	Real  time  2d  multi  person  pose  estimation  on  cpu  lightweight  openpose.  In  this  work  we  adapt  multi-person  pose  estimation  architecture  to  use  it  on  edge  devices.  We  follow  the  bottom-up  approach  from  OpenPose,  the  winner  of  COCO  2016  Keypoints  Challenge,  because  of  its  decent  quality  and  robustness  to  number  of  people  inside  the  frame.  With  proposed  network  design  and  optimized  post-processing  code  the  full  solution  runs  at  28  frames  per  second  (fps)  on  Intel$\unicode{xAE}$  NUC  6i7KYB  mini  PC  and  26  fps  on  Core$^{TM}$  i7-6850K  CPU.  The  network  model  has  4.1M  parameters  and  9  billions  floating-point  operations  (GFLOPs)  complexity,  which  is  just  ~15%  of  the  baseline  2-stage  OpenPose  with  almost  the  same  quality.  The  code  and  model  are  available  as  a  part  of  Intel$\unicode{xAE}$  OpenVINO$^{TM}$  Toolkit.
0	Texttubes  for  detecting  curved  text  in  the  wild.  We  present  a  detector  for  curved  text  in  natural  images.  We  model  scene  text  instances  as  tubes  around  their  medial  axes  and  introduce  a  parametrization-invariant  loss  function.  We  train  a  two-stage  curved  text  detector,  and  evaluate  it  on  the  curved  text  benchmarks  CTW-1500  and  Total-Text.  Our  approach  achieves  state-of-the-art  results  or  improves  upon  them,  notably  for  CTW-1500  by  over  8  percentage  points  in  F-score.
0	Unsupervised  video  anomaly  detection  via  normalizing  flows  with  implicit  latent  features.  Surveillance  anomaly  detection  searches  for  anomalous  events,  such  as  crimes  or  accidents,  among  normal  scenes.  Because  it  occurs  rarely,  most  training  data  consists  of  unlabeled,  normal  videos,  which  makes  the  task  challenging.  Most  existing  methods  use  an  autoencoder  (AE)  to  learn  reconstructing  normal  videos  and  detect  anomalies  by  a  failure  to  reconstruct  the  appearance  of  abnormal  scenes.  However,  because  anomalies  are  distinguished  by  appearance  or  motion,  many  previous  approaches  have  explicitly  separated  appearance  and  motion  information--for  example,  using  a  pre-trained  optical  flow  model.  This  explicit  separation  restricts  reciprocal  representation  capabilities  between  two  information.  In  contrast,  we  propose  an  implicit  two-path  AE  (ITAE),  a  structure  in  which  two  encoders  implicitly  model  appearance  and  motion  features,  and  a  single  decoder  that  combines  them  to  learn  normal  video  patterns.  For  the  complex  distribution  of  normal  scenes,  we  suggest  normal  density  estimation  of  ITAE  features  through  normalizing  flow  (NF)-based  generative  models  to  learn  the  tractable  likelihoods  and  find  anomalies  using  out-of-distribution  detection.  NF  models  intensify  ITAE  performance  by  learning  normality  through  implicitly  learned  features.  Finally,  we  demonstrate  the  effectiveness  of  ITAE  and  its  feature  distribution  modeling  in  three  benchmarks,  especially  on  the  Shanghai  Tech  Campus  (ST)  database  composed  of  various  anomalies  in  real-world  scenarios.
0	Egoshots  an  ego  vision  life  logging  dataset  and  semantic  fidelity  metric  to  evaluate  diversity  in  image  captioning  models.  Image  captioning  models  have  been  able  to  generate  grammatically  correct  and  human  understandable  sentences.  However  most  of  the  captions  convey  limited  information  as  the  model  used  is  trained  on  datasets  that  do  not  caption  all  possible  objects  existing  in  everyday  life.  Due  to  this  lack  of  prior  information  most  of  the  captions  are  biased  to  only  a  few  objects  present  in  the  scene,  hence  limiting  their  usage  in  daily  life.  In  this  paper,  we  attempt  to  show  the  biased  nature  of  the  currently  existing  image  captioning  models  and  present  a  new  image  captioning  dataset,  Egoshots,  consisting  of  978  real  life  images  with  no  captions.  We  further  exploit  the  state  of  the  art  pre-trained  image  captioning  and  object  recognition  networks  to  annotate  our  images  and  show  the  limitations  of  existing  works.  Furthermore,  in  order  to  evaluate  the  quality  of  the  generated  captions,  we  propose  a  new  image  captioning  metric,  object  based  Semantic  Fidelity  (SF).  Existing  image  captioning  metrics  can  evaluate  a  caption  only  in  the  presence  of  their  corresponding  annotations;  however,  SF  allows  evaluating  captions  generated  for  images  without  annotations,  making  it  highly  useful  for  real  life  generated  captions.
0	Hierarchical  rule  induction  network  for  abstract  visual  reasoning.  Abstract  reasoning  refers  to  the  ability  to  analyze  information,  discover  rules  at  an  intangible  level,  and  solve  problems  in  innovative  ways.  Raven's  Progressive  Matrices  (RPM)  test  is  typically  used  to  examine  the  capability  of  abstract  reasoning.  In  the  test,  the  subject  is  asked  to  identify  the  correct  choice  from  the  answer  set  to  fill  the  missing  panel  at  the  bottom  right  of  RPM  (e.g.,  a  3$\times$3  matrix),  following  the  underlying  rules  inside  the  matrix.  Recent  studies,  taking  advantage  of  Convolutional  Neural  Networks  (CNNs),  have  achieved  encouraging  progress  to  accomplish  the  RPM  test  problems.  Unfortunately,  simply  relying  on  the  relation  extraction  at  the  matrix  level,  they  fail  to  recognize  the  complex  attribute  patterns  inside  or  across  rows/columns  of  RPM.  To  address  this  problem,  in  this  paper  we  propose  a  Hierarchical  Rule  Induction  Network  (HriNet),  by  intimating  human  induction  strategies.  HriNet  extracts  multiple  granularity  rule  embeddings  at  different  levels  and  integrates  them  through  a  gated  embedding  fusion  module.  We  further  introduce  a  rule  similarity  metric  based  on  the  embeddings,  so  that  HriNet  can  not  only  be  trained  using  a  tuplet  loss  but  also  infer  the  best  answer  according  to  the  similarity  score.  To  comprehensively  evaluate  HriNet,  we  first  fix  the  defects  contained  in  the  very  recent  RAVEN  dataset  and  generate  a  new  one  named  Balanced-RAVEN.  Then  extensive  experiments  are  conducted  on  the  large-scale  dataset  PGM  and  our  Balanced-RAVEN,  the  results  of  which  show  that  HriNet  outperforms  the  state-of-the-art  models  by  a  large  margin.
0	Single  view  view  synthesis  with  multiplane  images.  A  recent  strand  of  work  in  view  synthesis  uses  deep  learning  to  generate  multiplane  images  (a  camera-centric,  layered  3D  representation)  given  two  or  more  input  images  at  known  viewpoints.  We  apply  this  representation  to  single-view  view  synthesis,  a  problem  which  is  more  challenging  but  has  potentially  much  wider  application.  Our  method  learns  to  predict  a  multiplane  image  directly  from  a  single  image  input,  and  we  introduce  scale-invariant  view  synthesis  for  supervision,  enabling  us  to  train  on  online  video.  We  show  this  approach  is  applicable  to  several  different  datasets,  that  it  additionally  generates  reasonable  depth  maps,  and  that  it  learns  to  fill  in  content  behind  the  edges  of  foreground  objects  in  background  layers.    Project  page  at  this  https  URL.
0	Dodnet  learning  to  segment  multi  organ  and  tumors  from  multiple  partially  labeled  datasets.  Due  to  the  intensive  cost  of  labor  and  expertise  in  annotating  3D  medical  images  at  a  voxel  level,  most  benchmark  datasets  are  equipped  with  the  annotations  of  only  one  type  of  organs  and/or  tumors,  resulting  in  the  so-called  partially  labeling  issue.  To  address  this,  we  propose  a  dynamic  on-demand  network  (DoDNet)  that  learns  to  segment  multiple  organs  and  tumors  on  partially  labeled  datasets.  DoDNet  consists  of  a  shared  encoder-decoder  architecture,  a  task  encoding  module,  a  controller  for  generating  dynamic  convolution  filters,  and  a  single  but  dynamic  segmentation  head.  The  information  of  the  current  segmentation  task  is  encoded  as  a  task-aware  prior  to  tell  the  model  what  the  task  is  expected  to  solve.  Different  from  existing  approaches  which  fix  kernels  after  training,  the  kernels  in  dynamic  head  are  generated  adaptively  by  the  controller,  conditioned  on  both  input  image  and  assigned  task.  Thus,  DoDNet  is  able  to  segment  multiple  organs  and  tumors,  as  done  by  multiple  networks  or  a  multi-head  network,  in  a  much  efficient  and  flexible  manner.  We  have  created  a  large-scale  partially  labeled  dataset,  termed  MOTS,  and  demonstrated  the  superior  performance  of  our  DoDNet  over  other  competitors  on  seven  organ  and  tumor  segmentation  tasks.  We  also  transferred  the  weights  pre-trained  on  MOTS  to  a  downstream  multi-organ  segmentation  task  and  achieved  state-of-the-art  performance.  This  study  provides  a  general  3D  medical  image  segmentation  model  that  has  been  pre-trained  on  a  large-scale  partially  labelled  dataset  and  can  be  extended  (after  fine-tuning)  to  downstream  volumetric  medical  data  segmentation  tasks.  The  dataset  and  code  areavailableat:  this  https  URL
0	Rethinking  semantic  segmentation  from  a  sequence  to  sequence  perspective  with  transformers.  Most  recent  semantic  segmentation  methods  adopt  a  fully-convolutional  network  (FCN)  with  an  encoder-decoder  architecture.  The  encoder  progressively  reduces  the  spatial  resolution  and  learns  more  abstract/semantic  visual  concepts  with  larger  receptive  fields.  Since  context  modeling  is  critical  for  segmentation,  the  latest  efforts  have  been  focused  on  increasing  the  receptive  field,  through  either  dilated/atrous  convolutions  or  inserting  attention  modules.  However,  the  encoder-decoder  based  FCN  architecture  remains  unchanged.  In  this  paper,  we  aim  to  provide  an  alternative  perspective  by  treating  semantic  segmentation  as  a  sequence-to-sequence  prediction  task.  Specifically,  we  deploy  a  pure  transformer  (ie,  without  convolution  and  resolution  reduction)  to  encode  an  image  as  a  sequence  of  patches.  With  the  global  context  modeled  in  every  layer  of  the  transformer,  this  encoder  can  be  combined  with  a  simple  decoder  to  provide  a  powerful  segmentation  model,  termed  SEgmentation  TRansformer  (SETR).  Extensive  experiments  show  that  SETR  achieves  new  state  of  the  art  on  ADE20K  (50.28%  mIoU),  Pascal  Context  (55.83%  mIoU)  and  competitive  results  on  Cityscapes.  Particularly,  we  achieve  the  first  position  in  the  highly  competitive  ADE20K  test  server  leaderboard  on  the  day  of  submission.
0	Is  it  enough  to  optimize  cnn  architectures  on  imagenet.  An  implicit  but  pervasive  hypothesis  of  modern  computer  vision  research  is  that  convolutional  neural  network  (CNN)  architectures  that  perform  better  on  ImageNet  will  also  perform  better  on  other  vision  datasets.  We  challenge  this  hypothesis  through  an  extensive  empirical  study  for  which  we  train  500  sampled  CNN  architectures  on  ImageNet  as  well  as  8  other  image  classification  datasets  from  a  wide  array  of  application  domains.  The  relationship  between  architecture  and  performance  varies  wildly,  depending  on  the  datasets.  For  some  of  them,  the  performance  correlation  with  ImageNet  is  even  negative.  Clearly,  it  is  not  enough  to  optimize  architectures  solely  for  ImageNet  when  aiming  for  progress  that  is  relevant  for  all  applications.  Therefore,  we  identify  two  dataset-specific  performance  indicators:  the  cumulative  width  across  layers  as  well  as  the  total  depth  of  the  network.  Lastly,  we  show  that  the  range  of  dataset  variability  covered  by  ImageNet  can  be  significantly  extended  by  adding  ImageNet  subsets  restricted  to  few  classes.
0	Structural  compression  of  convolutional  neural  networks  based  on  greedy  filter  pruning.  Convolutional  neural  networks  (CNNs)  have  state-of-the-art  performance  on  many  problems  in  machine  vision.  However,  networks  with  superior  performance  often  have  millions  of  weights  so  that  it  is  difficult  or  impossible  to  use  CNNs  on  computationally  limited  devices  or  to  humanly  interpret  them.  A  myriad  of  CNN  compression  approaches  have  been  proposed  and  they  involve  pruning  and  compressing  the  weights  and  filters.  In  this  article,  we  introduce  a  greedy  structural  compression  scheme  that  prunes  filters  in  a  trained  CNN.  We  define  a  filter  importance  index  equal  to  the  classification  accuracy  reduction  (CAR)  of  the  network  after  pruning  that  filter  (similarly  defined  as  RAR  for  regression).  We  then  iteratively  prune  filters  based  on  the  CAR  index.  This  algorithm  achieves  substantially  higher  classification  accuracy  in  AlexNet  compared  to  other  structural  compression  schemes  that  prune  filters.  Pruning  half  of  the  filters  in  the  first  or  second  layer  of  AlexNet,  our  CAR  algorithm  achieves  26%  and  20%  higher  classification  accuracies  respectively,  compared  to  the  best  benchmark  filter  pruning  scheme.  Our  CAR  algorithm,  combined  with  further  weight  pruning  and  compressing,  reduces  the  size  of  first  or  second  convolutional  layer  in  AlexNet  by  a  factor  of  42,  while  achieving  close  to  original  classification  accuracy  through  retraining  (or  fine-tuning)  network.  Finally,  we  demonstrate  the  interpretability  of  CAR-compressed  CNNs  by  showing  that  our  algorithm  prunes  filters  with  visually  redundant  functionalities.  In  fact,  out  of  top  20  CAR-pruned  filters  in  AlexNet,  17  of  them  in  the  first  layer  and  14  of  them  in  the  second  layer  are  color-selective  filters  as  opposed  to  shape-selective  filters.  To  our  knowledge,  this  is  the  first  reported  result  on  the  connection  between  compression  and  interpretability  of  CNNs.
0	Efficient  k  shot  learning  with  regularized  deep  networks.  Feature  representations  from  pre-trained  deep  neural  networks  have  been  known  to  exhibit  excellent  generalization  and  utility  across  a  variety  of  related  tasks.  Fine-tuning  is  by  far  the  simplest  and  most  widely  used  approach  that  seeks  to  exploit  and  adapt  these  feature  representations  to  novel  tasks  with  limited  data.  Despite  the  effectiveness  of  fine-tuning,  itis  often  sub-optimal  and  requires  very  careful  optimization  to  prevent  severe  over-fitting  to  small  datasets.  The  problem  of  sub-optimality  and  over-fitting,  is  due  in  part  to  the  large  number  of  parameters  used  in  a  typical  deep  convolutional  neural  network.  To  address  these  problems,  we  propose  a  simple  yet  effective  regularization  method  for  fine-tuning  pre-trained  deep  networks  for  the  task  of  k-shot  learning.  To  prevent  overfitting,  our  key  strategy  is  to  cluster  the  model  parameters  while  ensuring  intra-cluster  similarity  and  inter-cluster  diversity  of  the  parameters,  effectively  regularizing  the  dimensionality  of  the  parameter  search  space.  In  particular,  we  identify  groups  of  neurons  within  each  layer  of  a  deep  network  that  shares  similar  activation  patterns.  When  the  network  is  to  be  fine-tuned  for  a  classification  task  using  only  k  examples,  we  propagate  a  single  gradient  to  all  of  the  neuron  parameters  that  belong  to  the  same  group.  The  grouping  of  neurons  is  non-trivial  as  neuron  activations  depend  on  the  distribution  of  the  input  data.  To  efficiently  search  for  optimal  groupings  conditioned  on  the  input  data,  we  propose  a  reinforcement  learning  search  strategy  using  recurrent  networks  to  learn  the  optimal  group  assignments  for  each  network  layer.  Experimental  results  show  that  our  method  can  be  easily  applied  to  several  popular  convolutional  neural  networks  and  improve  upon  other  state-of-the-art  fine-tuning  based  k-shot  learning  strategies  by  more  than10%
0	An  evaluation  framework  and  database  for  mocap  based  gait  recognition  methods.  As  a  contribution  to  reproducible  research,  this  paper  presents  a  framework  and  a  database  to  improve  the  development,  evaluation  and  comparison  of  methods  for  gait  recognition  from  motion  capture  (MoCap)  data.  The  evaluation  framework  provides  implementation  details  and  source  codes  of  state-of-the-art  human-interpretable  geometric  features  as  well  as  our  own  approaches  where  gait  features  are  learned  by  a  modification  of  Fisher's  Linear  Discriminant  Analysis  with  the  Maximum  Margin  Criterion,  and  by  a  combination  of  Principal  Component  Analysis  and  Linear  Discriminant  Analysis.  It  includes  a  description  and  source  codes  of  a  mechanism  for  evaluating  four  class  separability  coefficients  of  feature  space  and  four  rank-based  classifier  performance  metrics.  This  framework  also  contains  a  tool  for  learning  a  custom  classifier  and  for  classifying  a  custom  query  on  a  custom  gallery.  We  provide  an  experimental  database  along  with  source  codes  for  its  extraction  from  the  general  CMU  MoCap  database.
0	Accessible  melanoma  detection  using  smartphones  and  mobile  image  analysis.  We  investigate  the  design  of  an  entire  mobile  imaging  system  for  early  detection  of  melanoma.  Different  from  previous  work,  we  focus  on  smartphone-captured  visible  light  images.  Our  design  addresses  two  major  challenges.  First,  images  acquired  using  a  smartphone  under  loosely-controlled  environmental  conditions  may  be  subject  to  various  distortions,  and  this  makes  melanoma  detection  more  difficult.  Second,  processing  performed  on  a  smartphone  is  subject  to  stringent  computation  and  memory  constraints.  In  our  work,  we  propose  a  detection  system  that  is  optimized  to  run  entirely  on  the  resource-constrained  smartphone.  Our  system  intends  to  localize  the  skin  lesion  by  combining  a  lightweight  method  for  skin  detection  with  a  hierarchical  segmentation  approach  using  two  fast  segmentation  methods.  Moreover,  we  study  an  extensive  set  of  image  features  and  propose  new  numerical  features  to  characterize  a  skin  lesion.  Furthermore,  we  propose  an  improved  feature  selection  algorithm  to  determine  a  small  set  of  discriminative  features  used  by  the  final  lightweight  system.  In  addition,  we  study  the  human-computer  interface  (HCI)  design  to  understand  the  usability  and  acceptance  issues  of  the  proposed  system.
0	Synthetic  data  for  text  localisation  in  natural  images.  In  this  paper  we  introduce  a  new  method  for  text  detection  in  natural  images.  The  method  comprises  two  contributions:  First,  a  fast  and  scalable  engine  to  generate  synthetic  images  of  text  in  clutter.  This  engine  overlays  synthetic  text  to  existing  background  images  in  a  natural  way,  accounting  for  the  local  3D  scene  geometry.  Second,  we  use  the  synthetic  images  to  train  a  Fully-Convolutional  Regression  Network  (FCRN)  which  efficiently  performs  text  detection  and  bounding-box  regression  at  all  locations  and  multiple  scales  in  an  image.  We  discuss  the  relation  of  FCRN  to  the  recently-introduced  YOLO  detector,  as  well  as  other  end-to-end  object  detection  systems  based  on  deep  learning.  The  resulting  detection  network  significantly  out  performs  current  methods  for  text  detection  in  natural  images,  achieving  an  F-measure  of  84.2%  on  the  standard  ICDAR  2013  benchmark.  Furthermore,  it  can  process  15  images  per  second  on  a  GPU.
0	Driver  gaze  region  estimation  without  using  eye  movement.  Automated  estimation  of  the  allocation  of  a  driver's  visual  attention  may  be  a  critical  component  of  future  Advanced  Driver  Assistance  Systems.  In  theory,  vision-based  tracking  of  the  eye  can  provide  a  good  estimate  of  gaze  location.  In  practice,  eye  tracking  from  video  is  challenging  because  of  sunglasses,  eyeglass  reflections,  lighting  conditions,  occlusions,  motion  blur,  and  other  factors.  Estimation  of  head  pose,  on  the  other  hand,  is  robust  to  many  of  these  effects,  but  cannot  provide  as  fine-grained  of  a  resolution  in  localizing  the  gaze.  However,  for  the  purpose  of  keeping  the  driver  safe,  it  is  sufficient  to  partition  gaze  into  regions.  In  this  effort,  we  propose  a  system  that  extracts  facial  features  and  classifies  their  spatial  configuration  into  six  regions  in  real-time.  Our  proposed  method  achieves  an  average  accuracy  of  91.4%  at  an  average  decision  rate  of  11  Hz  on  a  dataset  of  50  drivers  from  an  on-road  study.
0	Polysemous  codes.  This  paper  considers  the  problem  of  approximate  nearest  neighbor  search  in  the  compressed  domain.  We  introduce  polysemous  codes,  which  offer  both  the  distance  estimation  quality  of  product  quantization  and  the  efficient  comparison  of  binary  codes  with  Hamming  distance.  Their  design  is  inspired  by  algorithms  introduced  in  the  90's  to  construct  channel-optimized  vector  quantizers.  At  search  time,  this  dual  interpretation  accelerates  the  search.  Most  of  the  indexed  vectors  are  filtered  out  with  Hamming  distance,  letting  only  a  fraction  of  the  vectors  to  be  ranked  with  an  asymmetric  distance  estimator.    The  method  is  complementary  with  a  coarse  partitioning  of  the  feature  space  such  as  the  inverted  multi-index.  This  is  shown  by  our  experiments  performed  on  several  public  benchmarks  such  as  the  BIGANN  dataset  comprising  one  billion  vectors,  for  which  we  report  state-of-the-art  results  for  query  times  below  0.3\,millisecond  per  core.  Last  but  not  least,  our  approach  allows  the  approximate  computation  of  the  k-NN  graph  associated  with  the  Yahoo  Flickr  Creative  Commons  100M,  described  by  CNN  image  descriptors,  in  less  than  8  hours  on  a  single  machine.
0	Two  stage  convolutional  neural  network  architecture  for  lung  nodule  detection.  Early  detection  of  lung  cancer  is  an  effective  way  to  improve  the  survival  rate  of  patients.  It  is  a  critical  step  to  have  accurate  detection  of  lung  nodules  in  computed  tomography  (CT)  images  for  the  diagnosis  of  lung  cancer.  However,  due  to  the  heterogeneity  of  the  lung  nodules  and  the  complexity  of  the  surrounding  environment,  robust  nodule  detection  has  been  a  challenging  task.  In  this  study,  we  propose  a  two-stage  convolutional  neural  network  (TSCNN)  architecture  for  lung  nodule  detection.  The  CNN  architecture  in  the  first  stage  is  based  on  the  improved  UNet  segmentation  network  to  establish  an  initial  detection  of  lung  nodules.  Simultaneously,  in  order  to  obtain  a  high  recall  rate  without  introducing  excessive  false  positive  nodules,  we  propose  a  novel  sampling  strategy,  and  use  the  offline  hard  mining  idea  for  training  and  prediction  according  to  the  proposed  cascaded  prediction  method.  The  CNN  architecture  in  the  second  stage  is  based  on  the  proposed  dual  pooling  structure,  which  is  built  into  three  3D  CNN  classification  networks  for  false  positive  reduction.  Since  the  network  training  requires  a  significant  amount  of  training  data,  we  adopt  a  data  augmentation  method  based  on  random  mask.  Furthermore,  we  have  improved  the  generalization  ability  of  the  false  positive  reduction  model  by  means  of  ensemble  learning.  The  proposed  method  has  been  experimentally  verified  on  the  LUNA  dataset.  Experimental  results  show  that  the  proposed  TSCNN  architecture  can  obtain  competitive  detection  performance.
0	What  have  we  learned  from  deep  representations  for  action  recognition.  As  the  success  of  deep  models  has  led  to  their  deployment  in  all  areas  of  computer  vision,  it  is  increasingly  important  to  understand  how  these  representations  work  and  what  they  are  capturing.  In  this  paper,  we  shed  light  on  deep  spatiotemporal  representations  by  visualizing  what  two-stream  models  have  learned  in  order  to  recognize  actions  in  video.  We  show  that  local  detectors  for  appearance  and  motion  objects  arise  to  form  distributed  representations  for  recognizing  human  actions.  Key  observations  include  the  following.  First,  cross-stream  fusion  enables  the  learning  of  true  spatiotemporal  features  rather  than  simply  separate  appearance  and  motion  features.  Second,  the  networks  can  learn  local  representations  that  are  highly  class  specific,  but  also  generic  representations  that  can  serve  a  range  of  classes.  Third,  throughout  the  hierarchy  of  the  network,  features  become  more  abstract  and  show  increasing  invariance  to  aspects  of  the  data  that  are  unimportant  to  desired  distinctions  (e.g.  motion  patterns  across  various  speeds).  Fourth,  visualizations  can  be  used  not  only  to  shed  light  on  learned  representations,  but  also  to  reveal  idiosyncracies  of  training  data  and  to  explain  failure  cases  of  the  system.
0	Coogan  a  memory  efficient  framework  for  high  resolution  facial  attribute  editing.  In  contrast  to  great  success  of  memory-consuming  face  editing  methods  at  a  low  resolution,  to  manipulate  high-resolution  (HR)  facial  images,  i.e.,  typically  larger  than  7682  pixels,  with  very  limited  memory  is  still  challenging.  This  is  due  to  the  reasons  of  1)  intractable  huge  demand  of  memory;  2)  inefficient  multi-scale  features  fusion.  To  address  these  issues,  we  propose  a  NOVEL  pixel  translation  framework  called  Cooperative  GAN(CooGAN)  for  HR  facial  image  editing.  This  framework  features  a  local  path  for  fine-grained  local  facial  patch  generation  (i.e.,  patch-level  HR,  LOW  memory)  and  a  global  path  for  global  lowresolution  (LR)  facial  structure  monitoring  (i.e.,  image-level  LR,  LOW  memory),  which  largely  reduce  memory  requirements.  Both  paths  work  in  a  cooperative  manner  under  a  local-to-global  consistency  objective  (i.e.,  for  smooth  stitching).  In  addition,  we  propose  a  lighter  selective  transfer  unit  for  more  efficient  multi-scale  features  fusion,  yielding  higher  fidelity  facial  attributes  manipulation.  Extensive  experiments  on  CelebAHQ  well  demonstrate  the  memory  efficiency  as  well  as  the  high  image  generation  quality  of  the  proposed  framework.
0	Bifurcated  backbone  strategy  for  rgb  d  salient  object  detection.  Multi-level  feature  fusion  is  a  fundamental  topic  in  computer  vision.  It  has  been  exploited  to  detect,  segment  and  classify  objects  at  various  scales.  When  multi-level  features  meet  multi-modal  cues,  the  optimal  feature  aggregation  and  multi-modal  learning  strategy  become  a  hot  potato.  In  this  paper,  we  leverage  the  inherent  multi-modal  and  multi-level  nature  of  RGB-D  salient  object  detection  to  devise  a  novel  cascaded  refinement  network.  In  particular,  first,  we  propose  to  regroup  the  multi-level  features  into  teacher  and  student  features  using  a  bifurcated  backbone  strategy  (BBS).  Second,  we  introduce  a  depth-enhanced  module  (DEM)  to  excavate  informative  depth  cues  from  the  channel  and  spatial  views.  Then,  RGB  and  depth  modalities  are  fused  in  a  complementary  way.  Our  architecture,  named  Bifurcated  Backbone  Strategy  Network  (BBS-Net),  is  simple,  efficient,  and  backbone-independent.  Extensive  experiments  show  that  BBS-Net  significantly  outperforms  eighteen  SOTA  models  on  eight  challenging  datasets  under  five  evaluation  measures,  demonstrating  the  superiority  of  our  approach  ($\sim  4  \%$  improvement  in  S-measure  $vs.$  the  top-ranked  model:  DMRA-iccv2019).  In  addition,  we  provide  a  comprehensive  analysis  on  the  generalization  ability  of  different  RGB-D  datasets  and  provide  a  powerful  training  set  for  future  research.
0	Exploiting  context  for  robustness  to  label  noise  in  active  learning.  Several  works  in  computer  vision  have  demonstrated  the  effectiveness  of  active  learning  for  adapting  the  recognition  model  when  new  unlabeled  data  becomes  available.  Most  of  these  works  consider  that  labels  obtained  from  the  annotator  are  correct.  However,  in  a  practical  scenario,  as  the  quality  of  the  labels  depends  on  the  annotator,  some  of  the  labels  might  be  wrong,  which  results  in  degraded  recognition  performance.  In  this  paper,  we  address  the  problems  of  i)  how  a  system  can  identify  which  of  the  queried  labels  are  wrong  and  ii)  how  a  multi-class  active  learning  system  can  be  adapted  to  minimize  the  negative  impact  of  label  noise.  Towards  solving  the  problems,  we  propose  a  noisy  label  filtering  based  learning  approach  where  the  inter-relationship  (context)  that  is  quite  common  in  natural  data  is  utilized  to  detect  the  wrong  labels.  We  construct  a  graphical  representation  of  the  unlabeled  data  to  encode  these  relationships  and  obtain  new  beliefs  on  the  graph  when  noisy  labels  are  available.  Comparing  the  new  beliefs  with  the  prior  relational  information,  we  generate  a  dissimilarity  score  to  detect  the  incorrect  labels  and  update  the  recognition  model  with  correct  labels  which  result  in  better  recognition  performance.  This  is  demonstrated  in  three  different  applications:  scene  classification,  activity  classification,  and  document  classification.
0	Claster  clustering  with  reinforcement  learning  for  zero  shot  action  recognition.  Zero-shot  action  recognition  is  the  task  of  recognizingaction  classes  without  visual  examples,  only  with  a  seman-tic  embedding  which  relates  unseen  to  seen  classes.  Theproblem  can  be  seen  as  learning  a  function  which  general-izes  well  to  instances  of  unseen  classes  without  losing  dis-crimination  between  classes.  Neural  networks  can  modelthe  complex  boundaries  between  visual  classes,  which  ex-plains  their  success  as  supervised  models.  However,  inzero-shot  learning,  these  highly  specialized  class  bound-aries  may  not  transfer  well  from  seen  to  unseen  this  http  URL  this  paper  we  propose  a  centroid-based  representation,which  clusters  visual  and  semantic  representation,  consid-ers  all  training  samples  at  once,  and  in  this  way  generaliz-ing  well  to  instances  from  unseen  classes.  We  optimize  theclustering  using  Reinforcement  Learning  which  we  show  iscritical  for  our  approach  to  work.  We  call  the  proposedmethod  CLASTER  and  observe  that  it  consistently  outper-forms  the  state-of-the-art  in  all  standard  datasets,  includ-ing  UCF101,  HMDB51  and  Olympic  Sports;  both  in  thestandard  zero-shot  evaluation  and  the  generalized  zero-shotlearning.  Further,  we  show  that  our  model  performs  com-petitively  in  the  image  domain  as  well,  outperforming  thestate-of-the-art  in  many  settings.
0	Gmlight  lighting  estimation  via  geometric  distribution  approximation.  Lighting  estimation  from  a  single  image  is  an  essential  yet  challenging  task  in  computer  vision  and  computer  graphics.  Existing  works  estimate  lighting  by  regressing  representative  illumination  parameters  or  generating  illumination  maps  directly.  However,  these  methods  often  suffer  from  poor  accuracy  and  generalization.  This  paper  presents  Geometric  Mover's  Light  (GMLight),  a  lighting  estimation  framework  that  employs  a  regression  network  and  a  generative  projector  for  effective  illumination  estimation.  We  parameterize  illumination  scenes  in  terms  of  the  geometric  light  distribution,  light  intensity,  ambient  term,  and  auxiliary  depth,  and  estimate  them  as  a  pure  regression  task.  Inspired  by  the  earth  mover's  distance,  we  design  a  novel  geometric  mover's  loss  to  guide  the  accurate  regression  of  light  distribution  parameters.  With  the  estimated  lighting  parameters,  the  generative  projector  synthesizes  panoramic  illumination  maps  with  realistic  appearance  and  frequency.  Extensive  experiments  show  that  GMLight  achieves  accurate  illumination  estimation  and  superior  fidelity  in  relighting  for  3D  object  insertion.
0	Differentiable  patch  selection  for  image  recognition.  Neural  Networks  require  large  amounts  of  memory  and  compute  to  process  high  resolution  images,  even  when  only  a  small  part  of  the  image  is  actually  informative  for  the  task  at  hand.  We  propose  a  method  based  on  a  differentiable  Top-K  operator  to  select  the  most  relevant  parts  of  the  input  to  efficiently  process  high  resolution  images.  Our  method  may  be  interfaced  with  any  downstream  neural  network,  is  able  to  aggregate  information  from  different  patches  in  a  flexible  way,  and  allows  the  whole  model  to  be  trained  end-to-end  using  backpropagation.  We  show  results  for  traffic  sign  recognition,  inter-patch  relationship  reasoning,  and  fine-grained  recognition  without  using  object/part  bounding  box  annotations  during  training.
0	Context  matters  self  attention  for  sign  language  recognition.  This  paper  proposes  an  attentional  network  for  the  task  of  Continuous  Sign  Language  Recognition.  The  proposed  approach  exploits  co-independent  streams  of  data  to  model  the  sign  language  modalities.  These  different  channels  of  information  can  share  a  complex  temporal  structure  between  each  other.  For  that  reason,  we  apply  attention  to  synchronize  and  help  capture  entangled  dependencies  between  the  different  sign  language  components.  Even  though  Sign  Language  is  multi-channel,  handshapes  represent  the  central  entities  in  sign  interpretation.  Seeing  handshapes  in  their  correct  context  defines  the  meaning  of  a  sign.  Taking  that  into  account,  we  utilize  the  attention  mechanism  to  efficiently  aggregate  the  hand  features  with  their  appropriate  spatio-temporal  context  for  better  sign  recognition.  We  found  that  by  doing  so  the  model  is  able  to  identify  the  essential  Sign  Language  components  that  revolve  around  the  dominant  hand  and  the  face  areas.  We  test  our  model  on  the  benchmark  dataset  RWTH-PHOENIX-Weather  2014,  yielding  competitive  results.
0	Lates  latent  space  distillation  for  teacher  student  driving  policy  learning.  We  describe  a  policy  learning  approach  to  map  visual  inputs  to  driving  controls  that  leverages  side  information  on  semantics  and  affordances  of  objects  in  the  scene  from  a  secondary  teacher  model.  While  the  teacher  receives  semantic  segmentation  and  stop  "intention"  values  as  inputs  and  produces  an  estimate  of  the  driving  controls,  the  primary  student  model  only  receives  images  as  inputs,  and  attempts  to  imitate  the  controls  while  being  biased  towards  the  latent  representation  of  the  teacher  model.  The  latent  representation  encodes  task-relevant  information  in  the  inputs  of  the  teacher  model,  which  are  semantic  segmentation  of  the  image,  and  intention  values  for  driving  controls  in  the  presence  of  objects  in  the  scene  such  as  vehicles,  pedestrians  and  traffic  lights.  Our  student  model  does  not  attempt  to  infer  semantic  segmentation  or  intention  values  from  its  inputs,  nor  to  mimic  the  output  behavior  of  the  teacher.  It  instead  attempts  to  capture  the  representation  of  the  teacher  inputs  that  are  relevant  for  driving.  Our  training  does  not  require  laborious  annotations  such  as  maps  or  objects  in  three  dimensions;  even  the  teacher  model  just  requires  two-dimensional  segmentation  and  intention  values.  Moreover,  our  model  runs  in  real  time  of  59  FPS.  We  test  our  approach  on  recent  simulated  and  real-world  driving  datasets,  and  introduce  a  more  challenging  but  realistic  evaluation  protocol  that  considers  a  run  that  reaches  the  destination  successful  only  if  it  does  not  violate  common  traffic  rules.
0	Hue  histograms  to  spatiotemporal  local  features  for  action  recognition.  Despite  the  recent  developments  in  spatiotemporal  local  features  for  action  recognition  in  video  sequences,  local  color  information  has  so  far  been  ignored.  However,  color  has  been  proved  an  important  element  to  the  success  of  automated  recognition  of  objects  and  scenes.  In  this  paper  we  extend  the  space-time  interest  point  descriptor  STIP  to  take  into  account  the  color  information  on  the  features'  neighborhood.  We  compare  the  performance  of  our  color-aware  version  of  STIP  (which  we  have  called  HueSTIP)  with  the  original  one.
0	Person  in  wifi  fine  grained  person  perception  using  wifi.  Fine-grained  person  perception  such  as  body  segmentation  and  pose  estimation  has  been  achieved  with  many  2D  and  3D  sensors  such  as  RGB/depth  cameras,  radars  (e.g.,  RF-Pose)  and  LiDARs.  These  sensors  capture  2D  pixels  or  3D  point  clouds  of  person  bodies  with  high  spatial  resolution,  such  that  the  existing  Convolutional  Neural  Networks  can  be  directly  applied  for  perception.  In  this  paper,  we  take  one  step  forward  to  show  that  fine-grained  person  perception  is  possible  even  with  1D  sensors:  WiFi  antennas.  To  our  knowledge,  this  is  the  first  work  to  perceive  persons  with  pervasive  WiFi  devices,  which  is  cheaper  and  power  efficient  than  radars  and  LiDARs,  invariant  to  illumination,  and  has  little  privacy  concern  comparing  to  cameras.  We  used  two  sets  of  off-the-shelf  WiFi  antennas  to  acquire  signals,  i.e.,  one  transmitter  set  and  one  receiver  set.  Each  set  contains  three  antennas  lined-up  as  a  regular  household  WiFi  router.  The  WiFi  signal  generated  by  a  transmitter  antenna,  penetrates  through  and  reflects  on  human  bodies,  furniture  and  walls,  and  then  superposes  at  a  receiver  antenna  as  a  1D  signal  sample  (instead  of  2D  pixels  or  3D  point  clouds).  We  developed  a  deep  learning  approach  that  uses  annotations  on  2D  images,  takes  the  received  1D  WiFi  signals  as  inputs,  and  performs  body  segmentation  and  pose  estimation  in  an  end-to-end  manner.  Experimental  results  on  over  100000  frames  under  16  indoor  scenes  demonstrate  that  Person-in-WiFi  achieved  person  perception  comparable  to  approaches  using  2D  images.
0	Adashare  learning  what  to  share  for  efficient  deep  multi  task  learning.  Multi-task  learning  is  an  open  and  challenging  problem  in  computer  vision.  The  typical  way  of  conducting  multi-task  learning  with  deep  neural  networks  is  either  through  handcrafted  schemes  that  share  all  initial  layers  and  branch  out  at  an  adhoc  point,  or  through  separate  task-specific  networks  with  an  additional  feature  sharing/fusion  mechanism.  Unlike  existing  methods,  we  propose  an  adaptive  sharing  approach,  called  AdaShare,  that  decides  what  to  share  across  which  tasks  to  achieve  the  best  recognition  accuracy,  while  taking  resource  efficiency  into  account.  Specifically,  our  main  idea  is  to  learn  the  sharing  pattern  through  a  task-specific  policy  that  selectively  chooses  which  layers  to  execute  for  a  given  task  in  the  multi-task  network.  We  efficiently  optimize  the  task-specific  policy  jointly  with  the  network  weights,  using  standard  back-propagation.  Experiments  on  several  challenging  and  diverse  benchmark  datasets  with  a  variable  number  of  tasks  well  demonstrate  the  efficacy  of  our  approach  over  state-of-the-art  methods.  Project  page:  this  https  URL.
0	Learning  to  reconstruct  3d  manhattan  wireframes  from  a  single  image.  In  this  paper,  we  propose  a  method  to  obtain  a  compact  and  accurate  3D  wireframe  representation  from  a  single  image  by  effectively  exploiting  global  structural  regularities.  Our  method  trains  a  convolutional  neural  network  to  simultaneously  detect  salient  junctions  and  straight  lines,  as  well  as  predict  their  3D  depth  and  vanishing  points.  Compared  with  the  state-of-the-art  learning-based  wireframe  detection  methods,  our  network  is  much  simpler  and  more  unified,  leading  to  better  2D  wireframe  detection.  With  global  structural  priors  such  as  Manhattan  assumption,  our  method  further  reconstructs  a  full  3D  wireframe  model,  a  compact  vector  representation  suitable  for  a  variety  of  high-level  vision  tasks  such  as  AR  and  CAD.  We  conduct  extensive  evaluations  on  a  large  synthetic  dataset  of  urban  scenes  as  well  as  real  images.  Our  code  and  datasets  will  be  released.
0	Learning  dynamic  memory  networks  for  object  tracking.  Template-matching  methods  for  visual  tracking  have  gained  popularity  recently  due  to  their  comparable  performance  and  fast  speed.  However,  they  lack  effective  ways  to  adapt  to  changes  in  the  target  object's  appearance,  making  their  tracking  accuracy  still  far  from  state-of-the-art.  In  this  paper,  we  propose  a  dynamic  memory  network  to  adapt  the  template  to  the  target's  appearance  variations  during  tracking.  An  LSTM  is  used  as  a  memory  controller,  where  the  input  is  the  search  feature  map  and  the  outputs  are  the  control  signals  for  the  reading  and  writing  process  of  the  memory  block.  As  the  location  of  the  target  is  at  first  unknown  in  the  search  feature  map,  an  attention  mechanism  is  applied  to  concentrate  the  LSTM  input  on  the  potential  target.  To  prevent  aggressive  model  adaptivity,  we  apply  gated  residual  template  learning  to  control  the  amount  of  retrieved  memory  that  is  used  to  combine  with  the  initial  template.  Unlike  tracking-by-detection  methods  where  the  object's  information  is  maintained  by  the  weight  parameters  of  neural  networks,  which  requires  expensive  online  fine-tuning  to  be  adaptable,  our  tracker  runs  completely  feed-forward  and  adapts  to  the  target's  appearance  changes  by  updating  the  external  memory.  Moreover,  unlike  other  tracking  methods  where  the  model  capacity  is  fixed  after  offline  training  ---  the  capacity  of  our  tracker  can  be  easily  enlarged  as  the  memory  requirements  of  a  task  increase,  which  is  favorable  for  memorizing  long-term  object  information.  Extensive  experiments  on  OTB  and  VOT  demonstrates  that  our  tracker  MemTrack  performs  favorably  against  state-of-the-art  tracking  methods  while  retaining  real-time  speed  of  50  fps.
0	Banach  wasserstein  gan.  Wasserstein  Generative  Adversarial  Networks  (WGANs)  can  be  used  to  generate  realistic  samples  from  complicated  image  distributions.  The  Wasserstein  metric  used  in  WGANs  is  based  on  a  notion  of  distance  between  individual  images,  which  induces  a  notion  of  distance  between  probability  distributions  of  images.  So  far  the  community  has  considered  $\ell^2$  as  the  underlying  distance.  We  generalize  the  theory  of  WGAN  with  gradient  penalty  to  Banach  spaces,  allowing  practitioners  to  select  the  features  to  emphasize  in  the  generator.  We  further  discuss  the  effect  of  some  particular  choices  of  underlying  norms,  focusing  on  Sobolev  norms.  Finally,  we  demonstrate  a  boost  in  performance  for  an  appropriate  choice  of  norm  on  CIFAR-10  and  CelebA.
0	An  interpretable  deep  hierarchical  semantic  convolutional  neural  network  for  lung  nodule  malignancy  classification.  While  deep  learning  methods  are  increasingly  being  applied  to  tasks  such  as  computer-aided  diagnosis,  these  models  are  difficult  to  interpret,  do  not  incorporate  prior  domain  knowledge,  and  are  often  considered  as  a  "black-box."  The  lack  of  model  interpretability  hinders  them  from  being  fully  understood  by  target  users  such  as  radiologists.  In  this  paper,  we  present  a  novel  interpretable  deep  hierarchical  semantic  convolutional  neural  network  (HSCNN)  to  predict  whether  a  given  pulmonary  nodule  observed  on  a  computed  tomography  (CT)  scan  is  malignant.  Our  network  provides  two  levels  of  output:  1)  low-level  radiologist  semantic  features,  and  2)  a  high-level  malignancy  prediction  score.  The  low-level  semantic  outputs  quantify  the  diagnostic  features  used  by  radiologists  and  serve  to  explain  how  the  model  interprets  the  images  in  an  expert-driven  manner.  The  information  from  these  low-level  tasks,  along  with  the  representations  learned  by  the  convolutional  layers,  are  then  combined  and  used  to  infer  the  high-level  task  of  predicting  nodule  malignancy.  This  unified  architecture  is  trained  by  optimizing  a  global  loss  function  including  both  low-  and  high-level  tasks,  thereby  learning  all  the  parameters  within  a  joint  framework.  Our  experimental  results  using  the  Lung  Image  Database  Consortium  (LIDC)  show  that  the  proposed  method  not  only  produces  interpretable  lung  cancer  predictions  but  also  achieves  significantly  better  results  compared  to  common  3D  CNN  approaches.
0	3d  physnet  learning  the  intuitive  physics  of  non  rigid  object  deformations.  The  ability  to  interact  and  understand  the  environment  is  a  fundamental  prerequisite  for  a  wide  range  of  applications  from  robotics  to  augmented  reality.  In  particular,  predicting  how  deformable  objects  will  react  to  applied  forces  in  real  time  is  a  significant  challenge.  This  is  further  confounded  by  the  fact  that  shape  information  about  encountered  objects  in  the  real  world  is  often  impaired  by  occlusions,  noise  and  missing  regions  e.g.  a  robot  manipulating  an  object  will  only  be  able  to  observe  a  partial  view  of  the  entire  solid.  In  this  work  we  present  a  framework,  3D-PhysNet,  which  is  able  to  predict  how  a  three-dimensional  solid  will  deform  under  an  applied  force  using  intuitive  physics  modelling.  In  particular,  we  propose  a  new  method  to  encode  the  physical  properties  of  the  material  and  the  applied  force,  enabling  generalisation  over  materials.  The  key  is  to  combine  deep  variational  autoencoders  with  adversarial  training,  conditioned  on  the  applied  force  and  the  material  properties.  We  further  propose  a  cascaded  architecture  that  takes  a  single  2.5D  depth  view  of  the  object  and  predicts  its  deformation.  Training  data  is  provided  by  a  physics  simulator.  The  network  is  fast  enough  to  be  used  in  real-time  applications  from  partial  views.  Experimental  results  show  the  viability  and  the  generalisation  properties  of  the  proposed  architecture.
0	Domain  adaptive  object  detection  via  asymmetric  tri  way  faster  rcnn.  Conventional  object  detection  models  inevitably  encounter  a  performance  drop  as  the  domain  disparity  exists.  Unsupervised  domain  adaptive  object  detection  is  proposed  recently  to  reduce  the  disparity  between  domains,  where  the  source  domain  is  label-rich  while  the  target  domain  is  label-agnostic.  The  existing  models  follow  a  parameter  shared  siamese  structure  for  adversarial  domain  alignment,  which,  however,  easily  leads  to  the  collapse  and  out-of-control  risk  of  the  source  domain  and  brings  negative  impact  to  feature  adaption.  The  main  reason  is  that  the  labeling  unfairness  (asymmetry)  between  source  and  target  makes  the  parameter  sharing  mechanism  unable  to  adapt.  Therefore,  in  order  to  avoid  the  source  domain  collapse  risk  caused  by  parameter  sharing,  we  propose  an  asymmetric  tri-way  Faster-RCNN  (ATF)  for  domain  adaptive  object  detection.  Our  ATF  model  has  two  distinct  merits:  1)  A  ancillary  net  supervised  by  source  label  is  deployed  to  learn  ancillary  target  features  and  simultaneously  preserve  the  discrimination  of  source  domain,  which  enhances  the  structural  discrimination  (object  classification  vs.  bounding  box  regression)  of  domain  alignment.  2)  The  asymmetric  structure  consisting  of  a  chief  net  and  an  independent  ancillary  net  essentially  overcomes  the  parameter  sharing  aroused  source  risk  collapse.  The  adaption  safety  of  the  proposed  ATF  detector  is  guaranteed.  Extensive  experiments  on  a  number  of  datasets,  including  Cityscapes,  Foggy-cityscapes,  KITTI,  Sim10k,  Pascal  VOC,  Clipart  and  Watercolor,  demonstrate  the  SOTA  performance  of  our  method.
0	Efficient  vision  transformers  via  fine  grained  manifold  distillation.  This  paper  studies  the  model  compression  problem  of  vision  transformers.  Benefit  from  the  self-attention  module,  transformer  architectures  have  shown  extraordinary  performance  on  many  computer  vision  tasks.  Although  the  network  performance  is  boosted,  transformers  are  often  required  more  computational  resources  including  memory  usage  and  the  inference  complexity.  Compared  with  the  existing  knowledge  distillation  approaches,  we  propose  to  excavate  useful  information  from  the  teacher  transformer  through  the  relationship  between  images  and  the  divided  patches.  We  then  explore  an  efficient  fine-grained  manifold  distillation  approach  that  simultaneously  calculates  cross-images,  cross-patch,  and  random-selected  manifolds  in  teacher  and  student  models.  Experimental  results  conducted  on  several  benchmarks  demonstrate  the  superiority  of  the  proposed  algorithm  for  distilling  portable  transformer  models  with  higher  performance.  For  example,  our  approach  achieves  75.06%  Top-1  accuracy  on  the  ImageNet-1k  dataset  for  training  a  DeiT-Tiny  model,  which  outperforms  other  ViT  distillation  methods.
0	Data  driven  6d  pose  tracking  by  calibrating  image  residuals  in  synthetic  domains.  Tracking  the  6D  pose  of  objects  in  video  sequences  is  important  for  robot  manipulation.  This  work  presents  se(3)-TrackNet,  a  data-driven  optimization  approach  for  long  term,  6D  pose  tracking.  It  aims  to  identify  the  optimal  relative  pose  given  the  current  RGB-D  observation  and  a  synthetic  image  conditioned  on  the  previous  best  estimate  and  the  object's  model.  The  key  contribution  in  this  context  is  a  novel  neural  network  architecture,  which  appropriately  disentangles  the  feature  encoding  to  help  reduce  domain  shift,  and  an  effective  3D  orientation  representation  via  Lie  Algebra.  Consequently,  even  when  the  network  is  trained  solely  with  synthetic  data  can  work  effectively  over  real  images.  Comprehensive  experiments  over  multiple  benchmarks  show  se(3)-TrackNet  achieves  consistently  robust  estimates  and  outperforms  alternatives,  even  though  they  have  been  trained  with  real  images.  The  approach  runs  in  real  time  at  90.9Hz.  Code,  data  and  supplementary  video  for  this  project  are  available  at  this  https  URL
0	Multi  dimensional  gated  recurrent  units  for  automated  anatomical  landmark  localization.  We  present  an  automated  method  for  localizing  an  anatomical  landmark  in  three-dimensional  medical  images.  The  method  combines  two  recurrent  neural  networks  in  a  coarse-to-fine  approach:  The  first  network  determines  a  candidate  neighborhood  by  analyzing  the  complete  given  image  volume.  The  second  network  localizes  the  actual  landmark  precisely  and  accurately  in  the  candidate  neighborhood.  Both  networks  take  advantage  of  multi-dimensional  gated  recurrent  units  in  their  main  layers,  which  allow  for  high  model  complexity  with  a  comparatively  small  set  of  parameters.  We  localize  the  medullopontine  sulcus  in  3D  magnetic  resonance  images  of  the  head  and  neck.  We  show  that  the  proposed  approach  outperforms  similar  localization  techniques  both  in  terms  of  mean  distance  in  millimeters  and  voxels  w.r.t.  manual  labelings  of  the  data.  With  a  mean  localization  error  of  1.7  mm,  the  proposed  approach  performs  on  par  with  neurological  experts,  as  we  demonstrate  in  an  interrater  comparison.
0	Marioqa  answering  questions  by  watching  gameplay  videos.  We  present  a  new  benchmark  dataset  for  video  question  answering  (VideoQA)  designed  to  evaluate  algorithms'  capability  of  spatio-temporal  event  understanding.  Existing  datasets  either  require  very  high-level  reasoning  from  multi-modal  information  to  find  answers,  or  is  mostly  composed  of  the  questions  that  can  be  answered  by  watching  a  single  frame.  Therefore,  they  are  not  suitable  to  evaluate  models'  real  capacity  and  flexibility  for  VideoQA.  To  overcome  such  critical  limitations,  we  focus  on  event-centric  questions  that  require  understanding  temporal  relation  between  multiple  events  in  videos.  An  interesting  idea  in  dataset  construction  process  is  that  question-answer  pairs  are  automatically  generated  from  Super  Mario  video  gameplays  given  a  set  of  question  templates.  We  also  tackle  VideoQA  problem  in  the  new  dataset,  referred  to  as  MarioQA,  by  proposing  spatio-temporal  attention  models  based  on  deep  neural  networks.  Our  experiments  show  that  the  proposed  deep  neural  network  models  with  attention  have  meaningful  performance  improvement  over  several  baselines.
0	Full  flow  optical  flow  estimation  by  global  optimization  over  regular  grids.  We  present  a  global  optimization  approach  to  optical  flow  estimation.  The  approach  optimizes  a  classical  optical  flow  objective  over  the  full  space  of  mappings  between  discrete  grids.  No  descriptor  matching  is  used.  The  highly  regular  structure  of  the  space  of  mappings  enables  optimizations  that  reduce  the  computational  complexity  of  the  algorithm's  inner  loop  from  quadratic  to  linear  and  support  efficient  matching  of  tens  of  thousands  of  nodes  to  tens  of  thousands  of  displacements.  We  show  that  one-shot  global  optimization  of  a  classical  Horn-Schunck-type  objective  over  regular  grids  at  a  single  resolution  is  sufficient  to  initialize  continuous  interpolation  and  achieve  state-of-the-art  performance  on  challenging  modern  benchmarks.
0	End  to  end  denoising  of  dark  burst  images  using  recurrent  fully  convolutional  networks.  When  taking  photos  in  dim-light  environments,  due  to  the  small  amount  of  light  entering,  the  shot  images  are  usually  extremely  dark,  with  a  great  deal  of  noise,  and  the  color  cannot  reflect  real-world  color.  Under  this  condition,  the  traditional  methods  used  for  single  image  denoising  have  always  failed  to  be  effective.  One  common  idea  is  to  take  multiple  frames  of  the  same  scene  to  enhance  the  signal-to-noise  ratio.  This  paper  proposes  a  recurrent  fully  convolutional  network  (RFCN)  to  process  burst  photos  taken  under  extremely  low-light  conditions,  and  to  obtain  denoised  images  with  improved  brightness.  Our  model  maps  raw  burst  images  directly  to  sRGB  outputs,  either  to  produce  a  best  image  or  to  generate  a  multi-frame  denoised  image  sequence.  This  process  has  proven  to  be  capable  of  accomplishing  the  low-level  task  of  denoising,  as  well  as  the  high-level  task  of  color  correction  and  enhancement,  all  of  which  is  end-to-end  processing  through  our  network.  Our  method  has  achieved  better  results  than  state-of-the-art  methods.  In  addition,  we  have  applied  the  model  trained  by  one  type  of  camera  without  fine-tuning  on  photos  captured  by  different  cameras  and  have  obtained  similar  end-to-end  enhancements.
0	Transfer  learning  using  classification  layer  features  of  cnn.  Although  CNNs  have  gained  the  ability  to  transfer  learned  knowledge  from  source  task  to  target  task  by  virtue  of  large  annotated  datasets  but  consume  huge  processing  time  to  fine-tune  without  GPU.  In  this  paper,  we  propose  a  new  computationally  efficient  transfer  learning  approach  using  classification  layer  features  of  pre-trained  CNNs  by  appending  layer  after  existing  classification  layer.  We  demonstrate  that  fine-tuning  of  the  appended  layer  with  existing  classification  layer  for  new  task  converges  much  faster  than  baseline  and  in  average  outperforms  baseline  classification  accuracy.  Furthermore,  we  execute  thorough  experiments  to  examine  the  influence  of  quantity,  similarity,  and  dissimilarity  of  training  sets  in  our  classification  outcomes  to  demonstrate  transferability  of  classification  layer  features.
0	Computational  ceramicology.  Field  archeologists  are  called  upon  to  identify  potsherds,  for  which  purpose  they  rely  on  their  experience  and  on  reference  works.  We  have  developed  two  complementary  machine-learning  tools  to  propose  identifications  based  on  images  captured  on  site.  One  method  relies  on  the  shape  of  the  fracture  outline  of  a  sherd;  the  other  is  based  on  decorative  features.  For  the  outline-identification  tool,  a  novel  deep-learning  architecture  was  employed,  one  that  integrates  shape  information  from  points  along  the  inner  and  outer  surfaces.  The  decoration  classifier  is  based  on  relatively  standard  architectures  used  in  image  recognition.  In  both  cases,  training  the  classifiers  required  tackling  challenges  that  arise  when  working  with  real-world  archeological  data:  paucity  of  labeled  data;  extreme  imbalance  between  instances  of  the  different  categories;  and  the  need  to  avoid  neglecting  rare  classes  and  to  take  note  of  minute  distinguishing  features  of  some  classes.  The  scarcity  of  training  data  was  overcome  by  using  synthetically-produced  virtual  potsherds  and  by  employing  multiple  data-augmentation  techniques.  A  novel  form  of  training  loss  allowed  us  to  overcome  the  problems  caused  by  under-populated  classes  and  non-homogeneous  distribution  of  discriminative  features.
0	Two  stream  networks  for  self  supervised  ego  motion  estimation.  Learning  depth  and  camera  ego-motion  from  raw  unlabeled  RGB  video  streams  is  seeing  exciting  progress  through  self-supervision  from  strong  geometric  cues.  To  leverage  not  only  appearance  but  also  scene  geometry,  we  propose  a  novel  self-supervised  two-stream  network  using  RGB  and  inferred  depth  information  for  accurate  visual  odometry.  In  addition,  we  introduce  a  sparsity-inducing  data  augmentation  policy  for  ego-motion  learning  that  effectively  regularizes  the  pose  network  to  enable  stronger  generalization  performance.  As  a  result,  we  show  that  our  proposed  two-stream  pose  network  achieves  state-of-the-art  results  among  learning-based  methods  on  the  KITTI  odometry  benchmark,  and  is  especially  suited  for  self-supervision  at  scale.  Our  experiments  on  a  large-scale  urban  driving  dataset  of  1  million  frames  indicate  that  the  performance  of  our  proposed  architecture  does  indeed  scale  progressively  with  more  data.
0	Deep  joint  task  learning  for  generic  object  extraction.  This  paper  investigates  how  to  extract  objects-of-interest  without  relying  on  hand-craft  features  and  sliding  windows  approaches,  that  aims  to  jointly  solve  two  sub-tasks:  (i)  rapidly  localizing  salient  objects  from  images,  and  (ii)  accurately  segmenting  the  objects  based  on  the  localizations.  We  present  a  general  joint  task  learning  framework,  in  which  each  task  (either  object  localization  or  object  segmentation)  is  tackled  via  a  multi-layer  convolutional  neural  network,  and  the  two  networks  work  collaboratively  to  boost  performance.  In  particular,  we  propose  to  incorporate  latent  variables  bridging  the  two  networks  in  a  joint  optimization  manner.  The  first  network  directly  predicts  the  positions  and  scales  of  salient  objects  from  raw  images,  and  the  latent  variables  adjust  the  object  localizations  to  feed  the  second  network  that  produces  pixelwise  object  masks.  An  EM-type  method  is  presented  for  the  optimization,  iterating  with  two  steps:  (i)  by  using  the  two  networks,  it  estimates  the  latent  variables  by  employing  an  MCMC-based  sampling  method;  (ii)  it  optimizes  the  parameters  of  the  two  networks  unitedly  via  back  propagation,  with  the  fixed  latent  variables.  Extensive  experiments  suggest  that  our  framework  significantly  outperforms  other  state-of-the-art  approaches  in  both  accuracy  and  efficiency  (e.g.  1000  times  faster  than  competing  approaches).
0	Learning  the  model  update  for  siamese  trackers.  Siamese  approaches  address  the  visual  tracking  problem  by  extracting  an  appearance  template  from  the  current  frame,  which  is  used  to  localize  the  target  in  the  next  frame.  In  general,  this  template  is  linearly  combined  with  the  accumulated  template  from  the  previous  frame,  resulting  in  an  exponential  decay  of  information  over  time.  While  such  an  approach  to  updating  has  led  to  improved  results,  its  simplicity  limits  the  potential  gain  likely  to  be  obtained  by  learning  to  update.  Therefore,  we  propose  to  replace  the  handcrafted  update  function  with  a  method  which  learns  to  update.  We  use  a  convolutional  neural  network,  called  UpdateNet,  which  given  the  initial  template,  the  accumulated  template  and  the  template  of  the  current  frame  aims  to  estimate  the  optimal  template  for  the  next  frame.  The  UpdateNet  is  compact  and  can  easily  be  integrated  into  existing  Siamese  trackers.  We  demonstrate  the  generality  of  the  proposed  approach  by  applying  it  to  two  Siamese  trackers,  SiamFC  and  DaSiamRPN.  Extensive  experiments  on  VOT2016,  VOT2018,  LaSOT,  and  TrackingNet  datasets  demonstrate  that  our  UpdateNet  effectively  predicts  the  new  target  template,  outperforming  the  standard  linear  update.  On  the  large-scale  TrackingNet  dataset,  our  UpdateNet  improves  the  results  of  DaSiamRPN  with  an  absolute  gain  of  3.9%  in  terms  of  success  score.
0	Learning  more  with  less  conditional  pggan  based  data  augmentation  for  brain  metastases  detection  using  highly  rough  annotation  on  mr  images.  Accurate  Computer-Assisted  Diagnosis,  associated  with  proper  data  wrangling,  can  alleviate  the  risk  of  overlooking  the  diagnosis  in  a  clinical  environment.  Towards  this,  as  a  Data  Augmentation  (DA)  technique,  Generative  Adversarial  Networks  (GANs)  can  synthesize  additional  training  data  to  handle  the  small/fragmented  medical  imaging  datasets  collected  from  various  scanners;  those  images  are  realistic  but  completely  different  from  the  original  ones,  filling  the  data  lack  in  the  real  image  distribution.  However,  we  cannot  easily  use  them  to  locate  disease  areas,  considering  expert  physicians'  expensive  annotation  cost.  Therefore,  this  paper  proposes  Conditional  Progressive  Growing  of  GANs  (CPGGANs),  incorporating  highly-rough  bounding  box  conditions  incrementally  into  PGGANs  to  place  brain  metastases  at  desired  positions/sizes  on  256  X  256  Magnetic  Resonance  (MR)  images,  for  Convolutional  Neural  Network-based  tumor  detection;  this  first  GAN-based  medical  DA  using  automatic  bounding  box  annotation  improves  the  training  robustness.  The  results  show  that  CPGGAN-based  DA  can  boost  10%  sensitivity  in  diagnosis  with  clinically  acceptable  additional  False  Positives.  Surprisingly,  further  tumor  realism,  achieved  with  additional  normal  brain  MR  images  for  CPGGAN  training,  does  not  contribute  to  detection  performance,  while  even  three  physicians  cannot  accurately  distinguish  them  from  the  real  ones  in  Visual  Turing  Test.
0	Multi  task  learning  for  detecting  and  segmenting  manipulated  facial  images  and  videos.  Detecting  manipulated  images  and  videos  is  an  important  topic  in  digital  media  forensics.  Most  detection  methods  use  binary  classification  to  determine  the  probability  of  a  query  being  manipulated.  Another  important  topic  is  locating  manipulated  regions  (i.e.,  performing  segmentation),  which  are  mostly  created  by  three  commonly  used  attacks:  removal,  copy-move,  and  splicing.  We  have  designed  a  convolutional  neural  network  that  uses  the  multi-task  learning  approach  to  simultaneously  detect  manipulated  images  and  videos  and  locate  the  manipulated  regions  for  each  query.  Information  gained  by  performing  one  task  is  shared  with  the  other  task  and  thereby  enhance  the  performance  of  both  tasks.  A  semi-supervised  learning  approach  is  used  to  improve  the  network's  generability.  The  network  includes  an  encoder  and  a  Y-shaped  decoder.  Activation  of  the  encoded  features  is  used  for  the  binary  classification.  The  output  of  one  branch  of  the  decoder  is  used  for  segmenting  the  manipulated  regions  while  that  of  the  other  branch  is  used  for  reconstructing  the  input,  which  helps  improve  overall  performance.  Experiments  using  the  FaceForensics  and  FaceForensics++  databases  demonstrated  the  network's  effectiveness  against  facial  reenactment  attacks  and  face  swapping  attacks  as  well  as  its  ability  to  deal  with  the  mismatch  condition  for  previously  seen  attacks.  Moreover,  fine-tuning  using  just  a  small  amount  of  data  enables  the  network  to  deal  with  unseen  attacks.
0	Tract  orientation  mapping  for  bundle  specific  tractography.  While  the  major  white  matter  tracts  are  of  great  interest  to  numerous  studies  in  neuroscience  and  medicine,  their  manual  dissection  in  larger  cohorts  from  diffusion  MRI  tractograms  is  time-consuming,  requires  expert  knowledge  and  is  hard  to  reproduce.  Tract  orientation  mapping  (TOM)  is  a  novel  concept  that  facilitates  bundle-specific  tractography  based  on  a  learned  mapping  from  the  original  fiber  orientation  distribution  function  (fODF)  peaks  to  a  list  of  tract  orientation  maps  (also  abbr.  TOM).  Each  TOM  represents  one  of  the  known  tracts  with  each  voxel  containing  no  more  than  one  orientation  vector.  TOMs  can  act  as  a  prior  or  even  as  direct  input  for  tractography.  We  use  an  encoder-decoder  fully-convolutional  neural  network  architecture  to  learn  the  required  mapping.  In  comparison  to  previous  concepts  for  the  reconstruction  of  specific  bundles,  the  presented  one  avoids  various  cumbersome  processing  steps  like  whole  brain  tractography,  atlas  registration  or  clustering.  We  compare  it  to  four  state  of  the  art  bundle  recognition  methods  on  20  different  bundles  in  a  total  of  105  subjects  from  the  Human  Connectome  Project.  Results  are  anatomically  convincing  even  for  difficult  tracts,  while  reaching  low  angular  errors,  unprecedented  runtimes  and  top  accuracy  values  (Dice).  Our  code  and  our  data  are  openly  available.
0	Iterative  attention  mining  for  weakly  supervised  thoracic  disease  pattern  localization  in  chest  x  rays.  Given  image  labels  as  the  only  supervisory  signal,  we  focus  on  harvesting,  or  mining,  thoracic  disease  localizations  from  chest  X-ray  images.  Harvesting  such  localizations  from  existing  datasets  allows  for  the  creation  of  improved  data  sources  for  computer-aided  diagnosis  and  retrospective  analyses.  We  train  a  convolutional  neural  network  (CNN)  for  image  classification  and  propose  an  attention  mining  (AM)  strategy  to  improve  the  model's  sensitivity  or  saliency  to  disease  patterns.  The  intuition  of  AM  is  that  once  the  most  salient  disease  area  is  blocked  or  hidden  from  the  CNN  model,  it  will  pay  attention  to  alternative  image  regions,  while  still  attempting  to  make  correct  predictions.  However,  the  model  requires  to  be  properly  constrained  during  AM,  otherwise,  it  may  overfit  to  uncorrelated  image  parts  and  forget  the  valuable  knowledge  that  it  has  learned  from  the  original  image  classification  task.  To  alleviate  such  side  effects,  we  then  design  a  knowledge  preservation  (KP)  loss,  which  minimizes  the  discrepancy  between  responses  for  X-ray  images  from  the  original  and  the  updated  networks.  Furthermore,  we  modify  the  CNN  model  to  include  multi-scale  aggregation  (MSA),  improving  its  localization  ability  on  small-scale  disease  findings,  e.g.,  lung  nodules.  We  experimentally  validate  our  method  on  the  publicly-available  ChestX-ray14  dataset,  outperforming  a  class  activation  map  (CAM)-based  approach,  and  demonstrating  the  value  of  our  novel  framework  for  mining  disease  locations.
0	Image2gif  generating  cinemagraphs  using  recurrent  deep  q  networks.  Given  a  still  photograph,  one  can  imagine  how  dynamic  objects  might  move  against  a  static  background.  This  idea  has  been  actualized  in  the  form  of  cinemagraphs,  where  the  motion  of  particular  objects  within  a  still  image  is  repeated,  giving  the  viewer  a  sense  of  animation.  In  this  paper,  we  learn  computational  models  that  can  generate  cinemagraph  sequences  automatically  given  a  single  image.  To  generate  cinemagraphs,  we  explore  combining  generative  models  with  a  recurrent  neural  network  and  deep  Q-networks  to  enhance  the  power  of  sequence  generation.  To  enable  and  evaluate  these  models  we  make  use  of  two  datasets,  one  synthetically  generated  and  the  other  containing  real  video  generated  cinemagraphs.  Both  qualitative  and  quantitative  evaluations  demonstrate  the  effectiveness  of  our  models  on  the  synthetic  and  real  datasets.
0	Breaking  the  cycle  colleagues  are  all  you  need.  This  paper  proposes  a  novel  approach  to  performing  image-to-image  translation  between  unpaired  domains.  Rather  than  relying  on  a  cycle  constraint,  our  method  takes  advantage  of  collaboration  between  various  GANs.  This  results  in  a  multi-modal  method,  in  which  multiple  optional  and  diverse  images  are  produced  for  a  given  image.  Our  model  addresses  some  of  the  shortcomings  of  classical  GANs:  (1)  It  is  able  to  remove  large  objects,  such  as  glasses.  (2)  Since  it  does  not  need  to  support  the  cycle  constraint,  no  irrelevant  traces  of  the  input  are  left  on  the  generated  image.  (3)  It  manages  to  translate  between  domains  that  require  large  shape  modifications.  Our  results  are  shown  to  outperform  those  generated  by  state-of-the-art  methods  for  several  challenging  applications  on  commonly-used  datasets,  both  qualitatively  and  quantitatively.
0	On  the  difficulty  of  learning  and  predicting  the  long  term  dynamics  of  bouncing  objects.  The  ability  to  accurately  predict  the  surrounding  environment  is  a  foundational  principle  of  intelligence  in  biological  and  artificial  agents.  In  recent  years,  a  variety  of  approaches  have  been  proposed  for  learning  to  predict  the  physical  dynamics  of  objects  interacting  in  a  visual  scene.  Here  we  conduct  a  systematic  empirical  evaluation  of  several  state-of-the-art  unsupervised  deep  learning  models  that  are  considered  capable  of  learning  the  spatio-temporal  structure  of  a  popular  dataset  composed  by  synthetic  videos  of  bouncing  objects.  We  show  that  most  of  the  models  indeed  obtain  high  accuracy  on  the  standard  benchmark  of  predicting  the  next  frame  of  a  sequence,  and  one  of  them  even  achieves  state-of-the-art  performance.  However,  all  models  fall  short  when  probed  with  the  more  challenging  task  of  generating  multiple  successive  frames.  Our  results  show  that  the  ability  to  perform  short-term  predictions  does  not  imply  that  the  model  has  captured  the  underlying  structure  and  dynamics  of  the  visual  environment,  thereby  calling  for  a  careful  rethinking  of  the  metrics  commonly  adopted  for  evaluating  temporal  models.  We  also  investigate  whether  the  learning  outcome  could  be  affected  by  the  use  of  curriculum-based  teaching.
0	Uldor  a  universal  lesion  detector  for  ct  scans  with  pseudo  masks  and  hard  negative  example  mining.  Automatic  lesion  detection  from  computed  tomography  (CT)  scans  is  an  important  task  in  medical  imaging  analysis.  It  is  still  very  challenging  due  to  similar  appearances  (e.g.  intensity  and  texture)  between  lesions  and  other  tissues,  making  it  especially  difficult  to  develop  a  universal  lesion  detector.  Instead  of  developing  a  specific-type  lesion  detector,  this  work  builds  a  Universal  Lesion  Detector  (ULDor)  based  on  Mask  R-CNN,  which  is  able  to  detect  all  different  kinds  of  lesions  from  whole  body  parts.  As  a  state-of-the-art  object  detector,  Mask  R-CNN  adds  a  branch  for  predicting  segmentation  masks  on  each  Region  of  Interest  (RoI)  to  improve  the  detection  performance.  However,  it  is  almost  impossible  to  manually  annotate  a  large-scale  dataset  with  pixel-level  lesion  masks  to  train  the  Mask  R-CNN  for  lesion  detection.  To  address  this  problem,  this  work  constructs  a  pseudo  mask  for  each  lesion  region  that  can  be  considered  as  a  surrogate  of  the  real  mask,  based  on  which  the  Mask  R-CNN  is  employed  for  lesion  detection.  On  the  other  hand,  this  work  proposes  a  hard  negative  example  mining  strategy  to  reduce  the  false  positives  for  improving  the  detection  performance.  Experimental  results  on  the  NIH  DeepLesion  dataset  demonstrate  that  the  ULDor  is  enhanced  using  pseudo  masks  and  the  proposed  hard  negative  example  mining  strategy  and  achieves  a  sensitivity  of  86.21%  with  five  false  positives  per  image.
0	Vision  transformers  for  dense  prediction.  We  introduce  dense  vision  transformers,  an  architecture  that  leverages  vision  transformers  in  place  of  convolutional  networks  as  a  backbone  for  dense  prediction  tasks.  We  assemble  tokens  from  various  stages  of  the  vision  transformer  into  image-like  representations  at  various  resolutions  and  progressively  combine  them  into  full-resolution  predictions  using  a  convolutional  decoder.  The  transformer  backbone  processes  representations  at  a  constant  and  relatively  high  resolution  and  has  a  global  receptive  field  at  every  stage.  These  properties  allow  the  dense  vision  transformer  to  provide  finer-grained  and  more  globally  coherent  predictions  when  compared  to  fully-convolutional  networks.  Our  experiments  show  that  this  architecture  yields  substantial  improvements  on  dense  prediction  tasks,  especially  when  a  large  amount  of  training  data  is  available.  For  monocular  depth  estimation,  we  observe  an  improvement  of  up  to  28%  in  relative  performance  when  compared  to  a  state-of-the-art  fully-convolutional  network.  When  applied  to  semantic  segmentation,  dense  vision  transformers  set  a  new  state  of  the  art  on  ADE20K  with  49.02%  mIoU.  We  further  show  that  the  architecture  can  be  fine-tuned  on  smaller  datasets  such  as  NYUv2,  KITTI,  and  Pascal  Context  where  it  also  sets  the  new  state  of  the  art.  Our  models  are  available  at  this  https  URL.
0	Masked  face  recognition  human  vs  machine.  The  recent  COVID-19  pandemic  has  increased  the  focus  on  hygienic  and  contactless  identity  verification  methods.  However,  the  pandemic  led  to  the  wide  use  of  face  masks,  essential  to  keep  the  pandemic  under  control.  The  effect  of  wearing  a  mask  on  face  recognition  in  a  collaborative  environment  is  currently  sensitive  yet  understudied  issue.  Recent  reports  have  tackled  this  by  evaluating  the  masked  probe  effect  on  the  performance  of  automatic  face  recognition  solutions.  However,  such  solutions  can  fail  in  certain  processes,  leading  to  performing  the  verification  task  by  a  human  expert.  This  work  provides  a  joint  evaluation  and  in-depth  analyses  of  the  face  verification  performance  of  human  experts  in  comparison  to  state-of-the-art  automatic  face  recognition  solutions.  This  involves  an  extensive  evaluation  with  12  human  experts  and  4  automatic  recognition  solutions.  The  study  concludes  with  a  set  of  take-home  messages  on  different  aspects  of  the  correlation  between  the  verification  behavior  of  human  and  machine.
0	Weight  standardization.  Batch  Normalization  (BN)  has  become  an  out-of-box  technique  to  improve  deep  network  training.  However,  its  effectiveness  is  limited  for  micro-batch  training,  i.e.,  each  GPU  typically  has  only  1-2  images  for  training,  which  is  inevitable  for  many  computer  vision  tasks,  e.g.,  object  detection  and  semantic  segmentation,  constrained  by  memory  consumption.  To  address  this  issue,  we  propose  Weight  Standardization  (WS)  and  Batch-Channel  Normalization  (BCN)  to  bring  two  success  factors  of  BN  into  micro-batch  training:  1)  the  smoothing  effects  on  the  loss  landscape  and  2)  the  ability  to  avoid  harmful  elimination  singularities  along  the  training  trajectory.  WS  standardizes  the  weights  in  convolutional  layers  to  smooth  the  loss  landscape  by  reducing  the  Lipschitz  constants  of  the  loss  and  the  gradients;  BCN  combines  batch  and  channel  normalizations  and  leverages  estimated  statistics  of  the  activations  in  convolutional  layers  to  keep  networks  away  from  elimination  singularities.  We  validate  WS  and  BCN  on  comprehensive  computer  vision  tasks,  including  image  classification,  object  detection,  instance  segmentation,  video  recognition  and  semantic  segmentation.  All  experimental  results  consistently  show  that  WS  and  BCN  improve  micro-batch  training  significantly.  Moreover,  using  WS  and  BCN  with  micro-batch  training  is  even  able  to  match  or  outperform  the  performances  of  BN  with  large-batch  training.
0	Multi  domain  ct  metal  artifacts  reduction  using  partial  convolution  based  inpainting.  Recent  CT  Metal  Artifacts  Reduction  (MAR)  methods  are  often  based  on  image-to-image  convolutional  neural  networks  for  adjustment  of  corrupted  sinograms  or  images  themselves.  In  this  paper,  we  are  exploring  the  capabilities  of  a  multi-domain  method  which  consists  of  both  sinogram  correction  (projection  domain  step)  and  restored  image  correction  (image-domain  step).  Moreover,  we  propose  a  formulation  of  the  first  step  problem  as  sinogram  inpainting  which  allows  us  to  use  methods  of  this  specific  field  such  as  partial  convolutions.  The  proposed  method  allows  to  achieve  state-of-the-art  (-75%  MSE)  improvement  in  comparison  with  a  classic  benchmark  -  Li-MAR.
0	Vinvl  making  visual  representations  matter  in  vision  language  models.  This  paper  presents  a  detailed  study  of  improving  visual  representations  for  vision  language  (VL)tasks  and  develops  an  improved  object  detection  model  to  provide  object-centric  representations  of  images.  Compared  to  the  most  widely  used  bottom-up  and  top-down  model  [2],  the  new  model  is  bigger,better-designed  for  VL  tasks,  and  pre-trained  on  much  larger  training  corpora  that  combine  multiple  public  annotated  object  detection  datasets.  Therefore,  it  can  generate  representations  of  a  richer  collection  of  visual  objects  and  concepts.  While  previous  VL  research  focuses  mainly  on  improving  the  vision-language  fusion  model  and  leaves  the  object  detection  model  improvement  untouched,  we  show  that  visual  features  matter  significantly  in  VL  models.  In  our  experiments  we  feed  the  visual  features  generated  by  the  new  object  detection  model  into  a  Transformer-based  VL  fusion  model  OSCAR[21],and  utilize  an  improved  approach  OSCAR+  to  pre-train  the  VL  model  and  fine-tune  it  on  a  wide  range  of  downstream  VL  tasks.  Our  results  show  that  the  new  visual  features  significantly  improve  the  performance  across  all  VL  tasks,  creating  new  state-of-the-art  results  on  seven  public  benchmarks.  We  will  release  the  new  object  detection  model  to  public.
0	Few  shot  action  recognition  with  compromised  metric  via  optimal  transport.  Although  vital  to  computer  vision  systems,  few-shot  action  recognition  is  still  not  mature  despite  the  wide  research  of  few-shot  image  classification.  Popular  few-shot  learning  algorithms  extract  a  transferable  embedding  from  seen  classes  and  reuse  it  on  unseen  classes  by  constructing  a  metric-based  classifier.  One  main  obstacle  to  applying  these  algorithms  in  action  recognition  is  the  complex  structure  of  videos.  Some  existing  solutions  sample  frames  from  a  video  and  aggregate  their  embeddings  to  form  a  video-level  representation,  neglecting  important  temporal  relations.  Others  perform  an  explicit  sequence  matching  between  two  videos  and  define  their  distance  as  matching  cost,  imposing  too  strong  restrictions  on  sequence  ordering.  In  this  paper,  we  propose  Compromised  Metric  via  Optimal  Transport  (CMOT)  to  combine  the  advantages  of  these  two  solutions.  CMOT  simultaneously  considers  semantic  and  temporal  information  in  videos  under  Optimal  Transport  framework,  and  is  discriminative  for  both  content-sensitive  and  ordering-sensitive  tasks.  In  detail,  given  two  videos,  we  sample  segments  from  them  and  cast  the  calculation  of  their  distance  as  an  optimal  transport  problem  between  two  segment  sequences.  To  preserve  the  inherent  temporal  ordering  information,  we  additionally  amend  the  ground  cost  matrix  by  penalizing  it  with  the  positional  distance  between  a  pair  of  segments.  Empirical  results  on  benchmark  datasets  demonstrate  the  superiority  of  CMOT.
0	Action  based  representation  learning  for  autonomous  driving.  Human  drivers  produce  a  vast  amount  of  data  which  could,  in  principle,  be  used  to  improve  autonomous  driving  systems.  Unfortunately,  seemingly  straightforward  approaches  for  creating  end-to-end  driving  models  that  map  sensor  data  directly  into  driving  actions  are  problematic  in  terms  of  interpretability,  and  typically  have  significant  difficulty  dealing  with  spurious  correlations.  Alternatively,  we  propose  to  use  this  kind  of  action-based  driving  data  for  learning  representations.  Our  experiments  show  that  an  affordance-based  driving  model  pre-trained  with  this  approach  can  leverage  a  relatively  small  amount  of  weakly  annotated  imagery  and  outperform  pure  end-to-end  driving  models,  while  being  more  interpretable.  Further,  we  demonstrate  how  this  strategy  outperforms  previous  methods  based  on  learning  inverse  dynamics  models  as  well  as  other  methods  based  on  heavy  human  supervision  (ImageNet).
0	Micronet  towards  image  recognition  with  extremely  low  flops.  In  this  paper,  we  present  MicroNet,  which  is  an  efficient  convolutional  neural  network  using  extremely  low  computational  cost  (e.g.  6  MFLOPs  on  ImageNet  classification).  Such  a  low  cost  network  is  highly  desired  on  edge  devices,  yet  usually  suffers  from  a  significant  performance  degradation.  We  handle  the  extremely  low  FLOPs  based  upon  two  design  principles:  (a)  avoiding  the  reduction  of  network  width  by  lowering  the  node  connectivity,  and  (b)  compensating  for  the  reduction  of  network  depth  by  introducing  more  complex  non-linearity  per  layer.  Firstly,  we  propose  Micro-Factorized  convolution  to  factorize  both  pointwise  and  depthwise  convolutions  into  low  rank  matrices  for  a  good  tradeoff  between  the  number  of  channels  and  input/output  connectivity.  Secondly,  we  propose  a  new  activation  function,  named  Dynamic  Shift-Max,  to  improve  the  non-linearity  via  maxing  out  multiple  dynamic  fusions  between  an  input  feature  map  and  its  circular  channel  shift.  The  fusions  are  dynamic  as  their  parameters  are  adapted  to  the  input.  Building  upon  Micro-Factorized  convolution  and  dynamic  Shift-Max,  a  family  of  MicroNets  achieve  a  significant  performance  gain  over  the  state-of-the-art  in  the  low  FLOP  regime.  For  instance,  MicroNet-M1  achieves  61.1%  top-1  accuracy  on  ImageNet  classification  with  12  MFLOPs,  outperforming  MobileNetV3  by  11.3%.
0	Graph  regularized  nonnegative  tensor  ring  decomposition  for  multiway  representation  learning.  Tensor  ring  (TR)  decomposition  is  a  powerful  tool  for  exploiting  the  low-rank  nature  of  multiway  data  and  has  demonstrated  great  potential  in  a  variety  of  important  applications.  In  this  paper,  nonnegative  tensor  ring  (NTR)  decomposition  and  graph  regularized  NTR  (GNTR)  decomposition  are  proposed,  where  the  former  equips  TR  decomposition  with  local  feature  extraction  by  imposing  nonnegativity  on  the  core  tensors  and  the  latter  is  additionally  able  to  capture  manifold  geometry  information  of  tensor  data,  both  significantly  extend  the  applications  of  TR  decomposition  for  nonnegative  multiway  representation  learning.  Accelerated  proximal  gradient  based  methods  are  derived  for  NTR  and  GNTR.  The  experimental  result  demonstrate  that  the  proposed  algorithms  can  extract  parts-based  basis  with  rich  colors  and  rich  lines  from  tensor  objects  that  provide  more  interpretable  and  meaningful  representation,  and  hence  yield  better  performance  than  the  state-of-the-art  tensor  based  methods  in  clustering  and  classification  tasks.
0	Multimodal  few  shot  learning  with  frozen  language  models.  When  trained  at  sufficient  scale,  auto-regressive  language  models  exhibit  the  notable  ability  to  learn  a  new  language  task  after  being  prompted  with  just  a  few  examples.  Here,  we  present  a  simple,  yet  effective,  approach  for  transferring  this  few-shot  learning  ability  to  a  multimodal  setting  (vision  and  language).  Using  aligned  image  and  caption  data,  we  train  a  vision  encoder  to  represent  each  image  as  a  sequence  of  continuous  embeddings,  such  that  a  pre-trained,  frozen  language  model  prompted  with  this  prefix  generates  the  appropriate  caption.  The  resulting  system  is  a  multimodal  few-shot  learner,  with  the  surprising  ability  to  learn  a  variety  of  new  tasks  when  conditioned  on  examples,  represented  as  a  sequence  of  multiple  interleaved  image  and  text  embeddings.  We  demonstrate  that  it  can  rapidly  learn  words  for  new  objects  and  novel  visual  categories,  do  visual  question-answering  with  only  a  handful  of  examples,  and  make  use  of  outside  knowledge,  by  measuring  a  single  model  on  a  variety  of  established  and  new  benchmarks.
0	Tinyaction  challenge  recognizing  real  world  low  resolution  activities  in  videos.  This  paper  summarizes  the  TinyAction  challenge  which  was  organized  in  ActivityNet  workshop  at  CVPR  2021.  This  challenge  focuses  on  recognizing  real-world  low-resolution  activities  present  in  videos.  Action  recognition  task  is  currently  focused  around  classifying  the  actions  from  high-quality  videos  where  the  actors  and  the  action  is  clearly  visible.  While  various  approaches  have  been  shown  effective  for  recognition  task  in  recent  works,  they  often  do  not  deal  with  videos  of  lower  resolution  where  the  action  is  happening  in  a  tiny  region.  However,  many  real  world  security  videos  often  have  the  actual  action  captured  in  a  small  resolution,  making  action  recognition  in  a  tiny  region  a  challenging  task.  In  this  work,  we  propose  a  benchmark  dataset,  TinyVIRAT-v2,  which  is  comprised  of  naturally  occuring  low-resolution  actions.  This  is  an  extension  of  the  TinyVIRAT  dataset  and  consists  of  actions  with  multiple  labels.  The  videos  are  extracted  from  security  videos  which  makes  them  realistic  and  more  challenging.  We  use  current  state-of-the-art  action  recognition  methods  on  the  dataset  as  a  benchmark,  and  propose  the  TinyAction  Challenge.
0	Multiresolution  knowledge  distillation  for  anomaly  detection.  Unsupervised  representation  learning  has  proved  to  be  a  critical  component  of  anomaly  detection/localization  in  images.  The  challenges  to  learn  such  a  representation  are  two-fold.  Firstly,  the  sample  size  is  not  often  large  enough  to  learn  a  rich  generalizable  representation  through  conventional  techniques.  Secondly,  while  only  normal  samples  are  available  at  training,  the  learned  features  should  be  discriminative  of  normal  and  anomalous  samples.  Here,  we  propose  to  use  the  "distillation"  of  features  at  various  layers  of  an  expert  network,  pre-trained  on  ImageNet,  into  a  simpler  cloner  network  to  tackle  both  issues.  We  detect  and  localize  anomalies  using  the  discrepancy  between  the  expert  and  cloner  networks'  intermediate  activation  values  given  the  input  data.  We  show  that  considering  multiple  intermediate  hints  in  distillation  leads  to  better  exploiting  the  expert's  knowledge  and  more  distinctive  discrepancy  compared  to  solely  utilizing  the  last  layer  activation  values.  Notably,  previous  methods  either  fail  in  precise  anomaly  localization  or  need  expensive  region-based  training.  In  contrast,  with  no  need  for  any  special  or  intensive  training  procedure,  we  incorporate  interpretability  algorithms  in  our  novel  framework  for  the  localization  of  anomalous  regions.  Despite  the  striking  contrast  between  some  test  datasets  and  ImageNet,  we  achieve  competitive  or  significantly  superior  results  compared  to  the  SOTA  methods  on  MNIST,  F-MNIST,  CIFAR-10,  MVTecAD,  Retinal-OCT,  and  two  Medical  datasets  on  both  anomaly  detection  and  localization.
0	Hydra  pruning  adversarially  robust  neural  networks.  In  safety-critical  but  computationally  resource-constrained  applications,  deep  learning  faces  two  key  challenges:  lack  of  robustness  against  adversarial  attacks  and  large  neural  network  size  (often  millions  of  parameters).  While  the  research  community  has  extensively  explored  the  use  of  robust  training  and  network  pruning  independently  to  address  one  of  these  challenges,  only  a  few  recent  works  have  studied  them  jointly.  However,  these  works  inherit  a  heuristic  pruning  strategy  that  was  developed  for  benign  training,  which  performs  poorly  when  integrated  with  robust  training  techniques,  including  adversarial  training  and  verifiable  robust  training.  To  overcome  this  challenge,  we  propose  to  make  pruning  techniques  aware  of  the  robust  training  objective  and  let  the  training  objective  guide  the  search  for  which  connections  to  prune.  We  realize  this  insight  by  formulating  the  pruning  objective  as  an  empirical  risk  minimization  problem  which  is  solved  efficiently  using  SGD.  We  demonstrate  that  our  approach,  titled  HYDRA,  achieves  compressed  networks  with  state-of-the-art  benign  and  robust  accuracy,  simultaneously.  We  demonstrate  the  success  of  our  approach  across  CIFAR-10,  SVHN,  and  ImageNet  dataset  with  four  robust  training  techniques:  iterative  adversarial  training,  randomized  smoothing,  MixTrain,  and  CROWN-IBP.  We  also  demonstrate  the  existence  of  highly  robust  sub-networks  within  non-robust  networks.  Our  code  and  compressed  networks  are  publicly  available  at  \url{this  https  URL}.
0	Fusionlane  multi  sensor  fusion  for  lane  marking  semantic  segmentation  using  deep  neural  networks.  It  is  a  crucial  step  to  achieve  effective  semantic  segmentation  of  lane  marking  during  the  construction  of  the  lane  level  high-precision  map.  In  recent  years,  many  image  semantic  segmentation  methods  have  been  proposed.  These  methods  mainly  focus  on  the  image  from  camera,  due  to  the  limitation  of  the  sensor  itself,  the  accurate  three-dimensional  spatial  position  of  the  lane  marking  cannot  be  obtained,  so  the  demand  for  the  lane  level  high-precision  map  construction  cannot  be  met.  This  paper  proposes  a  lane  marking  semantic  segmentation  method  based  on  LIDAR  and  camera  fusion  deep  neural  network.  Different  from  other  methods,  in  order  to  obtain  accurate  position  information  of  the  segmentation  results,  the  semantic  segmentation  object  of  this  paper  is  a  bird's  eye  view  converted  from  a  LIDAR  points  cloud  instead  of  an  image  captured  by  a  camera.  This  method  first  uses  the  deeplabv3+  [\ref{ref:1}]  network  to  segment  the  image  captured  by  the  camera,  and  the  segmentation  result  is  merged  with  the  point  clouds  collected  by  the  LIDAR  as  the  input  of  the  proposed  network.  In  this  neural  network,  we  also  add  a  long  short-term  memory  (LSTM)  structure  to  assist  the  network  for  semantic  segmentation  of  lane  markings  by  using  the  the  time  series  information.  The  experiments  on  more  than  14,000  image  datasets  which  we  have  manually  labeled  and  expanded  have  shown  the  proposed  method  has  better  performance  on  the  semantic  segmentation  of  the  points  cloud  bird's  eye  view.  Therefore,  the  automation  of  high-precision  map  construction  can  be  significantly  improved.  Our  code  is  available  at  this  https  URL.
0	Mlp  mixer  an  all  mlp  architecture  for  vision.  Convolutional  Neural  Networks  (CNNs)  are  the  go-to  model  for  computer  vision.  Recently,  attention-based  networks,  such  as  the  Vision  Transformer,  have  also  become  popular.  In  this  paper  we  show  that  while  convolutions  and  attention  are  both  sufficient  for  good  performance,  neither  of  them  are  necessary.  We  present  MLP-Mixer,  an  architecture  based  exclusively  on  multi-layer  perceptrons  (MLPs).  MLP-Mixer  contains  two  types  of  layers:  one  with  MLPs  applied  independently  to  image  patches  (i.e.  "mixing"  the  per-location  features),  and  one  with  MLPs  applied  across  patches  (i.e.  "mixing"  spatial  information).  When  trained  on  large  datasets,  or  with  modern  regularization  schemes,  MLP-Mixer  attains  competitive  scores  on  image  classification  benchmarks,  with  pre-training  and  inference  cost  comparable  to  state-of-the-art  models.  We  hope  that  these  results  spark  further  research  beyond  the  realms  of  well  established  CNNs  and  Transformers.
0	An  interaction  oriented  agent  framework  for  open  environments.  The  aim  of  the  work  is  to  develop  formal  models  of  interaction  and  of  the  related  support  infrastructures,  that  overcome  the  limits  of  the  current  approaches.  We  propose  to  represent  explicitly  not  only  the  agents  but  also  the  computational  environment  in  terms  of  rules,  conventions,  resources,  tools,  and  services,  that  are  functional  to  the  coordination  and  cooperation  of  the  agents.  These  models  will  enable  the  verification  of  the  interaction  in  the  MAS,  thanks  to  the  introduction  of  a  novel  social  semantics  of  interaction  based  on  commitments  and  on  an  explicit  account  of  the  regulative  rules.
0	Road  detection  algorithm  of  integrating  region  and  edge  information.  In  the  unstructured  environment-the  road  of  cable  tunnel,  image  processing  is  used  to  detect  and  extract  the  road  information,  which  is  used  for  the  autonomous  navigation  system  of  the  cable  tunnel  robot.  Firstly,  the  adaptive  median  filter  is  used  to  smooth  the  road  image  acquisition.  Then  apply  the  Otsu  method  in  the  R  component  image  of  RGB  color  space  to  achieve  the  road  area  segmentation.  According  to  the  characteristic  of  the  segmentation  image,  the  mathematical  morphology  is  used  for  further  processing.  For  road  edge  detection  algorithm,  we  choose  Canny  algorithm  of  accurate  positioning  and  strong  anti-interference  ability.  In  order  to  achieve  self-adaptive  algorithm,  the  Otsu  algorithm  is  applied  to  the  threshold  selection  in  Canny  operator.  Combining  with  the  advantages  of  methods  above,  a  new  method  of  road  boundary  detection  is  proposed,  which  is  based  on  the  fusion  of  regional  and  edge  information.  The  experimental  results  show  that  the  algorithm  can  get  smooth  and  accurate  road  edge.
0	Performance  analysis  of  a  nb  iot  based  smart  lamp  solution  with  application  enabled  platform.  The  NB-IoT  (Narrow  Band  Internet  of  Things)  network  is  proved  be  suitable  for  connecting  everything  with  low  energy  consumption,  high  concurrency  at  a  low  cost.  In  this  paper,  we  proposed  a  NB-IoT  based  smart  lamp  scheme  to  solve  the  problems  of  traditional  street  lamps,  such  as  large  electricity  consumption,  difficult  maintenance  and  inconvenient  management.  Random  access,  staggered  peak  communication  and  reasonable  heartbeat  mechanism  are  proposed  to  improve  the  soft  capacity  of  the  system.  The  commercial  project  shows  solution  we  proposed  can  effectively  manage  every  single  lamp.  The  online  rate  and  the  communication  success  rate  are  all  up  to  expectations.
0	Ways  to  increase  the  efficiency  of  information  systems.  Fractions  of  seconds  can  change  a  person's  life  or  a  company's  destiny  as  we  live  in  a  dynamic  world,  in  a  permanent  move.  Thus,  the  need  of  being  informed,  regardless  of  place  or  time,  is  very  great.  Therefore,  the  need  for  efficient  information  systems  is  very  high.  The  efficiency  can  be  achieved  by  using  various  methods/  techniques  and  technologies  "to  build"  the  information  system.  The  ones,  we  will  present  in  our  paper,  are:  methods  of  Enterprise  Application  Integration  (EAI):  the  purpose  of  EAI  strategies  is  to  create  an  integrated,  complex,  coherent  software  solution,  beyond  any  geographical,  social,  national  or  heterogeneous  business  types  limits;  techniques  of  databases  optimization:  in  order  to  improve  the  performance  of  database  processing  can  be  applied  several  optimization  techniques;  and  "in  terms"  of  technology,  we  will  consider:  mobile  databases,  spatial  databases  and  data  warehouses  (using  Oracle  Warehouse  Builder).
0	Using  some  data  mining  techniques  for  early  diagnosis  of  lung  cancer.  Lung  cancer  is  a  disease  of  uncontrolled  cell  growth  in  tissues  of  the  lung,  Lung  cancer  is  one  of  the  most  common  and  deadly  diseases  in  the  world.  Detection  of  lung  cancer  in  its  early  stage  is  the  key  of  its  cure.  In  general,  a  measure  for  early  stage  lung  cancer  diagnosis  mainly  includes  those  utilizing  X-ray  chest  films,  CT,  MRI,  etc.  Medical  images  mining  is  a  promising  area  of  computational  intelligence  applied  to  automatically  analyzing  patient's  records  aiming  at  the  discovery  of  new  knowledge  potentially  useful  for  medical  decision  making.  Firstly  we  will  use  some  processes  are  essential  to  the  task  of  medical  image  mining,  Data  Preprocessing,  Feature  Extraction  and  Rule  Generation.  The  methods  used  in  this  paper  work  states,  to  classify  the  digital  X-ray  chest  films  into  two  categories:  normal  and  abnormal.  The  normal  state  is  the  one  that  characterize  a  healthy  patient.  The  abnormal  state  including  the  types  of  lung  cancer;  will  be  used  as  a  common  classification  method  indicating  a  machine  learning  method  known  as  neural  networks.  In  addition,  we  will  investigate  the  use  of  association  rules  in  the  problem  of  x-ray  chest  films  categorization.    The  digital  x-ray  chest  films  are  storied  in  huge  multimedia  databases  for  a  medical  purpose.  This  multimedia  database  provides  a  great  environment  to  apply  some  image  recognition  methods  to  extract  the  useful  knowledge  and  then  rules  from  the  mentioned  database.  These  rules  that  we  could  got  using  image  recognition  methods,  will  help  the  doctors  to  decide  important  decisions  on  a  particular  patient  state.
0	Feature  selection  for  multi  label  learning.  Feature  Selection  plays  an  important  role  in  machine  learning  and  data  mining,  and  it  is  often  applied  as  a  data  pre-processing  step.  This  task  can  speed  up  learning  algorithms  and  sometimes  improve  their  performance.  In  multi-label  learning,  label  dependence  is  considered  another  aspect  that  can  contribute  to  improve  learning  performance.  A  replicable  and  wide  systematic  review  performed  by  us  corroborates  this  idea.  Based  on  this  information,  it  is  believed  that  considering  label  dependence  during  feature  selection  can  lead  to  better  learning  performance.  The  hypothesis  of  this  work  is  that  multi-label  feature  selection  algorithms  that  consider  label  dependence  will  perform  better  than  the  ones  that  disregard  it.  To  this  end,  we  propose  multi-label  feature  selection  algorithms  that  take  into  account  label  relations.  These  algorithms  were  experimentally  compared  to  the  standard  approach  for  feature  selection,  showing  good  performance  in  terms  of  feature  reduction  and  predictability  of  the  classifiers  built  using  the  selected  features.
0	Learning  term  embeddings  for  hypernymy  identification.  Hypernymy  identification  aims  at  detecting  if  is  A  relationship  holds  between  two  words  or  phrases.  Most  previous  methods  are  based  on  lexical  patterns  or  the  Distributional  Inclusion  Hypothesis,  and  the  accuracy  of  such  methods  is  not  ideal.  In  this  paper,  we  propose  a  simple  yet  effective  supervision  framework  to  identify  hypernymy  relations  using  distributed  term  representations  (a.k.a  term  embeddings).  First,  we  design  a  distance-margin  neural  network  to  learn  term  embeddings  based  on  some  pre-extracted  hypernymy  data.  Then,  we  apply  such  embeddings  as  term  features  to  identify  positive  hypernymy  pairs  through  a  supervision  method.  Experimental  results  demonstrate  that  our  approach  outperforms  other  supervised  methods  on  two  popular  datasets  and  the  learned  term  embeddings  has  better  quality  than  existing  term  distributed  representations  with  respect  to  hypernymy  identification.
0	Fabric  identification  using  convolutional  neural  network.  Image-based  fabric  retrieval  technique  can  help  to  develop  new  fabrics  and  manage  products.  Efficiently  extracting  features  from  fabric  images  is  the  key  to  enhance  the  practicality  of  this  technology.  In  this  paper,  convolutional  neural  network  is  trained  with  a  dataset  of  19,894  different  yarn-dyed  fabric  patterns.  Center  loss  architecture  is  added  to  further  improve  the  discriminative  power  of  the  network.  By  properly  sampling  from  original  images,  the  network  model  can  efficiently  extract  discriminative  features  and  achieve  a  retrieval  accuracy  of  99.89%  on  our  test  set.  This  performance  maintains  well  when  simpler  deep  architecture  is  used,  but  decreases  quickly  if  the  contents  of  fed  fabric  image  are  reduced.
0	Iot  instrumented  food  and  grain  warehouse  traceability  system  for  farmers.  In  a  developing  country  like  India,  agriculture  is  one  of  the  main  sectors  in  terms  of  income.  Good  food  storage  plays  a  very  important  role  when  it  comes  to  food  security  that  is  affected  by  both  food  loss  and  food  wastage.  The  losses  can  be  reduced,  which  will  automatically  increase  the  amount  of  food  availability.  In  this  article,  we  proposed  an  IoT  enabled  monitoring  system  to  deploy  in  remote  areas  where  the  accessibility  is  very  minimum  for  farmers  with  good  storage  facilities  to  reduce  food  losses  and  increase  food  safety.  This  proposed  framework  monitors  warehouse  parameters  such  as  temperature,  humidity,  CO,  motion,  vibration,  and  smoke  which  is  highly  affected  to  grains.  The  ESP32  WiFi  module  collects  the  data  from  the  sensors  and  this  module  sends  data  to  Node-red  dashboard  through  MQTT  broker.  Multiple  IoT  nodes  installed  at  a  different  location  inside  the  warehouse  and  which  will  give  information  about  the  warehouse  environment  to  the  farmers  through  Mobile  SMS  and  E-mail  notification.
0	Human  resource  assessment  in  software  development  projects  using  fuzzy  linguistic  2  tuples.  Proper  selection  and  allocation  of  human  resources  to  software  development  tasks  is  one  of  the  key  challenges  in  software  development  projects.  In  this  paper  we  present  a  fuzzy  linguistic  approach  that  supports  the  selection  of  suitable  human  resources  based  on  their  skills  and  the  required  skills  for  each  project  task.  The  proposed  approach  uses  2-tuple  fuzzy  linguistic  terms  and  results  in  an  objective  aggregation  of  the  ratings  of  required  task  related  skills  and  provided  skills  from  candidate  human  resources.  The  approach  applies  a  group-based,  multi-criteria,  similarity  degree-based  aggregation  algorithm.  To  reflect  the  contribution  of  one  skill  to  the  learning  of  other  skills,  the  approach  also  considers  possible  relationships  between  skills.  A  numerical  example  is  presented  as  a  proof  of  concept  to  demonstrate  the  applicability  of  the  approach.
0	Multilateral  negotiation  in  boolean  games  with  incomplete  information  using  generalized  possibilistic  logic.  Boolean  games  are  a  game-theoretic  framework  in  which  propositional  logic  is  used  to  describe  agents'  goals.  In  this  paper  we  investigate  how  agents  in  Boolean  games  can  reach  an  efficient  and  fair  outcome  through  a  simple  negotiation  protocol.  We  are  particularly  interested  in  settings  where  agents  only  have  incomplete  knowledge  about  the  preferences  of  others.  After  explaining  how  generalized  possibilistic  logic  can  be  used  to  compactly  encode  such  knowledge,  we  analyze  how  a  lack  of  knowledge  affects  the  agreement  outcome.  In  particular,  we  show  how  knowledgeable  agents  can  obtain  a  more  desirable  outcome  than  others.
0	Web  page  classification  based  on  uncorrelated  semi  supervised  intra  view  and  inter  view  manifold  discriminant  feature  extraction.  Web  page  classification  has  attracted  increasing  research  interest.  It  is  intrinsically  a  multi-view  and  semi-supervised  application,  since  web  pages  usually  contain  two  or  more  types  of  data,  such  as  text,  hyperlinks  and  images,  and  unlabeled  pages  are  generally  much  more  than  labeled  ones.  Web  page  data  is  commonly  high-dimensional.  Thus,  how  to  extract  useful  features  from  this  kind  of  data  in  the  multi-view  semi-supervised  scenario  is  important  for  web  page  classification.  To  our  knowledge,  only  one  method  is  specially  presented  for  this  topic.  And  with  respect  to  a  few  semi-supervised  multi-view  feature  extraction  methods  on  other  applications,  there  still  exists  much  room  for  improvement.  In  this  paper,  we  firstly  design  a  feature  extraction  schema  called  semi-supervised  intra-view  and  inter-view  manifold  discriminant  (SI2MD)  learning,  which  sufficiently  utilizes  the  intra-view  and  inter-view  discriminant  information  of  labeled  samples  and  the  local  neighborhood  structures  of  unlabeled  samples.  We  then  design  a  semi-supervised  uncorrelation  constraint  for  the  SI2MD  schema  to  remove  the  multi-view  correlation  in  the  semi-supervised  scenario.  By  combining  the  SI2MD  schema  with  the  constraint,  we  propose  an  uncorrelated  semi-supervised  intra-view  and  inter-view  manifold  discriminant  (USI2MD)  learning  approach  for  web  page  classification.  Experiments  on  public  web  page  databases  validate  the  proposed  approach.
0	Neuropilot  a  cross  platform  framework  for  edge  ai.  Artificial  intelligence  (AI)  has  been  applied  from  cloud  servers  to  edge  devices  because  of  its  rapid  response,  privacy,  robustness,  and  the  efficient  use  of  network  bandwidth.  However,  it  is  challengeable  to  deploy  the  computation  and  memory-bandwidth  intensive  AI  to  edge  devices  for  the  power  and  hardware  resource  are  limited.  The  various  needs  of  applications,  diverse  devices  and  the  fragmented  supporting  tools  make  the  integration  a  tough  work.  In  this  paper,  the  NeuroPilot,  a  cross-platform  framework  for  edge  AI,  is  introduced.  Technologies  on  software,  hardware  and  integration  levels  are  proposed  to  achieve  the  high  performance  and  preserve  the  flexibility  meanwhile.  The  NeuroPilot  solution  provides  the  superior  edge  AI  ability  for  a  wide  range  of  applications.
0	Bridging  structure  and  feature  representations  in  graph  matching.  Structures  and  features  are  opposite  approaches  in  building  representations  for  object  recognition.  Bridging  the  two  is  an  essential  problem  in  pattern  recognition  as  the  two  opposite  types  of  information  are  fundamentally  different.  As  dissimilarities  can  be  computed  for  both  the  dissimilarity  representation  can  be  used  to  combine  the  two.  Attributed  graphs  contain  structural  as  well  as  feature-based  information.  Neglecting  the  attributes  yields  a  pure  structural  description.  Isolating  the  features  and  neglecting  the  structure  represents  objects  by  a  bag  of  features.  In  this  paper  we  will  show  that  weighted  combinations  of  dissimilarities  may  perform  better  than  these  two  extremes,  indicating  that  these  two  types  of  information  are  essentially  different  and  strengthen  each  other.  In  addition  we  present  two  more  advanced  integrations  than  weighted  combining  and  show  that  these  may  improve  the  classification  performances  even  further.
0	Prediction  of  cleaning  loss  of  combine  harvester  based  on  neural  network.  This  paper  explores  the  performance  and  obtains  a  reasonable  cleaning  effect  of  the  cleaning  system  of  combine  harvester  and  studies  the  relationship  between  the  cleaning  effect  of  the  combine  harv...
0	I  inclof  improved  incremental  local  outlier  detection  for  data  streams.  Data  streams  outlier  mining  is  an  important  and  active  research  issue  in  anomaly  detection.  Most  of  existing  methods  are  more  suitable  for  static  data,  since  algorithms  have  all  data  available  at  time  of  detection.  But,  as  data  streams  evolve  during  the  time,  traditional  methods  cannot  perform  well  on  them.  Therefore,  because  of  dynamic  nature  of  data  streams,  evaluating  objects  as  outlier  when  they  arrive,  although  meaningful,  often  can  lead  us  to  a  wrong  decision.  In  this  paper  an  Improved  Incremental  LOF  algorithm  is  proposed.  The  proposed  algorithm  considers  a  sliding  window  that  lets  data  profiles  update  during  the  window  and  then  declares  them  as  outlier/inlier,  therefore  it  can  significantly  distinct  outliers  from  new  data  behavior.  In  addition,  I-incLOF  declares  that  there  is  no  need  for  rerunning  deletion  algorithm  when  an  outliers  is  founded,  we  just  do  not  consider  them  in  the  new  points  neighbors.  Our  experimental  results  show  that  the  proposed  improved  incLOF  algorithm  was  successful  in  reducing  false-positive  rate  with  no  additional  computational  cost.
0	Optimization  of  wind  turbine  rotor  diameters  and  hub  heights  in  a  windfarm  using  differential  evolution  algorithm.  In  this  paper  some  of  the  optimized  wind  turbine  layouts  in  a  windfarm,  as  presented  by  many  authors,  have  been  chosen  as  basis  for  further  evaluation  and  study.  The  objective  of  most  of  earlier  studies  was  to  minimize  cost  per  kW  of  power  produced.  This  paper  focuses  from  different  perspective  of  optimization  of  turbine  rotor  diameters  and  hub  heights  to  improve  overall  windfarm  efficiency  at  a  reduced  cost.  Differential  Evolution  (DE)  algorithm  is  employed  to  optimize  rotor  diameters  and  hub  heights  of  turbines  in  a  windfarm.
0	Liver  disease  diagnosis  using  quantum  based  binary  neural  network  learning  algorithm.  In  this  paper,  a  liver  disease  diagnosis  is  carried  out  using  quantum-based  binary  neural  network  learning  algorithm  (QBNN-L).  The  proposed  method  constructively  form  the  neural  network  architecture,  and  weights  are  decided  by  quantum  computing  concept.  The  use  of  quantum  computing  improves  performance  in  terms  of  number  of  neurons  at  hidden  layer  and  classification  accuracy  and  precision.  Same  is  compared  with  various  classification  algorithms  such  as  logistic,  linear  logistic  regression,  multilayer  perceptron,  support  vector  machine  (SVM).  Results  are  showing  improvement  in  terms  of  generalization  accuracy  and  precision.
0	Novel  hand  biometric  system  using  invariant  descriptors.  Biometrics-based  hand  authentication  is  among  the  most  popular  biometrics  used  to  automatically  characterize  a  person  especially  in  forensic  applications.  Hand  recognition  systems  are  able  to  confirm  or  deny  the  identity  of  a  claimed  person  because  they  do  not  cause  anxiety  for  the  users.  However,  different  individuals  may  have  almost  similar  hands.  Therefore,  the  performance  of  the  hand  verification  process  depends  highly  on  the  hand  descriptors.  In  this  paper,  we  propose  a  new  approach  for  personal  verification  based  on  Scale  Invariant  Feature  Transform  (SIFT).  This  transform  proved  its  high  distinction  and  efficiency  in  many  applications  especially  in  object  recognition  and  video  tracking.  Two  public  databases  have  been  used  to  evaluate  performances.  Experimental  results  show  promising  recognition  rates  by  achieving  94%  for  IITD  hand  database  and  98%  for  Bosphorus  hand  database.
0	Recent  research  advances  in  black  and  white  visual  cryptography  schemes.  Visual  Cryptography  (VC)  is  a  type  of  image  secret  sharing  scheme  which  decrypts  an  original  secret  image  with  Human  Visual  System  (HVS).  In  this,  the  original  image  can  be  alienated  into  n  shadows  or  shares  and  allocated  to  n  participants;  stacking  any  k  shares  reveals  the  secret  image  which  ensures  the  security  measures.  In  this  paper,  we  examined  the  recent  research  advances  in  black  and  white  VCSs.  We  reviewed  the  existing  techniques  and  a  comparative  study  of  VC  for  binary  images  is  presented.  The  study  is  performed  with  respect  to  different  parameters  and  draws  the  current  barriers  related  to  the  visual  cryptography  schemes.
0	An  orthogonal  symbiotic  organisms  search  algorithm  to  determine  approximate  solution  of  systems  of  ordinary  differential  equations.  Determining  exact  solution  of  systems  of  ordinary  differential  equations  (ODEs)  is  a  challenging  task  in  many  real-life  problems  of  science  and  engineering.  In  this  paper,  an  attempt  is  made  to  determine  approximate  solutions  for  such  complicated  ODEs.  The  Fourier  series  expansion  is  used  as  an  approximator.  The  coefficients  of  Fourier  series  expansion  are  determined  by  nature-inspired  algorithms.  The  Symbiotic  Organism  Search  (SOS)  is  an  evolutionary  algorithm  proposed  by  Cheng  and  Prayogo  in  2014.  It  is  inspired  by  natural  phenomenon  of  organisms  interaction  in  an  ecosystem  for  their  survival.  Recently,  Panda  and  Pani  in  2017  reported  an  Orthogonal  SOS  (OSOS)  algorithm  by  incorporating  orthogonal  array  strategies  in  SOS,  which  enhances  the  exploration  capability  of  original  algorithm.  Here,  the  OSOS  algorithm  is  used  to  compute  the  coefficients  of  Fourier  series.  Simulation  studies  on  two  real-life  examples  using  systems  of  ODEs  reported  superior  performance  of  the  proposed  OSOS  learning  over  the  same  model  trained  by  three  recently  reported  nature-inspired  algorithms  OCBO,  OPSO,  and  WCA  in  terms  of  close  response  matching  and  minimal  generalized  distance  achieved.
0	A  new  rough  fuzzy  clustering  algorithm  and  its  applications.  Cluster  analysis  is  a  technique  that  divides  a  given  data  set  into  a  set  of  clusters  in  such  a  way  that  two  objects  from  the  same  cluster  are  as  similar  as  possible  and  the  objects  from  different  clusters  are  as  dissimilar  as  possible.  A  robust  rough-fuzzy  \(c\)-means  clustering  algorithm  is  applied  here  to  identify  clusters  having  similar  objects.  Each  cluster  of  the  robust  rough-fuzzy  clustering  algorithm  is  represented  by  a  set  of  three  parameters,  namely,  cluster  prototype,  a  possibilistic  fuzzy  lower  approximation,  and  a  probabilistic  fuzzy  boundary.  The  possibilistic  lower  approximation  helps  in  discovering  clusters  of  various  shapes.  The  cluster  prototype  depends  on  the  weighting  average  of  the  possibilistic  lower  approximation  and  probabilistic  boundary.  The  reported  algorithm  is  robust  in  the  sense  that  it  can  find  overlapping  and  vaguely  defined  clusters  with  arbitrary  shapes  in  noisy  environment.  The  effectiveness  of  the  clustering  algorithm,  along  with  a  comparison  with  other  clustering  algorithms,  is  demonstrated  on  synthetic  as  well  as  coding  and  non-coding  RNA  expression  data  sets  using  some  cluster  validity  indices.
0	Adaptative  shock  filter  for  image  characters  enhancement  and  denoising.  The  paper  proposes  an  adaptive  shock  filter  to  restore  noisy  blurred  image  characters.  This  filter  introduces  an  fuzzy  decision  mechanism  to  sharpen  image  features  like  edges  and  singularities  while  an  anisotropic  diffusion  process  is  used  to  remove  noise.  A  useful  application  of  the  proposed  filter  is  the  improvement  of  image  segmentation  and  binarization  task.  Its  efficiency  on  degraded  document  images  is  appropriately  proven  via  a  well-established  experimental  study  based  on  a  comparison  with  the  state-of-the-art  methods.
0	Analysis  of  fractional  order  deterministic  hiv  aids  model  during  drug  therapy  treatment.  In  this  study,  we  discussed  the  Caputo  sense  fractional-order  HIV/AIDS  model  including  the  drug  therapy,  and  mathematically  examined  the  dynamic  behaviour  of  the  model.  We  have  discussed  qualitative  analysis  of  the  proposed  mathematical  model  and  defined  the  existence  and  uniqueness  conditions.  Local  stability  is  also  checked  for  HIV-free  equilibrium  point.  We  have  given  some  facts  about  the  growth  rate  of  HIV/AIDS,  the  source  of  HIV  virus,  as  well  as  death  rate  of  CD4+  T  cells,  which  play  a  vital  role  in  HIV  dynamics.  The  numerical  simulations  are  demonstrated  to  reveal  the  analytical  results.
0	New  contributions  into  the  dezert  smarandache  theory  application  to  remote  sensing  image  classification.  The  theory  of  Dezert  and  Smarandache  (DSmT)  called  also  the  theory  of  plausible  and  paradoxical  reasoning  represents  a  suitable  formalism  to  fuse  paradoxical  sources.  The  application  of  the  DSmT  needs  at  first  the  generation  of  the  hyper-powerset  and  more  exactly  the  composites  events.  Thus,  we  present  in  this  paper  a  new  approach  to  construct  pertinent  paradoxical  classes  based  on  gray  levels  histograms,  which  allows  also  the  reduction  of  the  cardinality  of  the  hyper-powerset.  Then,  we  developed  a  new  technique  for  ordering  and  coding  generalized  focal  elements.  This  technique  is  exploited,  in  particular,  to  calculate  the  cardinality  of  Dezert  and  Smarandache.  Finally,  we  applied  this  new  methods  to  process  the  classification  of  a  hight  resolution  images  of  a  forest  area.
0	Robust  adaptive  backstepping  design  for  course  keeping  control  of  ship  with  parameter  uncertainty  and  input  saturation.  In  this  paper,  a  novel  robust  adaptive  control  algorithm  is  proposed  for  a  class  of  ship  course  autopilot  with  parameter  uncertainty  and  input  saturation.  With  the  help  of  Lyapunov  stability  theory  and  adaptive  backstepping  technique,  one  controller  is  constructed  by  considering  parameter  uncertainty  and  actuator  saturation  constraints,  and  the  stability  analysis  subject  to  the  effect  of  input  saturation  constrains  is  conducted  by  employing  an  auxiliary  design  system.  It  is  also  proved  that  the  proposed  algorithm  could  guarantee  the  closed-loop  system  to  be  uniformly  ultimately  bounded  and  the  output  converges  to  a  small  neighborhood  of  zero.  Simulation  results  are  given  to  illustrate  the  effectiveness  and  the  performance  of  the  proposed  scheme.
0	Improved  grey  wolf  optimizer  based  on  opposition  based  learning.  Swarm  intelligence  (SI)-based  algorithms  are  very  popular  optimization  techniques  to  deal  with  complex  and  nonlinear  optimization  problems.  Grey  wolf  optimizer  (GWO)  is  one  of  the  newest  and  efficient  algorithms  based  on  hunting  activity  and  leadership  hierarchy  of  grey  wolves.  To  avoid  the  slow  convergence  and  stagnation  problem  in  local  optima,  in  this  paper,  opposition-based  learning  (OBL)  is  incorporated  in  GWO  for  the  population  initialization  as  well  as  for  the  iteration  jumping.  In  this  strategy,  opposite  numbers  have  been  used  to  deal  with  the  problem  of  slow  convergence.  The  proposed  algorithm  is  named  as  opposition-based  explored  grey  wolf  optimizer  (OBE-GWO).  To  evaluate  the  performance  of  OBE-GWO,  it  is  tested  on  some  well-known  benchmark  problems.  The  experimental  analysis  concludes  the  better  performance  of  OBE-GWO.
0	A  generalized  s  transform  and  tt  transform  based  on  energy  normalized  window.  A  generalized  S  transform  based  on  energy  normalized  windows  is  presented,  which  avoids  the  weighting  effect  of  existing  S  transform  influenced  by  the  time-frequency  spectral  energy.  The  transform  can  reflect  the  spectral  energy  distribution  of  different  frequency  components  while  maintaining  the  excellent  time-frequency  resolution.  While  TT  transform  is  a  new  approach  to  process  non-stationary  signal  based  on  S  transform,  which  has  differential  concentration  for  different  frequency  components,  and  lower  frequency  components  can  be  suppressed  through  extracting  the  TT  spectrum’s  diagonal  elements.  However,  this  method  has  a  deficiency  that  part  of  high  frequency  interference  is  retained  during  processing  low  frequency  signal.  Therefore,  time-frequency  filtering  based  on  the  generalizing  S  transform  is  used  to  suppress  the  high  frequency  interference  and  TT  transform  is  adopted  to  suppress  the  low  frequency  contents.  Finally,  this  method  is  used  to  process  seismic  data,  the  result  demonstrates  the  effective  of  combining  energy  normalized  S  transform  and  TT  transform.
0	A  cluster  number  adaptive  fuzzy  c  means  algorithm  for  image  segmentation.  Aiming  at  partitioning  an  image  into  homogeneous  and  meaningful  regions,  automatic  image  segmentation  is  a  fundamental  but  challenging  problem  in  computer  vision.  It  is  well  known  that  Fuzzy  c-means  (FCM)  algorithm  is  one  of  the  most  popular  methods  for  image  segmentation.  However,  the  FCM-based  image  segmentation  algorithm  must  be  manually  estimated  to  determine  cluster  number  by  users.  In  this  paper,  we  propose  a  novel  cluster  number  adaptive  fuzzy  c-means  image  segmentation  algorithm  (CNAFCM)  for  automatically  grouping  the  pixels  of  an  image  into  different  homogeneous  regions  when  the  cluster  number  is  not  known  beforehand.  We  utilize  the  Grey  Level  Co-occurrence  Matrix  (GLCM)  feature  extracted  at  the  image  block  level  instead  of  at  the  pixel  level  to  estimate  the  cluster  number,  which  is  used  as  initialization  parameter  of  the  following  FCM  clustering  to  endow  the  novel  segmentation  algorithm  adaptively.  We  cluster  image  pixels  according  to  their  corresponding  Gabor  feature  vectors  to  improve  the  compactness  of  the  clusters  and  form  final  homogeneous  regions.  Experimental  results  show  that  proposed  CNAFCM  algorithm  not  only  can  spontaneously  estimate  the  appropriate  number  of  clusters  but  also  can  get  better  segmentation  quality,  in  compare  with  those  FCM-based  segmentation  methods  recently  proposed  in  the  literature.
0	Measures  of  semantic  similarity  of  nodes  in  a  social  network.  Assessing  the  similarity  between  node  profiles  in  a  social  network  is  an  important  tool  in  its  analysis.  Several  approaches  exist  to  study  profile  similarity,  including  semantic  approaches  and  natural  language  processing.  However,  to  date  there  is  no  research  combining  these  aspects  into  a  unified  measure  of  profile  similarity.  Traditionally,  semantic  similarity  is  assessed  using  keywords,  that  is,  formatted  text  information,  with  no  natural  language  processing  component.  This  study  proposes  an  alternative  approach,  whereby  the  similarity  assessment  based  on  keywords  is  applied  to  the  output  of  natural  language  processing  of  profiles.  A  unified  similarity  measure  results  from  this  approach.  The  approach  is  illustrated  on  a  real  data  set  extracted  from  Facebook  and  compared  with  other  similarity  measures  for  the  same  data.
0	Predicting  opponent  moves  for  improving  hearthstone  ai.  Games  pose  many  interesting  questions  for  the  development  of  artificial  intelligence  agents.  Especially  popular  are  methods  that  guide  the  decision-making  process  of  an  autonomous  agent,  which  is  tasked  to  play  a  certain  game.  In  previous  studies,  the  heuristic  search  method  Monte  Carlo  Tree  Search  (MCTS)  was  successfully  applied  to  a  wide  range  of  games.  Results  showed  that  this  method  can  often  reach  playing  capabilities  on  par  with  humans  or  even  better.  However,  the  characteristics  of  collectible  card  games  such  as  the  online  game  Hearthstone  make  it  infeasible  to  apply  MCTS  directly.  Uncertainty  in  the  opponent’s  hand  cards,  the  card  draw,  and  random  card  effects  considerably  restrict  the  simulation  depth  of  MCTS.  We  show  that  knowledge  gathered  from  a  database  of  human  replays  help  to  overcome  this  problem  by  predicting  multiple  card  distributions.  Those  predictions  can  be  used  to  increase  the  simulation  depth  of  MCTS.  For  this  purpose,  we  calculate  bigram-rates  of  frequently  co-occurring  cards  to  predict  multiple  sets  of  hand  cards  for  our  opponent.  Those  predictions  can  be  used  to  create  an  ensemble  of  MCTS  agents,  which  work  under  the  assumption  of  differing  card  distributions  and  perform  simulations  according  to  their  assigned  distribution.  The  proposed  ensemble  approach  outperforms  other  agents  on  the  game  Hearthstone,  including  various  types  of  MCTS.  Our  case  study  shows  that  uncertainty  can  be  handled  effectively  using  predictions  of  sufficient  accuracy,  ultimately,  improving  the  MCTS  guided  decision-making  process.  The  resulting  decision-making  based  on  such  an  MCTS  ensemble  proved  to  be  less  prone  to  errors  by  uncertainty  and  opens  up  a  new  class  of  MCTS  algorithms.
0	Conditioning  in  decomposable  compositional  models  in  valuation  based  systems.  Valuation-based  systems  (VBS)  can  be  considered  as  a  generic  uncertainty  framework  that  has  many  uncertainty  calculi,  such  as  probability  theory,  a  version  of  possibility  theory  where  combination  is  the  product  t-norm,  Spohn's  epistemic  belief  theory,  and  Dempster-  Shafer  belief  function  theory,  as  special  cases.  In  this  paper,  we  focus  our  attention  on  conditioning,  which  is  defined  using  the  combination,  marginalization,  and  removal  operators  of  VBS.  We  show  that  condi-  tioning  can  be  expressed  using  the  composition  operator.  We  define  de-  composable  compositional  models  in  the  VBS  framework.  Finally,  we  show  that  conditioning  in  decomposable  compositional  models  can  be  done  using  local  computation.  Since  all  results  are  obtained  in  the  VBS  framework,  they  hold  in  all  calculi  that  fit  in  the  VBS  framework.
0	On  a  generalization  of  the  modus  ponens  u  conditionality.  In  fuzzy  logic,  the  Modus  Ponens  property  for  fuzzy  implication  functions  is  usually  considered  with  respect  to  a  continuous  t-norm  T  and  for  this  reason  this  property  is  also  known  under  the  name  of  T-conditionality.  In  this  paper,  the  t-norm  T  is  substituted  by  a  uninorm  U  leading  to  the  property  of  U-conditionality.  The  new  property  is  studied  in  detail  and  it  is  shown  that  usual  implications  derived  from  t-norms  and  t-conorms  do  not  satisfy  it,  but  many  solutions  appear  among  those  implications  derived  from  uninorms.  In  particular,  the  case  of  residual  implications  derived  from  uninorms  or  RU-implications  is  investigated  in  detail  for  some  classes  of  uninorms.
0	Coherent  t  conditional  possibility  envelopes  and  nonmonotonic  reasoning.  Envelopes  of  coherent  T-conditional  possibilities  and  coherent  T-conditional  necessities  are  studied  and  an  analysis  of  some  inference  rules  which  play  an  important  role  in  nonmonotonic  reasoning  is  carried  out.
0	Evidential  missing  link  prediction  in  uncertain  social  networks.  Link  prediction  is  the  problem  of  determining  future  or  missing  associations  between  social  entities.  Most  of  the  methods  have  focused  on  social  networks  under  a  certain  framework  neglecting  some  of  the  inherent  properties  of  data  from  real  applications.  These  latter  are  usually  noisy,  missing  or  partially  observed.  Therefore,  uncertainty  is  an  important  feature  to  be  taken  into  account.  In  this  paper,  proposals  for  handling  the  problem  of  missing  link  prediction  while  being  attentive  to  uncertainty  are  presented  along  with  a  technique  for  uncertain  social  networks  generation.  Uncertainty  is  not  only  handled  in  the  graph  model  but  also  in  the  method  itself  using  the  assets  of  the  belief  function  theory  as  a  general  framework  for  reasoning  under  uncertainty.  The  approach  combines  sampling  techniques  and  information  fusion  and  returns  good  results  in  real-life  settings.
0	Allocation  of  service  centers  in  the  gis  with  the  largest  vitality  degree.  In  this  paper  the  questions  of  the  definition  of  the  centers  optimum  allocation  in  the  GIS  are  observed  by  the  minimax  criterion.  It  is  supposed  that  the  information  received  from  GIS  is  presented  like  a  fuzzy  graph.  In  this  case  the  task  of  the  definition  of  the  centers  optimum  allocation  transforms  into  the  task  of  the  definition  of  the  graph  vitality  fuzzy  set.  The  method  of  the  definition  of  the  graph  vitality  fuzzy  set  is  considered.  The  example  of  finding  optimum  allocation  of  centers  in  GIS  for  railway  stations  with  the  largest  vitality  degree  is  considered  as  well.
0	Probabilistic  fuzzy  systems  as  additive  fuzzy  systems.  Probabilistic  fuzzy  systems  combine  a  linguistic  description  of  the  system  behaviour  with  statistical  properties  of  data.  It  was  originally  derived  based  on  Zadeh’s  concept  of  probability  of  a  fuzzy  event.  Two  possible  and  equivalent  additive  reasoning  schemes  were  proposed,  that  lead  to  the  estimation  of  the  output’s  conditional  probability  density.  In  this  work  we  take  a  complementary  approach  and  derive  a  probabilistic  fuzzy  system  from  an  additive  fuzzy  system.  We  show  that  some  fuzzy  systems  with  universal  approximation  capabilities  can  compute  the  same  expected  output  value  as  probabilistic  fuzzy  systems  and  discuss  some  similarities  and  differences  between  them.  A  practical  relevance  of  this  functional  equivalence  result  is  that  learning  algorithms,  optimization  techniques  and  design  issues  can,  under  certain  circumstances,  be  transferred  across  different  paradigms.
0	A  framework  for  security  design  engineering  process.  Adopting  a  security  enhanced  software  development  practices  that  includes  secure  development  process  will  reduce  the  number  of  exploitable  faults  and  weaknesses  in  the  designed  software.  Current  software  development  processes  enforce  security  measures  during  design  phase  which  may  end  up  in  specifying  security  related  architecture  constraints  that  are  not  really  necessary.  To  eliminate  this  problem,  we  propose  a  Framework  for  Security  Design  Engineering  Process  that  involves  converting  security  requirements  and  threats  into  design  decisions  to  mitigate  the  identified  security  threats.  The  identified  design  attributes  are  prioritized  and  a  Security  Design  Template  (SDT)  is  prepared  to  find  out  the  specific  cryptographic  technique  that  would  eventually  help  in  the  later  stages  of  the  design  process  by  eliminating  unnecessary  design  constraints  in  a  particular  scenario.
1	Speeding  up  mutation  testing  via  data  compression  and  state  infection.  Mutation  testing  is  widely  considered  as  a  high-end  test  criterion  due  to  the  vast  number  of  mutants  it  generates.  Although  many  efforts  have  been  made  to  reduce  the  computational  cost  of  mutation  testing,  its  scalability  issue  remains  in  practice.  In  this  paper,  we  introduce  a  novel  method  to  speed  up  mutation  testing  based  on  state  infection  information.  In  addition  to  filtering  out  uninfected  test  executions,  we  further  select  a  subset  of  mutants  and  a  subset  of  test  cases  to  run  leveraging  data-compression  techniques.  In  particular,  we  adopt  Formal  Concept  Analysis  (FCA)  to  group  similar  mutants  together  and  then  select  test  cases  to  cover  these  mutants.  To  evaluate  our  method,  we  conducted  an  experimental  study  on  six  open  source  Java  projects.  We  used  EvoSuite  to  automatically  generate  test  cases  and  to  collect  mutation  data.  The  initial  results  show  that  our  method  can  reduce  the  execution  time  by  83.93%  with  only  0.257%  loss  in  precision.
1	Software  quality  research  from  processes  to  model  based  techniques.  In  this  article  we  state  that  cyber-physical  systems  and  the  Internet  of  Things  pose  challenges  to  software  quality  research.  In  the  emerging  real-time  digital  economy  companies  need  to  gain  deep  customer  insight.  The  state  of  the  art  in  modelbased  systems  and  model-based  testing  allows  software  engineers  for  product-based  and  quantitative  control  of  quality  and  for  increased  productivity.  These  gains  can  be  invested  to  better  understand  the  domain  and  business  conditions.  We  argue  that  cyber-physical  systems  will  pose  an  excellent  basis  for  conducting  collaborative  research  in  model-based  systems  and  model-based  testing  in  the  near  future  and  report  on  lessons  learnt  in  three  areas  of  software  (quality)  research:  (1)  process-oriented  quality,  (2)  model-based  systems  and  (3)  model-based  testing.
1	Analogies  and  differences  between  mutation  operators  for  ws  bpel  2  0  and  other  languages.  Applying  mutation  testing  to  a  program  written  in  a  certain  language  requires  that  a  set  of  mutation  operators  is  defined  for  that  language.  The  mutation  operators  need  to  adequately  cover  the  features  of  that  language  in  order  to  be  effective.  In  this  work,  we  evaluate  qualitatively  the  operators  defined  for  the  Web  Services  Business  Process  Execution  Language  2.0  (WS-BPEL)  and  study  the  differences  and  similarities  between  WS-BPEL  and  other  languages.  We  review  the  existing  operators  for  several  structured  and  object-oriented  general-purpose  programming  languages,  and  for  several  domain-specific  languages.  Results  confirm  that  WS-BPEL  is  very  different  from  other  languages,  as  half  of  the  mutation  operators  for  this  language  are  equivalent  to  those  of  other  languages.  Our  study  concludes  that  the  set  of  WS-BPEL  mutation  operators  can  be  improved.
1	Using  a  dsl  and  fine  grained  model  transformations  to  explore  the  boundaries  of  model  verification  extended  abstract.  Traditionally,  the  state-space  explosion  problem  in  model  checking  is  handled  by  applying  abstractions  and  simplifications  to  the  model  that  needs  to  be  verified.  In  this  paper,  we  propose  a  model-driven  engineering  approach  that  works  the  other  way  around.  Instead  of  making  a  concrete  model  more  abstract,  we  propose  to  refine  an  abstract  model  to  make  it  more  concrete.  We  propose  to  use  fine-grained  model  transformations  to  enable  model  checking  of  models  that  are  as  close  to  the  implementation  model  as  possible.  We  applied  our  approach  in  a  case  study.  The  results  show  that  models  that  are  more  concrete  can  be  validated  when  fine-grained  transformations  are  applied.
1	Evosuite  at  the  sbst  2013  tool  competition.  EvoSuite  is  a  mature  research  prototype  that  automatically  generates  unit  tests  for  Java  code.  This  paper  summarizes  the  results  and  experiences  in  participating  at  the  unit  testing  competition  held  at  SBST  2013,  where  EvoSuite  ranked  first  with  a  score  of  156.95.
1	Combining  minimization  and  generation  for  combinatorial  testing.  Combinatorial  Test  Design  (CTD)  is  an  effective  test  planning  technique  that  reveals  faults  resulting  from  feature  interactions  in  a  system.  The  standard  application  of  CTD  requires  manual  modeling  of  the  test  space,  including  a  precise  definition  of  restrictions  between  the  test  space  parameters,  and  produces  a  test  suite  that  corresponds  to  new  test  cases  to  be  implemented  from  scratch.  Interaction-based  Test-Suite  Minimization  (ITSM)  is  a  complementary  approach  to  standard  CTD,  which  reduces  a  given  test  suite  without  impacting  its  coverage  of  feature  interactions.  ITSM  requires  much  less  modeling  effort,  and  does  not  require  a  definition  of  restrictions  or  generation  of  new  test  data.  On  the  other  hand,  it  does  not  improve  the  coverage  obtained  by  the  given  test  suite.  In  this  work,  we  introduce  Minimization  Generation  CTD  (MG-CTD).  MG-CTD  is  a  combination  of  CTD  with  ITSM  for  addressing  situations  in  which  CTD  is  impractical,  and  ITSM  is  insufficient.  In  MG-CTD,  one  can  define  a  subset  of  the  parameters  that  can  be  freely  assigned,  as  in  CTD.  The  other  parameter  combinations  must  be  selected  from  an  existing  set,  as  in  ITSM.  MG-CTD  is  suitable  when  for  some  parts  of  the  test  space  it  is  easy  to  specify  restrictions  and  generate  new  test  data,  while  for  others  it  is  not.  MG-CTD  can  be  viewed  as  an  enhancement  of  ITSM,  and  always  achieves  better  interaction  coverage  than  ITSM.  We  discuss  the  trade-offs  between  CTD,  ITSM  and  MG-CTD,  and  present  an  efficient  implementation  which  is  based  on  binary  decision  diagrams.  We  then  present  some  of  the  measures  that  one  should  take  when  implementing  such  an  approach,  in  order  to  achieve  the  best  possible  coverage  in  the  final  result.  Finally,  we  demonstrate  MG-CTD  on  three  real-life  case  studies.
1	An  empirical  study  on  object  oriented  metrics  and  software  evolution  in  order  to  reduce  testing  costs  by  predicting  change  prone  classes.  Software  maintenance  cost  is  typically  more  than  fifty  percent  of  the  cost  of  the  total  software  life  cycle  and  software  testing  plays  a  critical  role  in  reducing  it.  Determining  the  critical  parts  of  a  software  system  is  an  important  issue,  because  they  are  the  best  place  to  start  testing  in  order  to  reduce  cost  and  duration  of  tests.  Software  quality  is  an  important  key  factor  to  determine  critical  parts  since  high  quality  parts  of  software  are  less  error  prone  and  easy  to  maintain.  As  object  oriented  software  metrics  give  important  evidence  about  design  quality,  they  can  help  software  engineers  to  choose  critical  parts,  which  should  be  tested  firstly  and  intensely.  In  this  paper,  we  present  an  empirical  study  about  the  relation  between  object  oriented  metrics  and  changes  in  software.  In  order  to  obtain  the  results,  we  analyze  modifications  in  software  across  the  historical  sequence  of  open  source  projects.  Empirical  results  of  the  study  indicate  that  the  low  level  quality  parts  of  a  software  change  frequently  during  the  development  and  management  process.  Using  this  relation  we  propose  a  method  that  can  be  used  to  estimate  change-prone  classes  and  to  determine  parts  which  should  be  tested  first  and  more  deeply.
1	Towards  a  deep  learning  model  for  vulnerability  detection  on  web  application  variants.  Reported  vulnerabilities  have  grown  significantly  over  the  recent  years,  with  SQL  injection  (SQLi)  being  one  of  the  most  prominent,  especially  in  web  applications.  For  these,  such  increase  can  be  explained  by  the  integration  of  multiple  software  parts  (e.g.,  various  plugins  and  modules),  often  developed  by  different  organizations,  composing  thus  web  application  variants.  Machine  Learning  has  the  potential  to  be  a  great  ally  on  finding  vulnerabilities,  aiding  experts  by  reducing  the  search  space  or  even  by  classifying  programs  on  their  own.  However,  previous  work  usually  does  not  consider  SQLi  or  utilizes  techniques  hard  to  scale.  Moreover,  there  is  a  clear  gap  in  vulnerability  detection  with  machine  learning  for  PHP,  the  most  popular  server-side  language  for  web  applications.  This  paper  presents  a  Deep  Learning  model  able  to  classify  PHP  slices  as  vulnerable  (or  not)  to  SQLi.  As  slices  can  belong  to  any  variant,  we  propose  the  use  of  an  intermediate  language  to  represent  the  slices  and  interpret  them  as  text,  resorting  to  well-studied  Natural  Language  Processing  (NLP)  techniques.  Preliminary  results  of  the  use  of  the  model  show  that  it  can  discover  SQLi,  helping  programmers  and  precluding  attacks  that  would  eventually  cost  a  lot  to  repair.
1	Software  testing  in  industry  and  academia  a  view  of  both  sides  in  japan.  Collaboration  between  industry  and  academia  is  important  for  solving  problems  and  creating  innovations.  Both  sides  of  industry  and  academia  are  important  for  each  other,  but  sometimes  they  are  incompatible.  In  this  paper,  we  discuss  software  testing  in  industry  and  academia  from  their  respective  views  in  Japan  on  the  basis  of  the  author's  experiences  and  studies.  High  quality  software  is  required  and  its  industry  is  influenced  by  economic  circumstances.  In  Japan,  there  are  also  problems  regarding  cost  reductions,  production  of  high  quality  software,  adapting  businesses,  and  so  on.  We  have  made  efforts  to  solve  these  problems  by  collaborating  with  people  in  both  industry  and  academia.  Our  efforts  include  developing  test  methodologies,  skill  standards,  education  syllabi,  and  so  on.
1	Testing  big  data  assuring  the  quality  of  large  databases.  The  volume  and  variety  of  modern  day  databases  presents  a  particular  challenge  to  the  system  testing  community.  The  question  is  how  to  go  about  testing  such  large  collections  of  various  data  types  ranging  from  tables  to  texts  and  images.  To  test  those  applications  which  use  them,  these  conglomerations  of  multiple  data  object  types  have  to  be  automatically  generated  and  validated.  There  is  no  other  way  but  to  automate  the  test  process.  This  contribution  outlines  the  challenge  and  presents  an  automated  approach  to  setting  up  and  testing  big  data  bases.  At  the  end  a  case  study  of  a  large  data  warehouse  is  discussed  with  lessons  learned  from  that  industrial  test  project.
1	Automated  significant  load  testing  for  ws  bpel  compositions.  Web  service  compositions  must  provide  services  to  hundreds  even  thousands  of  users  concurrently.  These  applications  must  be  load  tested  to  ensure  that  they  can  function  properly  under  high  load.  We  propose  in  this  paper  a  solution  for  load  testing  of  WS-BPEL  compositions.  It  is  based  on  Timed  Automata  as  model  for  specifying  requirements  to  test  under  various  load  conditions,  a  distributed  testing  architecture,  an  algorithm  for  online  load  test  generation  and  execution,  and  finally  an  automated  log  analysis  technique.  We  also  illustrate  our  approach  by  means  of  a  case  study.
1	Dynamic  prioritization  in  regression  testing.  Although  used  extensively  in  industry,  regression  testing  is  challenging  from  both  a  process  management  as  well  as  a  resource  management  perspective.  In  literature,  proposed  test  case  prioritization  techniques  assume  a  constant  pool  of  test  cases  with  non-changing  coverage  during  the  regression  testing  process,  and  therefore  they  work  with  a  fixed,  prioritized  test  suite.  However,  in  practice,  test  cases  and  their  coverage  metrics  may  change  during  regression  testing  due  to  modifications  of  software  artefacts  (e.g.  due  to  bug  fixing).  For  example,  modifying  obsolete  test  cases  or  source  code  may  change  the  coverage  metrics  during  the  process.  This  may  lead  to  some  changes  in  test  case  priorities.  Dealing  with  manual  tests  cases,  scheduling  test  case  execution  in  shared  environments  and  other  constraints  in  practice  may  cause  the  same  effect.  In  this  paper,  we  highlight  these  challenges  in  industrial  regression  testing  and  propose  a  paradigm  called  Dynamic  Prioritization,  which  uses  in-process  events  and  the  most  up-to-date  test  suite  to  re-order  test  cases.
1	Kepler  raising  browser  security  awareness.  Web  browser  security  is  lacking  and  browsers  are  often  unable  to  detect  what  is  unwanted  traffic,  like  contacting  tracking  sites.  Without  our  direct  knowledge,  we  are  exposed  to  different  kinds  of  security  risks  during  web  browsing.  Browsing  the  web  causes  our  browser  to  render  a  web  page  from  HTML,  CSS  and  JavaScript  but  many  things  that  we  cannot  see  happens  during  this.  The  web  page  may  have  saved  data  on  our  computer,  tracked  sites  we  have  visited,  sent  us  unwanted  advertisements  and  maybe  even  executed  a  malicious  script.  This  triggers  the  need  for  users  to  be  able  to  determine  themselves  which  risk  they  are  willing  to  take.  To  improve  browsing  security,  third  party  developers  are  developing  browser  extensions.  By  giving  the  users  detailed  information  about  web  requests,  we  raise  awareness  and  help  users  make  decisions  themselves  about  which  websites  are  secure.  To  this  end,  a  prototype  for  displaying  this  information  was  developed.  The  prototype  is  a  Google  Chrome  extension  which  gathers  and  displays  detailed  information  about  web  sites'  activities  in  real-time.  With  the  extension  we  are  able  to  inspect  every  request  made  by  the  browser  in  detail.  We  can  also  filter  the  data  according  to  country.  The  data  is  presented  in  a  human  readable  form  using  geolocation.  With  the  help  of  this  prototype,  users'  awareness  of  web  browsing  security  is  raised  in  an  informative  and  interesting  way.
1	String  analysis  as  an  abstract  interpretation.  We  formalize  a  string  analysis  within  abstract  interpretation  framework.  The  abstraction  of  strings  is  given  as  a  conjunction  of  predicates  that  describes  the  common  configuration  changes  on  the  reference  pushdown  automaton  while  processing  the  strings.  We  also  present  a  family  of  pushdown  automata  called  e  bounded  pushdown  automata.  This  family  covers  all  context-free  languages,  and  by  using  this  family  of  pushdown  automata,  we  can  prevent  abstract  values  from  becoming  infinite  conjunctions  and  guarantee  that  the  operations  required  in  the  analyzer  are  computable.
1	Gradual  program  verification.  Both  static  and  dynamic  program  verification  approaches  have  significant  disadvantages  when  considered  in  isolation.  Inspired  by  research  on  gradual  typing,  we  propose  gradual  verification  to  seamlessly  and  flexibly  combine  static  and  dynamic  verification.  Drawing  on  general  principles  from  abstract  interpretation,  and  in  particular  on  the  recent  Abstracting  Gradual  Typing  methodology  of  Garcia  et  al.,  we  systematically  derive  a  gradual  verification  system  from  a  static  one.  This  approach  yields,  by  construction,  a  gradual  verification  system  that  is  compatible  with  the  original  static  system,  but  overcomes  its  rigidity  by  resorting  to  dynamic  verification  when  desired.  As  with  gradual  typing,  the  programmer  can  control  the  trade-off  between  static  and  dynamic  checking  by  tuning  the  (im)precision  of  pre-  and  postconditions.  The  formal  semantics  of  the  gradual  verification  system  and  the  proofs  of  its  properties,  including  the  gradual  guarantees  of  Siek  et  al.,  have  been  fully  mechanized  in  the  Coq  proof  assistant.
1	Specification  the  biggest  bottleneck  in  formal  methods  and  autonomy.  Advancement  of  AI-enhanced  control  in  autonomous  systems  stands  on  the  shoulders  of  formal  methods,  which  make  possible  the  rigorous  safety  analysis  autonomous  systems  require.  An  aircraft  cannot  operate  autonomously  unless  it  has  design-time  reasoning  to  ensure  correct  operation  of  the  autopilot  and  runtime  reasoning  to  ensure  system  health  management,  or  the  ability  to  detect  and  respond  to  off-nominal  situations.  Formal  methods  are  highly  dependent  on  the  specifications  over  which  they  reason;  there  is  no  escaping  the  “garbage  in,  garbage  out”  reality.  Specification  is  difficult,  unglamorous,  and  arguably  the  biggest  bottleneck  facing  verification  and  validation  of  aerospace,  and  other,  autonomous  systems.
1	An  improved  unrolling  based  decision  procedure  for  algebraic  data  types.  Reasoning  about  algebraic  data  types  and  functions  that  operate  over  these  data  types  is  an  important  problem  for  a  large  variety  of  applications.  In  this  paper,  we  present  a  decision  procedure  for  reasoning  about  data  types  using  abstractions  that  are  provided  by  catamorphisms:  fold  functions  that  map  instances  of  algebraic  data  types  into  values  in  a  decidable  domain.  We  show  that  the  procedure  is  sound  and  complete  for  a  class  of  monotonic  catamorphisms.    Our  work  extends  a  previous  decision  procedure  that  solves  formulas  involving  algebraic  data  types  via  successive  unrollings  of  catamorphism  functions.  First,  we  propose  the  categories  of  monotonic  catamorphisms  and  associative-commutative  catamorphisms,  which  we  argue  provide  a  better  formal  foundation  than  previous  categorizations  of  catamorphisms.  We  use  monotonic  catamorphisms  to  fix  an  incompleteness  in  the  previous  unrolling  algorithm  and  associated  proof.  We  then  use  these  notions  to  address  two  open  problems  from  previous  work:  1  we  provide  a  bound  on  the  number  of  unrollings  necessary  for  completeness,  showing  that  it  is  exponentially  small  with  respect  to  formula  size  for  associative-commutative  catamorphisms,  and  2  we  demonstrate  that  associative-commutative  catamorphisms  can  be  combined  within  a  formula  whilst  preserving  completeness.
1	A  lightweight  technique  for  distributed  and  incremental  program  verification.  Applying  automated  verification  to  industrial  code  bases  creates  a  significant  computational  task  even  when  the  individual  conditions  to  be  checked  are  trivial.  This  affects  the  wall  clock  time  taken  to  verify  the  program  and  has  knock-on  effects  on  how  the  tools  are  used  and  on  project  management.  In  this  paper  a  simple  and  lightweight  technique  for  adding  incremental  and  distributed  capabilities  to  a  program  verification  system  is  given.  Experiments  with  an  implementation  of  the  technique  for  the  SPARK  tool  set  show  that  it  can  yield  an  average  29  fold  speed  increase  in  incremental  use  and  near  optimal  speedup  in  distributed  use.  Critically,  this  gives  a  qualitative  change  in  how  automated  verification  is  used  in  a  large  commercial  project.
1	Program  checking  with  less  hassle.  The  simple  and  often  imprecise  specifications  that  programmers  may  write  are  a  significant  limit  to  a  wider  application  of  rigorous  program  verification  techniques.  Part  of  the  reason  why  non-specialists  find  writing  good  specification  hard  is  that,  when  verification  fails,  they  receive  little  guidance  as  to  what  the  causes  might  be,  such  as  implementation  errors  or  inaccurate  specifications.  To  address  these  limitations,  this  paper  presents  two-step  verification,  a  technique  that  combines  implicit  specifications,  inlining,  and  loop  unrolling  to  provide  improved  user  feedback  when  verification  fails.  Two-step  verification  performs  two  independent  verification  attempts  for  each  program  element:  one  using  standard  modular  reasoning,  and  another  one  after  inlining  and  unrolling;  comparing  the  outcomes  of  the  two  steps  suggests  which  elements  should  be  improved.  Two-step  verification  is  implemented  in  AutoProof,  our  static  verifier  for  Eiffel  programs  integrated  in  EVE  the  Eiffel  Verification  Environment  and  available  online.
1	A  logical  system  for  modular  information  flow  verification.  Information  Flow  Control  (IFC)  is  important  to  ensure  secure  programs  where  secret  data  does  not  influence  any  public  data.  The  pervasive  standard  that  IFC  aims  to  is  non-interference.  Current  IFC  systems  are  separated  into  dynamic  IFC,  static  IFC,  and  hybrids  between  static  and  dynamic.  With  dynamic  IFC  suffering  from  high  overhead  and  limited  ability  to  prevent  implicit  flows  due  to  the  paths  not  taken,  we  propose  a  novel  modular  static  IFC  system.  To  the  best  of  our  knowledge,  this  is  the  first  modular  static  IFC  system.  Unlike  type-based  static  IFC  systems,  ours  is  logic-based.  The  limitation  of  type-based  IFC  systems  is  in  the  inviolability  of  static  security  label  declarations  for  fields.  As  such,  they  suffer  from  transient  leaks  on  fields.  Our  proposed  system  uses  a  Hoare-like  logic.  It  verifies  each  function  independently  with  the  help  of  separation  logic.  Furthermore,  we  provide  the  proof  of  correctness  for  our  novel  IFC  system  with  respect  to  termination-  and  timing-insensitive  non-interference.
1	Solving  mathrm  lia  star  using  approximations.  Linear  arithmetic  with  stars,  \(\mathrm  {LIA}  ^\star  \),  is  an  extension  of  Presburger  arithmetic  that  allows  forming  indefinite  summations  over  values  that  satisfy  a  formula.  It  has  found  uses  in  decision  procedures  for  multi-sets  and  for  vector  addition  systems.  \(\mathrm  {LIA}  ^\star  \)  formulas  can  be  translated  back  into  Presburger  arithmetic,  but  with  non-trivial  space  overhead.  In  this  paper  we  develop  a  decision  procedure  for  \(\mathrm  {LIA}  ^\star  \)  that  checks  satisfiability  of  \(\mathrm  {LIA}  ^\star  \)  formulas.  By  refining  on-demand  under  and  over-approximations  of  \(\mathrm  {LIA}  ^\star  \)  formulas,  it  can  avoid  the  space  overhead  that  is  integral  to  previous  approaches.  We  have  implemented  our  procedure  in  a  prototype  and  report  on  encouraging  results  that  suggest  that  \(\mathrm  {LIA}  ^\star  \)  formulas  can  be  checked  for  satisfiability  without  computing  a  prohibitively  large  equivalent  Presburger  formula.
1	Loop  detection  by  logically  constrained  term  rewriting.  Logically  constrained  rewrite  systems  constitute  a  very  general  rewriting  formalism  that  can  capture  simplification  processes  in  various  domains  as  well  as  computation  in  imperative  programs.  In  both  of  these  contexts,  nontermination  is  a  critical  source  of  errors.  We  present  new  criteria  to  find  loops  in  logically  constrained  rewrite  systems  which  are  implemented  in  the  tool  Ctrl.  We  illustrate  the  usefulness  of  these  criteria  in  three  example  applications:  to  find  loops  in  LLVM  peephole  optimizations,  to  detect  looping  executions  of  C  programs,  and  to  establish  nontermination  of  integer  transition  systems.
1	Characterising  users  through  an  analysis  of  on  line  technical  support  forums.  Users  of  software  systems  need  support.  When  users  choose  to  go  directly  to  online  forums  rather  than  report  issues  to  the  source  of  the  problem,  the  development  organization  loses  out  in  terms  of  gathering  information  on  where  their  systems  could  be  improved,  and  can  lose  customer  loyalty  and  goodwill.  This  in  turn  will  have  a  negative  effect  on  future  sales  and  system  enhancements.    The  objective  of  this  study  is  to  examine  online  forums  in  order  to  characterize  technical  support  TS  users,  and  create  a  framework  that  allows  commercial  organizations  and  other  interested  parties  to  identify  types  of  users  and  how  best  to  address  their  needs.    One  hundred  and  sixteen  threads  3,064  messages  from  eight  online  open  source  forums  were  analysed  using  a  grounded  theory  approach.  Also  the  literature  on  human  factors,  and  personalisation  was  examined  to  elicit  information  on  how  people  can  be  categorized.    We  found  that  users  of  TS  systems  can  be  grouped  according  to  their  level  of  expertise  and  what  they  value.  Additionally  we  identified  characteristics  of  the  communication  handling  process  that  influence  desirable  and  undesirable  outcomes.    The  contribution  of  this  research  is  an  empirically  derived  framework  that  identifies  TS  users  according  to  groups  of  characteristics.  We  hypothesise  that  a  user  will  be  more  satisfied  when  the  TS  service  provider  recognises  concepts  such  as  what  they  personally  value,  their  level  of  expertise  and  how  best  to  manage  emotions.
1	Continuous  integration  applied  to  software  intensive  embedded  systems  problems  and  experiences.  In  this  paper  we  present  a  summary  of  factors  that  must  be  taken  into  account  when  applying  continuous  integration  to  software-intensive  embedded  systems.  Experiences  are  presented  from  two  study  cases  regarding  seven  topics:  complex  user  scenarios,  compliance  to  standards,  long  build  times,  many  technology  fields,  security  aspects,  architectural  runway  and  test  environments.  In  the  analysis  we  show  how  issues  within  these  topics  obstruct  the  organization  from  working  according  to  the  practices  of  continuous  integration.  The  identified  impediments  are  mapped  to  a  list  of  continuous  integration  corner-stones  proposed  in  literature.
1	Software  effort  estimation  and  risk  management.  Every  software  business  has  to  be  able  to  budget  and  plan  its  software  development  projects  realistically.  Since  software  projects  are  usually  fraught  with  uncertainty,  they  inherently  involve  planning  risks  right  from  the  start.  Consequently,  it  is  important  to  manage  cost-related  risks  in  order  to  monitor  them  and  implement  adequate  contingency  plans  when  a  crisis  occurs.  Since  not  all  projects  are  equal,  it  is  important  to  identify  the  specific  factors  (cost  drivers)  that  have  a  positive  or  negative  influence  on  productivity.  Thus,  the  use  of  appropriate  cost  estimation  approaches  does  not  only  contribute  to  accurate  project  planning  and  successful  risk  management,  but  also  to  improved  software  processes  and  overall  organization  maturity.
1	Devops  adoption  benefits  and  challenges  in  practice  a  case  study.  DevOps  is  an  approach  in  which  traditional  software  engineering  roles  are  merged  and  communication  is  enhanced  to  improve  the  production  release  frequency  and  maintain  software  quality.  There  seem  to  be  benefits  in  adopting  DevOps  but  practical  industry  experiences  have  seldom  been  reported.  We  conducted  a  qualitative  multiple-case  study  and  interviewed  the  representatives  of  three  software  development  organizations  in  Finland.  The  responses  indicate  that  with  DevOps,  practitioners  can  increase  the  frequency  of  releases  and  improve  test  automation  practices.  DevOps  was  seen  to  encourage  collaboration  between  departments  which  boosts  communication  and  employee  welfare.  Continuous  releases  enable  a  more  experimental  approach  and  rapid  feedback  collection.  The  challenges  include  communication  structures  that  hinder  cross-department  collaboration  and  having  to  address  the  cultural  shift.  Dissimilar  development  and  production  environments  were  mentioned  as  some  of  the  technical  barriers.  DevOps  might  not  also  be  suitable  for  all  industries.  Ambiguity  in  the  definition  of  DevOps  makes  adoption  difficult  since  organizations  might  not  know  which  practices  they  should  implement  for  DevOps.
1	Search  based  approaches  for  software  development  effort  estimation.  In  the  last  years  the  use  of  Search-Based  techniques  has  been  suggested  to  estimate  software  development  effort.  These  techniques  are  meta-heuristics  able  to  find  optimal  or  near  optimal  solutions  to  problems  characterized  by  large  space.  In  the  context  of  effort  estimation  Search-Based  approaches  can  be  exploited  to  build  estimation  models  or  to  enhance  the  effectiveness  of  other  methods.  The  preliminary  investigations  carried  out  so  far  have  provided  promising  results.  Nevertheless,  the  capabilities  of  these  approaches  have  not  been  fully  explored  and  the  empirical  analyses  carried  out  so  far  have  not  considered  the  more  recent  recommendations  on  how  to  perform  this  kind  of  empirical  assessment  in  the  effort  estimation  context  and  in  Search-Based  Software  Engineering.  The  main  aim  of  the  PhD  dissertation  is  to  provide  an  insight  on  the  use  of  Search-Based  techniques  for  effort  estimation  trying  to  highlight  strengths  and  weaknesses.
1	A  study  of  the  scrum  master  s  role.  Scrum  is  an  increasingly  common  approach  to  software  development  adopted  by  organizations  around  the  world.  However,  as  organizations  transition  from  traditional  plan-driven  development  to  agile  development  with  Scrum,  the  question  arises  as  to  which  Scrum  role  (Product  Owner,  Scrum  Master,  or  Scrum  Team  Member)  corresponds  to  a  Project  Manager,  or  conversely  which  Scrum  role  should  the  Project  Managers  adopt?
1	A  model  based  integration  approach  for  reference  models.  A  variety  of  reference  models  (RMs)  such  as  CMMI,  COBIT  or  ITIL  support  IT  organizations  to  improve  their  processes.  As  these  RMs  cover  different  domains  and  also  share  some  similarities,  organizations  may  benefit  from  the  adoption  of  multiple  RMs.  However,  organizations  need  a  systematic  support  to  select  and  efficiently  implement  RMs.  We  present  the  MoSaIC  approach  for  a  semantic  RM  integration  based  on  common  meta-models  to  help  organizations  in  understanding  and  adopting  the  considered  RMs.
1	Hybrid  software  and  systems  development  in  practice  perspectives  from  sweden  and  uganda.  Many  organizations  are  adapting  the  use  of  hybrid  software  development  approaches  by  combining  traditional  methods  with  flexible  agile  practices.  This  paper  presents  the  initial  results  from  the  survey  on  the  use  of  hybrid  software  and  systems  approaches.  The  results  are  from  twenty  one  respondents  from  Sweden  and  Uganda.  Our  results  show  that  the  iterative  model  is  the  most  widely  used  process  model  in  both  Sweden  and  Uganda.  However,  the  traditional  process  models  are  also  used  in  combination  with  the  more  agile  models  like  Scrum.  From  the  results,  we  also  show  that  the  large  sized  companies  face  the  biggest  problems  during  implementation  of  agility  since  they  have  to  adhere  to  standards  and  control  measures.
1	Agile  quality  requirements  management  best  practices  portfolio  a  situational  method  engineering  approach.  Management  of  Quality  Requirements  (QRs)  is  determinant  for  the  success  of  software  projects.  However,  this  management  is  currently  under-considered  in  software  projects  and  in  particular,  in  agile  methods.  Although  agile  processes  are  focused  on  the  functional  aspects  of  the  software,  some  agile  practices  can  be  beneficial  for  the  management  of  QRs.  For  example,  the  collaboration  and  interaction  of  people  can  help  in  the  QR  elicitation  by  reducing  vagueness  of  requirements  through  communication.  In  this  paper,  we  present  the  initial  findings  of  our  research  investigating  what  industrial  practices,  from  the  agile  methods,  can  be  used  for  better  management  of  QRs  in  agile  software  development.  We  use  Situational  Method  Engineering  to  identify,  complement  and  classify  a  portfolio  of  best  practices  for  QR  management  in  agile  environments.  In  this  regard,  we  present  the  methodological  approach  that  we  are  applying  for  the  definition  of  these  guidelines  and  the  requirements  that  will  lead  us  to  compile  a  portfolio  of  agile  QR  management  best  practices.  The  proposed  requirements  correspond  to  the  whole  software  life  cycle  starting  in  the  elicitation  and  finalizing  in  the  deployment  phases.
1	An  agile  requirements  elicitation  approach  based  on  nfrs  and  business  process  models  for  micro  businesses.  Software  empowers  micro-businesses  to  be  more  competitive  -  be  it  accounting  software,  point-of-sales  software,  e-commerce  software,  and  the  like.  Before  having  such  software,  micro-businesses  must  express  their  requirements  properly  to  the  software  developers  first.  Since  micro-businesses  have  restrictions  with  their  budget,  manpower,  and  technical  exposure  to  software,  some  trade-offs  must  be  addressed.  In  such  case,  a  practical  requirements  approach  must  be  used.  This  approach  must  be  inexpensive,  non-technical,  and  must  not  involve  too  much  manpower.  This  paper  will  show  how  a  software  development  company,  Pentathlon  Systems  Resources  Incorporated,  has  applied  an  agile  requirements  elicitation  approach  to  a  micro-business,  an  amusement  arcade.  This  paper  will  demonstrate  how  several  models  and  techniques  such  as  goals,  business  process  models,  patterns,  and  non-functional  requirements,  have  helped  in  defining  the  software  requirements  of  the  micro-business.
1	The  choice  of  code  review  process  a  survey  on  the  state  of  the  practice.  Code  review  has  been  known  to  be  an  effective  quality  assurance  technique  for  decades.  In  the  last  years,  industrial  code  review  practices  were  observed  to  converge  towards  “change-based/modern  code  review”,  but  with  a  lot  of  variation  in  the  details  of  the  processes.  Recent  research  also  proposed  hypotheses  on  factors  that  influence  the  choice  of  process.  However,  all  current  research  in  this  area  is  based  on  small  and  largely  non-random  samples  of  cases.  Therefore,  we  set  out  to  assess  the  current  state  of  the  practice  and  to  test  some  of  these  hypotheses  with  a  survey  among  commercial  software  development  teams.  We  received  responses  from  240  teams.  They  support  many  of  the  stated  hypotheses,  e.g.,  that  change-based  code  review  is  the  dominating  style  of  code  review  in  the  industry,  and  that  teams  doing  change-based  code  review  have  a  lower  risk  that  review  use  fades  away.  However,  other  hypotheses  could  not  be  confirmed,  mainly  that  the  balance  of  effects  a  team  tries  to  reach  with  code  reviews  acts  as  a  mediator  in  determining  the  details  of  the  review  process.  Apart  from  these  findings,  we  contribute  the  survey  data  set  as  a  foundation  for  future  research.
1	Medical  device  software  development  a  perspective  from  a  lean  manufacturing  plant.  Developing  software  for  the  manufacture  of  medical  devices  is  a  sensitive  operation  from  many  perspectives,  such  as:  safety  and  regulatory  compliance.  Medical  Device  companies  are  required  to  have  a  well  defined  development  process  in  place,  which  includes  software  development,  and  be  able  to  demonstrate  that  they  have  followed  it  through  the  complete  life-cycle  of  the  device.  With  the  increasing  complexity  of  Medical  Devices,  and  more  detailed  software  development  regulations  among  some  of  the  influencing  factors,  we  take  a  look  at  how  some  of  these  factors  have  impacted  the  software  development  process  within  a  medical  device  manufacturing  plant.  We  find  that  tying  down  your  process  across  the  board  can  have  unwanted  consequences.  As  process  flexibility  is  required,  we  have  investigated  the  usefulness  of  Lean  Software  Development.
1	How  to  certify  the  very  small  entity  software  processes  using  iso  iec  29110.  A  large  majority  of  enterprises  worldwide  are  VSEs  (Very  Small  Entities).  Like  for  any  business,  VSEs  in  the  IT  business  sector  face  a  challenging  and  strong  competition.  Adopting  “best  practices”,  standardizing  processes  and  obtaining  an  international  recognition  or  certification,  are  key  factors  for  success.  This  paper  introduces  a  new  and  cost/effective  alternative  for  international  recognition  of  small  SMEs  (VSEs),  as  good  quality  software  producers.  The  new  standard  is  VSE  tailor-made,  lightweight,  certifiable  and  compatible  with  traditional  models  such  as  CMMI  [2],  ISO330xx,  or  ISO  9000.  New  standards  are  on  their  way  to  internationally  harmonize  these  conformity  assessments  and  certificates  for  VSEs.
1	An  iso  iec  15504  security  extension.  Software  companies  which  have  been  involved  in  a  process  improvement  programme  according  to  ISO/IEC  15504  have  already  performed  some  steps  in  order  to  implement  ISO/IEC  27000  as  an  information  security  management  framework.  After  analysing  in  depth  the  existing  relations  between  ISO/IEC  15504-5  base  practices  and  ISO/IEC  27002  security  controls,  in  this  paper  the  security  controls  covered  by  the  ISO/IEC  15504-5  processes  are  described,  the  changes  over  these  processes  which  would  be  necessary  for  the  implementation  of  the  controls  are  detailed  and  an  ISO/IEC  15504  Security  Extension  that  facilitates  the  implementation  of  both  standards  is  presented.
1	Agile  maturity  model  oxymoron  or  the  next  level  of  understanding.  From  the  agile  camp  you  can  hear  someone  to  say  that  CMMI  is  the  big  American  waterfall  model  monster,  and  is  outright  contra  productive  to  agile  methods.  From  the  CMMI  camp  you  can  hear  someone  to  say  that  agile  methods  is  hackers  from  hell  that  uses  the  agile  paradigm  to  enjoy  anarchy  with  no  rules.  You  can  also  hear  some  say  that  agile  works  the  best  in  CMMI  level  5  companies.  The  context  of  the  dilemma  however  is  slightly  awkward.  CMMI  describes  characteristics  of  good  development  practices,  and  agile  is  a  lifecycle  concept.  So  from  a  meta  point  of  view  they  can  easily  co-exist.  We  would  like  to  state  that  they  do,  and  that  you  need  both  to  support  the  best  development  performance.  Starting  in  December  2011  three  surveys  were  launched  to  get  an  idea  about  what  could  an  agile  maturity  model  deliver  and  what  might  be  its  added  value.  67  Participants  from  several  agile  or/and  CMMI®  related  LinkedIn  Groups  contributed  to  the  survey.  This  article  explains  the  survey  results  and  proposes  further  research  topics  and  harmonization  actions.
1	Systematic  literature  review  on  the  characteristics  of  agile  project  management  in  the  context  of  maturity  models.  Popularity  of  Agile  Methods  is  growing  up  and  along  with  this  popularity  is  also  growing  the  interest  in  adopting  these  methods  in  conjunction  with  maturity  models,  like  CMMI.  Dozens  of  reports  about  this  topic  can  be  found  with  different  results.  Therefore,  a  Systematic  Literature  Review  was  conducted  with  the  goal  of  identifying  characteristics  of  agile  project  management  in  organizations  using  agile  methods  and  maturity  models.  We  accepted  34  primary  studies  published  from  2001  to  2013.  The  results  show  that  the  area  still  lacks  details  on  how  to  perform  the  software  development  activities,  what  techniques  can  be  used  to  meet  issues  not  directly  addressed  by  agile  methods  without  losing  the  desired  agility  and  what  tools  can  be  used  to  facilitate  the  combination  of  approaches.
1	The  use  of  maturity  capability  frameworks  for  healthcare  process  assessment  and  improvement.  Process  assessment  enables  to  identify  strengths  and  weaknesses  of  selected  processes  in  a  specific  domain  typically  by  referencing  process  maturity/capability  frameworks.  Assessment  findings  are  usually  transformed  into  action-items  for  process  improvement.  In  healthcare  domain  where  hospitals  offer  high-risk  services  to  patients  every  day  in  a  complex,  dynamic,  and  multidisciplinary  environment,  establishing  process  thinking  and  effective  process  management  is  increasingly  demanded  but  not  an  easy  task  to  accomplish.  In  this  study,  we  investigate  the  maturity/capability  frameworks  that  are  proposed  or  used  for  assessing  and  improving  the  healthcare  processes.  We  searched  the  studies  reported  between  the  years  2000  and  2015  in  scientific  digital  libraries  and  identified  29  studies  out  of  958  initially  retrieved  in  a  systematic  way.  This  study  provides  an  analysis  of  six  studies  out  of  29  with  respect  to  a  number  of  research  questions  regarding  context,  scope,  time  coverage,  and  results  as  well  as  research  method  and  contribution.
1	State  of  practice  of  user  developer  communication  in  large  scale  it  projects.  [Context  and  motivation]  User  participation  in  software  development  is  considered  to  be  essential  for  successful  software  systems.  Especially  missing  direct  communication  between  users  and  developers  can  cause  various  issues  in  large-scale  IT  projects.  [Question/Problem]  We  want  to  understand  current  practices  of  user---developer  communication  in  large-scale  IT  projects,  the  factors  for,  and  consequences  of  communication  gaps,  and  what  experts  suggest  to  prevent  them.  [Principal  ideas/results]:  We  conducted  a  series  of  semi-structured  interviews  with  twelve  experts.  The  experts  work  on  the  coordination  of  Business  and  IT  and  describe  their  experiences  gained  in  69  large-scale  IT  projects.  The  analysis  of  our  interviews  showed  that  direct  user---  developer  communication  is  limited  and  that  no  commonly  used  method  for  the  user---developer  communication  in  the  design  and  implementation  activity  exists.  [Contribution]:  The  interviews  helped  us  to  understand  current  practices  and  issues  resulting  from  missing  communication.  Furthermore,  we  can  confirm  the  need  for  a  method  enhancing  user---developer-communication  in  large-scale  IT  projects.
1	Towards  a  more  semantically  transparent  i  visual  syntax.  [Context  and  motivation]i*  is  one  of  the  most  popular  modelling  languages  in  Requirements  Engineering.  i*  models  are  meant  to  support  communication  between  technical  and  non-technical  stakeholders  about  the  goals  of  the  future  system.  Recent  research  has  established  that  the  effectiveness  of  model-mediated  communication  heavily  depends  on  the  visual  syntax  of  the  modelling  language.  A  number  of  flaws  in  the  visual  syntax  of  i*  have  been  uncovered  and  possible  improvements  have  been  suggested.  [Question/problem]  Producing  effective  visual  notations  is  a  complex  task  that  requires  taking  into  account  various  interacting  quality  criteria.  In  this  paper,  we  focus  on  one  of  those  criteria:  Semantic  Transparency,  that  is,  the  ability  of  notation  symbols  to  suggest  their  meaning.  [Principal  ideas/results]  Complementarily  to  previous  research,  we  take  an  empirical  approach.  We  give  a  preview  of  a  series  of  experiments  designed  to  identify  a  new  symbol  set  for  i*  and  to  evaluate  its  semantic  transparency.  [Contribution]  The  reported  work  is  an  important  milestone  on  the  path  towards  cognitively  effective  requirements  modelling  notations.  Although  it  does  not  solve  all  the  problems  in  the  i*  notation,  it  illustrates  the  usefulness  of  an  empirical  approach  to  visual  syntax  definition.  This  approach  can  later  be  transposed  to  other  quality  criteria  and  other  notations.
1	High  level  requirements  management  and  complexity  costs  in  automotive  development  projects  a  problem  statement.  Effective  requirements  management  plays  an  important  role  when  it  comes  to  the  support  of  product  development  teams  in  the  automotive  industry.  A  precise  positioning  of  new  cars  in  the  market  is  based  on  features  and  characteristics  described  as  requirements  as  well  as  on  costs  and  profits.  [Question/problem]  However,  introducing  or  changing  requirements  does  not  only  impact  the  product  and  its  parts,  but  may  lead  to  overhead  costs  in  the  OEM  due  to  increased  complexity.  The  raised  overhead  costs  may  well  exceed  expected  gains  or  costs  from  the  changed  requirements.  [Principal  ideas/results]  By  connecting  requirements  with  direct  and  overhead  costs,  decision  making  based  on  requirements  could  become  more  valuable.  [Contribution]  This  problem  statement  results  from  a  detailed  examination  of  the  effects  of  requirements  management  practices  on  process  complexity  and  vice  versa  as  well  as  on  how  today's  requirements  management  tools  assist  in  this  respect.  We  present  findings  from  a  joined  research  project  of  RWTH  Aachen  University  and  Volkswagen.
1	The  amass  tool  platform  an  innovative  solution  for  assurance  and  certification  of  cyber  physical  systems.  Cyber-physical  systems  are  usually  subject  to  assurance  and  certifica-  tion  processes,  including  thorough  requirements  engineering  tasks,  to  ensure  that  they  are  acceptably  dependable.  The  underlyi  ...
1	Documenting  relations  between  requirements  and  design  decisions  a  case  study  on  design  session  transcripts.  Context/Motivation:  Developers  make  many  important  decisions  as  they  address  given  requirements  during  system  design.  Each  decision  is  explained  and  justified  by  decision-related  knowledge.  Typically,  this  knowledge  is  neither  captured  in  a  structured  way,  nor  linked  to  the  respective  requirements  in  detail.  Then,  it  is  not  obvious,  how  design  decisions  realize  the  given  requirements  and  whether  they  further  refine  or  shape  them.  Thus,  the  relations  and  alignment  of  requirements  and  design  cannot  be  assessed  properly.  Problem/Question:  While  there  are  several  studies  on  decision-making  in  general,  there  does  not  exist  a  study  uncovering  how  decision-related  knowledge  emerges  based  on  requirements.  Such  a  study  is  important  to  understand  the  intertwined  relations  of  requirements  and  design  decisions  as  well  as  how  requirement  descriptions  could  be  enhanced  with  feedback  from  design  decision-making.  Principal  Idea/Results:  We  applied  a  flexible  documentation  approach  for  decision-related  knowledge  on  discussion  transcripts  of  two  design  sessions  with  professional  designers.  We  analyzed  the  discussions  for  decision-related  knowledge  and  documented  it  together  with  its  relations  to  the  given  requirements.  Several  complex  and  incrementally  growing  knowledge  structures  for  decisions  were  found  to  emerge  in  relation  to  the  given  requirements.  Also,  we  uncovered  that  decision-related  knowledge  contained  uncertainties  about  requirements  and  further  refined  them.  Contribution:  Our  study  uncovers  detailed  relations  between  requirements  and  design  decisions  and  thereby  improves  the  understanding  of  their  mutual  impact  on  each  other.  We  also  derive  recommendations  for  the  cooperation  between  requirements  engineers  and  designers  in  practice.  In  addition,  we  demonstrate  that  our  documentation  approach  for  decision-related  knowledge  provides  a  comprehensive  view  on  decisions  and  their  relations  to  requirements.
1	Is  requirements  engineering  useless  in  game  development.  [Context/motivation]  Game  development  is  characterized  by  a  high  level  of  creativity  when  compared  to  other  fields  of  software  development.  Games  cover  a  multitude  of  themes  and  genres,  and  represent  a  heterogeneous  group  of  different  products  with  varying  requirements  and  business  goals.  [Question/problem]  Requirements  engineering  RE  should  be  relevant  to  game  development,  but  is  this  true  and  if  it  is,  how  does  game  industry  apply  RE  in  practice?  [Principal  ideas/Results]  We  interviewed  27  software  professionals  in  seven  organizations  to  understand  how  requirements  engineering  is  applied  in  game  developing  organizations.  The  results  suggest  that  in  game  development  business  practicalities  and  drive  for  "fun"  dominate  the  areas  associated  with  requirements  engineering.  Additionally,  game  development  organizations  apply  approaches  and  methods  that  are  comparable  to  requirements  engineering  and  requirement  management,  but  do  not  consciously  apply  common  RE  practices.  [Contribution]  This  paper  extends  our  understanding  of  requirements  engineering  in  video  game  development  and  contributes  to  the  requirements  engineering  body  of  knowledge.
1	Towards  automated  requirements  checking  throughout  development  processes  of  interactive  systems.  The  user-centered  development  process  of  interactive  systems  is  iterative  and,  during  multiple  iterations,  users  have  the  opportunity  to  bring  new  requirements  that  are  very  likely  to  have  an  impact,  not  only  in  future  development,  but  also  affect  previously  developed  artifacts.  Manual  testing  of  all  artifacts  when  new  requirements  are  introduced  can  be  cumbersome  and  time  consuming.  For  that,  we  need  flexible  methods  to  ensure  continuous  consistency  and  accuracy  among  the  various  artifacts  employed  to  build  interactive  systems.  The  ultimate  goal  of  this  position  paper  is  to  briefly  present  our  vision  on  an  approach  for  automating  the  requirements  assessment  using  a  Behavior-Driven  Development  perspective.  Thereby,  automated  tests  can  run  early  in  the  design  process,  providing  a  continuous  quality  assurance  of  requirements,  and  helping  clients  and  teams    to  identify  potential    problems  and  inconsistencies  before  commitments  with  software  implementation.
1	Increment  a  mixed  mde  ir  approach  for  regulatory  requirements  modeling  and  analysis.  [Context  and  motivation]  Regulatory  requirements  for  Nuclear  instrumentation  and  control  I&C  systems  are  first  class  requirements.  They  are  written  by  national  safety  entities  and  are  completed  through  a  large  documentation  set  of  national  recommendation  guides  and  national/international  standards.  [Question/Problem]  I&C  systems  important  to  safety  must  comply  to  all  of  these  requirements.  The  global  knowledge  of  this  domain  is  scattered  through  these  different  documents  and  not  formalized.  Its  organization  and  traceability  relationships  within  this  domain  is  mainly  implicit.  As  a  consequence,  such  long  lasting  nuclear  I&C  projects  set  important  challenges  in  terms  of  tacit  expertise  capitalization  and  domain  analysis.  [Principal  ideas/results]  To  tackle  this  domain  formalization  issue,  we  propose  a  dual  Model-driven  Engineering  MDE  and  Information  Retrieval  IR  approach  to  address  the  nuclear  regulatory  requirements  domain  definition,  and  assisted  traceability  based  on  the  acquired  requirements  model.  [Contributions]  In  this  paper,  we  present  the  Connexion  metamodel  that  provides  a  canvas  for  the  definition  and  capitalization  of  the  nuclear  regulatory  requirements  domain.  We  also  present  an  hybrid  MDE/IR-based  approach,  named  INCREMENT,  for  acquiring,  modeling  and  analyzing  these  regulatory  requirements.  This  approach  is  supported  by  a  tool  that  is  developed  in  the  context  of  the  CONNEXION  project,  which  gathers  French  major  nuclear  I&C  industrial  actors.
1	Requirements  quality  assurance  in  industry  why  what  and  how.  Context  and  Motivation:  Natural  language  is  the  most  common  form  to  specify  requirements  in  industry.  The  quality  of  the  specification  depends  on  the  capability  of  the  writer  to  formulate  requirements  aimed  at  different  stakeholders:  they  are  an  expression  of  the  customer’s  needs  that  are  used  by  analysts,  designers  and  testers.  Given  this  central  role  of  requirements  as  a  mean  to  communicate  intention,  assuring  their  quality  is  essential  to  reduce  misunderstandings  that  lead  to  potential  waste.  Problem:  Quality  assurance  of  requirement  specifications  is  largely  a  manual  effort  that  requires  expertise  and  domain  knowledge.  However,  this  demanding  cognitive  process  is  also  congested  by  trivial  quality  issues  that  should  not  occur  in  the  first  place.  Principal  ideas:  We  propose  a  taxonomy  of  requirements  quality  assurance  complexity  that  characterizes  cognitive  load  of  verifying  a  quality  aspect  from  the  human  perspective,  and  automation  complexity  and  accuracy  from  the  machine  perspective.  Contribution:  Once  this  taxonomy  is  realized  and  validated,  it  can  serve  as  the  basis  for  a  decision  framework  of  automated  requirements  quality  assurance  support.
1	Adapt  cases  extending  use  cases  for  adaptive  systems.  Adaptivity  is  prevalent  in  today's  software.  Mobile  devices  self-adapt  to  available  network  connections,  washing  machines  adapt  to  the  amount  of  laundry,  etc.  Current  approaches  for  engineering  such  systems  facilitate  the  specification  of  adaptivity  in  the  analysis  and  the  technical  design.  However,  the  modeling  of  platform  independent  models  for  adaptivity  in  the  logical  design  phase  remains  rather  neglected  causing  a  gap  between  the  analysis  and  the  technical  design  phase.      To  overcome  this  situation,  we  propose  an  approach  called  Adapt  Cases.  Adapt  Cases  allow  the  explicit  modeling  of  adaptivity  with  domain-specific  means,  enabling  adaptivity  to  gather  attention  early  in  the  software  engineering  process.  Since  our  approach  is  based  on  the  concept  of  use  cases  it  is  easy  adoptable  in  new  and  even  running  projects  that  use  the  UML  as  a  specification  language,  and  additionally,  can  be  easily  incorporated  into  model-based  development  environments.
1	Assured  and  correct  dynamic  update  of  controllers.  In  many  application  domains,  continuous  operation  is  a  desirable  attribute  for  software-intensive  systems.  As  the  environment  or  system  requirements  change,  so  the  system  should  change  and  adapt  without  stopping  or  unduly  disturbing  its  operation.  There  is,  therefore,  a  need  for  sound  engineering  techniques  that  can  cope  with  dynamic  change.  In  this  paper  we  address  the  problem  of  dynamic  update  of  controllers  in  reactive  systems  when  the  specification  (environment  assumptions,  requirements  and  interface)  of  the  current  system  changes.  We  present  a  general  approach  to  specifying  correctness  criteria  for  dynamic  update  and  a  technique  for  automatically  computing  a  controller  that  handles  the  transition  from  the  old  to  the  new  specification,  assuring  that  the  system  will  reach  a  state  in  which  such  a  transition  can  correctly  occur.  Indeed,  using  controller  synthesis  we  show  how  to  automatically  build  a  controller  that  guarantees  both  progress  towards  update  and  safe  update.  Seven  case  studies  have  been  implemented  to  validate  the  approach.
1	Gocc  a  configuration  compiler  for  self  adaptive  systems  using  goal  oriented  requirements  description.  Self-adaptive  systems  have  recently  attracted  attention  because  of  their  ability  to  cope  with  changing  environments,  including  system  intrusions  or  faults.  Such  software  must  modify  itself  to  better  fit  its  environment,  and  one  of  the  approaches  by  which  we  expect  this  capability  to  be  achieved  is  the  introduction  of  multiple  control  loops  to  assess  the  situation  and  to  determine  whether  a  change  in  behaviors  or  configurations  is  necessary  and  how  to  implement  the  change.  Development  of  such  systems  with  multiple  control  loops  complicates  the  task  of  identifying  components,  and  could  be  greatly  aided  by  appropriate  tool  support.  In  this  paper,  we  propose  an  architectural  compiler  for  self-adaptive  systems,  which  generates  architectural  configurations  from  the  goal-oriented  requirements  descriptions.  We  also  present  a  framework  for  generating  such  configurations  with  this  compiler  and  a  pattern  in  the  requirements  description.  We  evaluate  the  framework  experimentally  and  show  that  it  helps  to  generate  suitable  configurations  that  have  high  performance,  and  that  the  compiler  scales  well  to  large  input  models.
1	Guaranteeing  robustness  in  a  mobile  learning  application  using  formally  verified  mape  loops.  Mobile  learning  applications  support  traditional  indoor  lectures  with  outdoor  activities  using  mobile  devices.  An  example  scenario  is  a  team  of  students  that  use  triangulation  techniques  to  learn  properties  of  geometrical  figures.  In  previous  work,  we  developed  an  agent-based  mobile  learning  application  in  which  students  use  GPS-enabled  phones  to  calculate  distances  between  them.  From  practical  experience,  we  learned  that  the  required  level  of  GPS  accuracy  is  not  always  guaranteed,  which  undermines  the  use  of  the  application.  In  this  paper,  we  explain  how  we  have  extended  the  existing  application  with  a  self-adaptation  layer,  making  the  system  robust  to  degrading  GPS  accuracy.  The  self-adaptive  layer  is  conceived  as  a  set  of  interacting  MAPE  loops  (Monitor-Analysis-Plan-Execute),  distributed  over  the  phones.  To  guarantee  the  robustness  requirements,  we  formally  specify  the  self-adaptive  behaviors  using  timed  automata,  and  the  required  properties  using  timed  computation  tree  logic.  We  use  the  Uppaal  tool  to  model  the  self-adaptive  system  and  verify  the  robustness  requirements.  Finally,  we  discuss  how  the  formal  design  supported  the  implementation  of  the  self-adaptive  layer  on  top  of  the  existing  application.
1	Graf  graph  based  runtime  adaptation  framework.  One  approach  for  achieving  runtime  adaptability  in  software  is  to  use  application  frameworks  that  are  tailored  for  the  development  of  self-adaptive  systems.  In  this  paper,  we  present  the  Graph-based  Runtime  Adaptation  Framework  (GRAF),  which  enables  adaptivity  by  creating,  managing,  and  interpreting  graph-based  models  of  software  at  runtime.  Having  a  generic  graph  representation  in  our  approach  allows  for  flexible  adaptation  via  query  and  transformation  operations.  The  framework  is  especially  suited  for  the  migration  of  legacy  applications  towards  adaptive  software  and  attempts  to  reduce  necessary  changes  to  the  original  software.  As  a  proof  of  concept,  we  conduct  a  comprehensive  case  study  of  migrating  the  legacy  game  Jake2  to  achieve  runtime  adaptivity  using  GRAF.
1	Explanations  for  human  on  the  loop  a  probabilistic  model  checking  approach.  Many  self-adaptive  systems  benefit  from  human  involvement  and  oversight,  where  a  human  operator  can  provide  expertise  not  available  to  the  system  and  can  detect  problems  that  the  system  is  unaware  of.  One  way  of  achieving  this  is  by  placing  the  human  operator  on  the  loop  -  i.e.,  providing  supervisory  oversight  and  intervening  in  the  case  of  questionable  adaptation  decisions.  To  make  such  interaction  effective,  explanation  is  sometimes  helpful  to  allow  the  human  to  understand  why  the  system  is  making  certain  decisions  and  calibrate  confidence  from  the  human  perspective.  However,  explanations  come  with  costs  in  terms  of  delayed  actions  and  the  possibility  that  a  human  may  make  a  bad  judgement.  Hence,  it  is  not  always  obvious  whether  explanations  will  improve  overall  utility  and,  if  so,  what  kinds  of  explanation  to  provide  to  the  operator.  In  this  work,  we  define  a  formal  framework  for  reasoning  about  explanations  of  adaptive  system  behaviors  and  the  conditions  under  which  they  are  warranted.  Specifically,  we  characterize  explanations  in  terms  of  explanation  content,  effect,  and  cost.  We  then  present  a  dynamic  adaptation  approach  that  leverages  a  probabilistic  reasoning  technique  to  determine  when  the  explanation  should  be  used  in  order  to  improve  overall  system  utility.
1	Quality  aware  runtime  adaptation  in  complex  event  processing.  Complex  event  processing  (CEP)  is  a  fundamental  paradigm  for  a  software  system  to  self-adapt  to  environmental  changes.  CEP  provides  efficient  means  to  detect  (complex)  events  corresponding  to  environmental  changes  by  performing  a  real-time  analysis  on  many,  possibly  heterogeneous,  data  sources.  The  way  current  CEP  systems  detect  events  is  determined  at  design  time  without  accounting  for  dynamic  changes  of  the  environment  monitored  by  the  CEP  system.  This  can  lead  to  situations  where  the  performance,  quality  and  reliability  of  event  detection  significantly  drop  (e.g.,  due  to  mobility)  since  initial  assumptions  of  the  environment  are  violated  or  stated  too  general.  In  this  paper,  we  propose  A  daptive  CEP,  a  CEP  system  that  is  able  to  self-adapt  to  detected  changes  in  environmental  conditions.  We  propose  a  CEP  query  language  that  allows  specifying  changes  in  the  behavior  of  the  CEP  system  and  its  mechanisms  in  detecting  events  dependent  on  environmental  conditions.  This  way,  A  daptive  CEP  can  select  the  best-suited  configurations  for  given  quality  demands.  In  our  evaluation,  we  show  by  means  of  a  reference  concept  how  the  flexibility  exposed  by  the  query  language  helps  to  achieve  significant  performance  gains.
1	Managing  feature  models  with  familiar  a  demonstration  of  the  language  and  its  tool  support.  Developing  software  product  lines  involves  modeling  a  large  number  of  features,  usually  using  feature  models,  that  represent  different  viewpoints,  sub-systems  or  concerns  of  the  software  system.  To  manage  complexity  on  a  large  scale,  there  is  a  need  to  separate,  relate  and  compose  several  feature  models  while  automating  the  reasoning  on  their  compositions.  This  demonstration  gives  an  overview  of  a  Domain-Specific  Language,  familiar,  that  is  dedicated  to  the  management  of  feature  models.  Its  comprehensive  programming  environment,  based  on  Eclipse,  is  also  described.  It  complements  existing  tool  support  (i.e.,  FeatureIDE).
1	Managing  emerging  configuration  dependencies  in  multi  product  lines.  Large-scale  software-intensive  systems  often  consist  of  multiple  heterogeneous  and  loosely  coupled  systems,  which  work  together  to  form  a  system  of  systems  (SoS).  The  individual  systems  often  represent  configurable  units  that  need  be  adapted  to  customer  requirements.  In  such  multi  product  line  environments,  configuration  dependencies  between  the  product  lines  need  to  be  discovered  and  enforced.  Based  on  an  analysis  of  the  SoSs  of  our  industry  partners  in  the  domains  of  medical  systems  and  industrial  automation,  we  identify  several  types  of  configuration  dependencies  between  product  lines.  In  particular,  we  point  out  the  importance  of  emerging  dependencies,  which  are  not  known  in  advance,  but  are  detected  in  the  course  of  the  configuration  process.  For  handling  such  emerging  dependencies,  we  developed  tool  extensions  to  the  DOPLER  tool  suite  that  provide  support  for  the  distributed  configuration  of  multi  product  lines  by  multiple  users  and  that  support  the  inference  of  dependencies  from  emerging  dependencies  by  observing  user  actions.  We  conducted  a  preliminary  evaluation  to  assess  our  tool-supported  approach.  Our  extensible  set  of  dependency  types  turned  out  to  be  suitable  for  modeling  configuration  dependencies  between  product  lines  in  the  study  system.  The  evaluation  further  showed  the  usefulness  of  the  tool  for  capturing  emerging  dependencies  by  multiple  users  during  distributed  configuration.
1	Lazy  tso  reachability.  We  address  the  problem  of  checking  state  reachability  for  programs  running  under  Total  Store  Order  TSO.  The  problem  has  been  shown  to  be  decidable  but  the  cost  is  prohibitive,  namely  non-primitive  recursive.  We  propose  here  to  give  up  completeness.  Our  contribution  is  a  new  algorithm  for  TSO  reachability:  it  uses  the  standard  SC  semantics  and  introduces  the  TSO  semantics  lazily  and  only  where  needed.  At  the  heart  of  our  algorithm  is  an  iterative  refinement  of  the  program  of  interest.  If  the  program's  goal  state  is  SC-reachable,  we  are  done.  If  the  goal  state  is  not  SC-reachable,  this  may  be  due  to  the  fact  that  SC  underapproximates  TSO.  We  employ  a  second  algorithm  that  determines  TSO  computations  which  are  infeasible  under  SC,  and  hence  likely  to  lead  to  new  states.  We  enrich  the  program  to  emulate,  under  SC,  these  TSO  computations.  Altogether,  this  yields  an  iterative  under-approximation  that  we  prove  sound  and  complete  for  bug  hunting,  i.e.,  a  semi-decision  procedure  halting  for  positive  cases  of  reachability.We  have  implemented  the  procedure  as  an  extension  to  the  tool  Trencher  [1]  and  compared  it  to  the  Memorax  [2]  and  CBMC  [14]  model  checkers.
1	Map  based  transparent  persistence  for  very  large  models.  The  progressive  industrial  adoption  of  Model-Driven  Engineering  MDE  is  fostering  the  development  of  large  tool  ecosystems  like  the  Eclipse  Modeling  project.  These  tools  are  built  on  top  of  a  set  of  base  technologies  that  have  been  primarily  designed  for  small-scale  scenarios,  where  models  are  manually  developed.  In  particular,  efficient  runtime  manipulation  for  large-scale  models  is  an  under-studied  problem  and  this  is  hampering  the  application  of  MDE  to  several  industrial  scenarios.    In  this  paper  we  introduce  and  evaluate  a  map-based  persistence  model  for  MDE  tools.  We  use  this  model  to  build  a  transparent  persistence  layer  for  modeling  tools,  on  top  of  a  map-based  database  engine.  The  layer  can  be  plugged  into  the  Eclipse  Modeling  Framework,  lowering  execution  times  and  memory  consumption  levels  of  other  existing  approaches.  Empirical  tests  are  performed  based  on  a  typical  industrial  scenario,  model-driven  reverse  engineering,  where  very  large  software  models  originate  from  the  analysis  of  massive  code  bases.  The  layer  is  freely  distributed  and  can  be  immediately  used  for  enhancing  the  scalability  of  any  existing  Eclipse  Modeling  tool.
1	Javanni  a  verifier  for  javascript.  JavaScript  ranks  among  the  most  popular  programming  languages  for  the  web,  yet  its  highly  dynamic  type  system  and  occasionally  unintuitive  semantics  make  programming  particularly  error-prone.  This  paper  presents  Javanni,  a  verifier  for  JavaScript  programs  that  can  statically  detect  many  common  programming  errors.  Javanni  checks  the  absence  of  standard  type-related  errors  (such  as  accessing  undefined  fields)  without  requiring  user-written  annotations,  and  it  can  also  verify  full  functional-correctness  specifications.  Several  experiments  with  JavaScript  applications  reported  in  the  paper  demonstrate  that  Javanni  is  flexibly  usable  on  programs  with  non-trivial  specifications.  Javanni  is  available  online  within  the  CloudStudio  web  integrated  environment.
1	Business  process  privacy  analysis  in  pleak.  Pleak  is  a  tool  to  capture  and  analyze  privacy-enhanced  business  process  models  to  characterize  and  quantify  to  what  extent  the  outputs  of  a  process  leak  information  about  its  inputs.  Pleak  incorporates  an  extensible  set  of  analysis  plugins,  which  enable  users  to  inspect  potential  leakages  at  multiple  levels  of  detail.
1	Deepfault  fault  localization  for  deep  neural  networks.  Deep  Neural  Networks  (DNNs)  are  increasingly  deployed  in  safety-critical  applications  including  autonomous  vehicles  and  medical  diagnostics.  To  reduce  the  residual  risk  for  unexpected  DNN  behaviour  and  provide  evidence  for  their  trustworthy  operation,  DNNs  should  be  thoroughly  tested.  The  DeepFault  whitebox  DNN  testing  approach  presented  in  our  paper  addresses  this  challenge  by  employing  suspiciousness  measures  inspired  by  fault  localization  to  establish  the  hit  spectrum  of  neurons  and  identify  suspicious  neurons  whose  weights  have  not  been  calibrated  correctly  and  thus  are  considered  responsible  for  inadequate  DNN  performance.  DeepFault  also  uses  a  suspiciousness-guided  algorithm  to  synthesize  new  inputs,  from  correctly  classified  inputs,  that  increase  the  activation  values  of  suspicious  neurons.  Our  empirical  evaluation  on  several  DNN  instances  trained  on  MNIST  and  CIFAR-10  datasets  shows  that  DeepFault  is  effective  in  identifying  suspicious  neurons.  Also,  the  inputs  synthesized  by  DeepFault  closely  resemble  the  original  inputs,  exercise  the  identified  suspicious  neurons  and  are  highly  adversarial.
1	Change  and  delay  contracts  for  hybrid  system  component  verification.  In  this  paper,  we  present  reasoning  techniques  for  a  component-based  modeling  and  verification  approach  for  hybrid  systems  comprising  discrete  dynamics  as  well  as  continuous  dynamics,  in  which  the  components  have  local  responsibilities.  Our  approach  supports  component  contracts  i.e.,  input  assumptions  and  output  guarantees  of  interfaces  that  are  more  general  than  previous  component-based  hybrid  systems  verification  techniques  in  the  following  ways:  We  introduce  change  contracts,  which  characterize  how  current  values  exchanged  between  components  along  ports  relate  to  previous  values.  We  also  introduce  delay  contracts,  which  describe  the  change  relative  to  the  time  that  has  passed  since  the  last  value  was  exchanged.  Together,  these  contracts  can  take  into  account  what  has  changed  between  two  components  in  a  given  amount  of  time  since  the  last  exchange  of  information.  Most  crucially,  we  prove  that  the  safety  of  compatible  components  implies  safety  of  the  composite.  The  proof  steps  of  the  theorem  are  also  implemented  as  a  tactic  in  KeYmaerai¾?X,  allowing  automatic  generation  of  a  KeYmaerai¾?X  proof  for  the  composite  system  from  proofs  of  the  concrete  components.
1	Symbolic  model  generation  for  graph  properties.  Graphs  are  ubiquitous  in  Computer  Science.  For  this  reason,  in  many  areas,  it  is  very  important  to  have  the  means  to  express  and  reason  about  graph  properties.  In  particular,  we  want  to  be  able  to  check  automatically  if  a  given  graph  property  is  satisfiable.  Actually,  in  most  application  scenarios  it  is  desirable  to  be  able  to  explore  graphs  satisfying  the  graph  property  if  they  exist  or  even  to  get  a  complete  and  compact  overview  of  the  graphs  satisfying  the  graph  property.    We  show  that  the  tableau-based  reasoning  method  for  graph  properties  as  introduced  by  Lambers  and  Orejas  paves  the  way  for  a  symbolic  model  generation  algorithm  for  graph  properties.  Graph  properties  are  formulated  in  a  dedicated  logic  making  use  of  graphs  and  graph  morphisms,  which  is  equivalent  to  first-order  logic  on  graphs  as  introduced  by  Courcelle.  Our  parallelizable  algorithm  gradually  generates  a  finite  set  of  so-called  symbolic  models,  where  each  symbolic  model  describes  a  set  of  finite  graphs  i.e.,  finite  models  satisfying  the  graph  property.  The  set  of  symbolic  models  jointly  describes  all  finite  models  for  the  graph  property  complete  and  does  not  describe  any  finite  graph  violating  the  graph  property  sound.  Moreover,  no  symbolic  model  is  already  covered  by  another  one  compact.  Finally,  the  algorithm  is  able  to  generate  from  each  symbolic  model  a  minimal  finite  model  immediately  and  allows  for  an  exploration  of  further  finite  models.  The  algorithm  is  implemented  in  the  new  tool  AutoGraph.
1	Transformation  of  attributed  structures  with  cloning.  Copying,  or  cloning,  is  a  basic  operation  used  in  the  specification  of  many  applications  in  computer  science.  However,  when  dealing  with  complex  structures,  like  graphs,  cloning  is  not  a  straightforward  operation  since  a  copy  of  a  single  vertex  may  involve  (implicitly)  copying  many  edges.  Therefore,  most  graph  transformation  approaches  forbid  the  possibility  of  cloning.  We  tackle  this  problem  by  providing  a  framework  for  graph  transformations  with  cloning.  We  use  attributed  graphs  and  allow  rules  to  change  attributes.  These  two  features  (cloning/changing  attributes)  together  give  rise  to  a  powerful  formal  specification  approach.  In  order  to  handle  different  kinds  of  graphs  and  attributes,  we  first  define  the  notion  of  attributed  structures  in  an  abstract  way.  Then  we  generalise  the  sesqui-pushout  approach  of  graph  transformation  in  the  proposed  general  framework  and  give  appropriate  conditions  under  which  attributed  structures  can  be  transformed.  Finally,  we  instantiate  our  general  framework  with  different  examples,  showing  that  many  structures  can  be  handled  and  that  the  proposed  framework  allows  one  to  specify  complex  operations  in  a  natural  way.
1	Hybridtiger  hybrid  model  checking  and  domination  based  partitioning  for  efficient  multi  goal  test  suite  generation  competition  contribution.  In  theory,  software  model  checkers  are  well-suited  for  automated  test-case  generation.  The  idea  is  to  perform  (non-)reachability  queries  for  the  test  goals  and  extract  test  cases  from  resulting  counterexamples.  However,  in  case  of  realistic  programs,  even  simple  coverage  criteria  (e.g.,  branch  coverage)  force  model  checkers  to  deal  with  several  hundreds  or  even  thousands  of  test  goals.  Processing  each  of  these  test  goals  in  isolation  with  model  checking  techniques  does  not  scale.  Therefore,  our  tool  HybridTiger  builds  on  recent  ideas  on  multi-property  verification.  However,  since  every  additional  property  (i.e.,  test  goal)  reduces  the  model  checker’s  abstraction  possibilities,  we  split  the  set  of  all  test  goals  into  different  partitions.  In  Test-Comp  2019,  we  applied  a  random  partitioning  strategy  and  used  predicate  analysis  as  model  checking  technique.  In  Test-Comp  2020,  we  improved  our  technique  in  two  ways.  First,  we  exploit  domination  information  among  control-flow  locations  in  our  partitioning  strategy  to  group  test  goals  being  located  on  (preferably)  similar  paths.  Second,  we  account  to  inherent  weaknesses  of  the  predicate  analysis  by  applying  a  hybrid  software  model-checking  approach  that  switches  between  explicit  model  checking  and  predicate-based  model  checking  on-the-fly.  Our  tool  HybridTiger  is  integrated  into  the  software  analysis  framework  CPAchecker.
1	Conceptual  modeling  in  agile  information  systems  development.  The  nature  and  the  role  of  conceptual  modeling  in  information  systems  development  has  neither  in  theory  nor  in  practice  been  established  satisfactorily.  There  are  diverse  views  on  what  conceptual  modeling  is  and  on  how  to  perform  it.  In  one  extreme,  there  is  the  view  that  conceptual  modeling  is  an  (optional)  activity  whose  main  purpose  is  to  improve  communication  between  the  parties  involved  in  the  development  process.  In  the  other  extreme,  there  is  the  view  (shared  by  us)  that  conceptual  modeling  is  an  activity  that  is  necessarily  performed  in  all  cases,  and  whose  purpose  is  to  define  the  conceptual  schema,  that  is,  the  general  knowledge  a  system  needs  to  know  to  perform  its  functions.  The  latter  has  been  captured  in  what  we  call  the  principle  of  necessity  of  conceptual  Schemas,  which  states  that  "To  develop  an  information  system  it  is  necessary  to  define  its  conceptual  schema".  Agile  development  processes  have  added  even  more  confusion  to  conceptual  modeling.  The  value  of  "Working  software  over  comprehensive  documentation",  stated  in  the  manifesto  for  agile  software  development,  seems  to  undermine  conceptual  Schemas  in  favor  of  working  code.  However,  as  we  explain  in  the  talk,  it  does  not  need  to  be  so.  In  the  talk,  we  present  a  framework  that  describes  the  contents  of  conceptual  Schemas,  the  form  they  may  take  and  the  roles  they  play  in  information  systems  development.  Based  on  that  framework,  we  review  the  principle  of  necessity  of  conceptual  Schemas.  We  then  apply  the  framework  to  the  particular  case  of  agile  development,  and  discuss  the  validity  of  the  principle  of  necessity  in  that  case.  The  framework  is  intended  to  be  useful  for  inspiring  future  research,  and  for  improving  the  practice  and  teaching  of  conceptual  modeling.
1	Steering  through  incentives  in  large  scale  lean  software  development.  The  application  of  lean  principles  and  agile  project  management  techniques  in  the  domain  of  large-scale  software  product  development  has  gained  tremendous  momentum  over  the  last  decade.  This  results  in  empowerment  of  individuals  which  leads  to  increased  flexibility  but  at  the  same  time  sacrifices  managerial  control  through  traditional  steering  practices.  Hence,  the  design  of  adequate  incentive  schemes  in  order  to  align  local  optimization  and  opportunistic  behavior  with  the  overall  strategy  of  the  company  is  a  crucial  activity  from  a  business  perspective.
1	Where  the  truth  lies  aop  and  its  impact  on  software  modularity.  Modularity  is  the  single  attribute  of  software  that  allows  a  program  to  be  intellectually  manageable  [29].  The  recipe  for  modularizing  is  to  define  a  narrow  interface,  hide  an  implementation  detail,  keep  low  coupling  and  high  cohesion.  Over  a  decade  ago,  aspect-oriented  programming  (AOP)  was  proposed  in  the  literature  to  "modularize  the  un-modularizable"  [24].  Since  then,  aspect-oriented  languages  have  been  providing  new  abstraction  and  composition  mechanisms  to  deal  with  concerns  that  could  not  be  modularized  because  of  the  limited  abstractions  of  the  underlying  programming  language.  This  paper  is  a  continuation  of  our  earlier  work  [32]  and  further  investigates  AO  software  with  regard  to  coupling  and  cohesion.  We  compare  two  versions  (Java  and  AspectJ)  of  ten  applications  to  review  AOP  within  the  context  of  software  modularity.  It  turns  out  that  the  claim  that  "the  software  built  in  AOP  is  more  modular  than  the  software  built  in  OOP"  is  a  myth.
1	Refactoring  business  process  models  a  systematic  review.  Business  processes  are  nowadays  recognized  as  one  of  the  intangible  business  assets  that  provide  more  competitive  advantage  to  organizations.  Organizations  must  therefore  be  able  to  manage  their  business  process  models  and  deal  with  their  quality  problems,  i.e.  lack  of  understandability,  maintainability  or  reusability  among  others.  Such  quality  problems  are  exacerbated  in  business  processes  models  that  were  mined  by  reverse  engineering  from  enterprise  information  systems,  since  business  process  are  more  likely  to  undergo  inconsistencies,  redundancies,  etc.  Refactoring  has  proved  to  be  a  suitable  solution  to  cope  with  these  quality  problems.  Refactoring  changes  the  internal  structure  of  a  business  process  model  while  preserves  its  external  behaviour.  This  paper  presents  an  in-depth  systematic  review  for  collecting,  categorizing  and  analyzing  all  the  refactoring  methods  and  techniques  applied  to  business  process  models.  The  systematic  review  is  conducted  following  the  formal  methodology  proposed  by  Kitchenhan.  The  review  reports  206  related  studies,  from  which  16  were  considered  as  primary  studies.  The  most  valuable  conclusion  is  that  none  of  these  studies  proposes  refactoring  techniques  for  business  process  models  previously  obtained  by  reverse  engineering,  which  is  considered  as  a  greenfield  research  area.
1	Semiotics  in  visualisation.  Digital  visualisation  is  a  way  of  representing  data  and  information  with  the  aid  of  digital  means.  It  ranges  from  a  simple  form  such  as  a  graph  or  chart  to  a  complex  form  like  animated  visualisations  that  allows  user  to  interact  with  the  underlying  data  through  direct  manipulation  (Chen  et  al.,  2008).  The  notion  of  digital  visualisation  engages  human  interpretation  on  information  in  order  to  gain  insights  in  a  particular  context  (Robert,  2007,  Ware,  2012,  Czernicki,  2010).  Hence,  it  is  a  complex  process  involving  multiple  disciplines,  including  the  socio-technical  element.  The  social  element  relates  to  human  perception  in  interpreting  information.  The  technical  element  on  the  other  hand,  refers  to  the  technology  used  to  enable  visualisation,  for  example  the  SAS  suite  (SAS,  2014)  that  offers  visual  analytics  to  support  interactive  dashboard  and  reporting.
1	Effects  of  geographical  socio  cultural  and  temporal  distances  on  communication  in  global  software  development  during  requirements  change  management  a  pilot  study.  Trend  of  software  development  is  changing  rapidly  most  of  the  software  development  organizations  are  trying  to  globalize  their  activities  throughout  the  world.  This  trend  leads  towards  a  phenomenon  called  Global  Software  Development  (GSD).  The  main  reason  behind  the  software  globalization  is  its  various  benefits.  Besides  these  benefits,  software  organizations  are  facing  various  challenges.  One  of  these  challenges  is  communication  which  is  considered  a  big  challenge  in  GSD  and  it  becomes  more  complicated  during  the  Requirements  Change  Management  (RCM)  process  due  to  three  factors,  they  are  Geographical,  Socio-cultural  and  Temporal  distances.  This  paper  presents  a  framework  which  shows  the  effect  of  these  factors  on  communication  during  RCM  process  in  GSD.  Communication  is  the  core  function  of  collaboration  which  allows  information  to  be  exchanged  between  the  team  members.  A  pilot  study  has  been  conducted  in  three  GSD  organizations.  A  quantitative  research  method  has  been  used  to  collect  data.  The  findings  from  the  survey  data  show  that  these  three  factors  have  a  strong  negative  impact  on  communication  process  in  GSD.
1	Verification  of  a  failure  management  protocol  for  stateful  iot  applications.  Fog  computing  provides  computing,  storage  and  communication  resources  at  the  edge  of  the  network,  near  the  physical  world.  Devices  deployed  in  the  Fog  have  interesting  properties  such  as  short  delays,  responsiveness,  optimised  communications  and  privacy.  However,  these  devices  have  low  stability  and  are  prone  to  failures.  Thus,  there  is  a  need  for  management  protocols  to  tolerate  failures  of  IoT  applications  in  the  Fog.  We  propose  a  failure  management  protocol  which  recovers  from  failures  of  devices  and  software  elements  involved  in  an  IoT  application.  Designing  such  highly  distributed  management  protocols  is  a  difficult  and  error-prone  task.  Therefore,  the  main  contribution  of  this  paper  is  the  formal  specification  and  verification  of  this  failure  management  protocol.  Formal  specification  is  achieved  using  a  process  algebraic  language.  The  corresponding  formal  model  was  used  to  carry  out  extensive  analysis  of  the  protocol  to  ensure  that  it  preserves  important  architectural  invariants  and  functional  properties.  The  verification  step  was  performed  using  model  checking  techniques.  The  analysis  of  the  protocol  was  a  success  because  it  allowed  us  to  detect  and  correct  several  issues  in  the  protocol.
1	Adaptive  learning  for  learn  based  regression  testing.  Regression  testing  is  an  important  activity  to  prevent  the  introduction  of  regressions  into  software  updates.  Learn-based  testing  can  be  used  to  automatically  check  new  versions  of  a  system  for  regressions  on  a  system  level.  This  is  done  by  learning  a  model  of  the  system  and  model  checking  this  model  for  system  property  violations.
1	Compositionality  for  quantitative  specifications.  We  provide  a  framework  for  compositional  and  iterative  design  and  verification  of  systems  with  quantitative  information,  such  as  rewards,  time  or  energy.  It  is  based  on  disjunctive  modal  transition  systems  where  we  allow  actions  to  bear  various  types  of  quantitative  information.  Throughout  the  design  process  the  actions  can  be  further  refined  and  the  information  made  more  precise.  We  show  how  to  compute  the  results  of  standard  operations  on  the  systems,  including  the  quotient  (residual),  which  has  not  been  previously  considered  for  quantitative  non-deterministic  systems.  Our  quantitative  framework  has  close  connections  to  the  modal  nu-calculus  and  is  compositional  with  respect  to  general  notions  of  distances  between  systems  and  the  standard  operations.
1	The  jira  repository  dataset  understanding  social  aspects  of  software  development.  Issue  tracking  systems  store  valuable  data  for  testing  hypotheses  concerning  maintenance,  building  statistical  prediction  models  and  recently  investigating  developers  "affectiveness".  In  particular,  the  Jira  Issue  Tracking  System  is  a  proprietary  tracking  system  that  has  gained  a  tremendous  popularity  in  the  last  years  and  offers  unique  features  like  the  project  management  system  and  the  Jira  agile  kanban  board.  This  paper  presents  a  dataset  extracted  from  the  Jira  ITS  of  four  popular  open  source  ecosystems  (as  well  as  the  tools  and  infrastructure  used  for  extraction)  the  Apache  Software  Foundation,  Spring,  JBoss  and  CodeHaus  communities.  Our  dataset  hosts  more  than  1K  projects,  containing  more  than  700K  issue  reports  and  more  than  2  million  issue  comments.  Using  this  data,  we  have  been  able  to  deeply  study  the  communication  process  among  developers,  and  how  this  aspect  affects  the  development  process.  Furthermore,  comments  posted  by  developers  contain  not  only  technical  information,  but  also  valuable  information  about  sentiments  and  emotions.  Since  sentiment  analysis  and  human  aspects  in  software  engineering  are  gaining  more  and  more  importance  in  the  last  years,  with  this  repository  we  would  like  to  encourage  further  studies  in  this  direction.
1	Search  based  training  data  selection  for  cross  project  defect  prediction.  Context:  Previous  studies  have  shown  that  steered  training  data  or  dataset  selection  can  lead  to  better  performance  for  cross  project  defect  prediction  (CPDP).  On  the  other  hand,  data  quality  is  an  issue  to  consider  in  CPDP.      Aim:  We  aim  at  utilising  the  Nearest  Neighbor  (NN)-Filter,  embedded  in  a  genetic  algorithm,  for  generating  evolving  training  datasets  to  tackle  CPDP,  while  accounting  for  potential  noise  in  defect  labels.      Method:  We  propose  a  new  search  based  training  data  (i.e.,  instance)  selection  approach  for  CPDP  called  GIS  (Genetic  Instance  Selection)  that  looks  for  solutions  to  optimize  a  combined  measure  of  F-Measure  and  GMean,  on  a  validation  set  generated  by  (NN)-filter.  The  genetic  operations  consider  the  similarities  in  features  and  address  possible  noise  in  assigned  defect  labels.  We  use  13  datasets  from  PROMISE  repository  in  order  to  compare  the  performance  of  GIS  with  benchmark  CPDP  methods,  namely  (NN)-filter  and  naive  CPDP,  as  well  as  with  within  project  defect  prediction  (WPDP).      Results:  Our  results  show  that  GIS  is  significantly  better  than  (NN)-Filter  in  terms  of  F-Measure  (p  --  value  L  0.001,  Cohen's  d  =  0.697)  and  GMean  (p  --  value  L  0.001,  Cohen's  d  =  0.946).  It  also  outperforms  the  naive  CPDP  approach  in  terms  of  F-Measure  (p  --  value  L  0.001,  Cohen's  d  =  0.753)  and  GMean  (p  --  value  L  0.001,  Cohen's  d  =  0.994).  In  addition,  the  performance  of  our  approach  is  better  than  that  of  WPDP,  again  considering  F-Measure  (p  --  value  L  0.001,  Cohen's  d  =  0.227)  and  GMean  (p  --  value  L  0.001,  Cohen's  d  =  0.595)  values.      Conclusions:  We  conclude  that  search  based  instance  selection  is  a  promising  way  to  tackle  CPDP.  Especially,  the  performance  comparison  with  the  within  project  scenario  encourages  further  investigation  of  our  approach.  However,  the  performance  of  GIS  is  based  on  high  recall  in  the  expense  of  low  precision.  Using  different  optimization  goals,  e.g.  targeting  high  precision,  would  be  a  future  direction  to  investigate.
1	A  systematic  review  of  web  resource  estimation.  Background:  Web  development  plays  an  important  role  in  today's  industry,  so  an  in  depth  view  into  Web  resource  estimation  would  be  valuable.  However  a  systematic  review  (SR)  on  Web  resource  estimation  in  its  entirety  has  not  been  done.      Aim:  The  aim  of  this  paper  is  to  present  a  SR  of  Web  resource  estimation  in  order  to  define  the  current  state  of  the  art,  and  to  identify  any  research  gaps  that  may  be  present.      Method:  Research  questions  that  would  address  the  current  state  of  the  art  in  Web  resource  estimation  were  first  identified.  A  comprehensive  literature  search  was  then  executed  resulting  in  the  retrieval  of  84  empirical  studies  that  investigated  any  aspect  of  Web  resource  estimation.  Data  extraction  and  synthesis  was  performed  on  these  studies  with  these  research  questions  in  mind.      Results:  We  have  found  that  there  are  no  guidelines  with  regards  to  what  resource  estimation  technique  should  be  used  in  a  particular  estimation  scenario,  how  it  should  be  implemented,  and  how  its  effectiveness  should  be  evaluated.  Accuracy  results  vary  widely  and  are  dependent  on  numerous  factors.  Research  has  focused  on  development  effort/cost  estimation,  neglecting  other  facets  of  resource  estimation  like  quality  and  maintenance.  Size  measures  have  been  used  in  all  but  one  study  as  a  resource  predictor.      Conclusions:  Our  results  suggest  that  there  is  plenty  of  work  to  be  done  in  the  field  of  Web  resource  estimation  whether  it  be  investigating  a  more  comprehensive  approach  that  considers  more  than  a  single  resource  facet,  evaluating  other  possible  resource  predictors,  or  trying  to  determine  guidelines  that  would  help  simplify  the  process  of  selecting  a  resource  estimation  technique.
1	An  empirical  evaluation  of  distribution  based  thresholds  for  internal  software  measures.  Background  Setting  thresholds  is  important  for  the  practical  use  of  internal  software  measures,  so  software  modules  can  be  classified  as  having  either  acceptable  or  unacceptable  quality,  and  software  practitioners  can  take  appropriate  quality  improvement  actions.  Quite  a  few  methods  have  been  proposed  for  setting  thresholds  and  several  of  them  are  based  on  the  distribution  of  an  internal  measure's  values  (and,  possibly,  other  internal  measures),  without  any  explicit  relationship  with  any  external  software  quality  of  interest.      Objective  In  this  paper,  we  empirically  investigate  the  consequences  of  defining  thresholds  on  internal  measures  without  taking  into  account  the  external  measures  that  quantify  qualities  of  practical  interest.  We  focus  on  fault-proneness  as  the  specific  quality  of  practical  interest.      Method  We  analyzed  datasets  from  the  PROMISE  repository.  First,  we  computed  the  thresholds  of  code  measures  according  to  three  distribution-based  methods.  Then,  we  derived  statistically  significant  models  of  fault-proneness  that  use  internal  measures  as  independent  variables.  We  then  evaluated  the  indications  provided  by  the  distribution-based  thresholds  when  used  along  with  the  fault-proneness  models.      Results  Some  methods  for  defining  distribution-based  thresholds  requires  that  code  measures  be  normally  distributed.  However,  we  found  that  this  is  hardly  ever  the  case  with  the  PROMISE  datasets,  making  that  entire  class  of  methods  inapplicable.  We  adapted  these  methods  for  non-normal  distributions  and  obtained  thresholds  that  appear  reasonable,  but  are  characterized  by  a  large  variation  in  the  fault-proneness  risk  level  they  entail.  Given  a  dataset,  the  thresholds  for  different  internal  measures---when  used  as  independent  variables  of  statistically  significant  models---provide  fairly  different  values  of  fault-proneness.  This  is  quite  dangerous  for  practitioners,  since  they  get  thresholds  that  are  presented  as  equally  important,  but  practically  can  correspond  to  very  different  levels  of  user-perceivable  quality.  For  other  distribution-based  methods,  we  found  that  the  proposed  thresholds  are  practically  useless,  as  many  modules  with  values  of  internal  measures  deemed  acceptable  according  to  the  thresholds  actually  have  high  fault-proneness.  Also,  the  accuracy  of  all  of  these  methods  appears  to  be  lower  than  the  accuracy  obtained  by  simply  estimating  modules  at  random.      Conclusions  Our  results  indicate  that  distribution-based  thresholds  appear  to  be  unreliable  in  providing  sensible  indications  about  the  quality  of  software  modules.  Practitioners  should  instead  use  different  kinds  of  threshold-setting  methods,  such  as  the  ones  that  take  into  account  data  about  the  presence  of  faults  in  software  modules,  in  addition  to  the  values  of  internal  software  measures.
1	Size  and  cohesion  metrics  as  indicators  of  the  long  method  bad  smell  an  empirical  study.  Source  code  bad  smells  are  usually  resolved  through  the  application  of  well-defined  solutions,  i.e.,  refactorings.  In  the  literature,  software  metrics  are  used  as  indicators  of  the  existence  and  prioritization  of  resolving  bad  smells.  In  this  paper,  we  focus  on  the  long  method  smell  (i.e.  one  of  the  most  frequent  and  persistent  bad  smells)  that  can  be  resolved  by  the  extract  method  refactoring.  Until  now,  the  identification  of  long  methods  or  extract  method  opportunities  has  been  performed  based  on  cohesion,  size  or  complexity  metrics.  However,  the  empirical  validation  of  these  metrics  has  exhibited  relatively  low  accuracy  with  regard  to  their  capacity  to  indicate  the  existence  of  long  methods  or  extract  method  opportunities.  Thus,  we  empirically  explore  the  ability  of  size  and  cohesion  metrics  to  predict  the  existence  and  the  refactoring  urgency  of  long  method  occurrences,  through  a  case  study  on  java  open-source  methods.  The  results  of  the  study  suggest  that  one  size  and  four  cohesion  metrics  are  capable  of  characterizing  the  need  and  urgency  for  resolving  the  long  method  bad  smell,  with  a  higher  accuracy  compared  to  the  previous  studies.  The  obtained  results  are  discussed  by  providing  possible  interpretations  and  implications  to  practitioners  and  researchers.
1	Partial  order  reduction  for  deep  bug  finding  in  synchronous  hardware.  Symbolic  model  checking  has  become  an  important  part  of  the  verification  flow  in  industrial  hardware  design.  However,  its  use  is  still  limited  due  to  scaling  issues.  One  way  to  address  this  is  to  exploit  the  large  amounts  of  symmetry  present  in  many  real  world  designs.  In  this  paper,  we  adapt  partial  order  reduction  for  bounded  model  checking  of  synchronous  hardware  and  introduce  a  novel  technique  that  makes  partial  order  reduction  practical  in  this  new  domain.  These  approaches  are  largely  automatic,  requiring  only  minimal  manual  effort.  We  evaluate  our  technique  on  open-source  and  commercial  packet  mover  circuits  –  designs  containing  FIFOs  and  arbiters.
1	Forward  reachability  computation  for  autonomous  max  plus  linear  systems.  This  work  discusses  the  computation  of  forward  reachability  for  autonomous  (that  is,  deterministic)  Max-Plus-Linear  (MPL)  systems,  a  class  of  continuous-space  discrete-event  models  that  are  relevant  for  applications  dealing  with  synchronization  and  scheduling.  Given  an  MPL  model  and  a  set  of  initial  states,  we  characterize  and  compute  its  “reach  tube,”  namely  the  sequential  collection  of  the  sets  of  reachable  states  (these  sets  are  regarded  step-wise  as  “reach  sets”).  We  show  that  the  exact  computation  of  the  reach  sets  can  be  quickly  and  compactly  performed  by  manipulations  of  difference-bound  matrices,  and  derive  explicit  worst-case  bounds  for  the  complexity  of  these  operations.  The  concepts  and  techniques  are  implemented  within  the  toolbox  VeriSiMPL,  and  are  practically  elucidated  by  a  running  example.  We  further  display  the  computational  performance  of  the  approach  by  two  concluding  numerical  benchmarks:  the  technique  comfortably  handles  reachability  computations  over  twenty-dimensional  MPL  models  (i.e.,  models  with  twenty  continuous  variables),  and  it  clearly  outperforms  an  alternative  state-of-the-art  approach  in  the  literature.
1	Ultimate  kojak  competition  contribution.  Ultimate  Kojak  is  a  symbolic  software  model  checker  for  C  programs.  It  is  based  on  CEGAR  and  Craig  interpolation.  The  basic  algorithm,  described  in  an  earlier  work  [1],  was  extended  to  be  able  to  deal  with  recursive  programs  using  nested  word  automata  and  nested  (tree)  interpolants.  1  Verification  Approach  Ultimate  Kojak  computes  inductive  invariants  from  interpolants  to  prove  the  correctness  of  a  program.  A  program  is  represented  by  a  program  graph.  In  a  program  graph,  a  vertex  is  a  pair  consisting  of  a  program  location  and  an  invariant  describing  the  abstract  program  state.  An  edge  is  labelled  with  a  transition  formula  that  corresponds  to  a  block  of  program  statements.  A  program  assertion  is  represented  by  a  transition  to  an  error  state  where  the  transition  is  labelled  with  the  negated  assertion.  The  goal  is  to  show  the  unreachability  of  all  error  states.  The  program  graph  is  refined  by  the  algorithm  presented  in  a  paper  by  Ermis  et  al.  [1].  This  algorithm  computes  a  sequence  of  interpolants  for  an  infeasible  error  path  and  adds  them  to  the  invariant  annotated  at  the  corresponding  vertices.  Since  the  interpolants  are  only  invariants  for  the  particular  error  path,  we  also  have  to  add  new  vertices  for  the  case  where  the  interpolants  do  not  hold.  This  is  achieved  by  splitting  every  vertex  on  the  error  path  into  two  new  vertices  where  each  receives  a  new  invariant:  the  old  invariant  conjoined  with  the  interpolant  for  the  first,  and  the  old  invariant  conjoined  with  the  negated  interpolant  for  the  second.  Afterwards,  the  algorithm  removes  all  infeasible  edges,  thereby  refining  the  abstraction.  The  newest  version  of  Ultimate  Kojak  implements  this  algorithm  and  extends  it  by  handling  inter-procedural  control  flow.  We  use  nested  word  automata  to  represent  programs  containing  procedures  [2].  These  automata  have  call  and  return  transitions  and  support  procedure  summaries  to  prove  the  correctness  of  recursive  programs.  A  return  transition  conceptually  has  two  predecessors:  the  node  representing  the  call  site  and  the  node  representing  the  exit  point  of  the  called  procedure.  Therefore  our  error  paths  are  in  in  fact  trees.  To  obtain  Corresponding  author.  E.  Abraham  and  K.  Havelund  (Eds.):  TACAS  2014,  LNCS  8413,  pp.  421–423,  2014.  c  ©  Springer-Verlag  Berlin  Heidelberg  2014
1	Skink  static  analysis  of  programs  in  llvm  intermediate  representation.  Skink  is  a  static  analysis  tool  that  analyses  the  LLVM  intermediate  representation  LLVM-IR  of  a  program  source  code.  The  analysis  consists  of  checking  whether  there  is  a  feasible  execution  that  can  reach  a  designated  error  block  in  the  LLVM-IR.  The  result  of  a  program  analysis  is  "correct"  if  the  error  block  is  not  reachable,  "incorrect"  if  the  error  block  is  reachable,  or  "inconclusive"  if  the  status  of  the  program  could  not  be  determined.  In  this  paper,  we  introduce  Skinki¾?2.0  to  analyse  single  and  multi-threaded  C  programs.
1	Optimal  time  bounded  reachability  analysis  for  concurrent  systems.  Efficient  optimal  scheduling  for  concurrent  systems  on  a  finite  horizon  is  a  challenging  task  up  to  date:  Not  only  does  time  have  a  continuous  domain,  but  in  addition  there  are  exponentially  many  possible  decisions  to  choose  from  at  every  time  point.
1	Map2check  using  llvm  and  klee.  Map2Check  is  a  bug  hunting  tool  that  automatically  checks  safety  properties  in  C  programs.  It  tracks  memory  pointers  and  variable  assignments  to  check  user-specified  assertions,  overflow,  and  pointer  safety.  Here,  we  extend  Map2Check  to:  (i)  simplify  the  program  using  Clang/LLVM;  (ii)  perform  a  path-based  symbolic  execution  using  the  KLEE  tool;  and  (iii)  transform  and  instrument  the  code  using  the  LLVM  dynamic  information  flow.  The  SVCOMP’18  results  show  that  Map2Check  can  be  effective  in  generating  and  checking  test  cases  related  to  memory  management  of  C  programs.
1	Synthesis  from  ltl  specifications  with  mean  payoff  objectives.  The  classical  LTL  synthesis  problem  is  purely  qualitative:  the  given  LTL  specification  is  realized  or  not  by  a  reactive  system.  LTL  is  not  expressive  enough  to  formalize  the  correctness  of  reactive  systems  with  respect  to  some  quantitative  aspects.  This  paper  extends  the  qualitativeLTL  synthesis  setting  to  a  quantitative  setting.  The  alphabet  of  actions  is  extended  with  a  weight  function  ranging  over  the  integer  numbers.  The  value  of  an  infinite  word  is  the  mean-payoff  of  the  weights  of  its  letters.  The  synthesis  problem  then  amounts  to  automatically  construct  (if  possible)  a  reactive  system  whose  executions  all  satisfy  a  given  LTL  formula  and  have  mean-payoff  values  greater  than  or  equal  to  some  given  threshold.  The  latter  problem  is  called  LTL$_\textsf{MP}$  synthesis  and  the  LTL$_\textsf{MP}$  realizability  problem  asks  to  check  whether  such  a  system  exists.  By  reduction  to  two-player  mean-payoff  parity  games,  we  first  show  that  LTL$_\textsf{MP}$  realizability  is  not  more  difficult  than  LTL  realizability:  it  is  2ExpTime-Complete.  While  infinite  memory  strategies  are  required  to  realize  LTL$_\textsf{MP}$  specifications  in  general,  we  show  that  e-optimality  can  be  obtained  with  finite-memory  strategies,  for  any  e>0.  To  obtain  efficient  algorithms  in  practice,  we  define  a  Safraless  procedure  to  decide  whether  there  exists  a  finite-memory  strategy  that  realizes  a  given  specification  for  some  given  threshold.  This  procedure  is  based  on  a  reduction  to  two-player  energy  safety  games  which  are  in  turn  reduced  to  safety  games.  Finally,  we  show  that  those  safety  games  can  be  solved  efficiently  by  exploiting  the  structure  of  their  state  spaces  and  by  using  antichains  as  a  symbolic  data-structure.  All  our  results  extend  to  multi-dimensional  weights.  We  have  implemented  an  antichain-based  procedure  and  we  report  on  some  promising  experimental  results.
1	Value  slice  a  new  slicing  concept  for  scalable  property  checking.  A  backward  slice  is  a  commonly  used  preprocessing  step  for  scaling  property  checking.  For  large  programs  though,  the  reduced  size  of  the  slice  may  still  be  too  large  for  verifiers  to  handle.  We  propose  an  aggressive  slicing  method  that,  apart  from  slicing  out  the  same  statements  as  backward  slice,  also  eliminates  computations  that  only  decide  whether  the  point  of  property  assertion  is  reachable.  However,  for  precision,  we  also  carefully  identify  and  retain  all  computations  that  influence  the  values  of  the  variables  in  the  property.  The  resulting  slice,  called  value  slice,  is  smaller  and  scales  better  for  property  checking  than  backward  slice.    We  carry  experiments  on  property  checking  of  industry  strength  programs  using  three  comparable  slicing  techniques:  backward  slice,  value  slice  and  an  even  more  aggressive  slicing  technique  called  thin  slice  that  retains  only  those  statements  on  which  the  variables  in  the  property  are  data  dependent.  While  backward  slicing  enables  highest  precision  and  thin  slice  scales  best,  value  slice  based  property  checking  comes  close  to  the  best  in  both  scalability  and  precision.  This  makes  value  slice  a  good  compromise  between  backward  and  thin  slice  for  property  checking.
1	Stateless  model  checking  for  tso  and  pso.  We  present  a  technique  for  efficient  stateless  model  checking  of  programs  that  execute  under  the  relaxed  memory  models  TSO  and  PSO.  The  basis  for  our  technique  is  a  novel  representation  of  executions  under  TSO  and  PSO,  called  chronological  traces.  Chronological  traces  induce  a  partial  order  relation  on  relaxed  memory  executions,  capturing  dependencies  that  are  needed  to  represent  the  interaction  via  shared  variables.  They  are  optimal  in  the  sense  that  they  only  distinguish  computations  that  are  inequivalent  under  the  widely-used  representation  by  Shasha  and  Snir.  This  allows  an  optimal  dynamic  partial  order  reduction  algorithm  to  explore  a  minimal  number  of  executions  while  still  guaranteeing  full  coverage.  We  apply  our  techniques  to  check,  under  the  TSO  and  PSO  memory  models,  LLVM  assembly  produced  for  C/pthreads  programs.  Our  experiments  show  that  our  technique  reduces  the  verification  effort  for  relaxed  memory  models  to  be  almost  that  for  the  standard  model  of  sequential  consistency.  In  many  cases,  our  implementation  significantly  outperforms  other  comparable  tools.
1	Modular  and  efficient  divide  and  conquer  sat  solver  on  top  of  the  painless  framework.  Over  the  last  decade,  parallel  SATisfiability  solving  has  been  widely  studied  from  both  theoretical  and  practical  aspects.  There  are  two  main  approaches.  First,  divide-and-conquer  (D&C)  splits  the  search  space,  each  solver  being  in  charge  of  a  particular  subspace.  The  second  one,  portfolio  launches  multiple  solvers  in  parallel,  and  the  first  to  find  a  solution  ends  the  computation.  However  although  D&C  based  approaches  seem  to  be  the  natural  way  to  work  in  parallel,  portfolio  ones  experimentally  provide  better  performances.
1	Cpa  bam  slicing  block  abstraction  memoization  and  slicing  with  region  based  dependency  analysis.  Our  submission  to  SV-COMP’18  is  a  composite  tool  based  on  software  verification  framework  CPAchecker  and  static  analysis  platform  Frama-C.  The  base  verifier  uses  a  combination  of  predicate  and  explicit  value  analysis  with  block-abstraction  memoization  as  the  CPA-BAM-BnB  tool  presented  at  SV-COMP’17.  In  this  submission  we  augment  the  verifier  on  reachability  verification  tasks  with  a  slicer  that  is  able  to  remove  those  statements  that  are  irrelevant  to  the  reachability  of  error  locations  in  the  analysed  program.  The  slicer  is  based  on  context-sensitive  flow-insensitive  separation  analysis  with  typed  polymorphic  regions  and  simple  dependency  analysis  with  transitive  closures.  The  resulting  analysis  preserves  reachability  modulo  possible  non-termination  while  removing  enough  irrelevant  code  to  achieve  considerable  speedup  of  the  main  analysis.  The  slicer  is  implemented  as  a  Frama-C  plugin.
1	Text  rvhyper  a  runtime  verification  tool  for  temporal  hyperproperties.  We  present  \(\text  {RVHyper}\),  a  runtime  verification  tool  for  hyperproperties.  Hyperproperties,  such  as  non-interference  and  observational  determinism,  relate  multiple  computation  traces  with  each  other.  Specifications  are  given  as  formulas  in  the  temporal  logic  \(\text  {HyperLTL}\),  which  extends  linear-time  temporal  logic  (LTL)  with  trace  quantifiers  and  trace  variables.  \(\text  {RVHyper}\)  processes  execution  traces  sequentially  until  a  violation  of  the  specification  is  detected.  In  this  case,  a  counter  example,  in  the  form  of  a  set  of  traces,  is  returned.  As  an  example  application,  we  show  how  \(\text  {RVHyper}\)  can  be  used  to  detect  spurious  dependencies  in  hardware  designs.
1	Ufo  verification  with  interpolants  and  abstract  interpretation.  The  algorithms  underlying  Ufo  are  described  in  [1-3].  The  Ufo  tool  is  described  in  more  detail  in  [4].    Ufo  marries  the  power  and  efficiency  of  numerical  Abstract  Interpretation  (AI)  domains  [6]  with  the  generalizing  ability  of  interpolation-based  software  verification  in  an  abstraction  refinement  loop.
1	Seahorn  a  framework  for  verifying  c  programs  competition  contribution.  seahorn  is  a  framework  and  tool  for  verification  of  safety  properties  in  C  programs.  The  distinguishing  feature  of  seahorn  is  its  modular  design  that  separates  how  program  semantics  is  represented  from  the  verification  engine.  This  paper  describes  its  verification  approach  as  well  as  the  instructions  on  how  to  install  and  use  it.
1	Measuring  difficulty  levels  of  javascript  questions  in  question  answer  community  based  on  concept  hierarchy.  Online  inquiry  communities  such  as  Question-Answer  Communities  (QAC)  have  captured  interest  of  online  users  since  they  can  share  and  search  for  any  information  from  any  place  in  the  world.  The  number  of  questions  and  answers  submitted  to  a  popular  community  can  increase  rapidly,  and  that  can  make  it  difficult  for  users  who  look  for  the  “right”  questions  to  answer.  That  is,  from  the  view  of  knowledgeable  experienced  users,  they  tend  to  look  for  hard  challenging  questions  as  an  opportunity  to  share  their  knowledge  and  to  build  respect  with  the  community.  Hence  it  is  desirable  to  distinguish  difficult  questions  from  easy  ones.  Current  researches  estimate  complexity  of  questions  based  on  the  analysis  of  the  features  of  the  QAC  without  considering  the  contents  of  the  questions.  This  paper  presents  a  method  to  measure  question  difficulty  levels  based  directly  on  the  question  contents.  In  particular,  we  analyze  the  difficulty  of  terms  that  appear  in  a  JavaScript-related  question,  based  on  the  proposed  JavaScript  concept  hierarchy.  In  an  evaluation  of  the  performance  of  the  question  difficulty  estimation,  our  concept-based  measure  gives  similar  performance  to  that  of  the  existing  measure  based  on  the  features  of  the  QAC,  but  when  they  are  used  together,  the  performance  can  be  enhanced.
1	First  person  movement  control  with  palm  normal  and  hand  gesture  interaction  in  virtual  reality.  Virtual  reality  (VR)  has  become  a  very  popular  technology  in  recent  years,  which  used  in  the  field  of  multimedia  for  various  purposes.  One  of  the  applications  that  widely  used  for  simulating  physical  presence  in  the  real  world  is  walk-through  VR.  In  the  walk-through  VR  system  will  generate  simulation  character  or  avatar  for  user  which  able  to  control  movement,  especially  first  person  movement  walk-through  in  VR  with  many  input  devices.  In  this  paper,  we  present  a  human  -  computer  interaction  with  the  connection  of  Oculus  Rift  and  Leap  Motion  new  technological  devices  for  VR.  Oculus  Rift  is  VR  headset  or  head  -  mounted  display  devices  that  have  a  small  display  optic  in  front  of  each  eye.  Oculus  Rift  can  track  head  movement  and  change  view  point  follow  it.  Leap  Motion  is  in  -  air  controller  that  can  track  hand  gesture  of  the  user.  The  combination  of  them  will  make  users  feel  like  immerse  to  VR.  Users  can  move  avatar  any  way  in  VR  by  their  hand  interact  through  the  system  via  these  devices.  We  introduce  a  new  interactive  hand  gesture  system  with  palm  normal  for  control  steering  develop  by  the  game  engine  Unity3D  applies  synchronization  of  Oculus  Rift  and  Leap  Motion.  Our  design  and  development  method  will  allow  users  to  adjust  moving  speed  follows  the  hand  gesture  and  the  range  of  the  user's  hand  that  make  a  smoothly  moving  with  acceleration.
1	Sugarcane  yield  grade  prediction  using  random  forest  and  gradient  boosting  tree  techniques.  This  paper  presents  a  machine  learning  based  model  for  predicting  the  sugarcane  yield  grade  of  an  individual  plot.  The  dataset  used  in  this  work  is  obtained  from  a  set  of  sugarcane  plots  around  a  sugar  mill  in  Thailand.  The  features  used  in  the  prediction  consist  of  the  plot  characteristics  (soil  type,  plot  area,  groove  width,  plot  yield/  yield  grade  of  the  last  year),  sugarcane  characteristics  (cane  class  and  type),  plot  cultivation  scheme  (water  resource  type,  irrigation  method,  epidemic  control  method,  fertilizer  type/formula)  and  rain  volume.  We  use  two  predictive  algorithms:  (i)  random  forest  classification,  and  (ii)  gradient  boosting  tree  classification.  The  accuracies  of  our  machine  learning  based  predictive  methods  are  71.83%  and  71.64%,  respectively.  Meanwhile,  the  accuracies  of  two  non-machine-learning  baselines  are  51.52%  (using  the  actual  yield  of  the  last  year  as  the  prediction)  and  65.50%  (the  target  yield  of  each  plot  is  manually  predicted  by  human  expert),  respectively.  This  shows  that  our  work  is  accurate  enough  to  be  applied  for  decision  making  of  sugar  mill  operation  planning.
1	Inbound  logistics  cassava  starch  planning  with  application  of  gis  and  k  means  clustering  methods  in  thailand.  This  paper  present  the  decision  support  system  in  logistics  inbound  and  transportation  system  with  application  of  Geographic  Information  System  (GIS)  to  analyse  the  Cassava  Service  Centres  (CSC)  location  in  order  to  collect  cassava  roots  location  and  optimized  the  number  and  location  suitability  of  CSC.  The  methodology  used  K-mean  clustering  and  application  of  Geographic  Information  System  with  spatial  and  attribute  data,  and  network  analyst  extension  to  find,  compared  and  minimize  optimization  with  cost  for  investment  and  transportation  distance  solution  of  their  scenarios.  The  results  had  show  the  optimization  number  of  location  of  CSC  must  be  20  nodes,  investment  cost  for  CSC  location  was  reduced  to  9.8  million  baht,  and  distance  was  136,176.58  kilometres,  that  results  had  reduce  to  49.5  and  13.3  percent,  respectively.
1	Enhancing  modified  cuckoo  search  by  using  mantegna  levy  flights  and  chaotic  sequences.  In  this  paper,  we  present  an  improvement  of  the  modified  cuckoo  search  (MCS)  method.  We  focus  on  a  new  nest  generation  from  the  top  nest  group.  This  group  of  nests  assists  a  better  local  search.  We  use  Tent  map  chaotic  sequences  to  replace  the  constant  parameter,  inverse  golden  ratio  of  MCS.  This  process  aims  to  find  a  better  solution  in  case  of  multi-modal  problems.  The  Cauchy  Levy  distribution  is  replaced  by  Mantegna  Levy  distribution  generation.  This  process  assists  in  finding  a  better  solution  in  case  of  uni-modal  problems.  To  construct  a  more  suitable  with  wider  optimization  problems  and  good  convergence  property,  these  two  concepts  are  combined  together  as  Improved  MCS  with  Chaotic  Sequences  algorithm  (ICMCS).  The  proposed  algorithm  is  verified  using  nineteen  constrained  optimization  problems.  The  performance  of  the  proposed  algorithm  is  compared  with  the  original  MCS  algorithms.  The  optimal  solutions  obtained  in  this  study  are  more  superior  to  MCS.
1	Accommodation  recommendation  system  from  user  reviews  based  on  feature  based  weighted  non  negative  matrix  factorization  method.  Nowadays,  online  information  dominates  people  decision  making,  especially  in  a  daily  life  basis,  as  well  as  traveling.  Many  websites  provide  accommodation  information  and  reviews.  Hence,  this  information  overload  makes  choosing  the  right  accommodation  difficult  and  time-consuming  for  users.  We  propose  ACRoSS,  a  system  that  collects  data  from  websites  and  chooses  representative  reviews  by  feature-based  weighted  non-negative  matrix  factorization  method  regarding  predetermined  topics.  The  benefit  of  this  project  is  to  help  users  to  choose  an  accommodation  easily.  The  system  can  classify  the  topic  with  the  precision  of  61%.  The  percentages  of  users  that  have  high  satisfaction  in  representative  reviews  are  55.48%.
1	A  robust  algorithm  for  r  peak  detection  based  on  optimal  discrete  wavelet  transform.  Automated  ECG  signal  processing  can  assist  in  diagnosing  several  heart  diseases.  Many  R  peak  detection  methods  have  been  studied  because  the  accuracy  of  R  peak  detection  significantly  affects  the  quality  of  subsequent  ECG  feature  extraction.  Two  important  steps  in  R  peak  detection  algorithm  that  draw  attention  over  researchers  are  the  preprocessing  and  thresholding  stages.  Among  several  methods,  wavelet  transform  is  a  widely  used  method  for  removing  noise  in  the  preprocessing  stage.  Various  proposed  algorithms  require  prior  knowledge  of  frequency  spectrum  of  the  signal  under  consideration  in  order  to  select  the  wavelet  detail  coefficients  in  the  reconstruction  process.  Moreover,  parameter  fine  tuning  is  generally  involved  in  threshold  selection  to  accomplish  high  detection  accuracy.  As  a  result,  it  may  be  difficult  to  utilize  these  methods  for  general  ECG  data  sets.  Accordingly,  we  propose  an  automatic  and  parameter  free  method  that  optimally  selects  the  appropriate  detail  components  for  wavelet  reconstruction  as  well  as  the  adaptive  threshold.  The  proposed  algorithm  employs  the  analysis  of  probability  density  function  of  the  processed  ECG  signal.  The  validation  of  the  algorithm  was  performed  over  the  MIT-BIH  database  and  has  produced  an  average  sensitivity  of  99.63%  and  specificity  of  99.78%  which  is  in  the  same  range  as  the  previously  proposed  approaches.
1	Bank  direct  marketing  analysis  of  asymmetric  information  based  on  machine  learning.  The  bank  direct  marketing  campaign  for  offering  products  that  meet  the  customers'  needs  is  the  challenge  problems.  The  bank  direct  marketing  data  analysis  is  important  work  that  helps  the  banks  predict  whether  customers  will  sign  a  long  term  deposits  with  the  banks.  The  method  that  can  predict  such  customers'  needs  can  be  profitable  to  the  banks  for  improving  their  marketing  campaign  strategies.  Unfortunately,  it  is  very  hard  to  predict  the  customers'  needs  because  the  available  information  is  asymmetric.  In  this  paper,  we  propose  the  method  to  analyze  asymmetric  information  using  SMOTE  algorithm  and  Rotation  Forest  (PCA)-J48.  The  SMOTE  method  is  used  to  modify  the  data  and  improve  the  accuracy  of  the  prediction.  The  performance  of  the  proposed  method  is  evaluated  and  compared  to  Decision  Tree,  Rotation  Forest,  Navie  Bayes,  BayesNet,  Multilayer  Perceptron  Neural  Network,  RBF  Neural  Network.  The  experimental  results  show  the  predicting  accuracies  of  all  predictors.  The  experiments  show  that  Rotation  Forest  (PCA)-  J48  can  achieve  the  highest  value  of  accuracy  and  specificity.  However,  the  sensitivity  of  Rotation  Forest  (PCA)-J48  is  higher  than  all  methods  except  BayesNet  and  Rotation  Forest  (PCA)  RandomTree.
1	Two  factor  image  based  password  authentication  for  junior  high  school  students.  Internet  is  popular  for  people  at  all  ages.  A  computer  is  used  to  access  the  Internet  but  a  user  needs  to  authenticate  himself  to  the  system  in  order  to  gain  access.  The  problem  of  a  typical  authentication  technique  is  to  memorize  a  password.  Lots  of  junior  high  school  students  in  Pakthongchaiprachaniramit  School  cannot  remember  their  own  password  and  this  work  is  proposed  to  solve  the  problem  by  using  an  image-based  password  technique  instead.  However,  a  normal  image-based  password  can  be  easily  hacked  using  a  shoulder  surfing  attack.  This  work  proposed  the  two  factor  image-based  password  technique  using  image  and  random  questions.  However,  when  the  percentage  of  successful  log-in  was  compared,  this  work  outperformed  the  alphanumeric-based  password.  When  the  third  log-in  (15  days  after  registration)  was  conducted,  the  percentage  of  successful  log-in  of  our  proposed  work  was  95%  comparing  to  58.33%  of  alphanumeric-based  password.  In  addition,  the  proposed  technique  was  able  to  prevent  from  the  shoulder  surfing  attack.
1	A  game  based  learning  system  for  plant  monitoring  based  on  iot  technology.  environmental  awareness  has  been  concerned  as  the  primary  factor  for  environmental  sustainability.  It  can  be  said  that  people,  especially  young  adults,  should  be  educated  to  concern  and  aware  of  environmental  protection.  This  paper  addresses  how  to  stimulate  people  to  taking  care  of  tree  and  plant  by  proposing  a  game-based  learning  system  for  plant  monitoring  based  on  Internet  of  Thing  (IoT)  technology.  A  novel  approach  of  harmonization  between  the  three  main  components,  namely,  real  plant  caring,  game-based  learning,  and  IoT  technology,  are  discussed  and  proposed.  A  developed  game-based  learning  system  has  been  introduced  and  the  experimental  study  of  learners’  satisfaction  of  applying  the  proposed  game  in  practical  use  has  been  reported.
1	Process  modeling  for  industry  4  0  applications  towards  an  industry  4  0  process  modeling  language  and  method.  The  term  Industry  4.0  derives  from  the  new  (fourth)  industrial  revolution  enabling  suppliers  and  manufacturers  to  leverage  new  technological  concepts  like  Internet  of  Things,  Big  Data,  and  Cloud  Computing:  New  or  enhanced  products  and  services  can  be  created,  cost  be  reduced  and  productivity  be  increased.  Similar  terms  are  Smart  Factory  or  Smart  Manufacturing.  The  ideas,  concepts  and  technologies  are  not  hype  anymore  —  they  are  at  least  partly  reality,  but  many  software  specification  and  development  aspects  are  still  not  sufficiently  covered,  e.g.  standardization,  specification  and  modeling  languages.  This  paper  presents  an  Industry  4.0  process  modeling  language  (I4PML)  that  is  an  extension  (UML  profile  with  stereotypes)  of  OMG's  BPMN  (Business  Process  Model  and  Notation)  standard.  We  also  describe  a  method  for  the  specification  of  Industry  4.0  applications  using  UML  and  I4PML.
1	An  enhanced  cluster  head  selection  criterion  of  leach  in  wireless  sensor  networks.  Recent  advances  in  wireless  sensor  network  technology  have  enabled  the  implementation  of  a  variety  of  reliable,  and  distributed  monitoring  and  controlling  systems  used  in  several  sectors  such  as  environment,  healthcare,  civil,  and  military.  This  has  in  turn  led  to  the  development  of  many  of  their  features  and  optimizations  for  the  purpose  of  efficiency,  reliability,  and  scalability  but  with  the  key  constraint  of  limited  power  supply  sensor  nodes.  Several  efficient  power  control  and  management  schemes  have  been  explored  to  keep  the  sensor  nodes  in  the  network  operational  for  a  long  period  in  order  to  increase  network  lifetime.  One  of  the  more  promising  techniques  is  based  on  a  hierarchical  architecture  or  cluster-based  routing  that  can  be  employed  to  enable  the  sensor  networks  to  be  deployed  in  a  large-scale  area.  Low  Energy  Adaptive  Clustering  Hierarchy  (LEACH)  is  a  hierarchical  scheme  that  provides  simplicity  and  distributed  operations  with  power-aware  clustering.  It  is  also  widely  used  as  a  baseline  for  several  further  LEACH  derivations.  This  research  comparatively  investigates  the  performance  of  LEACH's  recent  optimizations,  especially  the  clustering  criteria  for  selecting  a  proper  set  of  cluster  heads.  To  further  enhance  the  probability  criteria  of  the  selected  nodes,  this  research  also  recommends  the  modification  of  using  adaptive  moving  window  average  and  selection  probability  into  an  energy  factor  during  the  threshold  selection  stage.  The  results  of  this  modification  show  an  improvement  of  overall  system  energy  usage.
1	When  and  how  to  use  multilevel  modelling.  Model-Driven  Engineering  (MDE)  promotes  models  as  the  primary  artefacts  in  the  software  development  process,  from  which  code  for  the  final  application  is  derived.  Standard  approaches  to  MDE  (like  those  based  on  MOF  or  EMF)  advocate  a  two-level  metamodelling  setting  where  Domain-Specific  Modelling  Languages  (DSMLs)  are  defined  through  a  metamodel  that  is  instantiated  to  build  models  at  the  metalevel  below.      Multilevel  modelling  (also  called  deep  metamodelling)  extends  the  standard  approach  to  metamodelling  by  enabling  modelling  at  an  arbitrary  number  of  metalevels,  not  necessarily  two.  Proposers  of  multilevel  modelling  claim  this  leads  to  simpler  model  descriptions  in  some  situations,  although  its  applicability  has  been  scarcely  evaluated.  Thus,  practitioners  may  find  it  difficult  to  discern  when  to  use  it  and  how  to  implement  multilevel  solutions  in  practice.      In  this  article,  we  discuss  those  situations  where  the  use  of  multilevel  modelling  is  beneficial,  and  identify  recurring  patterns  and  idioms.  Moreover,  in  order  to  assess  how  often  the  identified  patterns  arise  in  practice,  we  have  analysed  a  wide  range  of  existing  two-level  DSMLs  from  different  sources  and  domains,  to  detect  when  their  elements  could  be  rearranged  in  more  than  two  metalevels.  The  results  show  this  scenario  is  not  uncommon,  while  in  some  application  domains  (like  software  architecture  and  enterprise/process  modelling)  pervasive,  with  a  high  average  number  of  pattern  occurrences  per  metamodel.
1	Test  selection  for  deep  learning  systems.  Testing  of  deep  learning  models  is  challenging  due  to  the  excessive  number  and  complexity  of  the  computations  involved.  As  a  result,  test  data  selection  is  performed  manually  and  in  an  ad  hoc  way.  This  raises  the  question  of  how  we  can  automatically  select  candidate  data  to  test  deep  learning  models.  Recent  research  has  focused  on  defining  metrics  to  measure  the  thoroughness  of  a  test  suite  and  to  rely  on  such  metrics  to  guide  the  generation  of  new  tests.  However,  the  problem  of  selecting/prioritising  test  inputs  (e.g.,  to  be  labelled  manually  by  humans)  remains  open.  In  this  article,  we  perform  an  in-depth  empirical  comparison  of  a  set  of  test  selection  metrics  based  on  the  notion  of  model  uncertainty  (model  confidence  on  specific  inputs).  Intuitively,  the  more  uncertain  we  are  about  a  candidate  sample,  the  more  likely  it  is  that  this  sample  triggers  a  misclassification.  Similarly,  we  hypothesise  that  the  samples  for  which  we  are  the  most  uncertain  are  the  most  informative  and  should  be  used  in  priority  to  improve  the  model  by  retraining.  We  evaluate  these  metrics  on  five  models  and  three  widely  used  image  classification  problems  involving  real  and  artificial  (adversarial)  data  produced  by  five  generation  algorithms.  We  show  that  uncertainty-based  metrics  have  a  strong  ability  to  identify  misclassified  inputs,  being  three  times  stronger  than  surprise  adequacy  and  outperforming  coverage-related  metrics.  We  also  show  that  these  metrics  lead  to  faster  improvement  in  classification  accuracy  during  retraining:  up  to  two  times  faster  than  random  selection  and  other  state-of-the-art  metrics  on  all  models  we  considered.
1	How  far  we  have  progressed  in  the  journey  an  examination  of  cross  project  defect  prediction.  Background.  Recent  years  have  seen  an  increasing  interest  in  cross-project  defect  prediction  (CPDP),  which  aims  to  apply  defect  prediction  models  built  on  source  projects  to  a  target  project.  Currently,  a  variety  of  (complex)  CPDP  models  have  been  proposed  with  a  promising  prediction  performance.Problem.  Most,  if  not  all,  of  the  existing  CPDP  models  are  not  compared  against  those  simple  module  size  models  that  are  easy  to  implement  and  have  shown  a  good  performance  in  defect  prediction  in  the  literature.Objective.  We  aim  to  investigate  how  far  we  have  really  progressed  in  the  journey  by  comparing  the  performance  in  defect  prediction  between  the  existing  CPDP  models  and  simple  module  size  models.Method.  We  first  use  module  size  in  the  target  project  to  build  two  simple  defect  prediction  models,  ManualDown  and  ManualUp,  which  do  not  require  any  training  data  from  source  projects.  ManualDown  considers  a  larger  module  as  more  defect-prone,  while  ManualUp  considers  a  smaller  module  as  more  defect-prone.  Then,  we  take  the  following  measures  to  ensure  a  fair  comparison  on  the  performance  in  defect  prediction  between  the  existing  CPDP  models  and  the  simple  module  size  models:  using  the  same  publicly  available  data  sets,  using  the  same  performance  indicators,  and  using  the  prediction  performance  reported  in  the  original  cross-project  defect  prediction  studies.Result.  The  simple  module  size  models  have  a  prediction  performance  comparable  or  even  superior  to  most  of  the  existing  CPDP  models  in  the  literature,  including  many  newly  proposed  models.Conclusion.  The  results  caution  us  that,  if  the  prediction  performance  is  the  goal,  the  real  progress  in  CPDP  is  not  being  achieved  as  it  might  have  been  envisaged.  We  hence  recommend  that  future  studies  should  include  ManualDown/ManualUp  as  the  baseline  models  for  comparison  when  developing  new  CPDP  models  to  predict  defects  in  a  complete  target  project.
1	Type  based  call  graph  construction  algorithms  for  scala.  Call  graphs  have  many  applications  in  software  engineering.  For  example,  they  serve  as  the  basis  for  code  navigation  features  in  integrated  development  environments  and  are  at  the  foundation  of  static  analyses  performed  in  verification  tools.  While  many  call  graph  construction  algorithms  have  been  presented  in  the  literature,  we  are  not  aware  of  any  that  handle  Scala  features  such  as  traits  and  abstract  type  members.  Applying  existing  algorithms  to  the  JVM  bytecodes  generated  by  the  Scala  compiler  produces  very  imprecise  results  because  type  information  is  lost  during  compilation.  We  adapt  existing  type-based  call  graph  construction  algorithms  to  Scala  and  present  a  formalization  based  on  Featherweight  Scala.  An  experimental  evaluation  shows  that  our  most  precise  algorithm  generates  call  graphs  with  1.1--3.7  times  fewer  nodes  and  1.5--17.3  times  fewer  edges  than  a  bytecode-based  RTA  analysis.
1	Concept  location  using  formal  concept  analysis  and  information  retrieval.  The  article  addresses  the  problem  of  concept  location  in  source  code  by  proposing  an  approach  that  combines  Formal  Concept  Analysis  and  Information  Retrieval.  In  the  proposed  approach,  Latent  Semantic  Indexing,  an  advanced  Information  Retrieval  approach,  is  used  to  map  textual  descriptions  of  software  features  or  bug  reports  to  relevant  parts  of  the  source  code,  presented  as  a  ranked  list  of  source  code  elements.  Given  the  ranked  list,  the  approach  selects  the  most  relevant  attributes  from  the  best  ranked  documents,  clusters  the  results,  and  presents  them  as  a  concept  lattice,  generated  using  Formal  Concept  Analysis.      The  approach  is  evaluated  through  a  large  case  study  on  concept  location  in  the  source  code  on  six  open-source  systems,  using  several  hundred  features  and  bugs.  The  empirical  study  focuses  on  the  analysis  of  various  configurations  of  the  generated  concept  lattices  and  the  results  indicate  that  our  approach  is  effective  in  organizing  different  concepts  and  their  relationships  present  in  the  subset  of  the  search  results.  In  consequence,  the  proposed  concept  location  method  has  been  shown  to  outperform  a  standalone  Information  Retrieval  based  concept  location  technique  by  reducing  the  number  of  irrelevant  search  results  across  all  the  systems  and  lattice  configurations  evaluated,  potentially  reducing  the  programmers'  effort  during  software  maintenance  tasks  involving  concept  location.
1	Solving  the  search  for  source  code.  Programmers  frequently  search  for  source  code  to  reuse  using  keyword  searches.  The  search  effectiveness  in  facilitating  reuse,  however,  depends  on  the  programmer's  ability  to  specify  a  query  that  captures  how  the  desired  code  may  have  been  implemented.  Further,  the  results  often  include  many  irrelevant  matches  that  must  be  filtered  manually.  More  semantic  search  approaches  could  address  these  limitations,  yet  existing  approaches  are  either  not  flexible  enough  to  find  approximate  matches  or  require  the  programmer  to  define  complex  specifications  as  queries.      We  propose  a  novel  approach  to  semantic  code  search  that  addresses  several  of  these  limitations  and  is  designed  for  queries  that  can  be  described  using  a  concrete  input/output  example.  In  this  approach,  programmers  write  lightweight  specifications  as  inputs  and  expected  output  examples.  Unlike  existing  approaches  to  semantic  search,  we  use  an  SMT  solver  to  identify  programs  or  program  fragments  in  a  repository,  which  have  been  automatically  transformed  into  constraints  using  symbolic  analysis,  that  match  the  programmer-provided  specification.      We  instantiated  and  evaluated  this  approach  in  subsets  of  three  languages,  the  Java  String  library,  Yahooe  Pipes  mashup  language,  and  SQL  select  statements,  exploring  its  generality,  utility,  and  trade-offs.  The  results  indicate  that  this  approach  is  effective  at  finding  relevant  code,  can  be  used  on  its  own  or  to  filter  results  from  keyword  searches  to  increase  search  precision,  and  is  adaptable  to  find  approximate  matches  and  then  guide  modifications  to  match  the  user  specifications  when  exact  matches  do  not  already  exist.  These  gains  in  precision  and  flexibility  come  at  the  cost  of  performance,  for  which  underlying  factors  and  mitigation  strategies  are  identified.
1	Differential  testing  of  certificate  validation  in  ssl  tls  implementations  an  rfc  guided  approach.  Certificate  validation  in  Secure  Sockets  Layer  or  Transport  Layer  Security  protocol  (SSL/TLS)  is  critical  to  Internet  security.  Thus,  it  is  significant  to  check  whether  certificate  validation  in  SSL/TLS  implementations  is  correctly  implemented.  With  this  motivation,  we  propose  a  novel  differential  testing  approach  that  is  based  on  the  standard  Request  for  Comments  (RFC).  First,  rules  of  certificates  are  extracted  automatically  from  RFCs.  Second,  low-level  test  cases  are  generated  through  dynamic  symbolic  execution.  Third,  high-level  test  cases,  i.e.,  certificates,  are  assembled  automatically.  Finally,  with  the  assembled  certificates  being  test  cases,  certificate  validations  in  SSL/TLS  implementations  are  tested  to  reveal  latent  vulnerabilities  or  bugs.  Our  approach  named  RFCcert  has  the  following  advantages:  (1)  certificates  of  RFCcert  are  discrepancy-targeted,  since  they  are  assembled  according  to  standards  instead  of  genetics;  (2)  with  the  obtained  certificates,  RFCcert  not  only  reveals  the  invalidity  of  traditional  differential  testing  but  also  is  able  to  conduct  testing  that  traditional  differential  testing  cannot  do;  and  (3)  the  supporting  tool  of  RFCcert  has  been  implemented  and  extensive  experiments  show  that  the  approach  is  effective  in  finding  bugs  of  SSL/TLS  implementations.  In  addition,  by  providing  seed  certificates  for  mutation  approaches  with  RFCcert,  the  ability  of  mutation  approaches  in  finding  distinct  discrepancies  is  significantly  enhanced.
1	An  automated  testing  platform  for  mobile  applications.  With  the  growing  popularity  and  complexity  of  mobile  apps,  quality  assurance  becomes  more  and  more  important  in  mobile  app  development.  Unfortunately,  Android  is  also  suffering  from  the  notorious  fragmentation  problem.  To  tackle  these  problems,  we  introduce  an  automated  testing  platform,  BugRocket.  BugRocket  combines  a  distributed  testing  system  with  automated  testing  techniques  equipped  on  mobile  devices.  In  this  paper,  we  developed  a  toolset  for  automated  testing,  and  set  up  such  a  testing  platform  with  40  of  the  most  popular  Android  devices.  An  case  study  is  conducted  and  the  results  show  that  BugRocket  can  work  for  functional  testing  and  compatibility  testing.  Besides,  BugRocket  can  record  a  failed  run  as  long  as  annotated  GUI  model  and  system  logs  to  alleviate  locating  bugs  and  bug  fixing.
1	Asymmetry  in  coevolving  adversarial  systems.  Asymmetries  in  adversarial  systems  arise  from  differences  in  the  &#x0022;situations&#x0022;  of  attackers  and  defenders,  for  instance  corresponding  to  differences  in  information  access  or  cost/benefit  tradeoffs.  While  numerous  studies  have  shown  that  asymmetry  is  important,  less  has  been  done  to  rigorously  characterize  its  impact  or  specify  methods  by  which  it  can  be  leveraged  by  defenders.  This  paper  presents  a  formal  framework  for  analyzing  the  origins  and  roles  of  asymmetric  advantage  in  coevolving  adversarial  systems,  and  uses  this  framework  to  develop  quantitative  tools  for  understanding  and  exploiting  asymmetry.  The  proposed  framework  explains  why  asymmetry  has  such  profound  impact  on  the  behavior  of  coevolving  systems,  and  reveals  a  key  feature  of  these  systems:  they  can  be  reverse-engineered  using  only  limited  measurements.  The  analysis  yields  several  new  results,  including  1.)  a  demonstration  that  machine  learning  systems  increasingly  deployed  to  harden  vulnerabilities  in  essential  systems  are  themselves  highly  vulnerable,  and  2.)  a  methodology  for  designing  &#x0022;predictability-oriented&#x0022;  defenses  that  shifts  advantages  of  asymmetry  toward  defenders.  An  empirical  case  study  involving  detection  of  money-laundering  activity  within  the  global  interbank  transactions  system  illustrates  the  utility  of  the  proposed  analytic  framework  in  a  high-consequence  setting.
1	Scalability  of  cloud  based  scit  mtd.  In  order  to  support  large  volume  of  transactions  and  number  of  users,  as  estimated  by  the  load  demand  modeling,  a  system  needs  to  scale  in  order  to  continue  to  satisfy  required  quality  attributes.  In  particular,  for  systems  exposed  to  the  Internet,  scaling  up  may  increase  the  attack  surface  susceptible  to  malicious  intrusions.  The  new  proactive  approach  based  on  the  concept  of  Moving  Target  Defense  (MTD)  should  be  considered  as  a  complement  to  current  cybersecurity  protection.  In  this  paper,  we  analyze  the  scalability  of  the  Self  Cleansing  Intrusion  Tolerance  (SCIT)  MTD  approach  using  Cloud  infrastructure  services.  By  applying  the  model  of  MTD  with  continuous  rotation  and  diversity  to  a  multi-node  or  multi-instance  system,  we  argue  that  the  effectiveness  of  the  approach  is  dependent  on  the  share-nothing  architecture  pattern  of  the  large  system.  Furthermore,  adding  more  resources  to  the  MTD  mechanism  can  compensate  to  achieve  the  desired  level  of  secure  availability.
1	Design  and  application  of  a  sikuli  based  capture  replay  tool.  Graphical  user  interface  (GUI)  testing  is  an  important  branch  of  software  testing.  Using  capture-replay  technology  to  automate  GUI  testing  can  effectively  improve  efficiency  and  save  costs.  However,  most  of  the  traditional  capture-replay  tools  capture  their  GUI  elements  through  the  API  of  the  SUT  (System  Under  Test)  and  depend  on  the  openness  of  the  API;  the  generated  scripts  usually  involve  the  internal  code  of  the  GUI  elements,  and  the  readability  is  not  good.  In  this  article,  we  proposed  a  capture-replay  technology  based  on  the  open  source  tool-Sikuli,  which  can  generate  scripts  with  good  readability  based  on  the  testers'  operations  and  can  record  and  playback  any  visual  interface.
1	A  statistical  analysis  of  operational  profile  driven  testing.  The  amount  of  faults  in  software  products  is  constantly  increasing  and  it  is  not  feasible  to  remove  all  faults.  Within  many  organizations,  operational  profiles  are  used  to  focus  the  test  activities.  Prior  art  has  claimed  that  when  reliability  growth  tests  are  allocated  using  operational  profiles,  the  reliability  level  will  be  the  maximum  practically  achievable,  but  this  claim  has  never  been  proven  nor  rejected.  In  this  paper  a  statistical  model  is  introduced  that  captures  the  relation  between  faults  and  failures.  Using  the  model  the  effects  of  using  operational  profiles  for  reliability  growth  testing  is  analyzed.  The  analysis  shows  that  the  benefits  of  using  operational  profiles  depend  on  a  number  of  factors  such  as  the  number  of  faults  and  the  failure  probability.  The  results  show  that  using  operational  profiles  for  reliability  growth  testing  is  beneficial,  however  not  when  a  very  high  reliability  is  obtained,  due  to  the  effect  of  over  testing.
1	Standards  based  iot  testing  with  open  source  test  equipment.  While  the  Internet  of  Things  (IoT)  is  on  the  rise,  the  quality  assurance  of  interconnected  systems  becomes  an  ever-increasing  challenge.  By  increasing  the  number  of  networked  components,  the  complexity  grows  and  so  the  probability  of  malfunctions  and  vulnerabilities  does.  The  objective  of  this  paper  is  to  assess  the  need  for  standards-based  open-source  test  equipment  for  the  IoT.  While  many  IoT  components  communicate  over  standardized  protocols,  communication  protocols  for  IoT  like  MQTT  or  CoAP  evolved  over  time  without  a  holistic  approach  for  quality  assurance.  The  proposed  approach  showed  the  usefulness  of  the  Eclipse  IoT-Testware  in  terms  of  ensuring  protocol  conformance,  robustness  and  secure  implementations.  Despite  these  characteristics  are  not  unique  for  the  IoT,  they  were  considered  to  be  the  decisive  factors  for  the  proper  operation  of  IoT  systems.  Furthermore,  this  paper  focuses  also  on  increasing  the  usability.  Because  the  proposed  approach  applies  a  wide  range  of  technologies  and  methodologies  adopted  from  standardization  environments,  the  consideration  of  the  usability  for  a  broader  audience  was  required.
1	Testing  the  effectiveness  of  attack  detection  mechanisms  in  industrial  control  systems.  Industrial  Control  Systems  (ICS)  are  found  in  critical  infrastructure  such  as  for  power  generation  and  water  treatment.  When  security  requirements  are  incorporated  into  an  ICS,  one  needs  to  test  the  additional  code  and  devices  added  do  improve  the  prevention  and  detection  of  cyber  attacks.  Conducting  such  tests  in  legacy  systems  is  a  challenge  due  to  the  high  availability  requirement.  An  approach  using  Timed  Automata  (TA)  is  proposed  to  overcome  this  challenge.  This  approach  enables  assessment  of  the  effectiveness  of  an  attack  detection  method  based  on  process  invariants.  The  approach  has  been  demonstrated  in  a  case  study  on  one  stage  of  a  6-  stage  operational  water  treatment  plant.  The  model  constructed  captured  the  interactions  among  components  in  the  selected  stage.  In  addition,  a  set  of  attacks,  attack  detection  mechanisms,  and  security  specifications  were  also  modeled  using  TA.  These  TA  models  were  conjoined  into  a  network  and  implemented  in  UPPAAL.  The  models  so  implemented  were  found  effective  in  detecting  the  attacks  considered.  The  study  suggests  the  use  of  TA  as  an  effective  tool  to  model  an  ICS  and  study  its  attack  detection  mechanisms  as  a  complement  to  doing  so  in  a  real  plant–operational  or  under  design.
1	Formal  transaction  modeling  and  verification  for  an  adaptable  web  service  orchestration.  The  integrity  of  Web  service  orchestration  is  one  of  the  most  relevant  characters  in  the  development  of  compositions.  Particularly,  transactions  that  include  a  set  of  activities  which  are  performed  by  partner  services.  However,  due  to  the  complexity  of  these  activities,  several  failures  can  occur  during  the  execution  leading  to  violate  transactional  properties.  Therefore,  the  selfadaptive  behavior  is  a  feature  which  designers  need  to  include  in  their  orchestration  process  in  order  to  improve  the  reliability  of  these  properties.  In  this  paper,  we  propose  a  development  process  of  adaptable  transactional  Web  services  that  makes  use  of  Colored  Petri  Net  (CPN)  formalism  to  model  Web  service  orchestration  under  adaptive  transaction  constraints,  ASK-Computational  Tree  Logic  (ASK-CTL)  is  used  to  formally  define  transactions  properties;  and  Model  Checking  technique,  that  is  available  in  the  software  CPN  Tools,  is  used  to  verify  the  defined  properties.  An  example  of  a  bookstore  process  is  given  to  illustrate  our  proposal.
1	P2p  based  full  digital  co  simulation  and  verification  system  design.  With  the  embedded  system  showing  a  trend  of  network  development,  and  gradually  formed  a  Cyber-physical  systems.  How  to  ensure  the  safety  and  reliability  of  the  Cyber-physical  systems  is  a  hot  research  topic  in  the  present  study.  Full-digital  co-simulation  technology  is  an  important  means  of  verify  the  safety  and  reliability  of  embedded  system,  Cooperative  communication  is  one  of  the  key  models  of  full-digital  co-simulation  system.  Its  network  structure  may  affect  simulation  results  and  performance  of  system  directly.  Peer-to-Peer  (P2P)  network,  a  networking  pattern  developed  rapidly  in  recent  years,  boasts  unique  advantages  in  coordinated  working,  P2P  computing,  resource  sharing,  etc.  The  paper  studies  the  architecture  of  P2P  network,  proposes  a  P2P-based  full  digital  co-simulation  and  verification  system,  focuses  on  the  issuance/subscription  mechanism  of  software  bus,  recommended  strategy  for  timing  simulation  and  activity  detection  mechanism  of  simulation  node  in  the  system,  and  designs  experiments  for  analyzing  the  real-time  simulation  of  the  system.
1	Research  on  the  information  quality  measurement  of  judicial  documents.  This  paper  explores  and  defines  data  quality  metrics  of  document  data  in  judicial  field.  The  objective  information  theory  defines  information  as  an  objective  reflection  of  the  objects  and  the  state  of  movement  in  the  objective  world  and  the  subjective  world.  All  kinds  of  information  systems  in  physics  can  obtain,  transmit,  process  and  apply  various  objective  information.  Document  is  the  carrier  of  case  information,  and  the  entity  mapping  of  cases  in  the  objective  world.  In  order  to  describe  the  nature  of  the  case  infor-mation  and  its  various  dimensions  accurately,  this  paper  con-structs  and  quantifies  the  data  quality  evaluation  standards  of  the  documents  with  the  help  of  six-tuple  information  theory  and  the  related  knowledge  of  rough  sets.
1	Functional  dependency  detection  for  integration  test  cases.  This  paper  presents  a  natural  language  processing  (NLP)  based  approach  that,  given  software  requirements  specification,  allows  the  functional  dependency  detection  between  integration  test  cases.  We  analyze  a  set  of  internal  signals  to  the  implemented  modules  for  detecting  dependencies  between  requirements  and  thereby  identifying  dependencies  between  test  cases  such  that:  module  2  depends  on  module  1  if  an  output  internal  signal  from  module  1  enters  as  an  input  internal  signal  to  the  module  2.  Consequently,  all  requirements  (and  thereby  test  cases)  for  module  2  are  dependent  on  all  the  designed  requirements  (and  test  cases)  for  module  1.  The  dependency  information  between  requirements  (and  thus  corresponding  test  cases)  can  be  utilized  for  test  case  prioritization  and  scheduling.  We  have  implemented  our  approach  as  a  tool  and  the  feasibility  is  evaluated  through  an  industrial  use  case  in  the  railway  domain  at  Bombardier  Transportation  (BT),  Sweden.
1	Introduction  of  a  tool  based  continuous  information  security  management  system  an  exploratory  case  study.  Tighter  regulatory  demands  and  higher  customer  expectations  regarding  the  protection  of  information  force  enterprises  to  systematically  ensure  confidentiality,  integrity  and  availability  of  stored  information  and  processing  facilities.  Information  Security  Management  Systems  (ISMSs)  are  used  to  address  these  challenges.  Recent  studies  show  that  the  majority  of  companies  plans  to  establish  at  least  basic  information  security  management  to  prepare  for  future  developments.  Larger  enterprises  have  already  embraced  ISMSs,  whereas  small  and  medium-sized  enterprises  (SMEs)  are  catching  up  and  require  support  in  defining,  introducing  and  operating  them.  We  developed  ADAMANT,  an  SME-friendly  tool  that  supports  continuous  information  security  management  incorporating  stakeholders  of  different  domains.  In  this  paper,  we  evaluated  our  approach  to  introduce  an  ISMS  in  SMEs  using  an  introductory  information  security  training.  The  evaluation  shows  that  our  tool  improves  critical  information  security  management  tasks.  Furthermore,  integrating  ADAMANT  in  customized  security  trainings  allows  companies  to  directly  use  training  results  to  implement  an  ISMS.
1	Verification  of  computer  switching  networks  an  overview.  Formal  verification  has  seen  much  success  in  several  domains  of  hardware  and  software  design.  For  example,  in  hardware  verification  there  has  been  much  work  in  the  verification  of  microprocessors  (e.g.  [1])  and  memory  systems  (e.g.  [2]).  Similarly,  software  verification  has  seen  success  in  device-drivers  (e.g.  [3])  and  concurrent  software  (e.g.  [4]).  The  area  of  network  verification,  which  consists  of  both  hardware  and  software  components,  has  received  relatively  less  attention.  Traditionally,  the  focus  in  this  domain  has  been  on  performance  and  security,  with  less  emphasis  on  functional  correctness.  However,  increasing  complexity  is  resulting  in  increasing  functional  failures  and  thus  prompting  interest  in  verification  of  key  correctness  properties.  This  paper  reviews  the  formal  verification  techniques  that  have  been  used  here  thus  far,  with  the  goal  of  understanding  the  characteristics  of  the  problem  domain  that  are  helpful  for  each  of  the  techniques,  as  well  as  those  that  pose  specific  challenges.  Finally,  it  highlights  some  interesting  research  challenges  that  need  to  be  addressed  in  this  important  emerging  domain.
1	Neural  state  classification  for  hybrid  systems.  We  introduce  the  State  Classification  Problem  (SCP)  for  hybrid  systems,  and  present  Neural  State  Classification  (NSC)  as  an  efficient  solution  technique.  SCP  generalizes  the  model  checking  problem  as  it  entails  classifying  each  state  s  of  a  hybrid  automaton  as  either  positive  or  negative,  depending  on  whether  or  not  s  satisfies  a  given  time-bounded  reachability  specification.  This  is  an  interesting  problem  in  its  own  right,  which  NSC  solves  using  machine-learning  techniques,  Deep  Neural  Networks  in  particular.  State  classifiers  produced  by  NSC  tend  to  be  very  efficient  (run  in  constant  time  and  space),  but  may  be  subject  to  classification  errors.  To  quantify  and  mitigate  such  errors,  our  approach  comprises:  (i)  techniques  for  certifying,  with  statistical  guarantees,  that  an  NSC  classifier  meets  given  accuracy  levels;  (ii)  tuning  techniques,  including  a  novel  technique  based  on  adversarial  sampling,  that  can  virtually  eliminate  false  negatives  (positive  states  classified  as  negative),  thereby  making  the  classifier  more  conservative.  We  have  applied  NSC  to  six  nonlinear  hybrid  system  benchmarks,  achieving  an  accuracy  of  99.25%  to  99.98%,  and  a  false-negative  rate  of  0.0033  to  0,  which  we  further  reduced  to  0.0015  to  0  after  tuning  the  classifier.  We  believe  that  this  level  of  accuracy  is  acceptable  in  many  practical  applications,  and  that  these  results  demonstrate  the  promise  of  the  NSC  approach.
1	Test  coverage  estimation  using  threshold  accepting.  This  paper  is  concerned  with  model-based  testing  of  hybrid  systems.  In  our  previous  work  [6],  we  proposed  a  test  generation  algorithm,  called  gRRT,  guided  by  a  coverage  measure  defined  using  the  star  discrepancy  notion.  An  important  ingredient  in  this  algorithm  is  a  procedure  for  dynamically  estimating  the  coverage,  which  is  done  based  on  a  box  partition  of  the  continuous  state  space.  The  goal  of  this  estimation  is  to  identify  the  areas  in  the  state  space  which  have  not  been  sufficiently  visited.  A  drawback  of  this  guiding  method  is  that  its  complexity  depends  on  the  number  of  the  boxes  in  the  partition,  which  needs  to  be  fine  enough  to  guarantee  a  good  coverage  estimate.  Thus  in  high  dimensions  the  method  can  become  very  costly.  To  enhance  the  scalability  of  the  algorithm  gRRT  we  propose  in  this  paper  a  new  guiding  method,  motivated  by  the  observation  that  trying  to  optimize  the  coverage  in  each  exploration  step  is,  on  one  hand,  computationally  costly,  and  on  the  other  hand,  not  always  a  good  choice  since  this  may  make  the  system  try  to  expand  in  the  directions  which  are  not  reachable  (due  to  the  controllability  of  the  system).  Instead  of  considering  all  the  boxes  in  the  partition,  we  propose  to  use  a  randomized  search  to  quickly  find  a  region  that  yields  a  high  local  discrepancy  value.  This  randomized  search  is  based  on  threshold  accepting,  a  well-known  integer  optimization  heuristic.  We  also  present  some  experimental  results  obtained  on  a  challenging  circuit  benchmark  and  a  number  of  randomly  generated  examples,  which  shows  that  the  new  guiding  method  allows  achieving  better  time  and  coverage  efficiency.
1	On  finite  domains  in  first  order  linear  temporal  logic.  We  consider  First-Order  Linear  Temporal  Logic  (FO-LTL)  over  linear  time.  Inspired  by  the  success  of  formal  approaches  based  upon  finite-model  finders,  such  as  Alloy,  we  focus  on  finding  models  with  finite  first-order  domains  for  FO-LTL  formulas,  while  retaining  an  infinite  time  domain.  More  precisely,  we  investigate  the  complexity  of  the  following  problem:  given  a  formula  \(\varphi  \)  and  an  integer  n,  is  there  a  model  of  \(\varphi  \)  with  domain  of  cardinality  at  most  n?  We  show  that  depending  on  the  logic  considered  (FO  or  FO-LTL)  and  on  the  precise  encoding  of  the  problem,  the  problem  is  either  NP-complete,  NEXPTIME-complete,  PSPACE-complete  or  EXPSPACE-complete.  In  a  second  part,  we  exhibit  cases  where  the  Finite  Model  Property  can  be  lifted  from  fragments  of  FO  to  their  FO-LTL  extension.
1	On  automated  lemma  generation  for  separation  logic  with  inductive  definitions.  Separation  Logic  with  inductive  definitions  is  a  well-known  approach  for  deductive  verification  of  programs  that  manipulate  dynamic  data  structures.  Deciding  verification  conditions  in  this  context  is  usually  based  on  user-provided  lemmas  relating  the  inductive  definitions.  We  propose  a  novel  approach  for  generating  these  lemmas  automatically  which  is  based  on  simple  syntactic  criteria  and  deterministic  strategies  for  applying  them.  Our  approach  focuses  on  iterative  programs,  although  it  can  be  applied  to  recursive  programs  as  well,  and  specifications  that  describe  not  only  the  shape  of  the  data  structures,  but  also  their  content  or  their  size.  Empirically,  we  find  that  our  approach  is  powerful  enough  to  deal  with  sophisticated  benchmarks,  e.g.,  iterative  procedures  for  searching,  inserting,  or  deleting  elements  in  sorted  lists,  binary  search  tress,  red-black  trees,  and  AVL  trees,  in  a  very  efficient  way.
1	Meeting  intensity  as  an  indicator  for  project  pressure  exploring  meeting  profiles.  Meetings  are  hot  spots  of  communication  and  collaboration  in  software  development  teams.  Both  distributed  and  co-located  teams  need  to  meet  for  coordination,  communication,  and  collaboration.  It  is  difficult  to  assess  the  quality  of  these  three  crucial  aspects,  or  the  social  effectiveness  and  impact  of  a  meeting:  Personalities,  psychological  and  professional  aspects  interact.  It  is,  therefore,  challenging  to  identify  emerging  communication  problems  or  to  improve  collaboration  by  studying  a  wealth  of  interrelated  details  of  project  meetings.  However,  it  is  relatively  easy  to  count  meetings,  and  to  measure  when  and  how  long  they  took  place.  This  is  objective  information,  does  not  violate  privacy  of  participants,  and  the  data  might  even  be  retrieved  from  project  calendars  automatically.  In  an  exploratory  study,  we  observed  14  student  teams  working  on  comparable  four-month  projects.  Among  many  other  aspects,  we  counted  and  measured  meetings.  In  this  contribution,  we  compare  the  meeting  profiles  qualitatively,  and  derive  a  number  of  hypotheses  relevant  for  software  projects.
1	A  real  time  low  complexity  fall  detection  system  on  the  smartphone.  The  importance  of  detecting  falls  in  real-time  is  more  and  more  emphasized  recently.  In  this  paper,  we  develop  a  real-time  fall  detection  system  on  the  smartphone.  It  is  based  on  our  low-complexity  fall  detection  algorithm  [1]  that  can  detect  the  fall  after  the  fall  event  happens.  The  system  can  distinguish  dangerous  falls  by  setting  a  monitoring  time.  The  system  is  implemented  on  the  Android  platform  and  targeted  on  low  energy  consumption  and  fast  processing,  which  can  be  seamlessly  applied  into  wearable  products.
1	Robustness  analysis  of  string  transducers.  Many  important  functions  over  strings  can  be  represented  as  finite-state  string  transducers.  In  this  paper,  we  present  an  automatatheoretic  technique  for  algorithmically  verifying  that  such  a  function  is  robust  to  uncertainty.  A  function  encoded  as  a  transducer  is  defined  to  be  robust  if  for  each  small  (i.e.,  bounded)  change  to  any  input  string,  the  change  in  the  transducer’s  output  is  proportional  to  the  change  in  the  input.  Changes  to  input  and  output  strings  are  quantified  using  weighted  generalizations  of  the  Levenshtein  and  Manhattan  distances  over  strings.  Our  main  technical  contribution  is  a  set  of  decision  procedures  based  on  reducing  the  problem  of  robustness  verification  of  a  transducer  to  the  problem  of  checking  the  emptiness  of  a  reversal-bounded  counter  machine.  The  decision  procedures  under  the  generalized  Manhattan  and  Levenshtein  distance  metrics  are  in  Pspace  and  Expspace,  respectively.  For  transducers  that  are  Mealy  machines,  the  decision  procedures  under  these  metrics  are  in  Nlogspace  and  Pspace,  respectively.
1	Ethir  a  framework  for  high  level  analysis  of  ethereum  bytecode.  Analyzing  Ethereum  bytecode,  rather  than  the  source  code  from  which  it  was  generated,  is  a  necessity  when:  (1)  the  source  code  is  not  available  (e.g.,  the  blockchain  only  stores  the  bytecode),  (2)  the  information  to  be  gathered  in  the  analysis  is  only  visible  at  the  level  of  bytecode  (e.g.,  gas  consumption  is  specified  at  the  level  of  EVM  instructions),  (3)  the  analysis  results  may  be  affected  by  optimizations  performed  by  the  compiler  (thus  the  analysis  should  be  done  ideally  after  compilation).  This  paper  presents  EthIR,  a  framework  for  analyzing  Ethereum  bytecode,  which  relies  on  (an  extension  of)  Oyente,  a  tool  that  generates  CFGs;  EthIR  produces  from  the  CFGs,  a  rule-based  representation  (RBR)  of  the  bytecode  that  enables  the  application  of  (existing)  high-level  analyses  to  infer  properties  of  EVM  code.
1	Robustness  verification  for  classifier  ensembles.  We  give  a  formal  verification  procedure  that  decides  whether  a  classifier  ensemble  is  robust  against  arbitrary  randomized  attacks.  Such  attacks  consist  of  a  set  of  deterministic  attacks  and  a  distribution  over  this  set.  The  robustness-checking  problem  consists  of  assessing,  given  a  set  of  classifiers  and  a  labelled  data  set,  whether  there  exists  a  randomized  attack  that  induces  a  certain  expected  loss  against  all  classifiers.  We  show  the  NP-hardness  of  the  problem  and  provide  an  upper  bound  on  the  number  of  attacks  that  is  sufficient  to  form  an  optimal  randomized  attack.  These  results  provide  an  effective  way  to  reason  about  the  robustness  of  a  classifier  ensemble.  We  provide  SMT  and  MILP  encodings  to  compute  optimal  randomized  attacks  or  prove  that  there  is  no  attack  inducing  a  certain  expected  loss.  In  the  latter  case,  the  classifier  ensemble  is  provably  robust.  Our  prototype  implementation  verifies  multiple  neural-network  ensembles  trained  for  image-classification  tasks.  The  experimental  results  using  the  MILP  encoding  are  promising  both  in  terms  of  scalability  and  the  general  applicability  of  our  verification  procedure.
1	Wireless  sensor  dependent  ecological  momentary  assessment  for  pediatric  asthma  mhealth  applications.  Pediatric  asthma  is  a  prevalent  chronic  disease  condition  that  can  benefit  from  wireless  health  systems  through  constant  symptom  management.  In  this  paper,  we  propose  a  smart  watch  based  wireless  health  system  that  incorporates  wireless  sensing  and  ecological  momentary  assessment  (EMA)  to  determine  an  individual's  asthma  symptoms.  Since  asthma  is  a  multifaceted  disease,  this  approach  provides  individualized  symptom  assessments  through  various  physiological  and  environmental  wireless  sensor  based  EMA  triggers  specific  to  common  asthma  exacerbations.  Furthermore,  the  approach  described  here  improves  compliance  to  use  of  the  system  through  insightful  EMA  scheduling  related  to  sensor  detected  environmental  and  physiological  changes,  as  well  as  the  patient's  own  schedule.  After  testing  under  several  real  world  conditions,  it  was  found  that  the  system  is  sensitive  to  both  physiological  and  environmental  conditions  that  would  cause  asthma  symptoms.  Furthermore,  the  EMA  questionnaires  that  were  triggered  based  on  these  changes  were  specific  to  the  asthma  trigger  itself,  allowing  for  invaluable  context  behind  the  data  to  be  collected.
1	Recognizing  eating  gestures  using  context  dependent  hidden  markov  models.  This  paper  considers  the  problems  of  recognizing  eating  gestures  by  tracking  wrist  motion.  Hidden  Markov  models  (HMMs)  were  developed  to  capture  variations  in  motion  patterns  of  subgroups  of  participants.  Specifically,  we  examined  if  foreknowledge  of  the  gender,  age,  and  utensil  used  for  eating  could  improve  recognition  accuracy.  Improvement  in  accuracy  was  measured  by  comparing  to  a  baseline  HMM  that  was  trainedon  all  participants.  Data  was  collected  for  276  participants  eating  a  single  meal  within  a  cafeteria  setting.  A  total  of  44,873  gestures  were  manually  labeled  using  video  synchronized  with  the  wrist  motion  tracking  device.  Results  show  that  gender  HMMs  performed  slightly  better  than  the  baseline,  indicating  that  there  is  not  much  difference  in  wrist  motion  patterns  during  eating  between  females  and  males.  Age  HMMs  provided  a  4.3%  increase  in  accuracy  and  utensil  HMMs  provided  a  6.2%  increase  inaccuracy.  The  results  suggest  that  contextual  variables  can  be  used  for  improving  gesture  recognition.
1	Empowering  a  gait  feature  rich  timed  up  and  go  system  for  complex  ecological  environments.  Identifying  fall  risk  can  prevent  injuries  in  the  elderly  as  well  as  reduce  the  related  financial  burden.  A  balance  assessment,  Timed  Up  and  Go  (TUG),  has  been  widely  applied  to  estimate  fall  risk.  However,  the  standardized  TUG  usually  excludes  complex  factors  (e.g.  slopes  and  obstacles)  falling  short  of  representing  challenges  in  environments  that  many  older  adults  navigate.  Having  information  on  the  motor  performance  in  more  complex  settings  can  better  inform  clinicians  about  an  individual's  risk  of  falling.  To  this  end,  we  present  Smart  Insole  TUG  (SITUG),  a  cost-efficient,  real-time  and  self-assessment  system  suitable  for  the  complex  environmental  TUG.  Based  on  the  human  stride  mechanism,  our  SITUG  educes  four  refined  aspects  in  the  gait  feature  and  segments  the  TUG  process  by  six  detailed  phases,  providing  accurate  and  advanced  information  for  the  fall  risk  estimation.  We  evaluate  the  system  with  four  complex  environmental  TUG.  The  results  show  that  the  SITUG  achieves  mean  accuracy  94.1%  in  extracting  subcomponents  within  a  stride  and  93.13%  in  deriving  the  stride  length  based  on  the  verification  of  estimated  walking  distance.  Moreover,  this  system  can  distinguish  six  TUG  phases  with  the  correctness  around  90%.
1	Incremental  consistency  checking  for  complex  design  rules  and  larger  model  changes.  Advances  in  consistency  checking  in  model-based  software  development  made  it  possible  to  detect  errors  in  real-time.  However,  existing  approaches  assume  that  changes  come  in  small  quantities  and  design  rules  are  generally  small  in  scope.  Yet  activities  such  as  model  transformation,  re-factoring,  model  merging,  or  repairs  may  cause  larger  model  changes  and  hence  cause  performance  problems  during  consistency  checking.  The  goal  of  this  work  is  to  increase  the  performance  of  re-validating  design  rules.  This  work  proposes  an  automated  and  tool  supported  approach  that  re-validates  the  affected  parts  of  a  design  rule  only.  It  was  empirical  evaluated  on  19  design  rules  and  30  small  to  large  design  models  and  the  evaluation  shows  that  the  approach  improves  the  computational  cost  of  consistency  checking  with  the  gains  increasing  with  the  size  and  complexity  of  design  rules.
1	Scalable  armies  of  model  clones  through  data  sharing.  Cloning  a  model  is  usually  done  by  duplicating  all  its  runtime  objects  into  a  new  model.  This  approach  leads  to  memory  consumption  problems  for  operations  that  create  and  manipulate  large  quantities  of  clones  (e.g.,  design  space  exploration).  We  propose  an  original  approach  that  exploits  the  fact  that  operations  rarely  modify  a  whole  model.  Given  a  set  of  immutable  properties,  our  cloning  approach  determines  the  objects  and  fields  that  can  be  shared  between  the  runtime  representations  of  a  model  and  its  clones.  Our  generic  cloning  algorithm  is  parameterized  with  three  strategies  that  establish  a  trade-off  between  memory  savings  and  the  ease  of  clone  manipulation.  We  implemented  the  strategies  within  the  Eclipse  Modeling  Framework  (EMF)  and  evaluated  memory  footprints  and  computation  overheads  with  100  randomly  generated  metamodels  and  models.  Results  show  a  positive  correlation  between  the  proportion  of  shareable  properties  and  memory  savings,  while  the  worst  median  overhead  is  9,5%  when  manipulating  the  clones.
1	Blended  modelling  what  why  and  how.  Empirical  studies  indicate  that  user  experience  can  significantly  be  improved  in  model-driven  engineering.  Blended  modelling  aims  at  mitigating  this  by  enabling  users  to  interact  with  a  single  model  through  different  notations.  Blended  modelling  contributes  to  various  modelling  qualities,  including  comprehensibility,  analysability,  and  acceptability.  In  this  paper,  we  define  the  notion  of  blended  modelling  and  propose  a  set  of  dimensions  that  characterise  blended  modelling.  The  dimensions  are  grouped  in  two  classes:  user-oriented  dimensions  and  realisation-oriented  dimensions.  Each  dimension  describes  a  facet  that  is  relevant  to  blended  modelling  together  with  its  domain  (i.e.,  the  range  of  values  for  that  dimension).  The  dimensions  offer  a  basic  vocabulary  to  support  tool  developers  with  making  well-informed  design  decisions  as  well  as  users  to  select  appropriate  tools  and  configure  them  according  to  the  needs  at  hand.  We  illustrate  how  the  dimensions  apply  to  different  cases  relying  on  our  experience  with  blended  modelling.  We  discuss  the  impact  of  blended  modelling  on  usability  and  user  experience  and  sketch  metrics  to  measure  it.  Finally,  we  outline  a  number  of  core  research  directions  in  this  increasingly  important  modelling  area.
1	Making  software  verification  tools  really  work.  We  discuss  problems  and  barriers  which  stand  in  the  way  of  producing  verification  tools  that  are  robust,  scalable  and  integrated  in  the  software  development  cycle.  Our  analysis  is  that  these  barriers  span  a  spectrum  from  theoretical,  through  practical  and  even  logistical  issues.  Theoretical  issues  are  the  inherent  complexity  of  program  verification  and  the  absence  of  a  common,  accepted  semantic  model  in  tools.  Practical  hurdles  include  the  challenges  arising  from  real-world  systems  features,  such  as  floating-point  arithmetic  and  weak  memory.  Logistical  obstacles  we  identify  are  the  lack  of  standard  benchmarks  to  drive  tool  quality  and  efficiency,  and  the  difficulty  for  academic  research  institutions  of  allocating  resources  to  tool  development.  We  propose  simple  measures  which  we,  as  a  community,  could  adopt  to  make  the  design  of  serious  verification  tools  easier  and  more  credible.  Our  long-term  vision  is  for  the  community  to  produce  tools  that  are  indispensable  for  a  developer  but  so  seamlessly  integrated  into  a  development  environment,  as  to  be  invisible.
1	Trace  abstraction  refinement  for  timed  automata.  Timed  automata  are  a  well  known  formalism  for  modeling  real-time  systems.  Model  checking  of  timed  automata  is  important  for  ensuring  that  the  systems  satisfy  certain  properties.  Safety  is  one  of  the  most  important  properties  for  timed  automata.  In  this  paper  we  propose  a  method  for  the  safety  checking  of  timed  automata,  which  is  an  adaptation  of  the  general  trace  abstraction  refinement  framework  to  timed  automata.  The  feature  of  our  work  is  that  we  use  zone-based  LU-abstraction  instead  of  interpolation  techniques.  This  method  performs  zone  computation  only  when  necessary,  and  the  abstraction  on  zones  is  coarser  because  only  part  of  the  control  structure  is  considered  when  computing  LU-bounds.  We  give  an  example  to  show  when  this  method  could  perform  more  efficiently  than  the  traditional  zone-based  search  algorithm.
1	Linear  ranking  for  linear  lasso  programs.  The  general  setting  of  this  work  is  the  constraint-based  synthesis  of  termination  arguments.  We  consider  a  restricted  class  of  programs  called  lasso  programs.  The  termination  argument  for  a  lasso  program  is  a  pair  of  a  ranking  function  and  an  invariant.  We  present  the—to  the  best  of  our  knowledge—first  method  to  synthesize  termination  arguments  for  lasso  programs  that  uses  linear  arithmetic.We  prove  a  completeness  theorem.  The  completeness  theorem  establishes  that,  even  though  we  use  only  linear  (as  opposed  to  non-linear)  constraint  solving,  we  are  able  to  compute  termination  arguments  in  several  interesting  cases.  The  key  to  our  method  lies  in  a  constraint  transformation  that  replaces  a  disjunction  by  a  sum.
1	Formalizing  the  iso  iec  ieee  29119  software  testing  standard.  Model-based  testing  (MBT)  provides  a  systematic  and  automated  way  to  facilitate  rigorous  testing  of  software  systems.  MBT  has  been  an  intense  area  of  research  and  a  large  number  of  MBT  techniques  have  been  developed  in  the  literature  and  in  the  practice.  However,  all  of  the  techniques  have  been  developed  using  their  own  concepts  and  terminology  of  MBT,  which  are  very  often  different  than  other  techniques  and  at  times  have  conflicting  semantics.  Moreover,  while  working  on  MBT  projects  with  our  industrial  partners  in  the  last  several  years,  we  were  unable  to  find  a  unified  way  of  defining  MBT  techniques  based  on  standard  terminology.  To  precisely  define  MBT  concepts  with  the  aim  of  providing  common  understanding  of  MBT  terminology  across  techniques,  we  formalize  a  small  subset  of  the  recently  released  ISO/IEC/IEEE  29119  Software  Testing  Standard  as  a  conceptual  model  (UML  class  diagrams)  together  with  OCL  constraints.  The  conceptual  model  captures  all  the  necessary  concepts  based  on  the  standard  terminology  that  are  mandatory  or  optional  in  the  context  of  MBT  techniques  and  can  be  used  to  define  new  MBT  tools  and  techniques.  To  validate  the  conceptual  model,  we  instantiated  its  concepts  for  various  MBT  techniques  previously  developed  in  the  context  of  our  industrial  partners.  Such  instantiation  automatically  enforces  the  specified  OCL  constraints.  This  type  of  validation  provided  us  feedback  to  further  refine  the  conceptual  model.  Finally,  we  also  provide  our  experiences  and  lessons  learnt  for  such  formalization  and  validation.
1	Computer  assisted  integration  of  domain  specific  modeling  languages  using  text  analysis  techniques.  Following  the  principle  of  separation  of  concerns,  the  Model-driven  Engineering  field  has  developed  Domain-Specific  Modeling  Languages  DSML  to  address  the  increasing  complexity  of  the  systems  design.  In  this  context  of  heterogeneous  modeling  languages,  engineers  and  language  designers  are  facing  the  critical  problem  of  language  integration.  To  address  this  problem,  instead  of  doing  a  syntactic  analysis  based  on  the  domain  models  or  metamodels  as  it  is  common  practice  today,  we  propose  to  adopt  natural  language  processing  techniques  to  do  a  semantic  analysis  of  the  language  specifications.  We  evaluate  empirically  our  approach  on  seven  real  test  cases  and  compare  our  results  with  five  state  of  the  art  tools.  Results  show  that  the  semantic  analysis  of  textual  descriptions  that  accompany  DSMLs  can  efficiently  assist  engineers  to  make  well-informed  integration  choices.
1	Teaching  model  driven  engineering  from  a  relational  database  perspective.  We  reinterpret  MDE  from  the  viewpoint  of  relational  databases  to  provide  an  alternative  way  to  teach,  understand,  and  demonstrate  MDE  using  concepts  and  technologies  that  should  be  familiar  to  undergraduates.  We  use  1  relational  databases  to  express  models  and  metamodels,  2  Prolog  to  express  constraints  and  M2M  transformations,  3  Java  tools  to  implement  M2T  and  T2M  transformations,  and  4  OO  shell-scripting  languages  to  compose  MDE  transformations.  Case  studies  demonstrate  the  viability  of  our  approach.
1	Modeling  deep  reinforcement  learning  based  architectures  for  cyber  physical  systems.  Reinforcement  learning  is  a  sub-field  of  machine  learning  where  an  agent  aims  to  learn  a  behavior  or  a  policy  maximizing  a  reward  function  by  trial  and  error.  The  approach  is  particularly  interesting  for  the  design  of  autonomous  cyber-physical  systems  such  as  self-driving  cars.  In  this  work  we  present  a  generative,  domain-specific  modeling  framework  for  the  design,  training  and  integration  of  reinforcement  learning  systems.  It  consists  of  a  neural  network  modeling  language  which  is  used  to  design  the  models  to  be  trained,  e.g.  actor  and  critic  networks,  and  a  training  language  used  to  describe  the  training  procedure  and  set  the  corresponding  hyperparameters.  The  underlying  component  model  allows  the  modeler  to  embed  the  trained  networks  in  larger  component  &  connector  architectures.  We  illustrate  our  framework  by  the  example  of  a  self-driving  racing  car.
1	Cross  language  support  mechanisms  significantly  aid  software  development.  Contemporary  software  systems  combine  many  artifacts  specified  in  various  modeling  and  programming  languages,  domainspecific  and  general  purpose  as  well.  Since  multi-language  systems  are  so  widespread,  working  on  them  calls  for  tools  with  cross-language  support  mechanisms  such  as  (1)  visualization,  (2)  static  checking,  (3)  navigation,  and  (4)  refactoring  of  cross-language  relations.  We  investigate  whether  these  four  mechanisms  indeed  improve  efficiency  and  quality  of  development  of  multi-language  systems.  We  run  a  controlled  experiment  in  which  22  participants  perform  typical  software  evolution  tasks  on  the  JTrac  web  application  using  a  prototype  tool  implementing  these  mechanisms.  The  results  speak  clearly  for  integration  of  cross-language  support  mechanisms  into  software  development  tools,  and  justify  research  on  automatic  inference,  manipulation  and  handling  of  cross-language  relations.
1	Runtime  models  based  on  dynamic  decision  networks  enhancing  the  decision  making  in  the  domain  of  ambient  assisted  living  applications.  Dynamic  decision-making  for  self-Adaptive  systems  (SAS)  requires  the  runtime  trade-off  of  multiple  non-functional  requirements  (NFRs)  -Aka  quality  properties-And  the  costsbenefits  analysis  of  the  alternative  solutions.  Usually,  it  requires  the  specification  of  utility  preferences  for  NFRs  and  decisionmaking  strategies.  Traditionally,  these  preferences  have  been  defined  at  design-Time.  In  this  paper  we  develop  further  our  ideas  on  re-Assessment  of  NFRs  preferences  given  new  evidence  found  at  runtime  and  using  dynamic  decision  networks  (DDNs)  as  the  runtime  abstractions.  Our  approach  use  conditional  probabilities  provided  by  DDNs,  the  concepts  of  Bayesian  surprise  and  Primitive  Cognitive  Network  Process  (P-CNP),  for  the  determination  of  the  initial  preferences.  Specifically,  we  present  a  case  study  in  the  domain  problem  of  ambient  assisted  living  (AAL).  Based  on  the  collection  of  runtime  evidence,  our  approach  allows  the  identification  of  unknown  situations  at  the  design  stage.
1	Expressing  confidence  in  models  and  in  model  transformation  elements.  The  expression  and  management  of  uncertainty,  both  in  the  information  and  in  the  operations  that  manipulate  it,  is  a  critical  issue  in  those  systems  that  work  with  physical  environments.  Measurement  uncertainty  can  be  due  to  several  factors,  such  as  unreliable  data  sources,  tolerance  in  the  measurements,  or  the  inability  to  determine  if  a  certain  event  has  actually  happened  or  not.  In  particular,  this  contribution  focuses  on  the  expression  of  one  kind  of  uncertainty,  namely  the  confidence  on  the  model  elements,  i.e.,  the  degree  of  belief  that  we  have  on  their  occurrence,  and  on  how  such  an  uncertainty  can  be  managed  and  propagated  through  model  transformations,  whose  rules  can  also  be  subject  to  uncertainty.
1	Formalizing  execution  semantics  of  uml  profiles  with  fuml  models.  UML  Profiles  are  not  only  sets  of  annotations.  They  have  semantics.  Executing  a  model  on  which  a  profile  is  applied  requires  semantics  of  this  latter  to  be  considered.  The  issue  is  that  in  practice  semantics  of  profiles  are  mainly  specified  in  prose.  In  this  form  it  cannot  be  processed  by  tools  enabling  model  execution.  Although  latest  developments  advocate  for  a  standard  way  to  formalize  semantics  of  profiles,  no  such  approach  could  be  found  in  the  literature.  This  paper  addresses  this  issue  with  a  systematic  approach  based  on  fUML  to  formalize  the  execution  semantics  of  UML  profiles.  This  approach  is  validated  by  formalizing  the  execution  semantics  of  a  subset  of  the  MARTE  profile.  The  proposal  is  compatible  with  any  tool  implementing  UML  and  clearly  identifies  the  mapping  between  stereotypes  and  semantic  definitions.
1	Automatic  synthesis  of  heterogeneous  cpu  gpu  embedded  applications  from  a  uml  profile.  Modern  embedded  systems  present  an  ever  increasing  complexity  and  model-driven  engineering  has  been  shown  to  be  helpful  in  mitigating  it.  In  our  previous  works  we  exploited  the  power  of  model-driven  engineering  to  develop  a  round-trip  approach  for  aiding  the  evaluation  and  assessment  of  extra-functional  properties  preservation  from  models  to  code.  In  addition,  we  showed  how  the  round-trip  approach  could  be  employed  to  evaluate  different  deployment  strategies,  and  the  focus  was  on  homogeneous  CPUbased  platforms.  Due  to  the  fact  that  the  assortment  of  target-platforms  in  the  embedded  domain  is  inevitably  shifting  to  heterogeneous  solutions,  our  goal  is  to  broaden  the  scope  of  the  round-trip  approach  towards  mixed  CPU-GPU  configurations.  In  this  work  we  focus  on  the  modelling  of  heterogeneous  deployment  and  the  enhancement  of  the  current  automatic  code  generator  to  synthesize  code  targeting  such  heterogeneous  configurations.
1	Comprehending  feature  models  expressed  in  cvl.  Feature  modeling  is  a  common  way  to  present  and  manage  variability  of  software  and  systems.  As  a  prerequisite  for  effective  variability  management  is  comprehensible  representation,  the  main  aim  of  this  paper  is  to  investigate  difficulties  in  understanding  feature  models.  In  particular,  we  focus  on  the  comprehensibility  of  feature  models  as  expressed  in  Common  Variability  Language  (CVL),  which  was  recommended  for  adoption  as  a  standard  by  the  Architectural  Board  of  the  Object  Management  Group.  Using  an  experimental  approach  with  participants  familiar  and  unfamiliar  with  feature  modeling,  we  analyzed  comprehensibility  in  terms  of  comprehension  score,  time  spent  to  complete  tasks,  and  perceived  difficulty  of  different  feature  modeling  constructs.  The  results  showed  that  familiarity  with  feature  modeling  did  not  influence  the  comprehension  of  mandatory,  optional,  and  alternative  features,  although  unfamiliar  modelers  perceived  these  elements  more  difficult  than  familiar  modelers.  OR  relations  were  perceived  as  difficult  regardless  of  the  familiarity  level,  while  constraints  were  significantly  better  understood  by  familiar  modelers.  The  time  spent  to  complete  tasks  was  higher  for  familiar  modelers.
1	Towards  evolution  of  generic  variability  models.  We  present  an  approach  for  evolving  separate  variability  models  when  the  associated  base  model  is  altered.  The  Common  Variability  Language  (CVL)  is  a  generic  language  for  modeling  variability  in  base  models.  The  base  models  are  oblivious  to  the  associated  variability  models,  causing  additional  challenges  to  this  association  when  the  base  models  are  maintained.  Assuming  that  a  base  model  has  been  changed,  we  suggest  using  CVL  to  record  this  change.  Further  analysis  of  this  CVL  model  reveals  the  impact  of  the  change,  an  may  in  some  cases  result  in  automatic  evolution  of  the  variability  model  corresponding  to  the  changed  base  model.  We  illustrate  and  discuss  the  approach  using  an  example  from  the  train  domain.
1	Runtime  monitoring  of  functional  component  changes  with  behavior  models.  We  consider  the  problem  of  run-time  discovery  and  continuous  monitoring  of  new  components  that  live  in  an  open  environment.  We  focus  on  extracting  a  formal  model--which  may  not  be  available  upfront--by  observing  the  behavior  of  the  running  component.  We  show  how  the  model  built  at  run  time  can  be  enriched  through  new  observations  (dynamic  model  update).  We  also  use  the  inferred  model  to  perform  run-time  verification.  That  is,  we  try  to  identify  if  any  changes  are  made  to  the  component  that  modify  its  original  behavior,  contradict  the  previous  observations,  and  invalidate  the  inferred  model.
1	Towards  a  body  of  knowledge  for  model  based  software  engineering.  Model-based  Software  Engineering  (MBSE)  is  now  accepted  as  a  Software  Engineering  (SE)  discipline  and  is  being  taught  as  part  of  more  general  SE  curricula.  However,  an  agreed  core  of  concepts,  mechanisms  and  practices  ---  which  constitutes  the  Body  of  Knowledge  of  a  discipline  ---  has  not  been  captured  anywhere,  and  is  only  partially  covered  by  the  SE  Body  of  Knowledge  (SWEBOK).  With  the  goals  of  characterizing  the  contents  of  the  MBSE  discipline,  promoting  a  consistent  view  of  it  worldwide,  clarifying  its  scope  with  regard  to  other  SE  disciplines,  and  defining  a  foundation  for  a  curriculum  development  on  MBSE,  this  paper  provides  a  proposal  for  an  extension  of  the  contents  of  SWEBOK  with  the  set  of  fundamental  concepts,  terms  and  mechanisms  that  should  constitute  the  MBSE  Body  of  Knowledge.
1	Documenting  simulink  designs  of  embedded  systems.  The  importance  of  appropriate  software  design  documentation  has  been  well-established.  Yet  in  industrial  practice  design  documentation  of  large  software  systems  is  often  out  of  date  or  entirely  lacking  in  large  part  due  to  the  effort  required  to  produce  and  maintain  useful  design  documents.  While  model-based  design  (mbd)  partially  addresses  this  problem,  large  complex  models  still  require  additional  design  documentation  to  enable  development  and  maintenance.  This  paper  introduces  tool  support  for  documenting  the  Software  Design  Description  (sdd)  of  embedded  systems  developed  using  mbd  with  Simulink.  In  particular,  the  paper  proposes  a  template  for  a  sdd  of  a  Simulink  model.  Then,  the  tool  support  we  have  developed  for  semi-automatic  generation  of  sdds  from  the  template  is  introduced.  The  tool  support  integrates  MathWorks'  Simulink  Report  Generator  and  our  previously  developed  Signature  Tool  that  identifies  the  interfaces  of  Simulink  subsystems.
1	Mobilogleak  a  preliminary  study  on  data  leakage  caused  by  poor  logging  practices.  Logging  is  an  essential  software  practice  that  is  used  by  developers  to  debug,  diagnose  and  audit  software  systems.  Despite  the  advantages  of  logging,  poor  logging  practices  can  potentially  leak  sensitive  data.  The  problem  of  data  leakage  is  more  severe  in  applications  that  run  on  mobile  devices,  since  these  devices  carry  sensitive  identification  information  ranging  from  physical  device  identifiers  (e.g.,  IMEI  MAC  address)  to  communications  network  identifiers  (e.g.,  SIM,  IP,  Bluetooth  ID),  and  application-specific  identifiers  related  to  the  location  and  the  users'  accounts.  This  preliminary  study  explores  the  impact  of  logging  practices  on  data  leakage  of  such  sensitive  information.  Particularly,  we  want  to  investigate  whether  log-related  statements  inserted  into  an  application  code  could  lead  to  data  leakage.  While  studying  logging  practices  in  mobile  applications  is  an  active  research  area,  to  our  knowledge,  this  is  the  first  study  that  explores  the  interplay  between  logging  and  security  in  the  context  of  mobile  applications  for  Android.  We  propose  an  approach  called  MobiLogLeak,  an  approach  that  identifies  log  statements  in  deployed  apps  that  leak  sensitive  data.  MobiLogLeak  relies  on  taint  flow  analysis.  Among  5,000  Android  apps  that  we  studied,  we  found  that  200  apps  leak  sensitive  data  through  logging.
1	Harnessing  twitter  to  support  serendipitous  learning  of  developers.  Developers  often  rely  on  various  online  resources,  such  as  blogs,  to  keep  themselves  up-to-date  with  the  fast  pace  at  which  software  technologies  are  evolving.  Singer  et  al.  found  that  developers  tend  to  use  channels  such  as  Twitter  to  keep  themselves  updated  and  support  learning,  often  in  an  undirected  or  serendipitous  way,  coming  across  things  that  they  may  not  apply  presently,  but  which  should  be  helpful  in  supporting  their  developer  activities  in  future.  However,  identifying  relevant  and  useful  articles  among  the  millions  of  pieces  of  information  shared  on  Twitter  is  a  non-trivial  task.  In  this  work  to  support  serendipitous  discovery  of  relevant  and  informative  resources  to  support  developer  learning,  we  propose  an  unsupervised  and  a  supervised  approach  to  find  and  rank  URLs  (which  point  to  web  resources)  harvested  from  Twitter  based  on  their  informativeness  and  relevance  to  a  domain  of  interest.  We  propose  14  features  to  characterize  each  URL  by  considering  contents  of  webpage  pointed  by  it,  contents  and  popularity  of  tweets  mentioning  it,  and  the  popularity  of  users  who  shared  the  URL  on  Twitter.  The  results  of  our  experiments  on  tweets  generated  by  a  set  of  85,171  users  over  a  one-month  period  highlight  that  our  proposed  unsupervised  and  supervised  approaches  can  achieve  a  reasonably  high  Normalized  Discounted  Cumulative  Gain  (NDCG)  score  of  0.719  and  0.832  respectively.
1	Blockchain  and  contact  tracing  applications  for  covid  19  the  opportunity  and  the  challenges.  Contact  tracing  mobile  applications  have  been  emerging  as  potentially  automating  surveillance  technology  to  help  stem  the  spread  of  the  novel  coronavirus  (SARS-COV-2)  by  tracking  individuals  and  those  they  come  into  exposure  with.  The  avalanche  of  these  apps  left  the  software  security  researchers’  with  concerns  about  vulnerabilities  in  hastily  written  software.  On  the  other  hand,  the  COVID-19  pandemic  has  motivated  the  recent  interest  of  leveraging  blockchain  for  healthcare-related  scenarios,  including  proposing  and  developing  blockchain-based  contact  tracing  apps.  Utilizing  the  cryptographic  concepts  of  blockchain  to  secure  the  collected  data  could  help  in  winning  the  level  of  public  engagement  required  to  fight  the  spread  of  COVID-19.  But  will  blockchain  be  a  panacea  to  all  the  challenges  accompanying  these  apps?  Motivated  by  answering  this  question  and  following  a  twofold  process,  this  paper:  (i)  explores  the  current  landscape  of  contact  tracing  mobile  apps,  (ii)  examines  how  blockchain  technology  can  contribute  positively  to  this  landscape,  and  (iii)  reports  on  the  technical  and  social  challenges  that  still  accompany  the  deployment  of  blockchain-based  contact  tracing  apps.
1	Clone  notifier  developing  and  improving  the  system  to  notify  changes  of  code  clones.  A  code  clone  is  a  code  fragment  that  is  identical  or  similar  to  it  in  the  source  code.  It  has  been  identified  as  one  of  the  main  problems  in  software  maintenance.  When  a  developer  fixes  a  defect,  they  need  to  find  the  code  clones  corresponding  to  the  code  fragments.  In  this  paper,  we  present  Clone  Notifier,  a  system  that  alerts  on  creations  and  changes  of  code  clones  to  software  developers.  First,  Clone  Notifier  identifies  creations  and  changes  of  code  clones.  Subsequently,  it  groups  them  into  four  categories  (new,  deleted,  changed,  stable)  and  assigns  labels  (e.g.,  consistent,  inconsistent)  to  them.  Finally,  it  notifies  on  creations  and  changes  of  code  clones  along  with  the  corresponding  categories  and  labels.  Clone  Notifier  and  its  video  are  available  at:  https://github.com/s-tokui/CloneNotifier.
1	Detecting  code  clones  with  graph  neural  network  and  flow  augmented  abstract  syntax  tree.  Code  clones  are  semantically  similar  code  fragments  pairs  that  are  syntactically  similar  or  different.  Detection  of  code  clones  can  help  to  reduce  the  cost  of  software  maintenance  and  prevent  bugs.  Numerous  approaches  of  detecting  code  clones  have  been  proposed  previously,  but  most  of  them  focus  on  detecting  syntactic  clones  and  do  not  work  well  on  semantic  clones  with  different  syntactic  features.  To  detect  semantic  clones,  researchers  have  tried  to  adopt  deep  learning  for  code  clone  detection  to  automatically  learn  latent  semantic  features  from  data.  Especially,  to  leverage  grammar  information,  several  approaches  used  abstract  syntax  trees  (AST)  as  input  and  achieved  significant  progress  on  code  clone  benchmarks  in  various  programming  languages.  However,  these  AST-based  approaches  still  can  not  fully  leverage  the  structural  information  of  code  fragments,  especially  semantic  information  such  as  control  flow  and  data  flow.  To  leverage  control  and  data  flow  information,  in  this  paper,  we  build  a  graph  representation  of  programs  called  flow-augmented  abstract  syntax  tree  (FA-AST).  We  construct  FA-AST  by  augmenting  original  ASTs  with  explicit  control  and  data  flow  edges.  Then  we  apply  two  different  types  of  graph  neural  networks  (GNN)  on  FA-AST  to  measure  the  similarity  of  code  pairs.  As  far  as  we  have  concerned,  we  are  the  first  to  apply  graph  neural  networks  on  the  domain  of  code  clone  detection.  We  apply  our  FA-AST  and  graph  neural  networks  on  two  Java  datasets:  Google  Code  Jam  and  BigCloneBench.  Our  approach  outperforms  the  state-of-the-art  approaches  on  both  Google  Code  Jam  and  BigCloneBench  tasks.
1	Code  coverage  and  test  suite  effectiveness  empirical  study  with  real  bugs  in  large  systems.  During  software  maintenance,  testing  is  a  crucial  activity  to  ensure  the  quality  of  program  code  as  it  evolves  over  time.  With  the  increasing  size  and  complexity  of  software,  adequate  software  testing  has  become  increasingly  important.  Code  coverage  is  often  used  as  a  yardstick  to  gauge  the  comprehensiveness  of  test  cases  and  the  adequacy  of  testing.  A  test  suite  quality  is  often  measured  by  the  number  of  bugs  it  can  find  (aka.  kill).  Previous  studies  have  analysed  the  quality  of  a  test  suite  by  its  ability  to  kill  mutants,  i.e.,  artificially  seeded  faults.  However,  mutants  do  not  necessarily  represent  real  bugs.  Moreover,  many  studies  use  small  programs  which  increases  the  threat  of  the  applicability  of  the  results  on  large  real-world  systems.  In  this  paper,  we  analyse  two  large  software  systems  to  measure  the  relationship  of  code  coverage  and  its  effectiveness  in  killing  real  bugs  from  the  software  systems.  We  use  Randoop,  a  random  test  generation  tool  to  generate  test  suites  with  varying  levels  of  coverage  and  run  them  to  analyse  if  the  test  suites  can  kill  each  of  the  real  bugs  or  not.  In  this  preliminary  study,  we  have  performed  an  experiment  on  67  and  92  real  bugs  from  Apache  HTTPClient  and  Mozilla  Rhino,  respectively.  Our  experiment  finds  that  there  is  indeed  statistically  significant  correlation  between  code  coverage  and  bug  kill  effectiveness.  The  strengths  of  the  correlation,  however,  differ  for  the  two  software  systems.  For  HTTPClient,  the  correlation  is  moderate  for  both  statement  and  branch  coverage.  For  Rhino,  the  correlation  is  strong  for  both  statement  and  branch  coverage.
1	Two  improvements  to  detect  duplicates  in  stack  overflow.  Stack  Overflow  is  one  of  the  most  popular  question-and-answer  sites  for  programmers.  However,  there  are  a  great  number  of  duplicate  questions  that  are  expected  to  be  detected  automatically  in  a  short  time.  In  this  paper,  we  introduce  two  approaches  to  improve  the  detection  accuracy:  splitting  body  into  different  types  of  data  and  using  word-embedding  to  treat  word  ambiguities  that  are  not  contained  in  the  general  corpuses.  The  evaluation  shows  that  these  approaches  improve  the  accuracy  compared  with  the  traditional  method.
1	Towards  transparent  introspection.  There  is  a  growing  need  for  the  dynamic  analysis  of  sensitive  systems  thatdo  not  support  traditional  debugging  or  emulation  environments.  Analysiscan  alter  program  behavior,  necessitating  transparency.  For  example,  asthe  cat  and  mouse  game  between  malware  authors  and  malware  analystsprogresses,  malicious  software  can  increasingly  detect  and  confounddebuggers.  Analysts  must  understand  variable  values,  stack  traces,  andfactors  influencing  dynamic  behavior,  but  recent  malware  samples  leverageany  piece  of  information  or  artifact  available  that  signals  the  presence  ofa  debugger  or  emulator.  In  this  work,  we  advance  the  state-of-the-art  for  transparent  programanalysis  by  introducing  a  low-artifact  introspection  technique.  Ourapproach  uses  hardware-assisted  live  memory  snapshots  ofprocess  execution  on  native  targets  (e.g.,  x86  processors),  coupledwith  static  reasoning  about  programs.  We  produce  high-fidelity  data  and  control  flow  information  with  minimaldetectable  artifacts  that  could  influence  benign  subject  behavior  or  beleveraged  for  anti-analysis.  We  evaluate  our  system  using  two  hardwareimplementations  (x86-supported  System  Management  Mode  and  PCI-basedSlotScreamer  devices)  and  two  software  configurations  (benign  and  evasiveprograms).  We  also  analyze  the  theoretical  and  practical  limitations  of  our  technique.  We  discuss  an  expert  case  study  in  which  we  apply  our  technique  to  amalware  reverse  engineering  task.  Finally,  we  present  results  of  a  human  study  in  which  30  participantsperformed  debugging  tasks  using  information  provided  by  our  approach,  ourtool  was  as  useful  as  a  gdb  baseline,  but  applies  transparently.  Our  dynamic  analysis  approach  permitstransparent  introspection  to  access  previously-unavailable  informationabout  a  process's  internal  state  with  minimal  instrumentation  artifacts.
1	Systematic  comprehension  for  developer  reply  in  mobile  system  forum.  Review-based  software  development  has  become  increasingly  prevalent  in  recent  years.  Existing  efforts  aiming  at  either  informative  evaluation  or  sentiment  analysis  are  mainly  from  the  perspective  of  the  reviewers,  while  neglecting  the  attitude  and  behavior  of  the  developers.  Such  efforts  inevitably  suffer  from  recommendation  bias  in  practice,  and  thus  benefit  little  for  the  improvement  of  user  reviews.In  this  paper,  we  attempt  to  bridge  the  gap  between  user  review  and  developer  reply,  and  conduct  a  systematic  study  for  review  reply  in  development  forums,  especially  in  Chinese  mobile  system  forums.  To  this  end,  we  concentrate  on  three  research  questions:  1)  should  a  targeted  review  be  replied;  2)  how  long  time  it  should  be  replied;  3)  does  traditional  review  analysis  help  to  pursue  a  reply  for  certain  review?  To  answer  such  questions,  given  certain  review  datasets,  we  perform  a  systematical  study  including  the  following  three  stages:  1)  a  binary  classification  for  reply  behavior  prediction,  2)  a  regression  for  prediction  of  reply  time,  3)  a  systematic  factor  study  for  the  relationship  between  traditional  review  analysis  and  reply  performance.  To  enhance  the  accuracy  of  prediction  and  analysis,  we  proposed  a  CNN-based  weak-supervision  analysis  framework,  which  exploits  manifold  techniques  from  NLP  and  deep  learning.  We  validate  our  approach  via  extensive  comparison  experiments.  The  results  show  that  our  analysis  framework  is  effective.  More  importantly,  we  have  uncovered  several  interesting  findings,  which  provide  valuable  guidance  for  further  review  improvement  and  recommendation.
1	On  denotational  semantics  of  spatial  temporal  consistency  language  stec.  In  order  to  describe  the  requirement  of  spatial  and  temporal  consistency  of  cyber-physical  systems,  a  specification  language  called  as  STeC  was  proposed  by  Chen  in  [1].  In  this  paper,  we  focus  on  the  theory  of  semantics  of  STeC.  After  simply  restating  the  syntax  and  operational  semantics,  we  mainly  establish  the  denotational  semantics  of  STeC.  To  investigate  the  reasonability  of  the  denotational  semantics,  an  abstract  theorem  is  given  to  show  the  soundness  and  completeness  of  the  denotational  semantics.  Finally,  a  simple  case  about  China  Gaotie  (which  means  High-speed  train)  is  given  to  show  how  to  compute  the  operational  and  denotational  semantics.
1	Branch  obfuscation  using  black  boxes.  The  path  constraints  are  leaked  by  conditional  jump  instructions  which  are  the  binary  form  of  software's  internal  logic.  Based  on  the  problem  of  above,  reverse  engineering  using  path-sensitive  techniques  such  as  symbolic  execution  and  theorem  proving  poses  a  new  threat  to  software  intellectual  property  protection.  In  order  to  mitigate  path  information  leaking  problem,  we  propose  a  novel  obfuscation  technique  called  "black  box"  to  combat  the  state-of-art  reverse  engineering  techniques.  By  handling  the  branch  conditions  as  knowledge  embedded  into  black  boxes,  the  black  boxes  can  simulate  the  behaviors  of  the  obfuscated  branch  logic,  while  the  original  branch  condition  is  hidden.  We  show  that  based  on  the  incomprehensibility  of  black  boxes,  revealing  branch  conditions  hidden  by  our  method  is  considerably  harder  due  to  the  high  computational  cost.  The  results  of  the  experiment  further  indicate  that  besides  providing  effective  protection,  our  method  is  also  a  light-weight  branch  obfuscation  scheme.
1	Software  reliability  forecasting  singular  spectrum  analysis  and  arima  hybrid  model.  In  the  software  reliability  growth  phase,  the  nature  of  the  failure  data  is,  in  a  sense,  determined  by  the  software  testing  process.  A  hybrid  model  is  proposed  for  medium  and  long-term  software  failure  time  forecasting  in  this  paper.  The  hybrid  model  consists  of  two  methods,  Singular  Spectrum  Analysis  (SSA)  and  Auto  Regressive  Integrated  Moving  Average(ARIMA).  In  this  model,  the  time  series  of  software  failure  time  are  firstly  decomposed  into  several  sub-series  corresponding  to  some  tendentious  and  oscillation  (periodic  or  quasi-periodic)  components  and  noise  by  using  SSA  and  then  each  sub-series  is  predicted,  respectively,  through  an  appropriate  ARIMA  model,  and  lastly  a  correction  procedure  is  conducted  for  the  sum  of  the  prediction  results  to  ensure  the  superposed  residual  to  be  a  pure  random  series.  The  software  failure  data  of  two  real  projects  are  analyzed  as  case  studies.  The  results  have  been  compared  with  the  predictions  made  by  ARIMA  and  Singular  Spectrum  Analysis-Linear  Recurrent  Formulae  (SSA-LRF).  It  shows  that  the  hybrid  model  has  the  best  performance.
1	Exploiting  common  object  usage  in  test  case  generation.  Generated  test  cases  are  good  at  systematically  exploring  paths  and  conditions  in  software.  However,  generated  test  cases  often  do  not  make  sense.  We  adapt  test  case  generation  to  follow  patterns  of  common  object  usage,  as  mined  from  code  examples.  Our  experiments  show  that  generated  tests  thus  (a)  reuse  familiar  usage  patterns,  making  them  easier  to  understand  and  (b)  focus  on  common  usage,  thus  respecting  implicit  preconditions  and  avoiding  meaningless  tests.
1	Non  intrusive  mc  dc  measurement  based  on  traces.  We  present  a  novel,  non-intrusive  approach  to  MC/DC  coverage  measurement  using  modern  processor-based  tracing  facilities.  Our  approach  does  not  require  recompilation  or  instrumentation  of  the  software  under  test.  Instead,  we  use  the  Intel  Processor  Trace  (Intel  PT)  facility  present  on  modern  Intel  CPUs.  Our  tooling  consists  of  the  following  parts:  a  frontend  that  detects  so-called  decisions  (Boolean  expressions)  that  are  used  in  conditionals  in  C  source  code,  a  mapping  from  conditional  jumps  in  the  object  code  back  to  those  decisions,  and  an  analysis  that  computes  satisfaction  of  the  MC/DC  coverage  relation  on  those  decisions  from  an  execution  trace.  This  analysis  takes  as  input  a  stream  of  instruction  addresses  decoded  from  Intel  PT  trace  data,  which  was  recorded  while  running  the  software  under  test.  We  describe  our  architecture  and  discuss  limitations  and  future  work.
1	Conceptualization  and  evaluation  of  component  based  testing  unified  with  visual  gui  testing  an  empirical  study.  In  this  paper  we  present  the  results  of  a  two-phase  empirical  study  where  we  evaluate  and  compare  the  applicability  of  automated  component-based  Graphical  User  Interface  (GUI)  testing  and  Visual  GUI  Testing  (VGT)  in  the  tools  GUITAR  and  a  prototype  tool  we  refer  to  as  VGT  GUITAR.  First,  GUI  mutation  operators  are  defined  to  create  18  faulty  versions  of  an  application  on  which  both  tools  are  then  applied  in  an  experiment.  Results  from  456  test  case  executions  in  each  tool  show,  with  statistical  significance,  that  the  component-based  approach  reports  more  false  negatives  than  VGT  for  acceptance  tests  but  that  the  VGT  approach  reports  more  false  positives  for  system  tests.  Second,  a  case  study  is  performed  with  larger  open  source  applications,  ranging  from  8,803-55,006  lines  of  code.  Results  show  that  GUITAR  is  applicable  in  practice  but  has  some  challenges  related  to  GUI  component  states.  The  results  also  show  that  VGT  GUITAR  is  currently  not  applicable  in  practice  and  therefore  requires  further  research  and  development.  Based  on  the  study's  results  we  present  areas  of  future  work  for  both  test  approaches  and  conclude  that  the  approaches  have  different  benefits  and  drawbacks.  The  component-based  approach  is  robust  and  executes  tests  faster  than  the  VGT  approach,  with  a  factor  of  3.  However,  the  VGT  approach  can  perform  visual  assertions  and  is  perceived  more  flexible  than  the  component-  based  approach.  These  conclusions  let  us  hypothesize  that  a  combination  of  the  two  approaches  is  the  most  suitable  in  practice  and  therefore  warrants  future  research.
1	On  the  right  objectives  of  data  flow  testing.  This  paper  investigates  the  limits  of  current  data  flow  testing  approaches  from  a  radically  novel  viewpoint,  and  shows  that  the  static  data  flow  techniques  used  so  far  in  data  flow  testing  to  identify  the  test  objectives  fail  to  represent  the  universe  of  data  flow  relations  entailed  by  a  program.  This  paper  compares  the  data  flow  relations  computed  with  static  data  flow  approaches  with  the  ones  observed  while  executing  the  program.  To  this  end,  the  paper  introduces  a  dynamic  data  flow  technique  that  collects  the  data  flow  relations  observed  during  testing.  The  experimental  data  discussed  in  the  paper  suggest  that  data  flow  testing  based  on  static  techniques  misses  many  data  flow  test  objectives,  and  indicate  that  the  amount  of  missing  objectives  (false  negatives)  can  be  more  limiting  than  the  amount  of  infeasible  data  flow  relations  identified  statically  (false  positives).  This  opens  a  new  area  of  research  of  (dynamic)  data  flow  testing  techniques  that  can  better  encompass  the  test  objectives  of  data  flow  testing.
1	Crashing  simulated  planes  is  cheap  can  simulation  detect  robotics  bugs  early.  Robotics  and  autonomy  systems  are  becoming  increasingly  important,  moving  from  specialised  factory  domains  to  increasingly  general  and  consumer-focused  applications.  As  such  systems  grow  ubiquitous,  there  is  a  commensurate  need  to  protect  against  potentially  catastrophic  harm.  System-level  testing  in  simulation  is  a  particularly  promising  approach  for  assuring  robotics  systems,  allowing  for  more  extensive  testing  in  realistic  scenarios  and  seeking  bugs  that  may  not  manifest  at  the  unit-level.  Ideally,  such  testing  could  find  critical  bugs  well  before  expensive  field-testing  is  required.  However,  simulations  can  only  model  coarse  environmental  abstractions,  contributing  to  a  common  perception  that  robotics  bugs  can  only  be  found  in  live  deployment.  To  address  this  gap,  we  conduct  an  empirical  study  on  bugs  that  have  been  fixed  in  the  widely  used,  open-source  ArduPilot  system.  We  identify  bug-fixing  commits  by  exploiting  commenting  conventions  in  the  version-control  history.  We  provide  a  quantitative  and  qualitative  evaluation  of  the  bugs,  focusing  on  characterising  how  the  bugs  are  triggered  and  how  they  can  be  detected,  with  a  goal  of  identifying  how  they  can  be  best  identified  in  simulation,  well  before  field  testing.  To  our  surprise,  we  find  that  the  majority  of  bugs  manifest  under  simple  conditions  that  can  be  easily  reproduced  in  software-based  simulation.  Conversely,  we  find  that  system  configurations  and  forms  of  input  play  an  important  role  in  triggering  bugs.  We  use  these  results  to  inform  a  novel  framework  for  testing  for  these  and  other  bugs  in  simulation,  consistently  and  reproducibly.  These  contributions  can  inform  the  construction  of  techniques  for  automated  testing  of  robotics  systems,  with  the  goal  of  finding  bugs  early  and  cheaply,  without  incurring  the  costs  of  physically  testing  for  bugs  in  live  systems.
1	Empirical  studies  for  innovation  dissemination  ten  years  of  experience.  Context:  technology  transfer  and  innovation  dissemination  are  key  success  factors  for  an  enterprise.  The  shift  to  a  new  software  technology  determines  inevitable  changes  to  ingrained  and  familiar  processes  and  at  the  same  time  leads  to  changes  in  practices,  requires  training  and  commitment  on  behalf  of  technical  staff  and  management.  As  so,  it  cannot  leave  out  neither  organizational  nor  technical  factors.  Objective:  our  conjecture  is  that  the  process  of  innovation  dissemination  is  facilitated  if  the  new  technology  is  supported  by  empirical  evidence.  In  this  sense,  Empirical  Software  Engineering  (ESE)  serves  as  support  for  transferring  an  innovation,  either  it  being  a  process  or  product,  within  production  processes.  Method:  this  paper  investigates  the  relation  between  empirical  studies  and  technical/organizational  factors  in  order  to  identify  the  most  suitable  empirical  study  for  disseminating  an  innovation  in  an  enterprise.  The  analysis  has  been  carried  out  with  respect  to  empirical  studies  carried  out  during  a  ten  year  time  span  within  the  Software  Engineering  Research  LABoratory  (SERLAB),  at  the  University  of  Bari.  Results:  the  results  point  out  that  a  critical  factor  in  designing  empirical  studies  is  the  quality  model,  i.e.  measurement  program  defined  by  researchers  during  the  study  and  used  to  collect  relevant  information  on  the  effectiveness  and  efficacy  of  the  innovation  being  transferred,  in  order  to  gain  commitment  on  behalf  of  stakeholders  who  will  finally  adopt  the  technology.  Conclusion:  the  study  outcomes  provide  an  empirically  founded  guideline  that  can  be  used  when  choosing  the  most  appropriate  approach  for  addressing  an  innovation.
1	How  are  conceptual  models  used  in  industrial  software  development  a  descriptive  survey.  Background:  There  is  a  controversy  about  the  relevance,  role,  and  utility  of  models,  modeling,  and  modeling  languages  in  industry.  For  instance,  while  some  consider  UML  as  the  "lingua  franca  of  software  engineering",  others  claim  that  "the  majority1  [of  industry  practitioners]  simply  do  not  use  UML."      Objective:  We  aspire  to  evolve  this  debate  to  differentiate  the  circumstances  of  modeling,  and  the  degrees  of  formality  of  models.  Method:  We  have  conducted  an  online  survey  among  industry  practitioners  and  asked  them  how  and  for  what  purposes  they  use  models.  The  raw  (anonymized)  survey  data  is  published  online.      Results:  We  find  that  models  are  widely  used  in  industry,  and  UML  is  indeed  the  leading  language.  Three  distinct  usage  modes  of  models  are  reported,  the  most  frequent  of  which  is  informal  usage  for  communication  and  cognition.  MDE-style  usage  is  rare,  but  does  occur.  Software  architects  are  believed  to  benefit  most  from  modeling.      Conclusions:  Our  study  contrasts  and  complements  existing  studies,  and  offers  explanations  for  some  of  the  seeming  contradictions  of  previous  results.  There  might  be  cultural  differences  in  modeling  usage  that  are  worth  exploring  in  the  future.
1	A  machine  learning  approach  for  semi  automated  search  and  selection  in  literature  studies.  Background.  Search  and  selection  of  primary  studies  in  Systematic  Literature  Reviews  (SLR)  is  labour  intensive,  and  hard  to  replicate  and  update.  Aims.  We  explore  a  machine  learning  approach  to  support  semi-automated  search  and  selection  in  SLRs  to  address  these  weaknesses.  Method.  We  1)  train  a  classifier  on  an  initial  set  of  papers,  2)  extend  this  set  of  papers  by  automated  search  and  snowballing,  3)  have  the  researcher  validate  the  top  paper,  selected  by  the  classifier,  and  4)  update  the  set  of  papers  and  iterate  the  process  until  a  stopping  criterion  is  met.  Results.  We  demonstrate  with  a  proof-of-concept  tool  that  the  proposed  automated  search  and  selection  approach  generates  valid  search  strings  and  that  the  performance  for  subsets  of  primary  studies  can  reduce  the  manual  work  by  half.  Conclusions.  The  approach  is  promising  and  the  demonstrated  advantages  include  cost  savings  and  replicability.  The  next  steps  include  further  tool  development  and  evaluate  the  approach  on  a  complete  SLR.
1	Preliminary  study  on  applying  semi  supervised  learning  to  app  store  analysis.  Semi-Supervised  Learning  (SSL)  is  a  data  mining  technique  which  comes  between  supervised  and  unsupervised  techniques,  and  is  useful  when  a  small  number  of  instances  in  a  dataset  are  labelled  but  a  lot  of  unlabelled  data  is  also  available.  This  is  the  case  with  user  reviews  in  application  stores  such  as  the  Apple  App  Store  or  Google  Play,  where  a  vast  amount  of  reviews  are  available  but  classifying  them  into  categories  such  as  bug  related  review  or  feature  request  is  expensive  or  at  least  labor  intensive.  SSL  techniques  are  well-suited  to  this  problem  as  classifying  reviews  not  only  takes  time  and  effort,  but  may  also  be  unnecessary.  In  this  work,  we  analyse  SSL  techniques  to  show  their  viability  and  their  capabilities  in  a  dataset  of  reviews  collected  from  the  App  Store  for  both  transductive  (predicting  existing  instance  labels  during  training)  and  inductive  (predicting  labels  on  unseen  future  data)  performance.
1	Probabilistic  model  checking  of  pipe  protocol.  Pipe  protocol,  proposed  by  Zhao  [1]  in  early  2013,  is  one  application  layer  protocol  and  one  way  to  establish  the  Internet  of  Things,  under  which  can  different  kinds  of  hardware  platforms  communicate  with  each  other  faster  and  more  safely.  As  an  upper  layer  protocol,  Pipe  protocol  doesn't  define  details,  but  during  the  implementation,  probabilistic  and  nondeterministic  behaviors,  such  as  data  loss  and  external  choice,  are  possible  to  happen.  In  this  paper,  we  use  probabilistic  model  checker,  PRISM,  to  construct  the  probabilistic  Pipe  protocol  as  Probabilistic  Timed  Automata  (PTAs),  then  verify  some  useful  time-bounded  properties,  written  in  Probabilistic  Computation  Tree  Logic  (PCTL),  like,  "Maximum  probability  that  the  pipe  is  shut  down  after  Source  node  sends  all  5  data  packets  within  50  a#x03BC;s".  The  results  show  that  the  data  loss  probabilities,  number  of  data  and  deadline  should  be  restricted  suitably  if  we  require  the  maximum  probability  of  the  goal  reaches  some  value.  Our  model  is  proved  to  be  meaningful  and  we  can  give  helpful  suggestions  to  improve  the  implementation  of  Pipe  protocol.
1	Model  based  test  generation  using  evolutional  symbolic  grammar.  We  present  a  new  model-based  test  generation  approach  using  an  extended  symbolic  grammar,  which  is  used  as  a  formal  notation  for  enumerating  test  cases  for  communication  and  reactive  systems.  Our  model-based  test  generation  approach  takes  inputs  a  reactive  system  model,  in  Live  Sequence  Charts  (LSCs),  and  a  general  symbolic  grammar  serving  as  preliminary  test  coverage  criteria,  performs  an  automatic  simulation  for  consistency  testing  on  the  LSC  model  specification,  and  eventually  generates  an  evolved  symbolic  grammar  with  refined  test  coverage  criteria.  The  evolved  symbolic  grammar  can  either  be  used  to  generate  practical  test  cases  for  software  testing,  or  be  further  refined  by  applying  our  model-based  test  generation  approach  again  with  additional  test  coverage  criteria.
1	Designing  human  benchmark  experiments  for  testing  software  agents.  Background:  Software  agents  are  becoming  increasingly  common  in  the  engineering  of  software  systems.  We  explore  the  use  of  humans  in  creating  benchmarks  for  the  evaluation  of  software  agents.  In  our  case  studies,  we  address  the  domain  of  instructable  software  agents  (e-students)  as  proposed  by  the  Bootstrapped  Learning  project  [Oblinger,  2006].  Aim:  Our  aim  is  to  define  and  refine  requirements,  problem  solving  strategies,  and  evaluation  methodologies  for  e-students,  paving  the  way  for  rigorous  experiments  comparing  e-student  performance  with  human  benchmarks.  Method:  Little  was  known  about  what  factors  would  be  critical,  so  our  empirical  approach  is  exploratory  case  studies.  In  two  studies  covering  three  distinct  groups,  we  use  human  subjects  to  develop  an  evaluation  curriculum  for  e-students,  collecting  quantitative  data  through  online  quizzes  and  tests  and  qualitative  data  through  observation.  Results:  Though  we  collect  quantitative  data,  our  most  important  results  are  qualitative.  We  uncover  and  address  several  intrinsic  challenges  in  comparing  software  agents  with  humans,  including  the  greater  semantic  understanding  of  humans,  the  eidetic  memory  of  e-students,  and  the  importance  of  various  study  parameters  (including  timing  issues  and  lesson  complexity)  to  human  performance.  Conclusions:  Important  future  work  will  be  controlled  experiments  based  on  the  experience  of  these  case  studies.  These  will  provide  benchmark  human  performance  results  for  specific  problem  domains  for  comparison  to  e-student  results.
1	Using  the  scas  strategy  to  perform  the  initial  selection  of  studies  in  systematic  reviews  an  experimental  study.  Context:  Systematic  Review  (SR)  is  a  well-defined  and  rigorous  methodology  used  to  find  relevant  evidence  about  a  specific  topic  of  interest.  Depending  on  the  number  of  identified  primary  studies,  the  selection  activity  can  be  very  time-consuming  and  a  strategy,  like  SCAS,  to  semi-automate  this  activity  can  be  helpful.  Objective:  To  present  an  experimental  study  carried  out  to  evaluate  the  SCAS  strategy.  Method:  We  conducted  an  experiment  to  compare  the  efficiency  and  effectiveness  between  participants  using  SCAS  and  the  manual  approach.  They  received  necessary  training  for  applying  SCAS  using  tool  support.  They  were  divided  into  five  groups,  which  conducted  SRs  based  on  their  research  areas.  Results:  When  applying  SCAS,  the  average  effort  reduction  was  22.33%,  and  the  average  percentage  error  was  3.95%  with  a  minimal  loss  of  1.6  evidence  per  SR.  In  addition,  results  showed  an  overall  precision  of  65.49%  on  an  overall  recall  of  90.24%  when  using  SCAS.  The  overall  Kappa  showed  that  there  is  a  substantial  agreement  level  between  the  groups  and  SCAS.  Conclusion:  The  experiment  increased  the  confidence  in  the  strategy,  reinforcing  that  it  can  reduce  the  effort  required  to  select  primary  studies  without  adversely  affecting  the  overall  results  of  SRs.
1	Cross  project  software  fault  prediction  using  data  leveraging  technique  to  improve  software  quality.  Software  fault  prediction  is  a  process  to  detect  bugs  in  software  projects.  Fault  prediction  in  software  engineering  has  attracted  much  attention  from  the  last  decade.  The  early  prognostication  of  faults  in  software  minimize  the  cost  and  effort  of  errors  that  come  at  later  stages.  Different  machine  learning  techniques  have  been  utilized  for  fault  prediction,  that  is  proven  to  be  utilizable.  Despite,  the  significance  of  fault  prediction  most  of  the  companies  do  not  consider  fault  prediction  in  practice  and  do  not  build  useful  models  due  to  lack  of  data  or  lack  of  enough  data  to  strengthen  the  power  of  fault  predictors.  However,  models  trained  and  tested  on  less  amount  of  data  are  difficult  to  generalize,  because  they  do  not  consider  project  size,  project  differences,  and  features  selection.  To  overcome  these  issues,  we  proposed  an  instance-based  transfer  learning  through  data  leveraging  using  logistic  linear  regression  as  a  base  proposed  statistical  methodology.  In  our  study,  we  considered  three  software  projects  within  the  same  domain.  Finally,  we  performed  a  comparative  analysis  of  three  different  experiments  for  building  models  (targeted  project).  The  experimental  results  of  the  proposed  approach  show  promising  improvements  in  (SFP).
1	Test  case  reuse  in  enterprise  software  implementation  an  experience  report.  Organizations  are  making  substantial  investments  in  Enterprise  Software  Implementation  (ESI).  IT  service  providers  (ITSP)  execute  a  large  number  of  ESI  projects,  and  these  projects  tend  to  have  severe  cost  and  schedule  constraints.  Tata  Consultancy  Services  (TCS)  is  an  ITSP  that  has  developed  a  reusable  test  case  repository  with  an  objective  of  reducing  the  cost  of  testing  in  ESI  projects.  We  conducted  a  study  of  the  reuse  program  to  analyze  its  impact.  A  set  of  metrics  were  defined  and  relevant  data  was  gathered  from  users  of  the  repository.  In  this  paper,  we  document  the  findings  of  the  study.  Users  of  this  repository  have  reported  cost  savings  of  up  to  14.5%  in  their  test  design  efforts.  We  describe  the  repository  structure,  reuse  metrics  and  experience  gained  through  reuse.  We  motivate  the  need  for  further  research  on  test  notations/meta-models  for  transaction-oriented  business  applications  and  variability  management  within  these  models.
1	Tester  feedback  driven  fault  localization.  Coincidentally  correct  test  cases  are  those  that  execute  faulty  statements  but  do  not  cause  failures.  Such  test  cases  reduce  the  effectiveness  of  spectrum-based  fault  localization  techniques,  such  as  Ochiai,  because  the  correlation  of  failure  with  the  execution  of  a  faulty  statement  is  lowered.  Thus,  coincidentally  correct  test  cases  need  to  be  predicted  and  removed  from  the  test  suite  used  for  fault  localization.  Techniques  for  predicting  coincidentally  correct  test  cases  can  produce  false  positives,  such  as  when  one  predicts  a  fixed  percentage  that  is  higher  than  the  actual  percentage  of  coincidentally  correct  test  cases.  False  positives  may  cause  non-faulty  statements  to  be  assigned  higher  suspiciousness  scores  than  the  faulty  statements.  We  propose  an  approach  that  iteratively  predicts  and  removes  coincidentally  correct  test  cases.  In  each  iteration,  we  present  the  tester  the  set  of  statements  that  share  the  highest  Ochiai  suspiciousness  score.  If  the  tester  reports  that  these  statements  are  not  faulty,  we  use  that  feedback  to  determine  a  number  that  is  guaranteed  to  be  less  than  or  equal  to  the  actual  number  of  coincidentally  correct  test  cases.  We  predict  and  remove  that  number  of  coincidentally  correct  test  cases,  recalculate  the  suspiciousness  scores  of  the  remaining  statements,  and  repeat  the  process.  We  evaluated  our  approach  with  the  Siemens  benchmark  suite  and  the  Unix  utilities,  grep  and  gzip.  Our  approach  outperformed  an  existing  approach  that  predicts  a  fixed  percentage  of  test  cases  as  coincidentally  correct.  The  results  with  Ochiai  were  mixed.  In  some  cases,  our  approach  outperformed  Ochiai  by  up  to  67\%.  In  others,  Ochiai  was  more  effective.
1	Overcoming  web  server  benchmarking  challenges  in  the  multi  core  era.  Web-based  services  are  used  by  many  organizations  to  support  their  customers  and  employees.  An  important  consideration  in  developing  such  services  is  ensuring  the  Quality  of  Service  (QoS)  that  users  experience  is  acceptable.  Recent  years  have  seen  a  shift  toward  deploying  Web  service  son  multi-core  hardware.  Leveraging  the  performance  benefits  of  multi-core  hardware  is  a  non-trivial  task.  In  particular,  systematic  Web  server  benchmarking  techniques  are  needed  so  organizations  can  verify  their  ability  to  meet  customer  QoS  objectives  while  effectively  utilizing  such  hardware.  However,  our  recent  experiences  suggest  that  the  multi-core  era  imposes  significant  challenges  to  Web  server  benchmarking.  In  particular,  due  to  limitations  of  current  hardware  monitoring  tools,  we  found  that  a  large  number  of  experiments  are  needed  to  detect  complex  bottlenecks  that  can  arise  in  a  multi-core  system  due  to  contention  for  shared  resources  such  as  cache  hierarchy,  memory  controllers  and  processor  inter-connects.  Furthermore,  multiple  load  generator  instances  are  needed  to  adequately  stress  multi-core  hardware.  This  leads  to  practical  challenges  in  validating  and  managing  the  test  results.  This  paper  describes  the  automation  strategies  we  employed  to  overcome  these  challenges.  We  make  our  test  harness  available  for  other  researchers  and  practitioners  working  on  similar  studies.
1	Empirical  investigation  of  the  effects  of  test  suite  properties  on  similarity  based  test  case  selection.  Our  experience  with  applying  model-based  testing  on  industrial  systems  showed  that  the  generated  test  suites  are  often  too  large  and  costly  to  execute  given  project  deadlines  and  the  limited  resources  for  system  testing  on  real  platforms.  In  such  industrial  contexts,  it  is  often  the  case  that  only  a  small  subset  of  test  cases  can  be  run.  In  previous  work,  we  proposed  novel  test  case  selection  techniques  that  minimize  the  similarities  among  selected  test  cases  and  outperforms  other  selection  alternatives.  In  this  paper,  our  goal  is  to  gain  insights  into  why  and  under  which  conditions  similarity-based  selection  techniques,  and  in  particular  our  approach,  can  be  expected  to  work.  We  investigate  the  properties  of  test  suites  with  respect  to  similarities  among  fault  revealing  test  cases.  We  thus  identify  the  ideal  situation  in  which  a  similarity-based  selection  works  best,  which  is  useful  for  devising  more  effective  similarity  functions.  We  also  address  the  specific  situation  in  which  a  test  suite  contains  outliers,  that  is  a  small  group  of  very  different  test  cases,  and  show  that  it  decreases  the  effectiveness  of  similarity-based  selection.  We  then  propose,  and  successfully  evaluate  based  on  two  industrial  systems,  a  solution  based  on  rank  scaling  to  alleviate  this  problem.
1	Xstressor  automatic  generation  of  large  scale  worst  case  test  inputs  by  inferring  path  conditions.  An  important  part  of  software  testing  is  generation  of  worst-case  test  inputs,  which  exercise  a  program  under  extreme  loads.  For  such  a  task,  symbolic  execution  is  a  useful  tool  with  its  capability  to  reason  about  all  possible  execution  paths  of  a  program,  including  the  one  with  the  worst  case  behavior.  However,  symbolic  execution  suffers  from  the  path  explosion  problem  and  frequent  calls  to  a  constraint  solver,  which  make  it  impractical  to  be  used  at  a  large  scale.  To  address  the  issue,  this  paper  presents  XSTRESSOR  that  is  able  to  generate  test  inputs  that  can  run  specific  loops  in  a  program  with  the  worst-case  complexity  in  a  large  scale.  XSTRESSOR  synthetically  generates  the  path  condition  for  the  large-scale,  worst-case  execution  from  a  predictive  model  that  is  built  from  a  set  of  small  scale  tests.  XSTRESSOR  avoids  the  scaling  problem  of  prior  techniques  by  limiting  full-blown  symbolic  execution  and  run-time  calls  to  constraint  solver  to  small  scale  tests  only.  We  evaluate  XSTRESSOR  against  WISE  and  SPF-WCA,  the  most  closely  related  tools  to  generate  worst-case  test  inputs.  Results  show  that  XSTRESSOR  can  generate  the  test  inputs  faster  than  WISE  and  SPF-WCA,  and  also  scale  to  much  larger  input  sizes.
1	Invasive  software  testing  mutating  target  programs  to  diversify  test  exploration  for  high  test  coverage.  Software  testing  techniques  have  advanced  significantly  over  several  decades;  however,  most  of  current  techniques  still  test  a  target  program  as  it  is,  and  fail  to  utilize  valuable  information  of  diverse  test  executions  on  many  variants  of  the  original  program  in  test  generation.  This  paper  proposes  a  new  direction  for  software  testing  –  Invasive  Software  Testing  (IST).  IST  first  generates  a  set  of  target  program  variants  m1;  :::;mn  from  an  original  target  program  p  by  applying  mutation  operations  1;  :::;  n.  Second,  given  a  test  suite  T,  IST  executes  m1;  :::;mn  with  T  and  records  the  test  runs  which  increase  test  coverage  compared  to  p  with  T.  Based  on  the  recorded  information,  IST  generates  guideposts  for  automated  test  generation  on  p  toward  high  test  coverage.  Finally,  IST  generates  test  inputs  on  p  with  the  guideposts  and  achieves  higher  test  coverage.  We  developed  DEMINER  which  implements  IST  for  C  programs  through  software  mutation  and  concolic  testing.  Further,  we  showed  the  effectiveness  of  DEMINER  on  three  real-world  target  programs  Busybox-ls,  Busybox-printf,  and  GNU-find.  The  experiment  results  show  that  the  amount  of  the  improved  branch  coverage  by  DEMINER  is  24.7%  relatively  larger  than  those  of  the  conventional  concolic  testing  techniques  on  average.
1	From  autosar  models  to  co  simulation  for  mil  testing  in  the  automotive  domain.  Models  in  testing  are  important  for  describing,  understanding,  and  managing  tests.  In  the  automotive  domain,  AUTOSAR  is  an  important  standard  to  model  components  of  electronic  control  units.  AUTOSAR,  however,  lacks  information  about  tests  or  test  scenarios.  Early  testing  in  the  automotive  domain  is  often  done  by  Model-in-the-Loop  simulation.  Simulations  of  several  components  are  run  in  different  environments,  each.  Thus,  they  need  to  be  connected  (co-simulated)  for  integration  testing  of  these  components,  e.g.,  via  simulator  coupling.  This  paper  is  focused  on  closing  the  gap  between  AUTOSAR  models  and  co-simulation-based  test  scenarios  that  use  simulator  coupling.  Our  main  contribution  is  the  definition  of  a  modeling  language  to  describe  test  environments.  The  instances  of  this  language  can  be  derived  from  existing  AUTOSAR  models  and  can  also  be  used  to  automatically  generate  co-simulation-based  test  environments.  Furthermore,  we  provide  a  workflow  to  integrate  this  model  in  the  engineering  process  and  an  experiment  report  to  demonstrate  the  applicability  and  the  advantages  of  the  approach.
1	Oracle  based  regression  test  selection.  Regression  test  selection  (RTS)  techniques  attempt  to  reduce  regression  testing  costs  by  selecting  a  subset  of  a  software  system's  test  cases  for  use  in  testing  changes  made  to  that  system.  In  practice,  RTS  techniques  may  select  inordinately  large  sets  of  test  cases,  particularly  when  applied  to  industrial  systems  such  as  those  developed  at  ABB,  where  code  changes  may  have  far-reaching  impact.  In  this  paper,  we  present  a  new  RTS  technique  that  addresses  this  problem  by  focusing  on  specific  classes  of  faults  that  can  be  detected  by  internal  oracles  -  oracles  (rules)  that  enforce  constraints  on  system  states  during  system  execution.  Our  technique  uses  program  chopping  to  identify  code  changes  that  are  relevant  to  internal  oracles,  and  selects  test  cases  that  cover  these  changes.  We  present  the  results  of  an  empirical  study  that  show  that  our  technique  is  more  effective  and  efficient  than  other  RTS  techniques,  relative  to  the  classes  of  faults  targeted  by  the  internal  oracles.
1	Resttestgen  automated  black  box  testing  of  restful  apis.  RESTful  APIs  (or  REST  APIs  for  short)  represent  a  mainstream  approach  to  design  and  develop  Web  APIs  using  the  REpresentational  State  Transfer  architectural  style.  When  their  source  code  is  not  (or  just  partially)  available  or  the  analysis  across  many  dynamically  allocated  distributed  components  (typical  of  a  micro-services  architecture)  poses  obstacles  to  white-box  testing,  black-box  testing  becomes  a  viable  option.  Black-box  testing,  in  fact,  only  assumes  access  to  the  system  under  test  with  a  specific  interface.  This  paper  presents  RESTTESTGEN,  a  novel  approach  to  automatically  generate  test  cases  for  REST  APIs,  based  on  their  interface  definition  (in  Swagger).  Input  values  and  requests  are  generated  for  each  operation  of  the  API  under  test,  with  the  twofold  objective  of  testing  nominal  execution  scenarios  and  of  testing  error  scenarios.  Two  distinct  oracles  are  deployed  to  detect  when  test  cases  reveal  implementation  defects.  Our  empirical  investigation  shows  that  this  approach  is  effective  in  revealing  actual  faults  on  87  real-world  REST  APIs.
1	Practitioners  and  researchers  expectations  on  design  space  exploration  for  multicore  systems  in  the  automotive  and  avionics  domains  a  survey.  Background:  The  mobility  domains  are  moving  towards  the  adoption  of  multicore  technology.  Appropriate  methods,  techniques,  and  tools  need  to  be  developed  or  adapted  in  order  to  fulfill  the  existing  requirements.  This  is  a  case  for  design  space  exploration  methods  and  tools.  Objective:  Our  goal  was  to  understand  the  importance  of  different  design  space  exploration  goals  with  respect  to  their  relevance,  frequency  of  use,  and  tool  support  required  in  the  development  of  multicore  systems  from  the  point  of  view  of  the  ARAMiS  project  members.  Our  aim  was  to  use  the  results  to  guide  further  work  in  the  project.  Method:  We  conducted  a  survey  regarding  the  current  state  of  the  art  in  design  space  exploration  in  industry  and  research  and  collected  the  expectations  of  project  members  regarding  design  space  exploration  goals.  Results:  The  results  show  that  design  space  exploration  is  an  important  topic  in  industry  as  well  as  in  research.  It  is  used  very  often  with  different  important  goals  to  optimize  the  system.  Conclusions:  Current  tools  provide  only  partial  solutions  for  design  space  exploration.  Our  results  can  be  used  for  improving  them  and  guiding  their  development  according  to  the  priorities  explained  in  this  contribution.
1	Adopting  software  product  lines  a  systematic  mapping  study.  Context:  The  benefits  of  taking  a  product  line  approach  in  order  to  achieve  significant  reductions  in  cost  and  time  to  market  and,  at  the  same  time,  increasing  the  quality  has  encouraged  product  line  adoption.  Objective:  In  this  context,  this  study  focuses  on  some  SPL  adoption  aspects  and  has  the  following  goals:  investigate  state-of-the-art  SPL  adoption,  synthesize  available  evidence,  and  identify  gaps  between  required  strategies,  organizational  structures,  maturity  level  and  existing  adoption  barriers,  available  in  the  literature.  Method:  A  systematic  mapping  study  was  undertaken  to  analyze  the  important  aspects  that  should  be  considered  when  adopting  SPL  approaches.  A  set  of  four  questions  were  defined  in  which  34  primary  studies  were  evaluated.  Results:  A  total  of  34  primary  studies  were  considered.  They  reported  four  different  strategies  (Incremental,  Big  Bang,  Tactical  and  Pilot  project),  however  there  is  insufficient  information  about  how  such  strategies  link  to  factors  as  organizational  structure  and  process  maturity.  By  investigating  all  primary  studies  we  found  23  barriers  to  adoption.  Conclusions:  Researchers  need  to  consider  the  relationships  between  SPL  adoption  and  factors  such  as  company  maturity  and  organization  structure  in  more  detail.  There  is  also  a  need  for  patterns  to  assist  in  SPL  adoption  and  overcoming  SPL  adoption  barriers.
1	Do  software  engineering  practitioners  cite  research  on  software  testing  in  their  online  articles  a  preliminary  survey.  Background:  Software  engineering  (SE)  research  continues  to  study  the  degree  to  which  practitioners  perceive  that  SE  research  has  impact  on  practice.  Such  studies  typically  comprise  surveys  of  practitioner  opinions.  These  surveys  could  be  complemented  with  other  in  situ  practitioner  sources  e.g.  grey  literature.Objective:  To  investigate  whether  and  how  practitioners  cite  software  testing  research  in  their  online  articles.Method:  We  conduct  11,200  web  searches  using  a  customized  Google-based  search  tool,  scrape  the  pages  of  722  unique  results,  and  then  analyse  the  articles  for  citations  to  research.Results:  We  find  few  citations  to  research  (range  0%  -  1%  in  our  datasets)  although  this  is  similar  to  the  frequency  of  citations  to  practitioner  sites  (0%  -  4%).  We  find  and  discuss  the  only  two  significant  instances  of  practitioners  citing  research  in  our  datasets.Conclusion:  We  conducted  a  preliminary  survey  that  complements  the  findings  of  previous  work.  But  our  survey  contains  a  number  of  threats  to  validity.  Our  results  should  therefore  be  interpreted  as  hypotheses  to  motivate  further  investigation  into  the  frequency  to  which  practitioners  cite  research,  and  into  the  impact  of  research  on  practice.
1	Feature  weighting  for  case  based  reasoning  software  project  effort  estimation.  Background:  Software  effort  estimation  is  one  of  the  most  important  activities  in  software  development  process.  Unfortunately,  estimates  are  often  substantially  wrong  and  specifically  most  projects  encounter  effort  overruns.  Numerous  methods  have  been  proposed  including  Case  based  reasoning  (CBR).  Existing  research  shows  that  feature  subset  selection  (FSS)  is  an  important  aspect  of  CBR,  however,  searching  for  the  optimal  feature  weights  is  a  combinatorial  problem  and  therefore  NP-hard.      Objective:  To  develop  and  evaluate  efficient  algorithms  to  generalise  FSS  into  an  effective  feature  weighting  approach  that  can  improve  accuracy  further,  since  not  all  features  contribute  equally  to  solving  the  problem.      Method:  Use  various  search  algorithms  e.g.,  forward  sequential  weighting  (FSW)  and  random  mutation  hill  climbing  (RMHC)  to  assign  weight  to  features  in  order  to  improve  the  estimation  accuracy.  We  will  extend  an  existing  CBR  java  shell  ArchANGEL.  We  will  perform  experiments  based  on  repeated  measures  design  on  real  world  datasets  to  evaluate  these  algorithms.      Limitations  of  the  proposed  research:  Dataset  quality  cannot  be  assured  therefore  our  findings  could  be  influenced  by  noisy  data.  Older  datasets  may  be  misrepresenting  current  software  development  approaches  and  technologies.  CBR  could  be  sensitive  to  the  choice  of  distance  metric;  however,  we  will  only  use  standardised  Euclidean  distance.
1	Privdrm  a  privacy  preserving  secure  digital  right  management  system.  Digital  Right  Management  (DRM)  is  a  technology  developed  to  prevent  illegal  reproduction  and  distribution  of  digital  contents.  It  protects  the  rights  of  content  owners  by  allowing  only  authorised  consumers  to  legitimately  access  associated  digital  content.  DRM  systems  typically  use  a  consumer's  identity  for  authentication.  In  addition,  some  DRM  systems  collect  consumer's  preferences  to  obtain  a  content  license.  Thus,  the  behaviour  of  DRM  systems  disadvantages  the  digital  content  consumers  (i.e.  neglecting  consumers'  privacy)  focusing  more  on  securing  the  digital  content  (i.e.  biased  towards  content  owners).  This  paper  proposes  the  Privacy-Preserving  Digital  Rights  Management  System  (PrivDRM)  that  allows  a  consumer  to  acquire  digital  content  with  its  license  without  disclosing  complete  personal  information  and  without  using  any  third  parties.  To  evaluate  the  performance  of  the  proposed  solution,  a  prototype  of  the  PrivDRM  system  has  been  developed  and  investigated.  The  security  analysis  (attacks  and  threats)  are  analysed  and  showed  that  PrivDRM  supports  countermeasures  for  well-known  attacks  and  achieving  the  privacy  requirements.  In  addition,  a  comparison  with  some  well-known  proposals  shows  that  PrivDRM  outperforms  those  proposals  in  terms  of  processing  overhead.
1	Lazart  a  symbolic  approach  for  evaluation  the  robustness  of  secured  codes  against  control  flow  injections.  In  the  domain  of  smart  cards,  secured  devices  must  be  protected  against  high  level  attack  potential  [1].  According  to  norms  such  as  the  Common  Criteria  [2],  the  vulnerability  analysis  must  cover  the  current  state-of-the-art  in  term  of  attacks.  Nowadays,  a  very  classical  type  of  attack  is  fault  injection,  conducted  by  means  of  laser  based  techniques.  We  propose  a  global  approach,  called  Lazart,  to  evaluate  code  robustness  against  fault  injections  targeting  control  flow  modifications.  The  originality  of  Lazart  is  two  folds.  First,  we  encompass  the  evaluation  process  as  a  whole:  starting  from  a  fault  model,  we  produce  (or  establish  the  absence  of)  attacks,  taking  into  consideration  software  countermeasures.  Furthermore,  according  to  the  near  state-of-the-art,  our  methodology  takes  into  account  multiple  transient  fault  injections  and  their  combinatory.  The  proposed  approach  is  supported  by  an  effective  tool  suite  based  on  the  LLVM  format  [3]  and  the  KLEE  symbolic  test  generator  [4].
1	Messi  mutant  evaluation  by  static  semantic  interpretation.  Mutation  testing  is  effective  at  measuring  the  adequacy  of  a  test  suite,  but  it  can  be  computationally  expensive  to  apply  all  the  test  cases  to  each  mutant.  Previous  research  has  investigated  the  effect  of  reducing  the  number  of  mutants  by  selecting  certain  operators,  sampling  mutants  at  random,  or  combining  them  to  form  new  higher-order  mutants.  In  this  paper,  we  propose  a  new  approach  to  the  mutant  reduction  problem  using  static  analysis.  Symbolic  representations  are  generated  for  the  output  along  the  paths  through  each  mutant  and  these  are  compared  with  the  original  program.  By  calculating  the  range  of  their  output  expressions,  it  is  possible  to  determine  the  effect  of  each  mutation  on  the  program  output.  Mutants  with  little  effect  on  the  output  are  harder  to  kill.  We  confirm  this  using  random  testing  and  an  established  test  suite.  Competent  programmers  are  likely  to  only  make  small  mistakes  in  their  programming  code.  We  argue  therefore  that  test  suites  should  be  evaluated  against  those  mutants  that  are  harder  to  kill  without  being  equivalent  to  the  original  program.
1	Software  product  line  testing  a  3d  regression  testing  problem.  In  software  product  line  engineering,  testing  for  regression  concerns  not  only  versions,  as  in  one-off  product  development,  but  also  regression  across  variants.  We  propose  a  3D  process  model,  with  the  dimensions  of  level,  version  and  variant,  to  help  analyze,  plan  and  manage  software  product  line  testing.  We  derive  the  model  from  empirical  observations  of  regression  testing  practice  and  software  product  line  testing  theory  and  practice,  and  look  forward  to  see  the  model  evaluated  in  practitioner-oriented  research.
1	Fixing  of  security  vulnerabilities  in  open  source  projects  a  case  study  of  apache  http  server  and  apache  tomcat.  Software  vulnerabilities  are  particularly  dangerous  bugs  that  may  allow  an  attacker  to  violate  the  confidentiality,  integrity  or  availability  constraints  of  a  software  system.  Fixing  vulnerabilities  soon  is  of  primary  importance;  besides,  it  is  crucial  to  release  complete  patches  that  do  not  leave  any  corner  case  not  covered.  In  this  paper  we  study  the  process  of  vulnerability  fixing  in  Open  Source  Software.  We  focus  on  three  dimensions:  personal,  i.e.,  who  fixes  software  vulnerabilities;  temporal,  i.e.,  how  long  does  it  take  to  release  a  patch;  procedural,  i.e.,  what  is  the  process  followed  to  fix  the  vulnerability.  In  the  context  of  our  study  we  analyzed  337  CVE  Entries  regarding  Apache  HTTP  Server  and  Apache  Tomcat  and  we  manually  linked  them  to  the  patches  written  to  fix  such  vulnerabilities  and  their  related  commits.  The  results  show  that  developers  who  fix  software  vulnerabilities  are  much  more  experienced  than  the  average.  Furthermore,  we  observed  that  the  vulnerabilities  are  fixed  through  more  than  a  commit  and,  surprisingly,  that  in  about  3%  of  the  cases  such  vulnerabilities  show  up  again  in  future  releases  (i.e.,  they  are  not  actually  fixed).  In  the  light  of  such  results,  we  derived  some  lessons  learned  that  represent  a  starting  point  for  future  research  directions  aiming  at  better  supporting  developers  during  the  documentation  and  fixing  of  vulnerabilities.
1	Establishing  theoretical  minimal  sets  of  mutants.  Mutation  analysis  generates  tests  that  distinguish  variations,  or  mutants,  of  an  artifact  from  the  original.  Mutation  analysis  is  widely  considered  to  be  a  powerful  approach  to  testing,  and  hence  is  often  used  to  evaluate  other  test  criteria  in  terms  of  mutation  score,  which  is  the  fraction  of  mutants  that  are  killed  by  a  test  set.  But  mutation  analysis  is  also  known  to  provide  large  numbers  of  redundant  mutants,  and  these  mutants  can  inflate  the  mutation  score.  While  mutation  approaches  broadly  characterized  as  reduced  mutation  try  to  eliminate  redundant  mutants,  the  literature  lacks  a  theoretical  result  that  articulates  just  how  many  mutants  are  needed  in  any  given  situation.  Hence,  there  is,  at  present,  no  way  to  characterize  the  contribution  of,  for  example,  a  particular  approach  to  reduced  mutation  with  respect  to  any  theoretical  minimal  set  of  mutants.  This  paper's  contribution  is  to  provide  such  a  theoretical  foundation  for  mutant  set  minimization.  The  central  theoretical  result  of  the  paper  shows  how  to  minimize  efficiently  mutant  sets  with  respect  to  a  set  of  test  cases.  We  evaluate  our  method  with  a  widely-used  benchmark.
1	Transformation  rules  for  platform  independent  testing  an  empirical  study.  Most  Model-Driven  Development  projects  focus  on  model-level  functional  testing.  However,  our  recent  study  found  an  average  of  67%  additional  logic-based  test  requirements  from  the  code  compared  to  the  design  model.  The  fact  that  full  coverage  at  the  design  model  level  does  not  guarantee  full  coverage  at  the  code  level  indicates  that  there  are  semantic  behaviors  in  the  model  that  model-based  tests  might  miss,  e.g.,  conditional  behaviors  that  are  not  explicitly  expressed  as  predicates  and  therefore  not  tested  by  logic-based  coverage  criteria.  Avionics  standards  require  that  the  structure  of  safety  critical  software  is  covered  according  to  logic-based  coverage  criteria,  including  MCDC  for  the  highest  safety  level.  However,  the  standards  also  require  that  each  test  must  be  derived  from  the  requirements.  This  combination  makes  designing  tests  hard,  time  consuming  and  expensive  to  design.  This  paper  defines  a  new  model  that  uses  transformation  rules  to  help  testers  define  tests  at  the  platform  independent  model  level.  The  transformation  rules  have  been  applied  to  six  large  avionic  applications.  The  results  show  that  the  new  model  reduced  the  difference  between  model  and  code  with  respect  to  the  number  of  additional  test  requirements  from  an  average  of  67%  to  0%  in  most  cases  and  less  than  1%  for  all  applications.
1	Efficient  mutation  killers  in  action.  This  paper  presents  the  techniques  and  results  of  a  novel  model-based  test  case  generation  approach  that  automatically  derives  test  cases  from  UML  state  machines.  Mutation  testing  is  applied  on  the  modeling  level  to  generate  test  cases.  We  present  the  test  case  generation  approach,  discuss  the  tool  chain,  and  present  the  properties  of  the  generated  test  cases.  The  main  contribution  of  this  paper  is  an  empirical  study  of  a  car  alarm  system  where  different  strategies  for  killing  mutants  are  compared.  We  present  detailed  figures  on  the  effectiveness  of  the  test  case  generation  technique.  Although  UML  serves  as  an  input  language,  all  techniques  are  grounded  on  solid  foundations:  we  give  UML  state  transition  diagrams  a  formal  semantics  by  mapping  them  to  Back's  action  systems.
1	Model  based  fuzz  testing.  The  European  ITEA2  project  DIAMONDS  (Development  and  Industrial  Application  of  Multi-Domain  Security  Testing  Technologies)  develops  under  the  direction  of  Fraunhofer  FOKUS,  Berlin  efficient  and  automated  security  test  methods  for  security-critical,  networked  systems  in  various  industrial  domains  such  as  industrial  automation,  banking  and  telecommunications.  DIAMONDS  develops  methods  to  design  objective,  transparent,  repeatable,  and  automated  security  tests  that  focus  on  system  specifications  and  related  risks.  The  project  goals  include  the  development  of  a  security  test  pattern  catalogue  and  the  development  of  model-based  security  testing  techniques  such  as  risk-based  testing  and  model-based  fuzz  testing.  The  project  results  are  made  available  through  publications  and  contributions  to  the  standardization  at  ETSI  and  other  standardization  bodies.  The  presentation  focusses  on  model-based  fuzz  testing,  reviews  the  state  of  the  art,  compare  it  to  similar  approaches  such  as  mutation  testing,  and  presents  first  results  on  behaviour  fuzzing  for  security  testing.
1	Automated  scenario  based  testing  of  distributed  and  heterogeneous  systems.  In  this  document  we  outline  a  Ph.D.  research  plan  and  a  summary  of  preliminary  results  on  test  automation  for  distributed  and  heterogeneous  systems.
1	Agile  islands  in  a  waterfall  environment  challenges  and  strategies  in  automotive.  Driven  by  the  need  for  faster  time-to-market  and  reduced  development  lead-time,  large-scale  systems  engineering  companies  are  adopting  agile  methods  in  their  organizations.  This  agile  transformation  is  challenging  and  it  is  common  that  adoption  starts  bottom-up  with  agile  software  teams  within  the  context  of  traditional  company  structures.  This  creates  the  challenge  of  agile  teams  working  within  a  document-centric  and  plan-driven  (or  waterfall)  environment.  While  it  may  be  desirable  to  take  the  best  of  both  worlds,  it  is  not  clear  how  that  can  be  achieved  especially  with  respect  to  managing  requirements  in  large-scale  systems.  This  paper  presents  an  exploratory  case  study  focusing  on  two  departments  of  a  large-scale  systems  engineering  company  (automotive)  that  is  in  the  process  of  company-wide  agile  adoption.  We  present  challenges  that  agile  teams  face  while  working  within  a  larger  plan-driven  context  and  propose  potential  strategies  to  mitigate  the  challenges.  Challenges  relate  to,  e.g.,  development  teams  not  being  aware  of  the  high-level  requirements,  difficulties  to  manage  change  of  these  requirements  as  well  as  their  relationship  to  backlog  items  such  as  user  stories.  While  we  found  strategies  for  solving  most  of  the  challenges,  they  remain  abstract  and  empirical  research  on  their  effectiveness  is  currently  lacking.
1	Improving  the  scalability  of  formal  human  automation  interaction  verification  analyses  that  use  task  analytic  models.  The  enhanced  operator  function  model  with  communications  (EOFMCs)  is  a  task-analytic  modeling  formalism  used  for  including  human  behavior  in  formal  models  of  larger  systems.  This  allows  the  contribution  of  human  behavior  to  the  safety  of  the  system  to  be  evaluated  with  model  checking.  The  previous  method  for  translating  the  EOFMCs  into  model  checker  input  language  was  conceptually  straightforward,  but  extremely  statespace  inefficient.  This  limited  the  applications  that  could  be  formally  verified  using  EOFMC.  In  this  paper,  we  present  an  alternative  approach  for  formally  representing  EOFMCs  that  substantially  decreases  the  model's  statespace  size  and  verification  time.  This  paper  motivates  this  effort,  describes  how  the  improvement  was  achieved,  presents  benchmarks  demonstrating  the  improvements  in  statespace  size  and  verification  time,  discusses  the  implications  of  these  results,  and  outlines  directions  for  future  improvement.
1	Representativeness  models  of  systems  smart  grid  example.  Given  the  great  emphasis  being  placed  on  energy  efficiency  in  contemporary  society,  in  which  the  smart  grid  plays  a  prominent  role,  this  is  an  opportune  time  to  explore  methodologies  for  appropriately  representing  system  attributes.  We  suggest  this  is  important  for  effective  system  development  because  the  primary  factor  in  correctly  mapping  between  requirements  and  implementation  is  how  representative  the  system  design  is  of  requirements.  Since  representativeness  is  an  abstract  term,  it  is  imperative  to  identify  ways  to  quantify  it.  We  use  several  metrics.  Among  these  is  the  priority  of  system  elements  (e.g.,  electric  generator)  in  the  set  of  elements,  based  on  importance  to  system  success.  Secondly,  fault  tree  analysis  is  employed  to  identify  elements  that  operate  in  an  unsafe  state  and  the  probabilities  of  reaching  these  unsafe  states.  Thirdly,  state  transition  analysis  provides  traces  of  which  elements  are  on  the  routes  to  unsafe  states.  These  analyses  provide  the  information  needed  to  reduce  element  faults  and  failures  on  a  priority  basis.
1	A  content  analysis  process  for  qualitative  software  engineering  research.  A  review  of  the  qualitative  research  methods  discussed  in  papers  that  study  software  engineering  teams  showed  most  of  those  papers  did  not  follow  a  systematic  process  during  the  qualitative  analysis.  This  finding  is  concerning  as  this  deficiency  in  research  analysis  procedure  may  reduce  the  validity  and/or  completeness  of  the  qualitative  results.  Such  a  lack  of  rigor  may  be  a  result  of  qualitative  research  not  being  as  firmly  established  in  software  engineering  as  quantitative  research  methodologies.  In  engineering  research,  quantitative  methods  are  typically  more  prevalent  and  qualitative  analysis  is  part  of  a  mixed-method  analysis  process.  However,  when  researching  teams,  where  human  activity  is  abundant,  qualitative  analysis  may  need  to  take  precedence.  In  this  paper,  we  focus  on  the  qualitative  analysis  method  called  content  analysis  with  the  goal  of  presenting  a  rigorous  process  for  content  analysis  in  the  context  of  software  engineering.  We  then  present  and  demonstrate  the  use  of  that  content  analysis  process  for  software  engineering  researchers  using  two  examples.  An  analysis  of  215  articles  that  were  a  result  of  a  mapping  study  on  software  engineering  team  research  is  presented.  Those  papers  were  analyzed  to  determine  which  utilized  a  qualitative  data  analysis  method  in  their  research  in  addition  to  the  rigor  and  type  of  qualitative  analysis  performed.  We  ultimately  included  23  papers  in  this  study.  We  present  a  mapping  study  and  a  content  analysis  process  that  include  a  straightforward  way  to  select,  code,  and  present  data  in  both  inductive  and  deductive  studies.  We  demonstrated  the  process  using  the  keywords  from  the  papers  included  in  this  study  as  well  as  on  a  second  data  set  that  utilized  responses  from  structured  interview  transcripts  from  practicing  software  engineers.  The  first  dataset  also  resulted  in  a  taxonomy  to  categorize  software  engineering  team  research.  We  presented  and  demonstrated  a  content  analysis  process  in  terms  of  software  engineering  in  order  to  improve  future  qualitative  software  engineering  research  that  would  benefit  from  systematic  content  analysis.
1	An  empirical  comparison  of  developer  retention  in  the  rubygems  and  npm  software  ecosystems.  Software  ecosystems  can  be  viewed  as  socio-technical  networks  consisting  of  technical  components  (software  packages)  and  social  components  (communities  of  developers)  that  maintain  the  technical  components.  Ecosystems  evolve  over  time  through  socio-technical  changes  that  may  greatly  impact  the  ecosystem’s  sustainability.  Social  changes  like  developer  turnover  may  lead  to  technical  degradation.  This  motivates  the  need  to  identify  those  factors  leading  to  developer  abandonment,  in  order  to  automate  the  process  of  identifying  developers  with  high  abandonment  risk.  This  paper  compares  such  factors  for  two  software  package  ecosystems,  RubyGems  and  npm.  We  analyse  the  evolution  of  their  packages  hosted  on  GitHub,  considering  development  activity  in  terms  of  commits,  and  social  interaction  with  other  developers  in  terms  of  comments  associated  to  commits,  issues  or  pull  requests.  We  analyse  this  socio-technical  activity  for  more  than  30  and  60k  developers  for  RubyGems  and  npm,  respectively.  We  use  survival  analysis  to  identify  which  factors  coincide  with  a  lower  survival  probability.  Our  results  reveal  that  developers  with  a  higher  probability  to  abandon  an  ecosystem:  do  not  engage  in  discussions  with  other  developers;  do  not  have  strong  social  and  technical  activity  intensity;  communicate  or  commit  less  frequently;  and  do  not  participate  to  both  technical  and  social  activities  for  long  periods  of  time.  Such  observations  could  be  used  to  automate  the  identification  of  developers  with  a  high  probability  of  abandoning  the  ecosystem  and,  as  such,  reduce  the  risks  associated  to  knowledge  loss.
1	Information  security  and  open  source  dual  use  security  software  trust  paradox.  Nmap,  free  open  source  utility  for  network  exploration  or  security  auditing,  today  counts  for  thirteen  million  lines  of  code  representing  four  thousand  years  of  programming  effort.  Hackers  can  use  it  to  conduct  illegal  activities,  and  information  security  professionals  can  use  it  to  safeguard  their  network.  In  this  dual-use  context,  question  of  trust  is  raised.  Can  we  trust  programmers  developing  open  source  dual  use  security  software?  Motivated  by  this  research  question,  we  conducted  interviews  among  hackers  and  information  security  professionals,  and  explored  ohloh.net  database.  Our  results  show  that  contributors  behind  open  source  security  software  (OSSS)  are  hackers,  OSSS  have  important  dual-use  dimension,  information  security  professionals  generally  trust  OSSS,  and  large  organizations  will  avoid  adopting  and  using  OSSS.
1	Ginga  j  an  open  java  based  application  environment  for  interactive  digital  television  services.  This  paper  aims  to  present  a  Ginga-J’s  reference  implementation.  Although  based  on  a  particular  platform,  the  implementation  not  only  works  as  a  proof  of  concept,  but  also  raised  several  issues  and  difficulties  on  the  software  architecture  project  that  should  be  taken  into  account  to  ease  extensibility  and  porting  to  other  platforms.  Ginga  is  the  standard  middleware  for  the  Brazilian  DTV  System.  Its  imperative  environment  (Ginga-J)  is  based  on  new  JavaDTV  specification  and  mandatory  for  fixed  terrestrial  receptors.
1	Advances  in  evolutionary  multi  objective  optimization.  Started  during  1993-95  with  three  different  algorithms,  evolutionary  multi-objective  optimization  (EMO)  has  come  a  long  way  in  a  quick  time  to  establish  itself  as  a  useful  field  of  research  and  application.  Till  to  date,  there  exist  numerous  textbooks  and  edited  books,  commercial  softwares  dedicated  to  EMO  algorithms,  freely  downloadable  codes  in  most-used  computer  languages,  a  biannual  conference  series  (called  EMO  conference  series)  running  successfully  since  2001,  and  special  sessions  and  workshops  held  in  almost  all  major  evolutionary  computing  conferences.  In  this  paper,  we  discuss  briefly  the  principles  of  EMO  through  an  illustration  of  one  specific  algorithm.Thereafter,  we  focus  on  mentioning  a  few  recent  research  and  application  developments  of  EMO.  Specifically,  we  discuss  EMO's  use  with  multiple  criterion  decision  making  (MCDM)  procedures  and  EMO's  applicability  in  handling  of  a  large  number  of  objectives.  Besides,  the  concept  of  multi-objectivization  and  innovization  ---  which  are  practically  motivated,  is  discussed  next.  A  few  other  key  advancements  are  also  highlighted.  The  development  and  application  of  EMO  to  multi-objective  optimization  problems  and  their  continued  extensions  to  solve  other  related  problems  have  elevated  the  EMO  research  to  a  level  which  may  now  undoubtedly  be  termed  as  an  active  field  of  research  with  a  wide  range  of  theoretical  and  practical  research  and  application  opportunities.  EMO  concepts  are  ready  to  be  applied  to  search  based  software  engineering  (SBSE)  problems.
1	An  accidental  exploration  of  alternatives  to  evolutionary  algorithms  for  sbse.  SBSE  researchers  often  use  an  evolutionary  algorithm  to  solve  various  software  engineering  problems.  This  paper  explores  an  alternate  approach  of  sampling.  This  approach  is  called  SWAY  (Samplying  WAY)  and  finds  the  (near)  optimal  solutions  to  the  problem  by  (i)  creating  a  larger  initial  population  and  (ii)  intelligently  sampling  the  solution  space  to  find  the  best  subspace.  Unlike  evolutionary  algorithms,  SWAY  does  not  use  mutation  or  cross-over  or  multi-generational  reasoning  to  find  interesting  subspaces  but  relies  on  the  underlying  dimensions  of  the  solution  space.  Experiments  with  Software  Engineering  (SE)  models  shows  that  SWAY’s  performance  improvement  is  competitive  with  standard  MOEAs  while,  terminating  over  an  order  of  magnitude  faster.
1	Multi  level  automated  refactoring  using  design  exploration.  In  the  past  few  years,  there  has  been  a  growing  interest  in  automating  refactoring  activities  using  metaheuristic  approaches.  These  current  refactoring  approaches  involve  source-to-source  transformation.  However,  detailed  information  at  source-code  level  makes  precondition  checking  and  source-level  refactorings  hard  to  perform.  It  also  severely  limits  how  extensively  a  program  can  be  refactored.  While  design  improvement  tools  can  be  used  for  a  deep  and  fast  design  exploration,  it  is  left  to  the  programmer  to  manually  apply  the  required  refactorings  to  the  source  code,  which  is  a  burdensome  task.    To  tackle  the  above  problems,  our  proposal  is  based  on  a  multi-level  refactoring  approach  that  involves  both  design  and  source  code  in  the  refactoring  process.  Initially,  the  program  design  is  extracted  from  the  source  code.  Then,  in  a  design  exploration  phase,  using  a  metaheuristic  approach,  the  design  is  transformed  to  a  better  one  in  terms  of  a  metrics  suite  as  well  as  the  user  perspective.  The  source  code  is  then  refactored  based  on  both  the  improved  design  and  the  metrics  suite.  Using  this  approach,  we  expect  a  deeper  and  faster  exploration  of  the  program  design  space,  that  may  result  more  opportunities  for  design  improvement.
1	Avmf  an  open  source  framework  and  implementation  of  the  alternating  variable  method.  The  Alternating  Variable  Method  (AVM)  has  been  shown  to  be  a  fast  and  effective  local  search  technique  for  search-based  software  engineering.  Recent  improvements  to  the  AVM  have  generalized  the  representations  it  can  optimize  and  have  provably  reduced  its  running  time.  However,  until  now,  there  has  been  no  general,  publicly-available  implementation  of  the  AVM  incorporating  all  of  these  developments.  We  introduce  \(\mathrm{AVM}f\),  an  object-oriented  Java  framework  that  provides  such  an  implementation.  \(\mathrm{AVM}f\)  is  available  from  http://avmframework.org  for  configuration  and  use  in  a  wide  variety  of  projects.
1	Rosrv  runtime  verification  for  robots.  We  present  ROSRV,  a  runtime  verification  framework  for  robotic  applications  on  top  of  the  Robot  Operating  System  (ROS  [8]),  a  widely  used  open-source  framework  for  robot  software  development.  ROSRV  aims  to  address  the  safety  and  security  issues  of  robots  by  providing  a  transparent  monitoring  infrastructure  that  intercepts  and  monitors  the  commands  and  messages  passing  through  the  system.  Safety  and  security  properties  can  be  defined  in  a  formal  specification  language,  and  are  ensured  by  automatically  generated  monitors.  ROSRV  integrates  seamlessly  with  ROS—no  change  in  ROS  nor  the  application  code  is  needed.  ROSRV  has  been  applied  and  evaluated  on  a  commercial  robot.
1	A  scala  dsl  for  rete  based  runtime  verification.  Runtime  verification  (RV)  consists  in  part  of  checking  execution  traces  against  formalized  specifications.  Several  systems  have  emerged,  most  of  which  support  specification  notations  based  on  state  machines,  regular  expressions,  temporal  logic,  or  grammars.  The  field  of  Artificial  Intelligence  (AI)  has  for  an  even  longer  period  of  time  studied  rule-based  production  systems,  which  at  a  closer  look  appear  to  be  relevant  for  RV,  although  seemingly  focused  on  slightly  different  application  domains,  such  as  for  example  business  processes  and  expert  systems.  The  core  algorithm  in  many  of  these  systems  is  the  Rete  algorithm.  We  have  implemented  a  Rete-based  runtime  verification  system,  named  LogFire  (originally  intended  for  offline  log  analysis  but  also  applicable  to  online  analysis),  as  an  internal  DSL  in  the  Scala  programming  language,  using  Scala’s  support  for  defining  DSLs.  This  combination  appears  attractive  from  a  practical  point  of  view.  Our  contribution  is  in  part  conceptual  in  arguing  that  such  rule-based  frameworks  originating  from  AI  may  be  suited  for  RV.
1	First  international  competition  on  software  for  runtime  verification.  We  report  on  the  process  of  organizing  the  First  International  Competition  on  Software  for  Runtime  Verification  (CSRV).  The  report  describes  the  format,  participating  teams  and  evaluation  process.  The  competition  was  held  as  a  satellite  event  of  the  14th  International  Conference  on  Runtime  Verification  (RV’14).  The  Competition  was  organized  in  three  tracks:  offline  monitoring,  online  monitoring  of  C  programs,  and  online  monitoring  of  Java  programs.
1	Image  compression  using  haar  wavelet  based  tetrolet  transform.  Tetrolet  Transform,  a  wavelet  based  efficient  and  effective  transform,  utilizes  supports  formed  by  connecting  four  identical  squares  known  as  tetrominoes  such  that  each  is  connected  to  at  least  one  other  square  along  its  boundary.  Haar  wavelets  are  defined  on  these  tetrominoes  so  as  to  form  an  localized  orthonormal  basis.  The  procedure  is  applied  on  small  4×4  partitions  of  the  low  pass  image  and  the  sparsest  covering  from  each  partition  is  stored.  The  non-redundancy  in  the  wavelet  basis  results  in  sparse  image  representation.  Consequently  high  compression  ratio  is  achieved  after  the  application  of  a  wavelet  shrinkage  procedure  on  the  tetrolet  coefficients.  In  order  to  reduce  the  processing  cost  certain  modifications  in  the  original  Tetrolet  Transform  are  also  discussed.
1	Neural  predictive  monitoring.  Neural  State  Classification  (NSC)  is  a  recently  proposed  method  for  runtime  predictive  monitoring  of  Hybrid  Automata  (HA)  using  deep  neural  networks  (DNNs).  NSC  trains  a  DNN  as  an  approximate  reachability  predictor  that  labels  a  given  HA  state  x  as  positive  if  an  unsafe  state  is  reachable  from  x  within  a  given  time  bound,  and  labels  x  as  negative  otherwise.  NSC  predictors  have  very  high  accuracy,  yet  are  prone  to  prediction  errors  that  can  negatively  impact  reliability.  To  overcome  this  limitation,  we  present  Neural  Predictive  Monitoring  (NPM),  a  technique  based  on  NSC  and  conformal  prediction  that  complements  NSC  predictions  with  statistically  sound  estimates  of  uncertainty.  This  yields  principled  criteria  for  the  rejection  of  predictions  likely  to  be  incorrect,  without  knowing  the  true  reachability  values.  We  also  present  an  active  learning  method  that  significantly  reduces  both  the  NSC  predictor’s  error  rate  and  the  percentage  of  rejected  predictions.  Our  approach  is  highly  efficient,  with  computation  times  on  the  order  of  milliseconds,  and  effective,  managing  in  our  experimental  evaluation  to  successfully  reject  almost  all  incorrect  predictions.
1	An  open  source  monitoring  framework  for  enterprise  soa.  Web  services  monitoring  is  currently  emerging  as  an  effective  way  to  trace  faults  in  services  at  runtime.  The  lack  of  testing  information  provided  by  web  services  specifications  was  an  indication  that  other  methods  need  to  be  used  to  assess  the  quality  of  web  services.  This  is  mainly  due  to  the  fact  that  it  is  difficult  to  simulate  the  client  infrastructure  during  testing  of  web  services.  Monitoring  consists  of  inspecting  services  at  runtime  and  taking  adequate  actions  when  unacceptable  events  occur.  Monitoring  could  be  performed  by  different  stakeholders  and  could  target  different  properties  of  services.  Predominantly,  monitoring  is  performed  by  service  providers  to  manage  their  internal  resources  and  balance  their  requests  load.  In  our  effort  to  improve  the  monitoring  infrastructures,  we  propose  a  monitoring  framework  in  which  all  the  participants  (services  providers,  services  requestors)  can  contribute  to  monitoring  and  at  the  same  time  have  direct  access  to  the  monitoring  data.  This  paper  describes  a  monitoring  framework  developed  as  part  of  NEXOF-RA  project.  The  framework  offers  a  set  of  capabilities  for  a  collaborative  monitoring  of  web  services.  The  paper  presents  motivations,  system  design,  implementation  and  usage  of  the  framework.
1	Health  informatics  the  relevance  of  open  source  and  multilevel  modeling.  Health  information  features  significant  spatial-temporal  and  domain  complexities,  which  brings  challenges  to  the  implementation  of  patient-centered,  interoperable  and  semantically  coherent  healthcare  information  systems.  This  position  paper  supports  the  idea  that  the  multilevel  modeling  approach  is  essential  to  ensure  interoperability  at  the  semantic  level,  but  true  interoperability  is  only  achieved  by  the  adoption  of  open  standards,  and  open  source  implementations  are  needed  for  promote  competition  based  on  software  quality.  The  Multilevel  Healthcare  Information  Modelling  (MLHIM)  specifications  are  presented  as  the  fully  open  source  multilevel  modeling  reference  implementation,  and  best  practices  for  the  development  of  multilevel-based  open  source  healthcare  applications  are  suggested.
1	Examining  turnover  in  open  source  software  projects  using  logistic  hierarchical  linear  modeling  approach.  Developer  turnover  in  open  source  software  projects  is  a  critical  and  insufficiently  researched  problem.  Previous  research  has  focused  on  understanding  the  developer  motivations  to  contribute  using  either  the  individual  developer  perspective  or  the  project  perspective.  In  this  exploratory  study  we  argue  that  because  the  developers  are  embedded  in  projects  it  is  imperative  to  include  both  perspectives.  We  analyze  turnover  in  open  source  software  projects  by  including  both  individual  developer  level  factors,  as  well  as  project  specific  factors.  Using  the  Logistic  Hierarchical  Linear  Modeling  approach  allows  us  to  empirically  examine  the  factors  influencing  developer  turnover  and  also  how  these  factors  differ  among  developers  and  projects.
1	A  layered  approach  to  managing  risks  in  oss  projects.  In  this  paper,  we  propose  a  layered  approach  to  managing  risks  in  OSS  projects.  We  define  three  layers:  the  first  one  for  defining  risk  drivers  by  collecting  and  summarising  available  data  from  different  data  sources,  including  human-provided  contextual  information;  the  second  layer,  for  converting  these  risk  drivers  into  risk  indicators;  the  third  layer  for  assessing  how  these  indicators  impact  the  business  of  the  adopting  organisation.  The  contributions  are:  1)  the  complexity  of  gathering  data  is  isolated  in  one  layer  using  appropriate  techniques,  2)  the  context  needed  to  interpret  this  data  is  provided  by  expert  involvement  evaluating  risk  scenarios  and  answering  questionnaires  in  a  second  layer,  3)  a  pattern-based  approach  and  risk  reasoning  techniques  to  link  risks  to  business  goals  is  proposed  in  the  third  layer.
1	Bugtracking  a  tool  to  assist  in  the  identification  of  bug  reports.  Issue  tracking  systems  are  used,  in  most  software  projects,  but  in  particular  in  almost  all  free  open  source  software,  to  record  many  different  kinds  of  issues:  bug  reports,  feature  requests,  maintenance  tickets  and  even  design  discussions.  Identifying  which  of  those  issues  are  bug  reports  is  not  a  trivial  task.  When  researchers  want  to  conduct  studies  on  the  bug  reports,  managed  by  a  software  development  project,  first  of  all  they  need  to  perform  this  identification.
1	Open  source  open  innovation  and  intellectual  property  rights  a  lightning  talk.  Open  innovation  projects  are  fast  paced  aiming  at  producing  a  quick  proof  of  concept  of  an  innovative  software  product.  This  need  for  speedy  results  makes  the  use  of  open  source  components  as  a  basis  for  the  work  appealing.  Open  source  brings  with  it  an  inherent  risk  of  license  conflicts  that  may  become  an  issue  when  aiming  to  develope  an  innovative  demo  into  an  actual  product.  In  this  study,  the  first  results  of  investigating  the  knowledge  the  participants  of  innovation  projects  have  on  intellectual  property  are  presented.  The  effect  this  may  have  on  the  project  results  is  also  discussed.
1	Adaptive  game  based  e  leaming  using  semantic  web  technologies.  Technology  has  played  a  vital  role  in  education  and  significant  amount  of  research  have  been  performed  in  e-learning  field  to  allow  users  to  learn  beyond  the  traditional  class  rooms.  In  this  work,  we  have  developed  a  novel  adaptive  quiz/  game-based  e-learning  tool  using  Semantic  Web  technologies  which  re-uses  knowledge  from  DBpedia.  Although  there  are  works  on  game/quiz-based  e-learning  using  Semantic  Web,  none  of  the  previous  works  have  applied  personalization,  which  is  the  key  novelty  of  our  approach.  In  particular,  we  developed  a  novel  user  ontology  to  represent  information  about  users  and  apply  semantic  rules  to  adapt  the  game  to  different  users  based  on  their  knowledge.  In  the  quiz,  users  can  test  their  knowledge  in  Physics,  Chemistry,  and  Geography  domains  that  are  extracted  from  DBpedia.  In  order  to  test  the  effectiveness  of  the  proposed  adaptive  quiz-based  e-learning  system,  we  carried  out  user  evaluations  and  compare  our  system  with  a  baseline  system  (non-adaptive  Semantic  Web  based  version).  Evaluations  show  that  users  improved  their  performances  better  with  the  adaptive  game-based  e-learning  system  and  completed  the  quiz  faster  comparing  to  the  baseline  e-learning  system  without  adaptation.
1	Enhancing  throughput  viz  a  viz  fairness  in  uplink  diversity  based  vmimo  femtocell  environment.  Maximization  of  throughput  yet  maintaining  fairness  among  paired  user  equipments  (UEs)  in  uplink  Virtual  Multiple-Input  Multiple-Output  (VMIMO)  system  is  a  great  challenge  in  3rd  Generation  (3G)  Long-Term  Evolution  (LTE).  In  VMIMO  strategy,  different  UEs  with  single  antenna  are  paired  together  to  achieve  multiplexing/  diversity  gains  in  uplink  transmission.  User  pairing  scenario  poses  a  challenge  on  how  to  pair  two  UEs  together  for  achieving  maximum  system  throughput  yet  maintaining  fairness  among  UEs  in  Femtocell  environment.  In  this  paper,  we  propose  a  novel  adaptive  round  robin  scheduling  (A-RRS)  scheme  which  gives  a  tradeoff  between  throughput  and  fairness  in  single  user  VMIMO  (SU-VMIMO)  and  Multiple  User  VMIMO  (MU-VMIMO)  scenarios.  Simulation  results  show  that  the  proposed  A-RRS  scheme  outperform  other  traditional  pairing  schemes,  i.e.,  Round  robin  scheduling  (RRS),  Proportional  Fair  scheduling  (PFS)  and  Random  pairing  scheduling  (RPS)  etc  in  throughput  Vs  fairness  criteria.
1	Introduction  to  the  special  section  on  runtime  verification.  Runtime  verification  is  a  relatively  recent  area  of  research  that  concentrates  on  checking  system  execution  against  formally  specified  behavioral  properties.  Primary  research  directions  in  runtime  verification  include  logics  for  monitoring,  online  checking  algorithms,  extraction  of  observations  necessary  for  checking,  and  reduction  of  checking  overhead.  This  article  gives  a  brief  overview  of  runtime  verification  research.
1	Why  when  and  what  analyzing  stack  overflow  questions  by  topic  type  and  code.  Questions  from  Stack  Overflow  provide  a  unique  opportunity  to  gain  insight  into  what  programming  concepts  are  the  most  confusing.  We  present  a  topic  modeling  analysis  that  combines  question  concepts,  types,  and  code.  Using  topic  modeling,  we  are  able  to  associate  programming  concepts  and  identifiers  (like  the  String  class)  with  particular  types  of  questions,  such  as,  “how  to  perform  encoding”.
1	Will  my  patch  make  it  and  how  fast  case  study  on  the  linux  kernel.  The  Linux  kernel  follows  an  extremely  distributed  reviewing  and  integration  process  supported  by  130  developer  mailing  lists  and  a  hierarchy  of  dozens  of  Git  repositories  for  version  control.  Since  not  every  patch  can  make  it  and  of  those  that  do,  some  patches  require  a  lot  more  reviewing  and  integration  effort  than  others,  developers,  reviewers  and  integrators  need  support  for  estimating  which  patches  are  worthwhile  to  spend  effort  on  and  which  ones  do  not  stand  a  chance.  This  paper  crosslinks  and  analyzes  eight  years  of  patch  reviews  from  the  kernel  mailing  lists  and  committed  patches  from  the  Git  repository  to  understand  which  patches  are  accepted  and  how  long  it  takes  those  patches  to  get  to  the  end  user.  We  found  that  33%  of  the  patches  makes  it  into  a  Linux  release,  and  that  most  of  them  need  3  to  6  months  for  this.  Furthermore,  that  patches  developed  by  more  experienced  developers  are  more  easily  accepted  and  faster  reviewed  and  integrated.  Additionally,  reviewing  time  is  impacted  by  submission  time,  the  number  of  affected  subsystems  by  the  patch  and  the  number  of  requested  reviewers.
1	A  dataset  and  an  approach  for  identity  resolution  of  38  million  author  ids  extracted  from  2b  git  commits.  The  data  collected  from  open  source  projects  provide  means  to  model  large  software  ecosystems,  but  often  suffer  from  data  quality  issues,  specifically,  multiple  author  identification  strings  in  code  commits  might  actually  be  associated  with  one  developer.  While  many  methods  have  been  proposed  for  addressing  this  problem,  they  are  either  heuristics  requiring  manual  tweaking,  or  require  too  much  calculation  time  to  do  pairwise  comparisons  for  38M  author  IDs  in,  for  example,  the  World  of  Code  collection.  In  this  paper,  we  propose  a  method  that  finds  all  author  IDs  belonging  to  a  single  developer  in  this  entire  dataset,  and  share  the  list  of  all  author  IDs  that  were  found  to  have  aliases.  To  do  this,  we  first  create  blocks  of  potentially  connected  author  IDs  and  then  use  a  machine  learning  model  to  predict  which  of  these  potentially  related  IDs  belong  to  the  same  developer.  We  processed  around  38  million  author  IDs  and  found  around  14.8  million  IDs  to  have  an  alias,  which  belong  to  5.4  million  different  developers,  with  the  median  number  of  aliases  being  2  per  developer.  This  dataset  can  be  used  to  create  more  accurate  models  of  developer  behaviour  at  the  entire  OSS  ecosystem  level  and  can  be  used  to  provide  a  service  to  rapidly  resolve  new  author  IDs.
1	How  does  contributors  involvement  influence  the  build  status  of  an  open  source  software  project.  The  recent  introduction  of  the  pull-based  development  model  promoted  agile  development  practices  such  as  Code  Reviews  and  Continuous  Integration  (CI).  CI,  in  particular,  is  currently  a  standard  development  practice  in  open-source  software  (OSS)  projects.  Although  it  is  well-known  that  OSS  contributors  have  different  involvements  (e.g.,  while  some  developers  drive  the  project,  there  is  a  long  tail  of  peripheral  developers),  little  is  known  about  how  the  contributor's  degree  of  participation  can  influence  the  build  status  of  an  OSS  project.  Through  TravisTorrent's  dataset,  we  compare  the  success  rates  of  builds  made  by  casual  and  non-casual  contributors  and  what  factors  on  their  contributions  may  influence  the  build  result.  Our  results  suggest  that  there  is  no  representative  difference  between  their  build  success  (they  are  similar  in  85%  of  the  analyzed  projects),  meaning  that  being  a  casual  contributor  is  not  a  strong  indicator  for  creating  failing  builds.  Also,  factors  like  the  size  of  their  contributions  and  the  number  of  project  configurations  (jobs)  have  the  potential  of  impacting  the  build  success.
1	Sentiment  analysis  in  tickets  for  it  support.  Sentiment  analysis  has  been  adopted  in  software  engineeringfor  problems  such  as  software  usability  and  sentimentof  developers  in  open-source  projects.  This  paper  proposesa  method  to  evaluate  the  sentiment  contained  in  tickets  forIT  (Information  Technology)  support.IT  tickets  are  broadin  coverage  (e.g.  infrastructure,  software),  and  involve  errors,incidents,  requests,  etc.  The  main  challenge  is  to  automaticallydistinguish  between  factual  information,  whichis  intrinsically  negative  (e.g.  error  description),  from  thesentiment  embedded  in  the  description.  Our  approach  isto  automatically  create  a  Domain  Dictionary  that  containsterms  with  sentiment  in  the  IT  context,  used  to  filter  termsin  ticket  for  sentiment  analysis.  We  experiment  and  evaluatethree  approaches  for  calculating  the  polarity  of  terms  intickets.  Our  study  was  developed  using  34,895  tickets  fromfive  organizations,  from  which  we  randomly  selected  2,333tickets  to  compose  a  Gold  Standard.  Our  best  results  displayan  average  precision  and  recall  of  82.83%  and  88.42%,  whichoutperforms  the  compared  sentiment  analysis  solutions.
1	Qualboa  reusability  aware  recommendations  of  source  code  components.  Contemporary  software  development  processes  involve  finding  reusable  software  components  from  online  repositories  and  integrating  them  to  the  source  code,  both  to  reduce  development  time  and  to  ensure  that  the  final  software  project  is  of  high  quality.  Although  several  systems  have  been  designed  to  automate  this  procedure  by  recommending  components  that  cover  the  desired  functionality,  the  reusability  of  these  components  is  usually  not  assessed  by  these  systems.  In  this  work,  we  present  QualBoa,  a  recommendation  system  for  source  code  components  that  covers  both  the  functional  and  the  quality  aspects  of  software  component  reuse.  Upon  retrieving  components,  QualBoa  provides  a  ranking  that  involves  not  only  functional  matching  to  the  query,  but  also  a  reusability  score  based  on  configurable  thresholds  of  source  code  metrics.  The  evaluation  of  QualBoa  indicates  that  it  can  be  effective  for  recommending  reusable  source  code.
1	Lessons  learned  from  building  and  deploying  a  code  review  analytics  platform.  Tool-based  code  review  is  growing  in  popularity  and  has  become  a  standard  part  of  the  development  process  at  Mi-crosoft.  Adoption  of  these  tools  makes  it  possible  to  mine  data  from  code  reviews  and  provide  access  to  it.  In  this  paper,  we  pre-sent  an  experience  report  for  CodeFlow  Analytics,  a  system  that  collects  code  review  data,  generates  metrics  from  this  data,  and  provides  a  number  of  ways  for  development  teams  to  access  the  metrics  and  data.  We  discuss  the  design,  design  decisions  and  chal-lenges  that  we  encountered  when  building  CodeFlow  Analytics.  We  contacted  teams  that  used  CodeFlow  Analytics  over  the  past  two  years  and  discuss  what  prompted  them  to  use  CodeFlow  Ana-lytics,  how  they  have  used  it,  and  what  the  impact  has  been.  Fur-ther,  we  survey  research  that  has  been  enabled  by  using  the  Code-Flow  Analytics  platform.  We  provide  a  series  of  lessons  learned  from  this  experience  to  help  others  embarking  on  a  task  of  building  an  analytics  platform  in  an  enterprise  setting.
1	An  empirical  study  of  just  in  time  defect  prediction  using  cross  project  models.  Prior  research  suggests  that  predicting  defect-inducing  changes,  i.e.,  Just-In-Time  (JIT)  defect  prediction  is  a  more  practical  alternative  to  traditional  defect  prediction  techniques,  providing  immediate  feedback  while  design  decisions  are  still  fresh  in  the  minds  of  developers.  Unfortunately,  similar  to  traditional  defect  prediction  models,  JIT  models  require  a  large  amount  of  training  data,  which  is  not  available  when  projects  are  in  initial  development  phases.  To  address  this  flaw  in  traditional  defect  prediction,  prior  work  has  proposed  cross-project  models,  i.e.,  models  learned  from  older  projects  with  sufficient  history.  However,  cross-project  models  have  not  yet  been  explored  in  the  context  of  JIT  prediction.  Therefore,  in  this  study,  we  empirically  evaluate  the  performance  of  JIT  cross-project  models.  Through  a  case  study  on  11  open  source  projects,  we  find  that  in  a  JIT  cross-project  context:  (1)  high  performance  within-project  models  rarely  perform  well;  (2)  models  trained  on  projects  that  have  similar  correlations  between  predictor  and  dependent  variables  often  perform  well;  and  (3)  ensemble  learning  techniques  that  leverage  historical  data  from  several  other  projects  (e.g.,  voting  experts)  often  perform  well.  Our  findings  empirically  confirm  that  JIT  cross-project  models  learned  using  other  projects  are  a  viable  solution  for  projects  with  little  historical  data.  However,  JIT  cross-project  models  perform  best  when  the  data  used  to  learn  them  is  carefully  selected.
1	Mubench  a  benchmark  for  api  misuse  detectors.  Over  the  last  few  years,  researchers  proposed  a  multitude  of  automated  bug-detection  approaches  that  mine  a  class  of  bugs  that  we  call  API  misuses.  Evaluations  on  a  variety  of  software  products  show  both  the  omnipresence  of  such  misuses  and  the  ability  of  the  approaches  to  detect  them.  This  work  presents  MuBench,  a  dataset  of  89  API  misuses  that  we  collected  from  33  real-world  projects  and  a  survey.  With  the  dataset  we  empirically  analyze  the  prevalence  of  API  misuses  compared  to  other  types  of  bugs,  finding  that  they  are  rare,  but  almost  always  cause  crashes.  Furthermore,  we  discuss  how  to  use  it  to  benchmark  and  compare  API-misuse  detectors.
1	Which  work  item  updates  need  your  response.  Work-item  notifications  alert  the  team  collaborating  on  a  work-item  about  any  update  to  the  work-item  (e.g.,  addition  of  comments,  change  in  status).  However,  as  software  professionals  get  involved  with  multiple  tasks  in  project(s),  they  are  inundated  by  too  many  notifications  from  the  work-item  tool.  Users  are  upset  that  they  often  miss  the  notifications  that  solicit  their  response  in  the  crowd  of  mostly  useless  ones.  We  investigate  the  severity  of  this  problem  by  studying  the  work-item  repositories  of  two  large  collaborative  projects  and  conducting  a  user  study  with  one  of  the  project  teams.  We  find  that,  on  an  average,  only  1  out  of  every  5  notifications  that  are  received  by  the  users  require  a  response  from  them.  We  propose  TWINY  -  a  machine  learning  based  approach  to  predict  whether  a  notification  will  prompt  any  action  from  its  recipient.  Such  a  prediction  can  help  to  suitably  mark  up  notifications  and  to  decide  whether  a  notification  needs  to  be  sent  out  immediately  or  be  bundled  in  a  message  digest.  We  conduct  empirical  studies  to  evaluate  the  efficacy  of  different  classification  techniques  in  this  setting.  We  find  that  incremental  learning  algorithms  are  ideally  suited,  and  ensemble  methods  appear  to  give  the  best  results  in  terms  of  prediction  accuracy.
1	Trendy  bugs  topic  trends  in  the  android  bug  reports.  Studying  vast  volumes  of  bug  and  issue  discussions  can  give  an  understanding  of  what  the  community  has  been  most  concerned  about,  however  the  magnitude  of  documents  can  overload  the  analyst.  We  present  an  approach  to  analyze  the  development  of  the  Android  open  source  project  by  observing  trends  in  the  bug  discussions  in  the  Android  open  source  project  public  issue  tracker.  This  informs  us  of  the  features  or  parts  of  the  project  that  are  more  problematic  at  any  given  point  of  time.  In  turn,  this  can  be  used  to  aid  resource  allocation  (such  as  time  and  man  power)  to  parts  or  features.  We  support  these  ideas  by  presenting  the  results  of  issue  topic  distributions  over  time  using  statistical  analysis  of  the  bug  descriptions  and  comments  for  the  Android  open  source  project.  Furthermore,  we  show  relationships  between  those  time  distributions  and  major  development  releases  of  the  Android  OS.
1	An  extensive  dataset  of  uml  models  in  github.  The  Unified  Modeling  Language  (UML)  is  widely  taught  in  academia  and  has  good  acceptance  in  industry.  However,  there  is  not  an  ample  dataset  of  UML  diagrams  publicly  available.  Our  aim  is  to  offer  a  dataset  of  UML  files,  together  with  meta-data  of  the  software  projects  where  the  UML  files  belong  to.  Therefore,  we  have  systematically  mined  over  12  million  GitHub  projects  to  find  UML  files  in  them.  We  present  a  semi-automated  approach  to  collect  UML  stored  in  images,  .xmi,  and  .uml  files.  We  offer  a  dataset  with  over  93,000  UML  diagrams  from  over  24,000  projects  in  GitHub.
1	A  deeper  look  into  bug  fixes  patterns  replacements  deletions  and  additions.  Many  implementations  of  research  techniques  that  automatically  repair  software  bugs  target  programs  written  in  C.  Work  that  targets  Java  often  begins  from  or  compares  to  direct  translations  of  such  techniques  to  a  Java  context.  However,  Java  and  C  are  very  different  languages,  and  Java  should  be  studied  to  inform  the  construction  of  repair  approaches  to  target  it.  We  conduct  a  large-scale  study  of  bug-fixing  commits  in  Java  projects,  focusing  on  assumptions  underlying  common  search-based  repair  approaches.  We  make  observations  that  can  be  leveraged  to  guide  high  quality  automatic  software  repair  to  target  Java  specifically,  including  common  and  uncommon  statement  modifications  in  human  patches  and  the  applicability  of  previously-proposed  patch  construction  operators  in  the  Java  context.
1	Automated  topic  naming  to  support  cross  project  analysis  of  software  maintenance  activities.  Researchers  have  employed  a  variety  of  techniques  to  extract  underlying  topics  that  relate  to  software  development  artifacts.  Typically,  these  techniques  use  semi-unsupervised  machine-learning  algorithms  to  suggest  candidate  word-lists.  However,  word-lists  are  difficult  to  interpret  in  the  absence  of  meaningful  summary  labels.  Current  topic  modeling  techniques  assume  manual  labelling  and  do  not  use  domainspecific  knowledge  to  improve,  contextualize,  or  describe  results  for  the  developers.  We  propose  a  solution:  automated  labelled  topic  extraction.  Topics  are  extracted  using  Latent  Dirichlet  Allocation  (LDA)  from  commit-log  comments  recovered  from  source  control  systems  such  as  CVS  and  Bit-Keeper.  These  topics  are  given  labels  from  a  generalizable  cross-project  taxonomy,  consisting  of  non-functional  requirements.  Our  approach  was  evaluated  with  experiments  and  case  studies  on  two  large-scale  RDBMS  projects:  MySQL  and  MaxDB.  The  case  studies  show  that  labelled  topic  extraction  can  produce  appropriate,  context-sensitive  labels  relevant  to  these  projects,  which  provides  fresh  insight  into  their  evolving  software  development  activities.
1	Think  locally  act  globally  improving  defect  and  effort  prediction  models.  Much  research  energy  in  software  engineering  is  focused  on  the  creation  of  effort  and  defect  prediction  models.  Such  models  are  important  means  for  practitioners  to  judge  their  current  project  situation,  optimize  the  allocation  of  their  resources,  and  make  informed  future  decisions.  However,  software  engineering  data  contains  a  large  amount  of  variability.  Recent  research  demonstrates  that  such  variability  leads  to  poor  fits  of  machine  learning  models  to  the  underlying  data,  and  suggests  splitting  datasets  into  more  fine-grained  subsets  with  similar  properties.  In  this  paper,  we  present  a  comparison  of  three  different  approaches  for  creating  statistical  regression  models  to  model  and  predict  software  defects  and  development  effort.  Global  models  are  trained  on  the  whole  dataset.  In  contrast,  local  models  are  trained  on  subsets  of  the  dataset.  Last,  we  build  a  global  model  that  takes  into  account  local  characteristics  of  the  data.  We  evaluate  the  performance  of  these  three  approaches  in  a  case  study  on  two  defect  and  two  effort  datasets.  We  find  that  for  both  types  of  data,  local  models  show  a  significantly  increased  fit  to  the  data  compared  to  global  models.  The  substantial  improvements  in  both  relative  and  absolute  prediction  errors  demonstrate  that  this  increased  goodness  of  fit  is  valuable  in  practice.  Finally,  our  experiments  suggest  that  trends  obtained  from  global  models  are  too  general  for  practical  recommendations.  At  the  same  time,  local  models  provide  a  multitude  of  trends  which  are  only  valid  for  specific  subsets  of  the  data.  Instead,  we  advocate  the  use  of  trends  obtained  from  global  models  that  take  into  account  local  characteristics,  as  they  combine  the  best  of  both  worlds.
1	Mining  modern  repositories  with  elasticsearch.  Organizations  are  generating,  processing,  and  retaining  data  at  a  rate  that  often  exceeds  their  ability  to  analyze  it  effectively;  at  the  same  time,  the  insights  derived  from  these  large  data  sets  are  often  key  to  the  success  of  the  organizations,  allowing  them  to  better  understand  how  to  solve  hard  problems  and  thus  gain  competitive  advantage.  Because  this  data  is  so  fast-moving  and  voluminous,  it  is  increasingly  impractical  to  analyze  using  traditional  offline,  read-only  relational  databases.          Recently,  new  "big  data"  technologies  and  architectures,  including  Hadoop  and  NoSQL  databases,  have  evolved  to  better  support  the  needs  of  organizations  analyzing  such  data.  In  particular,  Elasticsearch  -  a  distributed  full-text  search  engine  -  explicitly  addresses  issues  of  scalability,  big  data  search,  and  performance  that  relational  databases  were  simply  never  designed  to  support.  In  this  paper,  we  reflect  upon  our  own  experience  with  Elasticsearch  and  highlight  its  strengths  and  weaknesses  for  performing  modern  mining  software  repositories  research.
1	Studying  the  dialogue  between  users  and  developers  of  free  apps  in  the  google  play  store.  The  popularity  of  mobile  apps  continues  to  grow  over  the  past  few  years.  Mobile  app  stores,  such  as  the  Google  Play  Store  and  Apple’s  App  Store  provide  a  unique  user  feedback  mechanism  to  app  developers  through  the  possibility  of  posting  app  reviews.  In  the  Google  Play  Store  (and  soon  in  the  Apple  App  Store),  developers  are  able  to  respond  to  such  user  feedback.  Over  the  past  years,  mobile  app  reviews  have  been  studied  excessively  by  researchers.  However,  much  of  prior  work  (including  our  own  prior  work)  incorrectly  assumes  that  reviews  are  static  in  nature  and  that  users  never  update  their  reviews.  In  a  recent  study,  we  started  analyzing  the  dynamic  nature  of  the  review-response  mechanism.  Our  previous  study  showed  that  responding  to  a  review  often  has  a  positive  effect  on  the  rating  that  is  given  by  the  user  to  an  app.  In  this  paper,  we  revisit  our  prior  finding  in  more  depth  by  studying  4.5  million  reviews  with  126,686  responses  for  2,328  top  free-to-download  apps  in  the  Google  Play  Store.  One  of  the  major  findings  of  our  paper  is  that  the  assumption  that  reviews  are  static  is  incorrect.  In  particular,  we  find  that  developers  and  users  in  some  cases  use  this  response  mechanism  as  a  rudimentary  user  support  tool,  where  dialogues  emerge  between  users  and  developers  through  updated  reviews  and  responses.  Even  though  the  messages  are  often  simple,  we  find  instances  of  as  many  as  ten  user-developer  back-and-forth  messages  that  occur  via  the  response  mechanism.  Using  a  mixed-effect  model,  we  identify  that  the  likelihood  of  a  developer  responding  to  a  review  increases  as  the  review  rating  gets  lower  or  as  the  review  content  gets  longer.  In  addition,  we  identify  four  patterns  of  developers:  1)  developers  who  primarily  respond  to  only  negative  reviews,  2)  developers  who  primarily  respond  to  negative  reviews  or  to  reviews  based  on  their  contents,  3)  developers  who  primarily  respond  to  reviews  which  are  posted  shortly  after  the  latest  release  of  their  app,  and  4)  developers  who  primarily  respond  to  reviews  which  are  posted  long  after  the  latest  release  of  their  app.  We  perform  a  qualitative  analysis  of  developer  responses  to  understand  what  drives  developers  to  respond  to  a  review.  We  manually  analyzed  a  statistically  representative  random  sample  of  347  reviews  with  responses  for  the  top  ten  apps  with  the  highest  number  of  developer  responses.  We  identify  seven  drivers  that  make  a  developer  respond  to  a  review,  of  which  the  most  important  ones  are  to  thank  the  users  for  using  the  app  and  to  ask  the  user  for  more  details  about  the  reported  issue.  Our  findings  show  that  it  can  be  worthwhile  for  app  owners  to  respond  to  reviews,  as  responding  may  lead  to  an  increase  in  the  given  rating.  In  addition,  our  findings  show  that  studying  the  dialogue  between  user  and  developer  can  provide  valuable  insights  that  can  lead  to  improvements  in  the  app  store  and  user  support  process.
1	Investigating  types  and  survivability  of  performance  bugs  in  mobile  apps.  A  recent  research  showed  that  mobile  apps  represent  nowadays  75%  of  the  whole  usage  of  mobile  devices.  This  means  that  the  mobile  user  experience,  while  tied  to  many  factors  (e.g.,  hardware  device,  connection  speed,  etc.),  strongly  depends  on  the  quality  of  the  apps  being  used.  With  “quality”  here  we  do  not  simply  refer  to  the  features  offered  by  the  app,  but  also  to  its  non-functional  characteristics,  such  as  security,  reliability,  and  performance.  This  latter  is  particularly  important  considering  the  limited  hardware  resources  (e.g.,  memory)  mobile  apps  can  exploit.  In  this  paper,  we  present  the  largest  study  at  date  investigating  performance  bugs  in  mobile  apps.  In  particular,  we  (i)  define  a  taxonomy  of  the  types  of  performance  bugs  affecting  Android  and  iOS  apps;  and  (ii)  study  the  survivability  of  performance  bugs  (i.e.,  the  number  of  days  between  the  bug  introduction  and  its  fixing).  Our  findings  aim  to  help  researchers  and  apps  developers  in  building  performance-bugs  detection  tools  and  focusing  their  verification  and  validation  activities  on  the  most  frequent  types  of  performance  bugs.
1	Msrbot  using  bots  to  answer  questions  from  software  repositories.  Software  repositories  contain  a  plethora  of  useful  information  that  can  be  used  to  enhance  software  projects.  Prior  work  has  leveraged  repository  data  to  improve  many  aspects  of  the  software  development  process,  such  as,  help  extract  requirement  decisions,  identify  potentially  defective  code  and  improve  maintenance  and  evolution.  However,  in  many  cases,  project  stakeholders  are  not  able  to  fully  benefit  from  their  software  repositories  due  to  the  fact  that  they  need  special  expertise  to  mine  their  repositories.  Also,  extracting  and  linking  data  from  different  types  of  repositories  (e.g.,  source  code  control  and  bug  repositories)  requires  dedicated  effort  and  time,  even  if  the  stakeholder  has  the  expertise  to  perform  such  a  task.  Therefore,  in  this  paper,  we  use  bots  to  automate  and  ease  the  process  of  extracting  useful  information  from  software  repositories.  Particularly,  we  lay  out  an  approach  of  how  bots,  layered  on  top  of  software  repositories,  can  be  used  to  answer  some  of  the  most  common  software  development/maintenance  questions  facing  developers.  We  perform  a  preliminary  study  with  12  participants  to  validate  the  effectiveness  of  the  bot.  Our  findings  indicate  that  using  bots  achieves  very  promising  results  compared  to  not  using  the  bot  (baseline).  Most  of  the  participants  (90.0%)  find  the  bot  to  be  either  useful  or  very  useful.  Also,  they  completed  90.8%  of  the  tasks  correctly  using  the  bot  with  a  median  time  of  40  seconds  per  task.  On  the  other  hand,  without  the  bot,  the  participants  completed  25.2%  of  the  tasks  with  a  median  time  of  240  seconds  per  task.  Our  work  has  the  potential  to  transform  the  MSR  field  by  significantly  lowering  the  barrier  to  entry,  making  the  extraction  of  useful  information  from  software  repositories  as  easy  as  chatting  with  a  bot.
1	An  automatic  method  for  assessing  the  versions  affected  by  a  vulnerability.  Vulnerability  data  sources  are  used  by  academics  to  build  models,  and  by  industry  and  government  to  assess  compliance.  Errors  in  such  data  sources  therefore  not  only  are  threats  to  validity  in  scientific  studies,  but  also  might  cause  organizations,  which  rely  on  retro  versions  of  software,  to  lose  compliance.  In  this  work,  we  propose  an  automated  method  to  determine  the  code  evidence  for  the  presence  of  vulnerabilities  in  retro  software  versions.  The  method  scans  the  code  base  of  each  retro  version  of  software  for  the  code  evidence  to  determine  whether  a  retro  version  is  vulnerable  or  not.  It  identifies  the  lines  of  code  that  were  changed  to  fix  vulnerabilities.  If  an  earlier  version  contains  these  deleted  lines,  it  is  highly  likely  that  this  version  is  vulnerable.  To  show  the  scalability  of  the  method  we  performed  a  large  scale  experiments  on  Chrome  and  Firefox  (spanning  7,236  vulnerable  files  and  approximately  9,800  vulnerabilities)  on  the  National  Vulnerability  Database  (NVD).  The  elimination  of  spurious  vulnerability  claims  (e.g.  entries  to  a  vulnerability  database  such  as  NVD)  found  by  our  method  may  change  the  conclusions  of  studies  on  the  prevalence  of  foundational  vulnerabilities.
1	Parameter  tuning  or  default  values  an  empirical  investigation  in  search  based  software  engineering.  Many  software  engineering  problems  have  been  addressed  with  search  algorithms.  Search  algorithms  usually  depend  on  several  parameters  (e.g.,  population  size  and  crossover  rate  in  genetic  algorithms),  and  the  choice  of  these  parameters  can  have  an  impact  on  the  performance  of  the  algorithm.  It  has  been  formally  proven  in  the  No  Free  Lunch  theorem  that  it  is  impossible  to  tune  a  search  algorithm  such  that  it  will  have  optimal  settings  for  all  possible  problems.  So,  how  to  properly  set  the  parameters  of  a  search  algorithm  for  a  given  software  engineering  problem?  In  this  paper,  we  carry  out  the  largest  empirical  analysis  so  far  on  parameter  tuning  in  search-based  software  engineering.  More  than  one  million  experiments  were  carried  out  and  statistically  analyzed  in  the  context  of  test  data  generation  for  object-oriented  software  using  the  EvoSuite  tool.  Results  show  that  tuning  does  indeed  have  impact  on  the  performance  of  a  search  algorithm.  But,  at  least  in  the  context  of  test  data  generation,  it  does  not  seem  easy  to  find  good  settings  that  significantly  outperform  the  “default”  values  suggested  in  the  literature.  This  has  very  practical  value  for  both  researchers  (e.g.,  when  different  techniques  are  compared)  and  practitioners.  Using  “default”  values  is  a  reasonable  and  justified  choice,  whereas  parameter  tuning  is  a  long  and  expensive  process  that  might  or  might  not  pay  off  in  the  end.
1	On  the  relation  of  control  flow  and  performance  feature  interactions  a  case  study.  Detecting  feature  interactions  is  imperative  for  accurately  predicting  performance  of  highly-configurable  systems.  State-of-the-art  performance  prediction  techniques  rely  on  supervised  machine  learning  for  detecting  feature  interactions,  which,  in  turn,  relies  on  time-consuming  performance  measurements  to  obtain  training  data.  By  providing  information  about  potentially  interacting  features,  we  can  reduce  the  number  of  required  performance  measurements  and  make  the  overall  performance  prediction  process  more  time  efficient.  We  expect  that  information  about  potentially  interacting  features  can  be  obtained  by  analyzing  the  source  code  of  a  highly-configurable  system,  which  is  computationally  cheaper  than  performing  multiple  performance  measurements.  To  this  end,  we  conducted  an  in-depth  qualitative  case  study  on  two  real-world  systems  (mbedTLS  and  SQLite),  in  which  we  explored  the  relation  between  internal  (precisely  control-flow)  feature  interactions,  detected  through  static  program  analysis,  and  external  (precisely  performance)  feature  interactions,  detected  by  performance-prediction  techniques  using  performance  measurements.  We  found  that  a  relation  exists  that  can  potentially  be  exploited  to  predict  performance  interactions.
1	How  bugs  are  born  a  model  to  identify  how  bugs  are  introduced  in  software  components.  When  identifying  the  origin  of  software  bugs,  many  studies  assume  that  “a  bug  was  introduced  by  the  lines  of  code  that  were  modified  to  fix  it”.  However,  this  assumption  does  not  always  hold  and  at  least  in  some  cases,  these  modified  lines  are  not  responsible  for  introducing  the  bug.  For  example,  when  the  bug  was  caused  by  a  change  in  an  external  API.  The  lack  of  empirical  evidence  makes  it  impossible  to  assess  how  important  these  cases  are  and  therefore,  to  which  extent  the  assumption  is  valid.  To  advance  in  this  direction,  and  better  understand  how  bugs  “are  born”,  we  propose  a  model  for  defining  criteria  to  identify  the  first  snapshot  of  an  evolving  software  system  that  exhibits  a  bug.  This  model,  based  on  the  perfect  test  idea,  decides  whether  a  bug  is  observed  after  a  change  to  the  software.  Furthermore,  we  studied  the  model’s  criteria  by  carefully  analyzing  how  116  bugs  were  introduced  in  two  different  open  source  software  projects.  The  manual  analysis  helped  classify  the  root  cause  of  those  bugs  and  created  manually  curated  datasets  with  bug-introducing  changes  and  with  bugs  that  were  not  introduced  by  any  change  in  the  source  code.  Finally,  we  used  these  datasets  to  evaluate  the  performance  of  four  existing  SZZ-based  algorithms  for  detecting  bug-introducing  changes.  We  found  that  SZZ-based  algorithms  are  not  very  accurate,  especially  when  multiple  commits  are  found;  the  F-Score  varies  from  0.44  to  0.77,  while  the  percentage  of  true  positives  does  not  exceed  63%.  Our  results  show  empirical  evidence  that  the  prevalent  assumption,  “a  bug  was  introduced  by  the  lines  of  code  that  were  modified  to  fix  it”,  is  just  one  case  of  how  bugs  are  introduced  in  a  software  system.  Finding  what  introduced  a  bug  is  not  trivial:  bugs  can  be  introduced  by  the  developers  and  be  in  the  code,  or  be  created  irrespective  of  the  code.  Thus,  further  research  towards  a  better  understanding  of  the  origin  of  bugs  in  software  projects  could  help  to  improve  design  integration  tests  and  to  design  other  procedures  to  make  software  development  more  robust.
1	Privacy  by  designers  software  developers  privacy  mindset.  Privacy  by  design  (PbD)  is  a  policy  measure  that  guides  software  developers  to  apply  inherent  solutions  to  achieve  better  privacy  protection.  For  PbD  to  be  a  viable  option,  it  is  important  to  understand  developers'  perceptions,  interpretation  and  practices  as  to  informational  privacy  (or  data  protection).  To  this  end,  we  conducted  in-depth  interviews  with  27  developers  from  different  domains,  who  practice  software  design.  Grounded  analysis  of  the  data  revealed  an  interplay  between  several  different  forces  affecting  the  way  in  which  developers  handle  privacy  concerns.  Borrowing  the  schema  of  Social  Cognitive  Theory  (SCT),  we  classified  and  analyzed  the  cognitive,  organizational  and  behavioral  factors  that  play  a  role  in  developers'  privacy  decision  making.  Our  findings  indicate  that  developers  use  the  vocabulary  of  data  security  to  approach  privacy  challenges,  and  that  this  vocabulary  limits  their  perceptions  of  privacy  mainly  to  third-party  threats  coming  from  outside  of  the  organization;  that  organizational  privacy  climate  is  a  powerful  means  for  organizations  to  guide  developers  toward  particular  practices  of  privacy;  and  that  software  architectural  patterns  frame  privacy  solutions  that  are  used  throughout  the  development  process,  possibly  explaining  developers'  preference  of  policy-based  solutions  to  architectural  solutions.  Further,  we  show,  through  the  use  of  the  SCT  schema  for  framing  the  findings  of  this  study,  how  a  theoretical  model  of  the  factors  that  influence  developers'  privacy  practices  can  be  conceptualized  and  used  as  a  guide  for  future  research  toward  effective  implementation  of  PbD.
1	Do  programmers  do  change  impact  analysis  in  debugging.  "Change  Impact  Analysis"  is  the  process  of  determining  the  consequences  of  a  modification  to  software.  In  theory,  change  impact  analysis  should  be  done  during  software  maintenance,  to  make  sure  changes  do  not  introduce  new  bugs.  Many  approaches  and  techniques  are  proposed  to  help  programmers  do  change  impact  analysis  automatically.  However,  it  is  still  an  open  question  whether  and  how  programmers  do  change  impact  analysis.  In  this  paper,  we  conducted  two  studies,  one  in-depth  study  and  one  breadth  study.  For  the  in-depth  study,  we  recorded  videos  of  nine  professional  programmers  repairing  two  bugs  for  two  hours.  For  the  breadth  study,  we  surveyed  35  professional  programmers  using  an  online  system.  We  found  that  the  programmers  in  our  studies  did  static  change  impact  analysis  before  they  made  changes  by  using  IDE  navigational  functionalities,  and  they  did  dynamic  change  impact  analysis  after  they  made  changes  by  running  the  programs.  We  also  found  that  they  did  not  use  any  change  impact  analysis  tools.
1	Eye  tracking  analysis  of  computer  program  comprehension  in  programmers  with  dyslexia.  This  paper  investigates  the  impact  of  dyslexia  on  the  reading  and  comprehension  of  computer  program  code.  Drawing  upon  work  from  the  fields  of  program  comprehension,  eye  tracking,  dyslexia,  models  of  reading  and  dyslexia  gaze  behaviour,  a  set  of  hypotheses  is  developed  with  which  to  investigate  potential  differences  in  the  gaze  behaviour  of  programmers  with  dyslexia  compared  to  typical  programmers.  The  hypotheses  posit  that,  in  general  terms,  programmers  with  dyslexia  will  show  gaze  behaviour  of  longer  duration  and  a  greater  number  of  fixations  on  program  features  than  typical  programmers.  An  experiment  is  described  in  which  28  programmers  (14  with  dyslexia,  14  without  dyslexia)  were  asked  to  read  and  explain  three  simple  computer  programs.  Eye  tracking  technology  is  used  to  capture  the  gaze  behaviour  of  the  programmers.  Data  analysis  suggests  that  the  code  reading  behaviour  of  programmers  with  dyslexia  is  not  what  would  be  expected  based  on  the  dyslexia  literature  relating  to  natural  text.  In  conjunction  with  further  exploratory  analysis,  observations  are  made  in  relation  to  spatial  differences  in  how  programmers  with  dyslexia  read  and  scan  code.  The  results  show  that  the  gaze  behaviour  of  programmers  with  dyslexia  requires  further  study  to  understand  effects  such  as  code  layout,  identifier  naming  and  line  length.  A  possible  impact  on  dyslexia  gaze  behaviour  is  from  the  visual  crowding  of  features  in  program  code  which  might  cause  certain  program  features  to  receive  less  attention  during  a  program  comprehension  task.
1	Do  bugs  foreshadow  vulnerabilities  an  in  depth  study  of  the  chromium  project.  As  developers  face  an  ever-increasing  pressure  to  engineer  secure  software,  researchers  are  building  an  understanding  of  security-sensitive  bugs  (i.e.  vulnerabilities).  Research  into  mining  software  repositories  has  greatly  increased  our  understanding  of  software  quality  via  empirical  study  of  bugs.  Conceptually,  however,  vulnerabilities  differ  from  bugs:  they  represent  an  abuse  of  functionality  as  opposed  to  insufficient  functionality  commonly  associated  with  traditional,  non-security  bugs.  We  performed  an  in-depth  analysis  of  the  Chromium  project  to  empirically  examine  the  relationship  between  bugs  and  vulnerabilities.  We  mined  374,686  bugs  and  703  post-release  vulnerabilities  over  five  Chromium  releases  that  span  six  years  of  development.  We  used  logistic  regression  analysis,  ranking  analysis,  bug  type  classifications,  developer  experience,  and  vulnerability  severity  metrics  to  examine  the  overarching  question:  are  bugs  and  vulnerabilities  in  the  same  files?  While  we  found  statistically  significant  correlations  between  pre-release  bugs  and  post-release  vulnerabilities,  we  found  the  association  to  be  weak.  Number  of  features,  source  lines  of  code,  and  pre-release  security  bugs  are,  in  general,  more  closely  associated  with  post-release  vulnerabilities  than  any  of  our  non-security  bug  categories.  In  further  analysis,  we  examined  sub-types  of  bugs,  such  as  stability-related  bugs,  and  the  associations  did  not  improve.  Even  the  files  with  the  most  severe  vulnerabilities  (by  measure  of  CVSS  or  bounty  payouts)  did  not  show  strong  correlations  with  number  of  bugs.  These  results  indicate  that  bugs  and  vulnerabilities  are  empirically  dissimilar  groups,  motivating  the  need  for  security  engineering  research  to  target  vulnerabilities  specifically.
1	Scalable  data  structure  detection  and  classification  for  c  c  binaries.  Many  existing  techniques  for  reversing  data  structures  in  C/C  ++  binaries  are  limited  to  low-level  programming  constructs,  such  as  individual  variables  or  structs.  Unfortunately,  without  detailed  information  about  a  program's  pointer  structures,  forensics  and  reverse  engineering  are  exceedingly  hard.  To  fill  this  gap,  we  propose  MemPick,  a  tool  that  detects  and  classifies  high-level  data  structures  used  in  stripped  binaries.  By  analyzing  how  links  between  memory  objects  evolve  throughout  the  program  execution,  it  distinguishes  between  many  commonly  used  data  structures,  such  as  singly-  or  doubly-linked  lists,  many  types  of  trees  (e.g.,  AVL,  red-black  trees,  B-trees),  and  graphs.  We  evaluate  the  technique  on  10  real  world  applications,  4  file  system  implementations  and  16  popular  libraries.  The  results  show  that  MemPick  can  identify  the  data  structures  with  high  accuracy.
1	Long  term  monitoring  of  transformation  from  pastoral  to  agricultural  land  use  using  time  series  landsat  data  in  the  feija  basin  southeast  morocco.  The  expansion  of  agricultural  land  at  the  cost  of  pastoral  land  is  the  common  cause  of  land  degradation  in  the  arid  areas  of  developing  countries,  especially  in  Morocco.  This  study  aims  to  assess  and  monitor  the  transformation  of  pastoral  land  to  agricultural  land  in  the  arid  environment  of  the  Feija  Basin  (Southeast  of  Morocco)  and  to  find  the  key  drivers  and  the  issues  resulting  from  this  transformation.  Spectral  mixture  analysis  was  applied  to  multi-temporal  (1975–2017)  and  multi-sensor  (i.e.  Multi-spectral  Scanner,  Thematic  Mapper,  and  Operational  Land  Imager)  Landsat  satellite  images,  from  which  land  use  classifications  were  derived.  The  remote  sensing  data  in  combination  with  ground  reference  data  (household  level),  groundwater  and  climate  statistics  were  used  to  validate  and  explain  the  derived  land  use  change  maps.  The  results  of  the  spatiotemporal  changes  in  agricultural  lands  show  two  patterns  of  changes,  a  middle  expansion  from  1975  to  2007,  and  a  rapid  expansion  from  2008  to  2017.  In  addition,  the  overall  accuracy  demonstrated  a  high  accuracy  of  94.4%.  In  1975  and  1984,  the  agricultural  lands  in  Feija  covered  0.17 km2  and  1.32 km2,  respectively,  compared  with  20.10 km2  in  2017.  Since  the  adoption  of  the  Green  Morocco  Plan  in  2008,  the  number  of  watermelon  farms  and  wells  has  increased  rapidly  in  the  study  area,  which  induced  a  piezometric  level  drawdown.  The  results  show  that  spectral  mixture  analysis  yields  high  accuracies  for  agricultural  lands  extraction  in  arid  dry  lands  and  accounts  for  mixed  pixels  issues.  Results  of  this  study  can  be  used  by  local  administrators  to  prepare  an  effective  environmental  management  plan  of  these  fragile  drylands.  The  proposed  method  can  be  replicated  in  other  regions  to  analyse  land  transformation  in  similar  arid  conditions.
1	An  empirical  study  of  early  access  games  on  the  steam  platform.  “Early  access”  is  a  release  strategy  for  software  that  allows  consumers  to  purchase  an  unfinished  version  of  the  software.  In  turn,  consumers  can  influence  the  software  development  process  by  giving  developers  early  feedback.  This  early  access  model  has  become  increasingly  popular  through  digital  distribution  platforms,  such  as  Steam  which  is  the  most  popular  distribution  platform  for  games.  The  plethora  of  options  offered  by  Steam  to  communicate  between  developers  and  game  players  contribute  to  the  popularity  of  the  early  access  model.  The  model  is  considered  a  success  by  the  game  development  community  as  several  games  using  this  approach  have  gained  a  large  user  base  (i.e.,  owners)  and  high  sales.  On  the  other  hand,  the  benefits  of  the  early  access  model  have  been  questioned  as  well.  In  this  paper,  we  conduct  an  empirical  study  on  1,182  Early  Access  Games  (EAGs)  on  the  Steam  platform  to  understand  the  characteristics,  advantages  and  limitations  of  the  early  access  model.  We  find  that  15%  of  the  games  on  Steam  make  use  of  the  early  access  model,  with  the  most  popular  EAG  having  as  many  as  29  million  owners.  88%  of  the  EAGs  are  classified  by  their  developers  as  so-called  “indie”  games,  indicating  that  most  EAGs  are  developed  by  individual  developers  or  small  studios.  We  study  the  interaction  between  players  and  developers  of  EAGs  and  the  Steam  platform.  We  observe  that  on  the  one  hand,  developers  update  their  games  more  frequently  in  the  early  access  stage.  On  the  other  hand,  the  percentage  of  players  that  review  a  game  during  its  early  access  stage  is  lower  than  the  percentage  of  players  that  review  the  game  after  it  leaves  the  early  access  stage.  However,  the  average  rating  of  the  reviews  is  much  higher  during  the  early  access  stage,  suggesting  that  players  are  more  tolerant  of  imperfections  in  the  early  access  stage.  The  positive  review  rate  does  not  correlate  with  the  length  or  the  game  update  frequency  of  the  early  access  stage.  Based  on  our  findings,  we  suggest  game  developers  to  use  the  early  access  model  as  a  method  for  eliciting  early  feedback  and  more  positive  reviews  to  attract  additional  new  players.  In  addition,  our  findings  suggest  that  developers  can  determine  their  release  schedule  without  worrying  about  the  length  of  the  early  access  stage  and  the  game  update  frequency  during  the  early  access  stage.
1	How  different  are  different  diff  algorithms  in  git.  Automatic  identification  of  the  differences  between  two  versions  of  a  file  is  a  common  and  basic  task  in  several  applications  of  mining  code  repositories.  Git,  a  version  control  system,  has  a  diff  utility  and  users  can  select  algorithms  of  diff  from  the  default  algorithm  Myers  to  the  advanced  Histogram  algorithm.  From  our  systematic  mapping,  we  identified  three  popular  applications  of  diff  in  recent  studies.  On  the  impact  on  code  churn  metrics  in  14  Java  projects,  we  obtained  different  values  in  1.7%  to  8.2%  commits  based  on  the  different  diff  algorithms.  Regarding  bug-introducing  change  identification,  we  found  6.0%  and  13.3%  in  the  identified  bug-fix  commits  had  different  results  of  bug-introducing  changes  from  10  Java  projects.  For  patch  application,  we  found  that  the  Histogram  is  more  suitable  than  Myers  for  providing  the  changes  of  code,  from  our  manual  analysis.  Thus,  we  strongly  recommend  using  the  Histogram  algorithm  when  mining  Git  repositories  to  consider  differences  in  source  code.
1	Practical  interruptible  conversations.  The  rigorous  and  comprehensive  verification  of  communication-based  software  is  an  important  engineering  challenge  in  distributed  systems.  Drawn  from  our  industrial  collaborations  [33,28]  on  Scribble,  a  choreography  description  language  based  on  multiparty  session  types,  this  paper  proposes  a  dynamic  verification  framework  for  structured  interruptible  conversation  programming.  We  first  present  our  extension  of  Scribble  to  support  the  specification  of  asynchronously  interruptible  conversations.  We  then  implement  a  concise  API  for  conversation  programming  with  interrupts  in  Python  that  enables  session  types  properties  to  be  dynamically  verified  for  distributed  processes.  Our  framework  ensures  the  global  safety  of  a  system  in  the  presence  of  asynchronous  interrupts  through  independent  runtime  monitoring  of  each  endpoint,  checking  the  conformance  of  the  local  execution  trace  to  the  specified  protocol.  The  usability  of  our  framework  for  describing  and  verifying  choreographic  communications  has  been  tested  by  integration  into  the  large  scientific  cyberinfrastructure  developed  by  the  Ocean  Observatories  Initiative.  Asynchronous  interrupts  have  proven  expressive  enough  to  represent  and  verify  their  main  classes  of  communication  patterns,  including  asynchronous  streaming  and  various  timeout-based  protocols,  without  requiring  additional  synchronisation  mechanisms.  Benchmarks  show  conversation  programming  and  monitoring  can  be  realised  with  little  overhead.
1	Measuring  the  impact  of  lexical  and  structural  inconsistencies  on  developers  cognitive  load  during  bug  localization.  A  large  portion  of  the  cost  of  any  software  lies  in  the  time  spent  by  developers  in  understanding  a  program’s  source  code  before  any  changes  can  be  undertaken.  Measuring  program  comprehension  is  not  a  trivial  task.  In  fact,  different  studies  use  self-reported  and  various  psycho-physiological  measures  as  proxies.  In  this  research,  we  propose  a  methodology  using  functional  Near  Infrared  Spectroscopy  (fNIRS)  and  eye  tracking  devices  as  an  objective  measure  of  program  comprehension  that  allows  researchers  to  conduct  studies  in  environments  close  to  real  world  settings,  at  identifier  level  of  granularity.  We  validate  our  methodology  and  apply  it  to  study  the  impact  of  lexical,  structural,  and  readability  issues  on  developers’  cognitive  load  during  bug  localization  tasks.  Our  study  involves  25  undergraduate  and  graduate  students  and  21  metrics.  Results  show  that  the  existence  of  lexical  inconsistencies  in  the  source  code  significantly  increases  the  cognitive  load  experienced  by  participants  not  only  on  identifiers  involved  in  the  inconsistencies  but  also  throughout  the  entire  code  snippet.  We  did  not  find  statistical  evidence  that  structural  inconsistencies  increase  the  average  cognitive  load  that  participants  experience,  however,  both  types  of  inconsistencies  result  in  lower  performance  in  terms  of  time  and  success  rate.  Finally,  we  observe  that  self-reported  task  difficulty,  cognitive  load,  and  fixation  duration  do  not  correlate  and  appear  to  be  measuring  different  aspects  of  task  difficulty.
1	Achieving  traceability  in  large  scale  continuous  integration  and  delivery  deployment  usage  and  validation  of  the  eiffel  framework.  The  importance  of  traceability  in  software  development  has  long  been  recognized,  not  only  for  reasons  of  legality  and  certification,  but  also  to  enable  the  development  itself.  At  the  same  time,  organizations  are  known  to  struggle  to  live  up  to  traceability  requirements,  and  there  is  an  identified  lack  of  studies  on  traceability  practices  in  the  industry,  not  least  in  the  area  of  tooling  and  infrastructure.  This  paper  presents,  investigates  and  discusses  Eiffel,  an  industry  developed  solution  designed  to  provide  real  time  traceability  in  continuous  integration  and  delivery.  The  traceability  needs  of  industry  professionals  are  also  investigated  through  interviews,  providing  context  to  that  solution.  It  is  then  validated  through  further  interviews,  a  comparison  with  previous  traceability  methods  and  a  review  of  literature.  It  is  found  to  address  the  identified  traceability  needs  and  found  in  some  cases  to  reduce  traceability  data  acquisition  times  from  days  to  minutes,  while  at  the  same  time  alternatives  offering  comparable  functionality  are  lacking.  In  this  work,  traceability  is  shown  not  only  to  be  an  important  concern  to  engineers,  but  also  regarded  as  a  prerequisite  to  successful  large  scale  continuous  integration  and  delivery.  At  the  same  time,  promising  developments  in  technical  infrastructure  are  documented  and  clear  differences  in  traceability  mindset  between  separate  industry  projects  is  revealed.
1	Raters  reliability  in  clone  benchmarks  construction.  Cloned  code  often  complicates  code  maintenance  and  evolution  and  must  therefore  be  effectively  detected.  One  of  the  biggest  challenges  for  clone  detectors  is  to  reduce  the  amount  of  irrelevant  clones  they  found,  called  false  positives.  Several  benchmarks  of  true  and  false  positive  clones  have  been  introduced,  enabling  tool  developers  to  compare,  assess  and  fine-tune  their  tools.  Manual  inspection  of  clone  candidates  is  performed  by  raters  that  do  not  have  expertise  on  the  underlying  code.  This  way  of  building  benchmarks  might  be  unreliable  when  considering  context-dependent  clones  i.e.,  clones  valid  for  a  specific  purpose.  Our  goal  is  to  investigate  the  reliability  of  rater  judgments  about  context-dependent  clones.  We  randomly  select  about  600  clones  from  two  projects  and  ask  several  raters,  including  experts  of  the  projects,  to  manually  classify  these  clones.  We  observe  that  judgments  of  non  expert  raters  are  not  always  repeatable.  We  also  observe  that  they  seldomly  agree  with  each  others  and  with  the  expert.  Finally,  we  find  that  the  project  and  the  fact  that  a  clone  is  a  true  or  false  positive  might  have  an  influence  on  the  agreement  between  the  expert  and  non  experts.  Therefore,  using  non  experts  to  produce  clone  benchmarks  could  be  unreliable.
1	Change  based  test  selection  an  empirical  evaluation.  Regression  test  selection  (i.e.,  selecting  a  subset  of  a  given  regression  test  suite)  is  a  problem  that  has  been  studied  intensely  over  the  last  decade.  However,  with  the  increasing  popularity  of  developer  tests  as  the  driver  of  the  test  process,  more  fine-grained  solutions  that  work  well  within  the  context  of  the  Integrated  Development  Environment  (IDE)  are  in  order.  Consequently,  we  created  two  variants  of  a  test  selection  heuristic  which  exploit  fine-grained  changes  recorded  during  actual  development  inside  the  IDE.  One  variant  only  considers  static  binding  of  method  invocations  while  the  other  variant  takes  dynamic  binding  into  account.  This  paper  investigates  the  tradeoffs  between  these  two  variants  in  terms  of  the  reduction  (i.e.,  How  many  tests  could  we  omit  from  the  test  suite,  and  how  much  did  we  gain  in  runtime  execution?)  as  well  as  the  fault  detection  ability  of  the  reduced  test  suite  (i.e.,  Were  tests  omitted  erroneously?).  We  used  our  approach  on  three  distinct  cases,  two  open  source  cases  --Cruisecontrol  and  PMD--  and  one  industrial  case  --  Historia.  Our  results  show  that  only  considering  static  binding  reduces  the  test  suite  significantly  but  occasionally  omits  a  relevant  test;  considering  dynamic  binding  rarely  misses  a  test  yet  often  boils  down  to  running  the  complete  test  suite.  Nevertheless,  our  analysis  provides  indications  on  when  a  given  variant  is  more  appropriate.
1	Gis  based  evaluation  of  groundwater  quality  and  suitability  in  dakhla  oases  egypt.  Groundwater  is  the  main  source  for  agricultural  and  domestic  purposes  in  the  Western  Desert  of  Egypt.  Groundwater  quality  is  significantly  influenced  by  the  surrounding  anthropogenic  activities.  This  paper  is  one  of  the  attempts  to  spatially  assess  groundwater  quality  and its  suitability  for  drinking  and  irrigation  in  Dakhla  Oases  using  Water  Quality  Index  (WQI)  and  GIS  techniques.  Calibrated  Landsat 8  OLI  satellite  images  were  processed  to  produce  Land  Use  Cover  map  (LULC)  to  assess  the  agricultural  and  human  activities  in  the  study  area.  Further,  eight  groundwater  quality  parameters  and  WQI  were  attributed  to  a  GIS  layer  for  71  investigated  wells  for  mapping  purposes  using  the  Inverse  Distance  Weighting  (IDW) method  in  ArcGIS.  LULC  map  showed  that  75%  of  the  study  area  is  a  bare  land  and 25%  is  urban  and  agricultural  areas.  Almost  all  the  studied  wells  recorded  total  dissolved  solids  and  sulfate  coinciding  with  the  Egyptian  permissible  limits  for  drinking  purposes.  Fe  and  Mn  levels  exceeded  the  allowable  limits  for  drinking  in  the  majority  of  Dakhla  wells.  Based  on  the  WQI,  38%  and  36.6%  of  the  study  area  fell  within  the  poor  water  category  according  to  the  Egyptian  and  WHO  standards,  respectively.  Most  of  the  groundwater  wells  were  of  the  best  quality  for  irrigation  with  regard  to  salinity  (less  than  2000 mg/L)  and  the  excellent  quality  in  terms  of  sodium  absorption  ratio  (< 10).  It  can  be  concluded  that  GIS  analyses  of  groundwater  quality  and  suitability  can  provide  one  of  the  necessary  inputs  for  management  and  planning  of  Dakhla  Oases  and  other  similar  regions.
1	Modelling  the  hurried  bug  report  reading  process  to  summarize  bug  reports.  Although  bug  reports  are  frequently  consulted  project  assets,  they  are  communication  logs,  by-products  of  bug  resolution,  and  not  artifacts  created  with  the  intent  of  being  easy  to  follow.  To  facilitate  bug  report  digestion,  we  propose  a  new,  unsupervised,  bug  report  summarization  approach  that  estimates  the  attention  a  user  would  hypothetically  give  to  different  sentences  in  a  bug  report,  when  pressed  with  time.  We  pose  three  hypotheses  on  what  makes  a  sentence  relevant:  discussing  frequently  discussed  topics,  being  evaluated  or  assessed  by  other  sentences,  and  keeping  focused  on  the  bug  report's  title  and  description.  Our  results  suggest  that  our  hypotheses  are  valid,  since  the  summaries  have  as  much  as  12  %  improvement  in  standard  summarization  evaluation  metrics  compared  to  the  previous  approach.  Our  evaluation  also  asks  developers  to  assess  the  quality  and  usefulness  of  the  summaries  created  for  bug  reports  they  have  worked  on.  Feedback  from  developers  not  only  shows  the  summaries  are  useful,  but  also  points  out  important  requirements  for  this,  and  any  bug  summarization  approach,  and  indicates  directions  for  future  work.
1	Meshing  agile  and  plan  driven  development  in  safety  critical  software  a  case  study.  Organizations  developing  safety-critical  software  are  increasingly  seeking  to  create  better  practices  by  meshing  agile  and  plan-driven  development  processes.  Significant  differences  between  the  agile  and  the  plan-driven  processes  make  meshing  difficult,  and  very  little  empirical  evidence  on  using  agile  processes  for  safety-critical  software  development  exists.  There  are  four  areas  of  concern,  in  particular,  for  meshing  the  development  of  safety-critical  software  concerning:  documentation,  requirements,  life  cycle  and  testing.  We  report  on  a  case  study  of  a  pharmaceutical  organization  in  which  a  Scrum  process  was  implemented  to  support  agile  software  development  in  a  plan-driven  safety-critical  project.  The  purpose  was  to  answer  the  following  research  question:  For  safety-critical  software,  what  can  a  software  team  do  to  mesh  agile  and  plan-driven  processes  effectively?  The  main  contribution  of  the  paper  is  an  elaborated  understanding  of  meshing  in  the  four  areas  of  concern  and  how  the  conditions  for  safety-critical  software  influence  them.  We  discuss  how  meshing  within  the  four  areas  of  concern  is  a  contribution  to  existing  research.
1	Improving  the  pull  requests  review  process  using  learning  to  rank  algorithms.  Collaborative  software  development  platforms  (such  as  GitHub  and  GitLab)  have  become  increasingly  popular  as  they  have  attracted  thousands  of  external  contributors  to  contribute  to  open  source  projects.  The  external  contributors  may  submit  their  contributions  via  pull  requests,  which  must  be  reviewed  before  being  integrated  into  the  central  repository.  During  the  review  process,  reviewers  provide  feedback  to  contributors,  conduct  tests  and  request  further  modifications  before  finally  accepting  or  rejecting  the  contributions.  The  role  of  reviewers  is  key  to  maintain  the  effective  review  process  of  the  project.  However,  the  number  of  decisions  that  reviewers  can  make  is  far  superseded  by  the  increasing  number  of  pull  requests  submissions.  To  help  reviewers  to  perform  more  decisions  on  pull  requests  within  their  limited  working  time,  we  propose  a  learning-to-rank  (LtR)  approach  to  recommend  pull  requests  that  can  be  quickly  reviewed  by  reviewers.  Different  from  a  binary  model  for  predicting  the  decisions  of  pull  requests,  our  ranking  approach  complements  the  existing  list  of  pull  requests  based  on  their  likelihood  of  being  quickly  merged  or  rejected.  We  use  18  metrics  to  build  LtR  models  and  we  use  six  different  LtR  algorithms,  such  as  ListNet,  RankNet,  MART  and  random  forest.  We  conduct  empirical  studies  on  74  Java  projects  to  compare  the  performances  of  the  six  LtR  algorithms.  We  compare  the  best  performing  algorithm  against  two  baselines  obtained  from  previous  research  regarding  pull  requests  prioritization:  the  first-in-and-first-out  (FIFO)  baseline  and  the  small-size-first  baseline.  We  then  conduct  a  survey  with  GitHub  reviewers  to  understand  the  perception  of  code  reviewers  regarding  the  usefulness  of  our  approach.  We  observe  that:  (1)  The  random  forest  LtR  algorithm  outperforms  other  five  well  adapted  LtR  algorithms  to  rank  quickly  merged  pull  requests.  (2)  The  random  forest  LtR  algorithm  performs  better  than  both  the  FIFO  and  the  small-size-first  baselines,  which  means  our  LtR  approach  can  help  reviewers  make  more  decisions  and  improve  their  productivity.  (3)  The  contributor’s  social  connections  and  contributor’s  experience  are  the  most  influential  metrics  to  rank  pull  requests  that  can  be  quickly  merged.  (4)  The  GitHub  reviewers  that  participated  in  our  survey  acknowledge  that  our  approach  complements  existing  prioritization  baselines  to  help  them  to  prioritize  and  to  review  more  pull  requests.
1	What  makes  a  popular  academic  ai  repository.  Many  AI  researchers  are  publishing  code,  data  and  other  resources  that  accompany  their  papers  in  GitHub  repositories.  In  this  paper,  we  refer  to  these  repositories  as  academic  AI  repositories.  Our  preliminary  study  shows  that  highly  cited  papers  are  more  likely  to  have  popular  academic  AI  repositories  (and  vice  versa).  Hence,  in  this  study,  we  perform  an  empirical  study  on  academic  AI  repositories  to  highlight  good  software  engineering  practices  of  popular  academic  AI  repositories  for  AI  researchers.  We  collect  1,149  academic  AI  repositories,  in  which  we  label  the  top  20%  repositories  that  have  the  most  number  of  stars  as  popular,  and  we  label  the  bottom  70%  repositories  as  unpopular.  The  remaining  10%  repositories  are  set  as  a  gap  between  popular  and  unpopular  academic  AI  repositories.  We  propose  21  features  to  characterize  the  software  engineering  practices  of  academic  AI  repositories.  Our  experimental  results  show  that  popular  and  unpopular  academic  AI  repositories  are  statistically  significantly  different  in  11  of  the  studied  features—indicating  that  the  two  groups  of  repositories  have  significantly  different  software  engineering  practices.  Furthermore,  we  find  that  the  number  of  links  to  other  GitHub  repositories  in  the  README  file,  the  number  of  images  in  the  README  file  and  the  inclusion  of  a  license  are  the  most  important  features  for  differentiating  the  two  groups  of  academic  AI  repositories.  Our  dataset  and  code  are  made  publicly  available  to  share  with  the  community.
1	Too  many  images  on  dockerhub  how  different  are  images  for  the  same  system.  Containerization  is  a  technique  used  to  encapsulate  a  software  system  and  its  dependencies  into  one  isolated  package,  which  is  called  a  container.  The  goal  of  these  containers  is  to  deploy  or  replicate  a  software  system  on  various  platforms  and  environments  without  facing  any  compatibility  or  dependency  issues.  Developers  can  instantiate  these  containers  from  images  using  Docker;  one  of  the  most  popular  containerization  platforms.  Furthermore,  many  of  these  images  are  publicly  available  on  DockerHub,  on  which  developers  can  share  their  images  with  the  community  who  in  turn  can  leverage  such  publicly  available  image.  However,  DockerHub  contains  thousands  of  images  for  each  software  system,  which  makes  the  selection  of  an  image  a  nontrivial  task.  In  this  paper,  we  investigate  the  differences  among  DockerHub  images  for  five  software  systems  and  936  images  with  the  goal  of  helping  Docker  tooling  creators  and  DockerHub  better  guide  users  select  a  suitable  image.  We  observe  that  users  tend  to  download  the  official  images  (images  that  are  provided  by  Docker  itself)  when  there  exist  a  large  number  of  image  choices  for  each  single  software  system  on  the  community  images  (images  that  are  provided  by  the  community  developers),  which  are  in  many  cases  more  resource  efficient  (have  less  duplicate  resources)  and  have  less  security  vulnerabilities.  In  fact,  we  observe  that  27%  (median),  35%  (median),  6%  (median),  and  9%  (median)  of  the  DockerHub  Debian,  Centos,  Ubuntu,  and  Alpine  based  images  are  identical  to  another  image  across  all  the  studied  software  systems.  Furthermore,  26%  (median),  49%  (median),  and  8%  (median)  of  the  Alpine,  Debian,  and  Ubuntu  based  community  images  are  more  resource  efficient  than  their  respective  official  images  across  all  the  five  studied  software  systems.  7%  (median)  of  the  community  Debian  based  images  have  less  security  vulnerabilities  than  their  respective  official  images  across  the  four  studied  software  systems,  for  which  an  official  Debian  based  image  exists.  Unfortunately,  the  description  of  78%  of  the  studied  images  do  not  guide  users  when  selecting  an  image  (the  description  does  not  exist  at  all  or  it  does  not  highlight  the  particularities  of  the  image),  we  suggest  that  Docker  tooling  creators  and  DockerHub  design  approaches  to  distinguish  DockerHub  images  and  help  users  find  the  most  suitable  images  for  their  needs.
1	Maximal  causal  models  for  sequentially  consistent  systems.  This  paper  shows  that  it  is  possible  to  build  a  maximal  and  sound  causal  model  for  concurrent  computations  from  a  given  execution  trace.  It  is  sound,  in  the  sense  that  any  program  which  can  generate  a  trace  can  also  generate  all  traces  in  its  causal  model.  It  is  maximal  (among  sound  models),  in  the  sense  that  by  extending  the  causal  model  of  an  observed  trace  with  a  new  trace,  the  model  becomes  unsound:  there  exists  a  program  generating  the  original  trace  which  cannot  generate  the  newly  introduced  trace.  Thus,  the  maximal  sound  model  has  the  property  that  it  comprises  all  traces  which  all  programs  that  can  generate  the  original  trace  can  also  generate.  The  existence  of  such  a  model  is  of  great  theoretical  value  as  it  can  be  used  to  prove  the  soundness  of  non-maximal,  and  thus  smaller,  causal  models.
1	Combining  model  checking  and  runtime  verification  for  safe  robotics.  A  major  challenge  towards  large  scale  deployment  of  autonomous  mobile  robots  is  to  program  them  with  formal  guarantees  and  high  assurance  of  correct  operation.  To  this  end,  we  present  a  framework  for  building  safe  robots.  Our  approach  for  validating  the  end-to-end  correctness  of  robotics  system  consists  of  two  parts:  (1)  a  high-level  programming  language  for  implementing  and  systematically  testing  the  reactive  robotics  software  via  model  checking;  (2)  a  signal  temporal  logic  (STL)  based  online  monitoring  system  to  ensure  that  the  assumptions  about  the  low-level  controllers  (discrete  models)  used  during  model  checking  hold  at  runtime.  Combining  model  checking  with  runtime  verification  helps  us  bridge  the  gap  between  software  verification  (discrete)  that  makes  assumptions  about  the  low-level  controllers  and  the  physical  world,  and  the  actual  execution  of  the  software  on  a  real  robotic  platform  in  the  physical  world.  To  demonstrate  the  efficacy  of  our  approach,  we  build  a  safe  adaptive  surveillance  system  and  present  software-in-the-loop  simulations  of  the  application.
1	Employing  source  code  information  to  improve  question  answering  in  stack  overflow.  Nowadays,  software  development  has  been  greatly  influenced  by  question-answering  communities,  such  as  Stack  Overflow.  A  new  problem-solving  paradigm  has  emerged,  as  developers  post  problems  they  encounter  that  are  then  answered  by  the  community.  In  this  paper,  we  propose  a  methodology  that  allows  searching  for  solutions  in  Stack  Overflow,  using  the  main  elements  of  a  question  post,  including  not  only  its  title,  tags,  and  body,  but  also  its  source  code  snippets.  We  describe  a  similarity  scheme  for  these  elements  and  demonstrate  how  structural  information  can  be  extracted  from  source  code  snippets  and  compared  to  further  improve  the  retrieval  of  questions.  The  results  of  our  evaluation  indicate  that  our  methodology  is  effective  on  recommending  similar  question  posts  allowing  community  members  to  search  without  fully  forming  a  question.
1	Ecosystems  in  github  and  a  method  for  ecosystem  identification  using  reference  coupling.  Software  projects  are  not  developed  in  isolation.  Recent  research  has  shifted  to  studying  software  ecosystems,  communities  of  projects  that  depend  on  each  other  and  are  developed  together.  However,  identifying  technical  dependencies  at  the  ecosystem  level  can  be  challenging.  In  this  paper,  we  propose  a  new  method,  known  as  reference  coupling,  for  detecting  technical  dependencies  between  projects.  The  method  establishes  dependencies  through  user-specified  cross-references  between  projects.  We  use  our  method  to  identify  ecosystems  in  GitHub-hosted  projects,  and  we  identify  several  characteristics  of  the  identified  ecosystems.  We  find  that  most  ecosystems  are  centered  around  one  project  and  are  interconnected  with  other  ecosystems.  The  predominant  type  of  ecosystems  are  those  that  develop  tools  to  support  software  development.  We  also  found  that  the  project  owners'  social  behaviour  aligns  well  with  the  technical  dependencies  within  the  ecosystem,  but  project  contributors'  social  behaviour  does  not  align  with  these  dependencies.  We  conclude  with  a  discussion  on  future  research  that  is  enabled  by  our  reference  coupling  method.
1	Candoia  a  platform  for  building  and  sharing  mining  software  repositories  tools  as  apps.  We  propose  Candoia,  a  novel  platform  and  ecosystem  for  building  and  sharing  Mining  Software  Repositories  (MSR)  tools.  Using  Candoia,  MSR  tools  are  built  as  apps,  and  Candoia  ecosystem,  acting  as  an  appstore,  allows  effective  sharing.  Candoia  platform  provides,  data  extraction  tools  for  curating  custom  datasets  for  user  projects,  and  data  abstractions  for  enabling  uniform  access  to  MSR  artifacts  from  disparate  sources,  which  makes  apps  portable  and  adoptable  across  diverse  software  project  settings  of  MSR  researchers  and  practitioners.  The  structured  design  of  a  Candoia  app  and  the  languages  selected  for  building  various  components  of  a  Candoia  app  promotes  easy  customization.  To  evaluate  Candoia  we  have  built  over  two  dozen  MSR  apps  for  analyzing  bugs,  software  evolution,  project  management  aspects,  and  source  code  and  programming  practices  showing  the  applicability  of  the  platform  for  building  a  variety  of  MSR  apps.  For  testing  portability  of  apps  across  diverse  project  settings,  we  tested  the  apps  using  ten  popular  project  repositories,  such  as  Apache  Tomcat,  JUnit,  Node.js,  etc,  and  found  that  apps  required  no  changes  to  be  portable.  We  performed  a  user  study  to  test  customizability  and  we  found  that  five  of  eight  Candoia  users  found  it  very  easy  to  customize  an  existing  app.  Candoia  is  available  for  download.
1	Improving  the  accuracy  of  duplicate  bug  report  detection  using  textual  similarity  measures.  The  paper  describes  an  improved  method  for  automatic  duplicate  bug  report  detection  based  on  new  textual  similarity  features  and  binary  classification.  Using  a  set  of  new  textual  features,  inspired  from  recent  text  similarity  research,  we  train  several  binary  classification  models.  A  case  study  was  conducted  on  three  open  source  systems:  Eclipse,  Open  Office,  and  Mozilla  to  determine  the  effectiveness  of  the  improved  method.  A  comparison  is  also  made  with  current  state-of-the-art  approaches  highlighting  similarities  and  differences.  Results  indicate  that  the  accuracy  of  the  proposed  method  is  better  than  previously  reported  research  with  respect  to  all  three  systems.
1	Logging  library  migrations  a  case  study  for  the  apache  software  foundation  projects.  Developers  leverage  logs  for  debugging,  performance  monitoring  and  load  testing.  The  increased  dependence  on  logs  has  lead  to  the  development  of  numerous  logging  libraries  which  help  developers  in  logging  their  code.  As  new  libraries  emerge  and  current  ones  evolve,  projects  often  migrate  from  an  older  library  to  another  one.In  this  paper  we  study  logging  library  migrations  within  Apache  Software  Foundation  (ASF)  projects.  From  our  manual  analysis  of  JIRA  issues,  we  find  that  33  out  of  223  (i.e.,  14%)  ASF  projects  have  undergone  at  least  one  logging  library  migration.  We  find  that  the  five  main  drivers  for  logging  library  migration  are:  1)  to  increase  flexibility  (i.e.,  the  ability  to  use  different  logging  libraries  within  a  project)  2)  to  improve  performance,  3)  to  reduce  effort  spent  on  code  maintenance,  4)  to  reduce  dependence  on  other  libraries  and  5)  to  obtain  specific  features  from  the  new  logging  library.  We  find  that  over  70%  of  the  migrated  projects  encounter  on  average  two  post-migration  bugs  due  to  the  new  logging  library.  Furthermore,  our  findings  suggest  that  performance  (traditionally  one  of  the  primary  drivers  for  migrations)  is  rarely  improved  after  a  migration.
1	Word  embeddings  for  the  software  engineering  domain.  The  software  development  process  produces  vast  amounts  of  textual  data  expressed  in  natural  language.  Outcomes  from  the  natural  language  processing  community  have  been  adapted  in  software  engineering  research  for  leveraging  this  rich  textual  information;  these  include  methods  and  readily  available  tools,  often  furnished  with  pre-trained  models.  State  of  the  art  pre-trained  models  however,  capture  general,  common  sense  knowledge,  with  limited  value  when  it  comes  to  handling  data  specific  to  a  specialized  domain.  There  is  currently  a  lack  of  domain-specific  pre-trained  models  that  would  further  enhance  the  processing  of  natural  language  artefacts  related  to  software  engineering.  To  this  end,  we  release  a  word2vec  model  trained  over  15GB  of  textual  data  from  Stack  Overflow  posts.  We  illustrate  how  the  model  disambiguates  polysemous  words  by  interpreting  them  within  their  software  engineering  context.  In  addition,  we  present  examples  of  fine-grained  semantics  captured  by  the  model,  that  imply  transferability  of  these  results  to  diverse,  targeted  information  retrieval  tasks  in  software  engineering  and  motivate  for  further  reuse  of  the  model.
1	Mining  and  extraction  of  personal  software  process  measures  through  ide  interaction  logs.  The  Personal  Software  Process  (PSP)  is  an  effective  software  process  improvement  method  that  heavily  relies  on  manual  collection  of  software  development  data.  This  paper  describes  a  semi-automated  method  that  reduces  the  burden  of  PSP  data  collection  by  extracting  the  required  time  and  size  of  PSP  measurements  from  IDE  interaction  logs.  The  tool  mines  enriched  event  data  streams  so  can  be  easily  generalized  to  other  developing  environment  also.  In  addition,  the  proposed  method  is  adaptable  to  phase  definition  changes  and  creates  activity  visualizations  and  summarizations  that  are  helpful  for  software  project  management.  Tools  and  processed  data  used  for  this  paper  are  available  on  GitHub  at:  https://github.com/unknowngithubuser1/data.
1	Undocumented  and  unchecked  exceptions  that  spell  trouble.  Modern  programs  rely  on  large  application  programming  interfaces  (APIs).  The  Android  framework  comprises  231  core  APIs,  and  is  used  by  countless  developers.  We  examine  a  sample  of  4,900  distinct  crash  stack  traces  from  1,800  different  Android  applications,  looking  for  Android  API  methods  with  undocumented  exceptions  that  are  part  of  application  crashes.  For  the  purposes  of  this  study,  we  take  as  a  reference  the  version  15  of  the  Android  API,  which  matches  our  stack  traces.  Our  results  show  that  a  significant  number  of  crashes  (19%)  might  have  been  avoided  if  these  methods  had  the  corresponding  exceptions  documented  as  part  of  their  interface.
1	Time  present  and  time  past  analyzing  the  evolution  of  javascript  code  in  the  wild.  JavaScript  is  one  of  the  web's  key  building  blocks.  It  is  used  by  the  majority  of  web  sites  and  it  is  supported  by  all  modern  browsers.  We  present  the  first  large-scale  study  of  client-side  JavaScript  code  over  time.  Specifically,  we  have  collected  and  analyzed  a  dataset  containing  daily  snapshots  of  JavaScript  code  coming  from  Alexa's  Top  10000  web  sites  (~7.5  GB  per  day)  for  nine  consecutive  months,  to  study  different  temporal  aspects  of  web  client  code.  We  found  that  scripts  change  often;  typically  every  few  days,  indicating  a  rapid  pace  in  web  applications  development.  We  also  found  that  the  lifetime  of  web  sites  themselves,  measured  as  the  time  between  JavaScript  changes,  is  also  short,  in  the  same  time  scale.  We  then  performed  a  qualitative  analysis  to  investigate  the  nature  of  the  changes  that  take  place.  We  found  that  apart  from  standard  changes  such  as  the  introduction  of  new  functions,  many  changes  are  related  to  online  configuration  management.  In  addition,  we  examined  JavaScript  code  reuse  over  time  and  especially  the  widespread  reliance  on  third-party  libraries.  Furthermore,  we  observed  how  quality  issues  evolve  by  employing  established  static  analysis  tools  to  identify  potential  software  bugs,  whose  evolution  we  tracked  over  time.  Our  results  show  that  quality  issues  seem  to  persist  over  time,  while  vulnerable  libraries  tend  to  decrease.
1	Test  coverage  in  python  programs.  We  study  code  coverage  in  several  popular  Python  projects:  flask,  matplotlib,  pandas,  scikit-learn,  and  scrapy.  Coverage  data  on  these  projects  is  gathered  and  hosted  on  the  Codecov  website,  from  where  this  data  can  be  mined.  Using  this  data,  and  a  syntactic  parse  of  the  code,  we  examine  the  effect  of  control  flow  structure,  statement  type  (e.g.,  if,  for)  and  code  age  on  test  coverage.  We  find  that  coverage  depends  on  control  flow  structure,  with  more  deeply  nested  statements  being  significantly  less  likely  to  be  covered.  This  is  a  clear  effect,  which  holds  up  in  every  project,  even  when  controlling  for  the  age  of  the  line  (as  determined  by  git  blame).  We  find  that  the  age  of  a  line  per  se  has  a  small  (but  statistically  significant)  positive  effect  on  coverage.  Finally,  we  find  that  the  kind  of  statement  (try,  if,  except,  raise,  etc)  has  varying  effects  on  coverage,  with  exception-handling  statements  being  covered  much  less  often.  These  results  suggest  that  developers  in  Python  projects  have  difficulty  writing  test  sets  that  cover  deeply-nested  and  error-handling  statements,  and  might  need  assistance  covering  such  code.
1	Fixing  the  out  of  sight  out  of  mind  problem  one  year  of  mood  based  microblogging  in  a  distributed  software  team.  Distributed  teams  face  the  challenge  of  staying  connected.  How  do  team  members  stay  connected  when  they  no  longer  see  each  other  on  a  daily  basis?  What  should  be  done  when  there  is  no  coffee  corner  to  share  your  latest  exploits?  In  this  paper  we  evaluate  a  microblogging  system  which  makes  this  possible  in  a  distributed  setting.  The  system,  WeHomer,  enables  the  sharing  of  information  and  corresponding  emotions  in  a  fully  distributed  organization.  We  analyzed  the  content  of  over  a  year  of  usage  data  by  19  team  members  in  a  structured  fashion,  performed  5  semi-structured  interviews  and  report  our  findings  in  this  paper.  We  draw  conclusions  about  the  topics  shared,  the  impact  on  software  teams  and  the  impact  of  distribution  and  team  composition.  Main  findings  include  an  increase  in  team-connectedness  and  easier  access  to  information  that  is  traditionally  harder  to  consistently  acquire.
1	The  emotional  side  of  software  developers  in  jira.  ABSTRACTIssue  tracking  systems  store  valuable  data  for  testing  hy-potheses  concerning  maintenance,  building  statistical  pre-diction  models  and  (recently)  investigating  developer  affec-tiveness.  For  the  latter,  issue  tracking  systems  can  be  minedto  explore  developers  emotions,  sentiments  and  politeness,  affects  for  short.  However,  research  on  affect  detection  insoftware  artefacts  is  still  in  its  early  stage  due  to  the  lack  ofmanually  validated  data  and  tools.In  this  paper,  we  contribute  to  the  research  of  affectson  software  artefacts  by  providing  a  labeling  of  emotionspresent  on  issue  comments.We  manually  labeled  2,000  issue  comments  and  4,000  sen-tences  written  by  developers  with  emotions  such  as  love,joy,  surprise,  anger,  sadness  and  fear.  Labeled  commentsand  sentences  are  linked  to  software  artefacts  reported  inour  previously  published  dataset  (containing  more  than  1Kprojects,  more  than  700K  issue  reports  and  more  than  2million  issue  comments).  The  enriched  dataset  presented  inthis  paper  allows  the  investigation  of  the  role  of  affects  insoftware  development.
1	Improving  change  recommendation  using  aggregated  association  rules.  Past  research  has  proposed  association  rule  mining  as  a  means  to  uncover  the  evolutionary  coupling  from  a  system’s  change  history.  These  couplings  have  various  applications,  such  as  improving  system  decomposition  and  recommending  related  changes  during  development.  The  strength  of  the  coupling  can  be  characterized  using  a  variety  of  interestingness  measures.  Existing  recommendation  engines  typically  use  only  the  rule  with  the  highest  interestingness  value  in  situations  where  more  than  one  rule  applies.  In  contrast,  we  argue  that  multiple  applicable  rules  indicate  increased  evidence,  and  hypothesize  that  the  aggregation  of  such  rules  can  be  exploited  to  provide  more  accurate  recommendations.To  investigate  this  hypothesis  we  conduct  an  empirical  study  on  the  change  histories  of  two  large  industrial  systems  and  four  large  open  source  systems.  As  aggregators  we  adopt  three  cumulative  gain  functions  from  information  retrieval.  The  experiments  evaluate  the  three  using  39  different  rule  interestingness  measures.  The  results  show  that  aggregation  provides  a  significant  impact  on  most  measure’s  value  and,  furthermore,  leads  to  a  significant  improvement  in  the  resulting  recommendation.
1	A  discriminative  model  approach  for  suggesting  tags  automatically  for  stack  overflow  questions.  Annotating  documents  with  keywords  or  `tags'  is  useful  for  categorizing  documents  and  helping  users  find  a  document  efficiently  and  quickly.  Question  and  answer  (Q&A)  sites  also  use  tags  to  categorize  questions  to  help  ensure  that  their  users  are  aware  of  questions  related  to  their  areas  of  expertise  or  interest.  However,  someone  asking  a  question  may  not  necessarily  know  the  best  way  to  categorize  or  tag  the  question,  and  automatically  tagging  or  categorizing  a  question  is  a  challenging  task.  Since  a  Q&A  site  may  host  millions  of  questions  with  tags  and  other  data,  this  information  can  be  used  as  a  training  and  test  dataset  for  approaches  that  automatically  suggest  tags  for  new  questions.  In  this  paper,  we  mine  data  from  millions  of  questions  from  the  Q&A  site  Stack  Overflow,  and  using  a  discriminative  model  approach,  we  automatically  suggest  question  tags  to  help  a  questioner  choose  appropriate  tags  for  eliciting  a  response.
1	Can  issues  reported  at  stack  overflow  questions  be  reproduced  an  exploratory  study.  Software  developers  often  look  for  solutions  to  their  code  level  problems  at  Stack  Overflow.  Hence,  they  frequently  submit  their  questions  with  sample  code  segments  and  issue  descriptions.  Unfortunately,  it  is  not  always  possible  to  reproduce  their  reported  issues  from  such  code  segments.  This  phenomenon  might  prevent  their  questions  from  getting  prompt  and  appropriate  solutions.  In  this  paper,  we  report  an  exploratory  study  on  the  reproducibility  of  the  issues  discussed  in  400  questions  of  Stack  Overflow.  In  particular,  we  parse,  compile,  execute  and  even  carefully  examine  the  code  segments  from  these  questions,  spent  a  total  of  200  man  hours,  and  then  attempt  to  reproduce  their  programming  issues.  The  outcomes  of  our  study  are  two-fold.  First,  we  find  that  68%  of  the  code  segments  require  minor  and  major  modifications  in  order  to  reproduce  the  issues  reported  by  the  developers.  On  the  contrary,  22%  code  segments  completely  fail  to  reproduce  the  issues.  We  also  carefully  investigate  why  these  issues  could  not  be  reproduced  and  then  provide  evidence-based  guidelines  for  writing  effective  code  examples  for  Stack  Overflow  questions.  Second,  we  investigate  the  correlation  between  issue  reproducibility  status  (of  questions)  and  corresponding  answer  meta-data  such  as  the  presence  of  an  accepted  answer.  According  to  our  analysis,  a  question  with  reproducible  issues  has  at  least  three  times  higher  chance  of  receiving  an  accepted  answer  than  the  question  with  irreproducible  issues.
1	Towards  automatically  identifying  paid  open  source  developers.  Open  source  development  contains  contributions  from  both  hired  and  volunteer  software  developers.  Identification  of  this  status  is  important  when  we  consider  the  transferability  of  research  results  to  the  closed  source  software  industry,  as  they  include  no  volunteer  developers.  While  many  studies  have  taken  the  employment  status  of  developers  into  account,  this  information  is  often  gathered  manually  due  to  the  lack  of  accurate  automatic  methods.  In  this  paper,  we  present  an  initial  step  towards  predicting  paid  and  unpaid  open  source  development  using  machine  learning  and  compare  our  results  with  automatic  techniques  used  in  prior  work.  By  relying  on  code  source  repository  meta-data  from  Mozilla,  and  manually  collected  employment  status,  we  built  a  dataset  of  the  most  active  developers,  both  volunteer  and  hired  by  Mozilla.  We  define  a  set  of  metrics  based  on  developers'  usual  commit  time  pattern  and  use  different  classification  methods  (logistic  regression,  classification  tree,  and  random  forest).  The  results  show  that  our  proposed  method  identify  paid  and  unpaid  commits  with  an  AUC  of  0.75  using  random  forest,  which  is  higher  than  the  AUC  of  0.64  obtained  with  the  best  of  the  previously  used  automatic  methods.
1	Leveraging  historical  versions  of  android  apps  for  efficient  and  precise  taint  analysis.  Today,  computing  on  various  Android  devices  is  pervasive.  However,  growing  security  vulnerabilities  and  attacks  in  the  Android  ecosystem  constitute  various  threats  through  user  apps.  Taint  analysis  is  a  common  technique  for  defending  against  these  threats,  yet  it  suffers  from  challenges  in  attaining  practical  simultaneous  scalability  and  effectiveness.  This  paper  presents  a  novel  approach  to  fast  and  precise  taint  checking,  called  incremental  taint  analysis,  by  exploiting  the  evolving  nature  of  Android  apps.  The  analysis  narrows  down  the  search  space  of  taint  checking  from  an  entire  app,  as  conventionally  addressed,  to  the  parts  of  the  program  that  are  different  from  its  previous  versions.  This  technique  improves  the  overall  efficiency  of  checking  multiple  versions  of  the  app  as  it  evolves.  We  have  implemented  the  techniques  as  a  tool  prototype,  EvoTaint,  and  evaluated  our  analysis  by  applying  it  to  real-world  evolving  Android  apps.  Our  preliminary  results  show  that  the  incremental  approach  largely  reduced  the  cost  of  taint  analysis,  by  78.6%  on  average,  yet  without  sacrificing  the  analysis  effectiveness,  relative  to  a  representative  precise  taint  analysis  as  the  baseline.
1	On  the  correlation  between  size  and  metric  validity.  Empirical  validation  of  code  metrics  has  a  long  history  of  success.  Many  metrics  have  been  shown  to  be  good  predictors  of  external  features,  such  as  correlation  to  bugs.  Our  study  provides  an  alternative  explanation  to  such  validation,  attributing  it  to  the  confounding  effect  of  size.  In  contradiction  to  received  wisdom,  we  argue  that  the  validity  of  a  metric  can  be  explained  by  its  correlation  to  the  size  of  the  code  artifact.  In  fact,  this  work  came  about  in  view  of  our  failure  in  the  quest  of  finding  a  metric  that  is  both  valid  and  free  of  this  confounding  effect.  Our  main  discovery  is  that,  with  the  appropriate  (non-parametric)  transformations,  the  validity  of  a  metric  can  be  accurately  (with  R-squared  values  being  at  times  as  high  as  0.97)  predicted  from  its  correlation  with  size.  The  reported  results  are  with  respect  to  a  suite  of  26  metrics,  that  includes  the  famous  Chidamber  and  Kemerer  metrics.  Concretely,  it  is  shown  that  the  more  a  metric  is  correlated  with  size,  the  more  able  it  is  to  predict  external  features  values,  and  vice-versa.  We  consider  two  methods  for  controlling  for  size,  by  linear  transformations.  As  it  turns  out,  metrics  controlled  for  size,  tend  to  eliminate  their  predictive  capabilities.  We  also  show  that  the  famous  Chidamber  and  Kemerer  metrics  are  no  better  than  other  metrics  in  our  suite.  Overall,  our  results  suggest  code  size  is  the  only  “unique”  valid  metric.
1	A  comparison  of  tree  and  line  oriented  observational  slicing.  Observation-based  slicing  and  its  generalization  observational  slicing  are  recently-introduced,  language-independent  dynamic  slicing  techniques.  They  both  construct  slices  based  on  the  dependencies  observed  during  program  execution,  rather  than  static  or  dynamic  dependence  analysis.  The  original  implementation  of  the  observation-based  slicing  algorithm  used  lines  of  source  code  as  its  program  representation.  A  recent  variation,  developed  to  slice  modelling  languages  (such  as  Simulink),  used  an  XML  representation  of  an  executable  model.  We  ported  the  XML  slicer  to  source  code  by  constructing  a  tree  representation  of  traditional  source  code  through  the  use  of  srcML.  This  work  compares  the  tree-  and  line-based  slicers  using  four  experiments  involving  twenty  different  programs,  ranging  from  classic  benchmarks  to  million-line  production  systems.  The  resulting  slices  are  essentially  the  same  size  for  the  majority  of  the  programs  and  are  often  identical.  However,  structural  constraints  imposed  by  the  tree  representation  sometimes  force  the  slicer  to  retain  enclosing  control  structures.  It  can  also  “bog  down”  trying  to  delete  single-token  subtrees.  This  occasionally  makes  the  tree-based  slices  larger  and  the  tree-based  slicer  slower  than  a  parallelised  version  of  the  line-based  slicer.  In  addition,  a  Java  versus  C  comparison  finds  that  the  two  languages  lead  to  similar  slices,  but  Java  code  takes  noticeably  longer  to  slice.  The  initial  experiments  suggest  two  improvements  to  the  tree-based  slicer:  the  addition  of  a  size  threshold,  for  ignoring  small  subtrees,  and  subtree  replacement.  The  former  enables  the  slicer  to  run  3.4  times  faster  while  producing  slices  that  are  only  about  9%  larger.  At  the  same  time  the  subtree  replacement  reduces  size  by  about  8–12%  and  allows  the  tree-based  slicer  to  produce  more  natural  slices.
1	Software  microbenchmarking  in  the  cloud  how  bad  is  it  really.  Rigorous  performance  engineering  traditionally  assumes  measuring  on  bare-metal  environments  to  control  for  as  many  confounding  factors  as  possible.  Unfortunately,  some  researchers  and  practitioners  might  not  have  access,  knowledge,  or  funds  to  operate  dedicated  performance-testing  hardware,  making  public  clouds  an  attractive  alternative.  However,  shared  public  cloud  environments  are  inherently  unpredictable  in  terms  of  the  system  performance  they  provide.  In  this  study,  we  explore  the  effects  of  cloud  environments  on  the  variability  of  performance  test  results  and  to  what  extent  slowdowns  can  still  be  reliably  detected  even  in  a  public  cloud.  We  focus  on  software  microbenchmarks  as  an  example  of  performance  tests  and  execute  extensive  experiments  on  three  different  well-known  public  cloud  services  (AWS,  GCE,  and  Azure)  using  three  different  cloud  instance  types  per  service.  We  also  compare  the  results  to  a  hosted  bare-metal  offering  from  IBM  Bluemix.  In  total,  we  gathered  more  than  4.5  million  unique  microbenchmarking  data  points  from  benchmarks  written  in  Java  and  Go.  We  find  that  the  variability  of  results  differs  substantially  between  benchmarks  and  instance  types  (by  a  coefficient  of  variation  from  0.03%  to  > 100%).  However,  executing  test  and  control  experiments  on  the  same  instances  (in  randomized  order)  allows  us  to  detect  slowdowns  of  10%  or  less  with  high  confidence,  using  state-of-the-art  statistical  tests  (i.e.,  Wilcoxon  rank-sum  and  overlapping  bootstrapped  confidence  intervals).  Finally,  our  results  indicate  that  Wilcoxon  rank-sum  manages  to  detect  smaller  slowdowns  in  cloud  environments.
1	Finding  better  active  learners  for  faster  literature  reviews.  Literature  reviews  can  be  time-consuming  and  tedious  to  complete.  By  cataloging  and  refactoring  three  state-of-the-art  active  learning  techniques  from  evidence-based  medicine  and  legal  electronic  discovery,  this  paper  finds  and  implements  FASTREAD,  a  faster  technique  for  studying  a  large  corpus  of  documents.  This  paper  assesses  FASTREAD  using  datasets  generated  from  existing  SE  literature  reviews  (Hall,  Wahono,  Radjenovic,  Kitchenham  et  al.).  Compared  to  manual  methods,  FASTREAD  lets  researchers  find  95%  relevant  studies  after  reviewing  an  order  of  magnitude  fewer  papers.  Compared  to  other  state-of-the-art  automatic  methods,  FASTREAD  reviews  20-50%  fewer  studies  while  finding  same  number  of  relevant  primary  studies  in  a  systematic  literature  review.
1	Usage  and  attribution  of  stack  overflow  code  snippets  in  github  projects.  Stack  Overflow  (SO)  is  the  most  popular  question-and-answer  website  for  software  developers,  providing  a  large  amount  of  copyable  code  snippets.  Using  those  snippets  raises  maintenance  and  legal  issues.  SO’s  license  (CC  BY-SA  3.0)  requires  attribution,  i.e.,  referencing  the  original  question  or  answer,  and  requires  derived  work  to  adopt  a  compatible  license.  While  there  is  a  heated  debate  on  SO’s  license  model  for  code  snippets  and  the  required  attribution,  little  is  known  about  the  extent  to  which  snippets  are  copied  from  SO  without  proper  attribution.  We  present  results  of  a  large-scale  empirical  study  analyzing  the  usage  and  attribution  of  non-trivial  Java  code  snippets  from  SO  answers  in  public  GitHub  (GH)  projects.  We  followed  three  different  approaches  to  triangulate  an  estimate  for  the  ratio  of  unattributed  usages  and  conducted  two  online  surveys  with  software  developers  to  complement  our  results.  For  the  different  sets  of  projects  that  we  analyzed,  the  ratio  of  projects  containing  files  with  a  reference  to  SO  varied  between  3.3%  and  11.9%.  We  found  that  at  most  1.8%  of  all  analyzed  repositories  containing  code  from  SO  used  the  code  in  a  way  compatible  with  CC  BY-SA  3.0.  Moreover,  we  estimate  that  at  most  a  quarter  of  the  copied  code  snippets  from  SO  are  attributed  as  required.  Of  the  surveyed  developers,  almost  one  half  admitted  copying  code  from  SO  without  attribution  and  about  two  thirds  were  not  aware  of  the  license  of  SO  code  snippets  and  its  implications.
1	Search  review  repeat  an  empirical  study  of  threats  to  replicating  slr  searches.  A  systematic  literature  review  (SLR)  is  an  empirical  method  used  to  provide  an  overview  of  existing  knowledge  and  to  aggregate  evidence  within  a  domain.  For  computer  science,  several  threats  to  the  completeness  of  such  reviews  have  been  identified,  leading  to  recommendations  and  guidelines  on  how  to  improve  their  quality.  However,  few  studies  address  to  what  extent  researchers  can  replicate  an  SLR.  To  conduct  a  replication,  researchers  have  to  first  understand  how  the  set  of  primary  studies  has  been  identified  in  the  original  study,  and  can  ideally  retrieve  the  same  set  when  following  the  reported  protocol.  In  this  article,  we  focus  on  this  initial  step  of  a  replication  and  report  a  two-fold  empirical  study:  Initially,  we  performed  a  tertiary  study  using  a  sample  of  SLRs  in  computer  science  and  identified  what  information  that  is  needed  to  replicate  the  searches  is  reported.  Based  on  the  results,  we  conducted  a  descriptive,  multi-case  study  on  digital  libraries  to  investigate  to  what  extent  these  allow  replications.  The  results  reveal  two  threats  to  replications  of  SLRs:  First,  while  researchers  have  improved  the  quality  of  their  reports,  relevant  details  are  still  missing—we  refer  to  a  reporting  threat.  Second,  we  found  that  some  digital  libraries  are  inconsistent  in  their  query  results—we  refer  to  a  searching  threat.  While  researchers  conducting  a  review  can  only  overcome  the  first  threat  and  the  second  may  not  be  an  issue  for  all  kinds  of  replications,  researchers  should  be  aware  of  both  threats  when  conducting,  reviewing,  and  building  on  SLRs.
1	Index  based  model  clone  detection.  Existing  algorithms  for  model  clone  detection  operate  in  batch  mode.  Consequently,  if  a  small  part  of  a  large  model  changes  during  maintenance,  the  entire  detection  needs  to  be  recomputed  to  produce  updated  cloning  information.  Since  this  can  take  several  hours,  the  lack  of  incremental  detection  algorithms  hinders  clone  management,  which  requires  up-to-date  cloning  information.  In  this  paper  we  present  an  index-based  algorithm  for  model  clone  detection  that  is  incremental  and  distributable.  We  present  a  case  study  that  demonstrates  its  capabilities,  outline  its  current  limitations  and  present  directions  for  future  work.
1	Extracting  code  clones  for  refactoring  using  combinations  of  clone  metrics.  Code  clone  detection  tools  may  report  a  large  number  of  code  clones,  while  software  developers  are  interested  in  only  a  subset  of  code  clones  that  are  relevant  to  software  development  tasks  such  as  refactoring.  Our  research  group  has  supported  many  software  developers  with  the  code  clone  detection  tool  CCFinder  and  its  GUI  front-end  Gemini.  Gemini  shows  clone  sets  (i.e.,  a  set  of  code  clones  identical  or  similar  to  each  other)  with  several  clone  metrics  including  their  length  and  the  number  of  code  clones;  however,  it  is  not  clear  how  to  use  those  metrics  to  extract  interesting  code  clones  for  developers.  In  this  paper,  we  propose  a  method  combining  clone  metrics  to  extract  code  clones  for  refactoring  activity.  We  have  conducted  an  empirical  study  on  a  web  application  developed  by  a  Japanese  software  company.  The  result  indicates  that  combinations  of  simple  clone  metric  is  more  effective  to  extract  refactoring  candidates  in  detected  code  clones  than  individual  clone  metric.
1	Evolution  of  code  clone  ratios  throughout  development  history  of  open  source  c  and  c  programs.  A  code  clone  is  a  fragment  of  code  which  is  duplicated  throughout  the  source  code  of  a  project.  Code  clones  have  been  shown  to  make  a  project  less  maintainable  because  all  code  clones  will  share  potential  bugs  and  problems.  Unlike  other  code  clone  research,  this  study  analyzes  the  code  clone  ratios  over  the  entire  development  lifetime  of  three  open-source  projects  written  in  C/C++  to  understand  code  clone  growth  in  software  over  development  and  potential  developer  habits  which  could  affect  this  growth.  The  study  utilizes  CCFinderX  and  Git  to  detect  clone  metrics  across  development  history.  The  results  from  each  project  show  very  low,  stable  ratios  across  development  history,  with  the  code  clone  ratios  only  fluctuating  greatly  during  the  beginning  of  development  mostly  and  very  little  refactoring  occurring.  This  study  goes  further  into  the  potential  cause  of  low  ratios  and  different  fluctuations  at  different  periods  of  development.
1	Research  in  cloning  beyond  code  a  first  roadmap.  Most  research  in  software  cloning  has  a  strong  focus  on  source  code.  However,  cloning  occurs  in  other  software  artifacts,  as  well.  In  this  paper,  we  summarize  existing  work  on  cloning  in  other  software  artifacts  and  provide  a  list  of  research  questions  for  future  work.
1	Foundations  for  streaming  model  transformations  by  complex  event  processing.  Streaming  model  transformations  represent  a  novel  class  of  transformations  to  manipulate  models  whose  elements  are  continuously  produced  or  modified  in  high  volume  and  with  rapid  rate  of  change.  Executing  streaming  transformations  requires  efficient  techniques  to  recognize  activated  transformation  rules  over  a  live  model  and  a  potentially  infinite  stream  of  events.  In  this  paper,  we  propose  foundations  of  streaming  model  transformations  by  innovatively  integrating  incremental  model  query,  complex  event  processing  (CEP)  and  reactive  (event-driven)  transformation  techniques.  Complex  event  processing  allows  to  identify  relevant  patterns  and  sequences  of  events  over  an  event  stream.  Our  approach  enables  event  streams  to  include  model  change  events  which  are  automatically  and  continuously  populated  by  incremental  model  queries.  Furthermore,  a  reactive  rule  engine  carries  out  transformations  on  identified  complex  event  patterns.  We  provide  an  integrated  domain-specific  language  with  precise  semantics  for  capturing  complex  event  patterns  and  streaming  transformations  together  with  an  execution  engine,  all  of  which  is  now  part  of  the  Viatra  reactive  transformation  framework.  We  demonstrate  the  feasibility  of  our  approach  with  two  case  studies:  one  in  an  advanced  model  engineering  workflow;  and  one  in  the  context  of  on-the-fly  gesture  recognition.
1	Beyond  loop  bounds  comparing  annotation  languages  for  worst  case  execution  time  analysis.  Worst-case  execution  time  (WCET)  analysis  is  concerned  with  computing  a  precise-as-possible  bound  for  the  maximum  time  the  execution  of  a  program  can  take.  This  information  is  indispensable  for  developing  safety-critical  real-time  systems,  e.  g.,  in  the  avionics  and  automotive  fields.  Starting  with  the  initial  works  of  Chen,  Mok,  Puschner,  Shaw,  and  others  in  the  mid  and  late  1980s,  WCET  analysis  turned  into  a  well-established  and  vibrant  field  of  research  and  development  in  academia  and  industry.  The  increasing  number  and  diversity  of  hardware  and  software  platforms  and  the  ongoing  rapid  technological  advancement  became  drivers  for  the  development  of  a  wide  array  of  distinct  methods  and  tools  for  WCET  analysis.  The  precision,  generality,  and  efficiency  of  these  methods  and  tools  depend  much  on  the  expressiveness  and  usability  of  the  annotation  languages  that  are  used  to  describe  feasible  and  infeasible  program  paths.  In  this  article  we  survey  the  annotation  languages  which  we  consider  formative  for  the  field.  By  investigating  and  comparing  their  individual  strengths  and  limitations  with  respect  to  a  set  of  pivotal  criteria,  we  provide  a  coherent  overview  of  the  state  of  the  art.  Identifying  open  issues,  we  encourage  further  research.  This  way,  our  approach  is  orthogonal  and  complementary  to  a  recent  approach  of  Wilhelm  et  al.  who  provide  a  thorough  survey  of  WCET  analysis  methods  and  tools  that  have  been  developed  and  used  in  academia  and  industry.
1	Transforming  xml  schemas  into  owl  ontologies  using  formal  concept  analysis.  Ontology  Web  Language  (OWL)  is  considered  as  a  data  representation  format  exploited  by  the  Extensible  Markup  Language  (XML)  format.  OWL  extends  XML  by  providing  properties  to  further  express  the  semantics  of  data.  To  this  effect,  transforming  XML  data  into  OWL  proves  important  and  constitutes  an  added  value  for  indexing  XML  documents  and  re-engineering  ontologies.  In  this  paper,  we  propose  a  formal  method  to  transform  XSD  schemas  into  OWL  schemas  using  transformation  patterns.  To  achieve  this  end,  we  extend  at  the  beginning,  a  set  of  existing  transformation  patterns  to  allow  the  maximum  transformation  of  XSD  schema  constructions.  In  addition,  a  formal  method  is  presented  to  transform  an  XSD  schema  using  the  extended  patterns.  This  method  named  PIXCO  comprises  several  processes.  The  first  process  models  both  the  transformation  patterns  and  all  the  constructions  of  XSD  schema  to  be  transformed.  The  patterns  are  modeled  using  the  context  of  Formal  Concept  Analysis.  The  XSD  constructions  are  modeled  using  a  proposed  mathematical  model.  This  modeling  will  be  used  in  the  design  of  the  following  process.  The  second  process  identifies  the  most  appropriate  patterns  to  transform  each  construction  set  of  XSD  schema.  The  third  process  generates  for  each  XSD  construction  set  an  OWL  model  according  to  the  pattern  that  is  identified.  Finally,  it  creates  the  OWL  file  encompassing  the  generated  OWL  models.
1	Variability  in  uml  language  and  semantics.  Practitioners,  who  use  UML  as  a  sketching  language  are  generally  not  too  concerned  about  the  precision  of  their  models,  but  developers  who  build  UML  models  to  rigorously  analyze  software  properties  (e.g.,  to  analyze  the  consistency  of  design  constraints)  or  that  can  be  mechanically  transformed  to  implementations  requiring  tools  and  tool  chains  that  are  based  on  a  precisely  defined  UML  semantics  (see  this  issue’s  Expert  Voice  by  Manfred  Broy  and  Maria  Victoria  Cengarle  as  well  as  the  regular  paper  on  the  many  semantics  of  sequence  diagrams  by  Zoltan  Micskei  and  Helene  Waeselynck).  This  needmotivatesmuchof  thework  ondefining  appropriate  formal  semantics  for  the  UML.  There  is  a  significantly  large  body  of  work  on  formalizing  the  UML—both  syntactical  appearance,  internal  representation  and  semantics  (in  terms  of  meaning),  and  the  collective  experience  suggests  that  defining  appropriate  semantics  for  the  UML  has  both  a  technical  and  a  strong  political/social  aspect.  This  non-technical  aspect  is  concerned  primarilywith  determiningwhat  constitutes  an  “appropriate”  language.  The  problem  is  that  different  stakeholders,  including  UML  modelers  fromdifferent  domains,  tool  vendorswith  specific  ready  to  use  solutions,  have  varying  views  of  what  constitutes  an  appropriate  UML  language  and  its  semantics.  It  is  not  easily  possible  to  support  these  sometimes  competing  views  in  a  single  language.  This  led  to  the  view  of  UML  as  a  “family  of  languages”  and  to  the  introduction  of  profile  mechanisms  and  “semantic  variation  points”  that  can  be  used  for  specializing  the  syntax  and  semantics  of  UML.
1	Addressing  the  evolution  of  automated  user  behaviour  patterns  by  runtime  model  interpretation.  The  use  of  high-level  abstraction  models  can  facilitate  and  improve  not  only  system  development  but  also  runtime  system  evolution.  This  is  the  idea  of  this  work,  in  which  behavioural  models  created  at  design  time  are  also  used  at  runtime  to  evolve  system  behaviour.  These  behavioural  models  describe  the  routine  tasks  that  users  want  to  be  automated  by  the  system.  However,  users'  needs  may  change  after  system  deployment,  and  the  routine  tasks  automated  by  the  system  must  evolve  to  adapt  to  these  changes.  To  facilitate  this  evolution,  the  automation  of  the  specified  routine  tasks  is  achieved  by  directly  interpreting  the  models  at  runtime.  This  turns  models  into  the  primary  means  to  understand  and  interact  with  the  system  behaviour  associated  with  the  routine  tasks  as  well  as  to  execute  and  modify  it.  Thus,  we  provide  tools  to  allow  the  adaptation  of  this  behaviour  by  modifying  the  models  at  runtime.  This  means  that  the  system  behaviour  evolution  is  performed  by  using  high-level  abstractions  and  avoiding  the  costs  and  risks  associated  with  shutting  down  and  restarting  the  system.
1	Modelling  the  interplay  of  security  privacy  and  trust  in  sociotechnical  systems  a  computer  aided  design  approach.  Personal  data  have  become  a  central  asset  for  multiple  enterprise  applications  and  online  services  offered  by  private  companies,  public  organisations  or  a  combination  of  both.  The  sensitivity  of  such  data  and  the  continuously  growing  legislation  that  accompanies  their  management  dictate  the  development  of  methods  that  allow  the  development  of  more  secure,  trustworthy  software  systems  with  focus  on  privacy  protection.  The  contribution  of  this  paper  is  the  definition  of  a  novel  requirements  engineering  method  that  supports  both  early  and  late  requirements  specification,  giving  emphasis  on  security,  privacy  and  trust.  The  novelty  of  our  work  is  that  it  provides  the  means  for  software  designers  and  security  experts  to  analyse  the  system-to-be  from  multiple  aspects,  starting  from  identifying  high-level  goals  to  the  definition  of  business  process  composition,  and  elicitation  of  mechanisms  to  fortify  the  system  from  external  threats.  The  method  is  supported  by  two  CASE  tools.  To  demonstrate  the  applicability  and  usefulness  of  our  work,  the  paper  shows  its  applications  to  a  real-world  case  study.
1	Teaching  model  driven  engineering  from  a  relational  database  perspective.  We  reinterpret  MDE  from  the  viewpoint  of  relational  databases  to  provide  an  alternative  way  to  understand,  demonstrate,  and  teach  MDE  using  concepts  and  technologies  that  should  be  familiar  to  undergraduates.  We  use  (1)  relational  database  schemas  to  express  metamodels,  (2)  relational  databases  to  express  models,  (3)  Prolog  to  express  constraints  and  M2M  transformations,  (4)  Java  tools  to  implement  M2T  and  T2M  transformations,  and  (5)  Java  to  execute  transformations.  Application  case  studies  and  a  user  study  illuminate  the  viability  and  benefits  of  our  approach.
1	An  algorithm  for  generating  model  sensitive  search  plans  for  pattern  matching  on  emf  models.  In  this  paper,  we  propose  a  new  model-sensitive  search  plan  generation  algorithm  to  speed  up  the  process  of  graph  pattern  matching.  This  dynamic-programming-based  algorithm,  which  is  able  to  handle  general  n-ary  constraints  in  an  integrated  manner,  collects  statistical  data  from  the  underlying  EMF  model  and  uses  this  information  for  optimization  purposes.  Additionally,  the  search  plan  generation  algorithm  itself  and  its  runtime  effects  on  the  pattern  matching  engine  have  been  evaluated  by  complexity  analysis  techniques  and  by  quantitative  performance  measurements,  respectively.
1	A  graph  theoretic  method  for  the  inductive  development  of  reference  process  models.  Business  process  management  is  one  of  the  most  widely  discussed  topics  in  information  systems  research.  As  process  models  advance  in  both  complexity  and  maturity,  reference  models,  serving  as  reusable  blueprints  for  the  development  of  individual  models,  gain  more  and  more  importance.  Only  a  few  business  domains  have  access  to  commonly  accepted  reference  models,  so  there  is  a  widespread  need  for  the  development  of  new  ones.  This  article  describes  a  new  inductive  approach  for  the  development  of  reference  models,  based  on  existing  individual  models  from  the  respective  domain.  It  employs  a  graph-based  paradigm,  exploiting  the  underlying  graph  structures  of  process  models  by  identifying  frequent  common  subgraphs  of  the  individual  models,  analyzing  their  order  relations,  and  merging  them  into  a  new  model.  This  newly  developed  approach  is  outlined  and  evaluated  in  this  contribution.  It  is  applied  in  three  different  case  studies  and  compared  to  other  approaches  to  the  inductive  development  of  reference  models  in  order  to  highlight  its  characteristics  as  well  as  assets  and  drawbacks.
1	A  uml  profile  for  the  design  quality  assessment  and  deployment  of  data  intensive  applications.  Big  Data  or  Data-Intensive  applications  (DIAs)  seek  to  mine,  manipulate,  extract  or  otherwise  exploit  the  potential  intelligence  hidden  behind  Big  Data.  However,  several  practitioner  surveys  remark  that  DIAs  potential  is  still  untapped  because  of  very  difficult  and  costly  design,  quality  assessment  and  continuous  refinement.  To  address  the  above  shortcoming,  we  propose  the  use  of  a  UML  domain-specific  modeling  language  or  profile  specifically  tailored  to  support  the  design,  assessment  and  continuous  deployment  of  DIAs.  This  article  illustrates  our  DIA-specific  profile  and  outlines  its  usage  in  the  context  of  DIA  performance  engineering  and  deployment.  For  DIA  performance  engineering,  we  rely  on  the  Apache  Hadoop  technology,  while  for  DIA  deployment,  we  leverage  the  TOSCA  language.  We  conclude  that  the  proposed  profile  offers  a  powerful  language  for  data-intensive  software  and  systems  modeling,  quality  evaluation  and  automated  deployment  of  DIAs  on  private  or  public  clouds.
1	A  systematic  literature  review  of  cross  domain  model  consistency  checking  by  model  management  tools.  Objective The  goal  of  this  study  is  to  identify  gaps  and  challenges  related  to  cross-domain  model  management  focusing  on  consistency  checking.  Method We  conducted  a  systematic  literature  review.  We  used  the  keyword-based  search  on  Google  Scholar,  and  we  identified  618 potentially  relevant  studies;  after  applying  inclusion  and  exclusion  criteria,  96 papers  were  selected  for  further  analysis.  Results The  main  findings/contributions  are:  (i)  a  list  of  available  tools  used  to  support  model  management;  (ii)  40%  of  the  tools  can  provide  consistency  checking  on  models  of  different  domains  and  25%  on  models  of  the  same  domain,  and  35%  do  not  provide  any  consistency  checking;  (iii)  available  strategies  to  keep  the  consistency  between  models  of  different  domains  are  not  mature  enough;  (iv)  most  of  the  tools  that  provide  consistency  checking  on  models  of  different  domains  can  only  capture  up  to  two  inconsistency  types;  (v)  the  main  challenges  associated  with  tools  that  manage  models  on  different  domains  are  related  to  interoperability  between  tools  and  the  consistency  maintenance.  Conclusion The  results  presented  in  this  study  can  be  used  to  guide  new  research  on  maintaining  the  consistency  between  models  of  different  domains.  Example  of  further  research  is  to  investigate  how  to  capture  the  Behavioral  and  Refinement  inconsistency  types.  This  study  also  indicates  that  the  tools  should  be  improved  in  order  to  address,  for  example,  more  kinds  of  consistency  check.
1	Mdwa  a  model  driven  web  augmentation  approach  coping  with  client  and  server  side  support.  Web  augmentation  is  a  set  of  techniques  allowing  users  to  define  and  execute  software  which  is  dependent  on  the  presentation  layer  of  a  concrete  Web  page.  Through  the  use  of  specialized  Web  augmentation  artifacts,  the  end  users  may  satisfy  several  kinds  of  requirements  that  were  not  considered  by  the  analysts,  developers  and  stakeholders  that  built  the  application.  Although  some  augmentation  approaches  are  contemplating  a  server-side  counterpart  (to  support  aspects  such  as  collaboration  or  cross-browser  session  management),  the  augmentation  artifacts  are  usually  purely  client-side.  The  server-side  support  increases  the  capabilities  of  the  augmentations,  since  it  may  allow  sharing  information  among  users  and  devices.  So  far,  this  support  is  often  defined  and  developed  in  an  ad  hoc  way.  Although  it  is  clear  that  server-side  support  brings  new  possibilities,  it  is  also  true  that  developing  and  deploying  server-side  Web  applications  is  a  challenging  task  that  end  users  hardly  may  handle.  This  work  presents  a  novel  approach  for  designing  Web  augmentation  applications  based  on  client-side  and  server-side  components.  We  propose  a  model-driven  approach  that  raises  the  abstraction  level  of  both,  client-  and  server-side  developments.  We  provide  a  set  of  tools  for  designing  the  composition  of  the  core  application  with  new  features  on  the  back-end  and  the  augmentation  of  pages  in  the  front-end.  The  usability  and  the  value  of  the  produced  augmentations  have  been  evaluated  through  two  experiments  involving  30  people  in  total.
1	Deontic  bpmn  a  powerful  extension  of  bpmn  with  a  trusted  model  transformation.  The  Business  Process  Model  and  Notation  (BPMN)  is  a  widely-used  standard  for  process  modelling.  A  drawback  of  BPMN,  however,  is  that  modality  is  implicitly  expressed  through  the  structure  of  the  process  flow  but  not  directly  within  the  corresponding  activity.  Thus,  an  extension  of  BPMN  with  deontic  logic  has  been  proposed  in  previous  work,  called  Deontic  BPMN.  Deontic  BPMN  reduces  the  structural  complexity  of  the  process  flow  and  increases  the  readability  by  explicitly  highlighting  obligatory  and  permissible  activities.  In  addition,  an  algebraic  graph  transformation  from  a  subset  of  BPMN  to  Deontic  BPMN,  called  Deontic  BpmnGTS,  has  been  defined.  The  goal  of  the  current  research  is  to  show  that  DeonticBpmnGTS  is  terminating  and  confluent,  resulting  in  a  globally  deterministic  transformation.  Moreover,  the  semantic  equivalence  of  BPMN  models  and  the  respective  Deontic  BPMN  models  is  proven  based  on  Abstract  State  Machines  (ASMs).  Thus,  DeonticBpmnGTS  can  be  called  a  trusted  model  transformation.
1	Performance  models  of  storage  contention  in  cloud  environments.  We  propose  simple  models  to  predict  the  performance  degradation  of  disk  requests  due  to  storage  device  contention  in  consolidated  virtualized  environments.  Model  parameters  can  be  deduced  from  measurements  obtained  inside  Virtual  Machines  (VMs)  from  a  system  where  a  single  VM  accesses  a  remote  storage  server.  The  parameterized  model  can  then  be  used  to  predict  the  effect  of  storage  contention  when  multiple  VMs  are  consolidated  on  the  same  server.  We  first  propose  a  trace-driven  approach  that  evaluates  a  queueing  network  with  fair  share  scheduling  using  simulation.  The  model  parameters  consider  Virtual  Machine  Monitor  level  disk  access  optimizations  and  rely  on  a  calibration  technique.  We  further  present  a  measurement-based  approach  that  allows  a  distinct  characterization  of  read/write  performance  attributes.  In  particular,  we  define  simple  linear  prediction  models  for  I/O  request  mean  response  times,  throughputs  and  read/write  mixes,  as  well  as  a  simulation  model  for  predicting  response  time  distributions.  We  found  our  models  to  be  effective  in  predicting  such  quantities  across  a  range  of  synthetic  and  emulated  application  workloads.
1	On  the  impact  of  size  to  the  understanding  of  uml  diagrams.  Practical  experience  suggests  that  usage  and  understanding  of  UML  diagrams  is  greatly  affected  by  the  quality  of  their  layout.  While  existing  research  failed  to  provide  conclusive  and  comprehensive  evidence  in  support  of  this  hypothesis,  our  own  previous  work  provided  substantial  evidence  to  this  effect,  also  suggesting  diagram  size  as  a  relevant  factor,  for  a  range  of  diagram  types  and  layouts.  Since  there  is  no  generally  accepted  precise  notion  of  “diagram  size,”  we  first  need  to  operationalize  this  concept,  analyze  its  impact  on  diagram  understanding,  and  derive  practical  advice  from  our  findings.  We  define  three  alternative,  plausible  metrics.  Since  they  are  all  highly  correlated  on  a  large  sample  of  UML  diagrams,  we  opt  for  the  simplest  one.  We  use  it  to  re-analyze  existing  experimental  data  on  diagram  understanding.  We  find  a  strong  negative  correlation  between  diagram  size  and  modeler  performance.  Our  results  are  statistically  highly  significant  and  exhibit  a  very  large  degree  of  validity.  We  utilize  these  results  to  derive  a  recommendation  on  diagram  sizes  that  are,  on  average,  optimal  for  model  understanding.  These  recommendations  are  implemented  in  a  plug-in  to  a  widely  used  modeling  tool,  providing  continuous  feedback  about  diagram  size  to  modelers.  The  effect  sizes  are  varying,  but  generally  suggest  that  the  impact  of  size  matches  or  exceeds  that  of  other  factors  in  diagram  understanding.  With  the  guideline  and  tool,  modelers  are  steered  toward  avoiding  too  large  diagrams.
1	Generating  test  cases  for  real  time  systems  based  on  symbolic  models.  The  state  space  explosion  problem  is  one  of  the  challenges  to  be  faced  by  test  case  generation  techniques,  particularly  when  data  values  need  to  be  enumerated.  This  problem  gets  even  worse  for  real-time  systems  (RTS)  that  also  have  time  constraints.  The  usual  solution  in  this  context,  based  on  finite  state  machines  or  time  automata,  consists  of  enumerating  data  values  (restricted  to  finite  domains)  while  treating  time  symbolically.  In  this  paper,  a  symbolic  model  for  conformance  testing  of  real-time  systems  software  named  TIOSTS  that  addresses  both  data  and  time  symbolically  is  presented.  Moreover,  a  test  case  generation  process  is  defined  to  select  more  general  test  cases  with  variables  and  parameters  that  can  be  instantiated  at  testing  execution  time.  Generation  is  based  on  a  combination  of  symbolic  execution  and  constraint  solving  for  the  data  part  and  symbolic  analysis  for  timed  aspects.  Furthermore,  the  practical  application  of  the  process  is  investigated  through  a  case  study.
1	Early  detection  of  collaboration  conflicts  and  risks.  Conflicts  among  developers'  inconsistent  copies  of  a  shared  project  arise  in  collaborative  development  and  can  slow  progress  and  decrease  quality.  Identifying  and  resolving  such  conflicts  early  can  help.  Identifying  situations  which  may  lead  to  conflicts  can  prevent  some  conflicts  altogether.  By  studying  nine  open-source  systems  totaling  3.4  million  lines  of  code,  we  establish  that  conflicts  are  frequent,  persistent,  and  appear  not  only  as  overlapping  textual  edits  but  also  as  subsequent  build  and  test  failures.  Motivated  by  this  finding,  we  develop  a  speculative  analysis  technique  that  uses  previously  unexploited  information  from  version  control  operations  to  precisely  diagnose  important  classes  of  conflicts.  Then,  we  design  and  implement  Crystal,  a  publicly  available  tool  that  helps  developers  identify,  manage,  and  prevent  conflicts.  Crystal  uses  speculative  analysis  to  make  concrete  advice  unobtrusively  available  to  developers.
1	Energy  storage  devices  in  electrified  railway  systems  a  review.  As  a  large  energy  consumer,  the  railway  systems  in  many  countries  have  been  electrified  gradually  for  the  purposes  of  performance  improvement  and  emission  reduction.  With  the  widespread  utilization  of  energy-saving  technologies  such  as  regenerative  braking  techniques,  and  in  support  of  the  full  electrification  of  railway  systems  in  a  wide  range  of  application  conditions,  energy  storage  systems  (ESSes)  have  come  to  play  an  essential  role.  In  this  paper,  some  recent  developments  in  railway  ESSes  are  reviewed  and  a  comprehensive  comparison  is  presented  for  various  ESS  technologies.  The  foremost  functionalities  of  the  railway  ESSes  are  presented  together  with  possible  solutions  proposed  from  the  academic  arena  and  current  practice  in  the  railway  industry.  In  addition,  the  challenges  and  future  trends  of  ESSes  in  the  railway  industry  are  briefly  discussed.
1	Control  theoretical  software  adaptation  a  systematic  literature  review.  Modern  software  applications  are  subject  to  uncertain  operating  conditions,  such  as  dynamics  in  the  availability  of  services  and  variations  of  system  goals.  Consequently,  runtime  changes  cannot  be  ignored,  but  often  cannot  be  predicted  at  design  time.  Control  theory  has  been  identified  as  a  principled  way  of  addressing  runtime  changes  and  it  has  been  applied  successfully  to  modify  the  structure  and  behavior  of  software  applications.  Most  of  the  times,  however,  the  adaptation  targeted  the  resources  that  the  software  has  available  for  execution  (CPU,  storage,  etc.)  more  than  the  software  application  itself.  This  paper  investigates  the  research  efforts  that  have  been  conducted  to  make  software  adaptable  by  modifying  the  software  rather  than  the  resource  allocated  to  its  execution.  This  paper  aims  to  identify:  the  focus  of  research  on  control-theoretical  software  adaptation;  how  software  is  modeled  and  what  control  mechanisms  are  used  to  adapt  software;  what  software  qualities  and  controller  guarantees  are  considered.  To  that  end,  we  performed  a  systematic  literature  review  in  which  we  extracted  data  from  42  primary  studies  selected  from  1512  papers  that  resulted  from  an  automatic  search.  The  results  of  our  investigation  show  that  even  though  the  behavior  of  software  is  considered  non-linear,  research  efforts  use  linear  models  to  represent  it,  with  some  success.  Also,  the  control  strategies  that  are  most  often  considered  are  classic  control,  mostly  in  the  form  of  Proportional  and  Integral  controllers,  and  Model  Predictive  Control.  The  paper  also  discusses  sensing  and  actuating  strategies  that  are  prominent  for  software  adaptation  and  the  (often  neglected)  proof  of  formal  properties.  Finally,  we  distill  open  challenges  for  control-theoretical  software  adaptation.
1	A  hierarchical  matrix  factorization  approach  for  location  based  web  service  qos  prediction.  With  the  rapid  growth  of  population  of  service-oriented  architecture  (SOA),  services  are  playing  an  important  role  in  software  development  process.  One  major  issue  we  should  consider  about  Web  services  is  to  dig  out  the  one  with  the  best  QoS  value  among  all  functionally-equivalent  candidates.  However,  since  there  are  a  great  number  of  missing  QoS  values  in  real  world  invocation  records,  we  can  hardly  do  a  detailed  comparison  among  those  selectable  Web  services.  To  address  this  problem,  we  propose  a  location-based  hierarchical  matrix  factorization  method  to  make  efficient  and  accurate  QoS  prediction.  In  our  method,  we  consider  both  global  context  and  local  information.  We  first  apply  matrix  factorization  (MF)  on  global  user-service  records  and  obtain  a  global  prediction  matrix.  After  that,  we  use  MF  to  predict  QoS  values  on  some  user-service  groups,  which  are  clustered  by  K-means  algorithm.  Then  we  combine  global  and  local  predicted  QoS  values  to  provide  our  final  prediction.  Extensive  experiments  show  the  effectiveness  of  our  hierarchical  approach  which  outperforms  other  popular  methods.
1	On  scheduling  constraint  abstraction  for  multi  threaded  program  verification.  Bounded  model  checking  is  among  the  most  efficient  techniques  for  the  automated  verification  of  concurrent  programs.  However,  due  to  the  nondeterministic  thread  interleavings,  a  large  and  complex  formula  is  usually  required  to  give  an  exact  encoding  of  all  possible  behaviors,  which  significantly  limits  the  scalability.  Observing  that  the  large  formula  is  usually  dominated  by  the  exact  encoding  of  the  scheduling  constraint,  this  paper  proposes  a  novel  scheduling  constraint  based  abstraction  refinement  method  for  multi-threaded  C  program  verification.  Our  method  is  both  efficient  in  practice  and  complete  in  theory,  which  is  challenging  for  existing  techniques.  To  achieve  this,  we  first  proposed  an  effective  and  powerful  technique  which  works  well  for  nearly  all  benchmarks  we  evaluated.  We  have  proposed  the  notion  of  Event  Order  Graph  (EOG)  ,  and  have  devised  two  graph-based  algorithms  over  EOG  for  counterexample  validation  and  refinement  generation,  which  can  often  obtain  a  small  yet  effective  refinement  constraint.  Then,  to  ensure  completeness,  our  method  was  enhanced  with  two  constraint-based  algorithms  for  counterexample  validation  and  refinement  generation.  Experimental  results  on  SV-COMP  2017  benchmarks  and  two  real-world  server  systems  indicate  that  our  method  is  promising  and  significantly  outperforms  the  state-of-the-art  tools.
1	Ranking  and  clustering  software  cost  estimation  models  through  a  multiple  comparisons  algorithm.  Software  Cost  Estimation  can  be  described  as  the  process  of  predicting  the  most  realistic  effort  required  to  complete  a  software  project.  Due  to  the  strong  relationship  of  accurate  effort  estimations  with  many  crucial  project  management  activities,  the  research  community  has  been  focused  on  the  development  and  application  of  a  vast  variety  of  methods  and  models  trying  to  improve  the  estimation  procedure.  From  the  diversity  of  methods  emerged  the  need  for  comparisons  to  determine  the  best  model.  However,  the  inconsistent  results  brought  to  light  significant  doubts  and  uncertainty  about  the  appropriateness  of  the  comparison  process  in  experimental  studies.  Overall,  there  exist  several  potential  sources  of  bias  that  have  to  be  considered  in  order  to  reinforce  the  confidence  of  experiments.  In  this  paper,  we  propose  a  statistical  framework  based  on  a  multiple  comparisons  algorithm  in  order  to  rank  several  cost  estimation  models,  identifying  those  which  have  significant  differences  in  accuracy,  and  clustering  them  in  nonoverlapping  groups.  The  proposed  framework  is  applied  in  a  large-scale  setup  of  comparing  11  prediction  models  over  six  datasets.  The  results  illustrate  the  benefits  and  the  significant  information  obtained  through  the  systematic  comparison  of  alternative  methods.
1	Programmer  friendly  refactoring  errors.  Refactoring  tools,  common  to  many  integrated  development  environments,  can  help  programmers  to  restructure  their  code.  These  tools  sometimes  refuse  to  restructure  the  programmer's  code,  instead  giving  the  programmer  a  textual  error  message  that  she  must  decode  if  she  wishes  to  understand  the  reason  for  the  tool's  refusal  and  what  corrective  action  to  take.  This  paper  describes  a  graphical  alternative  to  textual  error  messages  called  Refactoring  Annotations.  It  reports  on  two  experiments,  one  using  an  integrated  development  environment  and  the  other  using  paper  mockups,  that  show  that  programmers  can  use  Refactoring  Annotations  to  quickly  and  accurately  understand  the  cause  of  refactoring  errors.
1	How  software  designers  interact  with  sketches  at  the  whiteboard.  Whiteboard  sketches  play  a  crucial  role  in  software  development,  helping  to  support  groups  of  designers  in  reasoning  about  a  software  design  problem  at  hand.  However,  little  is  known  about  these  sketches  and  how  they  support  design  ‘in  the  moment’,  particularly  in  terms  of  the  relationships  among  sketches,  visual  syntactic  elements  within  sketches,  and  reasoning  activities.  To  address  this  gap,  we  analyzed  14 hours  of  design  activity  by  eight  pairs  of  professional  software  designers,  manually  coding  over  4000  events  capturing  the  introduction  of  visual  syntactic  elements  into  sketches,  focus  transitions  between  sketches,  and  reasoning  activities.  Our  findings  indicate  that  sketches  serve  as  a  rich  medium  for  supporting  design  conversations.  Designers  often  use  general-purpose  notations.  Designers  introduce  new  syntactic  elements  to  record  aspects  of  the  design,  or  re-purpose  sketches  as  the  design  develops.  Designers  constantly  shift  focus  between  sketches,  using  groups  of  sketches  together  that  contain  complementary  information.  Finally,  sketches  play  an  important  role  in  supporting  several  types  of  reasoning  activities  (mental  simulation,  review  of  progress,  consideration  of  alternatives).  But  these  activities  often  leave  no  trace  and  rarely  lead  to  sketch  creation.  We  discuss  the  implications  of  these  and  other  findings  for  the  practice  of  software  design  at  the  whiteboard  and  for  the  creation  of  new  electronic  software  design  sketching  tools.
1	Construction  and  validation  of  an  instrument  for  measuring  programming  skill.  Skilled  workers  are  crucial  to  the  success  of  software  development.  The  current  practice  in  research  and  industry  for  assessing  programming  skills  is  mostly  to  use  proxy  variables  of  skill,  such  as  education,  experience,  and  multiple-choice  knowledge  tests.  There  is  as  yet  no  valid  and  efficient  way  to  measure  programming  skill.  The  aim  of  this  research  is  to  develop  a  valid  instrument  that  measures  programming  skill  by  inferring  skill  directly  from  the  performance  on  programming  tasks.  Over  two  days,  65  professional  developers  from  eight  countries  solved  19  Java  programming  tasks.  Based  on  the  developers’  performance,  the  Rasch  measurement  model  was  used  to  construct  the  instrument.  The  instrument  was  found  to  have  satisfactory  (internal)  psychometric  properties  and  correlated  with  external  variables  in  compliance  with  theoretical  expectations.  Such  an  instrument  has  many  implications  for  practice,  for  example,  in  job  recruitment  and  project  allocation.
1	Empirical  evaluation  of  fault  localisation  using  code  and  change  metrics.  Fault  localisation  aims  to  reduce  the  debugging  efforts  of  human  developers  by  highlighting  the  program  elements  that  are  suspected  to  be  the  root  cause  of  the  observed  failure.  Spectrum  Based  Fault  Localisation  (SBFL),  a  coverage  based  approach,  has  been  widely  studied  in  many  researches  as  a  promising  localisation  technique.  Recently,  however,  it  has  been  proven  that  SBFL  techniques  have  reached  the  limit  of  further  improvement.  To  overcome  the  limitation,  we  extend  SBFL  with  code  and  change  metrics  that  have  been  mainly  studied  in  defect  prediction,  such  as  size,  age,  and  churn.  FLUCCS,  our  fault  learn-to-rank  localisation  technique,  employs  both  existing  SBFL  formulae  and  these  metrics  as  input.  We  investigate  the  effect  of  employing  code  and  change  metrics  for  fault  localisation  using  four  different  learn-to-rank  techniques:  Genetic  Programming,  Gaussian  Process  Modelling,  Support  Vector  Machine,  and  Random  Forest.  We  evaluate  the  performance  of  FLUCCS  with  386  real  world  faults  collected  from  Defects4J  repository.  The  results  show  that  FLUCCS  with  code  and  change  metrics  places  144  faults  at  the  top  and  304  faults  within  the  top  ten.  This  is  a  significant  improvement  over  the  state-of-art  SBFL  formulae,  which  can  locate  65  and  212  faults  at  the  top  and  within  the  top  ten,  respectively.  We  also  investigate  the  feasibility  of  cross-project  transfer  learning  of  fault  localisation.  The  results  show  that,  while  there  exist  project-specific  properties  that  can  be  exploited  for  better  localisation  per  project,  ranking  models  learnt  from  one  project  can  be  applied  to  others  without  significant  loss  of  effectiveness.
1	Migrating  load  testing  to  the  cloud  a  case  study.  Cloud  computing  has  emerged  as  a  new  paradigm  for  the  delivery  of  computing  resources.  It  brings  great  opportunities  to  software  testing,  especially  to  load  testing.  In  this  paper,  we  focus  on  migrating  conventional  load  testing  tools  to  the  cloud,  for  which  the  two  significant  issues  are  about  multi-tenancy  and  load  simulating  resource  management.  We  propose  a  four  layer  model  for  cloud-based  load  testing,  along  with  the  approach  of  test  request  admission  control  and  scheduling  to  solve  these  issues.  We  carried  out  a  concrete  case  study  on  our  proposed  approach  and  made  the  efficiency  of  cloud-based  load  testing  shown  successfully  by  two  contrast  experiments.
1	Engineering  intrusion  prevention  services  for  iaas  clouds  the  way  of  the  hypervisor.  Strong  user  expectations  for  protecting  their  cloud-hosted  IT  systems  make  enhanced  security  a  key  element  for  cloud  adoption.  This  means  that  cloud  infrastructure  security  should  be  guaranteed,  but  also  that  security  monitoring  services  should  be  correctly  designed  to  protect  the  user  Virtual  Machines  (VMs),  using  Intrusion  Detection  and  Prevention  Services  (IDPS).  This  paper  gives  an  overview  of  available  and  emerging  techniques  for  building  intrusion  monitoring  services,  analyzing  their  ability  to  address  the  VM  protection  requirements  in  a  cloud  context.  While  network-  and  host-based  security  monitoring  are  shown  not  to  be  well  suited  for  the  cloud,  this  paper  makes  a  position  statement,  recommending  a  new  monitoring  approach,  called  hyper  visor-based,  as  an  alternative.  This  approach  benefits  from  virtualization  to  monitor  through  the  hyper  visor,  and  from  outside  the  user  execution  context,  the  security  of  computing,  networking,  and  storage  resources  allocated  to  user  VMs.  Compared  to  traditional  IDPS  designs,  hyper  visor-based  architectures  are  shown  to  be  the  most  promising,  greatly  improving  user  VM  security.  This  analysis  also  highlights  the  privileged  role  of  the  cloud  provider  to  operate  such  type  of  IDPS,  since  it  may  perform  integrated  security  monitoring  as  provider  of  both  infrastructure  and  security  services.
1	Saas  testing  on  clouds  issues  challenges  and  needs.  With  the  fast  advance  of  cloud  computing,  developing  software-as-a-service  (SaaS)  is  becoming  a  hot  topic  in  cloud  computing  and  engineering.  Due  to  new  features  of  SaaS  applications,  SaaS  testing  becomes  an  interesting  hot  topic  for  both  industry  and  research  communities.  Although  there  are  a  number  of  published  papers  discussing  cloud  testing,  there  is  a  lack  of  research  papers  addressing  new  issues,  challenges,  and  needs  in  SaaS  testing.  This  paper  provides  a  tutorial  to  discuss  SaaS  testing,  including  its  concepts,  focuses  and  objective,  test  process,  test  environments,  and  requirements.  Moreover,  the  paper  discusses  the  special  SaaS  features,  and  examines  the  related  issues  and  challenges  and  needs  in  SaaS  testing.
1	Exploring  a  new  security  framework  for  cloud  storage  using  capabilities.  We  are  seeing  the  deployment  of  new  types  of  networks  such  as  sensor  networks  for  environmental  and  infrastructural  monitoring,  social  networks  such  as  facebook,  and  e-Health  networks  for  patient  monitoring.  These  networks  are  producing  large  amounts  of  data  that  need  to  be  stored,  processed  and  analysed.  Cloud  technology  is  being  used  to  meet  these  challenges.  However,  a  key  issue  is  how  to  provide  security  for  data  stored  in  the  Cloud.  This  paper  addresses  this  issue  in  two  ways.  It  first  proposes  a  new  security  framework  for  Cloud  security  which  deals  with  all  the  major  system  entities.  Secondly,  it  introduces  a  Capability  ID  system  based  on  modified  IPv6  addressing  which  can  be  used  to  implement  a  security  framework  for  Cloud  storage.  The  paper  then  shows  how  these  techniques  are  being  used  to  build  an  e-Health  system  for  patient  monitoring.
1	Interface  design  in  cyber  physical  systems  of  systems.  A  Cyber-Physical  System-of-Systems  (CPSoS)  is  a  possibly  huge  information  processing  and  energy  transforming  system.  Its  autonomous  Constituent  Systems  (CSs),  i.e.,  computer  systems,  and  optionally  physical  systems  and  humans,  interact  in  their  common  environment  to  realize  often  mutually  beneficial  emergent  CPSoS  services.  Many  key  challenges  in  understanding,  designing,  and  engineering  such  CPSoSs  are  related  to  the  interactions  of  CSs.  This  paper  conceptualizes  these  interactions  of  CSs,  i.e.,  their  interfaces,  at  three  abstraction  levels:  cyber-physical,  Itom,  and  service.  The  cyber-physical  level  concernes  message-based  interactions  in  cyber  space,  and  stigmergic  interactions  in  the  physical  environment.  The  Itom  level  involves  the  description  of  direct  and  indirect  information  transfers.  The  service  level  helps  to  tackle  challenges  related  to  the  dynamicity  and  evolution  of  CPSoSs.  The  second  part  of  the  paper  discusses  design  aspects  of  Relied  Upon  Interfaces  (RUIs)  which  are  the  interfaces  the  overall  emergent  CPSoS  service  relies  upon.  The  conceptualization  of  interfaces  in  CPSoSs  and  the  proposed  RUI  design  aspects  are  intended  to  simplify  the  modeling  of  CPSoSs  and  to  facilitate  their  deployment  with  existing  Internet-of-Things  and  cloud  technologies.
1	Folksonomy  based  in  depth  annotation  of  web  services.  Semantic  web  service  technologies  have  been  proposed  to  enable  automatic  web  service  discovery  and  composition.  But  such  approaches  are  suffered  from  significant  effort  to  construct  domain  ontologies  and  to  annotate  web  services  with  semantics  by  third  parties.  Hence  social  and  collaborative  tagging  systems  have  been  gaining  the  popularity  on  the  web.  Folksonomy-based  web  service  annotating  is  emerging,  i.e.  to  annotate  web  services  with  semantic  from  community-generated  folksonomies.  This  paper  focuses  on  how  to  provide  folksonomy-based  in-depth  annotation  of  web  services.  Herein,  the  in-depth  means  the  annotation  is  based  on  a  structured  folksonomy,  and  steps  inside  different  parts  of  the  web  services  in  an  automatic  way.  Two  problems  need  addressed:  exploring  semantics  for  the  folksonomy  from  original  tags  of  web  services,  and  automatically  assigning  tags  to  the  different  parts  of  web  services.  The  paper  proposes  an  approach  to  achieving  automatic  tags  assignment  of  web  services  with  a  structured  folksonomy.  Such  in-depth  annotation  facilitates  web  services  discovery  and  composition  by  providing  precise  tagging  of  input,  output  and  etc.  A  case-study  and  result  of  experiments  on  the  pairs  of  tag-service  extracted  from  a  web  service  portal,  seekda,  illustrates  the  effectiveness  of  the  approach.
1	Governance  implications  for  meeting  challenges  in  the  system  of  systems  engineering  field.  This  paper  explores  implications  of  Complex  System  Governance  (CSG)  for  challenges  faced  in  development  of  the  System  of  Systems  Engineering  (SoSE)  field.  The  paper  is  organized  around  four  primary  themes.  First,  the  SoSE  problem  domain  and  issues  in  the  field  are  examined.  Second,  an  emerging  role  of  CSG  is  developed  in  relationship  to  the  SoSE  problem  domain.  While  this  perspective  of  CSG  is  still  in  its  embryonic  stages,  the  potential  for  informing  the  SoSE  field  is  developed  from  the  axiomatic  basis  of  systems  theory  and  management  cybernetics.  Third,  an  organizing  construct  for  research  in  CSG  is  developed  around  seven  different  streams  of  inquiry.  This  development  also  includes  representative  questions  across  the  streams  of  inquiry.  Fourth,  the  paper  concludes  with  an  exploration  of  how  CSG  might  alleviate  problematic  areas  in  the  SoSE  field.
1	Trust  modeling  in  cloud  computing.  Though,  cloud  computing  continues  to  gain  popularity  and  has  enormous  economic  gains,  the  perceived  lack  of  trust  from  end-users  accessing  these  resources  continues  to  hinder  its  full  deployment,  usage  and  adoption.  Tradition  security  and  privacy  controls  continue  to  be  implemented  on  cloud  but  due  to  its  fluid  and  dynamic  nature,  research  work  in  the  area  of  end-user  attestable  trust  evaluation  of  the  cloud  platform  seem  to  be  limited.  With  the  nature  of  cloud,  service  level  agreements  are  not  enough,  the  user  would  want  a  transparent  system  with  a  traceability  facility  that  allows  the  user  to  determine  the  relationship  between  varying  trust  relationships  across  the  cloud  layers,  components,  algorithms  and  applications  especially  at  large  scale.  This  paper  presents  some  security  mechanisms  that  enable  cloud  service  end  users  to  evaluate  the  trust  level  of  various  cloud  services  and  resources.  These  mechanisms  are  evaluated  based  on  fuzzy  theory  on  a  Eucalyptus  cloud  platform.
1	A  cluster  feature  based  approach  for  qos  prediction  in  web  service  recommendation.  With  the  growing  popularity  of  Service-Oriented-Computing  (SOC)  architecture,  the  number  of  Web  services  on  the  internet  is  increasing  rapidly.  When  faced  with  a  large  number  of  candidate  services  with  similar  functionalities,  personalized  Web  service  recommendation  is  becoming  an  important  issue.  Quality-of-Service  (QoS)  is  usually  used  to  characterize  the  non-functional  properties  of  Web  services.  Thus  accurate  QoS  prediction  is  an  important  step  in  the  service  recommendation.  In  this  paper,  we  propose  a  Cluster  Feature  based  Latent  Factor  Model  (CFLFM)  for  QoS  prediction.  First,  we  cluster  users  and  services  into  several  groups  based  on  history  records,  respectively.  We  assume  that  users  or  services  in  the  same  cluster  share  some  latent  features.  By  incorporating  this  kind  of  information,  we  design  an  integrated  latent  factor  model.  Finally,  we  conduct  comprehensive  experiments  on  a  real-world  Web  service  dataset.  The  experimental  results  show  that  our  approach  can  achieve  higher  QoS  prediction  accuracy  than  other  competing  approaches.
1	Ontology  based  collaboration  in  multi  robot  system  approach  and  case  study.  The  paper  propose  an  approach  to  mobile  robots  collaboration  in  cyber-physical  system.  For  robot  collaboration,  the  negotiation  between  them  has  to  be  supported.  To  provide  semantic  interoperability  between  different  robots  they  are  described  in  terms  of  ontologies.  For  information  and  knowledge  exchange  between  cyber-physical  system  resources  a  smart  space  technology  is  used.  It  provides  possibilities  for  different  resources  to  implement  ontology  based  information  and  knowledge  sharing.  Presented  in  the  paper  case  study  aims  to  show  an  example  of  robot  communication  while  implementing  joint  activities.
1	Reliable  consumption  of  web  services  in  a  mobile  cloud  ecosystem  using  rest.  The  evolution  of  the  mobile  landscape  coupled  with  the  ubiquitous  nature  of  the  Internet  and  the  cloud  is  facilitating  the  deployment  of  enterprise  and  personalized  mobile  applications.  In  this  research,  we  proposed  a  proxy-enabled  unification  framework  that  integrates  heterogeneous  devices  with  multiple  SaaS  and  IaaS  cloud  layers  in  order  to  support  personalized  and  group  file  sharing.  However,  our  proposed  mobile-cloud  ecosystem  calls  for  open  research  questions  which  must  be  answered  such  as  i)  how  do  we  synchronize  the  data  across  the  consumer  devices  and  the  multi-IaaS  backend?,  ii)  how  do  we  authenticate  the  system  users?,  and  iii)  how  do  we  push  updates  in  a  low-latency  fashion?  This  paper  addresses  the  three  questions  by  proposing  the  adoption  of  the  REST  Web  Service  as  an  efficient  way  to  consume  the  data  on  the  mobile  devices.  However,  we  have  to  deal  with  the  "CAP  Theorem"  which  states  that  we  can  only  achieve  at  most  two  properties  at  a  time  out  of  the  following  three:  data  consistency,  system/data  availability,  and  partition  tolerance.  Since  partition  tolerance  is  a  given  in  a  distributed  system,  we  opt  for  the  availability  option  by  allowing  file  storage  on  the  consumer  devices  in  both  online  and  offline  modes.  Further,  we  propose  data  consistency  within  a  session  that  enforces  update  propagation  in  a  soft-real  time.  The  architecture  is  evaluated  based  on  latency  and  scalability  using  multi  consumer  devices  and  employed  Drop  box  and  Amazon  S3  as  the  IaaS  cloud  providers.
1	Revisiting  supervised  and  unsupervised  methods  for  effort  aware  cross  project  defect  prediction.  Cross-project  defect  prediction  (CPDP),  aiming  to  apply  defect  prediction  models  built  on  source  projects  to  a  target  project,  has  been  an  active  research  topic.  A  variety  of  supervised  CPDP  methods  and  some  simple  unsupervised  CPDP  methods  have  been  proposed.  In  a  recent  study,  Zhou  et  al.  found  that  simple  unsupervised  CPDP  methods  (i.e.,  ManualDown  and  ManualUp)  have  a  prediction  performance  comparable  or  even  superior  to  complex  supervised  CPDP  methods.  Therefore,  they  suggested  that  the  ManualDown  should  be  treated  as  the  baseline  when  considering  non-effort-aware  performance  measures  (NPMs)  and  the  ManualUp  should  be  treated  as  the  baseline  when  considering  effort-aware  performance  measures  (EPMs)  in  future  CPDP  studies.  However,  in  that  work,  these  unsupervised  methods  are  only  compared  with  existing  supervised  CPDP  methods  in  terms  of  one  or  two  NPMs  and  the  prediction  results  of  baselines  are  directly  collected  from  the  primary  literature.  Besides,  the  comparison  has  not  considered  other  recently  proposed  EPMs,  which  consider  context  switches  and  developer  fatigue  due  to  initial  false  alarms.  These  limitations  may  not  give  a  holistic  comparison  between  the  supervised  methods  and  unsupervised  methods.  In  this  paper,  we  aim  to  revisit  Zhou  et  al.'s  study.  To  the  best  of  our  knowledge,  we  are  the  first  to  make  a  comparison  between  the  existing  supervised  CPDP  methods  and  the  unsupervised  methods  proposed  by  Zhou  et  al.  in  the  same  experimental  setting,  considering  both  NPMs  and  EPMs.  We  also  propose  an  improved  supervised  CPDP  method  EASC  and  make  a  further  comparison  between  this  method  and  the  unsupervised  methods.  According  to  the  results  on  82  projects  in  terms  of  12  performance  measures,  we  find  that  when  considering  NPMs,  EASC  can  achieve  similar  results  with  the  unsupervised  method  ManualDown  without  statistically  significant  difference  in  most  cases.  However,  when  considering  EPMs,  our  proposed  supervised  method  EASC  can  statistically  significantly  outperform  the  unsupervised  method  ManualUp  with  a  large  improvement  in  terms  of  Cliff's  delta  in  most  cases.  Therefore,  the  supervised  CPDP  methods  are  more  promising  than  the  unsupervised  method  in  practical  application  scenarios,  since  the  limitation  of  testing  resource  and  the  impact  on  developers  cannot  be  ignored  in  these  scenarios.
1	Using  forward  snowballing  to  update  systematic  reviews  in  software  engineering.  Background:  A  Systematic  Literature  Review  (SLR)  is  a  methodology  used  to  aggregate  relevant  evidence  related  to  one  or  more  research  questions.  Whenever  new  evidence  is  published  after  the  completion  of  a  SLR,  this  SLR  should  be  updated  in  order  to  preserve  its  value.  However,  updating  SLRs  involves  significant  effort.  Objective:  The  goal  of  this  paper  is  to  investigate  the  application  of  forward  snowballing  to  support  the  update  of  SLRs.  Method:  We  compare  outcomes  of  an  update  achieved  using  the  forward  snowballing  versus  a  published  update  using  the  search-based  approach,  i.e.,  searching  for  studies  in  electronic  databases  using  a  search  string.  Results:  Forward  snowballing  showed  a  higher  precision  and  a  slightly  lower  recall.  It  reduced  in  more  than  five  times  the  number  of  primary  studies  to  filter  however  missed  one  relevant  study.  Conclusions:  Due  to  its  high  precision,  we  believe  that  the  use  of  forward  snowballing  considerably  reduces  the  effort  in  updating  SLRs  in  Software  Engineering;  however  the  risk  of  missing  relevant  papers  should  not  be  underrated.
1	Maintaining  systematic  literature  reviews  benefits  and  drawbacks.  Background:  Maintenance  and  traceability  (versioning)  are  constant  concerns  in  Software  Engineering  (SE),  however,  few  works  related  to  these  topics  in  Systematic  Literature  Reviews  (SLR)  were  found.  Goal:  The  goal  of  this  research  is  to  elucidate  how  SLRs  can  be  maintained  and  what  are  the  benefits  and  drawbacks  in  this  process.  Method:  This  work  presents  a  survey  where  experienced  researchers  that  conducted  SLRs  between  2011  and  2015  answered  questions  about  maintenance  and  traceability  and,  using  software  maintenance  concepts,  it  addresses  the  SLRs  maintenance  process.  From  the  79  e-mails  sent  we  reach  28  answers.  Results:  19  of  surveyed  researchers  have  shown  interest  in  keeping  their  SLRs  up-to-date,  but  they  have  expressed  concerns  about  the  effort  to  be  made  to  accomplish  it.  It  was  also  observed  that  20  participants  would  be  willing  to  share  their  SLRs  in  common  repositories,  such  as  GitHub.  Conclusions:  There  is  a  need  to  perform  maintenance  on  SLRs.  Thus,  we  are  proposing  a  SLR  maintenance  process,  taking  into  account  some  benefits  and  drawbacks  identified  during  our  study  and  presented  through  the  paper.
1	Effect  of  technical  and  social  factors  on  pull  request  quality  for  the  npm  ecosystem.  Background:  Pull  request  (PR)  based  development,  which  is  a  norm  for  the  social  coding  platforms,  entails  the  challenge  of  evaluating  the  contributions  of,  often  unfamiliar,  developers  from  across  the  open  source  ecosystem  and,  conversely,  submitting  a  contribution  to  a  project  with  unfamiliar  maintainers.  Previous  studies  suggest  that  the  decision  of  accepting  or  rejecting  a  PR  may  be  influenced  by  a  diverging  set  of  technical  and  social  factors,  but  often  focus  on  relatively  few  projects,  do  not  consider  ecosystem-wide  measures,  or  the  possible  non-monotonic  relationships  between  the  predictors  and  PR  acceptance  probability.  Aim:  We  aim  to  shed  light  on  this  important  decision  making  process  by  testing  which  measures  significantly  affect  the  probability  of  PR  acceptance  on  a  significant  fraction  of  a  large  ecosystem,  rank  them  by  their  relative  importance  in  predicting  PR  acceptance,  and  determine  the  shape  of  the  functions  that  map  each  predictor  to  PR  acceptance.  Method:  We  proposed  seven  hypotheses  regarding  which  technical  and  social  factors  might  affect  PR  acceptance  and  created  17  measures  based  on  them.  Our  dataset  consisted  of  470,925  PRs  from  3349  popular  NPM  packages  and  79,128  GitHub  users  who  created  those.  We  tested  which  of  the  measures  affect  PR  acceptance  and  ranked  the  significant  measures  by  their  importance  in  a  predictive  model.  Results:  Our  predictive  model  had  and  AUC  of  0.94,  and  15  of  the  17  measures  were  found  to  matter,  including  five  novel  ecosystem-wide  measures.  Measures  describing  the  number  of  PRs  submitted  to  a  repository  and  what  fraction  of  those  get  accepted,  and  signals  about  the  PR  review  phase  were  most  significant.  We  also  discovered  that  only  four  predictors  have  a  linear  influence  on  the  PR  acceptance  probability  while  others  showed  a  more  complicated  response.  Conclusion:  Our  findings  should  be  helpful  for  PR  creators,  integrators,  as  well  as  tool  designers  to  focus  on  the  important  factors  affecting  PR  acceptance.
1	Wrinkle  components  analysis  using  aatcc  smoothness  appearance  replica.  Abstract:  In  order  to  provide  a  fundamental  understanding  of  cloth  surface  smoothness,  the  wrinkle  components  in  theAATCC  standard  replica  were  studied.  Using  an  image  acquisition  system  with  a  line  light  source  and  a  camera,images  of  the  replica  were  obtained.  Four  images  of  each  replication  were  taken  at  intervals  of  90  o  degree  rotations.The  wrinkle  ridges  in  the  image  were  extracted  using  Canny  edge  detection,  while  the  shade  generated  by  the  wrinkleswas  obtained  using  line  threshold  method.  The  ridge  density,  the  distribution  of  ridge-to-ridge  distance  and  the  shadelength  distribution  were  measured  and  calculated.  The  three  factors  measured  showed  a  strong  non-linear  relationshipwith  the  replication  grades.  Keywords:  AATCC  smoothness  appearance  replica,  image  processing,  ridge,  line  threshold,  shadow  area  1.  서  론  삼차원  구조의  섬유집합체인  의류  제품은  멋과  아름다움을  꾸미는  대표적인  패션제품으로,  다양한  물리적,  화학적특성이  요구되며  이를  구현하기  위한  다양한  소재,  공정이개발되고  있다.  이러한  물리적,  화학적  특성들은  지속적인연구개발을  통하여  어느  정도  소비자의  요구에  부합되고있으며  이를  넘어선  기능성에  대한  연구와  IT  기술  등과의융합을  통한  다양한  기능부여를  위한  개발이  진행되고  있다.  이와  더불어  감성특성에  대한  소비자의  요구가  꾸준히증가되고  있다.  특히  의류제품에  필연적으로  발생하는  주름은  제품의  감성특성을  저하시키는  주요  원인으로,  사용과정  혹은  세탁과정에서  발생하는  주름에  대한  개선요구가지속적으로  제기되고  있다.  이러한  주름의  평가방법은  주름이  발생한  시료와  표준주름판(AATCC  smoothnessappearance  replica)을  사람이  육안으로  비교하여  평가하는방법이  일반적으로  사용되고  있다[1,2].  그러나  이러한  주름판을이용한주관적  평가는  평가자의  경험과  직관에  의존도가  높아  평가자마다  혹은  평가시점마다  평가결과가  일정하지  않으며,  표준주름판  역시  주름  정도에  대한  이론적기반도  없어,  실제  주름  정도를  표준화하기에는  부족하다.  이와  같은  문제점을  해결하기  위해  주름을  객관적으로평가하기  위한  여러  가지  연구가  진행이  되어  왔다[3-31].이러한  연구들은  대부분  카메라를  이용하여  얻어진  주름의이미지로부터  다양한  처리를  통하여  유용한  정보를  획득하고  이를  바탕으로  등급을  부여할  수  있는  방법에  집중되고있다.  정확하고  객관적인  평가를  위한  주름의  이미지  분석연구는  시료로부터  오차가  적고  활용에  적합한  이미지를고품질의  이미지를  얻기  위한  다양한  접근에  관한  연구와얻어진  이미지로부터  평가에  직접적으로  활용할  수  있는수치화된  데이터를  추출하는  연구  등  두  분야로  구분된다.  이미지를  획득하기  위하여  초기에  사용된  방법은  시료의측면에서  하나  또는  여러  개의  조명을  이용해  빛을  조사하고  시료의  이미지를  카메라를  사용하여  얻는  방법(angledlighting  and  camera)[3-10],  스캐너를  이용해  주름이  발생한  시료  이미지를  얻는  방법[11-14]  등이  있다.  이러한  방법에서ⓒ  주름  평가는  주름의  높이를  분석하는  대신  주름에
1	The  role  of  method  chains  and  comments  in  software  readability  and  comprehension  an  experiment.  Software  readability  and  comprehension  are  important  factors  in  software  maintenance.  There  is  a  large  body  of  research  on  software  measurement,  but  the  actual  factors  that  make  software  easier  to  read  or  easier  to  comprehend  are  not  well  understood.  In  the  present  study,  we  investigate  the  role  of  method  chains  and  code  comments  in  software  readability  and  comprehension.  Our  analysis  comprises  data  from  104  students  with  varying  programming  experience.  Readability  and  comprehension  were  measured  by  perceived  readability,  reading  time  and  performance  on  a  simple  cloze  test.  Regarding  perceived  readability,  our  results  show  statistically  significant  differences  between  comment  variants,  but  not  between  method  chain  variants.  Regarding  comprehension,  there  are  no  significant  differences  between  method  chain  or  comment  variants.  Student  groups  with  low  and  high  experience,  respectively,  show  significant  differences  in  perceived  readability  and  performance  on  the  cloze  tests.  Our  results  do  not  show  any  significant  relationships  between  perceived  readability  and  the  other  measures  taken  in  the  present  study.  Perceived  readability  might  therefore  be  insufficient  as  the  sole  measure  of  software  readability  or  comprehension.  We  also  did  not  find  any  statistically  significant  relationships  between  size  and  perceived  readability,  reading  time  and  comprehension.
1	Sentinel  a  hyper  heuristic  for  the  generation  of  mutant  reduction  strategies.  Mutation  testing  is  an  effective  approach  to  evaluate  and  strengthen  software  test  suites,  but  its  adoption  is  currently  limited  by  the  mutants'  execution  computational  cost.  Several  strategies  have  been  proposed  to  reduce  this  cost  (a.k.a.  mutation  cost  reduction  strategies),  however  none  of  them  has  proven  to  be  effective  for  all  scenarios  since  they  often  need  an  ad-hoc  manual  selection  and  configuration  depending  on  the  software  under  test  (SUT).  In  this  paper,  we  propose  a  novel  multi-objective  evolutionary  hyper-heuristic  approach,  dubbed  Sentinel,  to  automate  the  generation  of  optimal  cost  reduction  strategies  for  every  new  SUT.  We  evaluate  Sentinel  by  carrying  out  a  thorough  empirical  study  involving  40  releases  of  10  open-source  real-world  software  systems  and  both  baseline  and  state-of-the-art  strategies  as  a  benchmark  for  a  total  of  4,800  experiments,  which  results  are  evaluated  with  both  quality  indicators  and  statistical  significance  tests,  following  the  most  recent  best  practice  in  the  literature.  The  results  show  that  strategies  generated  by  Sentinel  outperform  the  baseline  strategies  in  95%  of  the  cases  always  with  large  effect  sizes,  and  they  also  obtain  statistically  significantly  better  results  than  state-of-the-art  strategies  in  88%  of  the  cases  with  large  effect  sizes  for  95%  of  them.  Also,  our  study  reveals  that  the  mutation  strategies  generated  by  Sentinel  for  a  given  software  version  can  be  used  without  any  loss  in  quality  for  subsequently  developed  versions  in  95%  of  the  cases.  These  results  show  that  Sentinel  is  able  to  automatically  generate  mutation  strategies  that  reduce  mutation  testing  cost  without  affecting  its  testing  effectiveness  (i.e.  mutation  score),  thus  taking  off  from  the  tester's  shoulders  the  burden  of  manually  selecting  and  configuring  strategies  for  each  SUT.
1	Formal  analysis  of  the  probability  of  interaction  fault  detection  using  random  testing.  Modern  systems  are  becoming  highly  configurable  to  satisfy  the  varying  needs  of  customers  and  users.  Software  product  lines  are  hence  becoming  a  common  trend  in  software  development  to  reduce  cost  by  enabling  systematic,  large-scale  reuse.  However,  high  levels  of  configurability  entail  new  challenges.  Some  faults  might  be  revealed  only  if  a  particular  combination  of  features  is  selected  in  the  delivered  products.  But  testing  all  combinations  is  usually  not  feasible  in  practice,  due  to  their  extremely  large  numbers.  Combinatorial  testing  is  a  technique  to  generate  smaller  test  suites  for  which  all  combinations  of  t  features  are  guaranteed  to  be  tested.  In  this  paper,  we  present  several  theorems  describing  the  probability  of  random  testing  to  detect  interaction  faults  and  compare  the  results  to  combinatorial  testing  when  there  are  no  constraints  among  the  features  that  can  be  part  of  a  product.  For  example,  random  testing  becomes  even  more  effective  as  the  number  of  features  increases  and  converges  toward  equal  effectiveness  with  combinatorial  testing.  Given  that  combinatorial  testing  entails  significant  computational  overhead  in  the  presence  of  hundreds  or  thousands  of  features,  the  results  suggest  that  there  are  realistic  scenarios  in  which  random  testing  may  outperform  combinatorial  testing  in  large  systems.  Furthermore,  in  common  situations  where  test  budgets  are  constrained  and  unlike  combinatorial  testing,  random  testing  can  still  provide  minimum  guarantees  on  the  probability  of  fault  detection  at  any  interaction  level.  However,  when  constraints  are  present  among  features,  then  random  testing  can  fare  arbitrarily  worse  than  combinatorial  testing.  As  a  result,  in  order  to  have  a  practical  impact,  future  research  should  focus  on  better  understanding  the  decision  process  to  choose  between  random  testing  and  combinatorial  testing,  and  improve  combinatorial  testing  in  the  presence  of  feature  constraints.
1	Equality  to  equals  and  unequals  a  revisit  of  the  equivalence  and  nonequivalence  criteria  in  class  level  testing  of  object  oriented  software.  Algebraic  specifications  have  been  used  in  the  testing  of  object-oriented  programs  and  received  much  attention  since  the  1990s.  It  is  generally  believed  that  class-level  testing  based  on  algebraic  specifications  involves  two  independent  aspects:  the  testing  of  equivalent  and  nonequivalent  ground  terms.  Researchers  have  cited  intuitive  examples  to  illustrate  the  philosophy  that  even  if  an  implementation  satisfies  all  the  requirements  specified  by  the  equivalence  of  ground  terms,  it  may  still  fail  to  satisfy  some  of  the  requirements  specified  by  the  nonequivalence  of  ground  terms.  Thus,  both  the  testing  of  equivalent  ground  terms  and  the  testing  of  nonequivalent  ground  terms  have  been  considered  as  significant  and  cannot  replace  each  other.  In  this  paper,  we  present  an  innovative  finding  that,  given  any  canonical  specification  of  a  class  with  proper  imports,  a  complete  implementation  satisfies  all  the  observationally  equivalent  ground  terms  if  and  only  if  it  satisfies  all  the  observationally  nonequivalent  ground  terms.  As  a  result,  these  two  aspects  of  software  testing  cover  each  other  and  can  therefore  replace  each  other.  These  findings  provide  a  deeper  understanding  of  software  testing  based  on  algebraic  specifications,  rendering  the  theory  more  elegant  and  complete.  We  also  highlight  a  couple  of  important  practical  implications  of  our  theoretical  results.
1	A  machine  learning  approach  to  software  requirements  prioritization.  Deciding  which,  among  a  set  of  requirements,  are  to  be  considered  first  and  in  which  order  is  a  strategic  process  in  software  development.  This  task  is  commonly  referred  to  as  requirements  prioritization.  This  paper  describes  a  requirements  prioritization  method  called  Case-Based  Ranking  (CBRank),  which  combines  project's  stakeholders  preferences  with  requirements  ordering  approximations  computed  through  machine  learning  techniques,  bringing  promising  advantages.  First,  the  human  effort  to  input  preference  information  can  be  reduced,  while  preserving  the  accuracy  of  the  final  ranking  estimates.  Second,  domain  knowledge  encoded  as  partial  order  relations  defined  over  the  requirement  attributes  can  be  exploited,  thus  supporting  an  adaptive  elicitation  process.  The  techniques  CBRank  rests  on  and  the  associated  prioritization  process  are  detailed.  Empirical  evaluations  of  properties  of  CBRank  are  performed  on  simulated  data  and  compared  with  a  state-of-the-art  prioritization  method,  providing  evidence  of  the  method  ability  to  support  the  management  of  the  tradeoff  between  elicitation  effort  and  ranking  accuracy  and  to  exploit  domain  knowledge.  A  case  study  on  a  real  software  project  complements  these  experimental  measurements.  Finally,  a  positioning  of  CBRank  with  respect  to  state-of-the-art  requirements  prioritization  methods  is  proposed,  together  with  a  discussion  of  benefits  and  limits  of  the  method.
1	Sethesaurus  wordnet  in  software  engineering.  Informal  discussions  on  social  platforms  (e.g.,  Stack  Overflow,  CodeProject)  have  accumulated  a  large  body  of  programming  knowledge  in  the  form  of  natural  language  text.  Natural  language  process  (NLP)  techniques  can  be  utilized  to  harvest  this  knowledge  base  for  software  engineering  tasks.  However,  consistent  vocabulary  for  a  concept  is  essential  to  make  an  effective  use  of  these  NLP  techniques.  Unfortunately,  the  same  concepts  are  often  intentionally  or  accidentally  mentioned  in  many  different  morphological  forms  (such  as  abbreviations,  synonyms  and  misspellings)  in  informal  discussions.  Existing  techniques  to  deal  with  such  morphological  forms  are  either  designed  for  general  English  or  mainly  resort  to  domain-specific  lexical  rules.  A  thesaurus,  which  contains  software-specific  terms  and  commonly-used  morphological  forms,  is  desirable  to  perform  normalization  for  software  engineering  text.  However,  constructing  this  thesaurus  in  a  manual  way  is  a  challenge  task.  In  this  paper,  we  propose  an  automatic  unsupervised  approach  to  build  such  a  thesaurus.  In  particular,  we  first  identify  software-specific  terms  by  utilizing  a  software-specific  corpus  (e.g.,  Stack  Overflow)  and  a  general  corpus  (e.g.,  Wikipedia).  Then  we  infer  morphological  forms  of  software-specific  terms  by  combining  distributed  word  semantics,  domain-specific  lexical  rules  and  transformations.  Finally,  we  perform  graph  analysis  on  morphological  relations.  We  evaluate  the  coverage  and  accuracy  of  our  constructed  thesaurus  against  community-cumulated  lists  of  software-specific  terms,  abbreviations  and  synonyms.  We  also  manually  examine  the  correctness  of  the  identified  abbreviations  and  synonyms  in  our  thesaurus.  We  demonstrate  the  usefulness  of  our  constructed  thesaurus  by  developing  three  applications  and  also  verify  the  generality  of  our  approach  in  constructing  thesauruses  from  data  sources  in  other  domains.
1	How  does  machine  learning  change  software  development  practices.  Adding  an  ability  for  a  system  to  learn  inherently  adds  uncertainty  into  the  system.  Given  the  rising  popularity  of  incorporating  machine  learning  into  systems,  we  wondered  how  the  addition  alters  software  development  practices.  We  performed  a  mixture  of  qualitative  and  quantitative  studies  with  14  interviewees  and  342  survey  respondents  from  26  countries  across  four  continents  to  elicit  significant  differences  between  the  development  of  machine  learning  systems  and  the  development  of  non-machine-learning  systems.  Our  study  uncovers  significant  differences  in  various  aspects  of  software  engineering  (e.g.,  requirements,  design,  testing,  and  process)  and  work  characteristics  (e.g.,  skill  variety,  problem  solving  and  task  identity).  Based  on  our  findings,  we  highlight  future  research  directions  and  provide  recommendations  for  practitioners.
1	Surveys  in  software  engineering  identifying  representative  samples.  Context:  The  representativeness  of  samples  in  Software  Engineering  primary  studies  is  still  a  great  challenge,  mainly  when  identifying  available  sources  for  establishing  adequate  sampling  frames,  characterizing  subjects  and  stimulating  their  participation  in  (opinion)  surveys.  The  lack  of  survey  guidelines  taking  into  account  the  specificities  of  Software  Engineering  increases  the  research  challenge.  Goal:  To  introduce  a  conceptual  framework  for  supporting  the  identification  of  representative  samples  for  surveys  in  Software  Engineering.  Method:  Based  on  knowledge  acquired  in  the  technical  literature  and  researchers'  experience,  to  organize  a  set  of  guidelines  to  systematically  support  sampling  in  Software  Engineering  surveys.  To  perform  in  vitro  empirical  studies  to  observe  and  evolve  the  guidelines.  Results:  An  empirically  evaluated  set  of  planning  activities  and  tasks  with  recommendations  to  support  the  identification  of  representative  samples  for  surveys  in  Software  Engineering  is  available.  Conclusion:  Surveys  have  been  supporting  relevant  investigations  in  Software  Engineering  in  the  last  decades.  This  conceptual  framework  can  contribute  to  strength  representativeness  of  their  results.  However,  some  important  issues  regarding  survey  research  are  still  open  and  deserves  attention  from  the  empirical  Software  Engineering  community.
1	Revisiting  the  size  effect  in  software  fault  prediction  models.  BACKGROUND:  In  object  oriented  (OO)  software  systems,  class  size  has  been  acknowledged  as  having  an  indirect  effect  on  the  relationship  between  certain  artifact  characteristics,  captured  via  metrics,  and  fault-proneness,  and  therefore  it  is  recommended  to  control  for  size  when  designing  fault  prediction  models.AIM:  To  use  robust  statistical  methods  to  assess  whether  there  is  evidence  of  any  true  effect  of  class  size  on  fault  prediction  models.METHOD:  We  examine  the  potential  mediation  and  moderation  effects  of  class  size  on  the  relationships  between  OO  metrics  and  number  of  faults.  We  employ  regression  analysis  and  bootstrapping-based  methods  to  investigate  the  mediation  and  moderation  effects  in  two  widely-used  datasets  comprising  seventeen  systems.RESULTS:  We  find  no  strong  evidence  of  a  significant  mediation  or  moderation  effect  of  class  size  on  the  relationships  between  OO  metrics  and  faults.  In  particular,  size  appears  to  have  a  more  significant  mediation  effect  on  CBO  and  Fan-out  than  other  metrics,  although  the  evidence  is  not  consistent  in  all  examined  systems.  On  the  other  hand,  size  does  appear  to  have  a  significant  moderation  effect  on  WMC  and  CBO  in  most  of  the  systems  examined.  Again,  the  evidence  provided  is  not  consistent  across  all  examined  systemsCONCLUSION:  We  are  unable  to  confirm  if  class  size  has  a  significant  mediation  or  moderation  effect  on  the  relationships  between  OO  metrics  and  the  number  of  faults.  We  contend  that  class  size  does  not  fully  explain  the  relationships  between  OO  metrics  and  the  number  of  faults,  and  it  does  not  always  affect  the  strength/magnitude  of  these  relationships.  We  recommend  that  researchers  consider  the  potential  mediation  and  moderation  effect  of  class  size  when  building  their  prediction  models,  but  this  should  be  examined  independently  for  each  system.
1	Would  you  like  to  motivate  software  testers  ask  them  how.  Context.  Considering  the  importance  of  software  testing  to  the  development  of  high  quality  and  reliable  software  systems,  this  paper  aims  to  investigate  how  can  work-related  factors  influence  the  motivation  of  software  testers.  Method.  We  applied  a  questionnaire  that  was  developed  using  a  previous  theory  of  motivation  and  satisfaction  of  software  engineers  to  conduct  a  survey-based  study  to  explore  and  understand  how  professional  software  testers  perceive  and  value  work-related  factors  that  could  influence  their  motivation  at  work.  Results.  With  a  sample  of  80  software  testers  we  observed  that  software  testers  are  strongly  motivated  by  variety  of  work,  creative  tasks,  recognition  for  their  work,  and  activities  that  allow  them  to  acquire  new  knowledge,  but  in  general  the  social  impact  of  this  activity  has  low  influence  on  their  motivation.  Conclusion.  This  study  discusses  the  difference  of  opinions  among  software  testers,  regarding  work-related  factors  that  could  impact  their  motivation,  which  can  be  relevant  for  managers  and  leaders  in  software  engineering  practice.
1	Effects  of  four  distances  on  communication  processes  in  global  software  projects.  Global  distribution  of  software  engineering  introduces  geographical,  temporal,  cultural  and  organizational  distance  into  teamwork.  Globally  distributed  software  projects  need  to  use  electronic  communication  tools  to  collaborate  across  these  distances.  Communication  media  differ  in  properties  and  capabilities  to  overcome  the  challenges  imposed  by  these  distances.      In  this  paper,  we  examine  the  effects  of  these  four  distances  to  communication  in  software  engineering  projects.  We  use  Media  Synchronicity  Theory  as  a  framework  to  analyze  the  capabilities  of  different  communication  media  to  support  software  engineering  across  distances.  We  report  our  findings  on  the  relationship  between  communication  media  and  distance  from  three  distributed  software  projects.  Based  on  the  results,  we  aim  at  providing  conclusions  on  choosing  communication  media  for  globally  distributed  software  projects.
1	Testability  first.  [Background]  The  pivotal  role  of  testing  in  high-quality  software  production  has  driven  a  significant  effort  in  evaluating  and  assessing  testing  practices.  [Aims]  We  explore  the  state  of  testing  in  a  large  industrial  project  over  an  extended  period.  [Method]  We  study  the  interplay  between  bugs  in  the  project  and  its  test  cases,  and  interview  developers  and  stakeholders  to  uncover  reasons  underpinning  our  observations.  [Results]  We  realized  that  testing  is  not  well  adopted,  and  that  testability(i.e.,  ease  of  testing)  is  low.  We  found  that  developers  tended  to  abandon  writing  tests  when  they  assessed  the  effort  to  be  high.  Frequent  changes  in  requirements  and  pressure  to  add  new  features  also  hindered  developers  from  writing  tests.  [Conclusions]  Regardless  of  the  debates  on  test  first  or  later,  we  hypothesize  that  the  underlying  reasons  for  poor  test  quality  are  rooted  in  a  lack  of  attention  to  testing  early  in  the  development  of  a  software  component,  leading  to  poor  testability  of  the  component.  However,  testability  is  usually  overlooked  in  research  that  studies  the  impact  of  testing  practices,  and  should  be  explicitly  taken  into  account.
1	Petri  nets  in  systems  biology.  Petri  nets  are  used  in  many  areas.  This  article  discusses  the  application  of  Petri  nets  in  systems  biology.  Using  an  example  from  biochemistry,  concepts  for  the  automatic  decomposition  of  biochemical  systems  are  introduced.  The  article  focuses  on  those  concepts  that  fulfill  steady-state  conditions.  Interestingly,  all  the  concepts  are  based  on  minimal,  semi-positive  transition  invariants.  The  article  describes,  which  new  definitions  for  network  decomposition  can  be  derived  and  how  they  can  be  interpreted  in  the  context  of  biology.  This  is  illustrated  with  the  example  of  the  citric  acid  cycle,  for  which  a  new  metabolic  pathway  could  be  predicted  with  the  help  of  such  an  analysis.
1	Empirical  assessment  of  two  approaches  for  specifying  software  product  line  use  case  scenarios.  Modularity  benefits,  including  the  independent  maintenance  and  comprehension  of  individual  modules,  have  been  widely  advocated.  However,  empirical  assessments  to  investigate  those  benefits  have  mostly  focused  on  source  code,  and  thus,  the  relevance  of  modularity  to  earlier  artifacts  is  still  not  so  clear  (such  as  requirements  and  design  models).  In  this  paper,  we  use  a  multimethod  technique,  including  designed  experiments,  to  empirically  evaluate  the  benefits  of  modularity  in  the  context  of  two  approaches  for  specifying  product  line  use  case  scenarios:  PLUSS  and  MSVCM.  The  first  uses  an  annotative  approach  for  specifying  variability,  whereas  the  second  relies  on  aspect-oriented  constructs  for  separating  common  and  variant  scenario  specifications.  After  evaluating  these  approaches  through  the  specifications  of  several  systems,  we  find  out  that  MSVCM  reduces  feature  scattering  and  improves  scenario  cohesion.  These  results  suggest  that  evolving  a  product  line  specification  using  MSVCM  requires  only  localized  changes.  On  the  other  hand,  the  results  of  six  experiments  reveal  that  MSVCM  requires  more  time  to  derive  the  product  line  specifications  and,  contrasting  with  the  modularity  results,  reduces  the  time  to  evolve  a  product  line  specification  only  when  the  subjects  have  been  well  trained  and  are  used  to  the  task  of  evolving  product  line  specifications.
1	Modeling  and  enforcing  secure  object  flows  in  process  driven  soas  an  integrated  model  driven  approach.  In  this  paper,  we  present  an  integrated  model-driven  approach  for  the  specification  and  the  enforcement  of  secure  object  flows  in  process-driven  service-oriented  architectures  (SOA).  In  this  context,  a  secure  object  flow  ensures  the  confidentiality  and  the  integrity  of  important  objects  (such  as  business  contracts  or  electronic  patient  records)  that  are  passed  between  different  participants  in  SOA-based  business  processes.  We  specify  a  formal  and  generic  metamodel  for  secure  object  flows  that  can  be  used  to  extend  arbitrary  process  modeling  languages.  To  demonstrate  our  approach,  we  present  a  UML  extension  for  secure  object  flows.  Moreover,  we  describe  how  platform-independent  models  are  mapped  to  platform-specific  software  artifacts  via  automated  model  transformations.  In  addition,  we  give  a  detailed  description  of  how  we  integrated  our  approach  with  the  Eclipse  modeling  tools.
1	Would  sociable  software  engineers  observe  better.  Quantitative  studies  in  Software  Engineering  are  frequently  dependent  on  primary  studies  in  which  population  is  usually  small  and  established  by  convenience.  It  brings  several  limitations  for  the  analysis  and  strength  of  results  due  sampling  issues.  Therefore,  when  these  studies  are  reapplied,  different  and  non-clustered  populations  are  established,  making  unfeasible  evidence  generalization  and  contributing  for  an  imbalance  between  research  and  practice.  Aiming  at  investigating  ways  to  overcome  the  absence  of  large  sampling  frames  in  Software  Engineering  studies,  this  short  paper  presents  the  results  of  an  initial  experience  concerned  with  the  systematic  recruitment  of  subjects  for  a  survey  regarding  software  requirements  effort  factors  by  using  social  networks  compared  with  recruitment  by  convenience.  We  have  observed  in  this  particular  case  that  using  social  networks  technology  does  not  guarantee  sample  enlargement  by  just  posting  invitations  in  specific  forums.  However,  its  usage  can  contribute  to  increase  the  subjects'  heterogeneity  and  to  increase  the  level  of  confidence  of  the  sample,  which  consequently  improve  our  capacity  of  observing  the  object  under  study,  with  the  probable  strengthen  of  results.
1	How  are  discussions  associated  with  bug  reworking  an  empirical  study  on  open  source  projects.  Background:  Bug  fixing  is  one  major  activity  in  software  maintenance  to  solve  unexpected  errors  or  crashes  of  software  systems.  However,  a  bug  fix  can  also  be  incomplete  and  even  introduce  new  bugs.  In  such  cases,  extra  effort  is  needed  to  rework  the  bug  fix.  The  reworking  requires  to  inspect  the  problem  again,  and  perform  the  code  change  and  verification  when  necessary.  Discussions  throughout  the  bug  fixing  process  are  important  to  clarify  the  reported  problem  and  reach  a  solution.  Aims:  In  this  paper,  we  explore  how  discussions  during  the  initial  bug  fix  period  (i.e.,  before  the  bug  reworking  occurs)  associate  with  future  bug  reworking.  We  focus  on  two  types  of  "reworked  bug  fixes":  1)  the  initial  bug  fix  made  in  a  re-opened  bug  report;  and  2)  the  initially  submitted  patch  if  multiple  patches  are  submitted  for  a  single  bug  report.  Method:  We  perform  a  case  study  using  five  open  source  projects  (i.e.,  Linux,  Firefox,  PDE,  Ant  and  HTTP).  The  discussions  are  studied  from  six  perspectives  (i.e.,  duration,  number  of  comments,  dispersion,  frequency,  number  of  developers  and  experience  of  developers).  Furthermore,  we  extract  topics  of  discussions  using  Latent  Dirichlet  Allocation  (LDA).  Results:  We  find  that  the  occurrence  of  bug  reworking  is  associated  with  various  perspectives  of  discussions.  Moreover,  discussions  on  some  topics  (e.g.,  code  inspection  and  code  testing)  can  decrease  the  frequency  of  bug  reworking.  Conclusions:  The  discussions  during  the  initial  bug  fix  period  may  serve  as  an  early  indicator  of  what  bug  fixes  are  more  likely  to  be  reworked.
1	Characterizing  software  engineering  work  with  personas  based  on  knowledge  worker  actions.  Mistaking  versatility  for  universal  skills,  some  companies  tend  to  categorize  all  software  engineers  the  same  not  knowing  a  difference  exists.  For  example,  a  company  may  select  one  of  many  software  engineers  to  complete  a  task,  later  finding  that  the  engineer's  skills  and  style  do  not  match  those  needed  to  successfully  complete  that  task.  This  can  result  in  delayed  task  completion  and  demonstrates  that  a  one-size  fits  all  concept  should  not  apply  to  how  software  engineers  work.  In  order  to  gain  a  comprehensive  understanding  of  different  software  engineers  and  their  working  styles  we  interviewed  21  participants  and  surveyed  868  software  engineers  at  a  large  software  company  and  asked  them  about  their  work  in  terms  of  knowledge  worker  actions.  We  identify  how  tasks,  collaboration  styles,  and  perspectives  of  autonomy  can  significantly  effect  different  approaches  to  software  engineering  work.  To  characterize  differences,  we  describe  empirically  informed  personas  on  how  they  work.  Our  defined  software  engineering  personas  include  those  with  focused  debugging  abilities,  engineers  with  an  active  interest  in  learning,  experienced  advisors  who  serve  as  experts  in  their  role,  and  more.  Our  study  and  results  serve  as  a  resource  for  building  products,  services,  and  tools  around  these  software  engineering  personas.
1	Building  an  ensemble  for  software  defect  prediction  based  on  diversity  selection.  Background:  Ensemble  techniques  have  gained  attention  in  various  scientific  fields.  Defect  prediction  researchers  have  investigated  many  state-of-the-art  ensemble  models  and  concluded  that  in  many  cases  these  outperform  standard  single  classifier  techniques.  Almost  all  previous  work  using  ensemble  techniques  in  defect  prediction  rely  on  the  majority  voting  scheme  for  combining  prediction  outputs,  and  on  the  implicit  diversity  among  single  classifiers.  Aim:  Investigate  whether  defect  prediction  can  be  improved  using  an  explicit  diversity  technique  with  stacking  ensemble,  given  the  fact  that  different  classifiers  identify  different  sets  of  defects.  Method:  We  used  classifiers  from  four  different  families  and  the  weighted  accuracy  diversity  (WAD)  technique  to  exploit  diversity  amongst  classifiers.  To  combine  individual  predictions,  we  used  the  stacking  ensemble  technique.  We  used  state-of-the-art  knowledge  in  software  defect  prediction  to  build  our  ensemble  models,  and  tested  their  prediction  abilities  against  8  publicly  available  data  sets.  Conclusion:  The  results  show  performance  improvement  using  stacking  ensembles  compared  to  other  defect  prediction  models.  Diversity  amongst  classifiers  used  for  building  ensembles  is  essential  to  achieving  these  performance  improvements.
1	Real  challenges  in  mobile  app  development.  Context:  Mobile  app  development  is  a  relatively  new  phenomenon  that  is  increasing  rapidly  due  to  the  ubiquity  and  popularity  of  smartphones  among  end-users.  Objective:  The  goal  of  our  study  is  to  gain  an  understanding  of  the  main  challenges  developers  face  in  practice  when  they  build  apps  for  different  mobile  devices.  Method:  We  conducted  a  qualitative  study,  following  a  Grounded  Theory  approach,  in  which  we  interviewed  12  senior  mobile  developers  from  9  different  companies,  followed  by  a  semi-structured  survey,  with  188  respondents  from  the  mobile  development  community.  Results:  The  outcome  is  an  overview  of  the  current  challenges  faced  by  mobile  developers  in  practice,  such  as  developing  apps  across  multiple  platforms,  lack  of  robust  monitoring,  analysis,  and  testing  tools,  and  emulators  that  are  slow  or  miss  many  features  of  mobile  devices.  Conclusion:  Based  on  our  findings  of  the  current  practices  and  challenges,  we  highlight  areas  that  require  more  attention  from  the  research  and  development  community.
1	A  scalable  and  efficient  approach  for  compiling  and  analyzing  commit  history.  Background:  Researchers  oftentimes  measure  quality  metrics  only  in  the  changed  files  when  analyzing  software  evolution  over  commit-history.  This  approach  is  not  suitable  for  compilation  and  using  program  analysis  techniques  that  require  byte-code.  At  the  same  time,  compiling  the  whole  software  not  only  is  costly  but  may  also  leave  us  with  many  uncompilable  and  unanalyzed  revisions.  Aims:  We  intend  to  demonstrate  if  analyzing  changes  in  a  module  results  in  achieving  a  high  compilation  ratio  and  a  better  understanding  of  software  quality  evolution.  Method:  We  conduct  a  large-scale  multi-perspective  empirical  study  on  37838  distinct  revisions  of  the  core  module  of  68  systems  across  Apache,  Google,  and  Netflix  to  assess  their  compilability  and  identify  when  the  software  is  uncompilable  as  a  result  of  a  developer's  fault.  We  study  the  characteristics  of  uncompilable  revisions  and  analyze  compilable  ones  to  understand  the  impact  of  developers  on  software  quality.  Results:  We  achieve  high  compilation  ratios:  98.4%  for  Apache,  99.0%  for  Google,  and  94.3%  for  Netflix.  We  identify  303  sequences  of  uncompile  commits  and  create  a  model  to  predict  uncompilability  based  on  commit  metadata  with  an  F1-score  of  0.89  and  an  AUC  of  0.96.  We  identify  statistical  differences  between  the  impact  of  affiliated  and  external  developers  of  organizations.  Conclusions:  Focusing  on  a  module  results  in  a  more  complete  and  accurate  software  evolution  analysis,  reduces  the  cost  and  complexity,  and  facilitates  manual  inspection.
1	Case  survey  studies  in  software  engineering  research.  Background:  Given  the  social  aspects  of  Software  Engineering  (SE),  in  the  last  twenty  years,  researchers  from  the  field  started  using  research  methods  common  in  social  sciences  such  as  case  study,  ethnography,  and  grounded  theory.  More  recently,  case  survey,  another  imported  research  method,  has  seen  its  increasing  use  in  SE  studies.  It  is  based  on  existing  case  studies  reported  in  the  literature  and  intends  to  harness  the  generalizability  of  survey  and  the  depth  of  case  study.  However,  little  is  known  on  how  case  survey  has  been  applied  in  SE  research,  let  alone  guidelines  on  how  to  employ  it  properly.  Aims:  This  article  aims  to  provide  a  better  understanding  of  how  case  survey  has  been  applied  in  Software  Engineering  research.  Method:  To  address  this  knowledge  gap,  we  performed  a  systematic  mapping  study  and  analyzed  12  Software  Engineering  studies  that  used  the  case  survey  method.  Results:  Our  findings  show  that  these  studies  presented  a  heterogeneous  understanding  of  the  approach  ranging  from  secondary  studies  to  primary  inquiries  focused  on  a  large  number  of  instances  of  a  research  phenomenon.  They  have  not  applied  the  case  survey  method  consistently  as  defined  in  the  seminal  methodological  papers.  Conclusions:  We  conclude  that  a  set  of  clearly  defined  guidelines  are  needed  on  how  to  use  case  survey  in  SE  research,  to  ensure  the  quality  of  the  studies  employing  this  approach  and  to  provide  a  set  of  clearly  defined  criteria  to  evaluate  such  work.
1	A  memory  deduplication  approach  based  on  group  in  virtualized  environments.  The  combination  of  cloud  computing  and  virtualization  technology  introduces  a  new  pattern  on  resource  allocation  and  utilization.  Memory  scanning  deduplication  techniques  based  on  eliminating  duplicated  pages  among  virtual  machines  can  promote  the  resource  utilization,  and  decrease  the  total  cost  of  ownership.  However,  the  existing  memory  deduplication  technologies  lack  the  supporting  of  isolation  and  trustworthiness  mechanism.  This  paper  proposes  a  memory  sharing  mechanism  based  on  user  groups.  This  mechanism  guarantees  isolation  between  the  different  users  on  the  same  host.  In  addition,  we  designed  a  sampling  hash  algorithm  to  make  the  memory  scanning  process  more  efficient.  We  have  implemented  our  approach  in  Linux  by  modifying  the  KSM  scanning  mechanism  and  splitting  the  global  ksmd  thread  into  per-group  ksmds.  The  experiment  results  show  the  work  can  optimize  the  memory-intensive  VMs,  and  efficiently  accelerate  the  memory  scanning  process.
1	Managing  cyber  security  risks  in  industrial  control  systems  with  game  theory  and  viable  system  modelling.  Cyber  security  risk  management  in  Industrial  Control  Systems  has  been  a  challenging  problem  for  both  practitioners  and  the  research  community.  Their  proprietary  nature  along  with  the  complexity  of  those  systems  renders  traditional  approaches  rather  insufficient  and  creating  the  need  for  the  adoption  of  a  holistic  point  of  view.  This  paper  draws  upon  the  principles  of  the  Viable  System  Model  and  Game  Theory  in  order  to  present  a  novel  systemic  approach  towards  cyber  security  management  in  this  field,  taking  into  account  the  complex  inter-dependencies  and  providing  cost-efficient  defence  solutions.
1	A  model  based  safety  architecture  framework  for  dutch  high  speed  train  lines.  This  paper  presents  a  model-based  safety  architecture  framework  (MBSAF)  for  capturing  and  sharing  architectural  knowledge  of  safety  cases  of  safety-critical  systems  of  systems  (SoS).  Whilst  architecture  frameworks  in  the  systems  engineering  domain  consider  safety  often  as  dependent  attribute,  this  study  focusses  specifically  on  sharing  architectural  knowledge  of  safety  cases  between  stakeholders  and  managing  safety  in  systems  development.  For  this  purpose,  we  adapt  the  A3  architecture  overview  (A3AO)  tool.  The  application  is  shown  though  the  case  study  of  Dutch  high  speed  train  lines  and  shows  how  to  derive  requirements  from  various  stakeholders  by  carrying  out  iterative  validations  of  the  A3AOs.  The  implemented  technique  consists  of  systems  modeling  language-based  (SysML)  diagrams.  Outcomes  of  the  assessment  lead  to  guidelines  for  two  A3AOs.  This  results  in  increasing  and  effective  interaction  between  stakeholders,  more  overview  for  managing  safety  complexity,  more  insight  into  finding  required  safety  information,  and  therefore;  an  increasing  efficiency  in  safety  engineering.
1	Release  readiness  classification  an  explorative  case  study.  Context:  To  survive  in  a  highly  competitive  software  market,  product  managers  are  striving  for  frequent,  incremental  releases  in  ever  shorter  cycles.  Release  decisions  are  characterized  by  high  complexity  and  have  a  high  impact  on  project  success.  Under  such  conditions,  using  the  experience  from  past  releases  could  help  product  managers  to  take  more  informed  decisions.      Goal  and  research  objectives:  To  make  decisions  about  when  to  make  a  release  more  operational,  we  formulated  release  readiness  (RR)  as  a  binary  classification  problem.  The  goal  of  our  research  presented  in  this  paper  is  twofold:  (i)  to  propose  a  machine  learning  approach  called  RC*  (Release  readiness  Classification  applying  predictive  techniques)  with  two  approaches  for  defining  the  training  set  called  incremental  and  sliding  window,  and  (ii)  to  empirically  evaluate  the  applicability  of  RC*  for  varying  project  characteristics.      Methodology:  In  the  form  of  explorative  case  study  research,  we  applied  the  RC*  method  to  four  OSS  projects  under  the  Apache  Software  Foundation.  We  retrospectively  covered  a  period  of  82  months,  90  releases  and  3722  issues.  We  use  Random  Forest  as  the  classification  technique  along  with  eight  independent  variables  to  classify  release  readiness  in  individual  weeks.  Predictive  performance  was  measured  in  terms  of  precision,  recall,  F-measure,  and  accuracy.      Results:  The  incremental  and  sliding  window  approaches  respectively  achieve  an  overall  76%  and  79%  accuracy  in  classifying  RR  for  four  analyzed  projects.  Incremental  approach  outperforms  sliding  window  approach  in  terms  of  stability  of  the  predictive  performance.  Predictive  performance  for  both  approaches  are  significantly  influenced  by  three  project  characteristics  i)  release  duration,  ii)  number  of  issues  in  a  release,  iii)  size  of  the  initial  training  dataset.      Conclusion:  As  our  initial  observation  we  identified,  incremental  approach  achieves  higher  accuracy  when  releases  have  long  duration,  low  number  of  issues  and  classifiers  are  trained  with  large  training  set.  On  the  other  hand,  sliding  window  approach  achieves  higher  accuracy  when  releases  have  short  duration  and  classifiers  are  trained  with  small  training  set.
1	Automatic  checking  of  conformance  to  requirement  boilerplates  via  text  chunking  an  industrial  case  study.  Context.  Boilerplates  have  long  been  used  in  Requirements  Engineering  (RE)  to  increase  the  precision  of  natural  language  requirements  and  to  avoid  ambiguity  problems  caused  by  unrestricted  natural  language.  When  boilerplates  are  used,  an  important  quality  assurance  task  is  to  verify  that  the  requirements  indeed  conform  to  the  boilerplates.  Objective.  If  done  manually,  checking  conformance  to  boilerplates  is  laborious,  presenting  a  particular  challenge  when  the  task  has  to  be  repeated  multiple  times  in  response  to  requirements  changes.  Our  objective  is  to  provide  automation  for  checking  conformance  to  boilerplates  using  a  Natural  Language  Processing  (NLP)  technique,  called  Text  Chunking,  and  to  empirically  validate  the  effectiveness  of  the  automation.  Method.  We  use  an  exploratory  case  study,  conducted  in  an  industrial  setting,  as  the  basis  for  our  empirical  investigation.  Results.  We  present  a  generalizable  and  tool-supported  approach  for  boilerplate  conformance  checking.  We  report  on  the  application  of  our  approach  to  the  requirements  document  for  a  major  software  component  in  the  satellite  domain.  We  compare  alternative  text  chunking  solutions  and  argue  about  their  effectiveness  for  boilerplate  conformance  checking.  Conclusion.  Our  results  indicate  that:  (1)  text  chunking  provides  a  robust  and  accurate  basis  for  checking  conformance  to  boilerplates,  and  (2)  the  effectiveness  of  boilerplate  conformance  checking  based  on  text  chunking  is  not  compromised  even  when  the  requirements  glossary  terms  are  unknown.  This  makes  our  work  particularly  relevant  to  practice,  as  many  industrial  requirements  documents  have  incomplete  glossaries.
1	Combinatorial  testing  tool  learnability  in  an  industrial  environment.  [Context]  Numerous  combinatorial  testing  techniques  are  available  for  generating  test  cases.  However,  many  of  them  are  never  used  in  practice.  [Objective]  Considering  that  learn  ability  plays  a  vital  role  in  initial  adoption  or  rejection  of  a  technology,  in  this  paper  we  aim  to  investigate  the  learnability  of  a  combinatorial  testing  tool  in  an  industrial  environment.  [Method]  A  case  study  research  method  was  designed  and  conducted,  by  including  i)  the  definition  of  learnability  measures  for  test  cases  models  built  using  a  combinatorial  testing  tool.  ii)  A  training  program  was  also  implemented.  iii)  Qualitative  and  quantitative  evaluation  based  on  a  three-level  strategy  was  carried  out  (Reaction,  Learning,  and  Performance).  [Results]  At  the  first  level,  the  tool  was  perceived  as  easy  to  learn  by  the  trainees  (from  a  five-point  ordinal  scale).  However,  at  the  second  level,  during  hands-on  learning,  it  changed  slightly:  According  to  the  working  diaries,  there  were  major  difficulties.  At  third  level,  analyzing  the  learning  curve  of  each  trainee,  we  observe  that  semantic  errors  made  per  each  subject  were  reduced  slightly  over  the  time.
1	Linear  evolution  of  domain  architecture  in  service  oriented  software  product  lines.  In  service-oriented  software  product  lines,  when  a  change  occurs  in  the  business  process  variability  model,  designing  the  domain  architecture  from  scratch  imposes  costs  of  re-architecture,  high  costs  of  change  in  the  domain-level  assets;  and  high  costs  of  change  in  many  products  based  on  the  new  domain  architecture.  In  this  paper,  focusing  on  the  linear  evolution  scenario  in  service-oriented  product  lines,  which  refers  to  propagating  changes  in  some  assets  to  some  other  assets,  both  in  the  domain  level,  we  deal  with  the  problem  of  propagating  changes  in  domain  requirements  (the  business  process  variability  model)  to  the  domain  architecture  level,  in  a  cost-optimal  and  consistency-preserving  way.  We  present  a  method  to  suggest  the  optimal  change  propagation  options  to  reach  the  aforementioned  goals.  The  method  showed  promising  to  provide  minimal  change  costs  as  well  as  to  fully  preserve  consistency  of  the  target  models  if  no  human  intervention  exists  in  the  change  propagation  results.
1	Relating  modal  refinements  covariant  contravariant  simulations  and  partial  bisimulations.  This  paper  studies  the  relationships  between  three  notions  of  behavioural  preorder  that  have  been  proposed  in  the  literature:  refinement  over  modal  transition  systems,  and  the  covariant-contravariant  simulation  and  the  partial  bisimulation  preorders  over  labelled  transition  systems.  It  is  shown  that  there  are  mutual  translations  between  modal  transition  systems  and  labelled  transition  systems  that  preserve,  and  reflect,  refinement  and  the  covariant-contravariant  simulation  preorder.  The  translations  are  also  shown  to  preserve  the  modal  properties  that  can  be  expressed  in  the  logics  that  characterize  those  preorders.  A  translation  from  labelled  transition  systems  modulo  the  partial  bisimulation  preorder  into  the  same  model  modulo  the  covariant-contravariant  simulation  preorder  is  also  offered,  together  with  some  evidence  that  the  former  model  is  less  expressive  than  the  latter.  In  order  to  gain  more  insight  into  the  relationships  between  modal  transition  systems  modulo  refinement  and  labelled  transition  systems  modulo  the  covariant-contravariant  simulation  preorder,  their  connections  are  also  phrased  and  studied  in  the  context  of  institutions.
1	Conservative  reasoning  about  the  probability  of  failure  on  demand  of  a  1  out  of  2  software  based  system  in  which  one  channel  is  possibly  perfect.  In  earlier  work,  [11]  (henceforth  LR),  an  analysis  was  presented  of  a  1-out-of-2  software-based  system  in  which  one  channel  was  “possibly  perfect”.  It  was  shown  that,  at  the  aleatory  level,  the  system  pfd  (probability  of  failure  on  demand)  could  be  bounded  above  by  the  product  of  the  pfd  of  channel  A  and  the  pnp  (probability  of  nonperfection)  of  channel  B.  This  result  was  presented  as  a  way  of  avoiding  the  well-known  difficulty  that  for  two  certainly-fallible  channels,  failures  of  the  two  will  be  dependent,  i.e.,  the  system  pfd  cannot  be  expressed  simply  as  a  product  of  the  channel  pfds.  A  price  paid  in  this  new  approach  for  avoiding  the  issue  of  failure  dependence  is  that  the  result  is  conservative.  Furthermore,  a  complete  analysis  requires  that  account  be  taken  of  epistemic  uncertainty-here  concerning  the  numeric  values  of  the  two  parameters  pfdA  and  pnpB.  Unfortunately  this  introduces  a  different  difficult  problem  of  dependence:  estimating  the  dependence  between  an  assessor's  beliefs  about  the  parameters.  The  work  reported  here  avoids  this  problem  by  obtaining  results  that  require  only  an  assessor's  marginal  beliefs  about  the  individual  channels,  i.e.,  they  do  not  require  knowledge  of  the  dependence  between  these  beliefs.  The  price  paid  is  further  conservatism  in  the  results.
1	Analyzing  mutable  checkpointing  via  invariants.  The  well-known  coordinated  snapshot  algorithm  of  mutable  checkpointing  [7,8,9]  is  studied.  We  equip  it  with  a  concise  formal  model  and  analyze  its  operational  behavior  via  an  invariant  characterizing  the  snapshot  computation.  By  this  we  obtain  a  clear  understanding  of  the  intermediate  behavior  and  a  correctness  proof  of  the  final  snapshot  based  on  a  strong  notion  of  consistency  (reachability  within  the  partial  order  representing  the  underlying  computation).  The  formal  model  further  enables  a  comparison  with  the  blocking  queue  algorithm  [13]  introduced  for  the  same  scenario  and  with  the  same  objective.
1	Enterprise  modelling  languages.  In  enterprise  modelling,  a  wide  range  of  models  and  languages  is  used  to  support  different  purposes.  If  left  uncontrolled,  this  variety  of  models  and  languages  can  easily  result  in  fragmented  perspective  on  an  enterprise,  its  processes  and  IT  support.  A  traditional  approach  to  address  this  problem  is  to  create  standard  modelling  languages  that  unify  and  integrate  different  modelling  perspectives,  such  as  e.g.  UML,  BPMN,  and  ArchiMate.  However,  one  can  observe  how,  in  actual  use,  the  ‘standardising’  and  ‘integrating’  effect  of  these  languages  erodes.  This  is  typically  manifested  by  the  emergence  of  ‘variants’,  ‘light  weight  versions’,  and  extensions  of  the  standard  dealing  with  ‘missing  aspects’.  The  empirical  data  suggests  that  these  ‘variants’  emerge  to  compensate  the  inability  of  a  standard  language  to  aptly  fit  the  needs  of  specific  modelling  situations.  In  this  paper,  we  reconsider  the  drivers  and  strategies  of  modelling  language  standardisation.  Relying  on  an  ongoing  research,  the  paper  develops  a  fundamental  understanding  of  the  role  of  fixed  language  in  the  context  of  conceptual  and  enterprise  modelling.  This  understanding  is  then  used  to  analyse  the  ‘variants’  in  the  actual  use  of  a  standard  process  modelling  language,  and  to  discuss  the  potential  insights  towards  its  standardisation  strategy.
1	Creating  a  business  case  from  a  business  model.  Intuitively,  business  cases  and  business  models  are  closely  connected.  However,  a  thorough  literature  review  revealed  no  research  on  the  combination  of  them.  Besides  that,  little  is  written  on  the  evaluation  of  business  models  at  all.  This  makes  it  difficult  to  compare  different  business  model  alternatives  and  choose  the  best  one.  In  this  article,  we  develop  a  business  case  method  to  objectively  compare  business  models.  It  is  an  eight-step  method,  starting  with  business  drivers  and  ending  with  an  implementation  plan.  We  demonstrate  the  method  with  a  case  study  for  innovations  at  housing  associations.  The  designed  business  case  method  can  be  used  to  compare  and  select  the  best  business  model  successfully.  In  doing  so,  the  business  case  method  increases  the  quality  of  the  decision  making  process  when  choosing  from  possible  business  models
1	Defining  risk  states  in  autonomous  road  vehicles.  The  more  highly  automated  road  vehicles  become,  the  more  complex  get  their  control  systems  along  with  the  taskto  identify  and  reach  the  safest  possible  state  at  an  acceptablyhigh  probability  in  as  many  operational  situations  as  possible.  In  this  paper,  we  outline  a  modeling  formalism  for  hazard-mitigating  controllers  capable  of  run-time  hazard  identificationand  mitigation.  This  formalism  equips  the  safety  engineer  withan  incremental  approach  to  (i)  identify  hazard  casual  factors  andwhole  endangerment  scenarios  and  (ii)  derive  operational  strategies  for  mitigating  these  scenarios.  We  exemplify  the  conceptionof  fail-safe  control  strategies  as  well  as  the  allocation  of  suchstrategies  to  a  control  system  architecture.
1	Introducing  meta  requirements  for  describing  system  of  systems.  Complex,  evolutionary  systems  operating  in  an  open  world  can  be  seen  as  a  composition  of  components  which  interact  each  other  in  order  to  fulfill  their  requirements.  Following  this  vision,  Systems  of  Systems  (SoSs)  literature  aims  at  supporting  the  life  of  such  complex  systems  taking  into  account  key  viewpoints  such  as  emergence,  time,  mobility,  evolution,  dynamicity.  Although  different  attempts  can  be  found  in  the  literature  to  address  mostly  specific  viewpoints  separately,  it  is  still  missing  a  unifying  approach  to  analyze  the  whole  set  of  viewpoints  and  their  relationships,  based  on  the  identification  of  meta-requirements  that  can  be  exploited  to  describe  any  System  of  Systems  (SoS).  To  this  end,  we  developed  a  unifying  meta-requirements  model  to  describe  SoSs  viewpoints  and  relate  them.  The  model  is  meant  to  be  used  to  support  the  derivation  of  the  requirements  for  any  SoS.  This  paper  introduces  the  problem,  and  presents  the  main  notions  of  the  meta-requirements  model  with  the  support  of  a  domain-specific  scenario.
1	Multi  objective  optimization  of  formal  specifications.  Even  in  the  domain  of  safety  critical  systems,  safety  and  reliability  are  not  the  only  goals  and  a  developing  engineer  is  faced  with  the  problem  to  find  good  compromises  wrt.  other  antagonistic  objectives,  in  particular  economic  aspects  of  a  system.  Thus  there  does  not  exist  a  single  optimal  design  variant  of  a  system  but  only  compromises  each  "best"  in  its  own  rights.  With  the  rising  complexity,  especially  of  cyber-physical  systems,  the  process  of  manually  finding  best  compromises  becomes  even  more  difficult.  To  cope  with  this  problem,  we  propose  a  model-based  optimization  approach  which  uses  quantitative  model-based  safety  analysis.  While  the  general  approach  is  tool-independent,  we  implement  it  technically  by  introducing  well  defined  variation  points  to  a  formal  system  model.  These  allow  enough  variability  to  cover  whole  families  of  systems  while  still  being  rigorous  enough  for  formal  analysis.  From  the  specification  of  this  family  of  system  variants  and  a  set  of  objective  functions,  we  compute  Pareto  optimal  sets,  which  represent  best  compromises.  In  this  paper  we  present  a  framework  which  allows  for  optimization  of  arbitrary  quantitative  goal  functions,  in  particular  probabilistic  temporal  logic  properties  used  for  model-based  safety  analysis.  Nevertheless,  the  approach  itself  is  well  applicable  to  other  domains.
1	Combining  goal  models  expert  elicitation  and  probabilistic  simulation  for  qualification  of  new  technology.  New  technologies  typically  involve  innovative  aspects  that  are  not  addressed  by  the  existing  normative  standards  and  hence  are  not  assessable  through  common  certification  procedures.  To  ensure  that  new  technologies  can  be  implemented  in  a  safe  and  reliable  manner,  a  specific  kind  of  assessment  is  performed,  which  in  many  industries,  e.g.,  the  energy  sector,  is  known  as  Technology  Qualification  (TQ).  TQ  aims  at  demonstrating  with  an  acceptable  level  of  confidence  that  a  new  technology  will  function  within  specified  limits.  Expert  opinion  plays  an  important  role  in  TQ,  both  to  identify  the  safety  and  reliability  evidence  that  needs  to  be  developed,  and  to  interpret  the  evidence  provided.  Hence,  it  is  crucial  to  apply  a  systematic  process  for  eliciting  expert  opinions,  and  to  use  the  opinions  for  measuring  the  satisfaction  of  a  technology's  safety  and  reliability  objectives.  In  this  paper,  drawing  on  the  concept  of  assurance  cases,  we  propose  a  goal-based  approach  for  TQ.  The  approach,  which  is  supported  by  a  software  tool,  enables  analysts  to  quantitatively  reason  about  the  satisfaction  of  a  technology's  overall  goals  and  further  to  identify  the  aspects  that  must  be  improved  to  increase  goal  satisfaction.  The  three  main  components  enabling  quantitative  assessment  are  goal  models,  expert  elicitation,  and  probabilistic  simulation.  We  report  on  an  industrial  pilot  study  where  we  apply  our  approach  for  assessing  a  new  offshore  technology.
1	Online  verification  of  value  passing  choreographies  through  property  oriented  passive  testing.  Choreography  supports  the  specification,  with  a  global  perspective,  of  the  interactions  between  roles  played  by  peers  in  a  collaboration.  Choreography  conformance  testing  aims  at  verifying  whether  a  set  of  distributed  peers  collaborates  wrt.  a  choreography.  Such  collaborations  are  usually  achieved  through  information  exchange,  thus  taking  data  into  account  during  the  testing  process  is  necessary.  We  address  this  issue  by  using  a  non-intrusive  passive  testing  approach  based  on  functional  properties.  A  property  can  express  a  critical  (positive  or  negative)  behavior  to  be  tested  on  an  isolated  peer  (locally)  or  on  a  set  of  peers  (globally).  We  support  online  verification  of  these  kind  of  properties  against  local  running  traces  of  each  peer  in  a  distributed  system  where  no  global  clock  is  needed.  Our  framework  is  fully  tool  supported.
1	On  the  need  for  training  failure  prediction  algorithms  in  evolving  software  systems.  Failure  prediction  is  a  promising  technique  to  improve  dependability  of  computer  systems,  in  particular  when  it  is  important  to  foresee  incoming  failures  and  take  corrective  actions  to  avoid  downtime  or  data  corruption.  Failure  prediction  is  especially  adequate  in  long  running  systems  where  internal  errors  accumulate  and  eventually  lead  to  failures.  The  problem  is  that  such  systems  do  evolve.  The  workload  and  even  the  system  itself  changes  over  time,  and  this  may  affect  the  performance  of  the  failure  predictor.  However,  training  failure  prediction  algorithms  is  a  complex  and  time-consuming  task  and  should  be  performed  only  when  needed.  Thus,  it  is  important  to  understand  if  a  system  change  affects  prediction  performance,  to  avoid  running  the  target  system  with  an  ineffective  predictor  and  prevent  unnecessary  retraining  efforts.  In  this  work  we  study  the  performance  of  a  failure  predictor  when  used  to  forecast  failures  in  a  web-serving  system  subject  to  successive  updates.  We  observe  and  analyze  the  variation  of  performance  in  terms  of  ROC-AUC  using  fault  injection  and  virtualization  for  the  generation  of  the  data  needed  for  the  assessment.  Our  results  suggest  that  re-training  is  indeed  necessary.
1	Early  software  project  estimation  the  six  sigma  way.  The  Buglione-Trudel  matrix  is  a  tool  that  provides  agile  teams  with  immediate  feedback  whether  their  priorities  meet  customer  needs.  Functional  size  measurement  yields  a  transfer  function  mapping  user  stories  to  Functional  User  Requirements  (FUR),  and  business  impact  of  non-functional  requirements  yield  another  transfer  function  mapping  the  same  user  stories  onto  customer’s  Business  Drivers.
1	Mobile  testing  as  a  service  mtaas  infrastructures  issues  solutions  and  needs.  With  the  rapid  advance  of  mobile  computing  technology  and  wireless  networking,  there  is  a  significant  increase  of  mobile  subscriptions.  This  drives  a  strong  demand  for  development  and  validation  of  mobile  APPs  and  SaaS  applications  on  mobile  web.  This  paper  is  written  to  offer  informative  and  insightful  discussion  about  mobile  testing-as-a-service  (MTaaS),  including  its  basic  concepts,  motivations,  distinct  features  and  requirements,  test  environments,  and  different  approaches.  Moreover,  it  presents  a  test  process  in  MTaaS  and  three  different  approaches.  Furthermore,  the  paper  proposes  one  mobile  test  cloud  infrastructure  for  mobile  TaaS,  and  discusses  the  required  mobile  test  frameworks  and  environments.  Finally,  the  paper  addresses  existing  issues,  challenges,  and  emergent  needs.
1	A  testing  service  for  lifelong  validation  of  dynamic  soa.  Service  Oriented  Architectures  (SOAs)  are  increasingly  being  used  to  support  the  information  infrastructures  of  organizations.  SOAs  are  dynamic  and  evolve  after  deployment  in  order  to  adapt  to  changes  in  the  requirements  and  infrastructure.  Consequently,  traditional  validation  approaches  based  on  offline  testing  conducted  before  deployment  are  not  adequate  anymore,  demanding  for  new  techniques  that  allow  testing  the  SOA  during  its  whole  lifecycle.  In  this  paper  we  propose  a  SOA  testing  approach  based  on  a  composite  service  that  is  able  to  trace  SOA  evolution  and  automatically  test  the  various  services  according  to  specific  testing  policies.  The  paper  describes  the  architecture  of  the  testing  service  and  presents  a  concrete  implementation  focused  on  robustness  testing.  Results  from  a  case  study  demonstrate  the  effectiveness  of  the  proposed  approach  in  discovering  and  testing  the  robustness  of  SOA  services.
1	Challenges  and  success  factors  for  large  scale  agile  transformations  a  research  proposal  and  a  pilot  study.  Agile  methods  have  become  an  appealing  alternative  for  large  companies  striving  to  improve  their  performance,  even  though  the  methods  were  originally  designed  for  small,  single  teams.  Our  recently  performed  systematic  literature  review  on  large-scale  agile  transformations  revealed  that  despite  the  popularity  of  the  topic  in  the  industry,  it  has  received  very  little  research  attention,  as  almost  90%  of  the  selected  papers  were  experience  reports.  In  our  literature  study,  we  identified  35  challenges  grouped  into  nine  categories,  and  29  success  factors  grouped  into  eleven  categories.      In  this  research  proposal  we  suggest  a  survey  research  to  understand  these  factors  with  a  larger  population  of  companies.  The  purpose  is  to  validate  and  deepen  our  literature  study  findings.  We  report  the  findings  from  a  pilot  study  performed  in  the  XP2016  Large-scale  agile  workshop,  where  we  received  19  answers  to  the  survey  and  several  comments  from  16  persons  participating  in  the  WorldCafe  discussion  groups.  The  improved  survey  will  be  rolled  out  to  a  large  number  of  companies.
1	Stress  in  agile  software  development  practices  and  outcomes.  Stress  is  an  important  workplace  issue,  affecting  both  the  health  of  individuals,  and  the  health  of  organizations.  Early  advocacy  for  Agile  Software  Development  suggested  it  might  help  avoid  stress,  with  practices  that  emphasize  a  sustainable  pace,  and  self-organizing  teams.  Our  analysis  of  a  2014  survey,  however,  suggested  that  stress  might  still  be  commonplace  in  Agile  teams,  especially  for  those  with  less  experience.  We  also  noticed  that  newcomers  to  Agile  emphasized  technical,  rather  than  collaborative,  practices,  and  speculated  this  might  explain  the  stress.  We  explored  this  in  our  analysis  of  a  follow-up  survey  conducted  in  2016,  and  report  our  findings  in  this  paper.  We  show  that  there  are  a  variety  of  factors  involved,  and  that  avoiding  stress  is  associated  with  both  collaborative  and  technical  practices,  and  a  range  of  outcomes.
1	When  in  rome  do  as  the  romans  do  cultural  barriers  to  being  agile  in  distributed  teams.  With  the  growing  interest  of  adopting  agile  methods  in  offshored  process,  many  companies  realized  that  the  use  of  agile  methods  and  practices  in  companies  located  outside  the  location  of  early  adopters  of  agile  methods  may  be  challenging.  India,  the  main  destination  of  offshoring  contracts,  have  received  particular  attention,  due  to  the  big  cultural  differences.  Critical  analysis  of  related  studies  suggests  that  impeding  behaviors  are  mostly  rooted  in  the  hierarchical  culture  of  Indian  organizations  and  related  management  behavior  of  command-and-control.  But  what  happens  in  distributed  projects  with  a  more  empowering  onshore  management?  In  this  paper,  we  present  the  findings  from  a  multiple-case  study  of  DevOps  teams  with  members  from  a  mature  agile  company  located  in  Sweden  and  a  more  hierarchical  offshore  vendor  from  India.  Based  on  two  focus  groups  we  list  culturally  different  behaviors  of  offshore  engineers  that  were  reported  to  impede  agile  ways  of  working.  Furthermore,  we  report  the  findings  from  surveying  36  offshore  team  members  from  five  DevOps  teams  regarding  their  likely  behavior  in  situations  reported  to  be  problematic.  Our  findings  confirm  a  number  of  previously  reported  behaviors  rooted  in  cultural  differences  that  impede  the  adoption  of  agile  ways  of  working  when  collaborating  with  offshore  engineers.  At  the  same  time,  our  survey  results  suggest  that  among  the  five  surveyed  teams  there  were  teams  that  succeeded  with  the  cultural  integration  of  the  offshore  team  members.  Finally,  our  findings  demonstrate  the  importance  of  cultural  training  especially  when  onboarding  new  team  members.
1	Challenges  to  teamwork  a  multiple  case  study  of  two  agile  teams.  Agile  software  development  has  become  the  standard  in  many  companies.  While  there  are  reports  of  major  improvements  with  agile  development  over  traditional  development,  many  teams  still  strive  to  work  effectively  as  a  team.  A  multiple  case  study  in  two  companies  discovered  challenges  related  to  communication,  learning  and  selecting  the  tasks  according  to  the  priority  list.  For  example,  the  fact  that  the  developers  were  not  actively  involved  in  the  planning  process,  resulted  in  weak  team  orientation;  even  though  the  teams  had  identified  and  discussed  recurring  problems,  they  found  it  difficult  to  improve  their  teamwork  practices;  and  because  customers  and  support  communicated  tasks  directly  to  the  developers  and  developers  chose  tasks  according  to  interest  and  expertise,  following  the  priority  list  became  difficult.  We  provide  practical  suggestions  for  teamwork  in  agile  software  development  that  intend  to  overcome  these  problems  and  strengthen  team  orientation  and  team  learning  in  order  to  achieve  effective  agile  teams.
1	Comparing  scaling  agile  frameworks  based  on  underlying  practices.  Context:  Agile  software  development  is  widely-used  by  small  teams  and  has  benefits  like  increased  transparency  or  faster  feedback.  However,  companies  want  to  benefit  from  Agile  also  in  the  development  of  big  products,  where  multiple  teams  are  involved.  Many  Scaling  Agile  Frameworks  exist,  but  only  few  can  be  found  in  industry,  especially  SAFe,  LeSS,  and  Nexus.  Objective:  The  aim  of  this  work  is  to  identify  commonalities  of  existing  Scaling  Agile  Frameworks  concerning  their  practices.  Method:  We  extracted  and  consolidated  the  practices  of  twelve  frameworks  and  compared  the  frameworks  based  on  their  practices  using  a  visualization.  Results:  Frameworks  prescribe  scaling  practices  as  well  as  practices  on  team  level.  There  are  practices  common  to  most  frameworks  like  the  scaled  Scrum  events,  e.g.,  a  scaled  planning  meeting  or  retrospective.  Conclusion:  Practitioners  are  enabled  to  make  informed  decisions  when  choosing  or  tailoring  their  individual  Scaling  Agile  Framework.
1	Challenges  with  lack  of  trust  in  agile  projects  with  autonomous  teams  and  fixed  priced  contracts.  Agile  principles  and  autonomous  teams  is  a  challenge  in  fixed-price,  fixed-scope  projects  which  often  arise  when  IT  providers  work  with  public  customers.  We  report  from  a  large  agile  development  project,  where  a  software  provider  is  developing  a  new  large  administrative  system  in  the  public  sector  in  Norway.  A  qualitative  case  study  was  conducted.  The  data  was  collected  mainly  from  observations  and  interviews.  The  data  analysis  pointed  out  several  challenges  in  fixed-price  agile  projects.  One  of  main  challenges  was  that  the  project  suffered  from  lack  of  trust.  The  customer  (with  the  fixed  priced  contract)  did  not  trust  the  software  provider  (trying  to  work  agile  with  autonomous  teams)  and  vice  versa.  Pleasing  the  customer  and  stronger  customer  involvement  are  actions  to  create  trust,  which  also  will  give  more  room  for  a  flexible  scope.
1	Validation  of  object  recognition  framework  on  android  mobile  platform.  In  recent  years  there  has  been  great  interest  in  implementing  object  recognition  frame  work  on  mobile  phones.  This  has  stemmed  from  the  fact  the  advances  in  object  recognition  algorithm  and  mobile  phone  capabilities  have  built  a  congenial  ecosystem.  Application  developers  on  mobile  platforms  are  trying  to  utilize  the  object  recognition  technology  to  build  better  human  computer  interfaces.  This  approach  is  in  the  nascent  phase  and  proper  application  framework  is  required.  In  this  paper,  we  propose  a  framework  to  overcome  design  challenges  and  provide  an  evaluation  methodology  to  assess  the  system  performance.  We  use  the  emerging  Android  mobile  platform  to  implement  and  test  the  framework.  We  performed  a  case  study  using  the  proposal  and  reported  the  test  result.  This  assessment  will  help  developers  make  wise  decisions  about  their  application  design.  Furthermore,  the  Android  API  developers  could  use  this  information  to  provide  better  interfaces  to  the  third  party  developers.  The  design  and  evaluation  methodology  could  be  extended  to  other  mobile  platforms  for  a  wider  consumer  base.
1	Inter  organizational  co  development  with  scrum  experiences  and  lessons  learned  from  a  distributed  corporate  development  environment.  Distributed  development  within  a  single  organization  adds  a  lot  of  overhead  to  every  software  development  process.  When  a  second  organization  joins  for  co-development,  complexity  reaches  the  next  level.  This  case  study  investigates  an  agile  approach  from  a  real  world  project  involving  two  unaffiliated  IT  organizations  that  collaborate  in  a  distributed  development  environment.  Adaptations  to  the  regular  Scrum  process  are  identified  and  evaluated  over  a  six-month-long  period  of  time.  The  evaluation  involves  a  detailed  problem  root  cause  analysis  and  suggestions  on  what  issues  to  act  first.  Key  lessons  learned  include  that  team  members  of  one  Scrum  team  should  not  be  distributed  over  several  sites  and  that  every  site  should  have  at  least  one  Scrum  master  and  one  product  owner.
1	Software  development  as  an  experiment  system  a  qualitative  survey  on  the  state  of  the  practice.  An  experiment-driven  approach  to  software  product  and  service  development  is  gaining  increasing  attention  as  a  way  to  channel  limited  resources  to  the  efficient  creation  of  customer  value.  In  this  approach,  software  functionalities  are  developed  incrementally  and  validated  in  continuous  experiments  with  stakeholders  such  as  customers  and  users.  The  experiments  provide  factual  feedback  for  guiding  subsequent  development.  Although  case  studies  on  experimentation  in  industry  exist,  the  understanding  of  the  state  of  the  practice  and  the  encountered  obstacles  is  incomplete.  This  paper  presents  an  interview-based  qualitative  survey  exploring  the  experimentation  experiences  of  ten  software  development  companies.  The  study  found  that  although  the  principles  of  continuous  experimentation  resonated  with  industry  practitioners,  the  state  of  the  practice  is  not  yet  mature.  In  particular,  experimentation  is  rarely  systematic  and  continuous.  Key  challenges  relate  to  changing  organizational  culture,  accelerating  development  cycle  speed,  and  measuring  customer  value  and  product  success.
1	What  influences  the  speed  of  prototyping  an  empirical  investigation  of  twenty  software  startups.  It  is  essential  for  startups  to  quickly  experiment  business  ideas  by  building  tangible  prototypes  and  collecting  user  feedback  on  them.  As  prototyping  is  an  inevitable  part  of  learning  for  early  stage  software  startups,  how  fast  startups  can  learn  depends  on  how  fast  they  can  prototype.  Despite  of  the  importance,  there  is  a  lack  of  research  about  prototyping  in  software  startups.  In  this  study,  we  aimed  at  understanding  what  are  factors  influencing  different  types  of  prototyping  activities.  We  conducted  a  multiple  case  study  on  twenty  European  software  startups.  The  results  are  two  folds;  firstly  we  propose  a  prototype-centric  learning  model  in  early  stage  software  startups.  Secondly,  we  identify  factors  occur  as  barriers  but  also  facilitators  for  prototyping  in  early  stage  software  startups.  The  factors  are  grouped  into  (1)  artifacts,  (2)  team  competence,  (3)  collaboration,  (4)  customer  and  (5)  process  dimensions.  To  speed  up  a  startup’s  progress  at  the  early  stage,  it  is  important  to  incorporate  the  learning  objective  into  a  well-defined  collaborative  approach  of  prototyping.
1	Compliance  and  testing  preorders  differ.  Contracts  play  an  essential  role  in  the  Service  Oriented  Computing,  for  which  they  need  to  be  equipped  with  a  sub-contract  relation.  We  compare  two  possible  formulations,  one  based  on  compliance  and  the  other  on  the  testing  theory  of  De  Nicola  and  Hennessy.  We  show  that  if  the  language  of  contracts  is  sufficiently  expressive  then  the  resulting  sub-contract  relations  are  incomparable.    However  if  we  put  natural  restrictions  on  the  contract  language  then  the  sub-contract  relations  coincide,  at  least  when  applied  to  servers.  But  when  formulated  for  clients  they  remain  incomparable,  for  many  reasonable  contract  languages.  Finally  we  give  one  example  of  a  contract  language  for  which  the  client-based  sub-contract  relations  coincide.
1	Formal  analysis  of  proactive  distributed  routing.  As  (network)  software  is  such  an  omnipresent  component  of  contemporary  mission-critical  systems,  formal  analysis  is  required  to  provide  the  necessary  certification  or  at  least  formal  assurances  for  these  systems.  In  this  paper  we  focus  on  modelling  and  analysing  the  Optimised  Link  State  Routing  (OLSR)  protocol,  a  distributed,  proactive  routing  protocol.  It  is  recognised  as  one  of  the  standard  ad-hoc  routing  protocols  for  Wireless  Mesh  Networks  (WMNs).  WMNs  are  instrumental  in  critical  systems,  such  as  emergency  response  networks  and  smart  electrical  grids.  We  use  the  model  checker  Uppaal  for  analysing  safety  properties  of  OLSR  as  well  as  to  point  out  a  case  of  OLSR  malfunctioning.
1	Jbernstein  a  validity  checker  for  generalized  polynomial  constraints.  Efficient  and  scalable  verification  of  nonlinear  real  arithmetic  constraints  is  essential  in  many  automated  verification  and  synthesis  tasks  for  hybrid  systems,  control  algorithms,  digital  signal  processors,  and  mixed  analog/digital  circuits.  Despite  substantial  advances  in  verification  technology,  complexity  issues  with  classical  decision  procedures  for  nonlinear  real  arithmetic  are  still  a  major  obstacle  for  formal  verification  of  real-world  applications.
1	Existential  quantification  as  incremental  sat.  This  paper  presents  an  elegant  algorithm  for  existential  quantifier  elimination  using  incremental  SAT  solving.  This  approach  contrasts  with  existing  techniques  in  that  it  is  based  solely  on  manipulating  the  SAT  instance  rather  than  requiring  any  reengineering  of  the  SAT  solver  or  needing  an  auxiliary  data-structure  such  as  a  BDD.  The  algorithm  combines  model  enumeration  with  the  generation  of  shortest  prime  implicants  so  as  to  converge  onto  a  quantifier-free  formula  presented  in  CNF.  We  apply  the  technique  to  a  number  of  hardware  circuits  and  transfer  functions  to  demonstrate  the  effectiveness  of  the  method.
1	Proving  non  termination  using  max  smt.  We  show  how  Max-SMT-based  invariant  generation  can  be  exploited  for  proving  non-termination  of  programs.  The  construction  of  the  proof  of  non-termination  is  guided  by  the  generation  of  quasi-invariants    properties  such  that  if  they  hold  at  a  location  during  execution  once,  then  they  will  continue  to  hold  at  that  location  from  then  onwards.  The  check  that  quasi-invariants  can  indeed  be  reached  is  then  performed  separately.  Our  technique  considers  strongly  connected  subgraphs  of  a  program's  control  flow  graph  for  analysis  and  thus  produces  more  generic  witnesses  of  non-termination  than  existing  methods.  Moreover,  it  can  handle  programs  with  unbounded  non-determinism  and  is  more  likely  to  converge  than  previous  approaches.
1	Automated  resource  analysis  with  coq  proof  objects.  This  paper  addresses  the  problem  of  automatically  performing  resource-bound  analysis,  which  can  help  programmers  understand  the  performance  characteristics  of  their  programs.  We  introduce  a  method  for  resource-bound  inference  that  (i)  is  compositional,  (ii)  produces  machine-checkable  certificates  of  the  resource  bounds  obtained,  and  (iii)  features  a  sound  mechanism  for  user  interaction  if  the  inference  fails.  The  technique  handles  recursive  procedures  and  has  the  ability  to  exploit  any  known  program  invariants.  An  experimental  evaluation  with  an  implementation  in  the  tool  Pastis  shows  that  the  new  analysis  is  competitive  with  state-of-the-art  resource-bound  tools  while  also  creating  Coq  certificates.
1	Beautiful  interpolants.  We  describe  a  compositional  approach  to  Craig  interpolation  based  on  the  heuristic  that  simpler  proofs  of  special  cases  are  more  likely  to  generalize.  The  method  produces  simple  interpolants  because  it  is  able  to  summarize  a  large  set  of  cases  using  one  relatively  simple  fact.  In  particular,  we  present  a  method  for  finding  such  simple  facts  in  the  theory  of  linear  rational  arithmetic.  This  makes  it  possible  to  use  interpolation  to  discover  inductive  invariants  for  numerical  programs  that  are  challenging  for  existing  techniques.  We  show  that  in  some  cases,  the  compositional  approach  can  also  be  more  efficient  than  traditional  lazy  SMT  as  a  decision  procedure.
1	Verification  of  threshold  based  distributed  algorithms  by  decomposition  to  decidable  logics.  Verification  of  fault-tolerant  distributed  protocols  is  an  immensely  difficult  task.  Often,  in  these  protocols,  thresholds  on  set  cardinalities  are  used  both  in  the  process  code  and  in  its  correctness  proof,  e.g.,  a  process  can  perform  an  action  only  if  it  has  received  an  acknowledgment  from  at  least  half  of  its  peers.  Verification  of  threshold-based  protocols  is  extremely  challenging  as  it  involves  two  kinds  of  reasoning:  first-order  reasoning  about  the  unbounded  state  of  the  protocol,  together  with  reasoning  about  sets  and  cardinalities.  In  this  work,  we  develop  a  new  methodology  for  decomposing  the  verification  task  of  such  protocols  into  two  decidable  logics:  EPR  and  BAPA.  Our  key  insight  is  that  such  protocols  use  thresholds  in  a  restricted  way  as  a  means  to  obtain  certain  properties  of  “intersection”  between  sets.  We  define  a  language  for  expressing  such  properties,  and  present  two  translations:  to  EPR  and  BAPA.  The  EPR  translation  allows  verifying  the  protocol  while  assuming  these  properties,  and  the  BAPA  translation  allows  verifying  the  correctness  of  the  properties.  We  further  develop  an  algorithm  for  automatically  generating  the  properties  needed  for  verifying  a  given  protocol,  facilitating  fully  automated  deductive  verification.  Using  this  technique  we  have  verified  several  challenging  protocols,  including  Byzantine  one-step  consensus,  hybrid  reliable  broadcast  and  fast  Byzantine  Paxos.
1	Predator  a  practical  tool  for  checking  manipulation  of  dynamic  data  structures  using  separation  logic.  Predator  is  a  new  open  source  tool  for  verification  of  sequential  C  programs  with  dynamic  linked  data  structures.  The  tool  is  based  on  separation  logic  with  inductive  predicates  although  it  uses  a  graph  description  of  heaps.  Predator  currently  handles  various  forms  of  lists,  including  singly-linked  as  well  as  doubly-linked  lists  that  may  be  circular,  hierarchically  nested  and  that  may  have  various  additional  pointer  links.  Predator  is  implemented  as  a  gcc  plug-in  and  it  is  capable  of  handling  lists  in  the  form  they  appear  in  real  system  code,  especially  the  Linux  kernel,  including  a  limited  support  of  pointer  arithmetic.  Collaboration  on  further  development  of  Predator  is  welcome.
1	Abstract  interpretation  with  unfoldings.  We  present  and  evaluate  a  technique  for  computing  path-sensitive  interference  conditions  during  abstract  interpretation  of  concurrent  programs.  In  lieu  of  fixed  point  computation,  we  use  prime  event  structures  to  compactly  represent  causal  dependence  and  interference  between  sequences  of  transformers.  Our  main  contribution  is  an  unfolding  algorithm  that  uses  a  new  notion  of  independence  to  avoid  redundant  transformer  application,  thread-local  fixed  points  to  reduce  the  size  of  the  unfolding,  and  a  novel  cutoff  criterion  based  on  subsumption  to  guarantee  termination  of  the  analysis.  Our  experiments  show  that  the  abstract  unfolding  produces  an  order  of  magnitude  fewer  false  alarms  than  a  mature  abstract  interpreter,  while  being  several  orders  of  magnitude  faster  than  solver-based  tools  that  have  the  same  precision.
1	Causal  termination  of  multi  threaded  programs.  We  present  a  new  model  checking  procedure  for  the  termination  analysis  of  multi-threaded  programs.  Current  termination  provers  scale  badly  in  the  number  of  threads;  our  new  approach  easily  handles  100  threads  on  multi-threaded  benchmarks  like  Producer-Consumer.  In  our  procedure,  we  characterize  the  existence  of  non-terminating  executions  as  Mazurkiewicz-style  concurrent  traces  and  apply  causality-based  transformation  rules  to  refine  them  until  a  contradiction  can  be  shown.  The  termination  proof  is  organized  into  a  tableau,  where  the  case  splits  represent  a  novel  type  of  modular  reasoning  according  to  different  causal  explanations  of  a  hypothetical  error.  We  report  on  experimental  results  obtained  with  a  tool  implementation  of  the  new  procedure,  called  Arctor,  on  previously  intractable  multi-threaded  benchmarks.
1	Automated  and  modular  refinement  reasoning  for  concurrent  programs.  We  present  civl,  a  language  and  verifier  for  concurrent  programs  based  on  automated  and  modular  refinement  reasoning.  civl  supports  reasoning  about  a  concurrent  program  at  many  levels  of  abstraction.  Atomic  actions  in  a  high-level  description  are  refined  to  fine-grain  and  optimized  lower-level  implementations.  A  novel  combination  of  automata  theoretic  and  logic-based  checks  is  used  to  verify  refinement.  Modular  specifications  and  proof  annotations,  such  as  location  invariants  and  procedure  pre-  and  post-conditions,  are  specified  separately,  independently  at  each  level  in  terms  of  the  variables  visible  at  that  level.  We  have  implemented  civl  as  an  extension  to  the  boogie  language  and  verifier.  We  have  used  civl  to  refine  a  realistic  concurrent  garbage  collection  algorithm  from  a  simple  high-level  specification  down  to  a  highly-concurrent  implementation  described  in  terms  of  individual  memory  accesses.  Open  image  in  new  window
1	Synthesis  of  fault  attack  countermeasures  for  cryptographic  circuits.  Fault  sensitivity  analysis  (FSA)  is  a  side-channel  attack  method  that  injects  faults  to  cryptographic  circuits  through  clock  glitching  and  applies  statistical  analysis  to  deduce  sensitive  data  such  as  the  cryptographic  key.  It  exploits  the  correlation  between  the  circuit’s  signal  path  delays  and  sensitive  data.  A  countermeasure,  in  this  case,  is  an  alternative  implementation  of  the  circuit  where  signal  path  delays  are  made  independent  of  the  sensitive  data.  However,  manually  developing  such  countermeasure  is  tedious  and  error  prone.  In  this  paper,  we  propose  a  method  for  synthesizing  the  countermeasure  automatically  to  defend  against  FSA  attacks.  Our  method  uses  a  syntax-guided  inductive  synthesis  procedure  combined  with  a  light-weight  static  analysis.  Given  a  circuit  and  a  set  of  sensitive  signals  as  input,  it  returns  a  functionally-equivalent  and  FSA-resistant  circuit  as  output,  where  all  path  delays  are  made  independent  of  the  sensitive  signals.  We  have  implemented  our  method  and  evaluated  it  on  a  set  of  cryptographic  circuits.  Our  experiments  show  that  the  method  is  both  scalable  and  effective  in  eliminating  FSA  vulnerabilities.
1	Model  checking  algorithms  for  ctmdps.  Continuous  Stochastic  Logic  (CSL)  can  be  interpreted  over  continuoustime  Markov  decision  processes  (CTMDPs)  to  specify  quantitative  properties  of  stochastic  systems  that  allow  some  external  control.  Model  checking  CSL  formulae  over  CTMDPs  requires  then  the  computation  of  optimal  control  strategies  to  prove  or  disprove  a  formula.  The  paper  presents  a  conservative  extension  of  CSL  over  CTMDPs--with  rewards--and  exploits  established  results  for  CTMDPs  for  model  checking  CSL.  A  new  numerical  approach  based  on  uniformization  is  devised  to  compute  time  bounded  reachability  results  for  time  dependent  control  strategies.  Experimental  evidence  is  given  showing  the  efficiency  of  the  approach.
1	Strix  explicit  reactive  synthesis  strikes  back.  Strix  is  a  new  tool  for  reactive  LTL  synthesis  combining  a  direct  translation  of  LTL  formulas  into  deterministic  parity  automata  (DPA)  and  an  efficient,  multi-threaded  explicit  state  solver  for  parity  games.  In  brief,  Strix  (1)  decomposes  the  given  formula  into  simpler  formulas,  (2)  translates  these  on-the-fly  into  DPAs  based  on  the  queries  of  the  parity  game  solver,  (3)  composes  the  DPAs  into  a  parity  game,  and  at  the  same  time  already  solves  the  intermediate  games  using  strategy  iteration,  and  (4)  finally  translates  the  winning  strategy,  if  it  exists,  into  a  Mealy  machine  or  an  AIGER  circuit  with  optional  minimization  using  external  tools.  We  experimentally  demonstrate  the  applicability  of  our  approach  by  a  comparison  with  Party,  BoSy,  and  ltlsynt  using  the  syntcomp2017  benchmarks.  In  these  experiments,  our  prototype  can  compete  with  BoSy  and  ltlsynt  with  only  Party  performing  slightly  better.  In  particular,  our  prototype  successfully  synthesizes  the  full  and  unmodified  LTL  specification  of  the  AMBA  protocol  for  \(n=2\)  masters.
1	Nonhomogeneous  place  dependent  markov  chains  unsynchronised  aimd  and  optimisation.  A  stochastic  algorithm  is  presented  for  a  class  of  optimisation  problems  that  arise  when  a  group  of  agents  compete  to  share  a  single  constrained  resource  in  an  optimal  manner.  The  approach  uses  intermittent  single-bit  feedback,  which  indicates  a  constraint  violation  and  does  not  require  inter-agent  communication.  The  algorithm  is  based  on  a  positive  matrix  model  of  AIMD,  which  is  extended  to  the  nonhomogeneous  Markovian  case.  The  key  feature  is  the  assignment  of  back-off  probabilities  to  the  individual  agents  as  a  function  of  the  past  average  access  to  the  resource.  This  leads  to  a  nonhomogeneous  Markov  chain  in  an  extended  state  space,  and  we  show  almost  sure  convergence  of  the  average  access  to  the  social  optimum.
1	Cerberus  bmc  a  principled  reference  semantics  and  exploration  tool  for  concurrent  and  sequential  c.  C  remains  central  to  our  infrastructure,  making  verification  of  C  code  an  essential  and  much-researched  topic,  but  the  semantics  of  C  is  remarkably  complex,  and  important  aspects  of  it  are  still  unsettled,  leaving  programmers  and  verification  tool  builders  on  shaky  ground.  This  paper  describes  a  tool,  Cerberus-BMC,  that  for  the  first  time  provides  a  principled  reference  semantics  that  simultaneously  supports  (1)  a  choice  of  concurrency  memory  model  (including  substantial  fragments  of  the  C11,  RC11,  and  Linux  kernel  memory  models),  (2)  a  modern  memory  object  model,  and  (3)  a  well-validated  thread-local  semantics  for  a  large  fragment  of  the  language.  The  tool  should  be  useful  for  C  programmers,  compiler  writers,  verification  tool  builders,  and  members  of  the  C/C++  standards  committees.
1	Cutting  the  mix.  While  linear  arithmetic  has  been  studied  in  the  context  of  SMT  individually  for  reals  and  integers,  mixed  linear  arithmetic  allowing  comparisons  between  integer  and  real  variables  has  not  received  much  attention.  For  linear  integer  arithmetic,  the  cuts  from  proofs  algorithm  has  proven  to  have  superior  performance  on  many  benchmarks.  In  this  paper  we  extend  this  algorithm  to  the  mixed  case  where  real  and  integer  variables  occur  in  the  same  linear  constraint.  Our  algorithm  allows  for  an  easy  integration  into  existing  SMT  solvers.  Experimental  evaluation  of  our  prototype  implementation  inside  the  SMT  solver  SMTInterpol  shows  that  this  algorithm  is  successful  on  benchmarks  that  are  hard  for  all  existing  solvers.  Open  image  in  new  window
1	Compositional  synthesis  of  reactive  controllers  for  multi  agent  systems.  In  this  paper  we  consider  the  controller  synthesis  problem  for  multi-agent  systems  that  consist  of  a  set  of  controlled  and  uncontrolled  agents.  Controlled  agents  may  need  to  cooperate  with  each  other  and  react  to  the  actions  of  uncontrolled  agents  in  order  to  fulfill  their  objectives.  Besides,  the  controlled  agents  may  be  imperfect,  i.e.,  only  partially  observe  their  environment,  for  example  due  to  the  limitations  in  their  sensors.  We  propose  a  framework  for  controller  synthesis  based  on  compositional  reactive  synthesis.  We  implement  the  algorithms  symbolically  and  apply  them  to  a  robot  motion  planning  case  study  where  multiple  robots  are  placed  on  a  grid-world  with  static  obstacles  and  other  dynamic,  uncontrolled  and  potentially  adversarial  robots.  We  consider  different  objectives  such  as  collision  avoidance,  keeping  a  formation  and  bounded  reachability.  We  show  that  by  taking  advantage  of  the  structure  of  the  system,  compositional  synthesis  algorithm  can  significantly  outperform  centralized  synthesis  approach,  both  from  time  and  memory  perspective,  and  can  solve  problems  where  the  centralized  algorithm  is  infeasible.  Our  findings  show  the  potential  of  symbolic  and  compositional  reactive  synthesis  methods  as  planning  algorithms  in  the  presence  of  dynamically  changing  and  possibly  adversarial  environment.
1	Bmc  for  weak  memory  models  relation  analysis  for  compact  smt  encodings.  We  present  Dartagnan,  a  bounded  model  checker  (BMC)  for  concurrent  programs  under  weak  memory  models.  Its  distinguishing  feature  is  that  the  memory  model  is  not  implemented  inside  the  tool  but  taken  as  part  of  the  input.  Dartagnan  reads  CAT,  the  standard  language  for  memory  models,  which  allows  to  define  x86/TSO,  ARMv7,  ARMv8,  Power,  C/C++,  and  Linux  kernel  concurrency  primitives.  BMC  with  memory  models  as  inputs  is  challenging.  One  has  to  encode  into  SMT  not  only  the  program  but  also  its  semantics  as  defined  by  the  memory  model.  What  makes  Dartagnan  scale  is  its  relation  analysis,  a  novel  static  analysis  that  significantly  reduces  the  size  of  the  encoding.  Dartagnan  matches  or  even  exceeds  the  performance  of  the  model-specific  verification  tools  Nidhugg  and  CBMC,  as  well  as  the  performance  of  Herd,  a  CAT-compatible  litmus  testing  tool.  Compared  to  the  unoptimized  encoding,  the  speed-up  is  often  more  than  two  orders  of  magnitude.
1	A  lightweight  methodology  for  safety  case  assembly.  We  describe  a  lightweight  methodology  to  support  the  automatic  assembly  of  safety  cases  from  tabular  requirements  specifications.  The  resulting  safety  case  fragments  provide  an  alternative,  graphical,  view  of  the  requirements.  The  safety  cases  can  be  modified  and  augmented  with  additional  information.  In  turn,  these  modifications  can  be  mapped  back  to  extensions  of  the  tabular  requirements,  with  which  they  are  kept  consistent,  thus  avoiding  the  need  for  engineers  to  maintain  an  additional  artifact.  We  formulate  our  approach  on  top  of  an  idealized  process,  and  illustrate  the  applicability  of  the  methodology  on  excerpts  of  requirements  specifications  for  an  experimental  Unmanned  Aircraft  System.
1	Concerns  on  the  differences  between  ai  and  system  safety  mindsets  impacting  autonomous  vehicles  safety.  The  inflection  point  in  the  development  of  some  core  technologies  enabled  the  Autonomous  Vehicles  (AV).  The  unprecedented  growth  rate  in  Artificial  Intelligence  (AI)  and  Machine  Learning  (ML)  capabilities,  focusing  only  on  AVs,  is  expected  to  shift  the  transportation  paradigm  and  bring  relevant  benefits  to  the  society,  such  as  accidents  reduction.  However,  recent  AVs  accidents  resulted  in  life  losses.  This  paper  presents  a  viewpoint  discussion  based  on  findings  from  a  preliminary  exploratory  literature  review.  It  was  identified  an  important  misalignment  between  AI  and  Safety  research  communities  regarding  the  impact  of  AI  on  the  safety  risks  in  AV.  This  paper  promotes  this  discussion,  raises  concerns  on  the  potential  consequences  and  suggests  research  topics  to  reduce  the  differences  between  AI  and  system  safety  mindsets.
1	Automotive  safety  concept  definition  for  mixed  criticality  integration  on  a  cots  multicore.  Mixed-criticality  systems  integrating  applications  subject  to  different  safety  assurance  levels  into  the  same  multicore  embedded  platform  can  provide  potential  benefits  in  terms  of  performance,  cost,  size,  weight,  and  power.  In  spite  of  this  evidence,  however,  several  hard  challenges  related  to  the  safety  certification  of  multicore  approaches  must  be  considered  before  endorsing  their  unrestrained  adoption.  This  paper  describes  an  ISO-26262  compliant  safety  concept  for  an  automotive  mixed-criticality  case-study  on  top  of  a  multicore  platform.  To  this  end,  key  aspects  such  as  time  and  space  partitioning  are  evaluated  and  enforced  by  means  of  hardware  protection  mechanisms.
1	Verifying  functional  behaviors  of  automotive  products  in  east  adl2  using  uppaal  port.  We  study  the  use  of  formal  modeling  and  verification  techniques  at  an  early  stage  in  the  development  of  safety-critical  automotive  products  which  are  originally  described  in  the  domain  specific  architectural  language  EAST-ADL2.  This  architectural  language  only  focuses  on  the  structural  definition  of  functional  blocks.  However,  the  behavior  inside  each  functional  block  is  not  specified  and  that  limits  formal  modeling  and  analysis  of  systems  behaviors  as  well  as  efficient  verification  of  safety  properties.  In  this  paper,  we  tackle  this  problem  by  proposing  one  modeling  approach,  which  formally  captures  the  behavioral  execution  inside  each  functional  block  and  their  interactions,  and  helps  to  improve  the  formal  modeling  and  verification  capability  of  EAST-ADL2:  the  behavior  of  each  elementary  function  of  EAST-ADL2  is  specified  in  UPPAAL  Timed  Automata.  The  formal  syntax  and  semantics  are  defined  in  order  to  specify  the  behavior  model  inside  EAST-ADL2  and  their  interactions.  A  composition  of  the  functional  behaviors  is  considered  a  network  of  Timed  Automata  that  enables  us  to  verify  behaviors  of  the  entire  system  using  the  UPPAAL  model  checker.  The  method  has  been  demonstrated  by  verifying  the  safety  of  the  Brake-by-wire  system  design.
1	Safer  hrc  safety  analysis  through  formal  verification  in  human  robot  collaboration.  Whereas  in  classic  robotic  applications  there  is  a  clear  segregation  between  robots  and  operators,  novel  robotic  and  cyber-physical  systems  have  evolved  in  size  and  functionality  to  include  the  collaboration  with  human  operators  within  common  workspaces.  This  new  application  field,  often  referred  to  as  Human-Robot  Collaboration  (HRC),  raises  new  challenges  to  guarantee  system  safety,  due  to  the  presence  of  operators.  We  present  an  innovative  methodology,  called  SAFER-HRC,  centered  around  our  logic  language  TRIO  and  the  companion  bounded  satisfiability  checker  Zot,  to  assess  the  safety  risks  in  an  HRC  application.  The  methodology  starts  from  a  generic  modular  model  and  customizes  it  for  the  target  system;  it  then  analyses  hazards  according  to  known  standards,  to  study  the  safety  of  the  collaborative  environment.
1	Assuring  compliance  with  protection  profiles  with  threatget.  We  present  ThreatGet  a  new  tool  for  security  analysis,  based  on  threat  modeling.  The  tool  is  integrated  into  a  model-based  engineering  platform,  supporting  an  iterative  and  model-based  risk  management  process.  We  explain  the  modeling  and  operation  of  ThreatGet  and  how  it  can  be  used  for  security  by  design.  As  a  specific  use  case,  we  demonstrate  how  ThreatGet  can  assess  compliance  with  a  protection  profile.
1	The  assurance  recipe  facilitating  assurance  patterns.  As  assurance  cases  have  grown  in  popularity  for  safety-critical  systems,  so  too  has  their  complexity  and  thus  the  need  for  methods  to  systematically  build  them.  Assurance  cases  can  grow  too  large  and  too  abstract  for  anyone  but  the  original  builders  to  understand,  making  reuse  difficult.  Reuse  is  important  because  different  systems  might  have  identical  or  similar  components,  and  a  good  solution  for  one  system  should  be  applicable  to  similar  systems.  Prior  research  has  shown  engineers  can  alleviate  some  of  the  complexity  issues  through  modularity  and  identifying  common  patterns  which  are  more  easily  understood  for  reuse  across  different  systems.  However,  we  believe  these  patterns  are  too  complicated  for  users  who  lack  expertise  in  software  engineering  or  assurance  cases.  This  paper  suggests  the  concept  of  lower-level  patterns  which  we  call  recipes.  We  use  the  safety-critical  field  of  synthetic  biology,  as  an  example  discipline  to  demonstrate  how  a  recipe  can  be  built  and  applied.
1	Optimal  guard  synthesis  for  memory  safety.  This  paper  presents  a  new  synthesis-based  approach  for  writing  low-level  memory-safe  code.  Given  a  partial  program  with  missing  guards,  our  algorithm  synthesizes  concrete  predicates  to  plug  in  for  the  missing  guards  such  that  all  buffer  accesses  in  the  program  are  memory  safe.  Furthermore,  guards  synthesized  by  our  technique  are  the  simplest  and  weakest  among  guards  that  guarantee  memory  safety,  relative  to  the  inferred  loop  invariants.  Our  approach  is  fully  automatic  and  does  not  require  any  hints  from  the  user.  We  have  implemented  our  algorithm  in  a  prototype  synthesis  tool  for  C  programs,  and  we  show  that  the  proposed  approach  is  able  to  successfully  synthesize  guards  that  closely  match  hand-written  programmer  code  in  a  set  of  real-world  C  programs.
1	Symdiff  a  language  agnostic  semantic  diff  tool  for  imperative  programs.  In  this  paper,  we  describe  SymDiff,  a  language-agnostic  tool  for  equivalence  checking  and  displaying  semantic  (behavioral)  differences  over  imperative  programs.  The  tool  operates  on  an  intermediate  verification  language  Boogie,  for  which  translations  exist  from  various  source  languages  such  as  C,  C#  and  x86.  We  discuss  the  tool  and  the  front-end  interface  to  target  various  source  languages.  Finally,  we  provide  a  brief  description  of  the  front-end  for  C  programs.
1	Synchronization  synthesis  for  network  programs.  In  software-defined  networking  (SDN),  a  controller  program  updates  the  forwarding  rules  installed  on  network  packet-processing  devices  in  response  to  events.  Such  programs  are  often  physically  distributed,  running  on  several  nodes  of  the  network,  and  this  distributed  setting  makes  programming  and  debugging  especially  difficult.  Furthermore,  bugs  in  these  programs  can  lead  to  serious  problems  such  as  packet  loss  and  security  violations.  In  this  paper,  we  propose  a  program  synthesis  approach  that  makes  it  easier  to  write  distributed  controller  programs.  The  programmer  can  specify  each  sequential  process,  and  add  a  declarative  specification  of  paths  that  packets  are  allowed  to  take.  The  synthesizer  then  inserts  enough  synchronization  among  the  distributed  controller  processes  such  that  the  declarative  specification  will  be  satisfied  by  all  packets  traversing  the  network.  Our  key  technical  contribution  is  a  counterexample-guided  synthesis  algorithm  that  furnishes  network  controller  processes  with  the  synchronization  constructs  required  to  prevent  any  races  causing  specification  violations.  Our  programming  model  is  based  on  Petri  nets,  and  generalizes  several  models  from  the  networking  literature.  Importantly,  our  programs  can  be  implemented  in  a  way  that  prevents  races  between  updates  to  individual  switches  and  in-flight  packets.  To  our  knowledge,  this  is  the  first  counterexample-guided  technique  that  automatically  adds  synchronization  constructs  to  Petri-net-based  programs.  We  demonstrate  that  our  prototype  implementation  can  fix  realistic  concurrency  bugs  described  previously  in  the  literature,  and  that  our  tool  can  readily  scale  to  network  topologies  with  1000+  nodes.
1	Angelic  verification  precise  verification  modulo  unknowns.  Verification  of  open  programs  can  be  challenging  in  the  presence  of  an  unconstrained  environment.  Verifying  properties  that  depend  on  the  environment  yields  a  large  class  of  uninteresting  false  alarms.  Using  a  verifier  on  a  program  thus  requires  extensive  initial  investment  in  modeling  the  environment  of  the  program.  We  propose  a  technique  called  angelic  verification  for  verification  of  open  programs,  where  we  constrain  a  verifier  to  report  warnings  only  when  no  acceptable  environment  specification  exists  to  prove  the  assertion.  Our  framework  is  parametric  in  a  vocabulary  and  a  set  of  angelic  assertions  that  allows  a  user  to  configure  the  tool.  We  describe  a  few  instantiations  of  the  framework  and  an  evaluation  on  a  set  of  real-world  benchmarks  to  show  that  our  technique  is  competitive  with  industrial-strength  tools  even  without  models  of  the  environment.
1	Predicate  abstraction  and  cegar  for  disproving  termination  of  higher  order  functional  programs.  We  propose  an  automated  method  for  disproving  termination  of  higher-order  functional  programs.  Our  method  combines  higher-order  model  checking  with  predicate  abstraction  and  CEGAR.  Our  predicate  abstraction  is  novel  in  that  it  computes  a  mixture  of  under-  and  overapproximations.  For  non-determinism  of  a  source  program  (such  as  random  number  generation),  we  apply  underapproximation  to  generate  a  subset  of  the  actual  branches,  and  check  that  some  of  the  branches  in  the  abstract  program  is  non-terminating.  For  operations  on  infinite  data  domains  (such  as  integers),  we  apply  overapproximation  to  generate  a  superset  of  the  actual  branches,  and  check  that  every  branch  is  non-terminating.  Thus,  disproving  non-termination  reduces  to  the  problem  of  checking  a  certain  branching  property  of  the  abstract  program,  which  can  be  solved  by  higher-order  model  checking.  We  have  implemented  a  prototype  non-termination  prover  based  on  our  method  and  have  confirmed  the  effectiveness  of  the  proposed  approach  through  experiments.
1	Regression  test  selection  for  distributed  software  histories.  Regression  test  selection  analyzes  incremental  changes  to  a  codebase  and  chooses  to  run  only  those  tests  whose  behavior  may  be  affected  by  the  latest  changes  in  the  code.  By  focusing  on  a  small  subset  of  all  the  tests,  the  testing  process  runs  faster  and  can  be  more  tightly  integrated  into  the  development  process.  Existing  techniques  for  regression  test  selection  consider  two  versions  of  the  code  at  a  time,  effectively  assuming  a  development  process  where  changes  to  the  code  occur  in  a  linear  sequence.    Modern  development  processes  that  use  distributed  version-control  systems  are  more  complex.  Software  version  histories  are  generally  modeled  as  directed  graphs;  in  addition  to  version  changes  occurring  linearly,  multiple  versions  can  be  related  by  other  commands,  e.g.,  branch,  merge,  rebase,  cherry-pick,  revert,  etc.  This  paper  describes  a  regression  test-selection  technique  for  software  developed  using  modern  distributed  version-control  systems.  By  modeling  different  branch  or  merge  commands  directly  in  our  technique,  it  computes  safe  test  sets  that  can  be  substantially  smaller  than  applying  previous  techniques  to  a  linearization  of  the  software  history.    We  evaluate  our  technique  on  software  histories  of  several  large  open-source  projects.  The  results  are  encouraging:  our  technique  obtained  an  average  of  10.89×  reduction  in  the  number  of  tests  over  an  existing  technique  while  still  selecting  all  tests  whose  behavior  may  differ.
1	Fast  algorithms  for  handling  diagonal  constraints  in  timed  automata.  A  popular  method  for  solving  reachability  in  timed  automata  proceeds  by  enumerating  reachable  sets  of  valuations  represented  as  zones.  A  naive  enumeration  of  zones  does  not  terminate.  Various  termination  mechanisms  have  been  studied  over  the  years.  Coming  up  with  efficient  termination  mechanisms  has  been  remarkably  more  challenging  when  the  automaton  has  diagonal  constraints  in  guards.
1	Overfitting  in  synthesis  theory  and  practice.  In  syntax-guided  synthesis  (SyGuS),  a  synthesizer’s  goal  is  to  automatically  generate  a  program  belonging  to  a  grammar  of  possible  implementations  that  meets  a  logical  specification.  We  investigate  a  common  limitation  across  state-of-the-art  SyGuS  tools  that  perform  counterexample-guided  inductive  synthesis  (CEGIS).  We  empirically  observe  that  as  the  expressiveness  of  the  provided  grammar  increases,  the  performance  of  these  tools  degrades  significantly.
1	Clock  bound  repair  for  timed  systems.  We  present  algorithms  and  techniques  for  the  repair  of  timed  system  models,  given  as  networks  of  timed  automata  (NTA).  The  repair  is  based  on  an  analysis  of  timed  diagnostic  traces  (TDTs)  that  are  computed  by  real-time  model  checking  tools,  such  as  UPPAAL,  when  they  detect  the  violation  of  a  timed  safety  property.  We  present  an  encoding  of  TDTs  in  linear  real  arithmetic  and  use  the  MaxSMT  capabilities  of  the  SMT  solver  Z3  to  compute  possible  repairs  to  clock  bound  values  that  minimize  the  necessary  changes  to  the  automaton.  We  then  present  an  admissibility  criterion,  called  functional  equivalence,  that  assesses  whether  a  proposed  repair  is  admissible  in  the  overall  context  of  the  NTA.  We  have  implemented  a  proof-of-concept  tool  called  TarTar  for  the  repair  and  admissibility  analysis.  To  illustrate  the  method,  we  have  considered  a  number  of  case  studies  taken  from  the  literature  and  automatically  injected  changes  to  clock  bounds  to  generate  faulty  mutations.  Our  technique  is  able  to  compute  a  feasible  repair  for  \(91\%\)  of  the  faults  detected  by  UPPAAL  in  the  generated  mutants.
1	Ivy  a  multi  modal  verification  tool  for  distributed  algorithms.  Ivy  is  a  multi-modal  verification  tool  for  correct  design  and  implementation  of  distributed  protocols  and  algorithms,  supporting  modular  specification,  implementation  and  proof.  Ivy  supports  proving  safety  and  liveness  properties  of  parameterized  and  infinite-state  systems  via  three  modes:  deductive  verification  using  an  SMT  solver,  abstraction  and  model  checking,  and  manual  proofs  using  natural  deduction.  It  supports  light-weight  formal  methods  via  compositional  specification-based  testing  and  bounded  model  checking.  Ivy  can  extract  executable  distributed  programs  by  translation  to  efficient  C++  code.  It  is  designed  to  support  decidable  automated  reasoning,  to  improve  proof  stability  and  to  provide  transparency  in  the  case  of  proof  failures.  For  this  purpose,  it  presents  concrete  finite  counterexamples,  automatically  audits  proofs  for  decidability  of  verification  conditions,  and  provides  modular  hiding  of  theories.
1	Mmint  a  a  tool  for  automated  change  impact  assessment  on  assurance  cases.  Assurance  cases  are  a  means  to  argue  about  the  safety,  security,  etc.,  of  software  systems  in  critical  domains.  As  systems  evolve,  their  assurance  cases  can  grow  in  complexity,  making  them  difficult  to  maintain.  In  this  paper,  we  present  a  tool  MMINT-A  that  can,  in  the  context  of  model-driven  development,  assess  the  impact  of  system  changes  on  their  assurance  cases.  To  achieve  this,  MMINT-A  implements  an  impact  assessment  algorithm  from  previous  work  [7,  8]  and  incorporates  a  graphical  assurance  case  editor,  an  annotation  mechanism,  and  two  summary  tables  for  the  assessment  results.  We  demonstrate  the  usage  of  MMINT-A  on  a  Power  Sliding  Door  example  from  the  automotive  domain.
1	Automated  anomaly  detection  in  cps  log  files.  When  Cyber-Physical  Systems  (CPS)  work  incorrectly  we  would  like  to  know  the  reason  for  this  behavior.  Experts  inspect  log  files  of  CPS  to  get  an  idea  about  what  went  wrong.  The  large  amount  of  information,  which  is  stored  in  those  log  files,  and  the  complexity  of  CPS  pose  a  challenge  to  experts  that  try  to  manually  detect  anomalies  in  the  system’s  behavior.  We  propose  to  automate  anomaly  detection  in  CPS  log  files  by  applying  a  clustering  approach  to  find  time  spans,  in  which  the  regarded  system  behaves  abnormal.  With  our  approach,  we  aim  to  significantly  reduce  the  time  and  effort  that  is  needed  by  experts  to  discover  anomalies  in  the  log  files  without  having  to  build  a  model  of  the  system  first.  The  results  from  our  evaluation  show  that  our  generic  approach  can  effectively  find  anomalies  for  different  types  of  CPS.
1	Just  enough  formality  in  assurance  argument  structures.  Safety  assurance  cases  (ACs)  are  structured  arguments  that  assert  the  safety  of  cyber-physical  systems.  ACs  use  reasoning  steps,  or  strategies,  to  show  how  a  safety  claim  is  decomposed  into  subclaims  which  are  then  supported  by  evidence.  In  practice,  ACs  are  informal,  and  thus  it  is  difficult  to  check  whether  these  decompositions  are  valid  and  no  subclaims  are  missed.  This  may  lead  to  the  approval  of  fallacious  safety  arguments  and  thus  the  deployment  of  unsafe  systems.  Fully  formalizing  ACs  to  facilitate  rigorous  evaluation  is  not  realistic  due  to  the  complexity  of  creating  and  comprehending  such  ACs.  We  take  an  intermediate  approach  by  formalizing  several  types  of  decomposition  strategies,  proving  the  conditions  under  which  they  are  deductive,  and  applying  them  as  templates  that  guard  against  common  errors  in  ACs.  We  demonstrate  our  approach  on  two  scenarios:  creation  of  ACs  with  deductive  reasoning  steps  and  evaluation  and  improvement  of  existing  ACs.
1	Repeatedly  executed  method  viewer  for  efficient  visualization  of  execution  paths  and  states  in  java.  The  state  of  a  program  at  runtime  is  useful  information  for  developers  to  understand  a  program.  Omniscient  debugging  and  logging-based  tools  enable  developers  to  investigate  the  state  of  a  program  at  an  arbitrary  point  of  time  in  an  execution.  While  these  tools  are  effective  to  analyze  the  state  at  a  single  point  of  time,  they  might  be  insufficient  to  understand  the  generic  behavior  of  a  method  which  includes  various  control-flow  paths.  In  this  paper,  we  propose  REMViewer  (Repeatedly-Executed-Method  Viewer),  or  a  tool  that  visualizes  multiple  execution  paths  of  a  Java  method.  The  tool  shows  each  execution  path  in  a  separated  view  so  that  developers  can  firstly  select  actual  execution  paths  of  interest  and  then  compare  the  state  of  local  variables  in  the  paths.
1	Developer  related  factors  in  change  prediction  an  empirical  assessment.  Predicting  the  areas  of  the  source  code  having  a  higher  likelihood  to  change  in  the  future  is  a  crucial  activity  to  allow  developers  to  plan  preventive  maintenance  operations  such  as  refactoring  or  peer-code  reviews.  In  the  past  the  research  community  was  active  in  devising  change  prediction  models  based  on  structural  metrics  extracted  from  the  source  code.  More  recently,  Elish  et  al.  showed  how  evolution  metrics  can  be  more  efficient  for  predicting  change-prone  classes.  In  this  paper,  we  aim  at  making  a  further  step  ahead  by  investigating  the  role  of  different  developer-related  factors,  which  are  able  to  capture  the  complexity  of  the  development  process  under  different  perspectives,  in  the  context  of  change  prediction.  We  also  compared  such  models  with  existing  change-prediction  models  based  on  evolution  and  code  metrics.  Our  findings  reveal  the  capabilities  of  developer-based  metrics  in  identifying  classes  of  a  software  system  more  likely  to  be  changed  in  the  future.  Moreover,  we  observed  interesting  complementarities  among  the  experimented  prediction  models,  that  may  possibly  lead  to  the  definition  of  new  combined  models  exploiting  developer-related  factors  as  well  as  product  and  evolution  metrics.
1	An  exploratory  study  on  the  relationship  between  changes  and  refactoring.  Refactoring  aims  at  improving  the  internal  structure  of  a  software  system  without  changing  its  external  behavior.  Previous  studies  empirically  assessed,  on  the  one  hand,  the  benefits  of  refactoring  in  terms  of  code  quality  and  developers'  productivity,  and  on  the  other  hand,  the  underlying  reasons  that  push  programmers  to  apply  refactoring.  Results  achieved  in  the  latter  investigations  indicate  that  besides  personal  motivation  such  as  the  responsibility  concerned  with  code  authorship,  refactoring  is  mainly  performed  as  a  consequence  of  changes  in  the  requirements  rather  than  driven  by  software  quality.  However,  these  findings  have  been  derived  by  surveying  developers,  and  therefore  no  software  repository  study  has  been  carried  out  to  corroborate  the  achieved  findings.  To  bridge  this  gap,  we  provide  a  quantitative  investigation  on  the  relationship  between  different  types  of  code  changes  (i.e.,  Fault  Repairing  Modification,  Feature  Introduction  Modification,  and  General  Maintenance  Modification)  and  28  different  refactoring  types  coming  from  3  open  source  projects.  Results  showed  that  developers  tend  to  apply  a  higher  number  of  refactoring  operations  aimed  at  improving  maintainability  and  comprehensibility  of  the  source  code  when  fixing  bugs.  Instead,  when  new  features  are  implemented,  more  complex  refactoring  operations  are  performed  to  improve  code  cohesion.  Most  of  the  times,  the  underlying  reasons  behind  the  application  of  such  refactoring  operations  are  represented  by  the  presence  of  duplicate  code  or  previously  introduced  self-admitted  technical  debts.
1	An  empirical  study  on  the  efficiency  of  graphical  vs  textual  representations  in  requirements  comprehension.  Graphical  representations  are  used  to  visualise,  specify,  and  document  software  artifacts  in  all  stages  of  software  development  process.  In  contrast  with  text,  graphical  representations  are  presented  in  two-dimensional  form,  which  seems  easy  to  process.  However,  few  empirical  studies  investigated  the  efficiency  of  graphical  representations  vs.  textual  ones  in  modelling  and  presenting  software  requirements.  Therefore,  in  this  paper,  we  report  the  results  of  an  eye-tracking  experiment  involving  28  participants  to  study  the  impact  of  structured  textual  vs.  graphical  representations  on  subjects'  efficiency  while  performing  requirement  comprehension  tasks.  We  measure  subjects'  efficiency  in  terms  of  the  percentage  of  correct  answers  (accuracy)  and  of  the  time  and  effort  spend  to  perform  the  tasks.  We  observe  no  statistically-significant  difference  in  term  of  accuracy.  However,  our  subjects  spent  more  time  and  effort  while  working  with  the  graphical  representation  although  this  extra  time  and  effort  does  not  affect  accuracy.  Our  findings  challenge  the  general  assumption  that  graphical  representations  are  more  efficient  than  the  textual  ones  at  least  in  the  case  of  developers  not  familiar  with  the  graphical  representation.  Indeed,  our  results  emphasise  that  training  can  significantly  improve  the  efficiency  of  our  subjects  working  with  graphical  representations.  Moreover,  by  comparing  the  visual  paths  of  our  subjects,  we  observe  that  the  spatial  structure  of  the  graphical  representation  leads  our  subjects  to  follow  two  different  strategies  (top-down  vs.  bottomup)  and  subsequently  this  hierarchical  structure  helps  developers  to  ease  the  difficulty  of  model  comprehension  tasks.
1	Learning  lexical  features  of  programming  languages  from  imagery  using  convolutional  neural  networks.  We  demonstrate  the  ability  of  deep  architectures,  specifically  convolutional  neural  networks,  to  learn  and  differentiate  the  lexical  features  of  different  programming  languages  presented  in  coding  video  tutorials  found  on  the  Internet.  We  analyze  over  17,000  video  frames  containing  examples  of  Java,  Python,  and  other  textual  and  non-textual  objects.  Our  results  indicate  that  not  only  can  computer  vision  models  based  on  deep  architectures  be  taught  to  differentiate  among  programming  languages  with  over  98%  accuracy,  but  can  learn  language-specific  lexical  features  in  the  process.  This  provides  a  powerful  mechanism  for  carrying  out  program  comprehension  research  on  repositories  where  source  code  is  represented  with  imagery  rather  than  text,  while  simultaneously  avoiding  the  computational  overhead  of  optical  character  recognition.
1	Exploring  large  scale  system  similarity  using  incremental  clone  detection  and  live  scatterplots.  Incremental  clone  detection  is  designed  to  efficiently  find  only  those  clones  that  cross  between  a  previous  version  of  a  system  and  a  new  version  in  order  to  update  a  previous  clone  analysis.  If  we  instead  use  a  different  system  as  the  new  version,  then  it  can  be  used  to  find  only  those  clones  that  cross  between  two  different  systems.  Live  scatter  plots  are  a  visualization  technique  that  helps  localize  clones  quickly  using  pop-up  information  directly  from  points  in  the  scatter  plot.  In  this  paper  we  explore  how  these  two  can  be  used  together  to  rapidly  expose  and  analyze  similarities  between  two  different  systems  at  several  levels  of  abstraction.  Using  the  NiCad  incremental  clone  detector,  we  find  function  clones  between  two  recent  versions  of  Linux  and  FreeBSD,  analyze  the  nature  and  causes  of  some  of  these  similarities  in  detail,  and  compare  our  observations  with  the  earlier  study  of  token-sequence  clones  between  previous  versions  of  these  systems  using  CCFinder  almost  a  decade  ago.
1	A  multi  modal  transformer  based  code  summarization  approach  for  smart  contracts.  Code  comment  has  been  an  important  part  of  computer  programs,  greatly  facilitating  the  understanding  and  maintenance  of  source  code.  However,  high-quality  code  comments  are  often  unavailable  in  smart  contracts,  the  increasingly  popular  programs  that  run  on  the  blockchain.  In  this  paper,  we  propose  a  Multi-Modal  Transformer-based  (MMTrans)  code  summarization  approach  for  smart  contracts.  Specifically,  the  MMTrans  learns  the  representation  of  source  code  from  the  two  heterogeneous  modalities  of  the  Abstract  Syntax  Tree  (AST),  i.e.,  Structure-based  Traversal  (SBT)  sequences  and  graphs.  The  SBT  sequence  provides  the  global  semantic  information  of  AST,  while  the  graph  convolution  focuses  on  the  local  details.  The  MMTrans  uses  two  encoders  to  extract  both  global  and  local  semantic  information  from  the  two  modalities  respectively,  and  then  uses  a  joint  decoder  to  generate  code  comments.  Both  the  encoders  and  the  decoder  employ  the  multi-head  attention  structure  of  the  Transformer  to  enhance  the  ability  to  capture  the  long-range  dependencies  between  code  tokens.  We  build  a  dataset  with  over  300K  pairs  of  smart  contracts,  and  evaluate  the  MMTrans  on  it.  The  experimental  results  demonstrate  that  the  MMTrans  outperforms  the  state-of-the-art  baselines  in  terms  of  four  evaluation  metrics  by  a  substantial  margin,  and  can  generate  higher  quality  comments.
1	Discovering  loners  and  phantoms  in  commit  and  issue  data.  The  interlinking  of  commit  and  issue  data  has  become  a  de-facto  standard  in  software  development.  Modern  issue  tracking  systems,  such  as  JIRA,  automatically  interlink  commits  and  issues  by  the  extraction  of  identifiers  (e.g.,  issue  key)  from  commit  messages.  However,  the  conventions  for  the  use  of  interlinking  methodologies  vary  between  software  projects.  For  example,  some  projects  enforce  the  use  of  identifiers  for  every  commit  while  others  have  less  restrictive  conventions.  In  this  work,  we  introduce  a  model  called  PaLiMod  to  enable  the  analysis  of  interlinking  characteristics  in  commit  and  issue  data.  We  surveyed  15  Apache  projects  to  investigate  differences  and  commonalities  between  linked  and  non-linked  commits  and  issues.  Based  on  the  gathered  information,  we  created  a  set  of  heuristics  to  interlink  the  residual  of  non-linked  commits  and  issues.  We  present  the  characteristics  of  Loners  and  Phantoms  in  commit  and  issue  data.  The  results  of  our  evaluation  indicate  that  the  proposed  PaLiMod  model  and  heuristics  enable  an  automatic  interlinking  and  can  indeed  reduce  the  residual  of  non-linked  commits  and  issues  in  software  projects.
1	Replicating  parser  behavior  using  neural  machine  translation.  More  than  other  machine  learning  techniques,  neural  networks  have  been  shown  to  excel  at  tasks  where  humans  traditionally  outperform  computers:  recognizing  objects  in  images,  distinguishing  spoken  words  from  background  noise  or  playing  "Go".  These  are  hard  problems,  where  hand-crafting  solutions  is  rarely  feasible  due  to  their  inherent  complexity.  Higher  level  program  comprehension  is  not  dissimilar  in  nature:  while  a  compiler  or  program  analysis  tool  can  extract  certain  facts  from  (correctly  written)  code,  it  has  no  intrinsic  'understanding'  of  the  data  and  for  the  majority  of  real-world  problems,  a  human  developer  is  needed  -  for  example  to  find  and  fix  a  bug  or  to  summarize  the  bahavior  of  a  method.  We  perform  a  pilot  study  to  determine  the  suitability  of  neural  machine  translation  (NMT)  for  processing  plain-text  source  code.  We  find  that,  on  one  hand,  NMT  is  too  fragile  to  accurately  tokenize  code,  while  on  the  other  hand,  it  can  precisely  recognize  different  types  of  tokens  and  make  accurate  guesses  regarding  their  relative  position  in  the  local  syntax  tree.  Our  results  suggest  that  NMT  may  be  exploited  for  annotating  and  enriching  out-of-context  code  snippets  to  support  automated  tooling  for  code  comprehension  problems.  We  also  identify  several  challenges  in  applying  neural  networks  to  learning  from  source  code  and  determine  key  differences  between  the  application  of  existing  neural  network  models  to  source  code  instead  of  natural  language.
1	A  tracelab  based  solution  for  creating  conducting  and  sharing  feature  location  experiments.  Similarly  to  other  fields  in  software  engineering,  the  results  of  case  studies  involving  feature  location  techniques  (FLTs)  are  hard  to  reproduce,  compare,  and  generalize,  due  to  factors  such  as,  incompatibility  of  different  datasets,  lack  of  publicly  available  implementation  or  implementation  details,  or  the  use  of  different  metrics  for  evaluating  FLTs.  To  address  these  issues,  we  propose  a  solution  for  creating,  conducting,  and  sharing  experiments  in  feature  location  based  on  TraceLab,  a  framework  for  conducting  research.  We  argue  that  this  solution  would  allow  rapid  advancements  in  feature  location  research  because  it  will  enable  researchers  to  create  new  FLTs  in  the  form  of  TraceLab  templates  or  components,  and  compare  them  with  existing  ones  using  the  same  datasets  and  the  same  metrics.  In  addition,  it  will  also  allow  sharing  these  FLTs  and  experiments  within  the  research  community.  Our  proposed  solution  provides  (i)  templates  and  components  for  creating  new  FLTs  and  instantiating  existing  ones,  (ii)  datasets  that  can  be  used  as  inputs  for  these  FLTs,  and  (iii)  metrics  for  comparing  these  FLTs.  The  proposed  solution  can  be  easily  extended  with  new  FLTs  (in  the  form  of  easily  configurable  templates  and  components),  datasets,  and  metrics.
1	Efficient  and  flexible  gui  test  execution  via  test  merging.  As  a  test  suite  evolves,  it  can  accumulate  redundant  tests.  To  address  this  problem,  many  test-suite  reduction  techniques,  based  on  different  measures  of  redundancy,  have  been  developed.  A  more  subtle  problem,  that  can  also  cause  test-suite  bloat  and  that  has  not  been  addressed  by  existing  research,  is  the  accumulation  of  similar  tests.  Similar  tests  are  not  redundant  by  any  measure;  but,  they  contain  many  common  actions  that  are  executed  repeatedly,  which  over  a  large  test  suite,  can  degrade  execution  time  substantially.          We  present  a  test  merging  technique  for  GUI  tests.  Given  a  test  suite,  the  technique  identifies  the  tests  that  can  be  merged  and  creates  a  merged  test,  which  covers  all  the  application  states  that  are  exercised  individually  by  the  tests,  but  with  the  redundant  common  steps  executed  only  once.  The  key  novelty  in  the  merging  technique  is  that  it  compares  the  dynamic  states  induced  by  the  tests  to  identify  a  semantically  meaningful  interleaving  of  steps  from  different  tests.  The  technique  not  only  improves  the  efficiency  of  test  execution,  but  also  ensures  that  there  is  no  loss  in  the  fault-revealing  power  of  the  original  tests.  In  the  empirical  studies,  conducted  using  four  open-source  web  applications  and  one  proprietary  enterprise  web  application,  in  which  over  $3300$  test  cases  and  19600  test  steps  were  analyzed,  the  technique  reduced  the  number  of  test  steps  by  29%  and  the  test-execution  time  by  39%.
1	Efficient  sensitivity  resistant  binary  instrumentation.  Binary  instrumentation  allows  users  to  inject  new  code  into  programs  without  requiring  source  code,  symbols,  or  debugging  information.  Instrumenting  a  binary  requires  structural  modifications  such  as  moving  code,  adding  new  code,  and  overwriting  existing  code;  these  modifications  may  unintentionally  change  the  program's  semantics.  Binary  instrumenters  attempt  to  preserve  the  intended  semantics  of  the  program  by  further  transforming  the  code  to  compensate  for  these  structural  modifications.  Current  instrumenters  may  fail  to  correctly  preserve  program  semantics  or  impose  significant  unnecessary  compensation  cost  because  they  lack  a  formal  model  of  the  impact  of  their  structural  modifications  on  program  semantics.  These  weaknesses  are  particularly  acute  when  instrumenting  highly  optimized  or  malicious  code,  making  current  instrumenters  less  useful  as  tools  in  the  security  or  high-performance  domains.  We  present  a  formal  specification  of  how  the  structural  modifications  used  by  instrumentation  affect  a  binary's  visible  behavior,  and  have  adapted  the  Dyninst  binary  instrumenter  to  use  this  specification,  thereby  guaranteeing  correct  instrumentation  while  greatly  reducing  compensation  costs.  When  compared  against  the  fastest  widely  used  instrumenters  our  technique  imposed  46%  less  overhead;  furthermore,  we  can  successfully  instrument  highly  defensive  binaries  that  are  specifically  looking  for  code  patching  and  instrumentation.
1	Persuasive  prediction  of  concurrency  access  anomalies.  Predictive  analysis  is  a  powerful  technique  that  exposes  concurrency  bugs  in  un-exercised  program  executions.  However,  current  predictive  analysis  approaches  lack  the  persuasiveness  property  as  they  offer  little  assistance  in  helping  programmers  fully  understand  the  execution  history  that  triggers  the  predicted  bugs.  We  present  a  persuasive  bug  prediction  technique  as  well  as  a  prototype  tool,  PECAN,  for  detecting  general  access  anomalies  (AAs)  in  concurrent  programs.  The  main  characteristic  of  PECAN  is  that,  in  addition  to  predict  AAs  in  a  more  general  way,  it  generates  "bug  hatching  clips"  that  deterministically  instruct  the  input  program  to  exercise  the  predicted  AAs.  The  key  ingredient  of  PECAN  is  an  efficient  offline  schedule  generation  algorithm,  with  proof  of  the  soundness,  that  guarantees  to  generate  a  feasible  schedule  for  every  real  AA  in  programs  that  use  locks  in  a  nested  way.  We  evaluate  PECAN  using  twenty-two  multi-threaded  subjects  including  six  large  concurrent  systems,  and  our  experiments  demonstrate  that  PECAN  is  able  to  effectively  predict  and  deterministically  expose  real  AAs.  Several  serious  and  previously  unknown  bugs  in  large  open  source  concurrent  systems  were  also  revealed  in  our  experiments.
1	Practical  program  repair  via  bytecode  mutation.  Automated  Program  Repair  (APR)  is  one  of  the  most  recent  advances  in  automated  debugging,  and  can  directly  fix  buggy  programs  with  minimal  human  intervention.  Although  various  advanced  APR  techniques  (including  search-based  or  semantic-based  ones)  have  been  proposed,  they  mainly  work  at  the  source-code  level  and  it  is  not  clear  how  bytecode-level  APR  performs  in  practice.  Also,  empirical  studies  of  the  existing  techniques  on  bugs  beyond  what  has  been  reported  in  the  original  papers  are  rather  limited.  In  this  paper,  we  implement  the  first  practical  bytecode-level  APR  technique,  PraPR,  and  present  the  first  extensive  study  on  fixing  real-world  bugs  (e.g.,  Defects4J  bugs)  using  JVM  bytecode  mutation.  The  experimental  results  show  that  surprisingly  even  PraPR  with  only  the  basic  traditional  mutators  can  produce  genuine  fixes  for  17  bugs;  with  simple  additional  commonly  used  APR  mutators,  PraPR  is  able  to  produce  genuine  fixes  for  43  bugs,  significantly  outperforming  state-of-the-art  APR,  while  being  over  10X  faster.  Furthermore,  we  performed  an  extensive  study  of  PraPR  and  other  recent  APR  tools  on  a  large  number  of  additional  real-world  bugs,  and  demonstrated  the  overfitting  problem  of  recent  advanced  APR  tools  for  the  first  time.  Lastly,  PraPR  has  also  successfully  fixed  bugs  for  other  JVM  languages  (e.g.,  for  the  popular  Kotlin  language),  indicating  PraPR  can  greatly  complement  existing  source-code-level  APR.
1	Deephunter  a  coverage  guided  fuzz  testing  framework  for  deep  neural  networks.  The  past  decade  has  seen  the  great  potential  of  applying  deep  neural  network  (DNN)  based  software  to  safety-critical  scenarios,  such  as  autonomous  driving.  Similar  to  traditional  software,  DNNs  could  exhibit  incorrect  behaviors,  caused  by  hidden  defects,  leading  to  severe  accidents  and  losses.  In  this  paper,  we  propose  DeepHunter,  a  coverage-guided  fuzz  testing  framework  for  detecting  potential  defects  of  general-purpose  DNNs.  To  this  end,  we  first  propose  a  metamorphic  mutation  strategy  to  generate  new  semantically  preserved  tests,  and  leverage  multiple  extensible  coverage  criteria  as  feedback  to  guide  the  test  generation.  We  further  propose  a  seed  selection  strategy  that  combines  both  diversity-based  and  recency-based  seed  selection.  We  implement  and  incorporate  5  existing  testing  criteria  and  4  seed  selection  strategies  in  DeepHunter.  Large-scale  experiments  demonstrate  that  (1)  our  metamorphic  mutation  strategy  is  useful  to  generate  new  valid  tests  with  the  same  semantics  as  the  original  seed,  by  up  to  a  98%  validity  ratio;  (2)  the  diversity-based  seed  selection  generally  weighs  more  than  recency-based  seed  selection  in  boosting  the  coverage  and  in  detecting  defects;  (3)  DeepHunter  outperforms  the  state  of  the  arts  by  coverage  as  well  as  the  quantity  and  diversity  of  defects  identified;  (4)  guided  by  corner-region  based  criteria,  DeepHunter  is  useful  to  capture  defects  during  the  DNN  quantization  for  platform  migration.
1	Efficient  mutation  analysis  by  propagating  and  partitioning  infected  execution  states.  Mutation  analysis  evaluates  a  testing  technique  by  measur-  ing  how  well  it  detects  seeded  faults  (mutants).  Mutation  analysis  is  hampered  by  inherent  scalability  problems  —  a  test  suite  is  executed  for  each  of  a  large  number  of  mutants.  Despite  numerous  optimizations  presented  in  the  literature,  this  scalability  issue  remains,  and  this  is  one  of  the  reasons  why  mutation  analysis  is  hardly  used  in  practice.  Whereas  most  previous  optimizations  attempted  to  stati-  cally  reduce  the  number  of  executions  or  their  computational  overhead,  this  paper  exploits  information  available  only  at  run  time  to  further  reduce  the  number  of  executions.  First,  state  infection  conditions  can  reveal  —  with  a  single  test  execution  of  the  unmutated  program  —  which  mutants  would  lead  to  a  different  state,  thus  avoiding  unnecessary  test  executions.  Second,  determining  whether  an  infected  execution  state  propagates  can  further  reduce  the  number  of  executions.  Mutants  that  are  embedded  in  compound  expressions  may  infect  the  state  locally  without  affecting  the  outcome  of  the  compound  expression.  Third,  those  mutants  that  do  infect  the  state  can  be  partitioned  based  on  the  resulting  infected  state  —  if  two  mutants  lead  to  the  same  infected  state,  only  one  needs  to  be  executed  as  the  result  of  the  other  can  be  inferred.  We  have  implemented  these  optimizations  in  the  Major  mu-  tation  framework  and  empirically  evaluated  them  on  14  open  source  programs.  The  optimizations  reduced  the  mutation  analysis  time  by  40%  on  average.
1	Abstracting  path  conditions.  We  present  a  symbolic-execution-based  algorithm  that  for  a  given  program  and  a  given  program  location  in  it  produces  a  nontrivial  necessary  condition  on  input  values  to  drive  the  program  execution  to  the  given  location.  The  algorithm  is  based  on  computation  of  loop  summaries  for  loops  along  acyclic  paths  leading  to  the  target  location.  We  also  propose  an  application  of  necessary  conditions  in  contemporary  bug-finding  and  test-generation  tools.  Experimental  results  on  several  small  benchmarks  show  that  the  presented  technique  can  in  some  cases  significantly  improve  performance  of  the  tools.
1	Assessing  the  state  and  improving  the  art  of  parallel  testing  for  c.  The  execution  latency  of  a  test  suite  strongly  depends  on  the  degree  of  concurrency  with  which  test  cases  are  executed.  However,  if  test  cases  are  not  designed  for  concurrent  execution,  they  may  interfere,  causing  result  deviations  compared  to  sequential  execution.  To  prevent  this,  each  test  case  can  be  provided  with  an  isolated  execution  environment,  but  the  resulting  overheads  diminish  the  merit  of  parallel  testing.  Our  large-scale  analysis  of  the  Debian  Buster  package  repository  shows  that  existing  test  suites  in  C  projects  make  limited  use  of  parallelization.  We  present  an  approach  to  (a)  analyze  the  potential  of  C  test  suites  for  safe  concurrent  execution,  i.e.,  result  invariance  compared  to  sequential  execution,  and  (b)  execute  tests  concurrently  with  different  parallelization  strategies  using  processes  or  threads  if  it  is  found  to  be  safe.  Applying  our  approach  to  9  C  projects,  we  find  that  most  of  them  cannot  safely  execute  tests  in  parallel  due  to  unsafe  test  code  or  unsafe  usage  of  shared  variables  or  files  within  the  program  code.  Parallel  test  execution  shows  a  significant  acceleration  over  sequential  execution  for  most  projects.  We  find  that  multi-threading  rarely  outperforms  multi-processing.  Finally,  we  observe  that  the  lack  of  a  common  test  framework  for  C  leaves  make  as  the  standard  driver  for  running  tests,  which  introduces  unnecessary  performance  overheads  for  test  execution.
1	Griffin  grouping  suspicious  memory  access  patterns  to  improve  understanding  of  concurrency  bugs.  This  paper  presents  Griffin,  a  new  fault-comprehension  technique.  Griffin  provides  a  way  to  explain  concurrency  bugs  using  additional  information  over  existing  fault-localization  techniques,  and  thus,  bridges  the  gap  between  fault-  localization  and  fault-fixing  techniques.  Griffin  inputs  a  list  of  memory-access  patterns  and  a  coverage  matrix,  groups  those  patterns  responsible  for  the  same  concurrency  bug,  and  outputs  the  grouped  patterns  along  with  suspicious  methods  and  bug  graphs.  Griffin  is  the  first  technique  that  handles  multiple  concurrency  bugs.  This  paper  also  describes  the  implementation  of  Griffin  in  Java  and  C++,  and  shows  the  empirical  evaluation  of  Griffin  on  a  set  of  subjects.  The  results  show  that,  for  our  subjects,  Griffin  clusters  failing  executions  and  memory-access  patterns  for  the  same  bug  with  few  false  positives,  provides  suspicious  methods  that  contain  the  locations  to  be  fixed,  and  runs  efficiently.
1	Analysis  of  performance  regression  testing  data  by  transaction  profiles.  Performance  regression  testing  is  an  important  step  in  the  software  development  lifecycle,  especially  for  enterprise  applications.  Commonly  the  analysis  of  performance  regression  testing  to  find  anomalies  is  carried  out  manually  and  therefore  can  be  error-prone,  time  consuming  and  sensitive  to  the  input  load.  In  our  research,  we  propose  a  new  technique  that  overcomes  the  above  problems  which  helps  the  performance  testing  teams  to  improve  their  process  and  speeds  up  the  entire  software  production  process.
1	Testing  concurrent  programs  on  relaxed  memory  models.  High-performance  concurrent  libraries,  such  as  lock-free  data  structures  and  custom  synchronization  primitives,  are  notoriously  difficult  to  write  correctly.  Such  code  is  often  implemented  without  locks,  instead  using  plain  loads  and  stores  and  low-level  operations  like  atomic  compare-and-swaps  and  explicit  memory  fences.  Such  code  must  run  correctly  despite  the  relaxed  memory  model  of  the  underlying  compiler,  virtual  machine,  and/or  hardware.  These  memory  models  may  reorder  the  reads  and  writes  issued  by  a  thread,  greatly  complicating  parallel  reasoning.      We  propose  Relaxer,  a  combination  of  predictive  dynamic  analysis  and  software  testing,  to  help  programmers  write  correct,  highly-concurrent  programs.  Our  technique  works  in  two  phases.  First,  Relaxer  examines  a  sequentially-consistent  run  of  a  program  under  test  and  dynamically  detects  potential  data  races.  These  races  are  used  to  predict  possible  violations  of  sequential  consistency  under  alternate  executions  on  a  relaxed  memory  model.  In  the  second  phase,  Relaxer  re-executes  the  program  with  a  biased  random  scheduler  and  with  a  conservative  simulation  of  a  relaxed  memory  model  in  order  to  create  with  high  probability  a  predicted  sequential  consistency  violation.  These  executions  can  be  used  to  test  whether  or  not  a  program  works  as  expected  when  the  underlying  memory  model  is  not  sequentially  consistent.      We  have  implemented  Relaxer  for  C  and  have  evaluated  it  on  several  synchronization  algorithms,  concurrent  data  structures,  and  parallel  applications.  Relaxer  generates  many  executions  of  these  benchmarks  with  violations  of  sequential  consistency,  highlighting  a  number  of  bugs  under  relaxed  memory  models.
1	Predictive  mutation  testing.  Mutation  testing  is  a  powerful  methodology  for  evaluating  test  suite  quality.  In  mutation  testing,  a  large  number  of  mutants  are  generated  and  executed  against  the  test  suite  to  check  the  ratio  of  killed  mutants.  Therefore,  mutation  testing  is  widely  believed  to  be  a  computationally  expensive  technique.  To  alleviate  the  efficiency  concern  of  mutation  testing,  in  this  paper,  we  propose  predictive  mutation  testing  (PMT),  the  first  approach  to  predicting  mutation  testing  results  without  mutant  execution.  In  particular,  the  proposed  approach  constructs  a  classification  model  based  on  a  series  of  features  related  to  mutants  and  tests,  and  uses  the  classification  model  to  predict  whether  a  mutant  is  killed  or  survived  without  executing  it.  PMT  has  been  evaluated  on  163  real-world  projects  under  two  application  scenarios  (i.e.,  cross-version  and  cross-project).  The  experimental  results  demonstrate  that  PMT  improves  the  efficiency  of  mutation  testing  by  up  to  151.4X  while  incurring  only  a  small  accuracy  loss  when  predicting  mutant  execution  results,  indicating  a  good  tradeoff  between  efficiency  and  effectiveness  of  mutation  testing.
1	Change  aware  preemption  prioritization.  Successful  software  evolves  as  developers  add  more  features,  respond  to  requirements  changes,  and  fix  faults.  Regression  testing  is  widely  used  for  ensuring  the  validity  of  evolving  software.  As  regression  test  suites  grow  over  time,  it  becomes  expensive  to  execute  them.  The  problem  is  exacerbated  when  test  suites  contain  multithreaded  tests.  These  tests  are  generally  long  running  as  they  explore  many  different  thread  schedules  searching  for  concurrency  faults  such  as  dataraces,  atomicity  violations,  and  deadlocks.  While  many  techniques  have  been  proposed  for  regression  test  prioritization,  selection,  and  minimization  for  sequential  tests,  there  is  not  much  work  for  multithreaded  code.      We  present  a  novel  technique,  called  Change-Aware  Preemption  Prioritization  (CAPP),  that  uses  information  about  the  changes  in  software  evolution  to  prioritize  the  exploration  of  schedules  in  a  multithreaded  regression  test.  We  have  implemented  CAPP  in  two  frameworks  for  systematic  exploration  of  multithreaded  Java  code.  We  evaluated  CAPP  on  the  detection  of  15  faults  in  multithreaded  Java  programs,  including  large  open-source  programs.  The  results  show  that  CAPP  can  substantially  reduce  the  exploration  required  to  detect  multithreaded  regression  faults.
1	Automatically  repairing  broken  workflows  for  evolving  gui  applications.  A  workflow  is  a  sequence  of  UI  actions  to  complete  a  specific  task.  In  the  course  of  a  GUI  application's  evolution,  changes  ranging  from  a  simple  GUI  refactoring  to  a  complete  rearchitecture  can  break  an  end-user's  well-established  workflow.  It  can  be  challenging  to  find  a  replacement  workflow.  To  address  this  problem,  we  present  a  technique  (and  its  tool  implementation,  called  FlowFixer)  that  repairs  a  broken  workflow.  FlowFixer  uses  dynamic  profiling,  static  analysis,  and  random  testing  to  suggest  a  replacement  UI  action  that  fixes  a  broken  workflow.          We  evaluated  FlowFixer  on  16  broken  workflows  from  5  realworld  GUI  applications  written  in  Java.  In  13  workflows,  the  correct  replacement  action  was  FlowFixer's  first  suggestion.  In  2  workflows,  the  correct  replacement  action  was  FlowFixer's  second  suggestion.  The  remaining  workflow  was  un-repairable.  Overall,  FlowFixer  produced  significantly  better  results  than  two  alternative  approaches.
1	Automated  testing  with  targeted  event  sequence  generation.  Automated  software  testing  aims  to  detect  errors  by  producing  test  inputs  that  cover  as  much  of  the  application  source  code  as  possible.  Applications  for  mobile  devices  are  typically  event-driven,  which  raises  the  challenge  of  automatically  producing  event  sequences  that  result  in  high  coverage.  Some  existing  approaches  use  random  or  model-based  testing  that  largely  treats  the  application  as  a  black  box.  Other  approaches  use  symbolic  execution,  either  starting  from  the  entry  points  of  the  applications  or  on  specific  event  sequences.  A  common  limitation  of  the  existing  approaches  is  that  they  often  fail  to  reach  the  parts  of  the  application  code  that  require  more  complex  event  sequences.          We  propose  a  two-phase  technique  for  automatically  finding  event  sequences  that  reach  a  given  target  line  in  the  application  code.  The  first  phase  performs  concolic  execution  to  build  summaries  of  the  individual  event  handlers  of  the  application.  The  second  phase  builds  event  sequences  backward  from  the  target,  using  the  summaries  together  with  a  UI  model  of  the  application.  Our  experiments  on  a  collection  of  open  source  Android  applications  show  that  this  technique  can  successfully  produce  event  sequences  that  reach  challenging  targets.
1	Energy  aware  test  suite  minimization  for  android  apps.  The  rising  popularity  of  mobile  apps  deployed  on  battery-constrained  devices  has  motivated  the  need  for  effective  energy-aware  testing  techniques.  Energy  testing  is  generally  more  labor  intensive  and  expensive  than  functional  testing,  as  tests  need  to  be  executed  in  the  deployment  environment  and  specialized  equipment  needs  to  be  used  to  collect  energy  measurements.  Currently,  there  is  a  dearth  of  automatic  mobile  testing  techniques  that  consider  energy  as  a  program  property  of  interest.  This  paper  presents  an  energy-aware  test-suite  minimization  approach  to  significantly  reduce  the  number  of  tests  needed  to  effectively  test  the  energy  properties  of  an  Android  app.  It  relies  on  an  energy-aware  coverage  criterion  that  indicates  the  degree  to  which  energy-greedy  segments  of  a  program  are  tested.  We  describe  and  evaluate  two  complementary  algorithms  for  test-suite  minimization.  Experiments  over  test  suites  provided  for  real-world  apps  have  corroborated  our  ability  to  reduce  the  test  suite  size  by  84%  on  average,  while  maintaining  the  effectiveness  of  test  suite  in  revealing  the  great  majority  of  energy  bugs.
1	Optimizing  monitoring  of  finite  state  properties  through  monitor  compaction.  Runtime  monitoring  has  proven  effective  in  detecting  property  violations,  but  it  can  incur  high  overhead  when  monitoring  just  a  single  property  -  particularly  when  the  property  relates  multiple  objects.  In  practice  developers  will  likely  monitor  multiple  properties  in  the  same  execution  which  will  lead  to  even  higher  overhead.          This  paper  presents  the  first  study  of  overhead  arising  during  the  simultaneous  monitoring  of  multiple  properties.  We  present  two  techniques  for  mitigating  overhead  in  such  cases  that  exploit  reductions  in  cost  that  arise  from  sharing  of  information  between  property  monitors.  This  sharing  permits  a  single  monitor  to  function  in  place  of  many  monitors.  We  evaluate  these  techniques  on  the  DaCapo  benchmarks  and  8  temporal  correctness  properties  and  find  that  they  offer  significant  overhead  reductions,  as  high  as  57%,  as  the  number  of  monitored  properties  increases.
1	Regression  mutation  testing.  Mutation  testing  is  one  of  the  most  powerful  approaches  for  evaluating  quality  of  test  suites.  However,  mutation  testing  is  also  one  of  the  most  expensive  testing  approaches.  This  paper  presents  Regression  Mutation  Testing  (ReMT),  a  new  technique  to  speed  up  mutation  testing  for  evolving  systems.  The  key  novelty  of  ReMT  is  to  incrementally  calculate  mutation  testing  results  for  the  new  program  version  based  on  the  results  from  the  old  program  version;  ReMT  uses  a  static  analysis  to  check  which  results  can  be  safely  reused.  ReMT  also  employs  a  mutation-specific  test  prioritization  to  further  speed  up  mutation  testing.  We  present  an  empirical  study  on  six  evolving  systems,  whose  sizes  range  from  3.9KLoC  to  88.8KLoC.  The  empirical  results  show  that  ReMT  can  substantially  reduce  mutation  testing  costs,  indicating  a  promising  future  for  applying  mutation  testing  on  evolving  software  systems.
1	Generating  parameterized  unit  tests.  State-of-the  art  techniques  for  automated  test  generation  focus  on  generating  executions  that  cover  program  behavior.  As  they  do  not  generate  oracles,  it  is  up  to  the  developer  to  figure  out  what  a  test  does  and  how  to  check  the  correctness  of  the  observed  behavior.  In  this  paper,  we  present  an  approach  to  generate  parameterized  unit  tests---unit  tests  containing  symbolic  pre-  and  postconditions  characterizing  test  input  and  test  result.  Starting  from  concrete  inputs  and  results,  we  use  test  generation  and  mutation  to  systematically  generalize  pre-  and  postconditions  while  simplifying  the  computation  steps.  Evaluated  on  five  open  source  libraries,  the  generated  parameterized  unit  tests  are  (a)  more  expressive,  characterizing  general  rather  than  concrete  behavior;  (b)  need  fewer  computation  steps,  making  them  easier  to  understand;  and  (c)  achieve  a  higher  coverage  than  regular  unit  tests.
1	Semi  valid  input  coverage  for  fuzz  testing.  We  define  semi-valid  input  coverage  (SVCov),  the  first  coverage  criterion  for  fuzz  testing.  Our  criterion  is  applicable  whenever  the  valid  inputs  can  be  defined  by  a  finite  set  of  constraints.  SVCov  measures  to  what  extent  the  tests  cover  the  domain  of  semi-valid  inputs,  where  an  input  is  semi-valid  if  and  only  if  it  satisfies  all  the  constraints  but  one.          We  demonstrate  SVCov's  practical  value  in  a  case  study  on  fuzz  testing  the  Internet  Key  Exchange  protocol  (IKE).  Our  study  shows  that  it  is  feasible  to  precisely  define  and  efficiently  measure  SVCov.  Moreover,  SVCov  provides  essential  information  for  improving  the  effectiveness  of  fuzz  testing  and  enhancing  fuzz-testing  tools  and  libraries.  In  particular,  by  increasing  coverage  under  SVCov,  we  have  discovered  a  previously  unknown  vulnerability  in  a  mature  IKE  implementation.
1	Efficiently  scripting  change  resilient  tests.  In  industrial  practice,  test  cases  often  start  out  as  steps  described  in  natural  language  and  are  intended  to  be  executed  by  a  human.  Since  tests  are  executed  repeatedly,  they  go  through  an  automation  process,  in  which  they  are  converted  to  automated  test  scripts  (or  programs)  that  perform  the  test  steps  mechanically.  Conventional  test-automation  techniques  can  be  time-consuming,  require  specialized  skills,  and  can  produce  fragile  scripts.  To  address  these  limitations,  we  present  a  tool,  called  ata,  for  automating  the  test-automation  task.  Using  a  novel  combination  of  natural-language  processing,  backtracking  exploration,  and  learning,  ata  can  significantly  improve  tester  productivity  in  automating  manual  tests.  ata  also  produces  change-resilient  scripts,  which  automatically  adapt  themselves  in  the  presence  of  certain  common  types  of  user-interface  changes.
1	An  empirical  analysis  of  the  co  evolution  of  schema  and  code  in  database  applications.  Modern  database  applications  are  among  the  most  widely  used  and  complex  software  systems.  They  constantly  evolve,  responding  to  changes  to  data,  database  schemas,  and  code.  It  is  challenging  to  manage  these  changes  and  ensure  that  everything  co-evolves  consistently.  For  example,  when  a  database  schema  is  modified,  all  the  code  that  interacts  with  the  database  must  be  changed  accordingly.  Although  database  evolution  and  software  evolution  have  been  extensively  studied  in  isolation,  the  co-evolution  of  schema  and  code  has  largely  been  unexplored.          This  paper  presents  the  first  comprehensive  empirical  analysis  of  the  co-evolution  of  database  schemas  and  code  in  ten  popular  large  open-source  database  applications,  totaling  over  160K  revisions.  Our  major  findings  include:  1)  Database  schemas  evolve  frequently  during  the  application  lifecycle,  exhibiting  a  variety  of  change  types  with  similar  distributions  across  the  studied  applications;  2)  Overall,  schema  changes  induce  significant  code-level  modifications,  while  certain  change  types  have  more  impact  on  code  than  others;  and  3)  Co-change  analyses  can  be  viable  to  automate  or  assist  with  database  application  evolution.  We  have  also  observed  that:  1)  80%  of  the  schema  changes  happened  in  20-30%  of  the  tables,  while  nearly  40%  of  the  tables  did  not  change;  and  2)  Referential  integrity  constraints  and  stored  procedures  are  rarely  used  in  our  studied  subjects.  We  believe  that  our  study  reveals  new  insights  into  how  database  applications  evolve  and  useful  guidelines  for  designing  assistive  tools  to  aid  their  evolution.
1	Systematic  testing  of  asynchronous  reactive  systems.  We  introduce  the  concept  of  a  delaying  explorer  with  the  goal  of  performing  prioritized  exploration  of  the  behaviors  of  an  asynchronous  reactive  program.  A  delaying  explorer  stratifies  the  search  space  using  a  custom  strategy,  and  a  delay  operation  that  allows  deviation  from  that  strategy.  We  show  that  prioritized  search  with  a  delaying  explorer  performs  significantly  better  than  existing  prioritization  techniques.  We  also  demonstrate  empirically  the  need  for  writing  different  delaying  explorers  for  scalable  systematic  testing  and  hence,  present  a  flexible  delaying  explorer  interface.  We  introduce  two  new  techniques  to  improve  the  scalability  of  search  based  on  delaying  explorers.  First,  we  present  an  algorithm  for  stratified  exhaustive  search  and  use  efficient  state  caching  to  avoid  redundant  exploration  of  schedules.  We  provide  soundness  and  termination  guarantees  for  our  algorithm.  Second,  for  the  cases  where  the  state  of  the  system  cannot  be  captured  or  there  are  resource  constraints,  we  present  an  algorithm  to  randomly  sample  any  execution  from  the  stratified  search  space.  This  algorithm  guarantees  that  any  such  execution  that  requires  $d$  delay  operations  is  sampled  with  probability  at  least  $1/L^d$,  where  $L$  is  the  maximum  number  of  program  steps.  We  have  implemented  our  algorithms  and  evaluated  them  on  a  collection  of  real-world  fault-tolerant  distributed  protocols.
1	Omen  a  precise  dynamic  deadlock  detector  for  multithreaded  java  libraries.  Designing  thread-safe  libraries  without  concurrency  defects  can  be  a  challenging  task.  Detecting  deadlocks  while  invoking  methods  in  these  libraries  concurrently  is  hard  due  to  the  possible  number  of  method  invocation  combinations,  the  object  assignments  to  the  parameters  and  the  associated  thread  interleavings.  In  this  paper,  we  describe  the  design  and  implementation  of  OMEN+  that  takes  a  multithreaded  library  as  the  input  and  detects  true  deadlocks  in  a  scalable  manner.  We  achieve  this  by  automatically  synthesizing  relevant  multithreaded  tests  and  analyze  the  associated  execution  traces  using  a  precise  deadlock  detector.  We  validate  the  usefulness  of  OMEN+  by  applying  it  on  many  multithreaded  Java  libraries  and  detect  a  number  of  deadlocks  even  in  documented  thread-safe  libraries.  The  tool  is  available  for  free  download  at  http://www.csa.iisc.ernet.in/~sss/tool/omenplus.html.
1	Practical  static  analysis  of  javascript  applications  in  the  presence  of  frameworks  and  libraries.  JavaScript  is  a  language  that  is  widely-used  for  both  web-  based  and  standalone  applications  such  as  those  in  the  upcoming  Windows  8  operating  system.  Analysis  of  JavaScript  has  long  been  known  to  be  challenging  due  to  its  dynamic  nature.  On  top  of  that,  most  JavaScript  applications  rely  on  large  and  complex  libraries  and  frameworks,  often  written  in  a  combination  of  JavaScript  and  native  code  such  as  C  and  C++.  Stubs  have  been  commonly  employed  as  a  partial  specification  mechanism  to  address  the  library  problem;  however,  they  are  tedious  to  write,  incomplete,  and  occasionally  incorrect.          However,  the  manner  in  which  library  code  is  used  within  applications  often  sheds  light  on  what  library  APIs  return  or  consume  as  parameters.  In  this  paper,  we  propose  a  technique  which  combines  pointer  analysis  with  use  analysis  to  handle  many  challenges  posed  by  large  JavaScript  libraries.  Our  approach  enables  a  variety  of  applications,  ranging  from  call  graph  discovery  to  auto-complete  to  supporting  runtime  optimizations.  Our  techniques  have  been  implemented  and  empirically  validated  on  a  set  of  25  Windows  8  JavaScript  applications,  averaging  1,587  lines  of  code,  demonstrating  a  combination  of  scalability  and  precision.
1	Graph  based  trace  analysis  for  microservice  architecture  understanding  and  problem  diagnosis.  Microservice  systems  are  highly  dynamic  and  complex.  For  such  systems,  operation  engineers  and  developers  highly  rely  on  trace  analysis  to  understand  architectures  and  diagnose  various  problems  such  as  service  failures  and  quality  degradation.  However,  the  huge  number  of  traces  produced  at  runtime  makes  it  challenging  to  capture  the  required  information  in  real-time.  To  address  the  faced  challenges,  in  this  paper,  we  propose  a  graph-based  microservice  trace  analysis  approach  GMTA  for  understanding  architecture  and  diagnosing  various  problems.  Built  on  a  graph-based  representation,  GMTA  includes  efficient  processing  of  traces  produced  on  the  fly.  It  abstracts  traces  into  different  paths  and  further  groups  them  into  business  flows.  To  support  various  analytical  applications,  GMTA  includes  an  efficient  storage  and  access  mechanism  by  combining  a  graph  database  and  a  real-time  analytics  database  and  using  a  carefully  designed  storage  structure.  Based  on  GMTA,  we  construct  analytical  applications  for  architecture  understanding  and  problem  diagnosis,  these  applications  support  various  needs  such  as  visualizing  service  dependencies,  making  architectural  decisions,  analyzing  the  changes  of  services  behaviors,  detecting  performance  issues,  and  locating  root  causes.  GMTA  has  been  implemented  and  deployed  in  eBay.  An  experimental  study  based  on  trace  data  produced  by  eBay  demonstrates  GMTA's  effectiveness  and  efficiency  for  architecture  understanding  and  problem  diagnosis.  Case  studies  conducted  in  eBay's  monitoring  team  and  Site  Reliability  Engineering  (SRE)  team  further  confirm  GMTA's  substantial  benefits  in  industrial-scale  microservice  systems.
1	Sample  size  vs  bias  in  defect  prediction.  Most  empirical  disciplines  promote  the  reuse  and  sharing  of  datasets,  as  it  leads  to  greater  possibility  of  replication.  While  this  is  increasingly  the  case  in  Empirical  Software  Engineering,  some  of  the  most  popular  bug-fix  datasets  are  now  known  to  be  biased.  This  raises  two  significant  concerns:  first,  that  sample  bias  may  lead  to  underperforming  prediction  models,  and  second,  that  the  external  validity  of  the  studies  based  on  biased  datasets  may  be  suspect.  This  issue  has  raised  considerable  consternation  in  the  ESE  literature  in  recent  years.  However,  there  is  a  confounding  factor  of  these  datasets  that  has  not  been  examined  carefully:  size.  Biased  datasets  are  sampling  only  some  of  the  data  that  could  be  sampled,  and  doing  so  in  a  biased  fashion;  but  biased  samples  could  be  smaller,  or  larger.  Smaller  data  sets  in  general  provide  less  reliable  bases  for  estimating  models,  and  thus  could  lead  to  inferior  model  performance.  In  this  setting,  we  ask  the  question,  what  affects  performance  more,  bias,  or  size?  We  conduct  a  detailed,  large-scale  meta-analysis,  using  simulated  datasets  sampled  with  bias  from  a  high-quality  dataset  which  is  relatively  free  of  bias.  Our  results  suggest  that  size  always  matters  just  as  much  bias  direction,  and  in  fact  much  more  than  bias  direction  when  considering  information-retrieval  measures  such  as  AUCROC  and  F-score.  This  indicates  that  at  least  for  prediction  models,  even  when  dealing  with  sampling  bias,  simply  finding  larger  samples  can  sometimes  be  sufficient.  Our  analysis  also  exposes  the  complexity  of  the  bias  issue,  and  raises  further  issues  to  be  explored  in  the  future.
1	String  analysis  for  side  channels  with  segmented  oracles.  We  present  an  automated  approach  for  detecting  and  quantifying  side  channels  in  Java  programs,  which  uses  symbolic  execution,  string  analysis  and  model  counting  to  compute  information  leakage  for  a  single  run  of  a  program.  We  further  extend  this  approach  to  compute  information  leakage  for  multiple  runs  for  a  type  of  side  channels  called  segmented  oracles,  where  the  attacker  is  able  to  explore  each  segment  of  a  secret  (for  example  each  character  of  a  password)  independently.  We  present  an  efficient  technique  for  segmented  oracles  that  computes  information  leakage  for  multiple  runs  using  only  the  path  constraints  generated  from  a  single  run  symbolic  execution.  Our  implementation  uses  the  symbolic  execution  tool  Symbolic  PathFinder  (SPF),  SMT  solver  Z3,  and  two  model  counting  constraint  solvers  LattE  and  ABC.  Although  LattE  has  been  used  before  for  analyzing  numeric  constraints,  in  this  paper,  we  present  an  approach  for  using  LattE  for  analyzing  string  constraints.  We  also  extend  the  string  constraint  solver  ABC  for  analysis  of  both  numeric  and  string  constraints,  and  we  integrate  ABC  in  SPF,  enabling  quantitative  symbolic  string  analysis.
1	What  change  history  tells  us  about  thread  synchronization.  Multi-threaded  programs  are  pervasive,  yet  difficult  to  write.  Missing  proper  synchronization  leads  to  correctness  bugs  and  over  synchronization  leads  to  performance  problems.  To  improve  the  correctness  and  efficiency  of  multi-threaded  software,  we  need  a  better  understanding  of  synchronization  challenges  faced  by  real-world  developers.  This  paper  studies  the  code  repositories  of  open-source  multi-threaded  software  projects  to  obtain  a  broad  and  in-  depth  view  of  how  developers  handle  synchronizations.  We  first  examine  how  critical  sections  are  changed  when  software  evolves  by  checking  over  250,000  revisions  of  four  representative  open-source  software  projects.  The  findings  help  us  answer  questions  like  how  often  synchronization  is  an  afterthought  for  developers;  whether  it  is  difficult  for  devel-  opers  to  decide  critical  section  boundaries  and  lock  variables;  and  what  are  real-world  over-synchronization  problems.  We  then  conduct  case  studies  to  better  understand  (1)  how  critical  sections  are  changed  to  solve  performance  prob-  lems  (i.e.  over-synchronization  issues)  and  (2)  how  soft-  ware  changes  lead  to  synchronization-related  correctness  problems  (i.e.  concurrency  bugs).  This  in-depth  study  shows  that  tool  support  is  needed  to  help  developers  tackle  over-synchronization  problems;  it  also  shows  that  concur-  rency  bug  avoidance,  detection,  and  testing  can  be  improved  through  better  awareness  of  code  revision  history.
1	Patdroid  permission  aware  gui  testing  of  android.  Recent  introduction  of  a  dynamic  permission  system  in  Android,  allowing  the  users  to  grant  and  revoke  permissions  after  the  installation  of  an  app,  has  made  it  harder  to  properly  test  apps.  Since  an  app's  behavior  may  change  depending  on  the  granted  permissions,  it  needs  to  be  tested  under  a  wide  range  of  permission  combinations.  At  the  state-of-the-art,  in  the  absence  of  any  automated  tool  support,  a  developer  needs  to  either  manually  determine  the  interaction  of  tests  and  app  permissions,  or  exhaustively  re-execute  tests  for  all  possible  permission  combinations,  thereby  increasing  the  time  and  resources  required  to  test  apps.  This  paper  presents  an  automated  approach,  called  PATDroid,  for  efficiently  testing  an  Android  app  while  taking  the  impact  of  permissions  on  its  behavior  into  account.  PATDroid  performs  a  hybrid  program  analysis  on  both  an  app  under  test  and  its  test  suite  to  determine  which  tests  should  be  executed  on  what  permission  combinations.  Our  experimental  results  show  that  PATDroid  significantly  reduces  the  testing  effort,  yet  achieves  comparable  code  coverage  and  fault  detection  capability  as  exhaustively  testing  an  app  under  all  permission  combinations.
1	A  dynamic  taint  analyzer  for  distributed  systems.  As  in  other  software  domains,  information  flow  security  is  a  fundamental  aspect  of  code  security  in  distributed  systems.  However,  most  existing  solutions  to  information  flow  security  are  limited  to  centralized  software.  For  distributed  systems,  such  solutions  face  multiple  challenges,  including  technique  applicability,  tool  portability,  and  analysis  scalability.  To  overcome  these  challenges,  we  present  DistTaint,  a  dynamic  information  flow  (taint)  analyzer  for  distributed  systems.  By  partial-ordering  method-execution  events,  DistTaint  infers  implicit  dependencies  in  distributed  programs,  so  as  to  resolve  the  applicability  challenge.  It  resolves  the  portability  challenge  by  working  fully  at  application  level,  without  customizing  the  runtime  platform.  To  achieve  scalability,  it  reduces  analysis  costs  using  a  multi-phase  analysis,  where  the  pre-analysis  phase  generates  method-level  results  to  narrow  down  the  scope  of  the  following  statement-level  analysis.  We  evaluated  DistTaint  against  eight  real-world  distributed  systems.  Empirical  results  showed  DistTaint’s  applicability  to,  portability  with,  and  scalability  for  industry-scale  distributed  systems,  along  with  its  capability  of  discovering  known  and  unknown  vulnerabilities.  A  demo  video  for  DistTaint  can  be  downloaded  from  https://www.dropbox.com/l/scl/AAAkrm4p63Ffx0rZqblY3zlLFuaohbRxs0  or  viewed  here  https://youtu.be/fy4yMIaKzPE  online.  The  tool  package  is  here:  https://www.dropbox.com/sh/kfr9ixucyny1jp2/AAC00aI-I8Od4ywZCqwZ1uaa?dl=0
1	Adaptively  generating  high  quality  fixes  for  atomicity  violations.  It  is  difficult  to  fix  atomicity  violations  correctly.  Existing  gate  lock  algorithm  (GLA)  simply  inserts  gate  locks  to  serialize  exe-cutions,  which  may  introduce  performance  bugs  and  deadlocks.  Synthesized  context-aware  gate  locks  (by  Grail)  require  complex  source  code  synthesis.  We  propose  Fixer  to  adaptively  fix  ato-micity  violations.  It  firstly  analyses  the  lock  acquisitions  of  an  atomicity  violation.  Then  it  either  adjusts  the  existing  lock  scope  or  inserts  a  gate  lock.  The  former  addresses  cases  where  some  locks  are  used  but  fail  to  provide  atomic  accesses.  For  the  latter,  it  infers  the  visibility  (being  global  or  a  field  of  a  class/struct)  of  the  gate  lock  such  that  the  lock  only  protects  related  accesses.  For  both  cases,  Fixer  further  eliminates  new  lock  orders  to  avoid  introducing  deadlocks.  Of  course,  Fixer  can  produce  both  kinds  of  fixes  on  atomicity  violations  with  locks.  The  experi-mental  results  on  15  previously  used  atomicity  violations  show  that:  Fixer  correctly  fixed  all  15  atomicity  violations  without  introducing  deadlocks.  However,  GLA  and  Grail  both  intro-duced  5  deadlocks.  HFix  (that  only  targets  on  fixing  certain  types  of  atomicity  violations)  only  fixed  2  atomicity  violations  and  introduced  4  deadlocks.  Fixer  also  provides  an  alternative  way  to  insert  gate  locks  (by  inserting  gate  locks  with  proper  visibility)  considering  fix  acceptance.
1	Flexjava  language  support  for  safe  and  modular  approximate  programming.  Energy  efficiency  is  a  primary  constraint  in  modern  systems.  Approximate  computing  is  a  promising  approach  that  trades  quality  of  result  for  gains  in  efficiency  and  performance.  State-  of-the-art  approximate  programming  models  require  extensive  manual  annotations  on  program  data  and  operations  to  guarantee  safe  execution  of  approximate  programs.  The  need  for  extensive  manual  annotations  hinders  the  practical  use  of  approximation  techniques.  This  paper  describes  FlexJava,  a  small  set  of  language  extensions,  that  significantly  reduces  the  annotation  effort,  paving  the  way  for  practical  approximate  programming.  These  extensions  enable  programmers  to  annotate  approximation-tolerant  method  outputs.  The  FlexJava  compiler,  which  is  equipped  with  an  approximation  safety  analysis,  automatically  infers  the  operations  and  data  that  affect  these  outputs  and  selectively  marks  them  approximable  while  giving  safety  guarantees.  The  automation  and  the  language–compiler  codesign  relieve  programmers  from  manually  and  explicitly  an-  notating  data  declarations  or  operations  as  safe  to  approximate.  FlexJava  is  designed  to  support  safety,  modularity,  generality,  and  scalability  in  software  development.  We  have  implemented  FlexJava  annotations  as  a  Java  library  and  we  demonstrate  its  practicality  using  a  wide  range  of  Java  applications  and  by  con-  ducting  a  user  study.  Compared  to  EnerJ,  a  recent  approximate  programming  system,  FlexJava  provides  the  same  energy  savings  with  significant  reduction  (from  2×  to  17×)  in  the  number  of  annotations.  In  our  user  study,  programmers  spend  6×  to  12×  less  time  annotating  programs  using  FlexJava  than  when  using  EnerJ.
1	Path  exploration  based  on  symbolic  output.  Efficient  program  path  exploration  is  important  for  many  software  engineering  activities  such  as  testing,  debugging  and  verification.  However,  enumerating  all  paths  of  a  program  is  prohibitively  expensive.  In  this  paper,  we  develop  a  partitioning  of  program  paths  based  on  the  program  output.  Two  program  paths  are  placed  in  the  same  partition  if  they  derive  the  output  similarly,  that  is,  the  symbolic  expression  connecting  the  output  with  the  inputs  is  the  same  in  both  paths.  Our  grouping  of  paths  is  gradually  created  by  a  smart  path  exploration.  Our  experiments  show  the  benefits  of  the  proposed  pathexploration  in  test-suite  construction.      Our  path  partitioning  produces  a  semantic  signature  of  a  program  ---  describing  all  the  different  symbolic  expressions  that  the  output  can  assume  along  different  program  paths.  To  reason  about  changes  between  program  versions,  we  can  therefore  analyze  their  semantic  signatures.  In  particular,  we  demonstrate  the  applications  of  our  path  partitioning  in  debugging  of  software  regressions.
1	Focus  shifting  patterns  of  oss  developers  and  their  congruence  with  call  graphs.  Developers  in  complex,  self-organized  open-source  projects  often  work  on  many  different  files,  and  over  time  switch  focus  between  them.  Shifting  focus  can  have  impact  on  the  software  quality  and  productivity,  and  is  thus  an  important  topic  of  investigation.  In  this  paper,  we  study  focus  shifting  patterns  (FSPs)  of  developers  by  comparing  trace  data  from  a  dozen  open  source  software  (OSS)  projects  of  their  longitudinal  commit  activities  and  file  dependencies  from  the  projects  call  graphs.  Using  information  theoretic  measures  of  network  structure,  we  find  that  fairly  complex  focus-shifting  patterns  emerge,  and  FSPs  in  the  same  project  are  more  similar  to  each  other.  We  show  that  developers  tend  to  shift  focus  along  with,  rather  than  away  from,  software  dependency  links  described  by  the  call  graphs.  This  tendency  becomes  weaker  as  either  the  interval  between  successive  commits,  or  the  organizational  distance  between  committed  files  (i.e.  directory  distance),  gets  larger.  Interestingly,  this  tendency  appears  stronger  with  more  productive  developers.  We  hope  our  study  will  initiate  interest  in  further  understanding  of  FSPs,  which  can  ultimately  help  to  (1)  improve  current  recommender  systems  to  predict  the  next  focus  of  developers,  and  (2)  provide  insight  into  better  call  graph  design,  so  as  to  facilitate  developers'  work.
1	Techniques  for  improving  regression  testing  in  continuous  integration  development  environments.  In  continuous  integration  development  environments,  software  engineers  frequently  integrate  new  or  changed  code  with  the  mainline  codebase.  This  can  reduce  the  amount  of  code  rework  that  is  needed  as  systems  evolve  and  speed  up  development  time.  While  continuous  integration  processes  traditionally  require  that  extensive  testing  be  performed  following  the  actual  submission  of  code  to  the  codebase,  it  is  also  important  to  ensure  that  enough  testing  is  performed  prior  to  code  submission  to  avoid  breaking  builds  and  delaying  the  fast  feedback  that  makes  continuous  integration  desirable.  In  this  work,  we  present  algorithms  that  make  continuous  integration  processes  more  cost-effective.  In  an  initial  pre-submit  phase  of  testing,  developers  specify  modules  to  be  tested,  and  we  use  regression  test  selection  techniques  to  select  a  subset  of  the  test  suites  for  those  modules  that  render  that  phase  more  cost-effective.  In  a  subsequent  post-submit  phase  of  testing,  where  dependent  modules  as  well  as  changed  modules  are  tested,  we  use  test  case  prioritization  techniques  to  ensure  that  failures  are  reported  more  quickly.  In  both  cases,  the  techniques  we  utilize  are  novel,  involving  algorithms  that  are  relatively  inexpensive  and  do  not  rely  on  code  coverage  information  --  two  requirements  for  conducting  testing  cost-effectively  in  this  context.  To  evaluate  our  approach,  we  conducted  an  empirical  study  on  a  large  data  set  from  Google  that  we  make  publicly  available.  The  results  of  our  study  show  that  our  selection  and  prioritization  techniques  can  each  lead  to  cost-effectiveness  improvements  in  the  continuous  integration  process.
1	Directed  test  generation  to  detect  loop  inefficiencies.  Redundant  traversal  of  loops  in  the  context  of  other  loops  has  been  recently  identified  as  a  source  of  performance  bugs  in  many  Java  libraries.  This  has  resulted  in  the  design  of  static  and  dynamic  analysis  techniques  to  detect  these  performance  bugs  automatically.  However,  while  the  effectiveness  of  dynamic  analyses  is  dependent  on  the  analyzed  input  tests,  static  analyses  are  less  effective  in  automatically  validating  the  presence  of  these  problems,  validating  the  fixes  and  avoiding  regressions  in  future  versions.  This  necessitates  the  design  of  an  approach  to  automatically  generate  tests  for  exposing  redundant  traversal  of  loops.          In  this  paper,  we  design  a  novel,  scalable  and  automatic  approach  that  addresses  this  goal.  Our  approach  takes  a  library  and  an  initial  set  of  coverage-driven  randomly  generated  tests  as  input  and  generates  tests  which  enable  detection  of  redundant  traversal  of  loops.  Our  approach  is  broadly  composed  of  three  phases  –  analysis  of  the  execution  of  random  tests  to  generate  method  summaries,  identification  of  methods  with  potential  nested  loops  along  with  the  appropriate  context  to  expose  the  problem,  and  test  generation  to  invoke  the  identified  methods  with  the  appropriate  parameters.  The  generated  tests  can  be  analyzed  by  existing  dynamic  tools  to  detect  possible  performance  issues.          We  have  implemented  our  approach  on  top  of  the  SOOT  bytecode  analysis  framework  and  validated  it  on  many  open-source  Java  libraries.  Our  experiments  reveal  the  effectiveness  of  our  approach  in  generating  224  tests  that  reveal  46  bugs  across  seven  libraries,  including  34  previously  unknown  bugs.  The  tests  generated  using  our  approach  significantly  outperform  the  randomly  generated  tests  in  their  ability  to  expose  the  inefficiencies,  demonstrating  the  usefulness  of  our  design.  The  implementation  of  our  tool,  named  Glider,  is  available  at  http://drona.csa.iisc.ac.in/~sss/tools/glider.
1	A  statistical  semantic  language  model  for  source  code.  Recent  research  has  successfully  applied  the  statistical  n-gram  language  model  to  show  that  source  code  exhibits  a  good  level  of  repetition.  The  n-gram  model  is  shown  to  have  good  predictability  in  supporting  code  suggestion  and  completion.  However,  the  state-of-the-art  n-gram  approach  to  capture  source  code  regularities/patterns  is  based  only  on  the  lexical  information  in  a  local  context  of  the  code  units.  To  improve  predictability,  we  introduce  SLAMC,  a  novel  statistical  semantic  language  model  for  source  code.  It  incorporates  semantic  information  into  code  tokens  and  models  the  regularities/patterns  of  such  semantic  annotations,  called  sememes,  rather  than  their  lexemes.  It  combines  the  local  context  in  semantic  n-grams  with  the  global  technical  concerns/functionality  into  an  n-gram  topic  model,  together  with  pairwise  associations  of  program  elements.  Based  on  SLAMC,  we  developed  a  new  code  suggestion  method,  which  is  empirically  evaluated  on  several  projects  to  have  relatively  18-68%  higher  accuracy  than  the  state-of-the-art  approach.
1	File  level  vs  module  level  regression  test  selection  for  net.  Regression  testing  is  used  to  check  the  correctness  of  evolving  software.  With  the  adoption  of  Agile  development  methodology,  the  number  of  tests  and  software  revisions  has  dramatically  increased,  and  hence  has  the  cost  of  regression  testing.  Researchers  proposed  regression  test  selection  (RTS)  techniques  that  optimize  regression  testing  by  skipping  tests  that  are  not  impacted  by  recent  program  changes.  Ekstazi  is  one  such  state-of-the  art  technique;  Ekstazi  is  implemented  for  the  Java  programming  language  and  has  been  adopted  by  several  companies  and  open-source  projects.          We  report  on  our  experience  implementing  and  evaluating  Ekstazi#,  an  Ekstazi-like  tool  for  .NET.  We  describe  the  key  challenges  of  bringing  the  Ekstazi  idea  to  the  .NET  platform.  We  evaluate  Ekstazi#  on  11  open-source  projects,  as  well  as  an  internal  Microsoft  project  substantially  larger  than  each  of  the  open-source  projects.  Finally,  we  compare  Ekstazi#  to  an  incremental  build  system  (also  developed  at  Microsoft),  which,  out  of  the  box,  provides  module-level  dependency  tracking  and  skipping  tasks  (including  test  execution)  whenever  dependencies  of  a  task  do  not  change  between  the  current  and  the  last  successful  build.  Ekstazi#  on  average  reduced  regression  testing  time  by  43.70%  for  the  open-source  projects  and  by  65.26%  for  the  Microsoft  project  (the  latter  is  in  addition  to  the  savings  provided  by  incremental  builds).
1	Bayesian  inference  using  data  flow  analysis.  We  present  a  new  algorithm  for  Bayesian  inference  over  probabilistic  programs,  based  on  data  flow  analysis  techniques  from  the  program  analysis  community.  Unlike  existing  techniques  for  Bayesian  inference  on  probabilistic  programs,  our  data  flow  analysis  algorithm  is  able  to  perform  inference  directly  on  probabilistic  programs  with  loops.  Even  for  loop-free  programs,  we  show  that  data  flow  analysis  offers  better  precision  and  better  performance  benefits  over  existing  techniques.  We  also  describe  heuristics  that  are  crucial  for  our  inference  to  scale,  and  present  an  empirical  evaluation  of  our  algorithm  over  a  range  of  benchmarks.
1	Tlv  abstraction  through  testing  learning  and  validation.  A  (Java)  class  provides  a  service  to  its  clients  (i.e.,  programs  which  use  the  class).  The  service  must  satisfy  certain  specifications.  Different  specifications  might  be  expected  at  different  levels  of  abstraction  depending  on  the  client's  objective.  In  order  to  effectively  contrast  the  class  against  its  specifications,  whether  manually  or  automatically,  one  essential  step  is  to  automatically  construct  an  abstraction  of  the  given  class  at  a  proper  level  of  abstraction.  The  abstraction  should  be  correct  (i.e.,  over-approximating)  and  accurate  (i.e.,  with  few  spurious  traces).  We  present  an  automatic  approach,  which  combines  testing,  learning,  and  validation,  to  constructing  an  abstraction.  Our  approach  is  designed  such  that  a  large  part  of  the  abstraction  is  generated  based  on  testing  and  learning  so  as  to  minimize  the  use  of  heavy-weight  techniques  like  symbolic  execution.  The  abstraction  is  generated  through  a  process  of  abstraction/refinement,  with  no  user  input,  and  converges  to  a  specific  level  of  abstraction  depending  on  the  usage  context.  The  generated  abstraction  is  guaranteed  to  be  correct  and  accurate.  We  have  implemented  the  proposed  approach  in  a  toolkit  named  TLV  and  evaluated  TLV  with  a  number  of  benchmark  programs  as  well  as  three  real-world  ones.  The  results  show  that  TLV  generates  abstraction  for  program  analysis  and  verification  more  efficiently.
1	The  economics  of  static  analysis  tools.  Static  analysis  tools  have  experienced  a  dichotomy  over  the  span  of  the  last  decade.  They  have  proven  themselves  to  be  useful  in  many  domains,  but  at  the  same  time  have  not  (in  general)  experienced  any  notable  concrete  integration  into  a  development  environment.  This  is  partly  due  to  the  inherent  complexity  of  the  tools  themselves,  as  well  as  due  to  other  intangible  factors.  Such  factors  usually  tend  to  include  questions  about  the  return  on  investment  of  the  tool  and  the  value  the  tool  provides  in  a  development  environment.  In  this  paper,  we  present  an  empirical  model  for  evaluating  static  analysis  tools  from  the  perspective  of  the  economic  value  they  provide.  We  further  apply  this  model  to  a  case  study  of  the  Static  Driver  Verier  (SDV)  tool  that  ships  with  the  Windows  Driver  Kit  and  show  the  usefulness  of  the  model  and  the  tool.
1	Validate  your  spdx  files  for  open  source  license  violations.  Licensing  decisions  for  new  Open  Source  Software  are  not  always  straightforward.  However,  the  license  that  accompanies  the  software  is  important  as  it  largely  affects  its  subsequent  distribution  and  reuse.  License  information  for  software  products  is  captured  -  among  other  data  -  in  the  Software  Package  Data  Exchange  (SPDX)  files.  The  SPDX  specification  is  gaining  popularity  in  the  software  industry  and  has  been  adopted  by  many  organizations  internally.  In  this  demonstration  paper,  we  present  our  tool  for  the  validation  of  SPDX  files  regarding  proper  license  use.  Software  packages  described  in  SPDX  format  are  examined  in  order  to  detect  license  violations  that  may  occur  when  a  product  combines  different  software  sources  that  carry  different  and  potentially  contradicting  licenses.  The  SPDX  License  Validation  Tool  (SLVT)  gives  the  opportunity  to  check  the  compatibility  of  one  or  more  SPDX  files.  The  evaluation  performed  on  a  number  of  software  packages  demonstrates  its  usefulness  for  drawing  conclusions  on  license  use,  revealing  violations  in  some  of  the  test  projects.
1	Continuous  variable  specific  resolutions  of  feature  interactions.  Systems  that  are  assembled  from  independently  developed  features  suffer  from  feature  interactions,  in  which  features  affect  one  another's  behaviour  in  surprising  ways.  The  Feature  Interaction  Problem  results  from  trying  to  implement  an  appropriate  resolution  for  each  interaction  within  each  possible  context,  because  the  number  of  possible  contexts  to  consider  increases  exponentially  with  the  number  of  features  in  the  system.  Resolution  strategies  aim  to  combat  the  Feature  Interaction  Problem  by  offering  default  strategies  that  resolve  entire  classes  of  interactions,  thereby  reducing  the  work  needed  to  resolve  lots  of  interactions.  However  most  such  approaches  employ  coarse-grained  resolution  strategies  (e.g.,  feature  priority)  or  a  centralized  arbitrator.          Our  work  focuses  on  employing  variable-specific  default-resolution  strategies  that  aim  to  resolve  at  runtime  features-  conflicting  actions  on  a  system's  outputs.  In  this  paper,  we  extend  prior  work  to  enable  co-resolution  of  interactions  on  coupled  output  variables  and  to  promote  smooth  continuous  resolutions  over  execution  paths.  We  implemented  our  approach  within  the  PreScan  simulator  and  performed  a  case  study  involving  15  automotive  features;  this  entailed  our  devising  and  implementing  three  resolution  strategies  for  three  output  variables.  The  results  of  the  case  study  show  that  the  approach  produces  smooth  and  continuous  resolutions  of  interactions  throughout  interesting  scenarios.
1	Panning  requirement  nuggets  in  stream  of  software  maintenance  tickets.  There  is  an  increasing  trend  to  outsource  maintenance  of  large  applications  and  application  portfolios  of  a  business  to  third  parties,  specialising  in  application  maintenance,  who  are  incented  to  deliver  the  best  possible  maintenance  at  the  lowest  cost.  To  do  so,  they  need  to  identify  repeat  problem  areas,  which  cause  more  maintenance  grief,  and  seek  a  unified  remedy  to  avoid  the  costs  spent  on  fixing  these  individually.  These  repeat  areas,  in  a  sense,  represent  major,  evolving  areas  of  need,  or  requirements,  for  the  customer.  The  information  about  the  repeating  problem  is  typically  embedded  in  the  unstructured  text  of  multiple  tickets,  waiting  to  be  found  and  addressed.  Currently,  repeat  problems  are  found  by  manual  analysis;  effective  solutions  depend  on  the  collective  experience  of  the  team  solving  them.  In  this  paper,  we  propose  an  approach  to  automatically  analyze  problem  tickets  to  discover  groups  of  problems  being  reported  in  them  and  provide  meaningful,  descriptive  labels  to  help  interpret  these  groups.  Our  approach  incorporates  a  cleansing  phase  to  handle  the  high  level  of  noise  observed  in  problem  tickets  and  a  method  to  incorporate  multiple  text  clustering  techniques  and  merge  their  results  in  a  meaningful  manner.  We  provide  detailed  experiments  to  quantitatively  and  qualitatively  evaluate  our  approach
1	Titan  a  toolset  that  connects  software  architecture  with  quality  analysis.  In  this  tool  demo,  we  will  illustrate  our  tool---Titan---that  supports  a  new  architecture  model:  design  rule  spaces  (DRSpaces).  We  will  show  how  Titan  can  capture  both  architecture  and  evolutionary  structure  and  help  to  bridge  the  gap  between  architecture  and  defect  prediction.  We  will  demo  how  to  use  our  toolset  to  capture  hundreds  of  buggy  files  into  just  a  few  architecturally  related  groups,  and  to  reveal  architecture  issues  that  contribute  to  the  error-proneness  and  change-proneness  of  these  groups.  Our  tool  has  been  used  to  analyze  dozens  of  large-scale  industrial  projects,  and  has  demonstrated  its  ability  to  provide  valuable  direction  on  which  parts  of  the  architecture  are  problematic,  and  on  why,  when,  and  how  to  refactor.  The  video  demo  of  Titan  can  be  found  at  https://art.cs.drexel.edu/~lx52/titan.mp4
1	Pspwizard  machine  assisted  definition  of  temporal  logical  properties  with  specification  patterns.  Model  checking  provides  a  powerful  means  to  assert  and  verify  desired  system  properties.  But,  for  the  verification  process  to  become  feasible,  a  correct  formulation  of  these  properties  in  a  temporal  logic  is  necessary  -  a  potential  barrier  to  application  in  practice.  Research  on  property  specification  has  supplied  us  with  rich  pattern  catalogs  that  capture  commonly  occurring  system  properties  in  different  temporal  logics.  Furthermore,  these  property  specification  pattern  catalogs  usually  offer  both  a  structured  English  grammar  to  facilitate  the  pattern  selection  and  an  associated  template  solutions  to  express  the  properties  formally.  Yet,  the  actual  use  of  property  specification  patterns  remains  cumbersome,  due  to  limited  tool  support.  For  this  reason,  we  have  developed  the  Property  Specification  Pattern  Wizard  (PSPWizard),  a  framework  that  defines  an  interface  for  the  currently  accepted  property  specification  pattern  libraries.  PSPWizard  consists  of  two  main  building  blocks:  a  mapping  generator  that  weaves  a  given  pattern  library  with  a  target  logic  and  a  GUI  front-end  to  the  structured  English  grammar  tailored  to  those  patterns  that  are  supported  in  the  target  logic.
1	Risky  files  an  approach  to  focus  quality  improvement  effort.  As  the  development  of  software  products  frequently  transitions  among  globally  distributed  teams,  the  knowledge  about  the  source  code,  design  decisions,  original  requirements,  and  the  history  of  troublesome  areas  gets  lost.  A  new  team  faces  tremendous  challenges  to  regain  that  knowledge.  In  numerous  projects  we  observed  that  only  1%  of  project  files  are  involved  in  more  than  60%  of  the  customer  reported  defects  (CFDs),  thus  focusing  quality  improvement  on  such  files  can  greatly  reduce  the  risk  of  poor  product  quality.  We  describe  a  mostly  automated  approach  that  annotates  the  source  code  at  the  file  and  module  level  with  the  historic  information  from  multiple  version  control,  issue  tracking,  and  an  organization's  directory  systems.  Risk  factors  (e.g,  past  changes  and  authors  who  left  the  project)  are  identified  via  a  regression  model  and  the  riskiest  areas  undergo  a  structured  evaluation  by  experts.  The  results  are  presented  via  a  web-based  tool  and  project  experts  are  then  trained  how  to  use  the  tool  in  conjunction  with  a  checklist  to  determine  risk  remediation  actions  for  each  risky  file.  We  have  deployed  the  approach  in  seven  projects  in  Avaya  and  are  continuing  deployment  to  the  remaining  projects  as  we  are  evaluating  the  results  of  earlier  deployments.  The  approach  is  particularly  helpful  to  focus  quality  improvement  effort  for  new  releases  of  deployed  products  in  a  resource-constrained  environment.
1	Bisecting  commits  and  modeling  commit  risk  during  testing.  Software  testing  is  one  of  the  costliest  stages  in  the  software  development  life  cycle.  One  approach  to  reducing  the  test  execution  cost  is  to  group  changes  and  test  them  as  a  batch  (i.e.  batch  testing).  However,  when  tests  fail  in  a  batch,  commits  in  the  batch  need  to  be  re-tested  to  identify  the  cause  of  the  failure,  i.e.  the  culprit  commit.  The  re-testing  is  typically  done  through  bisection  (i.e.  a  binary  search  through  the  commits  in  a  batch).  Intuitively,  the  effectiveness  of  batch  testing  highly  depends  on  the  size  of  the  batch.  Larger  batches  require  fewer  initial  test  runs,  but  have  a  higher  chance  of  a  test  failure  that  can  lead  to  expensive  test  re-runs  to  find  the  culprit.  We  are  unaware  of  research  that  investigates  and  simulates  the  impact  of  batch  sizes  on  the  cost  of  testing  in  industry.  In  this  work,  we  first  conduct  empirical  studies  on  the  effectiveness  of  batch  testing  in  three  large-scale  industrial  software  systems  at  Ericsson.  Using  9  months  of  testing  data,  we  simulate  batch  sizes  from  1  to  20  and  find  the  most  cost-effective  BatchSize  for  each  project.  Our  results  show  that  batch  testing  saves  72%  of  test  executions  compared  to  testing  each  commit  individually.  In  a  second  simulation,  we  incorporate  flaky  tests  that  pass  and  fail  on  the  same  commit  as  they  are  a  significant  source  of  additional  test  executions  on  large  projects.  We  model  the  degree  of  flakiness  for  each  project  and  find  that  test  flakiness  reduces  the  cost  savings  to  42%.  In  a  third  simulation,  we  guide  bisection  to  reduce  the  likelihood  of  batch-testing  failures.  We  model  the  riskiness  of  each  commit  in  a  batch  using  a  bug  model  and  a  test  execution  history  model.  The  risky  commits  are  tested  individually,  while  the  less  risky  commits  are  tested  in  a  single  larger  batch.  Culprit  predictions  with  our  approach  reduce  test  executions  up  to  9%  compared  to  Ericsson’s  current  bisection  approach.  The  results  have  been  adopted  by  developers  at  Ericsson  and  a  tool  to  guide  bisection  is  in  the  process  of  being  added  to  Ericsson’s  continuous  integration  pipeline.
1	Concolic  testing  with  adaptively  changing  search  heuristics.  We  present  Chameleon,  a  new  approach  for  adaptively  changing  search  heuristics  during  concolic  testing.  Search  heuristics  play  a  central  role  in  concolic  testing  as  they  mitigate  the  path-explosion  problem  by  focusing  on  particular  program  paths  that  are  likely  to  increase  code  coverage  as  quickly  as  possible.  A  variety  of  techniques  for  search  heuristics  have  been  proposed  over  the  past  decade.  However,  existing  approaches  are  limited  in  that  they  use  the  same  search  heuristics  throughout  the  entire  testing  process,  which  is  inherently  insufficient  to  exercise  various  execution  paths.  Chameleon  overcomes  this  limitation  by  adapting  search  heuristics  on  the  fly  via  an  algorithm  that  learns  new  search  heuristics  based  on  the  knowledge  accumulated  during  concolic  testing.  Experimental  results  show  that  the  transition  from  the  traditional  non-adaptive  approaches  to  ours  greatly  improves  the  practicality  of  concolic  testing  in  terms  of  both  code  coverage  and  bug-finding.
1	An  analysis  of  the  empirical  software  engineering  over  the  last  10  editions  of  brazilian  software  engineering  symposium.  Empirical  evaluations  developed  in  the  software  engineering  area  have  been  widely  applied  as  a  formalism  to  validate  and  ensure  the  credibility  of  the  works  proposed  by  the  researchers.  Even  though  the  adoption  of  empirical  evaluation  techniques  has  gained  popularity  in  recent  years,  its  application  has  been  questioned  both  qualitatively  and  quantitatively.  This  study  aims  at  analyzing  how  empirical  software  engineering  research  has  evolved  in  the  Brazilian  Symposium  on  Software  Engineering  (SBES)  community.  We  performed  a  controlled  quasi-experiment,  using  published  papers  over  the  last  10  years  in  SBES.  Our  experiment  was  divided  into  two  phases:  classification  by  type  and  quality  assessment  of  the  main  empirical  types.  In  the  first  phase,  the  sample  was  201  papers;  in  the  second  one,  the  sample  decreased  to  126  papers.  The  results  have  shown  failures  and  gaps  in  the  application  of  empirical  methods  when  assessing  the  quality  of  the  Software  Engineering  works.  We  believe  that  we  can  contribute  to  improve  how  the  studies  were  conducted  and  consequently  help  to  produce  more  reliable  results,  reducing  or  eliminating  biases:  an  important  qualitative  factor  in  scientific  work.  In  addition,  due  to  the  lack  of  assessment  supporting  tools,  we  developed  a  theoretical  protocol  to  support  the  assessment  process  and  proposed  improvements  for  papers  that  obtained  below-expected  rates.
1	Hearing  the  voice  of  developers  in  mobile  software  ecosystems.  In  a  Mobile  Software  Ecosystem  (MSECO),  there  is  no  direct  communication  between  the  organizations  that  maintain  mobile  platforms  (e.g.  Apple,  Google,  and  Microsoft)  and  developers  to  solve  technical  questions.  Thus,  QA  barrier  reduction;  management  of  technology  insertion;  and  fostering  of  relationships.
1	Specifying  privacy  requirements  with  goal  oriented  modeling  languages.  Context:  Privacy  of  personal  data  is  a  growing  concern  regarding  users  of  software  systems.  In  this  sense,  the  literature  reports  that  in  order  to  avoid  privacy  breaches,  there  must  be  systematic  approaches  to  specify  privacy  requirements  from  the  early  activities  of  software  development.  Objective:  Motivated  by  this  situation,  this  paper  presents  a  framework  of  privacy  modeling  capabilities  that  must  be  addressed  by  requirements  modeling  languages  to  better  support  privacy  specification.  The  capabilities  will  be  used  to  compare  three  goal-oriented  modeling  languages  (i*,  NFR-Framework  and  Secure-Tropos).  Method:  The  framework  was  created  with  basis  on  a  conceptual  foundation  and  a  conceptual  model  of  privacy  built  from  an  analysis  of  a  standard,  a  regulation,  guidelines  and  other  bibliographical  sources  related  to  privacy.  A  health  care  example  is  used  to  illustrate  how  the  framework  can  be  used  to  compare  the  chosen  modeling  languages.  Results:  Fourteen  privacy  modeling  capabilities  were  defined  in  the  framework  and  it  was  observed  that  the  analyzed  modeling  languages  do  not  fully  support  them.  Conclusions:  The  proposed  framework  contributes  towards  the  consolidation  of  a  privacy  conceptual  foundation  that  can  be  used  to  evaluate  modeling  languages  for  privacy  in  Requirements  Engineering.  The  comparison  performed  by  using  this  framework  indicates  Secure-Tropos  as  the  most  complete  language  to  model  privacy  among  the  analyzed  goal-oriented  modeling  languages.
1	Ccsharp  an  efficient  three  phase  code  clone  detector  using  modified  pdgs.  Detecting  code  clones  in  software  systems  is  becoming  more  and  more  important  with  the  blossom  of  open  source  projects.  In  spite  of  numerous  active  researches,  there  is  still  a  lack  of  detecting  clones  especially  high-level  clones  efficiently  and  accurately.  In  this  paper,  we  present  CCSharp,  a  three-phase  PDG-based  clone  detector  which  can  detect  much  more  clones  besides  high-level  ones  in  software  systems.  To  solve  the  problem  of  PDG-based  tool's  high  time  cost,  we  adopt  two  strategies  to  decrease  the  overall  computing  quantity  of  our  tool:  PDG's  structure  modification  and  characteristic  vector  filtering.  In  PDG's  structure  modification,  we  propose  a  novel  technique  to  merge  procedure  invocation  nodes  which  can  make  clone  detection  get  rid  of  influence  of  procedure's  parameters  and  disguise  as  well  as  downscale  PDG's  structure.  We  proceed  clone  detection  on  both  real-world  and  artificial  codebase  by  CCSharp  along  with  other  three  state-of-the-art  tools.  Experiment  results  show  that  CCSharp  has  both  high  recall  and  precision,  and  can  detect  much  more  unique  clones  compared  with  the  other  three  tools.
1	Entity  disambiguation  in  natural  language  text  requirements.  We  consider  the  problem  of  terminological  ambiguity  in  requirement  specifications  arising  from  term-aliasing,  wherein  multiple  terms  may  be  referring  to  the  same  entity  in  a  corpus  of  natural  language  text  requirements.  We  consider  the  case  of  syntactic  as  well  as  semantic  aliasing.  Syntactic  alias  identification  involves  automated  generation  of  patterns  for  identifying  syntactic  variances  of  terms  including  abbreviations  and  introduced-aliases.  Semantic  alias  identification  includes  extracting  multi-dimensional  features  (linguistic,  statistical,  and  locational)  from  given  requirement  text  to  estimate  semantic  relatedness  between  terms.  Based  upon  the  estimated  relatedness  and  standard  language  database  based  refinement,  clusters  of  potential  semantic  aliases  are  generated.  Results  of  these  analyses  with  user  refinement  should  lead  to  generation  of  entity-term  alias  glossary  and  unification  of  term  usage  across  requirements.  We  present  experimental  results  assessing  the  effectiveness  of  the  presented  approach  using  a  prototype  tool  for  an  automated  analysis  of  term-aliasing  in  the  requirements  given  as  plain  English  language  text.
1	A  practical  study  of  debugging  using  model  checking.  Debugging  is  one  of  the  most  time-consuming  tasks  in  software  development.  The  application  of  a  model-checking  technique  in  debugging  has  strong  potential  to  solve  this  problem.  Here,  lessons  learned  through  our  practical  experiences  with  POM/MC  are  discussed.  The  aim  of  this  proposed  hypothesis-based  method  of  debugging  is  not  only  to  reproduce  a  failure  as  counterexamples,  but  also  to  obtain  a  counterexample  that  is  useful  for  detecting  the  fault  or  the  cause  of  the  failure.  One  of  the  characteristics  of  the  proposed  approach  is  that  it  degenerates  a  source  code  in  order  to  clarify  the  fault.  An  example  of  this  degeneration  shows  that  the  method  is  useful  for  fault  analysis  and  avoidance  of  the  "state-explosion"  problem.  Furthermore,  the  characteristics  of  debugging  using  POM/MC  are  explained  from  the  viewpoint  of  debugging  hypotheses.
1	Improving  development  practices  through  experimentation  an  industrial  tdd  case.  Test-Driven  Development  (TDD),  an  agile  development  approach  that  enforces  the  construction  of  software  systems  by  means  of  successive  micro-iterative  testing  coding  cycles,  has  been  widely  claimed  to  increase  external  software  quality.  In  view  of  this,  some  managers  at  Paf-a  Nordic  gaming  entertainment  company—were  interested  in  knowing  how  would  TDD  perform  at  their  premises.  Eventually,  if  TDD  outperformed  their  traditional  way  of  coding  (i.e.,  YW,  short  for  Your  Way),  it  would  be  possible  to  switch  to  TDD  considering  the  empirical  evidence  achieved  at  the  company  level.  We  conduct  an  experiment  at  Paf  to  evaluate  the  performance  of  TDD,  YW  and  the  reverse  approach  of  TDD  (i.e.,  ITL,  short  for  Iterative-Test  Last)  on  external  quality.  TDD  outperforms  YW  and  ITL  at  Paf.  Despite  the  encouraging  results,  we  cannot  recommend  Paf  to  immediately  adopt  TDD  as  the  difference  in  performance  between  YW  and  TDD  is  small.  However,  as  TDD  looks  promising  at  Paf,  we  suggest  to  move  some  developers  to  TDD  and  to  run  a  future  experiment  to  compare  the  performance  of  TDD  and  YW.  TDD  slightly  outperforms  ITL  in  controlled  experiments  for  TDD  novices.  However,  more  industrial  experiments  are  still  needed  to  evaluate  the  performance  of  TDD  in  real-life  contexts.
1	Results  for  compositional  timed  testing.  Modern  industrial  systems  are  often  large  and  distributed.  Consequently,  building  the  test  harness  for  them  can  be  technically  challenging.  A  compositional  approach  attempts  to  overcome  this  problem  by  partitioning  the  system  into  smaller  parts  easier  to  test  separately.  And  in  particular,  compositionality  helps  to  avoid  as  much  as  possible  testing  the  whole  monolithic  system  thanks  to  mathematical  results  which  relate  the  global  correctness  of  the  system  to  the  correctness  of  its  constituent  parts.  In  this  paper,  we  present  a  compositionality  result  for  model-based  testing  in  the  setting  of  the  conformance  relation  tioco  which  is  dedicated  to  timed  systems.  We  show  how  to  exploit  this  result  in  practice  by  extending  a  previously  defined  symbolic  testing  framework.
1	Diagnosis  oriented  alarm  correlations.  Defect  detection  generally  includes  two  stages:  static  analysis  and  alarm  inspection.  Helping  the  user  in  the  alarm  inspection  task  is  a  major  challenge  for  current  static  analyzers.  A  large  number  of  independent  alarms  are  against  the  understanding  and  may  lead  developers  and  managers  to  reject  the  use  of  static  analysis  tools  due  to  the  overhead  of  alarm  inspection.  To  help  with  the  inspection  tasks,  we  formally  introduce  alarm  correlations.  If  the  occurrence  of  one  alarm  causes  another  alarm  to  occur,  we  say  they  are  correlated.  We  propose  a  framework  for  the  investigation  of  the  alarms,  so  as  to  help  classifying  them  by  their  correlations.  The  underlying  algorithms  were  implemented  inside  our  static  analysis  tool.  We  choose  one  common  semantic  alarm  as  case  study  and  proved  that  our  method  has  the  effect  of  reducing  33.1%  of  alarm  identification.  Using  correlation  information,  we  are  able  to  automate  alarm  identification  that  previously  had  to  be  done  manually.
1	User  guided  automation  for  testing  mobile  apps.  Mobile  devices  are  gradually  taking  over  traditional  computers'  dominance  in  human  lives.  With  the  ever-increasing  shipment  of  mobile  apps  running  on  these  devices,  their  quality  issues  become  a  severe  challenge.  Although  automated  testing  techniques  are  being  widely  studied,  they  mostly  fall  short  of  handling  mobile  apps'  complex  interactions,  e.g.,  A  finger  swipe  or  device  shaking  gesture,  leading  to  inadequate  testing.  In  this  paper,  we  present  a  novel  User  Guided  Automation  (UGA)  technique  to  address  testing  challenges  incurred  by  such  complex  interactions.  UGA  exploits  user  insights  to  complement  automated  testing  techniques  by  recording  user-guided  app  executions,  replaying  apps  to  certain  stop  points,  and  systematically  exploring  state  space  from  these  stop  points.  We  implemented  our  work  as  a  prototype  UGA  tool  on  Android  platform  and  evaluated  it  on  seven  real-world  Android  apps.  Evaluation  results  show  that  UGA  achieved  1.59-21.78×  improvement  in  terms  of  method  coverage  over  state-of-the-art  automated  techniques  in  testing  mobile  apps.
1	Towards  accurate  monitoring  of  extra  functional  properties  in  real  time  embedded  systems.  Management  and  preservation  of  Extra-Functional  Properties  (EFPs)  is  critical  in  real-time  embedded  systems  to  ensure  their  correct  behavior.  Deviation  of  these  properties,  such  as  timing  and  memory  usage,  from  their  acceptable  and  valid  values  can  impair  the  functionality  of  the  system.  In  this  regard,  monitoring  is  an  important  means  to  investigate  the  state  of  the  system  and  identify  such  violations.  The  monitoring  result  can  also  be  used  to  make  adaptation  and  re-configuration  decisions  in  the  system  as  well.  Most  of  the  works  related  to  monitoring  EFPs  are  based  on  the  assumption  that  monitoring  results  accurately  represent  the  true  state  of  the  system  at  the  monitoring  request  time  point.  In  some  systems  this  assumption  can  be  safe  and  valid.  However,  if  in  a  system  the  value  of  an  EFP  changes  frequently,  the  result  of  monitoring  may  not  accurately  represent  the  state  of  the  system  at  the  time  point  when  the  monitoring  request  has  been  issued.  The  consequences  of  such  inaccuracies  can  be  critical  in  certain  systems  and  applications.  In  this  paper,  we  mainly  introduce  and  discuss  this  practical  problem  and  also  provide  a  solution  to  improve  the  monitoring  accuracy  of  EFPs.
1	Modeling  security  threat  patterns  to  derive  negative  scenarios.  The  elicitation  of  security  requirements  is  a  crucial  issue  to  develop  secure  business  processes  and  information  systems  of  higher  quality.  Although  we  have  several  methods  to  elicit  security  requirements,  most  of  them  do  not  provide  sufficient  supports  to  identify  security  threats.  Since  threats  do  not  occur  so  frequently,  like  exceptional  events,  it  is  much  more  difficult  to  determine  the  potentials  of  threats  exhaustively  rather  than  identifying  normal  behavior  of  a  business  process.  To  reduce  this  difficulty,  accumulated  knowledge  of  threats  obtained  from  practical  setting  is  necessary.  In  this  paper,  we  present  the  technique  to  model  knowledge  of  threats  as  patterns  by  deriving  the  negative  scenarios  that  realize  threats  and  to  utilize  them  during  business  process  modeling.  The  knowledge  is  extracted  from  Security  Target  documents,  based  on  the  international  Common  Criteria  Standard,  and  the  patterns  are  described  with  transformation  rules  on  sequence  diagrams.  In  our  approach,  an  analyst  composes  normal  scenarios  of  a  business  process  with  sequence  diagrams,  and  the  threat  patterns  matched  to  them  derives  negative  scenarios.  Our  approach  has  been  demonstrated  on  several  examples,  to  show  its  practical  application.
1	An  algorithmic  based  change  effort  estimation  model  for  software  development.  Software  development  mostly  adopts  two  kinds  of  methodologies;  Traditional  and  Agile.  In  both  methodologies,  software  changes  are  inevitable  due  to  the  dynamic  nature  of  the  software  development  project  itself.  One  of  the  factors  that  influences  the  effectiveness  of  the  change  acceptance  decision  is  the  accuracy  of  the  change  effort  estimation.  There  are  two  current  models  that  have  been  widely  used  to  estimate  change  effort  which  are  algorithmic  and  non-algorithmic  models.  The  algorithmic  model  is  known  for  its  formal  and  structural  way  of  estimation  and  best  suited  for  Traditional  methodology.  While  non-algorithmic  model  is  widely  adopted  for  Agile  methodology  of  software  projects  due  to  its  easiness  and  requires  less  work  in  term  of  effort  predictability.  Nevertheless,  none  of  the  existing  change  effort  estimation  models  are  proven  to  suit  both,  Traditional  and  Agile  methodology.  Thus,  this  paper  proposes  an  algorithmic-based  change  effort  estimation  model  that  uses  change  impact  analysis  method  which  is  applicable  for  both  Traditional  and  Agile  methodologies.  The  proposed  model  uses  a  current  selected  change  impact  analysis  method  for  software  development  phase  which  is  the  SDP-CIAF  (Software  Development  Phase-Change  Impact  Analysis  Framework).  The  proposed  model  is  evaluated  through  an  extensive  experimental  validation  using  case  scenarios  of  six  real  Traditional  and  Agile  methodologies  software  projects.  The  evaluation  results  confirmed  the  applicability  for  both  Traditional  and  Agile  methodologies.
1	Intrusive  test  automation  with  failed  test  case  clustering.  Regression  testing  is  an  indispensable  process  in  software  development,  which  ensures  stable  features  have  not  been  adversely  broken  by  new  changes.  When  GUI  plays  an  important  role  in  an  application,  a  popular  choice  to  automate  the  tests  is  applying  GUI  Capture/Replay  tools.  Unfortunately,  in  many  applications  which  render  images  as  output,  the  correctness  of  a  replay  run  can  no  longer  be  straightforwardly  verified.  In  this  paper,  we  propose  a  test  automation  solution,  called  Intrusive  Test  Automation,  which  uses  program  instrumentation  technique  to  collect  the  runtime  internal  information  of  a  program.  As  a  result,  the  correctness  of  a  test  run  can  be  verified  by  the  runtime  traces.  In  addition,  when  large  number  of  failed  test  cases  are  reported  by  the  test  automation  system,  recommending  some  representative  test  cases  as  a  start  for  debugging  can  be  helpful  to  programmers.  This  paper  proposes  a  clustering  technique  based  on  the  information  collected  from  the  instrumented  code.  In  principle,  fixing  bugs  in  one  representative  test  case  can  fix  its  related  failed  test  cases  as  well.  A  case  study  is  presented  to  demonstrate  the  effectiveness  of  the  approach.
1	Autocomment  mining  question  and  answer  sites  for  automatic  comment  generation.  Code  comments  improve  software  maintainability.  To  address  the  comment  scarcity  issue,  we  propose  a  new  automatic  comment  generation  approach,  which  mines  comments  from  a  large  programming  Question  and  Answer  (Q&A)  site.  Q&A  sites  allow  programmers  to  post  questions  and  receive  solutions,  which  contain  code  segments  together  with  their  descriptions,  referred  to  as  code-description  mappings.  We  develop  AutoComment  to  extract  such  mappings,  and  leverage  them  to  generate  description  comments  automatically  for  similar  code  segments  matched  in  open-source  projects.  We  apply  AutoComment  to  analyze  Java  and  Android  tagged  Q&A  posts  to  extract  132,767  code-description  mappings,  which  help  AutoComment  to  generate  102  comments  automatically  for  23  Java  and  Android  projects.  The  user  study  results  show  that  the  majority  of  the  participants  consider  the  generated  comments  accurate,  adequate,  concise,  and  useful  in  helping  them  understand  the  code.
1	Data  mining  methods  and  cost  estimation  models  why  is  it  so  hard  to  infuse  new  ideas.  Infusing  new  technologies  and  methods  is  hard  and  can  often  be  described  as  "banging  ones  head  on  a  brick  wall".  The  is  especially  true  when  trying  to  get  project  managers,  systems,  engineers  and  cost  analysts  to  add  a  radically  new  tool  to  their  tool  box.  In  this  paper  we  suggest  that  the  underlying  causes  are  rooted  in  the  fact  that  the  different  players  have  fundamental  differences  in  mental  models,  vocabulary  and  objectives.  We  based  this  work  on  lessons  learned  from  ten  years  of  working  on  the  infusion  of  software  costing  models  into  NASA.  The  good  news  is  that,  lately,  a  crack  has  begun  to  appear  in  what  was  previously  a  brick  wall.
1	Assertion  guided  abstraction  a  cooperative  optimization  for  dynamic  partial  order  reduction.  We  propose  a  new  method  for  reducing  the  interleaving  space  during  stateless  model  checking  of  multithreaded  C/C++  programs.  The  problem  is  challenging  because  of  the  exponential  growth  of  possible  interleavings  between  threads.  We  have  developed  a  new  method,  called  assertion  guided  abstraction,  which  leverages  both  static  and  dynamic  program  analyses  in  a  cooperative  framework  to  reduce  the  interleaving  space.  Unlike  existing  methods  that  consider  all  interleavings  of  all  conflicting  memory  accesses  in  a  program,  our  new  method  relies  on  a  new  notion  of  predicate  dependence  based  on  which  we  can  soundly  abstract  the  interleaving  space  to  only  those  conflicting  memory  accesses  that  may  cause  assertion  violations  and/or  deadlocks.  Our  experimental  evaluation  of  assertion  guided  abstraction  on  open  source  benchmarks  shows  that  it  is  capable  of  achieving  a  significant  reduction,  thereby  allowing  for  the  verification  of  programs  that  were  previously  too  complex  for  existing  algorithms  to  handle.
1	Testing  cross  platform  mobile  app  development  frameworks  t.  Mobile  app  developers  often  wish  to  make  their  apps  available  on  a  wide  variety  of  platforms,  e.g.,  Android,  iOS,  and  Windows  devices.  Each  of  these  platforms  uses  a  different  programming  environment,  each  with  its  own  language  and  APIs  for  app  development.  Small  app  development  teams  lack  the  resources  and  the  expertise  to  build  and  maintain  separate  code  bases  of  the  app  customized  for  each  platform.  As  a  result,  we  are  beginning  to  see  a  number  of  cross-platform  mobile  app  development  frameworks.  These  frameworks  allow  the  app  developers  to  specify  the  business  logic  of  the  app  once,  using  the  language  and  APIs  of  a  home  platform  (e.g.,  Windows  Phone),  and  automatically  produce  versions  of  the  app  for  multiple  target  platforms  (e.g.,  iOS  and  Android).      In  this  paper,  we  focus  on  the  problem  of  testing  cross-platform  app  development  frameworks.  Such  frameworks  are  challenging  to  develop  because  they  must  correctly  translate  the  home  platform  API  to  the  (possibly  disparate)  target  platform  API  while  providing  the  same  behavior.  We  develop  a  differential  testing  methodology  to  identify  inconsistencies  in  the  way  that  these  frameworks  handle  the  APIs  of  the  home  and  target  platforms.  We  have  built  a  prototype  testing  tool,  called  X-Checker,  and  have  applied  it  to  test  Xamarin,  a  popular  framework  that  allows  Windows  Phone  apps  to  be  cross-compiled  into  native  Android  (and  iOS)  apps.  To  date,  X-Checker  has  found  47  bugs  in  Xamarin,  corresponding  to  inconsistencies  in  the  way  that  Xamarin  translates  between  the  semantics  of  the  Windows  Phone  and  the  Android  APIs.  We  have  reported  these  bugs  to  the  Xamarin  developers,  who  have  already  committed  patches  for  twelve  of  them.
1	Efficient  and  formal  generalized  symbolic  execution.  Programs  that  manipulate  dynamic  heap  objects  are  difficult  to  analyze  due  to  issues  like  aliasing.  Lazy  initialization  algorithm  enables  the  classical  symbolic  execution  to  handle  such  programs.  Despite  its  successes,  there  are  two  unresolved  issues:  (1)  inefficiency;  (2)  lack  of  formal  study.  For  the  inefficiency  issue,  we  have  proposed  two  improved  algorithms  that  give  significant  analysis  time  reduction  over  the  original  lazy  initialization  algorithm.  In  this  article,  we  formalize  the  lazy  initialization  algorithm  and  the  improved  algorithms  as  operational  semantics  of  a  core  subset  of  the  Java  Virtual  Machine  (JVM)  instructions,  and  prove  that  all  algorithms  are  relatively  sound  and  complete  with  respect  to  the  JVM  concrete  semantics.  Finally,  we  conduct  a  set  of  extensive  experiments  that  compare  the  three  algorithms  and  demonstrate  the  efficiency  of  the  improved  algorithms.
1	Fast  and  precise  points  to  analysis  with  incremental  cfl  reachability  summarisation  preliminary  experience.  We  describe  our  preliminary  experience  in  the  design  and  implementation  of  a  points-to  analysis  for  Java,  called  EMU,  that  enables  developers  to  perform  pointer-related  queries  in  programs  undergoing  constant  changes  in  IDEs.  EMU  achieves  fast  response  times  by  adopting  a  modular  approach  to  incrementally  updating  method  summaries  upon  small  code  changes:  the  points-to  information  in  a  method  is  summarised  indirectly  by  CFL  reachability  rather  than  directly  by  points-to  sets.  Thus,  the  impact  of  a  small  code  change  made  in  a  method  is  localised,  requiring  only  its  affected  part  to  be  re-summarised  just  to  reflect  the  change.  EMU  achieves  precision  by  being  context-sensitive  (for  both  method  invocation  and  heap  abstraction)  and  field-sensitive.  Our  evaluation  shows  that  EMU  can  be  promisingly  deployed  in  IDEs  where  the  changes  are  small.
1	Learning  a  dual  language  vector  space  for  domain  specific  cross  lingual  question  retrieval.  The  lingual  barrier  limits  the  ability  of  millions  of  non-English  speaking  developers  to  make  effective  use  of  the  tremendous  knowledge  in  Stack  Overflow,  which  is  archived  in  English.  For  cross-lingual  question  retrieval,  one  may  use  translation-based  methods  that  first  translate  the  non-English  queries  into  English  and  then  perform  monolingual  question  retrieval  in  English.  However,  translation-based  methods  suffer  from  semantic  deviation  due  to  inappropriate  translation,  especially  for  domain-specific  terms,  and  lexical  gap  between  queries  and  questions  that  share  few  words  in  common.  To  overcome  the  above  issues,  we  propose  a  novel  cross-lingual  question  retrieval  based  on  word  embed-dings  and  convolutional  neural  network  (CNN)  which  are  the  state-of-the-art  deep  learning  techniques  to  capture  word-  and  sentence-level  semantics.  The  CNN  model  is  trained  with  large  amounts  of  examples  from  Stack  Overflow  duplicate  questions  and  their  corresponding  translation  by  machine,  which  guides  the  CNN  to  learn  to  capture  informative  word  and  sentence  features  to  recognize  and  quantify  semantic  similarity  in  the  presence  of  semantic  deviations  and  lexical  gaps.  A  uniqueness  of  our  approach  is  that  the  trained  CNN  can  map  documents  in  two  languages  (e.g.,  Chinese  queries  and  English  questions)  in  a  dual-language  vector  space,  and  thus  reduce  the  cross-lingual  question  retrieval  problem  to  a  simple  k-nearest  neighbors  search  problem  in  the  dual-language  vector  space,  where  no  query  or  question  translation  is  required.  Our  evaluation  shows  that  our  approach  significantly  outperforms  the  translation-based  method,  and  can  be  extended  to  dual-language  documents  retrieval  from  different  sources.
1	Multi  modal  attention  network  learning  for  semantic  source  code  retrieval.  Code  retrieval  techniques  and  tools  have  been  playing  a  key  role  in  facilitating  software  developers  to  retrieve  existing  code  fragments  from  available  open-source  repositories  given  a  user  query  (e.g.,  a  short  natural  language  text  describing  the  functionality  for  retrieving  a  particular  code  snippet).  Despite  the  existing  efforts  in  improving  the  effectiveness  of  code  retrieval,  there  are  still  two  main  issues  hindering  them  from  being  used  to  accurately  retrieve  satisfiable  code  fragments  from  large-scale  repositories  when  answering  complicated  queries.  First,  the  existing  approaches  only  consider  shallow  features  of  source  code  such  as  method  names  and  code  tokens,  but  ignoring  structured  features  such  as  abstract  syntax  trees  (ASTs)  and  control-flow  graphs  (CFGs)  of  source  code,  which  contains  rich  and  well-defined  semantics  of  source  code.  Second,  although  the  deep  learning-based  approach  performs  well  on  the  representation  of  source  code,  it  lacks  the  explainability,  making  it  hard  to  interpret  the  retrieval  results  and  almost  impossible  to  understand  which  features  of  source  code  contribute  more  to  the  final  results.  To  tackle  the  two  aforementioned  issues,  this  paper  proposes  MMAN,  a  novel  Multi-Modal  Attention  Network  for  semantic  source  code  retrieval.  A  comprehensive  multi-modal  representation  is  developed  for  representing  unstructured  and  structured  features  of  source  code,  with  one  LSTM  for  the  sequential  tokens  of  code,  a  Tree-LSTM  for  the  AST  of  code  and  a  GGNN  (Gated  Graph  Neural  Network)  for  the  CFG  of  code.  Furthermore,  a  multi-modal  attention  fusion  layer  is  applied  to  assign  weights  to  different  parts  of  each  modality  of  source  code  and  then  integrate  them  into  a  single  hybrid  representation.  Comprehensive  experiments  and  analysis  on  a  large-scale  real-world  dataset  show  that  our  proposed  model  can  accurately  retrieve  code  snippets  and  outperforms  the  state-of-the-art  methods.
1	Automatic  generation  of  pull  request  descriptions.  Enabled  by  the  pull-based  development  model,  developers  can  easily  contribute  to  a  project  through  pull  requests  (PRs).  When  creating  a  PR,  developers  can  add  a  free-form  description  to  describe  what  changes  are  made  in  this  PR  and/or  why.  Such  a  description  is  helpful  for  reviewers  and  other  developers  to  gain  a  quick  understanding  of  the  PR  without  touching  the  details  and  may  reduce  the  possibility  of  the  PR  being  ignored  or  rejected.  However,  developers  sometimes  neglect  to  write  descriptions  for  PRs.  For  example,  in  our  collected  dataset  with  over  333K  PRs,  more  than  34%  of  the  PR  descriptions  are  empty.  To  alleviate  this  problem,  we  propose  an  approach  to  automatically  generate  PR  descriptions  based  on  the  commit  messages  and  the  added  source  code  comments  in  the  PRs.  We  regard  this  problem  as  a  text  summarization  problem  and  solve  it  using  a  novel  sequence-to-sequence  model.  To  cope  with  out-of-vocabulary  words  in  software  artifacts  and  bridge  the  gap  between  the  training  loss  function  of  the  sequence-to-sequence  model  and  the  evaluation  metric  ROUGE,  which  has  been  shown  to  correspond  to  human  evaluation,  we  integrate  the  pointer  generator  and  directly  optimize  for  ROUGE  using  reinforcement  learning  and  a  special  loss  function.  We  build  a  dataset  with  over  41K  PRs  and  evaluate  our  approach  on  this  dataset  through  ROUGE  and  a  human  evaluation.  Our  evaluation  results  show  that  our  approach  outperforms  two  baselines  by  significant  margins.
1	Concolic  testing  for  deep  neural  networks.  Concolic  testing  combines  program  execution  and  symbolic  analysis  to  explore  the  execution  paths  of  a  software  program.  In  this  paper,  we  develop  the  first  concolic  testing  approach  for  Deep  Neural  Networks  (DNNs).  More  specifically,  we  utilise  quantified  linear  arithmetic  over  rationals  to  express  test  requirements  that  have  been  studied  in  the  literature,  and  then  develop  a  coherent  method  to  perform  concolic  testing  with  the  aim  of  better  coverage.  Our  experimental  results  show  the  effectiveness  of  the  concolic  testing  approach  in  both  achieving  high  coverage  and  finding  adversarial  examples.
1	Tdroid  exposing  app  switching  attacks  in  android  with  control  flow  specialization.  The  Android  multitasking  mechanism  can  be  plagued  with  app  switching  attacks,  in  which  a  malicious  app  replaces  the  legitimate  top  activity  of  the  focused  app  with  one  of  its  own,  thus  mounting,  e.g.,  phishing  and  denial-of-service  attacks.  Existing  market-level  defenses  are  still  ineffective,  as  static  analysis  is  fundamentally  unable  to  reason  about  the  intention  of  an  app  and  dynamic  analysis  has  low  coverage.  We  introduce  TDroid,  a  new  market-level  approach  to  detecting  app  switching  attacks.  The  challenge  lies  in  how  to  handle  a  plethora  of  input-dependent  branch  predicates  (forming  an  exponential  number  of  paths)  that  control  the  execution  of  the  code  responsible  for  launching  such  attacks.  TDroid  tackles  this  challenge  by  combining  static  and  dynamic  analysis  to  analyze  an  app  without  producing  any  false  positives.  In  its  static  analysis,  TDroid  transforms  the  app  into  runnable  slices  containing  potentially  app  switching  attacks,  one  slice  per  attack.  In  its  dynamic  analysis,  TDroid  executes  these  slices  on  an  Android  phone  or  emulator  to  expose  their  malicious  GUIs.  The  novelty  lies  in  the  use  of  a  new  trigger-oriented  slicing  technique  in  producing  runnable  slices  so  that  certain  input-dependent  branch  predicates  are  specialized  to  execute  always  some  fixed  branches.  Evaluated  with  a  large  set  of  malware  apps,  TDroid  is  shown  to  outperform  the  state  of  the  art,  by  detecting  substantially  more  app  switching  attacks,  in  a  few  minutes  per  app,  on  average.
1	Business  sustainability  frameworks  a  survey.  This  paper  aims  to  formulate  a  number  of  research  questions  based  on  a  literature  survey.  A  critical  review  of  literature  has  been  undertaken  to  evaluate  the  existing  business  sustainability  frameworks.  Business  Sustainability  frameworks  from  a  number  of  domains  such  as  manufacturing,  e-learning,  agriculture,  bioenergy  and  Information  technology  has  been  reviewed.  The  knowledge  gap  in  this  space  has  been  clearly  outlined  and  a  number  of  research  questions  were  derived  from  this  discussion.  Based  upon  this  review  a  framework  will  be  proposed  using  the  inputs  from  the  existing  literature.  The  proposed  framework  will  be  validated  by  interviewing  domain  experts.  Once  validated  the  framework  would  be  applied  to  two  different  ICT  businesses  either  from  Australia  or  overseas.  This  will  be  a  case  study  based  research  where  the  two  ICT  companies  will  be  assessed  using  the  proposed  framework  and  the  results  from  this  case  study  will  be  used  to  draw  generalized  conclusions  related  to  the  sustainability  of  ICT  businesses.
1	Efficient  genetic  k  means  clustering  for  health  care  knowledge  discovery.  Data  mining  and  machine  learning  are  becoming  the  most  interesting  research  areas  and  increasingly  popular  in  health  organizations.  The  hidden  patterns  among  patients  data  can  be  extracted  by  applying  data  mining.  The  techniques  and  tools  of  data  mining  are  very  helpful  as  they  provide  health  care  professionals  with  significant  knowledge  toward  a  decision.  Researchers  have  shown  several  utilities  of  data  mining  techniques  such  as  clustering,  classification,  and  regression  in  health  care  domain.  Particularly,  clustering  algorithms  which  help  researchers  discover  new  insights  by  segmenting  patients  and  providing  them  with  effective  treatments.  This  paper,  reviews  existing  methods  of  clustering  and  present  an  efficient  K-Means  clustering  algorithm  which  uses  Self  Organizing  Map  (SOM)  method  to  overcome  the  problem  of  finding  number  of  centroids  in  traditional  K-Means.  The  SOM  based  clustering  is  very  efficient  due  to  its  unsupervised  learning  and  topology  preserving  properties.  Two-staged  clustering  algorithm  uses  SOM  to  produce  the  prototypes  in  the  first  stage  and  then  use  those  prototypes  to  create  clusters  in  the  second  stage.  Two  health  care  datasets  are  used  in  the  proposed  experiments  and  a  cluster  accuracy  metric  was  applied  to  evaluate  the  performance  of  the  algorithm.  Our  analysis  shows  that  the  proposed  method  is  accurate  and  shows  better  clustering  performance  along  with  valuable  insights  for  each  cluster.  Our  approach  is  unsupervised,  scalable  and  can  be  applied  to  various  domains.
1	Inferring  annotations  for  device  drivers  from  verification  histories.  This  paper  studies  and  optimizes  automated  program  verification.  Detailed  reasoning  about  software  behavior  is  often  facilitated  by  program  invariants  that  hold  across  all  program  executions.  Finding  program  invariants  is  in  fact  an  essential  step  in  automated  program  verification.  Automatic  discovery  of  precise  invariants,  however,  can  be  very  difficult  in  practice.  The  problem  can  be  simplified  if  one  has  access  to  a  candidate  set  of  assertions  (or  annotations)  and  the  search  for  invariants  is  limited  over  the  space  defined  by  these  annotations.  Then,  the  main  challenge  is  to  automatically  generate  quality  program  annotations.  We  present  an  approach  that  infers  program  annotations  automatically  by  leveraging  the  history  of  verifying  related  programs.  Our  algorithm  extracts  high-quality  annotations  from  previous  verification  attempts,  and  then  applies  them  for  verifying  new  programs.  We  present  a  case  study  where  we  applied  our  algorithm  to  Microsoft’s  Static  Driver  Verifier  (SDV).  SDV  is  an  industrial-strength  tool  for  verification  of  Windows  device  drivers  that  uses  manually-tuned  heuristics  for  obtaining  a  set  of  annotations.  Our  technique  inferred  program  annotations  comparable  in  performance  to  the  existing  annotations  used  in  SDV  that  were  devised  manually  by  human  experts  over  years.  Additionally,  the  inferred  annotations  together  with  the  existing  ones  improved  the  performance  of  SDV  overall,  proving  correct  47%  of  drivers  more  while  running  22%  faster  in  our  experiments.
1	A  performance  comparison  of  contemporary  algorithmic  approaches  for  automated  analysis  operations  on  feature  models.  The  formalization  of  variability  models  (e.g.  feature  models)  is  a  prerequisite  for  the  automated  analysis  of  these  models.  The  efficient  execution  of  the  analysis  operations  depends  on  the  selection  of  well-suited  solver  implementations.  Regarding  feature  models,  on  the  one  hand,  the  formalization  with  Boolean  expressions  enables  the  use  of  SAT  or  BDD  solvers.  On  the  other  hand,  feature  models  can  be  transformed  into  a  Constraint-Satisfaction  Problem  (CSP)  in  order  to  use  CSP  solvers  for  validation.  This  paper  presents  a  performance  comparison  regarding  nine  contemporary  high-performance  solvers,  three  for  each  base  problem  structure  (BDD,  CSP,  and  SAT).  Four  operations  on  90  feature  models  are  run  on  each  solver.  The  results  will  in  turn  clear  the  way  for  new  improvements  regarding  the  automatic  verification  of  software  product  lines,  since  the  efficient  execution  of  analysis  operations  is  essential  to  such  automatic  verification  approaches.
1	Curricular  change  management  with  git  and  drupal  a  tool  to  support  flexible  curricular  development  workflows.  The  Software  Enterprise  at  ASU  aims  at  equipping  students  with  practical  knowledge  of  modern  software  practices  through  a  multi-year  instructional  sequence  that  employs  an  iterative  feedback  pedagogical  model  in  order  for  graduates  to  be  adequately  prepared  for  the  workforce  upon  graduation.  By  means  of  the  Software  Enterprise  Website,  the  community  of  teaching  practitioners  in  this  area  having  similar  beliefs  and  approaches  can  find  a  great  deal  of  support  and  free  resources  for  the  purpose  of  Software  Engineering  Education.  The  goal  of  this  project  is  to  incorporate  a  versioning  tool  to  a  Content  Management  Framework  (CMF)  that  allows  the  website  administrator  to  create,  moderate  and  publish  content  revisions,  particularly  that  of  the  Curricular  Modules  which  comprise  of  content  representing  topics  offered  in  courses,  while  versioning  the  actual  content  using  GitHub,  a  web-based  repository  hosting  service.  In  this  case,  the  CMF  used  was  Drupal  7,  which  is  PHP-written  and  open-source,  and  provides  a  back-end  framework  for  a  broad  range  of  web  sites  worldwide  from  blogs  and  microsites  to  collaborative  social  communities.  The  versioning  module  was  developed  in  PHP  while  employing  the  Drupal  7  API  for  menu  structuring  and  CMS  integration,  and  the  GitHub  API  for  versioning  and  performing  easy  syncing  with  contents  on  GitHub  via  a  RESTful  interface.
1	Straighttaint  decoupled  offline  symbolic  taint  analysis.  Taint  analysis  has  been  widely  applied  in  ex  post  facto  security  applications,  such  as  attack  provenance  investigation,  computer  forensic  analysis,  and  reverse  engineering.  Unfortunately,  the  high  runtime  overhead  imposed  by  dynamic  taint  analysis  makes  it  impractical  in  many  scenarios.  The  key  obstacle  is  the  strict  coupling  of  program  execution  and  taint  tracking  logic  code.  To  alleviate  this  performance  bottleneck,  recent  work  seeks  to  offload  taint  analysis  from  program  execution  and  run  it  on  a  spare  core  or  a  different  CPU.  However,  since  the  taint  analysis  has  heavy  data  and  control  dependencies  on  the  program  execution,  the  massive  data  in  recording  and  transformation  overshadow  the  benefit  of  decoupling.  In  this  paper,  we  propose  a  novel  technique  to  allow  very  lightweight  logging,  resulting  in  much  lower  execution  slowdown,  while  still  permitting  us  to  perform  full-featured  offline  taint  analysis.  We  develop  StraightTaint,  a  hybrid  taint  analysis  tool  that  completely  decouples  the  program  execution  and  taint  analysis.  StraightTaint  relies  on  very  lightweight  logging  of  the  execution  information  to  reconstruct  a  straight-line  code,  enabling  an  offline  symbolic  taint  analysis  without  frequent  data  communication  with  the  application.  While  StraightTaint  does  not  log  complete  runtime  or  input  values,  it  is  able  to  precisely  identify  the  causal  relationships  between  sources  and  sinks,  for  example.  Compared  with  traditional  dynamic  taint  analysis  tools,  StraightTaint  has  much  lower  application  runtime  overhead.
1	Static  analysis  of  implicit  control  flow  resolving  java  reflection  and  android  intents  t.  Implicit  or  indirect  control  flow  is  a  transfer  of  control  between  procedures  using  some  mechanism  other  than  an  explicit  procedure  call.  Implicit  control  flow  is  a  staple  design  pattern  that  adds  flexibility  to  system  design.  However,  it  is  challenging  for  a  static  analysis  to  compute  or  verify  properties  about  a  system  that  uses  implicit  control  flow.      This  paper  presents  static  analyses  for  two  types  of  implicit  control  flow  that  frequently  appear  in  Android  apps:  Java  reflection  and  Android  intents.  Our  analyses  help  to  resolve  where  control  flows  and  what  data  is  passed.  This  information  improves  the  precision  of  downstream  analyses,  which  no  longer  need  to  make  conservative  assumptions  about  implicit  control  flow.      We  have  implemented  our  techniques  for  Java.  We  enhanced  an  existing  security  analysis  with  a  more  precise  treatment  of  reflection  and  intents.  In  a  case  study  involving  ten  real-world  Android  apps  that  use  both  intents  and  reflection,  the  precision  of  the  security  analysis  was  increased  on  average  by  two  orders  of  magnitude.  The  precision  of  two  other  downstream  analyses  was  also  improved.
1	Jct  a  java  code  tomograph.  We  are  concerned  with  analyzing  software,  in  particular,  with  its  nature  and  how  developer  decisions  and  behavior  impact  the  quality  of  the  product  they  produce.  This  is  the  domain  of  empirical  software  engineering  where  measurement  seeks  to  capture  attributes  affecting  the  product,  process,  and  resources  of  software  development.  A  well-established  means  to  study  software  attributes  is  metrics  data  mining.  But,  even  though  a  variety  of  frameworks  have  emerged  that  can  distill  desired  measures  from  software  systems  (e.g.,  JHawk  or  SonarJ),  a  systematic  approach  to  collecting  measures  from  large  data  sets  has  still  eluded  us.  For  this  reason,  we  have  developed  the  Java  Code  Tomograph  (jCT),  a  novel  framework  for  metrics  extraction  and  processing.  jCT  offers  an  extensible  measurement  infrastructure  with  built-in  support  for  the  curated  repositories  Qualitas  Corpus  and  Helix.  With  jCT,  large-scale  empirical  studies  of  code  within  the  same  software  system  or  across  different  software  systems  become  feasible.  In  this  paper,  we  provide  an  overview  of  jCT's  main  design  features  and  discuss  its  operation  in  relation  to  the  effectiveness  of  the  framework.
1	Synergizing  specification  miners  through  model  fissions  and  fusions  t.  Software  systems  are  often  developed  and  released  without  formal  specifications.  For  those  systems  that  are  formally  specified,  developers  have  to  continuously  maintain  and  update  the  specifications  or  have  them  fall  out  of  date.  To  deal  with  the  absence  of  formal  specifications,  researchers  have  proposed  techniques  to  infer  the  missing  specifications  of  an  implementation  in  a  variety  of  forms,  such  as  finite  state  automaton  (FSA).  Despite  the  progress  in  this  area,  the  efficacy  of  the  proposed  specification  miners  needs  to  improve  if  these  miners  are  to  be  adopted.      We  propose  SpecForge,  a  new  specification  mining  approach  that  synergizes  many  existing  specification  miners.  SpecForge  decomposes  FSAs  that  are  inferred  by  existing  miners  into  simple  constraints,  through  a  process  we  refer  to  as  model  fission.  It  then  filters  the  outlier  constraints  and  fuses  the  constraints  back  together  into  a  single  FSA  (i.e.,  model  fusion).  We  have  evaluated  SpecForge  on  execution  traces  of  10  programs,  which  includes  5  programs  from  DaCapo  benchmark,  to  infer  behavioral  models  of  13  library  classes.  Our  results  show  that  SpecForge  achieves  an  average  precision,  recall  and  F-measure  of  90.57%,  54.58%,  and  64.21%  respectively.  SpecForge  outperforms  the  best  performing  baseline  by  13.75%  in  terms  of  F-measure.
1	Automatic  loop  invariant  generation  anc  refinement  through  selective  sampling.  Automatic  loop-invariant  generation  is  important  in  program  analysis  and  verification.  In  this  paper,  we  propose  to  generate  loop-invariants  automatically  through  learning  and  verification.  Given  a  Hoare  triple  of  a  program  containing  a  loop,  we  start  with  randomly  testing  the  program,  collect  program  states  at  run-time  and  categorize  them  based  on  whether  they  satisfy  the  invariant  to  be  discovered.  Next,  classification  techniques  are  employed  to  generate  a  candidate  loop-invariant  automatically.  Afterwards,  we  refine  the  candidate  through  selective  sampling  so  as  to  overcome  the  lack  of  sufficient  test  cases.  Only  after  a  candidate  invariant  cannot  be  improved  further  through  selective  sampling,  we  verify  whether  it  can  be  used  to  prove  the  Hoare  triple.  If  it  cannot,  the  generated  counterexamples  are  added  as  new  tests  and  we  repeat  the  above  process.  Furthermore,  we  show  that  by  introducing  a  path-sensitive  learning,  i.e.,  partitioning  the  program  states  according  to  program  locations  they  visit  and  classifying  each  partition  separately,  we  are  able  to  learn  disjunctive  loop-invariants.  In  order  to  evaluate  our  idea,  a  prototype  tool  has  been  developed  and  the  experiment  results  show  that  our  approach  complements  existing  approaches.
1	Semantic  patch  inference.  We  propose  a  tool  for  inferring  transformation  specifications  from  a  few  examples  of  original  and  updated  code.  These  transformation  specifications  may  contain  multiple  code  fragments  from  within  a  single  function,  all  of  which  must  be  present  for  the  transformation  to  apply.  This  makes  the  inferred  transformations  context  sensitive.  Our  algorithm  is  based  on  depth-first  search,  with  pruning.  Because  it  is  applied  locally  to  a  collection  of  functions  that  contain  related  changes,  it  is  efficient  in  practice.  We  illustrate  the  approach  on  an  example  drawn  from  recent  changes  to  the  Linux  kernel.
1	Usage  costs  and  benefits  of  continuous  integration  in  open  source  projects.  Continuous  integration  (CI)  systems  automate  the  compilation,  building,  and  testing  of  software.  Despite  CI  rising  as  a  big  success  story  in  automated  software  engineering,  it  has  received  almost  no  attention  from  the  research  community.  For  example,  how  widely  is  CI  used  in  practice,  and  what  are  some  costs  and  benefits  associated  with  CI?  Without  answering  such  questions,  developers,  tool  builders,  and  researchers  make  decisions  based  on  folklore  instead  of  data.  In  this  paper,  we  use  three  complementary  methods  to  study  the  usage  of  CI  in  open-source  projects.  To  understand  which  CI  systems  developers  use,  we  analyzed  34,544  open-source  projects  from  GitHub.  To  understand  how  developers  use  CI,  we  analyzed  1,529,291  builds  from  the  most  commonly  used  CI  system.  To  understand  why  projects  use  or  do  not  use  CI,  we  surveyed  442  developers.  With  this  data,  we  answered  several  key  questions  related  to  the  usage,  costs,  and  benefits  of  CI.  Among  our  results,  we  show  evidence  that  supports  the  claim  that  CI  helps  projects  release  more  often,  that  CI  is  widely  adopted  by  the  most  popular  projects,  as  well  as  finding  that  the  overall  percentage  of  projects  using  CI  continues  to  grow,  making  it  important  and  timely  to  focus  more  research  on  CI.
1	An  extensible  framework  for  variable  precision  data  flow  analyses  in  mps.  Data-flow  analyses  are  used  as  part  of  many  software  engineering  tasks:  they  are  the  foundations  of  program  understanding,  refactorings  and  optimized  code  generation.  Similar  to  general-purpose  languages  (GPLs),  state-of-the-art  domain-specific  languages  (DSLs)  also  require  sophisticated  data-flow  analyses.  However,  as  a  consequence  of  the  different  economies  of  DSL  development  and  their  typically  relatively  fast  evolution,  the  effort  for  developing  and  evolving  such  analyses  must  be  lowered  compared  to  GPLs.  This  tension  can  be  resolved  with  dedicated  support  for  data-flow  analyses  in  language  workbenches.  In  this  tool  paper  we  present  MPS-DF,  which  is  the  component  in  the  MPS  language  workbench  that  supports  the  definition  of  data-flow  analyses  for  DSLs.  Language  developers  can  define  data-flow  graph  builders  declaratively  as  part  of  a  language  definition  and  compute  analysis  results  efficiently  based  on  these  data-flow  graphs.  MPS-DF  is  extensible  such  that  it  does  not  compromise  the  support  for  language  composition  in  MPS.  Additionally,  clients  of  MPS-DF  analyses  can  run  the  analyses  with  variable  precision  thus  trading  off  precision  for  performance.  This  allows  clients  to  tailor  an  analysis  to  a  particular  use  case.  Demo  video  of  MPS-DF:  https://youtu.be/laNDAZCe2jM.
1	Deepgauge  multi  granularity  testing  criteria  for  deep  learning  systems.  Deep  learning  (DL)  defines  a  new  data-driven  programming  paradigm  that  constructs  the  internal  system  logic  of  a  crafted  neuron  network  through  a  set  of  training  data.  We  have  seen  wide  adoption  of  DL  in  many  safety-critical  scenarios.  However,  a  plethora  of  studies  have  shown  that  the  state-of-the-art  DL  systems  suffer  from  various  vulnerabilities  which  can  lead  to  severe  consequences  when  applied  to  real-world  applications.  Currently,  the  testing  adequacy  of  a  DL  system  is  usually  measured  by  the  accuracy  of  test  data.  Considering  the  limitation  of  accessible  high  quality  test  data,  good  accuracy  performance  on  test  data  can  hardly  provide  confidence  to  the  testing  adequacy  and  generality  of  DL  systems.  Unlike  traditional  software  systems  that  have  clear  and  controllable  logic  and  functionality,  the  lack  of  interpretability  in  a  DL  system  makes  system  analysis  and  defect  detection  difficult,  which  could  potentially  hinder  its  real-world  deployment.  In  this  paper,  we  propose  DeepGauge,  a  set  of  multi-granularity  testing  criteria  for  DL  systems,  which  aims  at  rendering  a  multi-faceted  portrayal  of  the  testbed.  The  in-depth  evaluation  of  our  proposed  testing  criteria  is  demonstrated  on  two  well-known  datasets,  five  DL  systems,  and  with  four  state-of-the-art  adversarial  attack  techniques  against  DL.  The  potential  usefulness  of  DeepGauge  sheds  light  on  the  construction  of  more  generic  and  robust  DL  systems.
1	How  verified  or  tested  is  my  code  falsification  driven  verification  and  testing.  Formal  verification  has  advanced  to  the  point  that  developers  can  verify  the  correctness  of  small,  critical  modules.  Unfortunately,  despite  considerable  efforts,  determining  if  a  “verification”  verifies  what  the  author  intends  is  still  difficult.  Previous  approaches  are  difficult  to  understand  and  often  limited  in  applicability.  Developers  need  verification  coverage  in  terms  of  the  software  they  are  verifying,  not  model  checking  diagnostics.  We  propose  a  methodology  to  allow  developers  to  determine  (and  correct)  what  it  is  that  they  have  verified,  and  tools  to  support  that  methodology.  Our  basic  approach  is  based  on  a  novel  variation  of  mutation  analysis  and  the  idea  of  verification  driven  by  falsification.  We  use  the  CBMC  model  checker  to  show  that  this  approach  is  applicable  not  only  to  simple  data  structures  and  sorting  routines,  and  verification  of  a  routine  in  Mozilla’s  JavaScript  engine,  but  to  understanding  an  ongoing  effort  to  verify  the  Linux  kernel  read-copy-update  mechanism.  Moreover,  we  show  that  despite  the  probabilistic  nature  of  random  testing  and  the  tendency  to  incompleteness  of  testing  as  opposed  to  verification,  the  same  techniques,  with  suitable  modifications,  apply  to  automated  test  generation  as  well  as  to  formal  verification.  In  essence,  it  is  the  number  of  surviving  mutants  that  drives  the  scalability  of  our  methods,  not  the  underlying  method  for  detecting  faults  in  a  program.  From  the  point  of  view  of  a  Popperian  analysis  where  an  unkilled  mutant  is  a  weakness  (in  terms  of  its  falsifiability)  in  a  “scientific  theory”  of  program  behavior,  it  is  only  the  number  of  weaknesses  to  be  examined  by  a  user  that  is  important.
1	Comparing  model  coverage  and  code  coverage  in  model  driven  testing  an  exploratory  study.  The  Model  Driven  Architecture  (MDA)  approach  is  emerged  in  the  last  years  as  a  novel  software  design  methodology  for  the  development  of  software  systems.  In  this  approach  the  focus  of  software  development  is  shifted  from  writing  code  to  modeling.  In  MDA,  developers  implement  models  that  are  automatically  transformed  into  the  target  code  of  the  system.  Alongside  MDA,  the  Model  Driven  Testing  (MDT)  is  emerging  as  a  relevant  research  topic  in  both  industrial  and  scientific  communities.  MDT  is  a  methodology  where  test  cases  for  the  system  are  automatically  obtained  starting  from  test  models  to  maximize  specific  model  coverage  criteria.  Eventually,  test  cases  are  executed  to  verify  the  system  code  that  is  generated  through  an  MDA  approach.  In  this  paper,  we  conduct  an  exploratory  study  in  order  to  evaluate  the  differences  that  may  exist  between  the  model  coverage  guaranteed  by  the  test  cases  and  the  code  coverage  reached  when  they  are  executed  on  the  auto-generated  code.  Moreover,  we  identify  the  main  factors  that  may  influence  these  differences.
1	Hybrid  intrusion  detection  system  using  machine  learning  techniques  in  cloud  computing  environments.  Intrusion  detection  is  one  essential  tool  towards  building  secure  and  trustworthy  Cloud  computing  environment,  given  the  ubiquitous  presence  of  cyber  attacks  that  proliferate  rapidly  and  morph  dynamically.  In  our  current  working  paradigm  of  resource,  platform  and  service  consolidations,  Cloud  Computing  provides  a  significant  improvement  in  the  cost  metrics  via  dynamic  provisioning  of  IT  services.  Since  almost  all  cloud  computing  networks  lean  on  providing  their  services  through  Internet,  they  are  prone  to  experience  variety  of  security  issues.  Therefore,  in  cloud  environments,  it  is  necessary  to  deploy  an  Intrusion  Detection  System  (IDS)  to  detect  new  and  unknown  attacks  in  addition  to  signature  based  known  attacks,  with  high  accuracy.  In  our  deliberation  we  assume  that  a  system  or  a  network  “anomalous”  event  is  synonymous  to  an  “intrusion”  event  when  there  is  a  significant  departure  in  one  or  more  underlying  system  or  network  activities.  There  are  couple  of  recently  proposed  ideas  that  aim  to  develop  a  hybrid  detection  mechanism,  combining  advantages  of  signature-based  detection  schemes  with  the  ability  to  detect  unknown  attacks  based  on  anomalies.  In  this  work,  we  propose  a  network  based  anomaly  detection  system  at  the  Cloud  Hypervisor  level  that  utilizes  a  hybrid  algorithm:  a  combination  of  K-means  clustering  algorithm  and  SVM  classification  algorithm,  to  improve  the  accuracy  of  the  anomaly  detection  system.  Dataset  from  UNSW-NB15  study  is  used  to  evaluate  the  proposed  approach  and  results  are  compared  with  previous  studies.  The  accuracy  for  our  proposed  K-means  clustering  model  is  slightly  higher  than  others.  However,  the  accuracy  we  obtained  from  the  SVM  model  is  still  low  for  supervised  techniques.
1	Model  based  testing  for  software  safety  a  systematic  mapping  study.  Testing  safety-critical  systems  is  crucial  since  a  failure  or  malfunction  may  result  in  death  or  serious  injuries  to  people,  equipment,  or  environment.  An  important  challenge  in  testing  is  the  derivation  of  test  cases  that  can  identify  the  potential  faults.  Model-based  testing  adopts  models  of  a  system  under  test  and/or  its  environment  to  derive  test  artifacts.  This  paper  aims  to  provide  a  systematic  mapping  study  to  identify,  analyze,  and  describe  the  state-of-the-art  advances  in  model-based  testing  for  software  safety.  The  systematic  mapping  study  is  conducted  as  a  multi-phase  study  selection  process  using  the  published  literature  in  major  software  engineering  journals  and  conference  proceedings.  We  reviewed  751  papers  and  36  of  them  have  been  selected  as  primary  studies  to  answer  our  research  questions.  Based  on  the  analysis  of  the  data  extraction  process,  we  discuss  the  primary  trends  and  approaches  and  present  the  identified  obstacles.  This  study  shows  that  model-based  testing  can  provide  important  benefits  for  software  safety  testing.  Several  solution  directions  have  been  identified,  but  further  research  is  critical  for  reliable  model-based  testing  approach  for  safety.
1	An  empirically  validated  simulation  for  understanding  the  relationship  between  process  conformance  and  technology  skills.  Software  development  is  a  fast-paced  environment  where  developers  need  constant  update  to  ever-changing  technologies.  Furthermore,  process  improvement  initiatives  have  been  proven  useful  in  increasing  the  productivity  of  a  software  organization.  As  such,  these  organizations  need  to  decide  where  to  invest  their  training  budget.  As  a  result,  training  in  technological  update  to  their  workforce  or  training  in  process  conformance  with  its  productive  processes  become  conflicting  alternatives.  This  paper  presents  a  system  dynamics  simulation  of  a  software  factory  product  line.  The  objective  of  this  simulation  is  to  understand  the  changes  in  behavior  when  selecting  either  one  of  the  above-training  alternatives.  The  system  dynamics  model  was  validated  with  an  expert  panel,  and  the  simulation  results  have  been  empirically  validated--using  statistical  process  control--against  the  performance  baseline  of  a  real  software  development  organization.  With  the  simulation  under  statistical  control  and  performing  like  the  baseline,  the  independent  variables  representing  process  conformance  (process  training)  and  technology  skills  (skills  training)  were  modified  to  study  their  impact  on  product  defects  and  process  stability.  Our  results  show  that  while  both  variables  have  positive  impact  on  defects  and  process  stability,  investment  in  process  training  results  in  a  process  with  less  variation  and  with  fewer  defects.
1	Applying  black  box  testing  to  uml  ocl  database  models.  Most  Unified  Modeling  Language  (UML)  computer-aided  software  engineering  tools  have  been  insufficient  in  the  development  process  because  they  provide  little  support  for  conceptual  model  testing.  Model  testing  aims  to  ensure  the  correctness  of  a  UML/OCL  class  diagram,  or,  in  other  words,  that  a  given  class  diagram  can  perfectly  meet  the  user's  requirements.  This  study  proposes  the  validation  of  class  diagrams  with  black-box  testing,  a  technique  used  to  test  software  without  focusing  on  the  software's  implementation  or  structure.  An  approach  is  proposed  for  the  automatic  transformation  of  the  constraints  of  a  UML/OCL  class  diagram  into  test  cases.  Following  the  creation  of  the  test  cases,  they  are  executed  with  JUnit  and  the  results  produced  are  shown  to  the  tester.  To  demonstrate  the  applicability  of  this  approach,  an  effectiveness  evaluation  and  an  efficiency  evaluation  are  performed  here.  Evaluation  studies  show  that  all  faults  included  in  a  class  diagram  have  been  detected  within  an  efficient  time.
1	Renn  efficient  reverse  execution  with  neural  network  assisted  alias  analysis.  Reverse  execution  and  coredump  analysis  have  long  been  used  to  diagnose  the  root  cause  of  software  crashes.  Each  of  these  techniques,  however,  face  inherent  challenges,  such  as  insufficient  capability  when  handling  memory  aliases.  Recent  works  have  used  hypothesis  testing  to  address  this  drawback,  albeit  with  high  computational  complexity,  making  them  impractical  for  real  world  applications.  To  address  this  issue,  we  propose  a  new  deep  neural  architecture,  which  could  significantly  improve  memory  alias  resolution.  At  the  high  level,  our  approach  employs  a  recurrent  neural  network  (RNN)  to  learn  the  binary  code  pattern  pertaining  to  memory  accesses.  It  then  infers  the  memory  region  accessed  by  memory  references.  Since  memory  references  to  different  regions  naturally  indicate  a  non-alias  relationship,  our  neural  architecture  can  greatly  reduce  the  burden  of  doing  hypothesis  testing  to  track  down  non-alias  relation  in  binary  code.  Different  from  previous  researches  that  have  utilized  deep  learning  for  other  binary  analysis  tasks,  the  neural  network  proposed  in  this  work  is  fundamentally  novel.  Instead  of  simply  using  off-the-shelf  neural  networks,  we  designed  a  new  recurrent  neural  architecture  that  could  capture  the  data  dependency  between  machine  code  segments.  To  demonstrate  the  utility  of  our  deep  neural  architecture,  we  implement  it  as  RENN,  a  neural  network-assisted  reverse  execution  system.  We  utilize  this  tool  to  analyze  software  crashes  corresponding  to  40  memory  corruption  vulnerabilities  from  the  real  world.  Our  experiments  show  that  RENN  can  significantly  improve  the  efficiency  of  locating  the  root  cause  for  the  crashes.  Compared  to  a  state-of-the-art  technique,  RENN  has  36.25%  faster  execution  time  on  average,  detects  an  average  of  21.35%  more  non-alias  pairs,  and  successfully  identified  the  root  cause  of  12.5%  more  cases.
1	Codeexchange  supporting  reformulation  of  internet  scale  code  queries  in  context  t.  Programming  today  regularly  involves  searching  for  source  code  online,  whether  through  a  general  search  engine  such  as  Google  or  a  specialized  code  search  engine  such  as  SearchCode,  Ohloh,  or  GitHub.  Searching  typically  is  an  iterative  process,  with  develop-ers  adjusting  the  keywords  they  use  based  on  the  results  of  the  previous  query.  However,  searching  in  this  manner  is  not  ideal,  because  just  using  keywords  places  limits  on  what  developers  can  express  as  well  as  the  overall  interaction  that  is  required.  Based  on  the  observation  that  the  results  from  one  query  create  a  con-text  in  which  a  next  is  formulated,  we  present  CodeExchange,  a  new  code  search  engine  that  we  developed  to  explicitly  leverage  this  context  to  support  fluid,  expressive  reformulation  of  queries.  We  motivate  the  need  for  CodeExchange,  highlight  its  key  design  decisions  and  overall  architecture,  and  evaluate  its  use  in  both  a  field  deployment  and  a  laboratory  study.
1	Recommendation  system  for  software  refactoring  using  innovization  and  interactive  dynamic  optimization.  We  propose  a  novel  recommendation  tool  for  software  refactoring  that  dynamically  adapts  and  suggests  refactorings  to  developers  interactively  based  on  their  feedback  and  introduced  code  changes.  Our  approach  starts  by  finding  upfront  a  set  of  non-dominated  refactoring  solutions  using  NSGA-II  to  improve  software  quality,  reduce  the  number  of  refactorings  and  increase  semantic  coherence.  The  generated  non-dominated  refactoring  solutions  are  analyzed  using  our  innovization  component  to  extract  some  interesting  common  features  between  them.  Based  on  this  analysis,  the  suggested  refactorings  are  ranked  and  suggested  to  the  developer  one  by  one.  The  developer  can  approve,  modify  or  reject  each  suggested  refactoring,  and  this  feedback  is  used  to  update  the  ranking  of  the  suggested  refactorings.  After  a  number  of  introduced  code  changes,  a  local  search  is  performed  to  update  and  adapt  the  set  of  refactoring  solutions  suggested  by  NSGA-II.  We  evaluated  this  tool  on  four  large  open  source  systems  and  one  industrial  project  provided  by  our  partner.  Statistical  analysis  of  our  experiments  over  31  runs  shows  that  the  dynamic  refactoring  approach  performed  significantly  better  than  three  other  search-based  refactoring  techniques,  manual  refactorings,  and  one  refactoring  tool  not  based  on  heuristic  search.
1	Deviation  management  during  process  execution.  Software  development  companies  have  been  putting  a  lot  of  effort  in  adopting  process  models,  however  two  main  issues  remain.  On  the  one  hand,  process  models  are  inherently  incomplete,  since  companies  can  not  capture  all  possible  situations  in  a  single  model.  On  the  other  hand,  managers  can  not  force  process  participants  (agents)  to  strictly  follow  these  models.  The  effect  of  both  issues  is  that  companies  need  to  be  able  to  handle  deviations  during  process  enactment.  In  order  to  make  sure  that  process  agents  follow  the  process  model  and  that  their  deviations  get  detected  and  handled,  they  adopt  the  so-called  Process-centered  Software  Engineering  Environments  (PSEEs).  Unfortunately,  the  options  proposed  by  these  tools,  when  it  comes  to  handling  a  deviation,  are  rather  limited  to  basically  ignoring  or  forbidding  it.  In  the  present  work,  we  face  this  limitation  by  presenting  an  approach  for  detecting,  managing  and  tolerating  agent  deviations.  Besides,  in  this  paper  we  present  the  formal  specification  for  this  approach  in  the  Linear  Temporal  Logic  (LTL).  It  has  been  used  as  a  the  basis  of  our  PSEE  prototype.
1	Inferring  specifications  for  resources  from  natural  language  api  documentation.  Many  software  libraries,  especially  those  commercial  ones,  provide  API  documentation  in  natural  languages  to  describe  correct  API  usages.  However,  developers  may  still  write  code  that  is  inconsistent  with  API  documentation,  partially  because  many  developers  are  reluctant  to  carefully  read  API  documentation  as  shown  by  existing  research.  As  these  inconsistencies  may  indicate  defects,  researchers  have  proposed  various  detection  approaches,  and  these  approaches  need  many  known  specifications.  As  it  is  tedious  to  write  specifications  manually  for  all  APIs,  various  approaches  have  been  proposed  to  mine  specifications  automatically.  In  the  literature,  most  existing  mining  approaches  rely  on  analyzing  client  code,  so  these  mining  approaches  would  fail  to  mine  specifications  when  client  code  is  not  sufficient.  Instead  of  analyzing  client  code,  we  propose  an  approach,  called  Doc2Spec,  that  infers  resource  specifications  from  API  documentation  in  natural  languages.  We  evaluated  our  approach  on  the  Javadocs  of  five  libraries.  The  results  show  that  our  approach  performs  well  on  real  scale  libraries,  and  infers  various  specifications  with  relatively  high  precisions,  recalls,  and  F-scores.  We  further  used  inferred  specifications  to  detect  defects  in  open  source  projects.  The  results  show  that  specifications  inferred  by  Doc2Spec  are  useful  to  detect  real  defects  in  existing  projects.
1	An  automated  framework  for  recommending  program  elements  to  novices  n.  Novice  programmers  often  learn  programming  by  implementing  well-known  algorithms.  There  are  several  challenges  in  the  process.  Recommendation  systems  in  software  currently  focus  on  programmer  productivity  and  ease  of  development.  Teaching  aides  for  such  novice  programmers  based  on  recommendation  systems  still  remain  an  underexplored  area.  In  this  paper,  we  present  a  general  framework  for  recognizing  the  desired  target  for  partially-written  code  and  recommending  a  reliable  series  of  edits  to  transform  the  input  program  into  the  target  solution.  Our  code  analysis  is  based  on  graph  matching  and  tree  edit  algorithms.  Our  experimental  results  show  that  efficient  graph  comparison  techniques  can  accurately  match  two  portions  of  source  code  and  produce  an  accurate  set  of  source  code  edits.  We  provide  details  on  implementation  of  our  framework,  which  is  developed  as  a  plugin  for  Java  in  Eclipse  IDE.
1	Efficient  parametric  runtime  verification  with  deterministic  string  rewriting.  Early  efforts  in  runtime  verification  show  that  parametric  regular  and  temporal  logic  specifications  can  be  monitored  efficiently.  These  approaches,  however,  have  limited  expressiveness:  their  specifications  always  reduce  to  monitors  with  finite  state.  More  recent  developments  showed  that  parametric  context-free  properties  can  be  efficiently  monitored  with  overheads  generally  lower  than  12-15%.  While  context-free  grammars  are  more  expressive  than  finite-state  languages,  they  still  do  not  allow  every  computable  safety  property.  This  paper  presents  a  monitor  synthesis  algorithm  for  string  rewriting  systems  (SRS).  SRSs  are  well  known  to  be  Turing  complete,  allowing  for  the  formal  specification  of  any  computable  safety  property.  Earlier  attempts  at  Turing  complete  monitoring  have  been  relatively  inefficient.  This  paper  demonstrates  that  monitoring  parametric  SRSs  is  practical.  The  presented  algorithm  uses  a  modified  version  of  Aho-Corasick  string  searching  for  quick  pattern  matching  with  an  incremental  rewriting  approach  that  avoids  reexamining  parts  of  the  string  known  to  contain  no  redexes.
1	Auto  locating  and  fix  propagating  for  html  validation  errors  to  php  server  side  code.  Checking/correcting  HTML  validation  errors  in  Web  pages  is  helpful  for  Web  developers  in  finding/fixing  bugs.  However,  existing  validating/fixing  tools  work  well  only  on  static  HTML  pages  and  do  not  help  fix  the  corresponding  server  code  if  validation  errors  are  found  in  HTML  pages,  due  to  several  challenges  with  dynamically  generated  pages  in  Web  development.  We  propose  PhpSync,  a  novel  automatic  locating/fixing  tool  for  HTML  validation  errors  in  PHP-based  Web  applications.  Given  an  HTML  page  produced  by  a  server-side  PHP  program,  PhpSync  uses  Tidy,  an  HTML  validating/correcting  tool  to  find  the  validation  errors  in  that  HTML  page.  If  errors  are  detected,  it  leverages  the  fixes  from  Tidy  in  the  given  HTML  page  and  propagates  them  to  the  corresponding  location(s)  in  PHP  code.  Our  core  solutions  include  1)  a  symbolic  execution  algorithm  on  the  given  PHP  program  to  produce  a  single  tree-based  model,  called  D-model,  which  approximately  represents  its  possible  client  page  outputs,  2)  an  algorithm  mapping  any  text  in  the  given  HTML  page  to  the  text(s)  in  the  node(s)  of  the  D-model  and  then  to  the  PHP  code,  and  3)  a  fix-propagating  algorithm  from  the  fixes  in  the  HTML  page  to  the  PHP  code  via  the  D-model  and  the  mapping  algorithm.  Our  empirical  evaluation  shows  that  on  average,  PhpSync  achieves  96.7%  accuracy  in  locating  the  corresponding  locations  in  PHP  code  from  client  pages,  and  95%  accuracy  in  propagating  the  fixes  to  the  server-side  code.
1	Delta  debugging  microservice  systems.  Debugging  microservice  systems  involves  the  deployment  and  manipulation  of  microservice  systems  on  a  containerized  environment  and  faces  unique  challenges  due  to  the  high  complexity  and  dynamism  of  microservices.  To  address  these  challenges,  in  this  paper,  we  propose  a  debugging  approach  for  microservice  systems  based  on  the  delta  debugging  algorithm,  which  is  to  minimize  failure-inducing  deltas  of  circumstances  (e.g.,  deployment,  environmental  configurations)  for  effective  debugging.  Our  approach  includes  novel  techniques  for  defining,  deploying/manipulating,  and  executing  deltas  following  the  idea  of  delta  debugging.  In  particular,  to  construct  a  (failing)  circumstance  space  for  delta  debugging  to  minimize,  our  approach  defines  a  set  of  dimensions  that  can  affect  the  execution  of  microservice  systems.  Our  experimental  study  on  a  medium-size  microservice  benchmark  system  shows  that  our  approach  can  effectively  identify  failure-inducing  deltas  that  help  diagnose  the  root  causes.
1	Low  cost  air  quality  sensor  deployment  and  citizen  science  the  penuelas  project.  The  U.S.  Environmental  Protection  Agency  (EPA)  and  Desarrollo  Integral  del  Sur,  Inc  (DISUR),  a  Puerto  Rico-based  community  action  group,  collaborated  to  determine  the  efficacy  of  citizen  science  involving  the  use  of  low  cost  air  quality  sensors.  The  EPA  developed  a  unique  low  cost  AC  powered  multi-pollutant  Citizen  Science  Air  Monitor  (CSAM)  that  was  provided  to  the  community  group  along  with  the  training/tools  needed  for  its  operation.  The  citizens  self-organized  a  community  effort  to  conduct  approximately  five  months  of  intensive  air  quality  monitoring  in  an  area  of  Puerto  Rico  (Tallaboa-Encarnacion,  Penuelas)  having  little  historical  data  on  spatial  variability  (Ponce).  Real-time  measurements  of  the  particulate  matter  size  fraction  2.5  micron  (PM2.5),  nitrogen  dioxide  (NO2),  total  volatile  organic  compounds  (tVOCs),  and  meteorological  parameters  (wind  speed,  wind  directions,  temperature,  relative  humidity)  were  obtained.  The  study  provided  the  Penuelas  and  surrounding  communities  an  in-depth  investigation  of  local  air  quality  and  opportunities  for  citizen  scientists  to  gain  extensive  experience  in  the  use  of  emerging  sensor  technologies.  The  collaboration  also  provided  the  EPA  an  opportunity  to  evaluate  low  cost  sensor  performance  under  harsh  environmental  conditions  (high  relative  humidity  in  a  coastal  environment).  We  present  the  approach  and  preliminary  environmental  findings  of  the  EPA’s  efforts  the  deploy  a  low  cost  multi-pollutant  sensor  pod  associated  with  a  citizen  science  research  study.
1	Supporting  communication  and  cooperation  in  global  software  development  with  agile  service  networks.  Current  IT  markets  exhibit  many  constraints  (e.g.  budget,  staff  shortage,  etc.).  These  constraints  force  IT  companies  to  increase  productivity  using  globally  distributed  manpower.  Literature  shows  that  global  software  development  (GSD)  indeed  raises  productivity  but  reduces  communication  and  collaboration  between  teams.  Consequently,  the  risk  of  failure  increases.  To  ease  communication  and  collaboration  among  teams,  novel  engineering  methods  must  be  provided.  To  address  this  problem,  we  propose  using  Agile  Service  Networks  (ASNs).  ASNs  are  an  emergent  paradigm  in  which  service  oriented  applications  (network  nodes)  collaborate  through  agile  and  dynamic  service  interactions  (network  edges).  Agile  interaction  among  ASN  nodes,  allow  mitigating  distance  (typical  of  GSD)  by  dynamically  adapting  communication  and  collaboration  as  needed.  Through  ASNs,  GSD  can  be  seen  as  a  global  network  of  resources  (teams,  documentation,  knowledge,  etc.)  among  which  agile  interactions  allow  flexible  knowledge  exchange  and  team  collaboration.  To  establish  feasibility  of  our  proposal,  we  investigated  how  ASNs  can  support  GSD.  Based  on  existing  works  in  the  fields  of  both  ASNs  and  GSD,  we  mapped  GSD  challenges  on  ASNs  key  features  and  devised  a  meta-model  showing  how  ASNs  are  used  to  support  GSD  requirements.
1	Multi  tenant  architecture  comparison.  Software  architects  struggle  to  choose  an  adequate  architectural  style  for  multi-tenant  software  systems.  Bad  choices  result  in  poor  performance,  low  scalability,  limited  flexibility,  and  obstruct  software  evolution.  We  present  a  comparison  of  12  Multi-Tenant  Architecture  (MTA)  patterns  that  supports  architects  in  choosing  the  most  suitable  architectural  pattern,  using  17  assessment  criteria.  Both  patterns  and  criteria  were  evaluated  by  domain  experts.  Five  architecture  assessment  rules  of  thumb  are  presented  in  the  paper,  aimed  at  making  fast  and  efficient  design  decisions.  The  comparison  provides  architects  with  an  effective  method  for  selecting  the  applicable  multi-tenant  architecture  pattern,  saving  them  effort,  time,  and  mitigating  the  effects  of  making  wrong  decisions.
1	Performance  assessment  of  a  fast  temperature  sensing  system  based  on  bare  fbgs  and  fast  spectrum  analyzer.  Fiber  Bragg  Gratings  (FBGs)  are  one  of  the  most  widespread  types  of  fiber  sensors  for  measuring  temperature  and  deformations  because  of  their  robustness  and  sensitivity,  combined  with  the  possibility  of  multiplexing  to  form  complex  multipoint  sensing  systems.  Traditional  interrogation  techniques  -  especially  in  the  case  of  multiple  sensors  -  are  based  on  broadband  sources  and  spectrum  analyzers;  this  solution,  although  effective,  presents  however  two  weaknesses:  resolution  (at  least  for  not  too  expensive  devices)  and  time  necessary  for  the  spectral  scanning.  Recently  fast  spectrometers  have  been  introduced  in  the  market,  but  their  resolution  is  still  limited  to  about  100  pm,  which  corresponds  to  about  10  °C  when  used  for  temperature  sensing.The  paper  presents  a  fast  and  high  resolution  temperature  sensing  system  based  on  such  fast  spectrum  analyzers  and  critically  assesses  its  performance.  The  resolution  has  been  improved  by  using  a  spectral  fitting  method  with  Gaussian  functions  and  a  resolution  of  0.18  °C  has  been  achieved.  On  the  other  hand,  the  time  response  depends  not  only  on  the  interrogation  system  but  also  on  the  sensor  itself  so,  in  the  proposed  setup,  bare  FBG  sensors  are  used  instead  of  more  common  FBG  temperature  sensors  for  their  reduced  footprint  that  allows  point  measurements  and  very  small  time  constant,  comparable  with  thermocouples.  The  temperature  behavior  of  bare  FBGs  ,  however,  is  typically  not  provided  with  the  required  accuracy  by  FBG  manufacturer;  moreover  they  exhibit  a  strong  cross-sensitivity  with  strain,  so  a  proper  preliminary  characterization  is  necessary  before  their  practical  application.  This  has  been  carried  out  with  an  environmental  chamber  and  an  acquisition  and  processing  system  specifically  developed  to  manage  fast  spectrum  analyzers.  Given  its  performance,  the  potential  applications  of  the  proposed  sensing  system  are  mainly  in  the  medical  field  where  a  non-metallic  temperature  probe  with  small  dimensions  and  fast  response  is  required,  like  in  measuring  temperature  during  thermal  ablation  of  cells.
1	Software  packaging  approaches  a  comparison  framework.  Effective  software  modularity  brings  many  benefits  such  as  long-term  cost  reduction,  architecture  stability,  design  flexibility,  high  maintainability,  and  high  reusability.  A  module  could  be  a  class,  a  package,  a  component,  or  a  subsystem.  In  this  paper,  we  are  concerned  with  the  package  type  of  modules.  There  has  been  a  number  of  attempts  to  propose  approaches  for  automatic  packaging  of  classes  in  OO  development.  However,  there  is  no  framework  that  could  be  used  to  aid  practitioners  in  selecting  appropriate  approaches  suitable  for  their  particular  development  efforts.  In  this  paper  we  present  an  attribute-based  framework  to  classify  and  compare  these  approaches  and  provide  such  aid  to  practitioners.  The  framework  is  also  meant  to  guide  researchers  interested  in  proposing  new  packaging  approaches.  The  paper  discusses  a  number  of  representative  packaging  approaches  against  the  framework.  Analysis  of  the  discussion  suggests  open  points  for  future  research.
1	Data  driven  and  tool  supported  elicitation  of  quality  requirements  in  agile  companies.  Quality  requirements  (QRs)  are  a  key  artifact  needed  to  ensure  the  quality  and  success  of  a  software  system.  Despite  their  importance,  QRs  rarely  get  the  same  degree  of  attention  as  their  functional  counterpart  in  agile  software  development  (ASD)  projects.  Moreover,  crucial  information  that  can  be  obtained  from  software  development  repositories  (e.g.,  JIRA,  GitHub)  is  not  fully  exploited,  or  is  even  neglected,  in  QR  elicitation  activities.  In  this  work,  we  present  a  data-driven  tooled  approach  for  the  semi-automatic  generation  and  documentation  of  QRs  in  the  context  of  ASD.  The  approach  is  based  on  the  declaration  of  thresholds  over  quality-related  issues,  whose  violation  triggers  user-defined  alerts.  These  alerts  are  used  to  browse  a  catalog  of  QR  patterns  that  are  presented  to  the  ASD  team  by  means  of  a  dashboard  that  implements  several  analysis  techniques.  Once  selected,  the  patterns  generate  the  QRs,  which  are  documented  and  stored  in  the  product  backlog.  The  full  approach  is  implemented  via  a  configurable  platform.  Over  the  course  of  1 year,  four  companies  differing  in  size  and  profile  followed  this  approach  and  deployed  the  platform  in  their  premises  to  semi-automatically  generate  QRs  in  several  projects.  We  used  standardized  measurement  instruments  to  elicit  the  perception  of  22  practitioners  regarding  their  use  of  the  tool.  The  quantitative  and  qualitative  analyses  yielded  positive  results;  i.e.,  the  practitioners’  perception  with  regard  to  the  tool’s  understandability,  reliability,  usefulness,  and  relevance  was  positive.  We  conclude  that  the  results  show  potential  for  future  adoption  of  data-driven  elicitation  of  QRs  in  agile  companies  and  encourage  other  practitioners  to  use  the  presented  tool  and  adopt  it  in  their  companies.
1	Software  aging  and  rejuvenation  in  android  new  models  and  metrics.  Android  users  are  occasionally  troubled  by  the  slow  UI  responses  and  sudden  application/OS  crashes.  These  problems  are  mainly  caused  by  software  aging,  a  phenomenon  of  progressive  degradation  of  performance  and  dependability  typically  observed  in  long-running  software  systems.  A  countermeasure  to  software  aging  is  software  rejuvenation,  i.e.,  manual  or  scheduled  restart  at  different  levels,  such  as  application,  OS,  and  device.  Various  software  aging  and  rejuvenation  models  have  been  proposed  for  different  software  systems.  However,  these  traditional  models  cannot  be  applied  in  the  context  of  mobile  devices,  as  they  seldom  consider  the  patterns  of  usage  behavior  and  user  experience  specific  to  mobile  phones.  We  address  this  problem  based  on  the  observations  that  the  usage  time  of  mobile  phones  is  typically  fragmented  in  daily  life,  with  frequent  and  periodical  switches  between  active  and  sleep  modes,  and  that  the  user  experience  on  fluent  operation  in  the  active  mode  is  a  key  concern  for  mobile  users.  These  insights  are  exploited  to  model  the  usage  behavior  and  aging  process  by  individual  Stochastic  Petri-Nets,  and  then  to  compose  them  into  a  Continuous  Time  Markov  Chain  (CTMC).  Furthermore,  we  propose  proactive  rejuvenation  strategies  based  on  such  CTMCs  to  achieve  the  best  user  experience  and  the  least  user  interference,  such  as  restarting  the  device  when  it  is  in  sleep  mode  and  before  it  enters  an  aged  state.  To  consider  user  experience  -  a  key  concern  of  mobile  users  which  is  still  less  prominent  in  traditional  dependability  measurements  –  we  propose  new  related  metrics:  for  fluency  (i.e.,  the  probability  that  a  phone  offers  a  fast  UI  response  to  the  users),  and  for  the  degree  of  overall  user  experience.  We  demonstrate  the  effectiveness  and  advantages  of  the  proposed  models  and  metrics  via  simulations  as  well  as  an  empirical  study.
1	Test  driven  development  in  scientific  software  a  survey.  Scientific  software  developers  are  increasingly  employing  various  software  engineering  practices.  Specifically,  scientists  are  beginning  to  use  Test-Driven  Development  (TDD).  Even  with  this  increasing  use  of  TDD,  the  effect  of  TDD  on  scientific  software  development  is  not  fully  understood.  To  help  scientific  developers  determine  whether  TDD  is  appropriate  for  their  scientific  projects,  we  surveyed  scientific  developers  who  use  TDD  to  understand:  (1)  TDDs  effectiveness,  (2)  the  benefits  and  challenges  of  using  TDD,  and  (3)  the  use  of  refactoring  practices  (an  important  part  of  the  TDD  process).  Some  key  positive  results  include:  (1)  TDD  helps  scientific  developers  increase  software  quality,  in  particular  functionality  and  reliability;  and  (2)  TDD  helps  scientific  developers  reduce  the  number  of  problems  in  the  early  phase  of  projects.  Conversely,  some  key  challenges  include:  (1)  TDD  may  not  be  effective  for  all  types  of  scientific  projects;  and  (2)  Writing  a  good  test  is  the  most  difficult  task  in  TDD,  particularly  in  a  parallel  computing  environment.  To  summarize,  TDD  generally  has  a  positive  effect  on  the  quality  of  scientific  software,  but  it  often  requires  a  large  effort  investment.  The  results  of  this  survey  indicate  the  need  for  additional  empirical  evaluation  of  the  use  of  TDD  for  the  development  of  scientific  software  to  help  organizations  make  better  decisions.
1	Fsm  inference  and  checking  sequence  construction  are  two  sides  of  the  same  coin.  The  paper  focuses  on  the  problems  of  passive  and  active  FSM  inference  as  well  as  checking  sequence  generation.  We  consider  the  setting  where  an  FSM  cannot  be  reset  so  that  its  inference  is  constrained  to  a  single  trace  either  given  a  priori  in  a  passive  inference  scenario  or  to  be  constructed  in  an  active  inference  scenario  or  aiming  at  obtaining  checking  sequence  for  a  given  FSM.  In  each  of  the  last  two  cases,  the  expected  result  is  a  trace  representing  a  checking  sequence  for  an  inferred  machine,  if  it  was  not  given.  We  demonstrate  that  this  can  be  achieved  by  a  repetitive  use  of  a  procedure  that  infers  an  FSM  from  a  given  trace  (identifying  a  minimal  machine  consistent  with  a  trace)  avoiding  equivalent  conjectures.  We  thus  show  that  FSM  inference  and  checking  sequence  construction  are  two  sides  of  the  same  coin.  Following  an  existing  approach  of  constructing  conjectures  by  SAT  solving,  we  elaborate  first  such  a  procedure  and  then  based  on  it  the  methods  for  obtaining  checking  sequence  for  a  given  FSM  and  inferring  a  machine  from  a  black  box.  The  novelty  of  our  approach  is  that  it  does  not  use  any  state  identification  facilities.  We  demonstrate  that  the  proposed  approach  can  also  be  constrained  to  find  a  solution  in  a  subset  of  FSMs  represented  by  a  nondeterministic  mutation  machine.  Experiments  with  a  prototype  implementation  of  the  developed  approach  using  an  existing  SAT  solver  indicate  that  it  scales  for  FSMs  with  up  to  a  dozen  of  states  and  requires  relatively  short  sequences  to  identify  a  black  box  machine.
1	Embedding  requirements  within  model  driven  architecture.  Model-Driven  Architecture  (MDA)  brings  benefits  to  software  development,  among  them  the  potential  for  connecting  software  models  with  the  business  domain.  This  paper  focuses  on  the  upstream  or  Computation-Independent  Model  (CIM)  phase  of  MDA.  Our  contention  is  that,  whilst  there  are  many  models  and  notations  available  within  the  CIM  phase,  those  that  are  currently  popular  and  supported  by  the  Object  Management  Group  (OMG)  may  not  be  the  most  useful  notations  for  business  analysts  nor  sufficient  to  fully  support  software  requirements  and  specification.  Therefore,  with  specific  emphasis  on  the  value  of  the  Business  Process  Modelling  Notation  (BPMN)  for  business  analysts,  this  paper  provides  an  example  of  a  typical  CIM  approach  before  describing  an  approach  that  incorporates  specific  requirements  techniques.  A  framework  extension  to  MDA  is  then  introduced,  which  embeds  requirements  and  specification  within  the  CIM,  thus  further  enhancing  the  utility  of  MDA  by  providing  a  more  complete  method  for  business  analysis.
1	Computing  repair  trees  for  resolving  inconsistencies  in  design  models.  Resolving  inconsistencies  in  software  models  is  a  complex  task  because  the  number  of  repairs  grows  exponentially.  Existing  approaches  thus  emphasize  on  selected  repairs  only  but  doing  so  diminishes  their  usefulness.  This  paper  copes  with  the  large  number  of  repairs  by  focusing  on  what  caused  an  inconsistency  and  presenting  repairs  as  a  linearly  growing  repair  tree.  The  cause  is  computed  by  examining  the  run-time  evaluation  of  the  inconsistency  to  understand  where  and  why  it  failed.  The  individual  changes  that  make  up  repairs  are  then  modeled  in  a  repair  tree  as  alternatives  and  sequences  reflecting  the  syntactic  structure  of  the  inconsistent  design  rule.  The  approach  is  automated  and  tool  supported.  Its  scalability  was  empirically  evaluated  on  29  UML  models  and  18  OCL  design  rules  where  we  show  that  the  approach  computes  repair  trees  in  milliseconds  on  average.  We  believe  that  the  approach  is  applicable  to  arbitrary  modeling  and  constraint  languages.
1	Contract  based  program  repair  without  the  contracts.  Automated  program  repair  (APR)  is  a  promising  approach  to  automatically  fixing  software  bugs.  Most  APR  techniques  use  tests  to  drive  the  repair  process;  this  makes  them  readily  applicable  to  realistic  code  bases,  but  also  brings  the  risk  of  generating  spurious  repairs  that  overfit  the  available  tests.  Some  techniques  addressed  the  overfitting  problem  by  targeting  code  using  contracts  (such  as  pre-  and  postconditions),  which  provide  additional  information  helpful  to  characterize  the  states  of  correct  and  faulty  computations;  unfortunately,  mainstream  programming  languages  do  not  normally  include  contract  annotations,  which  severely  limits  the  applicability  of  such  contract-based  techniques.          This  paper  presents  JAID,  a  novel  APR  technique  for  Java  programs,  which  is  capable  of  constructing  detailed  state  abstractions---similar  to  those  employed  by  contract-based  techniques---that  are  derived  from  regular  Java  code  without  any  special  annotations.  Grounding  the  repair  generation  and  validation  processes  on  rich  state  abstractions  mitigates  the  overfitting  problem,  and  helps  extend  APR’s  applicability:  in  experiments  with  the  DEFECTS4J  benchmark,  a  prototype  implementation  of  JAID  produced  genuinely  correct  repairs,  equivalent  to  those  written  by  programmers,  for  25  bugs---improving  over  the  state  of  the  art  of  comparable  Java  APR  techniques  in  the  number  and  kinds  of  correct  fixes.
1	Answerbot  automated  generation  of  answer  summary  to  developersź  technical  questions.  The  prevalence  of  questions  and  answers  on  domain-specific  Q&A  sites  like  Stack  Overflow  constitutes  a  core  knowledge  asset  for  software  engineering  domain.  Although  search  engines  can  return  a  list  of  questions  relevant  to  a  user  query  of  some  technical  question,  the  abundance  of  relevant  posts  and  the  sheer  amount  of  information  in  them  makes  it  difficult  for  developers  to  digest  them  and  find  the  most  needed  answers  to  their  questions.  In  this  work,  we  aim  to  help  developers  who  want  to  quickly  capture  the  key  points  of  several  answer  posts  relevant  to  a  technical  question  before  they  read  the  details  of  the  posts.  We  formulate  our  task  as  a  query-focused  multi-answer-posts  summarization  task  for  a  given  technical  question.  Our  proposed  approach  AnswerBot  contains  three  main  steps  :  1)  relevant  question  retrieval,  2)  useful  answer  paragraph  selection,  3)  diverse  answer  summary  generation.  To  evaluate  our  approach,  we  build  a  repository  of  228,817  Java  questions  and  their  corresponding  answers  from  Stack  Overflow.  We  conduct  user  studies  with  100  randomly  selected  Java  questions  (not  in  the  question  repository)  to  evaluate  the  quality  of  the  answer  summaries  generated  by  our  approach  and  the  effectiveness  of  its  relevant  question  retrieval  and  answer  paragraph  selection  components.  Our  evaluation  shows  that  answer  summaries  generated  by  our  approach  are  relevant,  useful  and  diverse  to  developers’  technical  questions,  and  its  components  can  effectively  retrieve  relevant  questions  and  select  salient  answer  paragraphs  for  summarization.
1	An  optimal  strategy  for  algorithmic  debugging.  Algorithmic  debugging  is  a  technique  that  uses  an  internal  data  structure  to  represent  computations  and  ask  about  their  correctness.  The  strategy  used  to  explore  this  data  structure  is  essential  for  the  performance  of  the  technique.  The  most  efficient  strategy  in  practice  is  Divide  and  Query  that,  until  now,  has  been  considered  optimal  in  the  worst  case.  In  this  paper  we  first  show  that  the  original  algorithm  is  inaccurate  and  moreover,  in  some  situations  it  is  unable  to  find  all  possible  solutions,  thus  it  is  incomplete.  Then,  we  present  a  new  version  of  the  algorithm  that  solves  these  problems.  Moreover,  we  introduce  a  counterexample  showing  that  Divide  and  Query  is  not  optimal,  and  we  propose  the  first  optimal  strategy  for  algorithmic  debugging  with  respect  to  the  number  of  questions  asked  by  the  debugger.
1	Active  code  search  incorporating  user  feedback  to  improve  code  search  relevance.  Code  search  techniques  return  relevant  code  fragments  given  a  user  query.  They  typically  work  in  a  passive  mode:  given  a  user  query,  a  static  list  of  code  fragments  sorted  by  the  relevance  scores  decided  by  a  code  search  technique  is  returned  to  the  user.  A  user  will  go  through  the  sorted  list  of  returned  code  fragments  from  top  to  bottom.  As  the  user  checks  each  code  fragment  one  by  one,  he  or  she  will  naturally  form  an  opinion  about  the  true  relevance  of  the  code  fragment.  In  an  active  model,  those  opinions  will  be  taken  as  feedbacks  to  the  search  engine  for  refining  result  lists.      In  this  work,  we  incorporate  users'  opinion  on  the  results  from  a  code  search  engine  to  refine  result  lists:  as  a  user  forms  an  opinion  about  one  result,  our  technique  takes  this  opinion  as  feedback  and  leverages  it  to  re-order  the  results  to  make  truly  relevant  results  appear  earlier  in  the  list.  The  refinement  results  can  also  be  cached  to  potentially  improve  future  code  search  tasks.  We  have  built  our  active  refinement  technique  on  top  of  a  state-of-the-art  code  search  engine---Portfolio.  Our  technique  improves  Portfolio  in  terms  of  Normalized  Discounted  Cumulative  Gain  (NDCG)  by  more  than  11.3%,  from  0.738  to  0.821.
1	Combining  search  based  and  constraint  based  testing.  Many  modern  automated  test  generators  are  based  on  either  meta-heuristic  search  techniques  or  use  constraint  solvers.  Both  approaches  have  their  advantages,  but  they  also  have  specific  drawbacks:  Search-based  methods  get  stuck  in  local  optima  and  degrade  when  the  search  landscape  offers  no  guidance;  constraint-based  approaches,  on  the  other  hand,  can  only  handle  certain  domains  efficiently.  In  this  paper  we  describe  a  method  that  integrates  both  techniques  and  delivers  the  best  of  both  worlds.  On  a  high-level  view,  our  method  uses  a  genetic  algorithm  to  generate  tests,  but  the  twist  is  that  during  evolution  a  constraint  solver  is  used  to  ensure  that  mutated  offspring  efficiently  explores  different  control  flow.  Experiments  on  20  case  study  examples  show  that  on  average  the  combination  improves  branch  coverage  by  28%  over  search-based  techniques  and  by  13%  over  constraint-based  techniques.
1	Tracking  load  time  configuration  options.  Highly-configurable  software  systems  are  pervasive,  although  configuration  options  and  their  interactions  raise  complexity  of  the  program  and  increase  maintenance  effort.  Especially  load-time  configuration  options,  such  as  parameters  from  command-line  options  or  configuration  files,  are  used  with  standard  programming  constructs  such  as  variables  and  if  statements  intermixed  with  the  program's  implementation;  manually  tracking  configuration  options  from  the  time  they  are  loaded  to  the  point  where  they  may  influence  control-flow  decisions  is  tedious  and  error  prone.  We  design  and  implement  Lotrack,  an  extended  static  taint  analysis  to  automatically  track  configuration  options.  Lotrack  derives  a  configuration  map  that  explains  for  each  code  fragment  under  which  configurations  it  may  be  executed.  An  evaluation  on  Android  applications  shows  that  Lotrack  yields  high  accuracy  with  reasonable  performance.  We  use  Lotrack  to  empirically  characterize  how  much  of  the  implementation  of  Android  apps  depends  on  the  platform's  configuration  options  or  interactions  of  these  options.
1	42  variability  bugs  in  the  linux  kernel  a  qualitative  analysis.  Feature-sensitive  verification  pursues  effective  analysis  of  the  exponentially  many  variants  of  a  program  family.  However,  researchers  lack  examples  of  concrete  bugs  induced  by  variability,  occurring  in  real  large-scale  systems.  Such  a  collection  of  bugs  is  a  requirement  for  goal-oriented  research,  serving  to  evaluate  tool  implementations  of  feature-sensitive  analyses  by  testing  them  on  real  bugs.  We  present  a  qualitative  study  of  42  variability  bugs  collected  from  bug-fixing  commits  to  the  Linux  kernel  repository.  We  analyze  each  of  the  bugs,  and  record  the  results  in  a  database.  In  addition,  we  provide  self-contained  simplified  C99  versions  of  the  bugs,  facilitating  understanding  and  tool  evaluation.  Our  study  provides  insights  into  the  nature  and  occurrence  of  variability  bugs  in  a  large  C  software  system,  and  shows  in  what  ways  variability  affects  and  increases  the  complexity  of  software  bugs.
1	Ocor  an  overlapping  aware  code  retriever.  Code  retrieval  helps  developers  reuse  code  snippets  in  the  open-source  projects.  Given  a  natural  language  description,  code  retrieval  aims  to  search  for  the  most  relevant  code  relevant  among  a  set  of  code  snippets.  Existing  state-of-the-art  approaches  apply  neural  networks  to  code  retrieval.  However,  these  approaches  still  fail  to  capture  an  important  feature:  overlaps.  The  overlaps  between  different  names  used  by  different  people  indicate  that  two  different  names  may  be  potentially  related  (e.g.,  "message"  and  "msg"),  and  the  overlaps  between  identifiers  in  code  and  words  in  natural  language  descriptions  indicate  that  the  code  snippet  and  the  description  may  potentially  be  related.  To  address  this  problem,  we  propose  a  novel  neural  architecture  named  OCoR1,  where  we  introduce  two  specifically-designed  components  to  capture  overlaps:  the  first  embeds  names  by  characters  to  capture  the  overlaps  between  names,  and  the  second  introduces  a  novel  overlap  matrix  to  represent  the  degrees  of  overlaps  between  each  natural  language  word  and  each  identifier.  The  evaluation  was  conducted  on  two  established  datasets.  The  experimental  results  show  that  OCoR  significantly  outperforms  the  existing  state-of-the-art  approaches  and  achieves  13.1%  to  22.3%  improvements.  Moreover,  we  also  conducted  several  in-depth  experiments  to  help  understand  the  performance  of  the  different  components  in  OCoR.
1	Marble  model  based  robustness  analysis  of  stateful  deep  learning  systems.  State-of-the-art  deep  learning  (DL)  systems  are  vulnerable  to  adversarial  examples,  which  hinders  their  potential  adoption  in  safety-and  security-critical  scenarios.  While  some  recent  progress  has  been  made  in  analyzing  the  robustness  of  feed-forward  neural  networks,  the  robustness  analysis  for  stateful  DL  systems,  such  as  recurrent  neural  networks  (RNNs),  still  remains  largely  uncharted.  In  this  paper,  we  propose  Marble,  a  model-based  approach  for  quantitative  robustness  analysis  of  real-world  RNN-based  DL  systems.  Marble  builds  a  probabilistic  model  to  compactly  characterize  the  robustness  of  RNNs  through  abstraction.  Furthermore,  we  propose  an  iterative  refinement  algorithm  to  derive  a  precise  abstraction,  which  enables  accurate  quantification  of  the  robustness  measurement.  We  evaluate  the  effectiveness  of  Marble  on  both  LSTM  and  GRU  models  trained  separately  with  three  popular  natural  language  datasets.  The  results  demonstrate  that  (1)  our  refinement  algorithm  is  more  efficient  in  deriving  an  accurate  abstraction  than  the  random  strategy,  and  (2)  Marble  enables  quantitative  robustness  analysis,  in  rendering  better  efficiency,  accuracy,  and  scalability  than  the  state-of-the-art  techniques.
1	Safety  perspective  for  supporting  architectural  design  of  safety  critical  systems.  Various  software  architecture  viewpoint  approaches  have  been  introduced  to  model  the  architecture  views  for  stakeholder  concerns.  To  address  quality  concerns  in  software  architecture  views,  an  important  approach  is  to  define  architectural  perspectives  that  include  a  collection  of  activities,  tactics  and  guidelines  that  require  consideration  across  a  number  of  the  architectural  views.  Several  architectural  perspectives  have  been  defined  for  selected  quality  concerns.  In  this  paper  we  propose  the  Safety  Perspective  that  is  dedicated  to  ensure  that  the  safety  concern  is  properly  addressed  in  the  architecture  views.  The  proposed  safety  perspective  can  assist  the  system  and  software  architects  in  designing,  analyzing  and  communicating  the  decisions  regarding  safety  concerns.  We  illustrate  the  safety  perspective  for  a  real  industrial  case  study  and  discuss  the  lessons  learned.
1	Run  time  support  to  manage  architectural  variability  specified  with  cvl.  The  execution  context  in  which  pervasive  systems  or  mobile  computing  run  changes  continuously.  Hence,  applications  for  these  systems  should  be  adapted  at  run-time  according  to  the  current  context.  In  order  to  implement  a  context-aware  dynamic  reconfiguration  service,  most  approaches  usually  require  to  model  at  design-time  both  the  list  of  all  possible  configurations  and  the  plans  to  switch  among  them.  In  this  paper  we  present  an  alternative  approach  for  the  automatic  run-time  generation  of  application  configurations  and  the  reconfiguration  plans.  The  generated  configurations  are  optimal  regarding  different  criteria,  such  as  functionality  or  resource  consumption  (e.g.  battery  or  memory).  This  is  achieved  by:  (1)  modelling  architectural  variability  at  design-time  using  Common  Variability  Language  (CVL),  and  (2)  using  a  genetic  algorithm  that  finds  at  run-time  nearly-optimal  configurations  using  the  information  provided  by  the  variability  model.  We  also  specify  a  case  study  and  we  use  it  to  evaluate  our  approach,  showing  that  it  is  efficient  and  suitable  for  devices  with  scarce  resources.
1	Blueprints  for  architecture  drivers  and  architecture  solutions  for  industry  4  0  shopfloor  applications.  Industry  4.0  aims  at  evolving  the  current  industrial  processes  towards  directly  connecting  shopfloor  machines  to  systems  from  different  layers  of  the  automation  pyramid,  such  as  Enterprise  Resource  Planning  (ERP)  or  Manufacturing  Execution  Systems  (MES).  There  are  key  functional  and  quality  requirements  that  apply  to  most  Industry  4.0  systems  independent  of  the  application  domain,  e.g.,  requirements  related  to  interoperability,  recoverability,  security,  and  modifiability.  Despite  their  importance,  there  is  still  a  lack  of  understanding  of  (i)  architecture  drivers  that  focus  on  these  quality  aspects  and  (ii)  architecture  solutions  for  these  architecture  drivers  that  are  adequate  for  a  wide  range  of  Industry  4.0  contexts.  To  contribute  to  filling  this  gap,  we  present  in  this  paper  (i)  quality-centered  architecture  drivers  derived  from  industrial  cases,  and  (ii)  architecture  solutions  based  on  the  concepts  of  Digital  Twins,  Service-Oriented  Architecture,  and  Virtual  Automation  Bus  for  four  recurrent  production  plant  scenarios.  The  architecture  drivers  and  solutions  presented  in  this  paper  were  instantiated  in  different  Industry  4.0  contexts,  such  as  BaSys  4.0  (the  German  national  reference  project  for  Industry  4.0),  and  by  the  BaSys  industry  project  partners.
1	Understanding  architecture  decisions  in  context.  Many  organizations  struggle  with  efficient  architecture  decision-making  approaches.  Often,  the  decision-making  approaches  are  not  articulated  or  understood.  This  problem  is  particularly  evident  in  large,  globally  distributed  organizations  with  multiple  large  products  and  systems.  The  significant  architecture  decisions  of  a  system  are  a  critical  organization  knowledge  asset,  as  well  as  a  determinant  of  success.  However,  the  environment  in  which  decisions  get  made,  recorded,  and  followed-up  on  often  confounds  rather  than  helps  articulation  and  execution  of  architecture  decisions.  This  paper  looks  at  aspects  of  architecture  decision-making,  drawing  from  an  industry-based  case  study.  The  data  represents  findings  from  a  qualitative  case  study  involving  a  survey  and  three  focus  groups  across  multiple  organizations  in  a  global  technology  company.  Architects  in  this  organization  are  responsible  for  multiple  products  and  systems,  where  individual  products  can  include  up  to  50+  teams.  The  impact  is  not  just  on  others  in  the  system;  architecture  decisions  also  impact  other  decisions  and  other  architects.  The  findings  suggest  recommendations  for  organizations  to  improve  how  they  make  and  manage  architecture  decisions.  In  particular,  this  paper  notes  the  relevance  of  group  decision-making,  decision  scope,  and  social  factors  such  as  trust  in  effective  architecture  decision-making.
1	Towards  an  optimized  software  architecture  for  component  adaptation  at  middleware  level.  The  amount  of  software  in  the  automotive  domain  is  steadily  increasing.  Existing  functions  are  adapted  or  enhanced  on  a  regular  basis.  Often,  such  adaptations  do  not  allow  to  keep  the  interfaces  of  the  concerned  components  stable,  leading  to  incompatibilities  with  former  systems.  In  this  contribution,  we  propose  an  optimized  adaptation  software  architecture  to  deal  with  mismatching  interfaces.  We  extend  existing  middleware  solutions  with  transparent  adapter  loading  capabilities.  This  enables  for  seamless  adapter  integration  on  those  systems.  As  adapter  model  we  use  a  finite-state  machine  aside  with  a  domain  specific  language.  By  extracting  static  adaptations  from  the  state  machine  we  achieve  state  reduction  and  performance  gain.  The  approach  is  evaluated  using  an  automotive  case-study.
1	An  interface  diversified  honeypot  for  malware  analysis.  Defending  information  systems  against  advanced  attacks  is  a  challenging  task;  even  if  all  the  systems  have  been  properly  updated  and  all  the  known  vulnerabilities  have  been  patched,  there  is  still  the  possibility  of  previously  unknown  zero  day  attack  compromising  the  system.  Honeypots  offer  a  more  proactive  tool  for  detecting  possible  attacks.  What  is  more,  they  can  act  as  a  tool  for  understanding  attackers  intentions.  In  this  paper,  we  propose  a  design  for  a  diversified  honeypot.  By  increasing  variability  present  in  software,  diversification  decreases  the  number  of  assumptions  an  attacker  can  make  about  the  target  system.
1	Software  architecture  documentation  for  developers  a  survey.  Software  architecture  has  become  an  established  discipline  in  industry.  Nevertheless,  the  available  documentation  of  architecture  is  often  not  perceived  as  adequate  by  developers.  As  a  foundation  for  the  improvement  of  methods  and  tools  around  architecture  documentation,  we  conducted  a  survey  with  147  industrial  participants,  investigating  their  current  problems  and  wishes  for  the  future.  Participants  from  different  countries  in  Europe,  Asia,  North  and  South  America  shared  their  experiences.  This  paper  presents  the  results  of  the  survey.  The  results  confirmed  the  common  belief  that  architecture  documentation  is  most  frequently  outdated  and  inconsistent  and  backed  it  up  with  data.  Further,  developers  perceive  difficulties  with  a  "one-size-fits-all"  architecture  documentation,  which  does  not  adequately  provide  information  for  their  specific  task  and  context.  Developers  seek  for  more  interactive  ways  of  working  with  architecture  documentation  that  allow  finding  needed  information  more  easily  with  extended  navigation  and  search  possibilities.
1	A  flexible  architecture  for  key  performance  indicators  assessment  in  smart  cities.  The  concept  of  smart  and  sustainable  city  has  been  on  the  agenda  for  the  last  decade.  Smart  governance  is  about  the  use  of  innovation  for  supporting  enhanced  decision  making  and  planning  to  make  a  city  smart,  by  leveraging  on  Key  Performance  Indicators  (KPIs)  as  procedural  tools.  However,  developing  processes  and  instruments  able  to  evaluate  smart  cities  is  still  a  challenging  task,  due  to  the  rigidity  showed  by  the  existing  frameworks  in  the  definition  of  KPIs  and  modeling  of  the  subjects  to  be  evaluated.  Web-based  platforms,  spreadsheets  or  even  Cloud-based  applications  offer  limited  flexibility,  if  the  stakeholder  is  interested  not  only  in  using  but  also  in  defining  the  pieces  of  the  puzzle  to  be  composed.  In  this  paper  we  present  a  flexible  architecture  supporting  a  model-driven  approach  for  the  KPIs  assessment  in  smart  cities.  It  identifies  both  required  and  optional  components  and  functionalities  needed  for  realizing  the  automatic  KPIs  assessment,  while  showing  flexibility  points  allowing  for  different  specification  of  the  architecture,  thus  of  the  overall  methodology.
1	Assisting  women  in  career  change  towards  software  engineering  experience  from  czechitas  ngo.  Technological  growth  affects  the  way  we  live,  communicate,  and  work.  Over  the  coming  decades,  up  to  60%  of  existing  jobs  might  be  lost  to  automation.  This  creates  pressure  on  the  job  market,  demanding  more  tech  professionals,  while  creating  job  insecurity  for  positions  that  might  be  automated.  Although  tech  and  computing  is  an  appealing  career  choice  with  high  salary,  job  security,  working  flexibility,  space  for  creativity  and  career  growth,  vast  majority  of  women  are  dropping  their  interest  in  tech  during  adolescence,  with  very  limited  recovery  in  later  years,  when  preventable  reasons  are  stopping  them  from  reconsidering  their  decision  and  joining  tech.  In  this  paper,  we  share  our  experience  with  a  project  assisting  women  in  their  20s  and  30s  in  changing  career  towards  tech.  The  project  has  been  implemented  within  our  education  non-profit  organization,  called  Czechitas,  which  is  recognized  as  the  leading  platform  for  addressing  gender  diversity  in  tech  and  software  engineering  in  the  Czech  Republic.
1	A  service  oriented  reference  architecture  for  software  testing  tools.  Software  testing  is  recognized  as  a  fundamental  activity  for  assuring  software  quality.  Aiming  at  supporting  this  activity,  a  diversity  of  testing  tools  has  been  developed,  including  tools  based  on  SOA  (Service-Oriented  Architecture).  In  another  perspective,  reference  architectures  have  played  a  significant  role  in  aggregating  knowledge  of  a  given  domain,  contributing  to  the  success  in  the  development  of  systems  for  that  domain.  However,  there  exists  no  reference  architecture  for  the  testing  domain  that  contribute  to  the  development  of  testing  tools  based  on  SOA.  Thus,  the  main  contribution  of  this  paper  is  to  present  a  service-oriented  reference  architecture,  named  RefTEST-SOA  (Reference  Architecture  for  Software  Testing  Tools  based  on  SOA),  that  comprises  knowledge  and  experience  about  how  to  structure  testing  tools  organized  as  services  and  pursues  a  better  integration,  scalability,  and  reuse  provided  by  SOA  to  such  tools.  Results  of  our  case  studies  have  showed  that  RefTEST-SOA  is  a  viable  and  reusable  element  to  the  development  of  service-oriented  testing  tools.
1	An  evolutionary  game  for  integrity  attacks  and  defences  for  advanced  metering  infrastructure.  Smart  grids  are  complex  cyber-physical  systems  that  face  many  security  challenges.  Advanced  Metering  Infrastructure  (AMI),  which  is  one  of  the  main  components  of  the  smart  grid,  represents  an  important  branch  of  services  with  increasing  deployments  that  also  introduce  new  security  risks.  The  nodes  of  AMIs  are  featured  as  resource-constrained.  Therefore,  security  attacks  including  data  integrity  attacks  on  AMIs  are  of  serious  concern  and  require  efficient  selection  of  protective  strategies.  In  this  paper,  we  propose  an  evolutionary  game  framework  that  models  integrity  attacks  and  defenses  in  an  AMI.  The  aim  of  this  framework  is  to  study  possible  behaviors  of  adversaries  and  to  define  how  the  AMI  nodes  can  adaptively  select  their  strategies  with  maximum  payoffs  of  the  nodes.  We  present  a  case  study  and  illustrate  how  the  framework  can  be  applied  to  investigate  the  integrity  threats  in  AMI  systems.  We  show  the  evolution  process,  based  on  the  replicator  dynamic.
1	Using  constraints  to  diagnose  faulty  spreadsheets.  Spreadsheets  can  be  viewed  as  a  highly  flexible  programming  environment  for  end  users.  Spreadsheets  are  widely  adopted  for  decision  making  and  may  have  a  serious  economical  impact  for  the  business.  However,  spreadsheets  are  staggeringly  prone  to  errors.  Hence,  approaches  for  aiding  the  process  of  pinpointing  the  faulty  cells  in  a  spreadsheet  are  of  great  value.  We  present  a  constraint-based  approach,  ConBug,  for  debugging  spreadsheets.  The  approach  takes  as  input  a  (faulty)  spreadsheet  and  a  test  case  that  reveals  the  fault  and  computes  a  set  of  diagnosis  candidates  for  the  debugging  problem.  Therefore,  we  convert  the  spreadsheet  and  a  test  case  to  a  constraint  satisfaction  problem.  We  perform  an  empirical  evaluation  with  78  spreadsheets  from  different  sources,  where  we  demonstrate  that  our  approach  is  light-weight  and  efficient.  From  our  experimental  results,  we  conclude  that  ConBug  helps  end  users  to  pinpoint  faulty  cells.
1	Fully  automated  html  and  javascript  rewriting  for  constructing  a  self  healing  web  proxy.  Over  the  last  few  years,  the  complexity  of  web  applications  has  increased  to  provide  more  dynamic  web  applications  to  users.  The  drawback  of  this  complexity  is  the  growing  number  of  errors  in  the  front-end  applications.  In  this  paper,  we  present  BikiniProxy,  a  novel  technique  to  provide  self-healing  for  the  web.  BikiniProxy  is  designed  as  an  HTTP  proxy  that  uses  five  self-healing  strategies  to  rewrite  the  buggy  HTML  and  Javascript  code.  We  evaluate  BikiniProxy  with  a  new  benchmark  of  555  reproducible  Javascript  errors,  DeadClick.  We  create  DeadClick  by  randomly  crawling  the  Internet  and  collect  all  web  pages  that  contain  Javascript  errors.  Then,  we  observe  how  BikiniProxy  heals  those  errors  by  collecting  and  comparing  the  traces  of  the  original  and  healed  pages.  To  sum  up,  BikiniProxy  is  a  novel  fully-automated  self-healing  approach  that  is  specific  to  the  web,  evaluated  on  555  real  Javascript  errors,  and  based  on  original  self-healing  rewriting  strategies  for  HTML  and  Javascript.
1	Prediction  of  the  complexity  of  code  changes  based  on  number  of  open  bugs  new  feature  and  feature  improvement.  During  the  last  decade,  a  paradigm  shift  has  been  taken  place  in  the  software  development  process.  Advancement  in  the  internet  technology  has  eased  the  software  development  under  distributed  environment  irrespective  of  geographical  locations.  Result  of  this,  Open  Source  Software  systems  which  serve  as  key  components  of  critical  infrastructures  in  the  society  are  still  ever-expanding  now.  Open  source  software  is  evolved  through  an  active  participation  of  the  users  in  terms  of  reporting  of  bugs,  request  for  new  features  and  feature  improvements.  These  active  users  distributed  across  different  geographical  locations  and  are  working  towards  the  evolution  of  open  source  software.  The  code-changes  due  to  bug  fixes,  new  features  and  feature  improvements  for  a  given  time  period  are  used  to  predict  the  possible  code  changes  in  the  software  over  a  long  run  (potential  complexity  of  code  changes).  It  is  evident  that  the  open  source  software  are  evolved  through  these  modification  but  an  empirical  understanding  among  the  bug  fix,  new  features,  feature  improvements  and  modifications  in  the  files  are  unexplored  till  now.  In  this  paper,  we  have  predicted  the  potential  of  bugs  that  can  be  detected/fixed  and  new  features,  improvements  that  can  be  diffused  in  the  software  over  a  period  of  time.  We  have  quantified  the  complexity  of  code  changes  (entropy)  and  after  that  predicted  the  complexity  of  code  changes  by  applying  Cobb-Douglas  and  extended  Cobb-Douglas  (two  dimensions  and  three  dimensions)  based  diffusion  models.  The  developed  models  can  be  used  to  determine  the  quantitative  value  of  complexity  of  code  changes  for  reported  bugs,  new  features  and  feature  improvements  in  addition  to  their  potential  values.  This  empirical  study  mathematically  models  the  interaction  of  a  system  (the  debugging  and  code  change  system)  with  the  external  open  world  which  will  assist  support  managers  in  software  maintenance  activities  and  software  evolution.
1	Srats  software  reliability  assessment  tool  on  spreadsheet  experience  report.  This  paper  presents  a  software  reliability  assessment  tool  which  is  built  as  a  Microsoft  Excel  AddIn.  The  tool  handles  11  types  of  software  reliability  growth  models  (SRGMs)  and  easily  provides  the  maximum  likelihood  estimates  (MLEs)  from  input  data  on  spreadsheets.  The  tool  consists  of  two  components:  the  user  interface  written  by  Visual  Basic  for  Application  (VBA)  and  the  dynamic  link  library  (DLL)  written  by  C  language.  In  the  DLL,  the  state-of-the-art  parameter  estimation  algorithms  for  SRGMs  are  implemented,  which  are  developed  from  the  EM  (expectation-maximization)  algorithm.  These  algorithms  enable  us  to  derive  the  MLEs  for  any  patterns  of  data.
1	An  empirical  study  of  the  effectiveness  of  forcing  diversity  based  on  a  large  population  of  diverse  programs.  Use  of  diverse  software  components  is  a  viable  defence  against  common-mode  failures  in  redundant  software-based  systems.  Various  forms  of  """"Diversity-Seeking  Decisions""""  ("DSDs")  can  be  applied  to  the  process  of  developing,  or  procuring,  redundant  components,  to  improve  the  chances  of  the  resulting  components  not  failing  on  the  same  demands.  An  open  question  is  how  effective  these  decisions,  and  their  combinations,  are  for  achieving  large  enough  reliability  gains.  Using  a  large  population  of  software  programs,  we  studied  experimentally  the  effectiveness  of  specific  """"DSDs""""  (and  their  combinations)  mandating  differences  between  redundant  components.  Some  of  these  combinations  produced  much  better  improvements  in  system  probability  of  failure  per  demand  (PFD)  than  """"uncontrolled""""  diversity  did.  Yet,  our  findings  suggest  that  the  gains  from  such  """"DSDs""""  vary  significantly  between  them  and  between  the  application  problems  studied.  The  relationship  between  DSDs  and  system  PFD  is  complex  and  does  not  allow  for  simple  universal  rules  (e.g.  """"the  more  diversity  the  better"""")  to  apply.
1	Revolution  automatic  evolution  of  mined  specifications.  Specifications  mined  from  execution  traces  are  largely  used  to  support  testing  and  analysis  of  software  applications  with  little  runtime  variability.  However,  when  models  are  mined  from  applications  that  evolve  at  runtime,  the  resulting  models  become  quickly  obsolete,  and  thus  of  little  support  for  any  testing  and  analysis  activity.  To  cope  with  such  systems,  mined  specifications  must  be  consistently  updated  every  time  the  software  changes.  In  principle,  models  can  be  periodically  mined  from  scratch,  but  in  many  cases  this  solution  is  too  expensive  or  even  impossible.  In  this  paper  we  describe  Revolution,  an  approach  for  the  automatic  evolution  of  specifications  mined  by  applying  state  abstraction  techniques.  Revolution  produces  models  that  are  continuously  updated  and  thus  remain  aligned  with  the  actual  implementation.  Empirical  results  show  that  Revolution  can  suitably  address  run-time  evolving  applications.
1	The  impact  of  feature  selection  on  defect  prediction  performance  an  empirical  comparison.  Software  defect  prediction  aims  to  determine  whether  a  software  module  is  defect-prone  by  constructing  prediction  models.  The  performance  of  such  models  is  susceptible  to  the  high  dimensionality  of  the  datasets  that  may  include  irrelevant  and  redundant  features.  Feature  selection  is  applied  to  alleviate  this  issue.  Because  many  feature  selection  methods  have  been  proposed,  there  is  an  imperative  need  to  analyze  and  compare  these  methods.  Prior  empirical  studies  may  have  potential  controversies  and  limitations,  such  as  the  contradictory  results,  usage  of  private  datasets  and  inappropriate  statistical  test  techniques.  This  observation  leads  us  to  conduct  a  careful  empirical  study  to  reinforce  the  confidence  of  the  experimental  conclusions  by  considering  several  potential  source  of  bias,  such  as  the  noise  in  the  dataset  and  the  dataset  types.  In  this  paper,  we  investigate  the  impact  of  32  feature  selection  methods  on  the  defect  prediction  performance  over  two  versions  of  the  NASA  dataset  (i.e.,  the  noisy  and  clean  NASA  datasets)  and  one  open  source  AEEEM  dataset.  We  use  a  state-of-the-art  double  Scott-Knott  test  technique  to  analyze  these  methods.  Experimental  results  show  that  the  effectiveness  of  these  feature  selection  methods  on  defect  prediction  performance  varies  significantly  over  all  the  datasets.
1	Brace  cloud  based  software  reliability  assurance.  The  evolution  towards  virtualized  network  functions  (VNFs)  is  expected  to  enable  service  agility  within  the  telecommunications  industry.  To  this  end,  the  software  (or  VNFs)  from  which  such  services  are  composed  must  be  developed  and  delivered  over  very  short  time  scales.  In  order  to  guarantee  the  required  levels  of  software  quality  within  such  tight  schedules,  software  reliability  tools  must  evolve.  In  particular,  the  tools  should  provide  development  teams  spread  across  geography  and  time  with  reliable  and  actionable  insights  regarding  the  development  process.  In  this  paper,  we  present  BRACE  -  a  cloud-based,  integrated,  one-stop  center  for  software  reliability  tools.  BRACE  is  home  to  tools  for  software  reliability  modeling,  testing,  and  defect  analysis  each  of  which  is  provided  as-a-service  to  development  teams.  Initial  implementation  of  BRACE  includes  a  software  reliability  growth  modelling  (SRGM)  tool.  The  SRGM  tool  is  currently  being  used  to  enable  real-time  prediction  of  the  total  number  of  defects  in  software  being  developed,  and  for  providing  the  required  analytics  and  metrics  to  enable  managers  make  informed  decisions  regarding  resource  allocation  for  defect  correction  so  as  to  meet  set  deadlines
1	Detection  of  interferences  in  aspect  oriented  programs  using  executable  assertions.  Aspect-oriented  programming  (AOP)  is  a  technique  that  promotes  separation  of  concerns.  Unfortunately,  it  still  suffers  from  well-known  composition  issues,  in  particular  from  undesirable  interferences  when  multiple  concerns  are  applied  at  the  same  join  point.  In  this  paper  we  propose  an  approach  to  detect  interferences  side  effect  using  executable  assertions.  The  assertions  are  inserted  in  the  aspect  chain  to  detect  various  types  of  interferences.  The  implementation  is  based  on  the  AIRIA  resolver  construct,  recently  introduced  to  better  control  conflicting  aspects  in  AspectJ.  Resolvers  add  observation  points  that  were  lacking  in  AspectJ.  We  propose  to  take  advantage  of  this  to  implement  automated  detection  of  interferences  at  execution  time.  We  study  the  feasibility  of  this  approach  and  demonstrate  it  on  artificial  examples.
1	Verifying  the  consistency  of  uml  models.  The  Unified  Modelling  Language  (UML)  is  the  most  used  Object  Management  Group  specification  and  is  the  de-facto  standard  modeling  language  for  object-oriented  design  and  documentation.  When  UML  diagrams  convey  contradicting  or  conflicting  semantics,  the  diagrams  are  said  to  be  inconsistent.  Such  inconsistencies  may  be  a  source  of  faults  in  software  systems.  It  is  therefore  paramount  that  they  get  detected  (through  a  sort  of  verification  activity),  analyzed  and  fixed.  This  paper  presents  the  steps  to  verify,  with  OCL  constraints,  the  consistency  of  UML  models.  To  carry  out  the  verification  of  UML  consistency  models,  the  following  steps  were  identified:  1)  transformation  of  UML  consistency  rules  into  OCL  constraints,  2)  generation  of  a  plugin  in  Papyrus  that  include  the  OCL  constraints,  3)  importation  into  Papyrus  (by  using  EMF)  of  UML  models  found  in  ReMoDD  and  UML  Repository,  4)  execution  of  the  plugin  with  the  OCL  constraints  against  the  imported  UML  models.
1	Root  cause  prediction  based  on  bug  reports.  This  paper  proposes  a  supervised  machine  learning  approach  for  predicting  the  root  cause  of  a  given  bug  report.  Knowing  the  root  cause  of  a  bug  can  help  developers  in  the  debugging  process—either  directly  or  indirectly  by  choosing  proper  tool  support  for  the  debugging  task.  We  mined  54755  closed  bug  reports  from  the  issue  trackers  of  103  GitHub  projects  and  applied  a  set  of  heuristics  to  create  a  benchmark  consisting  of  10459  reports.  A  subset  was  manually  classified  into  three  groups  (semantic,  memory,  and  concurrency)  based  on  the  bugs’  root  causes.  Since  the  types  of  root  cause  are  not  equally  distributed,  a  combination  of  keyword  search  and  random  selection  was  applied.  Our  data  set  for  the  machine  learning  approach  consists  of  369  bug  reports  (122  concurrency,  121  memory,  and  126  semantic  bugs).  The  bug  reports  are  used  as  input  to  a  natural  language  processing  algorithm.  We  evaluated  the  performance  of  several  classifiers  for  predicting  the  root  causes  for  the  given  bug  reports.  Linear  Support  Vector  machines  achieved  the  highest  mean  precision  (0.74)  and  recall  (0.72)  scores.  The  created  bug  data  set  and  classification  are  publicly  available.
1	Proving  model  equivalence  in  model  based  design.  We  introduce  the  concept  of  constrained  equivalence  of  models  in  model-based  development  and  present  a  proof  technology  for  establishing  constrained  equivalence  for  models  documented  in  Math  Works  Simulink.  We  illustrate  the  approach  using  a  simple  model  of  an  automobile  anti-lock  braking  system.
1	Optimal  resource  allocation  in  a  virtualized  software  aging  platform  with  software  rejuvenation.  Nowadays,  virtualized  platforms  have  become  the  most  popular  option  to  deploy  complex  enough  services.  The  reason  is  that  virtualization  allows  resource  providers  to  increase  resource  utilization.  Deployed  services  are  expected  to  be  always  available,  but  these  long-running  services  are  especially  sensitive  to  suffer  from  software  aging  phenomenon.  This  term  refers  to  an  accumulation  of  errors,  which  usually  causes  resource  exhaustion,  and  eventually  makes  the  service  hang/crash.  To  counteract  this  phenomenon,  a  preventive  approach  to  fault  management,  called  software  rejuvenation  has  been  proposed.  In  this  paper,  we  propose  a  framework  which  provides  transparent  and  predictive  software  rejuvenation  to  web  services  that  suffer  software  aging  on  virtualized  platforms,  achieving  high  levels  of  availability.  To  exploit  the  provider  resources,  the  framework  also  seeks  to  maximize  the  number  of  services  running  simultaneously  on  the  platform,  while  guaranteeing  the  resources  needed  by  each  service.
1	Lessons  learned  using  a  static  analysis  tool  within  a  continuous  integration  system.  Static  analysis  tools  are  used  for  improving  software  quality  and  reliability.  Since  these  tools  can  be  time  consuming  when  used  for  analysis  of  big  codebases,  they  are  normally  run  during  scheduled  (e.g.  nightly)  builds.  However,  the  sooner  a  defect  is  found,  the  easier  it  is  to  fix  efficiently.  In  order  to  detect  defects  faster,  some  analysis  tools  offer  an  integration  with  the  integrated  development  environment  of  the  developers  at  the  cost  of  not  always  detecting  all  the  issues.  To  detect  defects  earlier  and  still  provide  a  reliable  solution,  one  could  think  of  running  an  analysis  tool  at  every  build  of  a  continuous  integration  system.  In  this  paper,  we  share  the  lessons  learned  during  the  integration  of  the  static  analysis  tool  Klocwork  (that  we  are  developing)  with  our  continuous  integration  system.  We  think  that  the  lessons  learned  will  be  beneficial  for  most  companies  developing  safety-critical  software  (or  less  critical  systems)  that  wish  to  run  their  analysis  tool  more  often  in  their  build  system.  We  report  these  lessons  learned  along  with  examples  of  our  successes  and  failures.
1	Experience  report  orthogonal  classification  of  safety  critical  issues.  Techniques  to  classify  defects  have  been  used  for  decades,  providing  relevant  information  on  how  to  improve  systems.  Such  techniques  heavily  rely  on  human  experience  and  have  been  generalized  to  cover  different  types  of  systems  at  different  maturity  levels.  However,  their  application  to  safety-critical  systems  development  and  operation  phases  neither  is  very  common,  or  at  least  not  spread  publicly,  nor  disseminated  in  the  industrial  and  academic  worlds.  This  practical  experience  report  presents  the  results  and  conclusions  from  applying  a  mature  Orthogonal  Defect  Classification  (ODC)  to  a  large  set  of  safety-critical  issues.  The  work  is  based  on  the  analysis  of  more  than  240  real  issues  (defects)  identified  during  all  the  lifecycle  phases  of  4  safety-critical  systems  in  the  aerospace  and  space  domains.  The  outcomes  reveal  the  challenges  in  properly  classifying  this  specific  type  of  issues  with  the  broader  ODC  approach.  The  difficulties  are  identified  and  systematized  and  specific  proposals  for  improvement  are  proposed.
1	Pat  3  an  extensible  architecture  for  building  multi  domain  model  checkers.  Model  checking  is  emerging  as  an  effective  software  verification  method.  Although  it  is  desirable  to  have  a  dedicated  model  checker  for  each  application  domain,  implementing  one  is  rather  challenging.  In  this  work,  we  develop  an  extensible  and  integrated  architecture  in  PAT3  (PAT  version  3.*)  to  support  the  development  of  model  checkers  for  wide  range  application  domains.  PAT3  adopts  a  layered  design  with  an  intermediate  representation  layer  (IRL),  which  separates  modeling  languages  from  model  checking  algorithms  so  that  the  algorithms  can  be  shared  by  different  languages.  IRL  contains  several  common  semantic  models  to  support  wide  application  domains,  and  builds  both  explicit  model  checking  and  symbolic  model  checking  under  one  roof.  PAT3  architecture  provides  extensibility  in  many  possible  aspects:  modeling  languages,  model  checking  algorithms,  reduction  techniques  and  even  IRLs.  Various  model  checkers  have  been  developed  under  this  new  architecture  in  recent  months.  This  paper  discusses  the  structure  and  extensibility  of  this  new  architecture.
1	Using  machine  learning  techniques  to  detect  metamorphic  relations  for  programs  without  test  oracles.  Much  software  lacks  test  oracles,  which  limits  automated  testing.  Metamorphic  testing  is  one  proposed  method  for  automating  the  testing  process  for  programs  without  test  oracles.  Unfortunately,  finding  the  appropriate  metamorphic  relations  required  for  use  in  metamorphic  testing  remains  a  labor  intensive  task,  which  is  generally  performed  by  a  domain  expert  or  a  programmer.  In  this  work  we  present  a  novel  approach  for  automatically  predicting  metamorphic  relations  using  machine  learning  techniques.  Our  approach  uses  a  set  of  features  developed  using  the  control  flow  graph  of  a  function  for  predicting  likely  metamorphic  relations.  We  show  the  effectiveness  of  our  method  using  a  set  of  real  world  functions  often  used  in  scientific  applications.
1	Introducing  trust  in  service  oriented  distributed  systems  through  blockchain.  Business  process  management  is  concerned  with  the  design  execution,  improvement,  and  monitoring  of  business  processes.  Systems  that  support  the  enactment  and  execution  of  processes  have  extensively  been  used  by  companies  to  streamline  and  automate  intra-organizational  processes.  However,  today's  business  enterprises  must  deal  with  global  competition,  heterogeneity,  and  rapidly  develop  new  services  and  products.  To  address  these  requirements,  the  services  reuse-based  approach  allowed  enterprises  to  reconsider  and  optimize  the  way  they  do  business,  and  change  their  information  systems  and  applications  to  support  collaborative  business  processes.  Service  choreographies  support  the  reuse-based  service-oriented  philosophy  in  that  they  represent  a  powerful  and  flexible  approach  to  realize  systems  by  (possibly)  reusing  services  and  composing  them  in  a  fully  distributed  way.  Nevertheless,  for  inter-organizational  processes,  challenges  of  collaborative  design  and  lack  of  mutual  trust  have  hampered  a  broader  uptake.  In  this  paper,  we  show  an  early  stage  approach  to  address  the  problem  of  trust  in  services  choreography  by  using  Blockchain  technologies,  in  order  to  support  the  decentralized  and  peer-to-peer  collaboration  in  a  trustworthy  manner,  even  in  a  network  without  any  mutual  trust  between  nodes.
1	Ufo  hidden  backdoor  discovery  and  security  verification  in  iot  device  firmware.  Recently,  the  use  of  embedded  devices  such  as  WiFi  APs,  IP  CAM,  and  drones  in  Internet  of  Things  (IoT)  applications  has  become  more  widespread.  These  embedded  devices  are  connected  to  networks  and  are  often  used  for  critical  services.  Thus,  they  receive  significant  attention  from  hackers  who  attempt  to  find  a  major  intrusion  vector  in  IoT  applications.  Hackers  focus  on  identifying  hidden  backdoors  in  embedded  devices  to  gain  full  remote  access;  if  they  gain  access,  they  can  cause  significant  damage  to  critical  infrastructures.  Therefore,  to  improve  embedded  device  security,  this  study  introduces  Universal  Firmware  vulnerability  Observer  (UFO);  UFO  is  a  firmware  vulnerability  discovery  system,  which  can  automatically  perform  tasks  such  as  reversing  firmware  embedded  filesystem,  identifying  vulnerability,  and  exploring  password  leaks  to  meet  the  IoT  firmware  security  verification  standards,  including  OWASP,  UL-2900,  and  ICSA  Labs.  In  addition,  we  design  a  Shell  Script  Dependency  algorithm  to  help  identify  hidden  backdoor  problems  by  discovering  suspicious  shell  script  execution  paths  in  the  extracted  firmware  filesystem.  We  use  237  real-world  embedded  device  firmware  files  to  evaluate  UFO.  The  results  indicate  that  the  effectiveness  of  reversing  firmware  binary  is  96%,  which  is  significantly  higher  than  that  of  open  source  tools.  Besides,  we  also  conclude  that  73%  of  firmware  files  contain  Common  Vulnerabilities  and  Exposures  in  their  embedded  Linux  kernel,  22%  of  firmware  files  can  leak  login  passwords,  and  6%  of  firmware  files  contain  hidden  backdoors.  Moreover,  we  reported  hidden  backdoor  problems  to  two  IoT  device  vendors  in  Taiwan  and  received  their  confirmation.  UFO  can  be  successfully  used  for  verifying  firmware  security  and  discovering  hidden  backdoor  threats  in  commercial  IoT  devices.
1	Debugging  with  dynamic  temporal  assertions.  Bugs  vary  in  their  root  causes  and  their  revealed  behaviors,  some  may  cause  a  crash  or  a  core  dump,  while  others  may  cause  an  incorrect  or  missing  output  or  an  unexpected  behavior.  Moreover,  most  bugs  are  revealed  long  after  their  actual  cause.  A  variable  might  be  assigned  early  in  the  execution,  and  that  value  may  cause  a  bug  far  from  that  last  assigned  place.  This  often  requires  users  to  manually  track  heuristic  information  over  different  execution  states.  This  information  may  include  a  trace  of  specific  variables'  values  and  their  assigned  locations,  functions  and  their  returned  values,  and  detailed  execution  paths.  This  paper  introduces  Dynamic  Temporal  Assertions  (DTA)  into  the  conventional  source-level  debugging  session.  It  extends  a  typical  gdb  like  source  level  debugger  named  UDB  with  on-the-fly  temporal  assertions.  Each  assertion  is  capable  of:  1)  validating  a  sequence  of  execution  states,  named  temporal  interval,  and  2)  referencing  out-of-scope  variables,  which  may  not  be  live  in  the  execution  state  at  evaluation  time.  These  new  DTA  assertions  are  not  bounded  by  the  limitations  of  ordinary  in-code  assertions  such  as  locality,  temporality,  and  static  hardwiring  into  the  source  code.  Furthermore,  they  advance  typical  interactive  debugging  sessions  and  their  conditional  breakpoints  and  watch  points.
1	Anomaly  detection  as  a  service.  Cloud  systems  are  complex,  large,  and  dynamic  systems  whose  behavior  must  be  continuously  analyzed  to  timely  detect  misbehaviors  and  failures.  Although  there  are  solutions  to  flexibly  monitor  cloud  systems,  cost-effectively  controlling  the  anomaly  detection  logic  is  still  a  challenge.  In  particular,  cloud  operators  may  need  to  quickly  change  the  types  of  detected  anomalies  and  the  scope  of  anomaly  detection,  for  instance  based  on  observations.  This  kind  of  intervention  still  consists  of  a  largely  manual  and  inefficient  ad-hoc  effort.  In  this  paper,  we  present  Anomaly  Detection  as-a-Service  (ADaaS),  which  uses  the  same  as-a-service  paradigm  often  exploited  in  cloud  systems  to  declarative  control  the  anomaly  detection  logic.  Operators  can  use  ADaaS  to  specify  the  set  of  indicators  that  must  be  analyzed  and  the  types  of  anomalies  that  must  be  detected,  without  having  to  address  any  operational  aspect.  Early  results  with  lightweight  detectors  show  that  the  presented  approach  is  a  promising  solution  to  deliver  better  control  of  the  anomaly  detection  logic.
1	A  study  of  regression  test  selection  in  continuous  integration  environments.  Continuous  integration  (CI)  systems  perform  the  automated  build,  test  execution,  and  delivery  of  the  software.  CI  can  provide  fast  feedback  on  software  changes,  minimizing  the  time  and  effort  required  in  each  iteration.  In  the  meantime,  it  is  important  to  ensure  that  enough  testing  is  performed  prior  to  code  submission  to  avoid  breaking  builds.  Recent  approaches  have  been  proposed  to  improve  the  cost-effectiveness  of  regression  testing  through  techniques  such  as  regression  test  selection  (RTS).  These  approaches  target  at  CI  environments  because  traditional  RTS  techniques  often  use  code  instrumentation  or  very  fine-grained  dependency  analysis,  which  may  not  be  able  to  handle  rapid  changes.  In  this  paper,  we  study  in-depth  the  usage  of  RTS  in  CI  environments  for  different  open-source  projects.  We  analyze  918  open-source  projects  using  CI  in  GitHub  to  understand  1)  under  what  conditions  RTS  is  needed,  and  2)  how  to  balance  the  trade-offs  between  granularity  levels  to  perform  cost-effective  RTS.  The  findings  of  this  study  can  aid  practitioners  and  researchers  to  develop  more  advanced  RTS  techniques  for  being  adapted  to  CI  environments.
1	Software  aging  and  rejuvenation  in  the  cloud  a  literature  review.  With  cloud  computing  becoming  pervasive  in  IT  enterprises,  the  long-running  performance  of  cloud-based  applications  and  services  and  of  the  underlying  virtualization  systems  is  a  paramount  concern.  Researchers  have  been  studying  the  phenomenon  of  software  aging,  and  the  corresponding  rejuvenation  countermeasures,  in  cloud-related  contexts  since  about  10  years.  This  paper  reviews  the  effort  conducted  so  far  by  the  software  aging  and  rejuvenation  (SAR)  community  in  the  cloud  domain.  Figures  about  current  research  trends  and  future  directions  can  be  derived  from  the  reported  results.
1	Towards  anomaly  detectors  that  learn  continuously.  In  this  paper,  we  first  discuss  the  challenges  of  adapting  an  already  trained  DNN-based  anomaly  detector  with  knowledge  mined  during  the  execution  of  the  main  system.  Then,  we  present  a  framework  for  the  continual  learning  of  anomaly  detectors,  which  records  in-field  behavioural  data  to  determine  what  data  are  appropriate  for  adaptation.  We  evaluated  our  framework  to  improve  an  anomaly  detector  taken  from  the  literature,  in  the  context  of  misbehavior  prediction  for  self-driving  cars.  Our  results  show  that  our  solution  can  reduce  the  false  positive  rate  by  a  large  margin  and  adapt  to  nominal  behaviour  changes  while  maintaining  the  original  anomaly  detection  capability.
1	Connecting  kids  to  stem  through  entrepreneurship  and  innovation.  In  this  paper,  the  authors  outline  preliminary  results  on  the  introduction  of  entrepreneurs  and  innovators  in  classrooms,  demonstrating  the  positive  impact  on  both  teaching  kids  STEM,  as  well  as  influencing  certain  demographics  to  aspire  to  STEM-related  careers.  The  authors  hypothesize  that  connecting  students  to  STEM  in  elementary  school,  primarily  through  the  introduction  to  STEM-based  entrepreneurs  and  innovators,  will  ultimately  result  in  a  more  STEM-inspired  and  STEM-literate  workforce.  If  this  model  is  followed  collectively  by  educators,  business,  and  government  alike,  the  authors  believe  that  significant  progress  can  be  made  in  mitigating  the  loss  of  students  interested  in  STEM  during  middle  school.
1	Application  of  tuned  liquid  damper  for  controlling  structural  vibration  due  to  earthquake  excitations.  Current  trends  in  construction  industry  demands  for  taller  and  lighter  structures,  which  are  also  more  flexible  and  having  quite  low  damping  value.  This  increases  failure  possibilities  and  also,  problems  from  serviceability  point  of  view.  Several  techniques  are  available  today  to  minimize  the  vibration  of  the  structure,  out  of  which  concept  of  using  of  Tuned  Liquid  Damper  (TLD)  is  a  newer  one.  The  TLDs  have  been  used  to  control  the  wind  induced  structural  vibration.  However,  the  seismic  effective  of  TLD  remain  an  important  issue  for  the  study.  In  this  study,  an  attempt  has  been  made  to  study  the  effectiveness  of  Tuned  Liquid  Damper  (TLD)  for  controlling  seismic  vibration  of  the  structure.  Finite  element  elements  are  used  to  model  the  structure  and  the  liquid  in  the  TLD.  A  computer  code  is  developed  in  MATLAB  to  study  the  response  of  the  structure,  the  liquid  sloshing  in  the  TLD  and  coupled  structure-TLD  system.  A  ten  storey  and  two  bay  frame  structure  was  analyzed  using  different  ground  motions.  First  one  was  a  sinusoidal  loading  corresponding  to  the  resonance  condition  with  the  fundamental  frequency  of  the  structure,  second  one  was  corresponding  to  compatible  time  history  as  per  response  spectra  of  Indian  Standard-1893  (Part  -1):2002  for  5%  damping  at  rocky  soil.  It  is  observed  from  the  present  study  that,  TLD  can  be  used  to  control  the  vibration  of  the  structure  due  earthquake  excitations.  Only  Tuned  Liquid  Dampers,  which  are  properly  tuned  to  natural  frequency  of  structure,  are  more  effective  in  controlling  the  vibration.  The  damping  effect  of  TLD  decreases  with  mistuning  of  the  TLD.
1	The  development  of  a  larvicidal  tablet  using  saba  banana  musa  acuminata  x  balbisiana  peel  extract.  Mosquitoes  are  a  major  problem  around  the  world,  with  the  diseases  they  spread  being  responsible  for  millions  of  deaths  every  year  (Kamaraj  et  al.,  2011).  Mosquito  population  may  be  controlled  by  reducing  the  larvae  population  which  may  be  found  in  mosquito  breeding  areas  such  as  stagnant  water.  Larvicidal  properties  have  been  found  within  certain  plant  extracts.  Fernandes  et  al.  (2014)  found  that  Manilkara  subsericea  extract  has  insecticidal  properties.  A  study  by  Kamaraj  et  al.  (2011)  also  found  that  Annona  squamosa  L.,  also  known  as  the  custard  apple,  exhibits  larvicidal  activity  against  mosquito  larvae.  In  this  study,  Musa  acuminata  x  balbisiana  peel  extract  was  hypothesized  to  have  such  properties,  possibly  due  to  secondary  metabolites  within  extracts  which  may  affect  the  digestive  system  of  larvae.  The  extract  was  obtained  by  mixing  the  peels  with  70%  isopropyl  alcohol.  The  tablets  were  formulated  by  mixing  the  saba  banana  peel  extract  with  sodium  bicarbonate,  citric  acid  and  corn  starch.  The  tablets  were  made  with  two  different  concentrations  of  the  extract,  one  in  5%  concentration  and  another  in  10%  concentration.  The  experiment  setup  was  done  by  placing  the  tablets  or  the  commercial  larvicide  in  100  mL  of  water  with  20  mosquito  larvae.  The  mean  of  the  dead  mosquitoes  in  each  group  was  observed  and  compared.  To  compare  the  data,  the  t-test  for  two  sample  means  (two-tailed)  was  used.  It  was  found  that  2  days  after  application,  the  experimental  tablets  were  likely  less  effective  than  commercial  larvicide,  however  after  3  days,  it  was  found  that  the  10%  extract  group  was  as  effective  as  the  commercial  larvicide  group.  The  t-tests  between  the  5%  and  10%  extract  groups  showed  that  the  5%  extract  group  was  not  able  to  kill  as  much  larvae.  This  led  to  the  conclusion  that  the  proposed  treatments  were  possibly  effective,  especially  the  tablets  of  the  10%  extract  group,  although  slower  than  commercial  larvicide.  Future  works  may  experiment  with  higher  extract  concentrations  such  as  20%  and  30%  and  other  extraction  methods  and  may  also  include  an  analysis  of  the  chemical  composition  of  the  extract  and  the  growth  factor  rather  than  the  mortality  factor  of  the  treatment.
1	Environmental  impact  assessment  of  post  tensioned  and  conventional  reinforced  concrete  slab  design.  This  research  determines  environmental  advantages  achievable  through  application  of  alternate  concrete  slab  construction  methods  for  a  typical  concrete  office  building.  The  collaborative  pilot  study  has  been  conducted  to  compare  the  significant  material  reductions  on  alternate  slab  systems  to  improve  sustainable  design  methods.  These  structures  were  designed  in  accordance  with  all  relevant  Australian  standards,  incorporating  with  various  concrete  strengths  and  various  geometric  parameters  such  as  number  of  columns,  spans  with  respect  to  slab  thickness.  The  results  indicated  that  environmental  advantages  are  achievable  through  the  application  of  alternate  concrete  slab  construction  methods  for  the  typical  office  structure.  Analysis  results  indicated  an  18.2  to  37.0  %  reduction  in  concrete  volume  and  21.0  to  43.6%  reduction  in  steel  mass  for  the  post  tensioned  structure  to  compare  with  the  conventional  reinforced  concrete  slab.  Similar  results  were  observed  when  comparing  reductions  in  embodied  energy  28.1  to  40.9%.
1	Deadlock  and  temporal  properties  analysis  in  mixed  reality  applications.  Mixed  reality  systems  overlay  real  data  with  virtual  information  in  order  to  assist  users  in  their  current  task,  they  are  used  in  many  fields  (surgery,  maintenance,  entertainment).  Such  systems  generally  combine  several  hardware  components  operating  at  different  time  scales,  and  software  that  has  to  cope  with  these  timing  constraints.  MIRELA,  for  Mixed  Reality  Language,  is  a  framework  aimed  at  modelling,  analysing  and  implementing  systems  composed  of  sensors,  processing  units,  shared  memories  and  rendering  loops,  communicating  in  a  well-defined  manner  and  submitted  to  timing  constraints.  The  paper  describes  how  harmful  software  behaviour,  which  may  result  in  possible  hardware  deterioration  or  revert  the  system's  primary  goal  from  user  assistance  to  user  impediment,  may  be  detected  such  as  (global  and  local)  deadlocks  or  starvation  features.  This  also  includes  a  study  of  temporal  properties  resulting  in  a  finer  understanding  of  the  software  timing  behaviour,  in  order  to  fix  it  if  needed.
1	Automatically  repairing  web  application  firewalls  based  on  successful  sql  injection  attacks.  Testing  and  fixing  Web  Application  Firewalls  (WAFs)  are  two  relevant  and  complementary  challenges  for  security  analysts.  Automated  testing  helps  to  cost-effectively  detect  vulnerabilities  in  a  WAF  by  generating  effective  test  cases,  i.e.,  attacks.  Once  vulnerabilities  have  been  identified,  the  WAF  needs  to  be  fixed  by  augmenting  its  rule  set  to  filter  attacks  without  blocking  legitimate  requests.  However,  existing  research  suggests  that  rule  sets  are  very  difficult  to  understand  and  too  complex  to  be  manually  fixed.  In  this  paper,  we  formalise  the  problem  of  fixing  vulnerable  WAFs  as  a  combinatorial  optimisation  problem.  To  solve  it,  we  propose  an  automated  approach  that  combines  machine  learning  with  multi-objective  genetic  algorithms.  Given  a  set  of  legitimate  requests  and  bypassing  SQL  injection  attacks,  our  approach  automatically  infers  regular  expressions  that,  when  added  to  the  WAF&#x2019;s  rule  set,  prevent  many  attacks  while  letting  legitimate  requests  go  through.  Our  empirical  evaluation  based  on  both  open-source  and  proprietary  WAFs  shows  that  the  generated  filter  rules  are  effective  at  blocking  previously  identified  and  successful  SQL  injection  attacks  (recall  between  54.6%  and  98.3%),  while  triggering  in  most  cases  no  or  few  false  positives  (false  positive  rate  between  0%  and  2%).
1	Discovering  model  transformation  pre  conditions  using  automatically  generated  test  models.  Specifying  a  model  transformation  is  challenging  as  it  must  be  able  to  give  a  meaningful  output  for  any  input  model  in  a  possibly  infinite  modeling  domain.  Transformation  pre-conditions  constrain  the  input  domain  by  rejecting  input  models  that  are  not  meant  to  be  transformed  by  a  model  transformation.  This  paper  presents  a  systematic  approach  to  discover  such  pre-conditions  when  it  is  hard  for  a  human  developer  to  foresee  complex  graphs  of  objects  that  are  not  meant  to  be  transformed.  The  approach  is  based  on  systematically  generating  a  finite  number  of  test  models  using  our  tool,  PRAMANA  to  first  cover  the  input  domain  based  on  input  domain  partitioning.  Tracing  a  transformation's  execution  reveals  why  some  pre-conditions  are  missing.  Using  a  benchmark  transformation  from  simplified  UML  class  diagram  models  to  RDBMS  models  we  discover  new  pre-conditions  that  were  not  initially  specified.
1	Spotting  problematic  code  lines  using  nonintrusive  programmers  biofeedback.  Recent  studies  have  shown  that  programmers'  cognitive  load  during  typical  code  development  activities  can  be  assessed  using  wearable  and  low  intrusive  devices  that  capture  peripheral  physiological  responses  driven  by  the  autonomic  nervous  system.  In  particular,  measures  such  as  heart  rate  variability  (HRV)  and  pupillography  can  be  acquired  by  nonintrusive  devices  and  provide  accurate  indication  of  programmers'  cognitive  load  and  attention  level  in  code  related  tasks,  which  are  known  elements  of  human  error  that  potentially  lead  to  software  faults.  This  paper  presents  an  experimental  study  designed  to  evaluate  the  possibility  of  using  HRV  and  pupillography  together  with  eye  tracking  to  identify  and  annotate  specific  code  lines  (or  even  finer  grain  lexical  tokens)  of  the  program  under  development  (or  under  inspection)  with  information  on  the  cognitive  load  of  the  programmer  while  dealing  with  such  lines  of  code.  The  experimental  data  is  discussed  in  the  paper  to  assess  different  alternatives  for  using  code  annotations  representing  programmers'  cognitive  load  while  producing  or  reading  code.  In  particular,  we  propose  the  use  of  biofeedback  code  highlighting  techniques  to  provide  online  programmer's  warnings  for  potentially  problematic  code  lines  that  may  need  a  second  look  at  (to  remove  possible  bugs),  and  biofeedback-driven  software  testing  to  optimize  testing  effort,  focusing  the  tests  on  code  areas  with  higher  bug  probability
1	Kerep  experience  in  extracting  knowledge  on  distributed  system  behavior  through  request  execution  path.  Expertise  on  distributed  systems  is  critical  for  system  maintenance  and  improvement.  However,  it  is  challenging  to  keep  the  up-to-date  knowledge  from  distributed  systems  due  to  the  complexity  and  continuous  updates.  Hence,  computing  platform  providers  study  on  how  to  extract  knowledge  directly  from  system  behavior.  In  this  paper,  we  propose  a  methodology  called  KEREP  to  automatically  extract  knowledge  on  distributed  system  behavior  through  request  execution  path.  Technologies  are  devised  to  construct  component  structures,  to  depict  the  in-depth  dynamic  behavior  and  to  identify  the  heartbeat  mechanisms  of  target  distributed  systems.  Experiments  on  two  real-world  distributed  systems  show  the  KEREP  methodology  extracts  accurate  knowledge  of  request  processing  and  discovers  undocumented  features  with  good  execution  performance.
1	Av  fuzzer  finding  safety  violations  in  autonomous  driving  systems.  This  paper  proposes  AV-FUZZER,  a  testing  framework,  to  find  the  safety  violations  of  an  autonomous  vehicle  (AV)  in  the  presence  of  an  evolving  traffic  environment.  We  perturb  the  driving  maneuvers  of  traffic  participants  to  create  situations  in  which  an  AV  can  run  into  safety  violations.  To  optimally  search  for  the  perturbations  to  be  introduced,  we  leverage  domain  knowledge  of  vehicle  dynamics  and  genetic  algorithm  to  minimize  the  safety  potential  of  an  AV  over  its  projected  trajectory.  The  values  of  the  perturbation  determined  by  this  process  provide  parameters  that  define  participants’  trajectories.  To  improve  the  efficiency  of  the  search,  we  design  a  local  fuzzer  that  increases  the  exploitation  of  local  optima  in  the  areas  where  highly  likely  safety-hazardous  situations  are  observed.  By  repeating  the  optimization  with  significantly  different  starting  points  in  the  search  space,  AV-FUZZER  determines  several  diverse  AV  safety  violations.  We  demonstrate  AV-FUZZER  on  an  industrial-grade  AV  platform,  Baidu  Apollo,  and  find  five  distinct  types  of  safety  violations  in  a  short  period  of  time.  In  comparison,  other  existing  techniques  can  find  at  most  two.  We  analyze  the  safety  violations  found  in  Apollo  and  discuss  their  overarching  causes.
1	Enforcing  structure  on  textual  use  cases  via  annotation  models.  Textual  use  cases  are  commonly  used  to  represent  software  requirements  at  initial  stages.  However  in  most  of  the  cases,  these  documents  are  unstructured.  In  this  paper,  we  present  a  linguistic  engine  for  processing  textual  use  cases  and  extract  a  structured  model  in  terms  of  an  annotation  model  out  of  these  use  cases.  An  annotation  model  of  a  use  case  can  further  be  used  to  generate  various  UML  requirements  models,  Business  Process  Models  and  ontology.  The  implementation  details  of  Natural  Language  Processing  (NLP)  technique  employed  by  us  for  the  linguistic  engine  is  described  in  this  paper  in  detail.  Also,  we  consider  a  corpus  containing  123  use  cases  from  real-life  industrial  projects  within  our  company,  and  translate  them  into  annotation  models  using  our  NLP  technique.  For  evaluating  the  performance  of  conversion  we  use  a  few  metrics  and  report  some  promising  results  for  our  linguistic  engine.
1	Accountability  for  abstract  component  design.  The  importance  of  the  services-based  market,  62.9%  of  the  World  gross  domestic  product  (GDP)  [1],  triggered  an  increase  in  the  use  of  software  offered  on-line  as  services  (SaaS).  The  use  of  such  software  usually  implies  the  flow  of  personal  data  on-line  between  several  parties.  This  can  make  users  reluctant  to  their  use.  In  this  work,  we  consider  this  issue  at  the  design-time  of  the  software  and  we  propose  some  foundations  for  an  accountable  software  design.  Accountability  for  a  software  is  a  property  describing,  among  other  aspects,  its  liability  to  end-users  for  the  usage  of  the  data  it  has  been  entrusted.  We  propose  to  enrich  software's  component  design  by  accountability  clauses  using  an  abstract  accountability  language  (AAL).  We  also  define  conditions  for  the  well-formedness  of  an  accountable  component  design  and  show  how  they  can  be  checked  using  a  model-checking  tool.
1	Fi4fa  a  formalism  for  incompletion  inconsistency  interference  and  impermanence  failures  analysis.  To  architect  dependable  distributed  component-based,  transactional  systems,  failures  as  well  as  their  mitigation  behaviors  must  be  analyzed.  Analysis  helps  in  planning  if,  where  and  which  mitigation  means  are  needed  to  increase  quality,  by  reducing  the  failures  that  threaten  the  system's  dependability.  Fault  Propagation  and  Transformation  Calculus  (FPTC)  is  a  technique  for  automatically  calculating  the  failure  behavior  of  the  entire  system  from  the  failure  behavior  of  its  components  [1].  FPTC,  however,  considers  few  failure  types  and  offers  no  support  to  analyse  the  mitigation  behaviour.  To  overcome  these  limitations  and  support  the  mitigation's  planning,  we  introduce  a  new  formalism,  called  FI4FA.  FI4FA  focuses  on  failures  avoidable  through  transaction-based  mitigations.  FI4FA  extends  FPTC  by  enabling  the  analysis  of  I4  (incompletion,  inconsistency,  interference  and  impermanence)  failures  as  well  as  the  analysis  of  the  mitigations,  needed  to  guarantee  completion,  consistency,  isolation  and  durability.  We  also  illustrate  the  usage  of  FI4FA  on  a  set  of  examples.
1	Model  based  design  and  testing  of  decisional  autonomy  and  cooperation  in  cyber  physical  systems.  This  article  presents  a  study  on  the  benefits  offered  by  Coloured  Petri  Nets  in  capturing  and  separating  permanent  and  temporary  behavioural  information  and  on  the  systematic  support  they  hereby  provide  to  model-based  design  and  testing  of  cyber-physical  systems.  In  particular,  it  illustrates  the  application  of  CPN  modelling  to  capture  the  behaviour  of  cooperative  mobile  robots  and  highlights  their  benefits  in  terms  of  compactness  and  scalability.  Finally,  the  article  reports  on  the  applicability  of  test  case  generation  algorithms  supporting  the  coverage  of  the  underlying  CPN  models  with  respect  to  different  testing  criteria.
1	Research  landscape  of  patterns  and  architectures  for  iot  security  a  systematic  review.  We  have  entered  a  tremendous  computerized  rev-olution  of  the  Internet  of  Things  (IoT)  era  when  everything  is  connected.  The  popularity  of  IoT  systems  makes  security  for  the  IoT  of  paramount  importance.  Security  patterns  consist  of  domain-independent  time-proven  security  knowledge  and  expertise.  Would  they  be  applicable  to  develop  secure  IoT  systems?  We  aim  to  draw  a  research  landscape  of  patterns  and  architectures  for  IoT  security  by  conducting  a  systematic  literature  review.  From  more  than  a  thousand  of  candidate  papers,  we  have  systematically  distinguished  and  analyzed  twenty-two  (22)  papers  that  have  been  published  around  patterns  and  architectures  for  IoT  security  (and  privacy).  Our  analysis  shows  a  rise  in  the  number  of  publications  tending  to  security  patterns  and  architectures  in  the  last  two  years.  Within  this  rise,  we  see  that  most  patterns  and  architectures  are  applicable  for  all  IoT  systems,  while  some  are  limited  within  specific  domains.  However,  there  are  gaps  in  this  research  area  that  can  be  filled  in  to  promote  the  utilization  of  patterns  for  IoT  security  and  privacy.
1	Bridging  cultural  differences  a  grounded  theory  perspective.  Cultural  differences  often  arise  in  distributed  software  development.  The  impact  of  cultural  differences  on  distributed  teams  is  under-explored.  The  lack  of  knowledge  of  cultural  differences  can  cause  major  problems  to  the  distributed  teams.  We  have  conducted  a  Grounded  Theory  study  to  uncover  the  strategies  adopted  by  Agile  practitioners  to  overcome  the  cultural  differences  in  distributed  software  development.  We  interviewed  18  Agile  practitioners  across  10  software  companies  in  the  USA  and  India  over  a  period  of  1.5  years.  In  this  paper,  we  report  that  our  participants  adopt  five  effective  strategies  to  bridge  cultural  differences  in  distributed  Agile  software  development:  engendering  cultural  awareness,  understanding  cultural  differences,  sharing  work  practices,  rotating  team  ambassadors,  and  managing  language  barriers.
1	A  mini  experiment  of  offering  stem  education  to  several  age  groups  through  the  use  of  robots.  This  paper  reports  the  use  of  robots  in  teaching  the  essence  of  Science,  Technology,  Engineering,  and  Mathematics  (STEM)  to  students  from  first  grade  to  12th  grade.  This  STEM  skills  learning  experience  was  developed  gradually  over  4.5  years  and  tested  by  over  350  students.  Since  the  sample  size  is  small  and  the  time  span  is  short,  only  limited  statistics  data  is  available.  The  main  goal  of  this  paper  is  sharing  firsthand  experience  of  using  the  same  set  of  robots  to  teach  students  over  12  grades.  This  paper  starts  with  general  information  of  the  LEGO  Mindstorms  EV3  robots,  the  basic  methods,  and  the  concept  of  STEM  in  very  simple  terms.  This  paper  then  explains  how  building  and  programming  robots  was  implemented  in  different  ways  for  different  age  and  experience  groups.  There  are  5  groups  in  discussion.  The  first  group  is  the  youngest  and  without  any  experience.  The  second  group  is  older  but  without  experience.  The  third  group  is  older  and  with  some  experience.  The  fourth  group  is  a  group  of  middle  school  age  (grade  5  to  8)  students,  mostly  with  some  pre-exposure  to  robots.  The  last  group  consists  of  high  school  students.  Each  group  is  given  an  age-appropriate  goal.  It  is  observed  that  they  have  different  challenges  to  conquer.  While  using  LEGO  Mindstorms  EV3  (and  the  previous  version,  NXT)  is  well  studied,  having  hands-on  experience  of  building  a  learning  path  for  students  from  first  grade  to  12th  grade  is  of  value.  Finally,  the  expectation  of  education  results  in  the  future  is  touched  upon.
1	Investigating  functional  and  code  size  measures  for  mobile  applications.  This  paper  investigates  the  use  of  the  COSMIC  functional  size  measurement  method  for  mobile  applications.  Some  proposals  have  been  recently  introduced  to  size  mobile  applications  in  terms  of  COSMIC.  In  this  work  we  empirically  analyse  whether  the  COSMIC  functional  size  of  mobile  applications  can  be  exploited  to  estimate  the  size  of  the  final  applications  in  terms  of  lines  of  code  and  number  of  bytes  of  the  source  code  and  byte  code.  To  this  end,  we  take  into  account  a  total  of  7  different  code  size  measures  collected  from  13  Android  applications.  The  results  of  the  empirical  study  show  that  the  COSMIC  functional  size  is  strongly  correlated  to  all  the  size  measures  taken  into  account  and  that  it  can  be  also  used  to  predict  the  mobile  application  size  in  terms  of  bytes  with  a  high  accuracy.
1	Correlations  between  problem  domain  and  solution  domain  size  measures  for  open  source  software.  Predicting  how  much  effort  will  be  required  to  complete  a  software  project  as  early  as  possible  is  a  very  important  factor  in  the  success  of  software  development  projects.  Including  function  points  and  its  variants,  there  are  several  size  measures  and  corresponding  measurement  methods  that  can  be  used  for  effort  estimation.  However,  in  most  of  the  projects,  there  is  limited  amount  of  information  available  in  the  early  stages  and  significant  effort  is  spent  for  size  measurement  and  effort  estimation  with  such  methods.  This  paper  analyzes  the  correlation  between  the  size  metrics  of  conceptual  model  of  the  problem  domain  and  the  resulting  software.  For  this  purpose,  we  consider  open  source  project  management  and  game  software.  We  apply  linear  regression  and  cross  validation  techniques  to  investigate  the  relation  between  the  sizes  of  problem  domain  (i.e.,  Conceptual)  and  solution  domain  (i.e.,  Design)  models.  The  results  reveal  a  high  correlation  between  the  number  of  conceptual  classes  in  the  problem  domain  model  and  the  number  of  software  classes  constituting  the  corresponding  software.  The  results  suggest  that  it  is  possible  to  use  problem  domain  descriptions  in  the  early  stages  of  software  development  projects  to  make  plausible  predictions  for  the  size  of  the  software.
1	Configuration  of  cardinality  based  feature  models  using  generative  constraint  satisfaction.  Existing  feature  modeling  approaches  and  tools  are  based  on  classical  constraint  satisfaction  which  consists  of  a  fixed  set  of  variables  and  a  fixed  set  of  constraints  on  these  variables.  In  many  applications  however,  features  may  not  only  be  selected  but  cloned  so  that  the  numbers  of  involved  variables  and  constraints  are  not  known  from  the  beginning.  We  present  a  novel  configuration  approach  for  corresponding  cardinality-based  feature  models  based  on  the  formalism  of  generative  constraint  satisfaction  which  -  in  extension  to  many  existing  approaches  -  is  able  to  handle  constraints  in  the  context  of  multiple  (cloned)  features  (e.g.,  by  automatically  creating  new  feature  clones  on  the  fly).
1	Alfred  a  methodology  to  enable  component  fault  trees  for  layered  architectures.  Identifying  drawbacks  or  insufficiencies  in  terms  of  safety  is  important  also  in  early  development  stages  of  safety  critical  systems.  In  industry,  development  artefacts  such  as  components  or  units,  are  often  reused  from  existing  artefacts  to  save  time  and  costs.  When  development  artefacts  are  reused,  their  existing  safety  analysis  models  are  an  important  input  for  an  early  safety  assessment  for  the  new  system,  since  they  already  provide  a  valid  model.  Component  fault  trees  support  such  reuse  strategies  by  a  compositional  horizontal  approach.  But  current  development  strategies  do  not  only  divide  systems  horizontally,  e.g.,  By  encapsulating  different  functionality  into  separate  components  and  hierarchies  of  components,  but  also  vertically,  e.g.  Into  software  and  hardware  architecture  layers.  Current  safety  analysis  methodologies,  such  as  component  fault  trees,  do  not  support  such  vertical  layers.  Therefore,  we  present  here  a  methodology  that  is  able  to  divide  safety  analysis  models  into  different  layers  of  a  systems  architecture.  We  use  so  called  Architecture  Layer  Failure  Dependencies  to  enable  component  fault  trees  on  different  layers  of  an  architecture.  These  dependencies  are  then  used  to  generate  safety  evidence  for  the  entire  system  and  over  all  different  architecture  layers.  A  case  study  applies  the  approach  to  hardware  and  software  layers.
1	A  holistic  component  based  approach  to  autosar  designs.  The  development  of  new  automotive  functions  in  a  car  must  comply  with  the  AUTOSAR  standard.  Such  functions  are  distributed  over  the  ECUs  of  the  car  connected  with  buses.  The  development  of  these  distributed  functions  is  not  easy  in  AUTOSAR  because  there  is  no  global  view  of  the  system.  In  this  paper  we  propose  an  approach  that  counters  this  difficulty  by  designing  automotive  systems  with  a  global  view  and  then  transforming  these  systems  into  AUTOSAR  systems.
1	Confirming  distortional  behaviors  in  software  cost  estimation  practice.  Cost  estimation  of  software  projects  is  an  important  management  activity.  Despite  research  efforts  the  accuracy  of  estimates  does  not  seem  to  improve.  In  this  paper  we  confirm  intentional  distortions  of  estimates  reported  in  a  previous  study.  This  study  is  based  on  questionnaire  responses  from  48  software  practitioners  from  eight  different  companies.  The  results  of  the  questionnaire  suggest  that  prevalence  of  intentional  distortions  is  affected  by  the  organizational  type  and  the  development  process  in  use.  Further,  we  extend  the  results  with  information  about  three  companies'  estimation  practices  and  related  distortions  collected  in  interviews  with  three  managers.  Lastly,  based  on  these  results  and  additional  organizational  politics  theory  we  describe  organizational  politics  tactics  that  affect  cost  estimates.
1	Adapting  software  quality  models  practical  challenges  approach  and  first  empirical  results.  Measuring  and  evaluating  software  quality  has  become  a  fundamental  task.  Many  models  have  been  proposed  to  support  stakeholders  in  dealing  with  software  quality.  However,  in  most  cases,  quality  models  do  not  fit  perfectly  for  the  target  application  context.  Since  approaches  for  efficiently  adapting  quality  models  are  largely  missing,  many  quality  models  in  practice  are  built  from  scratch  or  reuse  only  high-level  concepts  of  existing  models.  We  present  a  tool-supported  approach  for  the  efficient  adaptation  of  quality  models.  An  initial  empirical  investigation  indicates  that  the  quality  models  obtained  applying  the  proposed  approach  are  considerably  more  consistently  and  appropriately  adapted  than  those  obtained  following  an  ad-hoc  approach.  Further,  we  could  observe  that  model  adaptation  is  significantly  more  efficient  (~factor  8)  when  using  this  approach.
1	Augmenting  app  reviews  with  app  changelogs  an  approach  for  app  reviews  classification.  Recent  research  on  the  automatic  classification  of  app  reviews  either  focused  on  grouping  app  reviews  into  categories  relevant  to  software  evolution,  or  employed  app  reviews  as  the  only  research  data  to  improve  app  reviews  classification.  Although  it  was  reported  that  app  review  classification  can  benefit  from  supplementing  user  reviews  with  the  data  from  other  sources,  only  a  few  studies  employed  app  changelogs  for  this  purpose.  This  paper  explores  how  to  augment  app  reviews  with  changelogs  to  improve  the  accuracy  and  performance  of  classifying  functional  and  non-functional  requirements  in  app  reviews.  Specifically,  we  propose  AUG-AC  as  an  approach  to  extract  feature  words  from  app  changelogs  and  construct  the  augments  for  app  reviews.  Next,  we  designed  a  series  of  experiments  to  evaluate  our  approach,  varying  in  the  length  of  AC-based  augments  for  app  reviews.  The  results  show  that  AUG-AC  outperforms  the  existing  method  by  using  app  changelogs  as  a  source  of  data  next  to  app  reviews.
1	Bug  inducing  analysis  to  prevent  fault  prone  bug  fixes.  Bug  fix  is  an  important  and  challenging  task  in  software  development  and  maintenance.  Bug  fix  is  also  a  dangerous  change,  because  it  might  induce  new  bugs.  It  is  difficult  to  decide  whether  a  bug  fix  is  safe  in  practice.  In  this  paper,  we  conducted  an  empirical  study  on  bug  inducing  analysis  to  discover  the  types  and  features  of  fault  prone  bug  fixes.  We  use  a  classical  algorithm  to  track  the  location  of  the  code  changes  introducing  the  bugs.  The  change  types  of  the  codes  will  be  checked  by  an  automatic  tool  and  whether  this  change  is  a  bug  fix  change  is  recorded.  We  analyze  the  statistics  to  find  out  what  types  of  change  are  most  prone  to  induce  new  bugs  when  they  are  intended  to  fix  a  bug.  Finally,  some  guidelines  are  provided  to  help  developers  prevent  such  fault  prone  bug  fixes.
1	Long  term  active  integrator  prediction  in  the  evaluation  of  code  contributions.  In  open  source  software  (OSS)  projects,  integrators  are  given  high-level  access  to  repositories  so  that  they  could  maintain  and  manage  projects.  Although  integrators  play  a  critical  role  in  evaluating  code  changes  for  OSS  projects,  they  may  be  short-term  active.  Long-term  active  integrators  keep  in  evaluating  code  update  submission  and  managing  responses  from  contributors.  In  order  to  survive  and  succeed,  OSS  projects  need  to  attract  and  retain  long-term  active  integrators.  To  assist  OSS  projects  to  retain  active  integrators,  we  propose  a  method  called  LTAPredict  to  predict  whether  integrators  will  be  long-  term  active  in  the  evaluation  of  code  contributions.  LTAPredict  collects  activity  data  of  integrators,  extracts  a  rich  set  of  features,  and  makes  prediction  via  machine  learning  techniques.  We  perform  experiments  on  37  popular  projects,  containing  a  total  of  1,073  integrators.  Results  show  that  based  on  the  Decision  Tree,  LTAPredict  achieves  the  accuracy  as  0.829,  the  precision  as  0.81,  the  recall  as  0.827  and  the  F1  as  0.818.  Meanwhile,  we  evaluate  the  feature  importance  to  identify  the  most  significant  indicators  of  long-term  active  integrators.  We  observe  that  whether  integrators  becoming  long-term  active  is  associated  with  the  number  of  active  months  and  social  distance  with  contributors  in  their  first  year  as  integrators.  These  findings  assist  OSS  projects  to  identify  potential  long-term  active  integrators  and  adopt  better  strategies  to  retain  them  in  the  evaluation  of  code  contributions.  Keywords-Long-term  active  integrator;  Code  contributions;  Open  source  software
1	Mutation  analysis  for  javascriptweb  application  testing.  When  developers  test  modern  web  applications  that  use  JavaScript,  challenging  issues  lie  in  their  event-driven,  asynchronous,  and  dynamic  features.  Many  researchers  have  assessed  the  adequacy  of  test  cases  with  code  coverage  criteria;  however,  in  this  paper,  we  show  that  the  code  coverage-based  approach  possibly  misses  some  faults  in  the  applications.  We  propose  a  mutation  analysis  approach  for  estimating  the  faultfinding  capability  of  test  cases.  We  assume  that  developers  can  find  overlooked  fault  instances  and  improve  the  test  cases  with  the  estimated  capability.  To  create  a  set  of  faulty  programs,  i.e.,  mutants,  we  classify  the  JavaScript  features  in  web  applications  and  then  define  a  comprehensive  set  of  mutation  operators.  We  conducted  a  case  study  on  a  real-world  application  and  found  that  our  approach  supported  the  improvement  of  test  cases  to  expose  hand-seeded  faults  by  an  extra  ten  percent.  Keywords-JavaScript;  Mutation  Analysis  and  Testing;  Web  Applications;  Test  Criteria
1	Multi  objective  optimization  for  software  testing  effort  estimation.  Software  Testing  Effort  (STE),  which  contributes  about  25-40%  of  the  total  development  effort,  plays  a  significant  role  in  software  development.  In  addressing  the  issues  faced  by  companies  in  finding  relevant  datasets  for  STE  estimation  modeling  prior  to  development,  cross-company  modeling  could  be  leveraged.  The  study  aims  at  assessing  the  effectiveness  of  cross-company  (CC)  and  within-company  (WC)  projects  in  STE  estimation.  A  robust  multi-objective  Mixed-Integer  Linear  Programming  (MILP)  optimization  framework  for  the  selection  of  CC  and  WC  projects  was  constructed  and  estimation  of  STE  was  done  using  Deep  Neural  Networks.  Results  from  our  study  indicate  that  the  application  of  the  MILP  framework  yielded  similar  results  for  both  WC  and  CC  modeling.  The  modeling  framework  will  serve  as  a  foundation  to  assist  in  STE  estimation  prior  to  the  development  of  new  a  software  project.
1	Mining  binary  constraints  in  the  construction  of  feature  models.  Feature  models  provide  an  effective  way  to  organize  and  reuse  requirements  in  a  specific  domain.  A  feature  model  consists  of  a  feature  tree  and  cross-tree  constraints.  Identifying  features  and  then  building  a  feature  tree  takes  a  lot  of  effort,  and  many  semi-automated  approaches  have  been  proposed  to  help  the  situation.  However,  finding  cross-tree  constraints  is  often  more  challenging  which  still  lacks  the  help  of  automation.  In  this  paper,  we  propose  an  approach  to  mining  cross-tree  binary  constraints  in  the  construction  of  feature  models.  Binary  constraints  are  the  most  basic  kind  of  cross-tree  constraints  that  involve  exactly  two  features  and  can  be  further  classified  into  two  sub-types,  i.e.  requires  and  excludes.  Given  these  two  sub-types,  a  pair  of  any  two  features  in  a  feature  model  falls  into  one  of  the  following  classes:  no  constraints  between  them,  a  requires  between  them,  or  an  excludes  between  them.  Therefore  we  perform  a  3-class  classification  on  feature  pairs  to  mine  binary  constraints  from  features.  We  incorporate  a  support  vector  machine  as  the  classifier  and  utilize  a  genetic  algorithm  to  optimize  it.  We  conduct  a  series  of  experiments  on  two  feature  models  constructed  by  third  parties,  to  evaluate  the  effectiveness  of  our  approach  under  different  conditions  that  might  occur  in  practical  use.  Results  show  that  we  can  mine  binary  constraints  at  a  high  recall  (near  100%  in  most  cases),  which  is  important  because  finding  a  missing  constraint  is  very  costly  in  real,  often  large,  feature  models.
1	Logical  structure  extraction  from  software  requirements  documents.  Software  requirements  documents  (SRDs)  are  often  authored  in  general-purpose  rich-text  editors,  such  as  MS  Word.  SRDs  contain  instances  of  logical  structures,  such  as  use  case,  business  rule,  and  functional  requirement.  Automated  recognition  and  extraction  of  these  instances  enables  advanced  requirements  management  features,  such  as  automated  traceability,  template  conformance  checking,  guided  editing,  and  interoperability  with  requirements  management  tools  such  as  RequisitePro.  The  variability  in  content  and  physical  representation  of  these  instances  poses  challenges  to  their  accurate  recognition  and  extraction.  To  address  these  challenges,  we  present  a  framework  allowing  1)  the  specification  of  logical  structures  in  terms  of  their  content,  textual  rendering,  and  variability  and  2)  the  extraction  of  instances  of  such  structures  from  rich-text  documents.  Our  evaluation  involves  36  different  logical  structures  identified  in  43  SRDs  and  shows  that  the  intended  content,  style,  and  variability  of  these  structures  can  be  specified  in  the  framework  such  that  their  instances  can  be  extracted  from  the  documents  with  high  precision  and  recall,  both  close  to  100%.
1	Irequire  gathering  end  user  requirements  for  new  apps.  Mobile  devices  such  as  Smartphones  and  Internet  Tablets  have  become  an  integral  part  of  our  life.  We  can  install  applications  providing  various  functionalities.  Our  research  focuses  on  an  application  which  enables  end-users  to  blog  requirements  in  situ.  The  gathered  end-user  needs  can  be  seen  as  a  starting  point  for  the  development  of  applications  and  the  evolution  of  mobile  platforms.
1	Reuse  of  architecturally  derived  standards  requirements.  Requirements  reuse  promises  to  reduce  product  development  cost  and  improve  product  quality.  Applying  a  standard  set  of  requirements  to  multiple  products  configured  into  the  same  system  can  ensure  all  the  products  take  advantage  of  the  system's  architectural  features  and  do  not  adversely  interact  with  each  other.  While  existing  literature  provides  guidance  for  developing  requirements  suitable  for  reuse,  little  has  been  written  on  the  practical  realities  an  organization  faces  in  attempting  to  reuse  requirements.  This  paper  addresses  that  gap  by  describing  a  commercial  engineering  company's  deployment  of  a  requirements  reuse  process,  the  problems  encountered,  the  results  obtained,  and  the  plans  for  future  improvement  of  the  process.
1	Experience  requirements  in  video  games  definition  and  testability.  A  properly  formed  requirement  is  testable,  a  necessity  for  ensuring  that  design  goals  are  met.  While  challenging  in  productivity  applications,  entertainment  applications  such  as  games  compound  the  problem  due  to  their  subjective  nature.  We  report  here  on  our  efforts  to  create  testable  experience  requirements,  the  associated  scope  challenges  and  challenges  with  test  design  and  result  interpretation.  We  further  report  on  issues  experienced  when  performing  focus  group  testing  and  provide  practitioner  guidance.
1	Ucanalyzer  a  tool  to  analyze  use  case  textual  descriptions.  Use  case  modeling  is  a  popular  and  widely  used  specification  documentation  strategy  that  facilitates  a  developer  to  specify  the  functional  requirements  of  a  software  system.  There  have  been  many  efforts  made  to  document  problem  specification  in  the  use  cases  by  employing  a  restricted  form  of  natural  language,  authoring  guidelines  and  checklist,  but  no  tool  support  is  available  to  assess  and  validate  their  quality.  In  this  paper,  we  present  a  tool,  UCAnalyzer,  to  analyze  use  case  textual  descriptions.  UCAnalyzer  has  three  key  modules:  (1)  a  use  case  textual  description  editor  module,  (2)  an  analysis  module  to  assess  the  quality  of  use  case  textual  description,  and  (3)  a  module  to  highlight  errors  and  provide  suggestions.
1	An  information  theoretic  approach  for  extracting  and  tracing  non  functional  requirements.  Non-functional  requirements  (NFRs)  are  high-level  quality  constraints  that  a  software  system  should  exhibit.  Detecting  such  constraints  early  in  the  process  is  critical  for  the  stability  of  software  architectural  design.  However,  due  to  their  pervasive  nature,  and  the  lack  of  robust  modeling  and  documentation  techniques,  NFRs  are  often  overlooked  during  the  requirements  elicitation  phase.  Realizing  such  constraints  at  later  stages  of  the  development  process  often  leads  to  architecture  erosion  and  poor  traceability.  Motivated  by  these  observations,  we  propose  an  unsupervised,  computationally  efficient,  and  scalable  approach  for  extracting  and  tracing  NFRs  in  software  systems.  Based  on  main  assumptions  of  the  cluster  hypothesis  and  information  theory,  the  proposed  approach  exploits  the  semantic  knowledge  embedded  in  the  textual  content  of  requirements  specifications  to  discover,  classify,  and  trace  high-level  software  quality  constraints  imposed  by  the  system's  functional  features.  Three  experimental  systems  are  used  to  conduct  the  experimental  analysis  in  this  paper.  Results  show  that  the  proposed  approach  can  discover  software  NFRs  with  an  average  accuracy  of  73%,  enabling  these  NFRs  to  be  traced  to  their  implementations  with  accuracy  levels  adequate  for  practical  applications.
1	Concern  driven  development  with  jucmnav.  The  User  Requirements  Notation  (URN)  enables  the  graphical  modeling  of  requirements  with  goals  and  scenarios,  and  jUCMNav  is  a  free,  Eclipse-based  tool  that  supports  modeling  and  analysis  with  URN.  Concern-Driven  Development  (CDD)  enables  requirements  engineers  to  encapsulate  and  reason  about  concerns,  whether  they  are  crosscutting  (i.e.,  aspects)  or  not.  However,  to  truly  capitalize  on  the  benefits  promised  by  CDD,  concerns  need  to  be  encapsulated  across  software  development  phases,  i.e.,  across  different  types  of  models  at  different  levels  of  abstraction.  Recently,  URN  was  extended  to  support  aspect-oriented  concepts.  This  demonstration  focuses  on  the  new  concern-driven  modeling  features  of  jUCMNav,  together  with  its  capabilities  to  compose  aspects  together,  and  to  transform  aspectual  scenario  models  into  design  models  in  the  Reusable  Aspect  Models  notation.  jUCMNav  is  hence  one  of  the  few  tools  that  enable  CDD  from  requirements  to  design.
1	An  approach  to  carry  out  consistency  analysis  on  requirements  validating  and  tracking  requirements  through  a  configuration  structure.  Requirements  management  and  traceability  have  always  been  one  of  grand  challenges  in  software  development  area.  Studies  reveal  that  30-40%  of  software  defects  can  be  traced  to  gaps  or  errors  in  requirements  Although  several  models  and  techniques  have  been  defined  to  optimize  the  requirements  process,  ensuring  alignment  and  consistency  of  elicited  requirements  continues  to  be  a  challenge.  All  software  engineering  standards  and  methodologies  recognize  the  importance  of  maintaining  relationships  among  the  software  elements  for  traceability.  We  have  leveraged  the  structured  relationships  among  the  requirement  elements  to  come  up  with  an  approach  to  systematically  carry  out  consistency  analysis  of  requirements  for  software  systems.  The  framework  has  multiple  models:  a  multi  layered  requirement  model,  a  configuration  structure  to  link  and  track  the  requirement  items,  a  consistency  analysis  method  to  identify  the  inconsistencies  in  the  requirements  and  a  consistency  index  computation  to  indicate  the  level  of  consistency  in  overall  requirements  of  the  software  system.  This  approach  helps  to  validate  the  requirements  from  both  completeness  and  correctness  perspectives  and  also  check  their  consistency  in  forward  and  backward  directions.  The  paper  outlines  the  framework,  describes  the  encompassing  models  and  the  implementation  details  from  pilot  of  the  framework  to  an  industry  case  study  along  with  results.
1	An  approach  for  decision  support  on  the  uncertainty  in  feature  model  evolution.  Software  systems  could  be  seen  as  a  hierarchy  of  features  which  are  evolving  due  to  the  dynamic  of  the  working  environments.  The  companies  who  build  software  thus  need  to  make  an  appropriate  strategy,  which  takes  into  consideration  of  such  dynamic,  to  select  features  to  be  implemented.  In  this  work,  we  propose  an  approach  to  facilitate  such  selection  by  providing  a  means  to  capture  the  uncertainty  of  evolution  in  feature  models.  We  also  provide  two  analyses  to  support  the  decision  makers.  The  approach  is  exemplified  in  the  Smart  Grid  scenario.
1	Tracebok  toward  a  software  requirements  traceability  body  of  knowledge.  This  paper  introduces  the  idea  of  building  a  body  of  knowledge  (BoK)  to  gather  the  better  practices  in  software  requirements  traceability  that  could  bring  major  benefits  for  analyzing,  managing,  and  implementing  software  changes  impact.  The  implementation  of  traceability  in  the  organizations  is  still  a  challenge  even  many  studies  have  been  conducted  on  the  subject.  The  aim  of  this  work  is  to  evaluate  traceability  approaches  focusing  on  software  product  traceability  by  a  systematic  review.  We  conducted  a  series  of  interviews  with  practitioners  to  complement  the  evaluation.  Based  on  this  study,  we  categorize  the  approaches,  and  we  propose  a  body  of  knowledge  (Bok)  on  traceability  requirements  TraceBoK.  This  BoK  intends  to  assist  software  engineers  to  decide  how  to  define  an  implementation  of  traceability  in  the  development  and  maintenance  of  software  products.  Experts  in  traceability  have  evaluated  this  BoK,  and  we  notice  that  it  meets  practitioners  needs.  However,  some  improvements  must  be  done  in  the  future  versions,  and  we  have  made  TraceBoK  available  at  tracebok.org.  We  hope  that  the  proposed  BoK  helps  practitioners  to  apply  better  and  understand  traceability  in  both  academic  and  industry  areas.
1	The  impact  of  domain  knowledge  on  the  effectiveness  of  requirements  idea  generation  during  requirements  elicitation.  It  is  believed  that  the  effectiveness  of  requirements  engineering  activities  depends  at  least  partially  on  the  individuals  involved.  One  of  the  factors  that  seems  to  influence  an  individual's  effectiveness  in  requirements  engineering  activities  is  knowledge  of  the  problem  being  solved,  i.e.,  domain  knowledge.  While  a  requirements  engineer's  having  in-depth  domain  knowledge  helps  him  or  her  to  understand  the  problem  easier,  he  or  she  can  fall  for  tacit  assumptions  of  the  domain  and  might  overlook  issues  that  are  obvious  to  domain  experts.  This  paper  describes  a  controlled  experiment  to  test  the  hypothesis  that  adding  to  a  requirements  elicitation  team  for  a  computer-based  system  in  a  particular  domain,  requirements  analysts  that  are  ignorant  of  the  domain  improves  the  effectiveness  of  the  requirements  elicitation  team.  The  results,  although  not  conclusive,  show  some  support  for  accepting  the  hypothesis.  The  results  were  analyzed  also  to  determine  the  effect  of  creativity,  industrial  experience,  and  requirements  engineering  experience.  The  results  suggest  other  hypotheses  to  be  studied  in  the  future.
1	App  review  analysis  via  active  learning  reducing  supervision  effort  without  compromising  classification  accuracy.  Automated  app  review  analysis  is  an  important  avenue  for  extracting  a  variety  of  requirements-related  information.  Typically,  a  first  step  toward  performing  such  analysis  is  preparing  a  training  dataset,  where  developers  (experts)  identify  a  set  of  reviews  and,  manually,  annotate  them  according  to  a  given  task.  Having  sufficiently  large  training  data  is  important  for  both  achieving  a  high  prediction  accuracy  and  avoiding  overfitting.  Given  millions  of  reviews,  preparing  a  training  set  is  laborious.  We  propose  to  incorporate  active  learning,  a  machine  learning  paradigm,  in  order  to  reduce  the  human  effort  involved  in  app  review  analysis.  Our  app  review  classification  framework  exploits  three  active  learning  strategies  based  on  uncertainty  sampling.  We  apply  these  strategies  to  an  existing  dataset  of  4,400  app  reviews  for  classifying  app  reviews  as  features,  bugs,  rating,  and  user  experience.  We  find  that  active  learning,  compared  to  a  training  dataset  chosen  randomly,  yields  a  significantly  higher  prediction  accuracy  under  multiple  scenarios.
1	Breaking  the  big  bang  practice  of  traceability  pushing  timely  trace  recommendations  to  project  stakeholders.  In  many  software  intensive  systems  traceability  is  used  to  support  a  variety  of  software  engineering  activities  such  as  impact  analysis,  compliance  verification,  and  requirements  validation.  However,  in  practice,  traceability  links  are  often  created  towards  the  end  of  the  project  specifically  for  approval  or  certification  purposes.  This  practice  can  result  in  inaccurate  and  incomplete  traces,  and  also  means  that  traceability  links  are  not  available  to  support  early  development  efforts.  We  address  these  problems  by  presenting  a  trace  recommender  system  which  pushes  recommendations  to  project  stakeholders  as  they  create  or  modify  traceable  artifacts.  We  also  introduce  the  novel  concept  of  a  trace  obligation,  which  is  used  to  track  satisfaction  relations  between  a  target  artifact  and  a  set  of  source  artifacts.  We  model  traceability  events  and  subsequent  actions,  including  user  recommendations,  using  the  Business  Process  Modeling  Notation  (BPMN).  We  demonstrate  and  evaluate  the  efficacy  of  our  approach  through  an  illustrative  example  and  a  simulation  conducted  using  the  software  engineering  artifacts  of  a  robotic  system  for  supporting  arm  rehabilitation.  Our  results  show  that  tracking  trace  obligations  and  generating  trace  recommendations  throughout  the  active  phases  of  a  project  can  lead  to  early  construction  of  traceability  knowledge.
1	Requirements  dependency  extraction  by  integrating  active  learning  with  ontology  based  retrieval.  Context:  Incomplete  or  incorrect  detection  of  requirement  dependencies  has  proven  to  result  in  reduced  release  quality  and  substantial  rework.  Additionally,  the  extraction  of  dependencies  is  challenging  since  requirements  are  mostly  documented  in  natural  language,  which  makes  it  a  cognitively  difficult  task.  Moreover,  with  ever-changing  and  new  requirements,  a  manual  analysis  process  must  be  repeated,  which  imposes  extra  hardship  even  for  domain  experts.Objective:  The  three  main  objectives  of  this  research  are:  1)  Proposing  a  new  dependency  extraction  method  using  a  variant  of  Active  Learning  (AL).  2)  Evaluating  this  AL  and  Ontology-based  Retrieval  (OBR)  as  baseline  methods  for  dependency  extraction  on  the  two  industrial  data  sets.  3)  Analyzing  the  value  gained  from  integrating  these  diverse  approaches  to  form  two  hybrid  methods.Method:  Building  on  the  general  AL,  ensemble  and  semi-supervised  machine  learning,  a  variant  of  AL  was  developed,  which  was  further  integrated  with  OBR  to  form  two  hybrid  methods  (Hybrid1,  Hybrid2)  for  extracting  three  types  of  dependencies  (requires,  refines,  other):  Hybrid1  used  OBR  as  a  substitute  for  human  expert;  Hybrid2  used  dependencies  extracted  through  the  OBR  as  an  additional  input  for  training  set  in  AL.Results:  For  two  industrial  case  studies,  AL  extracted  more  dependencies  than  OBR.  Hybrid1  showed  improvement  for  both  data  sets.  For  one  of  them,  F1  score  increased  to  82.6%  compared  to  the  AL  baseline  score  of  49.9%.  Hybrid2  increased  the  accuracy  by  25%  to  the  level  of  75.8%  compared  to  the  AL  baseline  accuracy.  OBR  also  complemented  the  AL  approach  by  reducing  50%  of  the  human  effort.
1	Rationalization  of  goal  models  in  grl  using  formal  argumentation.  We  apply  an  existing  formal  framework  for  practical  reasoning  with  arguments  and  evidence  to  the  Goal-oriented  Requirements  Language  (GRL),  which  is  part  of  the  User  Requirements  Notation  (URN).  This  formal  framework  serves  as  a  rationalization  for  elements  in  a  GRL  model:  using  attack  relations  between  arguments  we  can  automatically  compute  the  acceptability  status  of  elements  in  a  GRL  model,  based  on  the  acceptability  status  of  their  underlying  arguments  and  the  evidence.  We  integrate  the  formal  framework  into  the  GRL  metamodel  and  we  set  out  a  research  to  further  develop  this  framework.
1	Managing  changing  compliance  requirements  by  predicting  regulatory  evolution.  Over  time,  laws  change  to  meet  evolving  social  needs.  Requirements  engineers  that  develop  software  for  regulated  domains,  such  as  healthcare  or  finance,  must  adapt  their  software  as  laws  change  to  maintain  legal  compliance.  In  the  United  States,  regulatory  agencies  will  almost  always  release  a  proposed  regulation,  or  rule,  and  accept  comments  from  the  public.  The  agency  then  considers  these  comments  when  drafting  a  final  rule  that  will  be  binding  on  the  regulated  domain.  Herein,  we  examine  how  these  proposed  rules  evolve  into  final  rules,  and  propose  an  Adaptability  Framework.  This  framework  can  aid  software  engineers  in  predicting  what  areas  of  a  proposed  rule  are  most  likely  to  evolve,  allowing  engineers  to  begin  building  towards  the  more  stable  sections  of  the  rule.  We  develop  the  framework  through  a  formative  study  using  the  Health  Insurance  Portability  and  Accountability  (HIPAA)  Security  Rule  and  apply  it  in  a  summative  study  on  the  Health  Information  Technology:  Initial  Set  of  Standards,  Implementation  Specifications,  and  Certification  Criteria  for  Electronic  Health  Record  Technology.
1	Quality  requirements  elicitation  based  on  inquiry  of  quality  impact  relationships.  Quality  requirements,  an  important  class  of  non  functional  requirements,  are  inherently  difficult  to  elicit.  Particularly  challenging  is  the  definition  of  good-enough  quality.  The  problem  cannot  be  avoided  though,  because  hitting  the  right  quality  level  is  critical.  Too  little  quality  leads  to  churn  for  the  software  product.  Excessive  quality  generates  unnecessary  cost  and  drains  the  resources  of  the  operating  platform.  To  address  this  problem,  we  propose  to  elicit  the  specific  relationships  between  software  quality  levels  and  their  impacts  for  given  quality  attributes  and  stakeholders.  An  understanding  of  each  such  relationship  can  then  be  used  to  specify  the  right  level  of  quality  by  deciding  about  acceptable  impacts.  The  quality-impact  relationships  can  be  used  to  design  and  dimension  a  software  system  appropriately  and,  in  a  second  step,  to  develop  service  level  agreements  that  allow  re-use  of  the  obtained  knowledge  of  good-enough  quality.  This  paper  describes  an  approach  to  elicit  such  quality-impact  relationships  and  to  use  them  for  specifying  quality  requirements.  The  approach  has  been  applied  with  user  representatives  in  requirements  workshops  and  used  for  determining  Quality  of  Service  (QoS)  requirements  based  the  involved  users’  Quality  of  Experience  (QoE).  The  paper  describes  the  approach  in  detail  and  reports  early  experiences  from  applying  the  approach.  Index  Terms-Requirement  elicitation,  quality  attributes,  non-functional  requirements,  quality  of  experience  (QoE),  quality  of  service  (QoS).
1	How  practitioners  approach  gameplay  requirements  an  exploration  into  the  context  of  massive  multiplayer  online  role  playing  games.  Gameplay  requirements  are  central  to  game  development.  In  the  business  context  of  massive  multiplayer  online  role-playing  games  (MMOGs)  where  game  companies'  revenues  rely  on  players'  monthly  subscriptions,  gameplay  is  also  recognized  as  the  key  to  player  retention.  However,  information  on  what  gameplay  requirements  are  and  how  practitioners  `engineer'  them  in  real  life  is  scarce.  This  exploratory  study  investigates  how  practitioners  developing  MMOGs  reason  about  gameplay  requirements  and  handle  them  in  their  projects.  12  practitioners  from  three  leading  MMOGs-producing  companies  were  interviewed  and  their  gameplay  requirements  documents  were  reviewed.  The  study's  most  important  findings  are  that  in  MMOG  projects:  (1)  gameplay  requirements  are  co-created  with  players,  (2)  are  perceived  and  treated  by  practitioners  as  sets  of  choices  and  consequences,  (3)  gameplay  is  endless  within  a  MMOG,  and  while  gameplay  requirements  do  not  support  any  game-end  goal,  they  do  support  a  level-end  goal,  (4)  `paper-prototyping'  and  play-testing  are  pivotal  to  gameplay  validation,  (5)  balancing  the  elements  of  the  gameplay  is  an  on-going  task,  perceived  as  the  most  difficult  and  labor-consuming,  (6)  gameplay  happens  both  in-game  and  out-of-the  game.  We  conclude  with  discussion  on  validity  threats  to  our  results  and  on  implications  for  research  and  practice.
1	Vetting  automatically  generated  trace  links  what  information  is  useful  to  human  analysts.  Automated  traceability  has  been  investigated  for  over  a  decade  with  promising  results.  However,  a  human  analyst  is  needed  to  vet  the  generated  trace  links  to  ensure  their  quality.  The  process  of  vetting  trace  links  is  not  trivial  and  while  previous  studies  have  analyzed  the  performance  of  the  human  analyst,  they  have  not  focused  on  the  analyst's  information  needs.  The  aim  of  this  study  is  to  investigate  what  context  information  the  human  analyst  needs.  We  used  design  science  research,  in  which  we  conducted  interviews  with  ten  practitioners  in  the  traceability  area  to  understand  the  information  needed  by  human  analysts.  We  then  compared  the  information  collected  from  the  interviews  with  existing  literature.  We  created  a  prototype  tool  that  presents  this  information  to  the  human  analyst.  To  further  understand  the  role  of  context  information,  we  conducted  a  controlled  experiment  with  33  participants.  Our  interviews  reveal  that  human  analysts  need  information  from  three  different  sources:  1)  from  the  artifacts  connected  by  the  link,  2)  from  the  traceability  information  model,  and  3)  from  the  tracing  algorithm.  The  experiment  results  show  that  the  content  of  the  connected  artifacts  is  more  useful  to  the  analyst  than  the  contextual  information  of  the  artifacts.
1	Towards  feature  oriented  variability  reconfiguration  in  dynamic  software  product  lines.  Dynamic  Software  Product  Line  (DSPL)  provides  a  new  paradigm  for  developing  self-adaptive  systems  with  the  principles  of  software  product  line  engineering.  DSPL  emphasizes  variability  analysis  and  design  at  development  time  and  variability  binding  and  reconfiguration  at  runtime,  thus  requires  some  kinds  of  variability  mechanisms  to  map  high-level  variations  (usually  represented  by  features)  to  low-level  implementation  and  support  runtime  reconfiguration.  Existing  work  on  DSPL  usually  assumes  that  variation  features  can  be  directly  mapped  to  coarse-grained  elements  like  services,  components  or  plug-ins,  making  the  methods  hard  to  be  applied  for  traditional  software  systems.  In  this  paper,  we  propose  a  feature-oriented  method  to  support  runtime  variability  reconfiguration  in  DSPLs.  The  method  introduces  the  concept  of  role  model,  an  intermediate  level  between  feature  variations  and  implementations  to  improve  their  traceability.  On  the  other  hand,  the  method  involves  a  reference  implementation  framework  based  on  dynamic  aspect  mechanisms  to  implement  the  runtime  reconfiguration.  We  illustrate  the  process  of  applying  the  proposed  method  with  a  concrete  case  study,  which  helps  to  validate  the  effectiveness  of  our  method.
1	Reverse  engineering  reusable  language  modules  from  legacy  domain  specific  languages.  The  use  of  domain-specific  languages  DSLs  has  become  a  successful  technique  in  the  development  of  complex  systems.  Nevertheless,  the  construction  of  this  type  of  languages  is  time-consuming  and  requires  highly-specialized  knowledge  and  skills.  An  emerging  practice  to  facilitate  this  task  is  to  enable  reuse  through  the  definition  of  language  modules  which  can  be  later  put  together  to  build  up  new  DSLs.  Still,  the  identification  and  definition  of  language  modules  are  complex  and  error-prone  activities,  thus  hindering  the  reuse  exploitation  when  developing  DSLs.  In  this  paper,  we  propose  a  computer-aided  approach  to  i  identify  potential  reuse  in  a  set  of  legacy  DSLs;  and  ii  capitalize  such  potential  reuse  by  extracting  a  set  of  reusable  language  modules  with  well  defined  interfaces  that  facilitate  their  assembly.  We  validate  our  approach  by  using  realistic  DSLs  coming  out  from  industrial  case  studies  and  obtained  from  public  GitHub  repositories.
1	Achieving  reuse  with  pluggable  software  units.  In  this  paper  we  present  a  solution  to  software  reuse  based  on  Pluggable  Units  (PUs)  that  can  be  used  to  compose  new  applications  from  existing  parts.  Although  this  goal  has  been  achieved  in  hardware  design  through  the  creation  of  integrated  circuits  (ICs),  the  attempts  to  build  a  software  equivalent  were  not  fully  successful.  Pluggable  units  are  a  full  fledged  software  implementation  of  the  IC  concept  while  providing  new  features  not  existing  in  hardware,  namely  the  ability  to  compose  software  hierarchically.  An  application  example  is  provided  in  JUse,  a  new  Java-based  language  supporting  pluggable  units  and  in  JWIDGET,  a  pluggable  version  of  Java/Swing.
1	Evaluating  lehman  s  laws  of  software  evolution  within  software  product  lines  a  preliminary  empirical  study.  The  evolution  of  a  single  system  is  a  task  where  we  deal  with  the  modification  of  a  single  product.  Lehman’s  laws  of  software  evolution  were  broadly  evaluated  within  this  type  of  systems  and  the  results  shown  that  these  single  systems  evolve  according  to  his  stated  laws  over  time.  However,  when  dealing  with  Software  Product  Lines  (SPL),  we  need  to  deal  with  the  modification  of  several  products  which  include  common,  variable  and  product  specific  assets.  Because  of  the  several  assets  within  SPL,  each  stated  law  may  have  a  different  behavior  for  each  asset  kind.  Nonetheless,  we  do  not  know  if  the  stated  laws  are  still  valid  for  SPL  since  they  were  not  yet  evaluated  in  this  context.  Thus,  this  paper  details  an  empirical  investigation  where  four  of  the  Lehman’s  Laws  (LL)  of  Software  Evolution  were  used  in  an  SPL  industrial  project  to  understand  how  the  SPL  assets  evolve  over  time.  This  project  relates  to  an  application  in  the  medical  domain  developed  in  a  medium-size  company  in  Brazil.  It  contains  45  modules  and  a  total  of  70.652  bug  requests  in  the  tracking  system,  gathered  along  the  past  10  years.  We  employed  two  techniques  -  the  KPSS  Test  and  linear  regression  analysis,  to  assess  the  relationship  between  LL  and  SPL  assets.  Finally,  results  showed  that  three  laws  were  supported  based  on  the  data  employed  (continuous  change,  increasing  complexity,  and  declining  quality).  The  other  law  (continuing  growth)  was  partly  supported,  depending  on  the  SPL  evaluated  asset  (common,  variable  or  product-specific).
1	Documentation  reuse  hot  or  not  an  empirical  study.  Having  available  a  high  quality  documentation  is  critical  for  software  projects.  This  is  why  documentation  tools  such  as  Javadoc  are  so  popular.  As  for  code,  documentation  should  be  reused  when  possible  to  increase  developer  productivity  and  simplify  maintenance.  In  this  paper,  we  perform  an  empirical  study  of  duplications  in  JavaDoc  documentation  on  a  corpus  of  seven  famous  Java  APIs.  Our  results  show  that  copy-pastes  of  JavaDoc  documentation  tags  are  abundant  in  our  corpus.  We  also  show  that  these  copy-pastes  are  caused  by  four  different  kinds  of  relations  in  the  underlying  source  code.  In  addition,  we  show  that  popular  documentation  tools  do  not  provide  any  reuse  mechanism  to  cope  with  these  relations.  Finally,  we  make  a  proposal  for  a  simple  but  efficient  automatic  reuse  mechanism.
1	A  language  for  building  verified  software  components.  Safe  and  secure  reuse  is  only  achievable  by  deploying  formally  verified  software  components.  This  paper  presents  essential  design  objectives  for  languages  for  building  such  components  and  highlights  key  features  in  RESOLVE—a  prototype  for  such  languages.  It  explains  why  the  language  must  include  specifications  as  an  integral  constituent  and  must  have  clean  and  rich  semantics,  which  preclude  unconstrained  aliasing  and  other  unwanted  side-effects.  In  order  that  arbitrarily  complex  components  can  be  given  concise  and  verification  amenable  specifications,  an  adequate  language  must  also  include  an  open-ended  mechanism  for  incorporating  additional  mathematical  theories.  Given  these  essential  characteristics,  safe  and  secure  reuse  cannot  be  attained  within  popular  languages,  such  as  C++  or  Java,  either  by  constraining  them  or  by  extending  them.  Better  languages  are  necessary.
1	Finding  errors  in  python  programs  using  dynamic  symbolic  execution.  For  statically  typed  languages,  dynamic  symbolic  execution  (also  called  concolic  testing)  is  a  mature  approach  to  automated  test  generation.  However,  extending  it  to  dynamic  languages  presents  several  challenges.  Complex  semantics,  fragmented  and  incomplete  type  information,  and  calls  to  foreign  functions  lacking  precise  models  make  symbolic  execution  difficult.  We  propose  a  symbolic  execution  approach  that  mixes  concrete  and  symbolic  values  and  incrementally  solves  path  constraints  in  search  for  alternate  executions  by  lazily  instantiating  axiomatizations  for  called  functions  as  needed.  We  present  the  symbolic  execution  model  underlying  this  approach  and  illustrate  the  workings  of  our  prototype  concolic  testing  tool  on  an  actual  Python  software  package.
1	Inspecting  code  churns  to  prioritize  test  cases.  Within  the  context  of  software  evolution,  due  to  time-to-market  pressure,  it  is  not  uncommon  that  a  company  has  not  enough  time  and/or  resources  to  re-execute  the  whole  test  suite  on  the  new  software  version,  to  check  for  non-regression.  To  face  this  issue,  many  Regression  Test  Prioritization  techniques  have  been  proposed,  aimed  at  ranking  test  cases  in  a  way  that  tests  more  likely  to  expose  faults  have  higher  priority.  Some  of  these  techniques  exploit  code  churn  metrics,  i.e.  some  quantification  of  code  changes  between  two  subsequent  versions  of  a  software  artifact,  which  have  been  proven  to  be  effective  indicators  of  defect-prone  components.  In  this  paper,  we  first  present  three  new  Regression  Test  Prioritization  strategies,  based  on  a  novel  code  churn  metric,  that  we  empirically  assessed  on  an  open  source  software  system.  Results  highlighted  that  the  proposal  is  promising,  but  that  it  might  be  further  improved  by  a  more  detailed  analysis  on  the  nature  of  the  changes  introduced  between  two  subsequent  code  versions.  To  this  aim,  in  this  paper  we  also  sketch  a  more  refined  approach  we  are  currently  investigating,  that  quantifies  changes  in  a  code  base  at  a  finer  grained  level.  Intuitively,  we  seek  to  prioritize  tests  that  stress  more  fault-prone  changes  (e.g.,  structural  changes  in  the  control  flow),  w.r.t.  those  that  are  less  likely  to  introduce  errors  (e.g.,  the  renaming  of  a  variable).  To  do  so,  we  propose  the  exploitation  of  the  Abstract  Syntax  Tree  (AST)  representation  of  source  code,  and  to  quantify  differences  between  ASTs  by  means  of  specifically  designed  Tree  Kernel  functions,  a  type  of  similarity  measure  for  tree-based  data  structures,  which  have  shown  to  be  very  effective  in  other  domains,  thanks  to  their  customizability.
1	A  survey  on  testing  for  cyber  physical  system.  Cyber  Physical  Systems  CPS  bridge  the  cyber-world  of  computing  and  communications  with  the  physical  world  and  require  development  of  secure  and  reliable  software.  It  asserts  a  big  challenge  not  only  on  testing  and  verifying  the  correctness  of  all  physical  and  cyber  components  of  such  big  systems,  but  also  on  integration  of  these  components.  This  paper  develops  a  categorization  of  multiple  levels  of  testing  required  to  test  CPS  and  makes  a  comparison  of  these  levels  with  the  levels  of  software  testing  based  on  the  V-model.  It  presents  a  detailed  state-of-the-art  survey  on  the  testing  approaches  performed  on  the  CPS.  Further,  it  provides  challenges  in  CPS  testing.
1	The  distributed  gauss  newton  methods  for  solving  the  inverse  of  approximated  hessian  with  application  to  target  localization.  Gauss-Newton  method  is  a  Hessian  free  second-order  Newton's  optimization  method,  which  can  be  used  to  solve  the  common  nonlinear  least  squares  (NLLS)  problems.  In  this  paper,  we  consider  the  collaborative  target  localization  problem  in  a  wireless  sensor  network  with  a  random  topology  structure.  By  using  a  specialized  decomposition  technique,  the  centralized  Gauss-Newton  method  is  modified  as  a  distributed  solution  by  exchanging  locally  information  with  the  neighboring  anchor  nodes.  In  the  recent  diffusion  Gauss-Newton  algorithm,  the  main  difficulty  is  the  computation  of  inverse  because  the  approximated  Hessian  is  not  always  full  column  rank.  To  address  this  problem,  we  propose  that  two  methods  can  be  used  to  approximate  the  inverse  matrix,  i.e.,  Levenberg-Marquardt  parameter  and  Moore-Penrose  inverse  methods.  The  former  in  essence  is  a  damped  iteration  method,  while  the  latter  depends  largely  on  the  adjustment  of  step  size.  In  addition  to  proposing  the  methods  above  for  diffusion  Gauss-Newton  algorithm,  we  also  compare  their  performance  to  solve  the  collaborative  localization  in  wireless  sensor  networks,  which  can  be  modeled  as  a  NLLS  problem.  Simulation  results  show  both  of  them  are  applicable  to  diffusion  GN,  and  LM  method  has  some  slight  advantages  in  these  respects  such  as  resistance  to  environment  noise  while  guaranteeing  convergence,  and  dynamical  increasing/decreasing  to  step  size.
1	Human  speech  and  facial  emotion  recognition  technique  using  svm.  Human  Speech  and  Facial  are  the  most  significant  information  carriers  for  human  cognitive-communication  and  recognizing  human’s  identity  and  emotional  status.  With  the  further  growth  of  computer  processing  capability  and  the  increase  of  demand  for  intelligent  living,  recognition  of  emotion  based  on  face  and  speech  became  the  most  significant  in  the  applications  of  Human-Computer  Interaction  (HCI).  In  this  paper,  Human  Speech  and  Facial  based  emotion  recognition  technique  using  a  support  vector  machine  (SVM)  has  been  proposed  for  improving  the  performance  of  detection  with  multi-emotions  effectively.  The  obtained  results  of  the  proposed  technique  show  that  the  average  rate  of  recognition  is  higher  than  other  recently  existing  techniques,  and  the  obtained  accuracy  is  92.88%  for  facial  model  and  85.72  %  for  speech  model  with  low  time-consuming.
1	Improving  arrival  time  prediction  of  thailand  s  passenger  trains  using  historical  travel  times.  The  State  Railway  of  Thailand  provides  passengers  with  train  location  information  on  their  Web  site,  which  includes  the  name  of  the  last  station  that  each  train  arrives  at  or  departs  from,  along  with  the  timestamps  and  the  accumulative  train  delay  (in  minutes)  from  the  train  timetable.  This  information  allows  passengers  to  intuitively  predict  the  arrival  time  at  their  station  by  adding  the  last  known  train  delay  to  the  scheduled  arrival  time.  This  paper  aims  at  providing  a  more  accurate  prediction  of  passenger  train's  arrival  times  using  the  historical  travel  times  between  train  stations.  Two  algorithms  that  use  train  location  information  and  historical  travel  times  are  proposed  and  evaluated.  The  first  algorithm  uses  the  moving  average  of  historical  travel  times.  The  second  algorithm  utilizes  the  travel  times  of  the  k-nearest  neighbors  (k-NN)  of  the  last  known  arrival  time.  To  evaluate  the  proposed  algorithms,  we  collected  six  months  of  data  for  three  different  trains  and  calculated  prediction  errors  using  mean  absolute  error  (MAE).  The  prediction  errors  of  the  proposed  algorithms  are  compared  to  the  prediction  errors  of  the  baseline  algorithm  that  predicts  the  arrival  time  by  adding  the  last  known  train  delay  to  the  scheduled  train  arrival  time.  Both  algorithms  outperform  the  baseline  prediction.  The  algorithm  based  on  moving  average  travel  time  improves  the  prediction  error  by  22.9  percent  on  average,  and  the  algorithm  based  on  k-NN  improves  the  prediction  error  by  23.0  percent  on  average  (k=16).
1	Evaluating  normalization  functions  with  search  algorithms  for  solving  ocl  constraints.  The  use  of  search  algorithms  requires  the  definition  of  a  fitness  function  that  guides  the  algorithms  to  find  an  optimal  solution.  The  definition  of  a  fitness  function  may  require  the  use  of  a  normalization  function  for  various  purposes  such  as  assigning  equal  importance  to  various  factors  constituting  a  fitness  function  and  normalizing  only  one  factor  of  a  fitness  function  to  give  it  less/more  importance  than  the  others.  In  our  previous  work,  we  defined  various  branch  distance  functions  (a  commonly  used  heuristic  in  the  literature  at  the  code-level)  corresponding  to  the  constructs  defined  in  the  Object  Constraint  Language  (OCL)  to  solve  OCL  constraints  to  generate  test  data  for  supporting  automated  Model-Based  Testing  (MBT).  The  definition  of  several  of  these  distance  functions  required  the  use  of  a  normalization  function.  In  this  paper,  we  extend  the  empirical  evaluation  reported  in  one  of  the  works  in  the  literature  that  compares  the  impact  of  using  various  normalization  functions  for  calculating  branch  distances  at  the  code-level  on  the  performance  of  search  algorithms.    The  empirical  evaluation  reported  in  this  paper  assesses  the  impact  of  the  commonly  used  normalization  functions  for  the  branch  distance  calculation  of  OCL  constraints  at  the  model-level.  Results  show  that  for  one  of  the  newly  studied  algorithms  Harmony  Search  (HS)  and  Random  Search  (RS),  the  use  of  the  normalization  functions  has  no  impact  on  the  performance  of  the  search.  However,  HS  achieved  100%  success  rates  for  all  the  problems,  where  RS  obtained  very  poor  success  rates  (less  than  38%).  Based  on  the  results,  we  conclude  that  the  randomness  in  creating  a  solution  in  search  algorithms  may  mask  the  impact  of  using  a  normalization  function.
1	Association  rule  mining  for  finding  usability  problem  patterns  a  case  study  on  stackoverflow.  Usually,  developers  suffer  from  usability  related  problems  during  working  with  software  development  tools.  Such  problems  should  be  detected  to  improve  developer  team  performance.  To  tackle  this  problem,  pattern  discovery  techniques  can  be  used  to  locate  usability  problems  by  analyzing  the  feedback  of  users  extracted  from  software  repositories.  In  this  paper,  a  comprehensive  data  analysis  methodology  is  presented  to  extract  hidden  knowledge  for  acquiring  user  challenges  regarding  tools  interaction.  The  main  motivation  of  this  paper  is  to  involve  the  role  of  knowledge  in  software  development  process  such  as  Agile  development.  Rich  user  feedback  datasets  from  StackOverflow  programming  Question  and  Answer  (Q&A)  repository  have  been  used  as  the  input  of  Apriori  algorithm  while  required  preprocessing  has  been  considered.  The  generated  results  are  association  rules  representing  the  usability  problem  patterns  among  tools  and  technologies  interactions.  The  results  can  also  be  used  for  effort  planning  when  a  software  upgrade  needs  to  be  considered.
1	Scalable  practical  byzantine  fault  tolerance  with  short  lived  signature  schemes.  The  Practical  Byzantine  Fault  Tolerance  (PBFT)  algorithm  is  a  popular  solution  for  establishing  consensus  in  blockchain  systems.  The  execution  time  of  the  PBFT  consensus  algorithm  has  an  important  effect  on  the  blockchain  throughput.  Digital  signatures  are  extensively  used  in  PBFT  to  ensure  the  authenticity  of  messages  during  the  different  phases.  Due  to  the  round-based  and  broadcast  natures  of  PBFT,  nodes  need  to  verify  multiple  signatures  received  from  their  peers,  which  incurs  significant  computational  overhead  and  slows  down  the  consensus  process.  To  address  this  issue,  we  propose  an  efficient  short-lived  signature  based  PBFT  variant,  which  utilizes  short-length  cryptographic  keys  to  sign/verify  messages  in  PBFT  for  a  short  period  of  time  and  blockchain-aided  key  distribution  mechanisms  to  update  those  keys  periodically.  We  also  present  efficient  algorithms  for  accelerating  the  software  implementation  of  the  BLS  threshold  signature  scheme.  Our  extensive  experiments  with  three  elliptic  curves  and  two  signature  schemes  demonstrate  the  efficacy  of  using  short-lived  signature  schemes  for  improving  the  scalability  of  PBFT  significantly.
1	Survey  of  swarm  intelligence  algorithms.  Swarm  Intelligence  (SI)  is  an  AI  technique  that  has  the  collective  behavior  of  a  decentralized,  self-organized  system.  SI  has  more  advantages  such  as  scalability,  adaptability,  collective  robustness  and  individual  simplicity  and  also  has  the  ability  to  solve  complex  problems.  Besides,  SI  algorithms  also  have  few  issues  in  time-critical  applications,  parameter  tuning,  and  stagnation.  SI  algorithms  need  to  be  studied  more  to  overcome  these  kinds  of  issues.  In  this  paper,  we  studied  a  few  popular  algorithms  in  detail  to  identify  important  control  parameters  and  randomized  distribution.  We  also  studied  and  summarized  the  performance  comparison  of  SI  algorithms  in  different  applications.
1	The  impact  of  design  and  uml  modeling  on  codebase  quality  and  sustainability.  The  general  consensus  of  researchers  and  practitioners  is  that  up-front  and  continuous  software  design  using  modeling  languages  such  as  UML  improve  code  quality  and  reliability  particularly  as  the  software  evolves  over  time.  Software  designs  and  models  help  in  managing  the  underlying  code  complexities  which  are  crucial  for  sustainability.  Recently,  there  has  been  increasing  evidence  suggesting  broader  adoption  of  modeling  languages  such  as  UML.  However,  our  understanding  of  the  impact  of  using  such  modeling  and  design  languages  remains  limited.  This  paper  reports  on  a  study  that  aims  to  characterize  this  impact  on  code  quality  and  sustainability.  We  identify  a  sample  of  open  source  software  repositories  with  extensive  use  of  designs  and  modeling  and  compare  their  code  qualities  with  similar  code-centric  repositories.  Our  evaluation  focuses  on  various  code  quality  attributes  such  as  code  smells  and  technical  debt.  We  also  conduct  code  evolution  analysis  over  five-year  period  and  collect  additional  data  from  questionnaires  and  interviews  with  active  repository  contributors.  This  study  finds  that  repositories  with  significant  use  of  models  and  design  activities  are  associated  with  reduced  critical  code  smells  but  are  also  associated  with  increase  in  non-critical  code  smells.  The  study  also  finds  that  modeling  and  design  activities  are  associated  with  significant  reduction  in  measures  of  technical  debt.  Analyzing  code  evolution  over  five  year  period  reveals  that  UML  repositories  start  with  significantly  lower  technical  debt  density  measures  but  tend  to  decline  over  time.
1	A  predictive  analytics  approach  in  determining  the  predictors  of  student  attrition  in  the  higher  education  institutions  in  the  philippines.  The  paper  identified  the  predictors  of  student  attrition  in  the  Higher  Education  Institution  (HEI)  through  predictive  analytics  approach.  The  prediction  model  used  in  the  study  includes  variable  optimization  through  Genetic  Algorithm  (GA)  and  decision  tree  generation  phase  through  C4.5  algorithm.  The  college  student  leavers'  data  from  one  of  the  Higher  Education  in  the  Philippines  from  the  school  year  2008-2009  until  the  school  year  2018-2019  was  used  as  datasets  of  the  study.  Out  of  forty  identified  reasons  for  leaving  as  variables,  there  were  nine  (9)  identified  predictors  of  student  attrition.  Through  the  identified  predictors,  administrators  of  educational  institutions  may  design  intervention  plans  related  to  the  student  attrition.
1	Rgb  color  image  encryption  based  on  choquet  fuzzy  integral.  HighlightsThis  research  presents  a  new  image  encryption  scheme  using  the  Choquet  fuzzy  integral  based  keystream  generator.The  properties  of  the  dynamical  keystream  generator  with  mathematical  analysis  are  proved  rigorously.Experimental  results  prove  the  security  and  the  validity  of  the  proposed  algorithm.  In  recent  years,  one  can  see  an  increasing  interest  in  the  security  of  digital  images.  This  research  presents  a  new  RGB  color  image  encryption  using  keystream  generator  based  on  Choquet  fuzzy  integral  (CFI).  The  properties  of  the  dynamical  keystream  generator  with  mathematical  analysis  are  presented  in  this  work.  In  the  proposed  method,  the  CFI  is  first  used  to  generate  pseudo-random  keystreams.  Then,  each  of  the  color  pixels  is  decomposed  into  three  gray-level  components.  The  output  of  the  CFI  is  used  to  randomly  shift  the  bits  of  three  gray-level  components.  Finally,  three  components  of  RGB  color  pixels  and  the  generated  keystream  are  coupled  to  encrypt  the  permuted  components.  Performance  aspects  of  the  proposed  algorithm  such  as  the  entropy  analysis,  differential  analysis,  statistical  analysis,  cipher  random  analysis,  and  cipher  sensitivity  analysis  are  introduced  to  evaluate  the  security  of  the  new  scheme.  The  experimental  results  reveal  the  fact  that  the  proposed  algorithm  is  suitable  for  practical  use  in  protecting  the  security  of  digital  image  information  distributed  via  the  Internet.
1	Function  as  a  service  performance  evaluation  a  multivocal  literature  review.  Abstract  Function-as-a-Service  (FaaS)  is  one  form  of  the  serverless  cloud  computing  paradigm  and  is  defined  through  FaaS  platforms  (e.g.,  AWS  Lambda)  executing  event-triggered  code  snippets  (i.e.,  functions).  Many  studies  that  empirically  evaluate  the  performance  of  such  FaaS  platforms  have  started  to  appear  but  we  are  currently  lacking  a  comprehensive  understanding  of  the  overall  domain.  To  address  this  gap,  we  conducted  a  multivocal  literature  review  (MLR)  covering  112  studies  from  academic  (51)  and  grey  (61)  literature.  We  find  that  existing  work  mainly  studies  the  AWS  Lambda  platform  and  focuses  on  micro-benchmarks  using  simple  functions  to  measure  CPU  speed  and  FaaS  platform  overhead  (i.e.,  container  cold  starts).  Further,  we  discover  a  mismatch  between  academic  and  industrial  sources  on  tested  platform  configurations,  find  that  function  triggers  remain  insufficiently  studied,  and  identify  HTTP  API  gateways  and  cloud  storages  as  the  most  used  external  service  integrations.  Following  existing  guidelines  on  experimentation  in  cloud  systems,  we  discover  many  flaws  threatening  the  reproducibility  of  experiments  presented  in  the  surveyed  studies.  We  conclude  with  a  discussion  of  gaps  in  literature  and  highlight  methodological  suggestions  that  may  serve  to  improve  future  FaaS  performance  evaluation  studies.
1	Diversified  keyword  search  based  web  service  composition.  Abstract  To  assist  system  engineers  in  efficiently  building  service-based  software  systems,  the  keyword  search  based  service  composition  approach  on  service  connection  graphs  (scgraphs)  has  been  proposed  recently.  However,  due  to  the  ambiguity  of  keywords,  a  keyword  query  may  represent  a  bunch  of  different  user  requirements.  Thus  the  current  approach  that  only  returns  the  composition  with  the  optimal  Quality  of  Service  (QoS)  cannot  guarantee  to  hit  the  spot.  In  this  paper,  in  order  to  satisfy  the  various  possible  requirements  underlying  a  given  keyword  query,  we  formally  introduce  the  top-k  diverse  service  composition  problem,  and  present  a  novel  diversified  keyword  search  approach  on  scgraphs  to  address  it.  Specifically,  we  firstly  propose  an  All-Then-Diversify  (ATD)  algorithm  that  enumerates  all  potential  compositions  by  searching  a  scgraph  and  then  derives  the  top-k  diverse  subsets  by  deriving  the  maximal  independent  sets  of  a  similarity  graph.  Then,  due  to  the  possibly  large  number  of  potential  compositions,  we  present  a  Pop-And-Diversify  (PAD)  algorithm  that  only  maintains  a  similarity  graph  of  the  top  compositions  that  have  been  found  so  far  during  the  search  and  computes  its  maximal  independent  sets  incrementally  until  convergence,  thereby  reducing  unnecessary  computation  overheads.  Moreover,  we  propose  two  composition  similarity  measurements  w.r.t.  the  categories  or  descriptions  of  services  respectively.  Lastly,  the  experimental  results  on  ProgrammableWeb.com  demonstrate  that,  our  approach  outperforms  another  state-of-the-art  composition  diversification  approach  on  both  metrics  of  density  and  redundancy,  and  meanwhile,  improves  the  efficiency  of  diversification  significantly.
1	P2p  based  multidimensional  indexing  methods  a  survey.  P2P-based  multidimensional  index  (MI)  is  a  hotspot  which  absorbs  many  researchers  to  dedicate  them  into.  However,  no  summarization  or  review  on  this  technology  has  been  made  at  present.  To  the  best  of  our  knowledge,  this  is  the  first  work  on  reviewing  P2P-based  MI.  This  paper  innovatively  adopts  visualization  technique  to  show  the  research  groups  and  then  analyzes  investigating  style  of  research  groups.  Based  on  evolution  of  P2P-based  MI  inheriting  from  centralized  MI  and  P2P,  we  divide  P2P-based  MI  methods  into  4  categories:  extending  centralized  MI,  extending  P2P,  combining  centralized  MI  and  P2P,  and  miscellaneous.  For  each  category,  the  paper  selects  classical  techniques  and  describes  them  in  detail.  This  is  the  first  time  of  doing  the  classification  job  over  massive  related  works.  Finally,  load  balancing  and  update  strategies  are  described  and  discussed  for  they  are  important  factors  related  to  performance.  We  believe  many  researchers  will  get  benefits  from  our  work  for  further  studies.
1	A  novel  color  image  encryption  algorithm  based  on  dna  sequence  operation  and  hyper  chaotic  system.  A  new  color  image  encryption  algorithm  based  on  DNA  (Deoxyribonucleic  acid)  sequence  addition  operation  is  presented.  Firstly,  three  DNA  sequence  matrices  are  obtained  by  encoding  the  original  color  image  which  can  be  converted  into  three  matrices  R,  G  and  B.  Secondly,  we  use  the  chaotic  sequences  generated  by  Chen's  hyper-chaotic  maps  to  scramble  the  locations  of  elements  from  three  DNA  sequence  matrices,  and  then  divide  three  DNA  sequence  matrices  into  some  equal  blocks  respectively.  Thirdly,  we  add  these  blocks  by  using  DNA  sequence  addition  operation  and  Chen's  hyper-chaotic  maps.  At  last,  by  decoding  the  DNA  sequence  matrices  and  recombining  the  three  channels  R,  G  and  B,  we  get  the  encrypted  color  image.  The  simulation  results  and  security  analysis  show  that  our  algorithm  not  only  has  good  encryption  effect,  but  also  has  the  ability  of  resisting  exhaustive  attack,  statistical  attack  and  differential  attack.
1	Reliability  guaranteed  energy  minimization  on  mixed  criticality  systems.  Analyze  resource  demand  of  MC  task  set  under  reliability  and  deadline  constraints.Develop  a  heuristic  approach  to  solve  the  formulated  problem.Evaluate  the  proposed  approach  through  simulation  under  various  scenarios.Achieve  up  to  10%  more  energy  saving  comparing  with  the  existing  approaches.  This  paper  studies  the  energy  minimization  problem  in  mixed-criticality  systems  that  have  stringent  reliability  and  deadline  constraints.  We  first  analyze  the  resource  demand  of  a  mixed-criticality  task  set  that  has  both  reliability  and  deadline  requirements.  Based  on  the  analysis,  we  present  a  heuristic  task  scheduling  algorithm  that  minimizes  system's  energy  consumption  and  at  the  same  time  also  guarantees  system's  reliability  and  deadline  constraints.  Extensive  experiments  are  conducted  to  evaluate  and  validate  the  performance  of  the  proposed  algorithm.  The  empirical  results  show  that  the  algorithm  further  improves  energy  saving  by  up  to  10%  compared  with  the  approaches  proposed  in  our  earlier  work.
1	Evolution  and  change  management  of  xml  based  systems.  XML  is  de-facto  a  standard  language  for  data  exchange.  Structure  of  XML  documents  exchanged  among  different  components  of  a  system  (e.g.  services  in  a  Service-Oriented  Architecture)  is  usually  described  with  XML  schemas.  It  is  a  common  practice  that  there  is  not  only  one  but  a  whole  family  of  XML  schemas  each  applied  in  a  particular  logical  execution  part  of  the  system.  In  such  systems,  the  design  and  later  maintenance  of  the  XML  schemas  is  not  a  simple  task.  In  this  paper  we  aim  at  a  part  of  this  problem  -  evolution  of  the  family  of  the  XML  schemas.  A  single  change  in  user  requirements  or  surrounding  environment  of  the  system  may  influence  more  XML  schemas  in  the  family.  A  designer  needs  to  identify  the  XML  schemas  affected  by  a  change  and  ensure  that  they  are  evolved  coherently  with  each  other  to  meet  the  new  requirement.  Doing  this  manually  is  very  time  consuming  and  error  prone.  In  this  paper  we  show  that  much  of  the  manual  work  can  be  automated.  For  this,  we  introduce  a  technique  based  on  the  principles  of  Model-Driven  Development.  A  designer  is  required  to  make  a  change  only  once  in  a  conceptual  schema  of  the  problem  domain  and  our  technique  ensures  semi-automatic  coherent  propagation  to  all  affected  XML  schemas  (and  vice  versa).  We  provide  a  formal  model  of  possible  evolution  changes  and  their  propagation  mechanism.  We  also  evaluate  the  approach  on  a  real-world  evolution  scenario.
1	Web  application  testing  a  systematic  literature  review.  Context:  The  web  has  had  a  significant  impact  on  all  aspects  of  our  society.  As  our  society  relies  more  and  more  on  the  web,  the  dependability  of  web  applications  has  become  increasingly  important.  To  make  these  applications  more  dependable,  for  the  past  decade  researchers  have  proposed  various  techniques  for  testing  web-based  software  applications.  Our  literature  search  for  related  studies  retrieved  193  papers  in  the  area  of  web  application  testing,  which  have  appeared  between  2000  and  2013.  Objective:  As  this  research  area  matures  and  the  number  of  related  papers  increases,  it  is  important  to  systematically  identify,  analyze,  and  classify  the  publications  and  provide  an  overview  of  the  trends  and  empirical  evidence  in  this  specialized  field.  Methods:  We  systematically  review  the  body  of  knowledge  related  to  functional  testing  of  web  application  through  a  systematic  literature  review  (SLR)  study.  This  SLR  is  a  follow-up  and  complimentary  study  to  a  recent  systematic  mapping  (SM)  study  that  we  conducted  in  this  area.  As  part  of  this  study,  we  pose  three  sets  of  research  questions,  define  selection  and  exclusion  criteria,  and  synthesize  the  empirical  evidence  in  this  area.  Results:  Our  pool  of  studies  includes  a  set  of  95  papers  (from  the  193  retrieved  papers)  published  in  the  area  of  web  application  testing  between  2000  and  2013.  The  data  extracted  during  our  SLR  study  is  available  through  a  publicly-accessible  online  repository.  Among  our  results  are  the  followings:  (1)  the  list  of  test  tools  in  this  area  and  their  capabilities,  (2)  the  types  of  test  models  and  fault  models  proposed  in  this  domain,  (3)  the  way  the  empirical  studies  in  this  area  have  been  designed  and  reported,  and  (4)  the  state  of  empirical  evidence  and  industrial  relevance.  Conclusion:  We  discuss  the  emerging  trends  in  web  application  testing,  and  discuss  the  implications  for  researchers  and  practitioners  in  this  area.  The  results  of  our  SLR  can  help  researchers  to  obtain  an  overview  of  existing  web  application  testing  approaches,  fault  models,  tools,  metrics  and  empirical  evidence,  and  subsequently  identify  areas  in  the  field  that  require  more  attention  from  the  research  community.
1	Requirements  cybernetics.  Users'  behavioral  data  provides  important  cue  for  product  improvement.Requirements  elicitation  process  is  formulated  as  a  feedback  control  system.This  is  an  attempt  to  seek  synergies  between  requirements  and  cybernetics.Potential  control  variables  of  data-driven  elicitation  are  discussed.  Users'  behavioral  data  provides  important  cue  for  product  improvement.  Today's  web  based  applications  collect  various  kinds  of  service  data,  which  is  an  ideal  source  of  information  for  product  designers  to  better  understand  users'  needs  and  behaviors.  This  paper  first  discusses  the  types  of  data  collected  so  far,  and  then  such  data-driven  requirements  elicitation  process  is  formulated  as  a  feedback  control  system,  where  the  classical  requirements  elicitation  philosophy  turns  into  a  continuous  optimization  to  user  behavioral  models.  To  this  end,  it  is  important  to  know  how  the  data  collection  function  reflects  user  behavior,  and  how  specific  data  analysis  approaches  help  making  design  decisions.  This  is  an  attempt  to  seek  practical  synergies  between  the  two  disciplines  of  requirements  and  cybernetics,  to  explore  the  possibilities  of  formulating  problems  in  requirements  with  concepts  and  frameworks  from  cybernetics,  and  understand  to  what  extent  that  known  research  results  from  cybernetics  can  be  applied  to  address  requirements  problems.  In  particular,  control  frameworks  for  the  user  data  driven  requirements  elicitation  process  are  experimented,  and  potential  control  variables  are  discussed.  We  use  two  example  cases  to  illustrate  the  proposed  approach,  an  online  dictionary  service  and  a  mobile  music  player  service.
1	Towards  a  standardized  cloud  service  description  based  on  usdl.  Abstract      In  recent  years,  cloud  computing  paradigm  has  attracted  a  lot  of  attention  from  both  industry  and  academia.  However,  each  cloud  provider  uses  its  own  techniques  (languages,  standards,  ontologies,  or  models,  etc.)  to  describe  cloud  services.  The  diversity  of  these  techniques  leads  to  the  vendor  lock-in  problem,  and  thus,  the  lack  of  a  cloud  service  description  standardization.  In  addition,  existing  service  descriptions  cover  only  particular  aspects  and  neglect  others.  For  example,  WSDL  covers  only  technical  aspect  and  does  not  cover  business  and  semantic  ones.  Our  objective  is  to  define  a  standardized  cloud  service  description  that  covers  technical,  operational,  business,  and  semantic  aspects.  In  this  paper,  we  introduce  different  approaches  that  have  dealt  with  cloud  service  description,  and  thus,  we  adopt  USDL  language  as  an  appropriate  technique  to  describe  cloud  services  thanks  to  its  expressivity  by  covering  three  perspectives  (technical,  operational,  and  business).  But,  USDL  is  still  limited  because  it  cannot  cover  semantic  aspect  and  it  is  not  intended  for  cloud  computing  domain.  After  that,  we  highlight  USDL  limitations  that  can  appear  in  cloud  computing  domain  and  that  should  be  taken  into  consideration  in  our  research  work.  This  paper  will  focus  on  establishing  a  WSMO-based  ontology  to  define  semantically  cloud  services.  This  new  cloud  service  description  is  based  on  USDL  and  we  will  enhance  it  by  taking  into  consideration  some  USDL  limitations.  Finally,  we  test  our  proposed  cloud  service  description  model  on  a  case  study  to  prove  its  applicability.
1	Energy  efficient  heterogeneous  resource  management  for  wireless  monitoring  systems.  Abstract      Various  energy-saving  designs  have  been  proposed  for  reducing  the  power  consumption  of  processors  through  dynamic  voltage  and  frequency  scaling  (DVFS).  When  dynamic  random  access  memory  (DRAM)  or  peripheral  power  consumption  is  high,  dynamic  power  management  (DPM)  can  be  adopted  to  dynamically  activate  or  deactivate  devices  or  to  switch  them  into  energy-saving  states  during  idle  periods.  This  paper  proposes  a  heterogeneous  resource  management  mechanism  to  manage  device  scheduling  for  multiple  tasks  and  task  scheduling  in  a  processor.  A  wireless  network  monitoring  system  was  analyzed  as  a  case  study,  wherein  a  resource  sharing  mechanism  was  developed  for  managing  the  scheduling  of  multiple  wireless  adapters,  and  the  concept  of  instantaneous  utilization  was  leveraged  to  enable  chain-based  task  scheduling.  This  paper  explores  DVFS  and  DPM  energy  saving  techniques  for  peripherals  and  a  processor  by  considering  both  the  required  device  time  and  processor  time  for  each  task  without  violating  performance  requirements  under  constraints  of  buffer  size.  The  proposed  algorithms  were  then  implemented  on  a  wireless  network  monitoring  system  and  real  traces  were  collected  from  a  laboratory  and  downloaded  from  the  UMass  Trace  Repository  for  use  as  inputs.  A  series  of  experiments  was  conducted  to  evaluate  the  quality  of  our  algorithms  for  energy  saving  within  the  constraints  of  system  performance  requirements  and  hardware  resources.
1	The  relation  between  developers  communication  and  fix  inducing  changes  an  empirical  study.  Abstract  Background  Many  open  source  and  industrial  projects  involve  several  developers  spread  around  the  world  and  working  in  different  timezones.  Such  developers  usually  communicate  through  mailing  lists,  issue  tracking  systems  or  chats.  Lack  of  adequate  communication  can  create  misunderstanding  and  could  possibly  cause  the  introduction  of  bugs.  Aim  This  paper  aims  at  investigating  the  relation  between  the  bug  inducing  and  fixing  phenomenon  and  the  lack  of  written  communication  between  committers  in  open  source  projects.  Method  We  performed  an  empirical  study  that  involved  four  open  source  projects,  namely  Apache  httpd,  GNU  GCC,  Mozilla  Firefox,  and  Xorg  Xserver.  For  each  project  change  history  data,  issue  tracker  comments,  mailing  list  messages,  and  chat  logs  were  analyzed  in  order  to  answer  four  research  questions  about  the  relation  between  the  social  importance  and  communication  level  of  committers  and  their  proneness  to  induce  bug  fixes.  Results  and  implications  Results  indicate  that  the  majority  of  bugs  are  fixed  by  committers  who  did  not  induce  them,  a  smaller  but  substantial  percentage  of  bugs  is  fixed  by  committers  that  induced  them,  and  very  few  bugs  are  fixed  by  committers  that  were  not  directly  involved  in  previous  changes  on  the  same  files  of  the  fix.  More  importantly,  committers  inducing  fixes  tend  to  have  a  lower  level  of  communication  between  each  other  than  that  of  other  committers.  This  last  finding  suggests  that  increasing  the  level  of  communication  between  fix-inducing  committers  could  reduce  the  number  of  fixes  induced  in  a  software  project.
1	Simulation  based  embedded  agile  development.  Developers  of  embedded  systems  are  driven  to  constantly  improve  product  quality,  reduce  cost,  and  rapidly  deliver  reliable  working  code.  The  embedded  software  domain  applies  constraints  which  can  hinder  agile  methodologies  commonly  used  to  achieve  such  benefits.  Simulation-based  software  development  is  one  proven  method  that  addresses  these  constraints.  When  tailored  to  work  in  conjunction  with  agile  principles,  simulation  centric  development  facilitates  meeting  product  goals.  In  this  article,  the  authors  discuss  techniques  for  leveraging  software-in-simulation  (SiS)  in  conjunction  with  agile  methodologies,  and  illustrate  how  combining  agile  with  SiS  contributes  to  gaining  more  benefit  from  agile  practices  and  facilitates  better  software  development.
1	The  value  of  a  single  solution  for  end  to  end  alm  tool  support.  A  single  solution  for  application  lifecycle  management  (ALM)  support  lets  stakeholders  see  their  requirements  throughout  the  ALM  process.  This  ranges  from  idea  origination  through  to  seeing  when,  and  to  which  environments,  an  application  was  released  and  the  changes  it  has  undergone.  Such  a  solution  provides  metrics  that  natively  cover  the  ALM  process  and  provide  new  insights.  Finally,  it  provides  a  collaboration  platform  that  brings  all  stakeholders  together  with  a  single  view  over  a  single  source  of  truth  throughout  ALM.
1	Lean  software  development  a  tutorial.  To  put  the  concept  of  lean  software  development  in  context,  it's  useful  to  point  out  similarities  and  differences  with  agile  software  development.  Agile  development  methods  have  generally  expected  system  architecture  and  interaction  design  to  occur  outside  the  development  team,  or  to  occur  in  very  small  increments  within  the  team.  Because  of  this,  agile  practices  often  prove  to  be  insufficient  in  addressing  issues  of  solution  design,  user  interaction  design,  and  high-level  system  architecture.  Increasingly,  agile  development  practices  are  being  thought  of  as  good  ways  to  organize  software  development,  but  insufficient  ways  to  address  design.  Because  design  is  fundamentally  iterative  and  development  is  fundamentally  iterative,  the  two  disciplines  suffer  if  they  are  not  carefully  integrated  with  each  other.  Because  lean  development  lays  out  a  set  of  principles  that  demand  a  whole-product,  complete  life-cycle,  cross-functional  approach,  it's  the  more  likely  candidate  to  guide  the  combination  of  design,  development,  deployment,  and  validation  into  a  single  feedback  loop  focused  on  the  discovery  and  delivery  of  value.
1	Using  stereotypes  to  help  characterize  commits.  Individual  commits  to  a  version  control  system  are  automatically  characterized  based  on  the  stereotypes  of  added  and  deleted  methods.  The  stereotype  of  each  method  is  automatically  reverse  engineerd  using  a  previously  defined  taxonomy.  Method  stereotypes  reflect  intrinsic  atomic  behavior  of  a  method  and  its  role  in  the  class.  The  stereotypes  of  the  added  and  deleted  methods  form  a  descriptors  are  then  used  to  categorize  commits,  into  types,  based  on  the  impact  of  the  changes  to  a  class  (or  classes).  The  goal  is  to  gain  a  better  understanding  of  the  design  changes  to  a  system  over  its  history  and  provide  a  means  for  documenting  the  commit.
1	An  approach  to  analyzing  the  software  process  change  impact  using  process  slicing  and  simulation.  When  a  software  process  is  changed,  a  project  manager  needs  to  perform  two  types  of  change  impact  analysis  activities:  one  for  identifying  the  affected  elements  of  a  software  process  which  is  affected  by  the  change  and  the  other  for  analyzing  the  quantitative  impact  of  the  change  on  the  project  performance.  We  propose  an  approach  to  obtain  the  affected  elements  of  a  software  process  using  process  slicing  and  developing  a  simulation  model  based  on  the  affected  elements  to  quantitatively  analyzing  the  change  using  simulation.  We  suggest  process  slicing  to  obtain  the  elements  affected  by  the  change.  Process  slicing  identifies  the  affected  elements  of  a  software  process  using  a  process  dependency  model.  The  process  dependency  model  contains  activity  control  dependencies,  artifact  information  dependencies,  and  role  replacement  dependencies.  We  also  suggest  transformation  algorithms  to  automatically  derive  the  simulation  model  from  the  process  model  containing  the  affected  elements.  The  quantitative  analysis  can  be  performed  by  simulating  the  simulation  model.  In  addition,  we  provide  the  tool  to  support  our  approach.  We  perform  a  case  study  to  validate  the  usefulness  of  our  approach.  The  result  of  the  case  study  shows  that  our  approach  can  reduce  the  effort  to  identify  the  elements  affected  by  changes  and  examine  alternatives  for  the  project.
1	Predicting  bug  fixing  time  a  replication  study  using  an  open  source  software  project.  Abstract  Background  :  On  projects  with  tight  schedules  and  limited  budgets,  it  may  not  be  possible  to  resolve  all  known  bugs  before  the  next  release.  Estimates  of  the  time  required  to  fix  known  bugs  (the  “bug  fixing  time”)  would  assist  managers  in  allocating  bug  fixing  resources  when  faced  with  a  high  volume  of  bug  reports.  Aim  :  In  this  work,  we  aim  to  replicate  a  model  for  predicting  bug  fixing  time  with  open  source  data  from  Bugzilla  Firefox.  Method  :  To  perform  the  replication  study,  we  follow  the  replication  guidelines  put  forth  by  Carver  [J.  C.  Carver,  Towards  reporting  guidelines  for  experimental  replications:  a  proposal,  in:  1st  International  Workshop  on  Replication  in  Empirical  Software  Engineering,  2010.].  Similar  to  the  original  study,  we  apply  a  Markov-based  model  to  predict  the  number  of  bugs  that  can  be  fixed  monthly.  In  addition,  we  employ  Monte-Carlo  simulation  to  predict  the  total  fixing  time  for  a  given  number  of  bugs.  We  then  use  the  k-nearest  neighbors  algorithm  to  classify  fixing  times  into  slow  and  fast.  Result  :  The  results  of  the  replicated  study  on  Firefox  are  consistent  with  those  of  the  original  study.  The  results  show  that  there  are  similarities  in  the  bug  handling  behaviour  of  both  systems.  Conclusion  :  We  conclude  that  the  model  that  estimates  the  bug  fixing  time  is  robust  enough  to  be  generalized,  and  we  can  rely  on  this  model  for  our  future  research.
1	Supporting  end  users  to  control  their  smart  home  design  implications  from  a  literature  review  and  an  empirical  investigation.  Abstract  Designing  tools  that  allow  end  users  to  easily  control  and  manage  a  smart  home  is  a  critical  issue  that  researchers  in  Ambient  Intelligence  and  Internet  of  Things  have  to  address.  Because  of  the  variety  of  available  solutions,  with  their  advantages  and  limitations,  it  is  not  straightforward  to  understand  which  are  the  requirements  that  must  be  satisfied  to  effectively  support  end  users.  This  paper  aims  to  contribute  to  this  topic  through  a  systematic  and  rigorous  activity  based  on  two  main  pillars  of  the  empirical  research  in  software  engineering:  i)  a  literature  review  addressing  design  and  evaluation  of  tools  for  smart  home  control  oriented  to  end  users,  and  ii)  an  experimental  study  in  which  three  tools,  that  emerged  from  the  literature  review  as  the  most  suitable  and  widespread,  were  compared  in  order  to  identify  the  interaction  mechanisms  that  end  users  appreciate  most.  On  the  basis  of  the  obtained  results,  a  set  of  design  implications  that  may  drive  the  development  of  future  tools  for  smart  home  control  and  management  are  presented.
1	Specifying  uncertainty  in  use  case  models.  Abstract  Context  Latent  uncertainty  in  the  context  of  software-intensive  systems  (e.g.,  Cyber-Physical  Systems  (CPSs))  demands  explicit  attention  right  from  the  start  of  development.  Use  case  modeling—a  commonly  used  method  for  specifying  requirements  in  practice,  should  also  be  extended  for  explicitly  specifying  uncertainty.  Objective  Since  uncertainty  is  a  common  phenomenon  in  requirements  engineering,  it  is  best  to  address  it  explicitly  by  identifying,  qualifying,  and,  where  possible,  quantifying  uncertainty  at  the  beginning  stage.  The  ultimate  aim,  though  not  within  the  scope  of  this  paper,  was  to  use  these  use  cases  as  the  starting  point  to  create  test-ready  models  to  support  automated  testing  of  CPSs  under  uncertainty.  Method  We  extend  the  Restricted  Use  Case  Modeling  (RUCM)  methodology  and  its  supporting  tool  to  specify  uncertainty  as  part  of  system  requirements.  Such  uncertainties  include  those  caused  by  insufficient  domain  expertise  of  stakeholders,  disagreements  among  them,  and  known  uncertainties  about  assumptions  about  the  environment  of  the  system.  The  extended  RUCM,  called  U-RUCM,  inherits  the  features  of  RUCM,  such  as  automated  analyses  and  generation  of  models,  to  mention  but  a  few.  Consequently,  U-RUCM  provides  all  the  key  benefits  offered  by  RUCM  (i.e.,  reducing  ambiguities  in  requirements),  but  also,  it  allows  specification  of  uncertainties  with  the  possibilities  of  reasoning  and  refining  existing  ones  and  even  uncovering  unknown  ones.  Results  We  evaluated  U-RUCM  with  two  industrial  CPS  case  studies.  After  refining  RUCM  models  (specifying  initial  requirements),  by  applying  the  U-RUCM  methodology,  we  successfully  identified  and  specified  additional  306%  and  512%  (previously  unknown)  uncertainty  requirements,  as  compared  to  the  initial  requirements  specified  in  RUCM.  This  showed  that,  with  U-RUCM,  we  were  able  to  get  a  significantly  better  and  more  precise  characterization  of  uncertainties  in  requirement  engineering.  Conclusion  Evaluation  results  show  that  U-RUCM  is  an  effective  methodology  (with  tool  support)  for  dealing  with  uncertainty  in  requirements  engineering.  We  present  our  experience,  lessons  learned,  and  future  challenges,  based  on  the  two  industrial  case  studies.
1	Modern  software  cybernetics.  Classify  software  cybernetics  as  Software  Cybernetics  I  and  II.Identify  the  transition  from  Software  Cybernetics  I  to  Software  Cybernetics  II.Indicate  that  some  new  research  areas  are  related  to  Software  Cybernetics  II.Highlight  new  research  trends  of  Software  Cybernetics  II  for  further  research.  Software  cybernetics  research  is  to  apply  a  variety  of  techniques  from  cybernetics  research  to  software  engineering  research.  For  more  than  fifteen  years  since  2001,  there  has  been  a  dramatic  increase  in  work  relating  to  software  cybernetics.  From  cybernetics  viewpoint,  the  work  is  mainly  on  the  first-order  level,  namely,  the  software  under  observation  and  control.  Beyond  the  first-order  cybernetics,  the  software,  developers/users,  and  running  environments  influence  each  other  and  thus  create  feedback  to  form  more  complicated  systems.  We  classify  software  cybernetics  as  Software  Cybernetics  I  based  on  the  first-order  cybernetics,  and  as  Software  Cybernetics  II  based  on  the  higher  order  cybernetics.  This  paper  provides  a  review  of  the  literature  on  software  cybernetics,  particularly  focusing  on  the  transition  from  Software  Cybernetics  I  to  Software  Cybernetics  II.  The  results  of  the  survey  indicate  that  some  new  research  areas  such  as  Internet  of  Things,  big  data,  cloud  computing,  cyber-physical  systems,  and  even  creative  computing  are  related  to  Software  Cybernetics  II.  The  paper  identifies  the  relationships  between  the  techniques  of  Software  Cybernetics  II  applied  and  the  new  research  areas  to  which  they  have  been  applied,  formulates  research  problems  and  challenges  of  software  cybernetics  with  the  application  of  principles  of  Phase  II  of  software  cybernetics;  identifies  and  highlights  new  research  trends  of  software  cybernetic  for  further  research.
1	Haolap  a  hadoop  based  olap  system  for  big  data.  In  recent  years,  facing  information  explosion,  industry  and  academia  have  adopted  distributed  file  system  and  MapReduce  programming  model  to  address  new  challenges  the  big  data  has  brought.  Based  on  these  technologies,  this  paper  presents  HaoLap  (Hadoop  based  oLap),  an  OLAP  (OnLine  Analytical  Processing)  system  for  big  data.  Drawing  on  the  experience  of  Multidimensional  OLAP  (MOLAP),  HaoLap  adopts  the  specified  multidimensional  model  to  map  the  dimensions  and  the  measures;  the  dimension  coding  and  traverse  algorithm  to  achieve  the  roll  up  operation  on  dimension  hierarchy;  the  partition  and  linearization  algorithm  to  store  dimensions  and  measures;  the  chunk  selection  algorithm  to  optimize  OLAP  performance;  and  MapReduce  to  execute  OLAP.  The  paper  illustrates  the  key  techniques  of  HaoLap  including  system  architecture,  dimension  definition,  dimension  coding  and  traversing,  partition,  data  storage,  OLAP  and  data  loading  algorithm.  We  evaluated  HaoLap  on  a  real  application  and  compared  it  with  Hive,  HadoopDB,  HBaseLattice,  and  Olap4Cloud.  The  experiment  results  show  that  HaoLap  boost  the  efficiency  of  data  loading,  and  has  a  great  advantage  in  the  OLAP  performance  of  the  data  set  size  and  query  complexity,  and  meanwhile  HaoLap  also  completely  support  dimension  operations.
1	Delta  oriented  model  based  integration  testing  of  large  scale  systems.  Software  architecture  specifications  are  of  growing  importance  for  coping  with  the  complexity  of  large-scale  systems.  They  provide  an  abstract  view  on  the  high-level  structural  system  entities  together  with  their  explicit  dependencies  and  build  the  basis  for  ensuring  behavioral  conformance  of  component  implementations  and  interactions,  e.g.,  using  model-based  integration  testing.  The  increasing  inherent  diversity  of  such  large-scale  variant-rich  systems  further  complicates  quality  assurance.  In  this  article,  we  present  a  combination  of  architecture-driven  model-based  testing  principles  and  regression-inspired  testing  strategies  for  efficient,  yet  comprehensive  variability-aware  conformance  testing  of  variant-rich  systems.  We  propose  an  integrated  delta-oriented  architectural  test  modeling  and  testing  approach  for  component  as  well  as  integration  testing  that  allows  the  generation  and  reuse  of  test  artifacts  among  different  system  variants.  Furthermore,  an  automated  derivation  of  retesting  obligations  based  on  accurate  delta-oriented  architectural  change  impact  analysis  is  provided.  Based  on  a  formal  conceptual  framework  that  guarantees  stable  test  coverage  for  every  system  variant,  we  present  a  sample  implementation  of  our  approach  and  an  evaluation  of  the  validity  and  efficiency  by  means  of  a  case  study  from  the  automotive  domain.
1	Multi  objective  optimization  of  energy  consumption  and  execution  time  in  a  single  level  cache  memory  for  embedded  systems.  We  present  a  new  parallel  optimization  method  to  optimize  cache  memory  designs.It  obtains  optimal  designs  of  cache  memories  with  respect  to  designer's  metrics.We  also  reduce  the  execution  time  and  energy  consumption  up  to  64.43%  and  91.69%.As  a  result,  we  have  outlined  other  interesting  future  research  lines  in  this  area.  Current  embedded  systems  are  specifically  designed  to  run  multimedia  applications.  These  applications  have  a  big  impact  on  both  performance  and  energy  consumption.  Both  metrics  can  be  optimized  selecting  the  best  cache  configuration  for  a  target  set  of  applications.  Multi-objective  optimization  may  help  to  minimize  both  conflicting  metrics  in  an  independent  manner.  In  this  work,  we  propose  an  optimization  method  that  based  on  Multi-Objective  Evolutionary  Algorithms,  is  able  to  find  the  best  cache  configuration  for  a  given  set  of  applications.  To  evaluate  the  goodness  of  candidate  solutions,  the  execution  of  the  optimization  algorithm  is  combined  with  a  static  profiling  methodology  using  several  well-known  simulation  tools.  Results  show  that  our  optimization  framework  is  able  to  obtain  an  optimized  cache  for  Mediabench  applications.  Compared  to  a  baseline  cache  memory,  our  design  method  reaches  an  average  improvement  of  64.43  and  91.69%  in  execution  time  and  energy  consumption,  respectively.
1	Examining  decision  characteristics  challenges  for  agile  software  development.  In-depth  case  via  interviews,  focus  groups,  meeting  observation  &  document  analysis.We  conducted  18  individual  interviews,  two  focus  groups  and  observed  21  meetings.Results  identified  particular  decision  characteristics  across  four  key  agile  values.Results  show  characteristics  decision  process,  intelligence  and  quality  challenges.This  study  provides  a  framework  to  evaluate  agile  decision  making.  Although  agile  software  development  is  often  associated  with  improved  decision  making,  existing  studies  tend  to  focus  on  narrow  aspects  of  decision  making  in  such  environments.  There  is  a  lack  of  clarity  on  how  teams  make  and  evaluate  a  myriad  of  decisions  from  software  feature  inception  to  product  delivery  and  refinement.  Indeed  there  is  relatively  little  known  about  a)  the  decision  characteristics  related  to  agile  values,  and  b)  the  challenges  they  present  for  decision  making  on  agile  teams.  We  present  an  in-depth  exploratory  case  study  based  on  a  pluralistic  approach  comprising  semi-structured  interviews,  focus  groups,  team  meeting  observations,  and  document  analysis.  The  study  identifies  failings  of  decision  making  in  an  agile  setting.  Explicitly  considering  the  decision  process,  information  intelligence  used  in  decision  making,  and  decision  quality,  the  key  contribution  of  this  paper  is  the  development  of  an  over-arching  framework  of  agile  decision  making,  which  identifies  particular  decision  characteristics  across  4  key  agile  values  and  the  related  challenges  for  agile  team  decision  making.  It  provides  a  framework  for  researchers  and  practitioners  to  evaluate  the  decision  challenges  of  an  agile  software  development  team  and  to  improve  decision  quality.
1	Methodological  construction  of  product  form  stochastic  petri  nets  for  performance  evaluation.  Product-forms  in  Stochastic  Petri  nets  (SPNs)  are  obtained  by  a  compositional  technique  for  the  first  time,  by  combining  small  SPNs  with  product-forms  in  a  hierarchical  manner.  In  this  way,  performance  engineering  methodology  is  enhanced  by  the  greatly  improved  efficiency  endowed  to  the  steady-state  solution  of  a  much  wider  range  of  Markov  models.  Previous  methods  have  relied  on  analysis  of  the  whole  net  and  so  are  not  incremental-hence  they  are  intractable  in  all  but  small  models.  We  show  that  the  product-form  condition  for  open  nets  depends,  in  general,  on  the  transition  rates,  whereas  closed  nets  have  only  structural  conditions  for  a  product-form,  except  in  rather  pathological  cases.  Both  the  ''building  blocks''  formed  by  the  said  small  SPNs  and  their  compositions  are  solved  for  their  product-forms  using  the  Reversed  Compound  Agent  Theorem  (RCAT),  which,  to  date,  has  been  used  exclusively  in  the  context  of  process-algebraic  models.  The  resulting  methodology  provides  a  powerful,  general  and  rigorous  route  to  product-forms  in  large  stochastic  models  and  is  illustrated  by  several  detailed  examples.
1	Reusability  of  open  source  software  across  domains.  Investigation  of  domain-specific  reuse  opportunities.Empirical  comparison  of  application  domains  in  terms  of  availability  of  reusable  software  assets.Empirical  comparison  of  application  domains  in  terms  of  structural  reusability.  Exploiting  the  enormous  amount  of  open  source  software  (OSS)  as  a  vehicle  for  reuse  is  a  promising  opportunity  for  software  engineers.  However,  this  task  is  far  from  trivial,  since  such  projects  are  sometimes  not  easy  to  understand  and  adapt  to  target  systems,  whereas  at  the  same  time  the  reusable  assets  are  not  obvious  to  identify.  In  this  study,  we  assess  open  source  software  projects,  with  respect  to  their  reusability,  i.e.,  the  easiness  to  adapt  them  in  a  new  system.  By  taking  into  account  that  domain-specific  reuse  is  more  beneficial  than  domain-agnostic;  we  focus  this  study  on  identifying  the  application  domains  that  contain  the  most  reusable  software  projects.  To  achieve  this  goal,  we  compared  the  reusability  of  approximately  600  OSS  projects  from  ten  application  domains  through  a  case  study.  The  results  of  the  study  suggested  that  in  every  aspect  of  reusability,  there  are  different  dominant  application  domains.  However,  Science  and  Engineering  Applications  and  Software  Development  Tools,  have  proven  to  be  the  ones  that  are  the  most  reuse-friendly.  Based  on  this  observation,  we  suggest  software  engineers,  who  are  focusing  on  the  specific  application  domains,  to  consider  reusing  assets  from  open  source  software  projects.
1	A  computer  system  architecture  providing  a  user  friendly  man  machine  interface  for  accessing  assistive  technology  in  cloud  computing.  HighlightsInvestigating  how  cloud  computing  can  support  the  Assistive  Technology  (AT).Providing  solutions  to  I/O  interaction  issues.Design,  development,  and  evaluation  of  a  cloud  architecture  for  AT.Performance  evaluation  in  private  and  public  cloud  scenarios.Social,  e-learning  and  business  impact.  Assistive  Technology  (AT)  includes  hardware  peripherals,  software  applications  and  systems  that  enable  a  user  with  a  disability  to  use  a  PC.  Thus,  when  a  disabled  user  needs  to  work  in  a  particular  environment  (e.g.,  at  work,  at  school,  in  a  government  office,  etc.)  he/she  has  to  properly  configure  the  used  PC.  However,  often,  the  configuration  of  AT  software  interfaces  is  not  trivial  at  all.  This  paper  presents  the  software  design,  implementation,  and  evaluation  of  a  computer  system  architecture  providing  a  software  user-friendly  man  machine  interface  for  accessing  AT  software  in  cloud  computing.  The  main  objective  of  such  an  architecture  is  to  provide  a  new  type  of  software  human-computer  interaction  for  accessing  AT  services  over  the  cloud.  Thus,  end  users  can  interact  with  their  personalized  computer  environments  using  any  physical  networked  PC.  The  advantage  of  this  approach  is  that  users  do  not  have  to  install  and/or  setup  any  additional  software  on  physical  PCs  and  they  can  access  their  own  AT  virtual  environments  from  everywhere.  In  particular,  the  usability  of  prototype  based  on  the  Remote  Desktop  Protocol  (RDP)  is  evaluated  in  both  private  and  public  cloud  scenarios.
1	A  product  line  model  driven  engineering  approach  for  generating  feature  based  mobile  applications.  Abstract      A  significant  challenge  faced  by  the  mobile  application  industry  is  developing  and  maintaining  multiple  native  variants  of  mobile  applications  to  support  different  mobile  operating  systems,  devices  and  varying  application  functional  requirements.  The  current  industrial  practice  is  to  develop  and  maintain  these  variants  separately.  Any  potential  change  has  to  be  applied  across  variants  manually,  which  is  neither  efficient  nor  scalable.  We  consider  the  problem  of  supporting  multiple  platforms  as  a  ‘software  product-line  engineering’  problem.  The  paper  proposes  a  novel  application  of  product-line  model-driven  engineering  to  mobile  application  development  and  addresses  the  key  challenges  of  feature-based  native  mobile  application  variants  for  multiple  platforms.  Specifically,  we  deal  with  three  types  of  variations  in  mobile  applications:  variation  due  to  operation  systems  and  their  versions,  software  and  hardware  capabilities  of  mobile  devices,  and  functionalities  offered  by  the  mobile  application.  We  develop  a  tool  MOPPET  that  automates  the  proposed  approach.  Finally,  the  results  of  applying  the  approach  on  two  industrial  case  studies  show  that  the  proposed  approach  is  applicable  to  industrial  mobile  applications  and  have  potential  to  significantly  reduce  the  development  effort  and  time.
1	Commtpst  deep  learning  source  code  for  commenting  positions  prediction.  Abstract  Existing  techniques  for  automatic  code  commenting  assume  that  the  code  snippet  to  be  commented  has  been  identified,  thus  requiring  users  to  provide  the  code  snippet  in  advance.  A  smarter  commenting  approach  is  desired  to  first  self-determine  where  to  comment  in  a  given  source  code  and  then  generate  comments  for  the  code  snippets  that  need  comments.  To  achieve  the  first  step  of  this  goal,  we  propose  a  novel  method,  CommtPst,  to  automatically  find  the  appropriate  commenting  positions  in  the  source  code.  Since  commenting  is  closely  related  to  the  code  syntax  and  semantics,  we  adopt  neural  language  model  (word  embeddings)  to  capture  the  code  semantic  information,  and  analyze  the  abstract  syntax  trees  to  capture  code  syntactic  information.  Then,  we  employ  LSTM  (long  short  term  memory)  to  model  the  long-term  logical  dependency  of  code  statements  over  the  fused  semantic  and  syntactic  information  and  learn  the  commenting  patterns  on  the  code  sequence.  We  evaluated  CommtPst  using  large  data  sets  from  dozens  of  open-source  software  systems  in  GitHub.  The  experimental  results  show  that  the  precision,  recall  and  F-Measure  values  achieved  by  CommtPst  are  0.792,  0.602  and  0.684,  respectively,  which  outperforms  the  traditional  machine  learning  method  with  11.4%  improvement  on  F-measure.
1	Early  validation  of  cyber  physical  space  systems  via  multi  concerns  integration.  Abstract  Cyber–physical  space  systems  are  engineered  systems  operating  within  physical  space  with  design  requirements  that  depend  on  space,  e.g.,  regarding  location  or  movement  behavior.  They  are  built  from  and  depend  upon  the  seamless  integration  of  computation  and  physical  components.  Typical  examples  include  systems  where  software-driven  agents  such  as  mobile  robots  explore  space  and  perform  actions  to  complete  particular  missions.  Design  of  such  a  system  often  depends  on  multiple  concerns  expressed  by  different  stakeholders,  capturing  different  aspects  of  the  system.  We  propose  a  model-driven  approach  supporting  (a)  separation  of  concerns  during  design,  (b)  systematic  and  semi-automatic  integration  of  separately  modeled  concerns,  and  finally  (c)  early  validation  via  statistical  model  checking.  We  evaluate  our  approach  over  two  different  case  studies  of  cyber–physical  space  systems.
1	Software  analytics  so  what.  The  guest  editors  of  this  special  issue  of  IEEE  Software  invited  submissions  that  reflected  the  benefits  (and  drawbacks)  of  software  analytics,  an  area  of  explosive  growth.  They  had  so  many  excellent  submissions  that  they  had  to  split  this  special  issue  into  two  volumes--you'll  see  even  more  content  in  the  September/October  issue.  They  divided  the  articles  on  conceptual  grounds,  so  both  volumes  will  feature  equally  excellent  work.  The  Web  extra  at  http://youtu.be/nO6X0azR0nw  is  a  video  interview  in  which  IEEE  Software  editor  in  chief  Forrest  Shull  speaks  with  Tim  Menzies  about  the  growing  importance  of  software  analytics.
1	Exploiting  big  data  s  benefits.  Knowing  about  big  data's  potential  for  exploiting  new  business  ideas  is  a  key  capability  for  staying  successful  in  the  market.  Potential  analysis  provides  a  systematic  way  to  identify  and  close  the  gap  between  big  data's  possible  benefits  and  the  ability  to  turn  that  data  into  business  value.
1	Security  and  privacy  on  the  web  guest  editors  introduction.  Software  enables  every  aspect  of  the  Web.  Everything  from  device  communication  to  online  social  networks  is  achievable  only  because  of  multiple  lines  of  code.  For  various  reasons,  designing  and  building  security  and  privacy  into  Web  software  is  often  an  afterthought  for  most  developers.  This  results  in  easily  compromised  systems  that  pose  significant  privacy  and  security  risks  to  users.  The  Web  extra  at  https://youtu.be/juxM-mJERxc  is  an  audio  recording  of  Davide  Falessi  speaking  with  Guest  Editors  Tyrone  Grandison,  CEO  of  Proficiency  Labs,  and  Larry  Koved,  Principal  Research  Staff  Member  at  IBM  Research,  about  why,  at  a  bare  minimum,  Web  software  developers  must  ensure  that  their  code  is  sufficiently  hardened  to  protect  against  URL  interpretation  attacks,  input  validation  attacks,  SQL  injection  attacks,  impersonation  attacks,  basic  inference  attacks,  buffer  overflow  attacks,  and  inadvertent  data  disclosure  attacks.
1	Expert  perspectives  on  ai.  IEEE  Software:  With  the  rapid  changes  occurring  in  the  fields  of  artificial  intelligence  (AI)  and  machine  learning  (ML),  what  areas  do  you  think  are  the  most  important  to  focus  on  right  now,  especially  in  relation  to  software  engineering?
1	The  online  controlled  experiment  lifecycle.  Unlike  other  techniques  for  learning  from  customers,  online  controlled  experiments  (OCEs)  establish  an  accurate  and  causal  relationship  between  a  change  and  the  impact  observed.  We  show  that  OCEs  help  optimize  infrastructure  needs  and  aid  in  project  planning  and  measuring  team  efforts.  We  conclude  that  product  development  should  fully  integrate  the  experiment  lifecycle  to  benefit  from  the  OCEs.
1	Testing  quality  assurance  in  data  migration  projects.  New  business  models,  constant  technological  progress,  as  well  as  ever-changing  legal  regulations  require  that  companies  replace  their  business  applications  from  time  to  time.  As  a  side  effect,  this  demands  for  migrating  the  data  from  the  existing  source  application  to  a  target  application.  Since  the  success  of  the  application  replacement  as  a  form  of  IT  maintenance  is  contingent  on  the  underlying  data  migration  project,  it  is  crucial  to  accomplish  the  migration  in  time  and  on  budget.  This  however,  calls  for  a  stringent  data  migration  process  model  combined  with  well-defined  quality  assurance  measures.  The  paper  presents,  first,  a  field-tested  process  model  for  data  migration  projects.  Secondly,  it  points  out  the  typical  risks  we  have  frequently  observed  in  the  course  of  this  type  of  project.  Thirdly,  the  paper  provides  practice-based  testing  and  quality  assurance  techniques  to  reduce  or  even  eliminate  these  data  migration  risks.
1	Recommending  framework  extension  examples.  The  use  of  software  frameworks  enables  the  delivery  of  common  functionality  but  with  significantly  less  effort  than  when  developing  from  scratch.  To  meet  application  specific  requirements,  the  behavior  of  a  framework  needs  to  be  customized  via  extension  points.  A  common  way  of  customizing  framework  behavior  is  by  passing  a  framework  related  object  as  an  argument  to  an  API  call.  Such  an  object  can  be  created  by  subclassing  an  existing  framework  class  or  interface,  or  by  directly  customizing  an  existing  framework  object.  However,  to  do  this  effectively  requires  developers  to  have  extensive  knowledge  of  the  framework's  extension  points  and  their  interactions.  To  aid  the  developers  in  this  regard,  we  propose  and  evaluate  a  graph  mining  approach  for  extension  point  management.  Specifically,  we  propose  a  taxonomy  of  extension  patterns  to  categorize  the  various  ways  an  extension  point  has  been  used  in  the  code  examples.  Our  approach  mines  a  large  amount  of  code  examples  to  discover  all  extension  points  and  patterns  for  each  framework  class.  Given  a  framework  class  that  is  being  used,  our  approach  aids  the  developer  by  following  a  two-step  recommendation  process.  First,  it  recommends  all  the  extension  points  that  are  available  in  the  class.  Once  the  developer  chooses  an  extension  point,  our  approach  then  discovers  all  of  its  usage  patterns  and  recommends  the  best  code  examples  for  each  pattern.  Using  five  frameworks,  we  evaluate  the  performance  of  our  two-step  recommendation,  in  terms  of  precision,  recall,  and  F-measure.  We  also  report  several  statistics  related  to  framework  extension  points.
1	Interactive  exploration  of  collaborative  software  development  data.  Modern  collaborative  software-development  tools  generate  a  rich  data  record,  over  the  lifecycle  of  the  project,  which  can  be  analyzed  to  provide  team  members  and  managers  with  insights  into  the  performance  and  contributions  of  individual  members  and  the  overall  team  dynamic.  This  data  can  be  analyzed  from  different  perspectives,  sliced  and  diced  across  different  dimensions,  and  visualized  in  different  ways.  Frequently  the  most  useful  analysis  depends  on  the  actual  data,  which  makes  the  design  of  single  authoritative  visualization  a  challenge.  In  this  paper  we  describe  an  analysis  and  visualization  tool  that  supports  the  flexible  run-time  mapping  of  such  a  data  record  to  a  number  of  alternative  visualizations.  We  have  used  our  framework  to  analyze  and  gain  an  understanding  of  how  individuals  work  within  their  teams  and  how  teams  differ  in  their  work  on  these  term  projects.
1	Fault  interaction  and  its  repercussions.  Multiple  faults  in  a  program  can  interact  to  form  new  behaviors  in  a  program  that  would  not  be  realized  if  the  program  were  to  contain  the  individual  faults.  This  paper  presents  an  in-depth  study  of  the  effects  of  the  interaction  of  faults  within  a  program.  Many  researchers  attempt  to  ameliorate  the  effects  of  faulty  programs.  Unfortunately,  such  researchers  are  left  to  rely  upon  intuition  about  fault  behavior  due  to  the  paucity  of  formalized  studies  of  faults  and  their  behavior.  In  an  attempt  to  advance  the  understanding  of  faults  and  their  behavior,  we  conducted  a  study  of  fault  interaction  across  six  subjects  with  more  than  65,000  multiple-fault  versions.  The  results  of  our  study  show  four  significant  types  of  interaction,  with  one  type  —  faults  obscuring  the  effects  of  other  faults  —  as  the  most  prevalent  type.  The  prevalence  of  obscuring  faults'  effects  has  an  adverse  effect  on  many  automated  software-engineering  techniques,  such  as  regression-testing,  fault-localization,  and  fault-clustering  techniques.  Given  that  software  commonly  contains  more  than  a  single  fault,  these  results  have  implications  for  developers  and  researchers  alike  by  informing  them  of  expected  complications,  which  in  many  instances  are  opposite  to  intuition.
1	Relating  requirements  to  implementation  via  topic  analysis  do  topics  extracted  from  requirements  make  sense  to  managers  and  developers.  Large  organizations  like  Microsoft  tend  to  rely  on  formal  requirements  documentation  in  order  to  specify  and  design  the  software  products  that  they  develop.  These  documents  are  meant  to  be  tightly  coupled  with  the  actual  implementation  of  the  features  they  describe.  In  this  paper  we  evaluate  the  value  of  high-level  topic-based  requirements  traceability  in  the  version  control  system,  using  Latent  Dirichlet  Allocation  (LDA).  We  evaluate  LDA  topics  on  practitioners  and  check  if  the  topics  and  trends  extracted  matches  the  perception  that  Program  Managers  and  Developers  have  about  the  effort  put  into  addressing  certain  topics.  We  found  that  effort  extracted  from  version  control  that  was  relevant  to  a  topic  often  matched  the  perception  of  the  managers  and  developers  of  what  occurred  at  the  time.  Furthermore  we  found  evidence  that  many  of  the  identified  topics  made  sense  to  practitioners  and  matched  their  perception  of  what  occurred.  But  for  some  topics,  we  found  that  practitioners  had  difficulty  interpreting  and  labelling  them.  In  summary,  we  investigate  the  high-level  traceability  of  requirements  topics  to  version  control  commits  via  topic  analysis  and  validate  with  the  actual  stakeholders  the  relevance  of  these  topics  extracted  from  requirements.
1	Scotch  test  to  code  traceability  using  slicing  and  conceptual  coupling.  Maintaining  traceability  links  between  unit  tests  and  tested  classes  is  an  important  factor  for  effectively  managing  the  development  and  evolution  of  software  systems.  Exploiting  traceability  links  helps  in  program  comprehension  and  maintenance  by  ensuring  consistency  between  unit  tests  and  tested  classes  during  maintenance  activities.  Unfortunately,  it  is  often  the  case  that  such  links  are  not  explicitly  maintained  and  thus  they  have  to  be  recovered  manually  during  software  evolution.  A  novel  automated  solution  to  this  problem,  based  on  dynamic  slicing  and  conceptual  coupling,  is  presented.  The  resulting  tool,  SCOTCH  (Slicing  and  Coupling  based  Test  to  Code  trace  Hunter),  is  empirically  evaluated  on  three  systems:  an  open  source  system  and  two  industrial  systems.  The  results  indicate  that  SCOTCH  identifies  traceability  links  between  unit  test  classes  and  tested  classes  with  a  high  accuracy  and  greater  stability  than  existing  techniques,  highlighting  its  potential  usefulness  as  a  feature  within  a  software  development  environment.
1	Improving  the  robustness  and  efficiency  of  continuous  integration  and  deployment.  Modern  software  is  developed  at  a  rapid  pace.  To  sustain  that  rapid  pace,  organizations  rely  heavily  on  automated  build,  test,  and  release  steps.  To  that  end,  Continuous  Integration  and  Continuous  Deployment  (CI/CD)  services  take  the  incremental  codebase  changes  that  are  produced  by  developers,  compile  them,  link,  and  package  them  into  software  deliverables,  verify  their  functionality,  and  deliver  them  to  end  users.  While  CI/CD  processes  provide  mission-critical  features,  if  they  are  misconfigured  or  poorly  operated,  the  pace  of  development  may  be  slowed  or  even  halted.  To  prevent  such  issues,  in  this  thesis,  we  set  out  to  study  and  improve  the  robustness  and  efficiency  of  CI/CD.  The  thesis  will  include  (1)  conceptual  contributions  in  the  form  of  empirical  studies  of  large  samples  of  adopters  of  CI/CD  tools  to  discover  best  practices  and  common  limitations,  as  well  as  (2)  technical  contributions  in  the  form  of  tools  that  support  stakeholders  to  avoid  common  limitations  (e.g.,  data  misinterpretation  issues,  CI  configuration  mistakes).
1	A  manual  categorization  of  android  app  development  issues  on  stack  overflow.  While  many  tutorials,  code  examples,  and  documentation  about  Android  APIs  exist,  developers  still  face  various  problems  with  the  implementation  of  Android  Apps.  Many  of  these  issues  are  discussed  on  QaA-sites,  such  as  Stack  Overflow.  In  this  paper  we  present  a  manual  categorization  of  450  Android  related  posts  of  Stack  Overflow  concerning  their  question  and  problem  types.  The  idea  is  to  find  dependencies  between  certain  problems  and  question  types  to  get  better  insights  into  issues  of  Android  App  development.  The  categorization  is  developed  using  card  sorting  with  three  experienced  Android  App  developers.  An  initial  approach  to  automate  the  classification  of  Stack  Overflow  posts  using  Lucene  is  also  presented.  The  study  highlights  that  the  most  common  question  types  are  'How  to?'  and  'What  is  the  problem?'.  The  problems  that  are  discussed  most  often  are  related  to  'User  Interface'  and  'Core  Elements'.  In  particular,  the  problem  category  'Layout'  is  often  related  to  'What  is  the  problem?'  and  'Frameworks'  issues  often  come  with  'Is  it  possible?'  questions.
1	Nlp2api  query  reformulation  for  code  search  using  crowdsourced  knowledge  and  extra  large  data  analytics.  Software  developers  frequently  issue  generic  natural  language  (NL)  queries  for  code  search.  Unfortunately,  such  queries  often  do  not  lead  to  any  relevant  results  with  contemporary  code  (or  web)  search  engines  due  to  vocabulary  mismatch  problems.  In  our  technical  research  paper  (accepted  at  ICSME  2018),  we  propose  a  technique–NLP2API–that  reformulates  such  NL  queries  using  crowdsourced  knowledge  and  extra-large  data  analytics  derived  from  Stack  Overflow  Q  &  A  site.  In  this  paper,  we  discuss  all  the  artifacts  produced  by  our  work,  and  provide  necessary  details  for  downloading  and  verifying  them.
1	Concepts  operations  and  feasibility  of  a  projection  based  variation  control  system.  Highly  configurable  software  often  uses  preprocessor  annotations  to  handle  variability.  However,  understanding,  maintaining,  and  evolving  code  with  such  annotations  is  difficult,  mainly  because  a  developer  has  to  work  with  all  variants  at  a  time.  Dedicated  methods  and  tools  that  allow  working  on  a  subset  of  all  variants  could  ease  the  engineering  of  highly  configurable  software.  We  investigate  the  potential  of  one  kind  of  such  tools:  projection-based  variation  control  systems.  For  such  systems  we  aim  to  understand:  (i)  what  end-user  operations  they  need  to  support,  and  (ii)  whether  they  can  realize  the  actual  evolution  of  real-world,  highly  configurable  software.  We  conduct  an  experiment  that  investigates  variability-related  evolution  patterns  and  that  evaluates  the  feasibility  of  a  projection-based  variation  control  system  by  replaying  parts  of  the  history  of  a  highly  configurable  real-world  3D  printer  firmware  project.  Among  others,  we  show  that  the  prototype  variation  control  system  does  indeed  support  the  evolution  of  a  highly  configurable  system  and  that  in  general,  it  does  not  degrade  the  code.
1	Automatic  means  of  identifying  evolutionary  events  in  software  development.  The  software  development  process  patterns  in  open  source  software  projects  are  not  well  known.  Consequently,  the  longevity  of  new  open  source  software  projects  is  left  up  to  subjective  experiences  of  the  development  team.  In  this  study,  we  are  investigating  a  data  mining  approach  for  identifying  relevant  patterns  in  software  development  process.  We  demonstrate  the  capabilities  of  wavelet  analysis  on  27  open  source  software  projects  for  identifying  similar  evolutionary  patterns  or  events  in  different  projects.  The  analysis  identified  close  to  1000  evolutionary  patterns  common  to  multiple  projects.  The  analysis  of  some  of  the  patterns  shows  that  the  end  of  source  code  evolution  of  a  project  is  determined  early  in  the  project.  In  addition,  strong  fluctuations  of  activity  in  sequential  periods  are  identified  as  good  indicators  of  problems  in  projects.  In  conclusion,  the  analysis  reveals  that  wavelet  analysis  can  be  a  powerful  and  objective  tool  for  identifying  evolutionary  events  that  can  be  used  as  estimation  basis  or  management  guide  in  software  projects.
1	Prevalence  and  maintenance  of  automated  functional  tests  for  web  applications.  Functional  testing  requires  executing  particular  sequences  of  user  actions.  Test  automation  tools  enable  scripting  user  actions  such  that  they  can  be  repeated  more  easily.  SELENIUM,  for  instance,  enables  testing  web  applications  through  scripts  that  interact  with  a  web  browser  and  assert  properties  about  its  observable  state.  However,  little  is  known  about  how  common  such  tests  are  in  practice.  We  therefore  present  a  cross-sectional  quantitative  study  of  the  prevalence  of  SELENIUM-based  tests  among  open-source  web  applications,  and  of  the  extent  to  which  such  tests  are  used  within  individual  applications.  Automating  functional  tests  also  brings  about  the  problem  of  maintaining  test  scripts.  As  the  system  under  test  evolves,  its  test  scripts  are  bound  to  break.  Even  less  is  known  about  the  way  test  scripts  change  over  time.  We  therefore  also  present  a  longitudinal  quantitative  study  of  whether  and  for  how  long  test  scripts  are  maintained,  as  well  as  a  longitudinal  qualitative  study  of  the  kind  of  changes  they  undergo.  To  the  former's  end,  we  propose  two  new  metrics  based  on  whether  a  commit  to  the  application's  version  repository  touches  a  test  file.  To  the  latter's  end,  we  propose  to  categorize  the  changes  within  each  commit  based  on  the  elements  of  the  test  upon  which  they  operate.  As  such,  we  are  able  to  identify  the  elements  of  a  test  that  are  most  prone  to  change.
1	Semi  automatic  identification  and  representation  of  subsystem  variability  in  simulink  models.  This  paper  presents  a  semi-automated  framework  for  identifying  and  representing  different  kinds  of  variability  in  Simulink  models.  Based  on  the  observed  variants  found  in  similar  subsystem  patterns  inferred  using  Simone,  a  text-based  model  clone  detection  tool,  we  propose  a  set  of  variability  operators  for  Simulink  models.  By  applying  these  operators  to  six  example  systems,  we  are  able  to  represent  the  variability  in  their  similar  subsystem  patterns  as  a  single  subsystem  template  directly  in  the  Simulink  environment.  The  product  of  our  framework  is  a  single  consolidated  subsystem  model  capable  of  expressing  the  observed  variability  across  all  instances  of  each  inferred  pattern.  The  process  of  pattern  inference  and  variability  analysis  is  largely  automated  and  can  be  easily  applied  to  other  collections  of  Simulink  models.  The  framework  is  aimed  at  providing  assistance  to  engineers  to  identify,  understand,  and  visualize  patterns  of  subsystems  in  a  large  model  set.  This  understanding  may  help  in  reducing  maintenance  effort  and  bug  identification  at  an  early  stage  of  the  software  development.
1	Investigating  instability  architectural  smells  evolution  an  exploratory  case  study.  Architectural  smells  may  substantially  increase  maintenance  effort  and  thus  require  extra  attention  for  potential  refactoring.  While  we  currently  understand  this  concept  and  have  identified  different  types  of  such  smells,  we  have  not  yet  studied  their  evolution  in  depth.  This  is  necessary  to  inform  their  prioritisation  and  refactoring.  This  study  analyses  the  evolution  of  individual  architectural  smell  instances  over  time,  and  the  characteristics  that  define  these  instances.  Three  different  types  of  architectural  smells  are  taken  into  consideration  and  mined  from  a  total  of  524  versions  across  14  different  projects.  The  results  show  how  different  smell  types  differ  in  multiple  aspects,  such  as  their  growth  rate,  the  importance  of  the  affected  elements  over  time  in  the  dependency  network  of  the  system,  and  the  time  each  instance  affects  the  system.  They  also  cast  valuable  insights  on  what  aspects  are  the  most  important  to  consider  during  prioritisation  and  refactoring  activities.
1	Reflexion  models  for  state  machine  extraction  and  verification.  High-level  design  models  are  often  used  for  describing  the  behavior  or  structure  of  a  software  system.  It  is  generally  much  easier  and  more  adequate  to  understand  a  software  system  on  this  level  than  on  the  level  of  individual  code  lines.  Such  models  are  also  created  by  developers  as  they  gain  an  understanding  of  the  software.  Unfortunately,  these  models  often  do  not  correspond  to  what  is  really  in  the  code.  Murphy  et  al.  introduced  the  idea  of  reflexion  models  in  1995  to  overcome  this  problem.  Their  approach  is  today  widely  used  for  architecture  conformance  checking  and  reconstruction.  In  this  paper,  we  introduce  reflexion  models  for  state  machines.  Our  approach  allows  to  check  the  correspondence  of  a  hypothetical  state  machine  model  with  the  code.  It  returns  information  about  convergence,  partial  convergence,  divergence,  or  absence  of  the  specified  states  and  transitions.  Similar  to  the  original  reflexion  model,  the  approach  can  be  used  for  conformance  checking  as  well  as  interactive  reverse  engineering  of  state  machine  models.  We  concentrate  on  the  latter  and  show  the  potential  of  the  approach  in  several  case  studies.
1	Drone  predicting  priority  of  reported  bugs  by  multi  factor  analysis.  Bugs  are  prevalent.  To  improve  software  quality,  developers  often  allow  users  to  report  bugs  that  they  found  using  a  bug  tracking  system  such  as  Bugzilla.  Users  would  specify  among  other  things,  a  description  of  the  bug,  the  component  that  is  affected  by  the  bug,  and  the  severity  of  the  bug.  Based  on  this  information,  bug  triagers  would  then  assign  a  priority  level  to  the  reported  bug.  As  resources  are  limited,  bug  reports  would  be  investigated  based  on  their  priority  levels.  This  priority  assignment  process  however  is  a  manual  one.  Could  we  do  better?  In  this  paper,  we  propose  an  automated  approach  based  on  machine  learning  that  would  recommend  a  priority  level  based  on  information  available  in  bug  reports.  Our  approach  considers  multiple  factors,  temporal,  textual,  author,  related-report,  severity,  and  product,  that  potentially  affect  the  priority  level  of  a  bug  report.  These  factors  are  extracted  as  features  which  are  then  used  to  train  a  discriminative  model  via  a  new  classification  algorithm  that  handles  ordinal  class  labels  and  imbalanced  data.  Experiments  on  more  than  a  hundred  thousands  bug  reports  from  Eclipse  show  that  we  can  outperform  baseline  approaches  in  terms  of  average  F-measure  by  a  relative  improvement  of  58.61%.
1	Modeling  changeset  topics  for  feature  location.  Feature  location  is  a  program  comprehension  activity  in  which  a  developer  inspects  source  code  to  locate  the  classes  or  methods  that  implement  a  feature  of  interest.  Many  feature  location  techniques  (FLTs)  are  based  on  text  retrieval  models,  and  in  such  FLTs  it  is  typical  for  the  models  to  be  trained  on  source  code  snapshots.  However,  source  code  evolution  leads  to  model  obsolescence  and  thus  to  the  need  to  retrain  the  model  from  the  latest  snapshot.  In  this  paper,  we  introduce  a  topic-modeling-based  FLT  in  which  the  model  is  built  incrementally  from  source  code  history.  By  training  an  online  learning  algorithm  using  changesets,  the  FLT  maintains  an  up-to-date  model  without  incurring  the  non-trivial  computational  cost  associated  with  retraining  traditional  FLTs.  Overall,  we  studied  over  600  defects  and  features  from  4  open-source  Java  projects.  We  also  present  a  historical  simulation  that  demonstrates  how  the  FLT  performs  as  a  project  evolves.  Our  results  indicate  that  the  accuracy  of  a  changeset-based  FLT  is  similar  to  that  of  a  snapshot-based  FLT,  but  without  the  retraining  costs.
1	Dum  tool.  With  object-oriented  programming  languages  (e.g.,  Java  or  C#),  the  identification  of  unreachable  source  code  may  be  very  complex  especially  when  working  at  method  level.  To  deal  with  the  detection  of  unreachable  methods,  we  have  defined  an  approach  named  DUM:  Detecting  Unreachable  Methods.  We  implemented  a  prototype  of  a  supporting  software  we  named  DUM-Tool.  It  works  on  Java  byte-code  and  detects  unreachable  methods  by  traversing  a  graph-based  representation  of  a  subject  software.
1	Newcomer  candidate  characterizing  contributions  of  a  novice  developer  to  github.  To  attract,  onboard,  and  retain  any  newcomer  in  Open  Source  Software  (OSS)  projects  is  vital  to  their  livelihood.  Recent  studies  conclude  that  OSS  projects  risk  failure  due  to  abandonment  and  poor  participation  of  newcomers.  Evidence  suggests  more  new  users  are  joining  GitHub,  however,  the  extent  to  which  they  contribute  to  OSS  projects  is  unknown.  In  this  study,  we  coin  the  term  ‘newcomer  candidate’  to  describe  new  users  to  the  GitHub  platform.  Our  objective  is  to  track  and  characterize  their  initial  contributions.  As  a  preliminary  survey,  we  collected  208  newcomer  candidate  contributions  in  GitHub.  Using  this  dataset,  we  then  plan  to  track  their  contributions  to  reveal  insights.  We  will  use  a  mixed-methods  approach,  i.e.,  quantitative  and  qualitative,  to  identify  whether  or  not  newcomer  candidates  practice  social  coding,  the  kinds  of  their  contributions,  projects  they  target,  and  the  proportion  that  they  eventually  onboard  to  an  OSS  project.
1	3  1  challenges  for  the  future  of  universities.  Highlights?  Existing  approaches  to  research  and  education  are  increasingly  perceived  as  unable  or  at  least  insufficient  to  capture  and  take  into  account  the  complexity  and  the  dynamism  of  the  globalized  society.  ?  Research  and  innovation  are  intrinsically  different  processes  and  therefore  require  different  methods,  skills,  and  funding  mechanisms.  ?  We  need  to  deeply  and  radically  rethink  the  roots  of  the  education  model  used  in  our  curricula,  and  also  their  organization  and  timespan.  ?  Radical  changes  require  a  radical  rethinking  and  questioning  of  the  overall  organization,  focus,  and  operational  model  of  universities.  Universities  are  looking  for  effective  strategies  to  cope  with  the  global  changes  that  have  extended  across  the  world  in  the  past  years.  Existing  approaches  to  research  and  education  are  increasingly  perceived  as  unable  or  at  least  insufficient  to  capture  and  take  into  account  the  complexity  and  the  dynamism  of  the  globalized  society.  This  is  particularly  true  for  the  ICT  sector,  which  has  been  radically  transformed  by  technologies  such  as  mobile  devices,  ubiquitous  connectivity,  and  pervasive  ICT.  Indeed,  as  these  technologies  are  inherently  disruptive,  they  are  profoundly  impacting  and  transforming  the  economy  and  the  entire  society  in  general.This  paper  aims  at  discussing  the  issues  and  problems  that  universities  are  facing  to  deal  with  the  growth  and  evolution  of  the  ICT  sector.  In  particular,  the  paper  proposes  3+1  challenges  they  need  to  address  and  master.  The  challenges  deal  with  three  fundamental  functions  of  modern  universities:  research,  innovation,  and  education.  Moreover,  the  paper  proposes  a  fourth  challenge  related  primarily  to  the  attitude  and  behavior  of  faculty  members  and  academic  boards.  The  ultimate  goal  of  the  paper  is  to  contribute  to  the  development  of  an  effective  and  useful  debate  about  the  strategies  to  support  the  evolution  and  growth  of  universities,  as  key  players  to  promote  the  public  good  and  the  overall  progress  of  our  society.
1	Data  management  for  component  based  embedded  real  time  systems  the  database  proxy  approach.  We  introduce  the  concept  of  database  proxies  intended  to  mitigate  the  gap  between  two  disjoint  productivity-enhancing  techniques:  component  based  software  engineering  (CBSE)  and  real-time  database  management  systems  (RTDBMS).  The  two  techniques  promote  opposing  design  goals  and  their  coexistence  is  neither  obvious  nor  intuitive.  CBSE  promotes  encapsulation  and  decoupling  of  component  internals  from  the  component  environment,  whilst  an  RTDBMS  provide  mechanisms  for  efficient  and  predictable  global  data  sharing.  A  component  with  direct  access  to  an  RTDBMS  is  dependent  on  that  specific  RTDBMS  and  may  not  be  useable  in  an  alternative  environment.  For  components  to  remain  encapsulated  and  reusable,  database  proxies  decouple  components  from  an  underlying  database  residing  in  the  component  framework,  while  providing  temporally  predictable  access  to  data  maintained  in  a  database.  Our  approach  provide  access  to  features  such  as  extensive  data  modeling  tools,  predictable  access  to  hard  real-time  data,  dynamic  access  to  soft  real-time  data  using  standardized  queries  and  controlled  data  sharing;  thus  allowing  developers  to  employ  the  full  potential  of  both  CBSE  and  an  RTDBMS.  Our  approach  primarily  targets  embedded  systems  with  a  subset  of  functionality  with  real-time  requirements.  The  implementation  results  show  that  the  benefits  of  using  proxies  do  not  come  at  the  expense  of  significant  run-time  overheads  or  less  accurate  timing  predictions.
1	Empirical  research  for  software  architecture  decision  making  an  analysis.  Abstract  Context  Despite  past  empirical  research  in  software  architecture  decision  making,  we  have  not  yet  systematically  studied  how  to  perform  such  empirical  research.  Software  architecture  decision  making  involves  humans,  their  behavioral  issues  and  practice.  As  such,  research  on  decision  making  needs  to  involve  not  only  engineering  but  also  social  science  research  methods.  Objective  This  paper  studies  empirical  research  on  software  architecture  decision  making.  We  want  to  understand  what  research  methods  have  been  used  to  study  human  decision  making  in  software  architecture.  Further,  we  want  to  provide  guidance  for  future  studies.  Method  We  analyzed  research  papers  on  software  architecture  decision  making.  We  classified  the  papers  according  to  different  sub-dimensions  of  empirical  research  design  like  research  logic,  research  purpose,  research  methodology  and  process.  We  introduce  the  study  focus  matrix  and  the  research  cycle  to  capture  the  focus  and  the  goals  of  a  software  architecture  decision  making  study.  We  identify  gaps  in  current  software  architecture  decision  making  research  according  to  the  classification  and  discuss  open  research  issues  inspired  by  social  science  research.  Conclusion  We  show  the  variety  of  research  designs  and  identify  gaps  with  respect  to  focus  and  goals.  Few  papers  study  decision  making  behavior  in  software  architecture  design.  Also  these  researchers  study  mostly  the  process  and  much  less  the  outcome  and  the  factors  influencing  decision  making.  Furthermore,  there  is  a  lack  of  improvements  for  software  architecture  decision  making  and  in  particular  insights  into  behavior  have  not  led  to  new  practices.  The  study  focus  matrix  and  the  research  cycle  are  two  new  instruments  for  researchers  to  position  their  research  clearly.  This  paper  provides  a  retrospective  for  the  community  and  an  entry  point  for  new  researchers  to  design  empirical  studies  that  embrace  the  human  role  in  software  architecture  decision  making.
1	A  general  framework  for  comparing  automatic  testing  techniques  of  android  mobile  apps.  Abstract      As  an  increasing  number  of  new  techniques  are  developed  for  quality  assurance  of  Android  applications  (apps),  there  is  a  need  to  evaluate  and  empirically  compare  them.  Researchers  as  well  as  practitioners  will  be  able  to  use  the  results  of  such  comparative  studies  to  answer  questions  such  as,  “What  technique  should  I  use  to  test  my  app?”  Unfortunately,  there  is  a  severe  lack  of  rigorous  empirical  studies  on  this  subject.  In  this  paper,  for  the  first  time,  we  present  an  empirical  study  comparing  all  existing  fully  automatic  “online”  testing  techniques  developed  for  the  Android  platform.  We  do  so  by  first  reformulating  each  technique  within  the  context  of  a  general  framework.  We  recognize  the  commonalities  between  the  techniques  to  develop  the  framework.  We  then  use  the  salient  features  of  each  technique  to  develop  parameters  of  the  framework.  The  result  is  a  general  recasting  of  all  existing  approaches  in  a  plug-in  based  formulation,  allowing  us  to  vary  the  parameters  to  create  instances  of  each  technique,  and  empirically  evaluate  them  on  a  common  set  of  subjects.  Our  results  show  that  (1)  the  proposed  general  framework  abstracts  all  the  common  characteristics  of  online  testing  techniques  proposed  in  the  literature,  (2)  it  can  be  exploited  to  design  experiments  aimed  at  performing  objective  comparisons  among  different  online  testing  approaches  and  (3)  some  parameters  that  we  have  identified  influence  the  performance  of  the  testing  techniques.
1	Effective  and  efficient  detection  of  software  theft  via  dynamic  api  authority  vectors.  We  design  a  novel  feature  of  a  program  for  detecting  software  theft.We  reflect  the  sequence  and  the  frequency  information  of  a  program  to  our  feature.Our  proposed  method  is  credible,  resilient,  and  scalable.Our  method  outperforms  existing  software  theft  detection  methods  in  our  experiments.  Software  theft  has  become  a  very  serious  threat  to  both  the  software  industry  and  individual  software  developers.  A  software  birthmark  indicates  unique  characteristics  of  a  program  in  question,  which  can  be  used  for  analyzing  the  similarity  of  a  pair  of  programs  and  detecting  theft.  This  paper  proposes  a  novel  birthmark,  a  dynamic  API  authority  vector  (DAAV).  DAAV  satisfies  four  essential  requirements  for  good  birthmarkscredibility,  resiliency,  scalability,  and  packing-freewhile  existing  static  birthmarks  are  unable  to  handle  the  packed  programs  and  existing  dynamic  birthmarks  do  not  satisfy  credibility  and  resiliency.  Through  our  extensive  experiments  with  a  set  of  Windows  applications,  DAAV  is  shown  to  have  not  only  the  credibility  and  resiliency  higher  than  the  existing  dynamic  birthmarks  but  also  the  accuracy  comparable  to  that  of  existing  static  birthmarks.  This  result  indicates  that  our  proposed  birthmark  provides  high  accuracy  and  also  covers  packed  programs  successfully  in  detecting  software  theft.
1	Going  with  the  flow  an  activity  theory  analysis  of  flow  techniques  in  software  development.  Abstract      Managing  flow  is  fundamental  to  continuous  development,  particularly  in  knowledge  intensive  work  activities  such  as  software  development.  However,  while  numerous  articles  describe  flow  tools  and  practice  there  is  little  research  on  their  application  in  context.  This  is  a  significant  limitation  given  that  software  development  is  a  highly  complex  and  socially  embedded  activity.  This  research  applies  activity  theory  (AT)  to  examine  the  adoption  of  flow  techniques  by  using  the  multiple-case  method  in  two  companies.  AT  is  particularly  pertinent  in  this  study  as  it  identifies  contradictions,  which  manifest  themselves  as  problems  such  as  errors  or  a  breakdown  of  communication  in  the  organisation  and  congruencies  between  flow  techniques  and  the  development  context  and  indeed  contradictions  between  components  of  flow  techniques  themselves.  Rather  than  view  contradictions  as  a  threat  to  flow  or  as  an  argument  to  abandon,  a  theoretical  contribution  of  this  study  is  that  it  shows  how  contradictions  and  congruencies  can  be  used  to  reflect,  learn,  and  identify  new  ways  of  structuring  and  enacting  the  flow  activity.  It  also  provides  an  immediate  practical  contribution  by  identifying  a  set  of  lessons  drawn  from  the  cases  studied  that  may  be  applicable  in  future  implementations  of  flow  techniques.
1	Ldfr  learning  deep  feature  representation  for  software  defect  prediction.  Abstract  Software  Defect  Prediction  (SDP)  aims  to  detect  defective  modules  to  enable  the  reasonable  allocation  of  testing  resources,  which  is  an  economically  critical  activity  in  software  quality  assurance.  Learning  effective  feature  representation  and  addressing  class  imbalance  are  two  main  challenges  in  SDP.  Ideally,  the  more  discriminative  the  features  learned  from  the  modules  and  the  better  the  rescue  performed  on  the  imbalance  issue,  the  more  effective  it  should  be  in  detecting  defective  modules.  In  this  study,  to  solve  these  two  challenges,  we  propose  a  novel  framework  named  LDFR  by  Learning  Deep  Feature  Representation  from  the  defect  data  for  SDP.  Specifically,  we  use  a  deep  neural  network  with  a  new  hybrid  loss  function  that  consists  of  a  triplet  loss  to  learn  a  more  discriminative  feature  representation  of  the  defect  data  and  a  weighted  cross-entropy  loss  to  remedy  the  imbalance  issue.  To  evaluate  the  effectiveness  of  the  proposed  LDFR  framework,  we  conduct  extensive  experiments  on  a  benchmark  dataset  with  27  defect  data  (each  with  three  types  of  features),  using  three  traditional  and  three  effort-aware  indicators.  Overall,  the  experimental  results  demonstrate  the  superiority  of  our  LDFR  framework  in  detecting  defective  modules  when  compared  with  27  baseline  methods,  except  in  terms  of  the  indicator  of  Precision.
1	Mahtab  phase  wise  acceleration  of  regression  testing  for  c.  Abstract  Software  regression  testing  consists  of  offline,  online,  and  execution  phases  which  are  executed  sequentially.  The  offline  phase  involves  code  instrumentation  and  test-coverage  collection.  Subsequently,  the  online  phase  performs  program  differencing,  test-suite  selection  and  prioritization.  Finally,  the  selected  test-cases  are  executed  against  the  new  version  of  software  for  its  re-validation.  Regression  testing  is  a  time-consuming  process  and  is  often  on  the  critical  path  of  the  project.  To  improve  the  turn-around  time  of  software  development  cycle,  our  goal  is  to  reduce  regression  testing  time  across  all  phases  using  multi-core  parallelization.  This  poses  several  challenges  that  stem  from  I/O,  dependence  on  third-party  libraries,  and  inherently  sequential  components  in  the  overall  testing  process.  We  propose  parallelization  test-windows  to  effectively  partition  test-cases  across  threads.  To  measure  the  benefit  of  prioritization  coupled  with  multi-threaded  execution,  we  propose  a  new  metric,  EPSilon,  for  rewarding  failure  observation  frequency  in  the  timeline  of  test-execution.  To  measure  the  rate  of  code-change  coverage  due  to  regression  test  prioritization,  we  introduce  ECC,  a  variant  of  the  widely  used  APFD  metric.  We  illustrate  the  effectiveness  of  our  approach  using  the  popular  Software-artifact  Infrastructure  Repository  (SIR)  and  five  real-world  projects  from  GitHub.
1	A  robust  blind  color  image  watermarking  in  quaternion  fourier  transform  domain.  Highlights?  To  embed  the  digital  watermark  in  quaternion  Fourier  transform  domain,  which  improves  the  robustness  against  color  attacks.  ?  To  correct  the  watermarked  image  by  LS-SVM  classification,  which  improve  the  robustness  against  geometrical  distortions.  ?  To  modify  the  symmetric  quaternion  Fourier  transform  coefficients,  which  improve  the  watermark  imperceptibility.  Most  of  the  existing  color  image  watermarking  schemes  were  designed  to  mark  the  image  luminance  component  only,  which  have  some  disadvantages:  (i)  they  are  sensitive  to  color  attacks  because  of  ignoring  the  correlation  between  different  color  channels,  (ii)  they  are  always  not  robust  to  geometric  distortions  for  neglecting  the  watermark  desynchronization.  It  is  a  challenging  work  to  design  a  robust  color  image  watermarking  scheme.  Based  on  quaternion  Fourier  transform  and  least  squares  support  vector  machine  (LS-SVM),  we  propose  a  robust  blind  color  image  watermarking  in  quaternion  Fourier  transform  domain,  which  has  good  visual  quality.  Firstly,  the  original  color  image  is  divided  into  color  image  blocks.  Then,  the  fast  quaternion  Fourier  transform  is  performed  on  the  color  image  block.  Finally,  the  digital  watermark  is  embedded  into  original  color  image  by  adaptively  modulating  the  real  quaternion  Fourier  transform  coefficients  of  color  image  block.  For  watermark  decoding,  the  LS-SVM  correction  with  pseudo-Zernike  moments  is  utilized.  Experimental  results  show  that  the  proposed  color  image  watermarking  is  not  only  robust  against  common  image  processing  operations  such  as  filtering,  JPEG  compression,  histogram  equalization,  and  image  blurring,  but  also  robust  against  the  geometrical  distortions.
1	Architecture  level  hazard  analysis  using  aadl.  Abstract      Software  systems  are  becoming  increasingly  important  in  safety-critical  areas.  Designing  safe  software  requires  a  significant  emphasis  on  hazards  in  the  early  design  phase  of  software  development.  In  this  paper,  we  propose  a  hazard  analysis  approach  based  on  Architecture  Analysis  and  Design  Language  (AADL).  First,  to  make  up  the  deficiencies  of  Error  Model  Annex  (EMV2),  we  create  Hazard  Model  Annex  (HMA)  to  specify  the  hazard  sources,  hazards,  hazard  trigger  mechanisms,  and  mishaps.  By  using  HMA,  a  safety  model  can  be  built  by  annotating  an  architecture  model  with  the  error  model  and  hazard  model.  Then,  an  architecture-level  hazard  analysis  approach  is  proposed  to  automatically  generate  the  hazard  analysis  table.  The  approach  contains  the  model  transformation  from  a  safety  model  to  a  Deterministic  Stochastic  Petri  Nets  (DSPNs)  model  for  calculating  the  occurrence  probability  of  hazards  and  mishaps.  In  addition,  we  present  the  formal  semantics  for  each  constituent  part  of  the  safety  model,  define  the  model  mapping  rules,  and  verify  the  semantic  preservation  of  the  transformation.  Finally,  HMA  is  implemented  to  build  safety  models  and  two  Eclipse  plug-ins  of  our  methodology  are  also  implemented.  A  case  study  on  a  flight  control  software  system  has  been  employed  to  demonstrate  the  feasibility  of  our  proposed  technique.
1	Parsed  use  case  descriptions  as  a  basis  for  object  oriented  class  model  generation.  Abstract:  Object-oriented  analysis  and  design  has  become  a  major  approach  in  the  design  of  software  systems.  Recent  developments  in  CASE  tools  provide  help  in  documenting  the  analysis  and  design  stages  and  in  detecting  incompleteness  and  inconsistency  in  analysis.  However,  these  tools  do  not  contribute  to  the  initial  and  difficult  stage  of  the  analysis  process  of  identifying  the  objects/classes,  attributes  and  relationships  used  to  model  the  problem  domain.  This  paper  presents  a  tool,  Class-Gen,  which  can  partially  automate  the  identification  of  objects/classes  from  natural  language  requirement  specifications  for  object  identification.  Use  case  descriptions  (UCDs)  provide  the  input  to  Class-Gen  which  parses  and  analyzes  the  text  written  in  English.  A  parsed  use  case  description  (PUCD)  is  generated  which  is  then  used  as  the  basis  for  the  construction  of  an  initial  UML  class  model  representing  object  classes  and  relationships  identified  in  the  requirements.  PUCDs  enable  the  extraction  of  nouns,  verbs,  adjectives  and  adverbs  from  traditional  UCDs  for  the  identification  process.  Finally  Class-Gen  allows  the  initial  class  model  to  be  refined  manually.  Class-Gen  has  been  evaluated  against  a  collection  of  unseen  requirements.  The  results  of  the  evaluation  are  encouraging  as  they  demonstrate  the  potential  for  such  tools  to  assist  with  the  software  development  process.
1	Identification  of  extract  method  refactoring  opportunities  for  the  decomposition  of  methods.  The  extraction  of  a  code  fragment  into  a  separate  method  is  one  of  the  most  widely  performed  refactoring  activities,  since  it  allows  the  decomposition  of  large  and  complex  methods  and  can  be  used  in  combination  with  other  code  transformations  for  fixing  a  variety  of  design  problems.  Despite  the  significance  of  Extract  Method  refactoring  towards  code  quality  improvement,  there  is  limited  support  for  the  identification  of  code  fragments  with  distinct  functionality  that  could  be  extracted  into  new  methods.  The  goal  of  our  approach  is  to  automatically  identify  Extract  Method  refactoring  opportunities  which  are  related  with  the  complete  computation  of  a  given  variable  (complete  computation  slice)  and  the  statements  affecting  the  state  of  a  given  object  (object  state  slice).  Moreover,  a  set  of  rules  regarding  the  preservation  of  existing  dependences  is  proposed  that  exclude  refactoring  opportunities  corresponding  to  slices  whose  extraction  could  possibly  cause  a  change  in  program  behavior.  The  proposed  approach  has  been  evaluated  regarding  its  ability  to  capture  slices  of  code  implementing  a  distinct  functionality,  its  ability  to  resolve  existing  design  flaws,  its  impact  on  the  cohesion  of  the  decomposed  and  extracted  methods,  and  its  ability  to  preserve  program  behavior.  Moreover,  precision  and  recall  have  been  computed  employing  the  refactoring  opportunities  found  by  independent  evaluators  in  software  that  they  developed  as  a  golden  set.
1	A  robust  data  hiding  algorithm  for  h  264  avc  video  streams.  This  paper  presents  a  robust  readable  data  hiding  algorithm  for  H.264/AVC  video  streams  without  intra-frame  distortion  drift.  We  first  encode  the  embedded  data  using  BCH  (n,  k,  t)  syndrome  code  before  data  hiding  to  improve  robustness,  then  we  embed  the  encoded  data  into  coefficients  of  the  4x4  luminance  discrete  cosine  transform  (DCT)  blocks  in  I  frames  which  meet  our  conditions  to  avert  the  distortion  drift,  and  finally  we  recover  the  original  video  as  much  as  possible  when  the  hidden  data  is  extracted  out.  The  experimental  results  show  that  our  scheme  can  get  more  robustness,  effectively  avert  intra-frame  distortion  drift  and  get  high  visual  quality.
1	Static  change  impact  analysis  techniques.  We  present  a  comparative  study  of  different  static  CIA  techniques,  Columbus,  ROSE,  and  IRC2M.IRC2M  and  ROSE  achieve  relatively  better  accuracy  compared  to  Columbus.Combination  of  different  CIA  techniques  can  obviously  improve  the  accuracy  over  their  individual  one.Combining  all  three  CIA  techniques  obtain  a  similar  precision  and  recall  as  combining  ROSE  with  IRC2M.  Software  Change  Impact  Analysis  (CIA)  is  an  essential  technique  to  identify  the  potential  effects  caused  by  software  changes  during  software  maintenance  and  evolution.  A  rich  body  of  CIA  techniques,  especially  static  CIA  techniques,  have  continuously  emerged  in  recent  years  such  as  structural  static  analysis,  textual  analysis,  and  historical  analysis.  However,  there  were  only  a  few  works  focusing  on  comparison  of  static  CIA  techniques.  This  article  attempts  to  bridge  this  gap  by  presenting  a  comparative  study  of  three  class-level  static  CIA  techniques,  i.e.,  Columbus,  ROSE,  and  IRC2M.  We  compare  them  based  on  a  CIA  comparative  framework  and  conduct  an  empirical  study  to  evaluate  these  three  CIA  techniques  and  their  combinations  based  on  five  real-world  programs.  The  empirical  results  show  that:  (1)  IRC2M  and  ROSE  achieve  relatively  better  precision,  recall  and  F-measure  compared  to  Columbus;  (2)  combination  of  any  two  CIA  techniques  can  improve  the  precision  and  recall  over  their  individual  one;  moreover,  combining  ROSE  with  IRC2M  produces  the  best  impact  results;  and  (3)  combining  all  three  CIA  techniques  obtain  a  similar  precision  and  recall  as  combining  ROSE  with  IRC2M.
1	A  ground  truth  dataset  and  classification  model  for  detecting  bots  in  github  issue  and  pr  comments.  Abstract  Bots  are  frequently  used  in  Github  repositories  to  automate  repetitive  activities  that  are  part  of  the  distributed  software  development  process.  They  communicate  with  human  actors  through  comments.  While  detecting  their  presence  is  important  for  many  reasons,  no  large  and  representative  ground-truth  dataset  is  available,  nor  are  classification  models  to  detect  and  validate  bots  on  the  basis  of  such  a  dataset.  This  paper  proposes  a  ground-truth  dataset,  based  on  a  manual  analysis  with  high  interrater  agreement,  of  pull  request  and  issue  comments  in  5,000  distinct  Github  accounts  of  which  527  have  been  identified  as  bots.  Using  this  dataset  we  propose  an  automated  classification  model  to  detect  bots,  taking  as  main  features  the  number  of  empty  and  non-empty  comments  of  each  account,  the  number  of  comment  patterns,  and  the  inequality  between  comments  within  comment  patterns.  We  obtained  a  very  high  weighted  average  precision,  recall  and  F1-score  of  0.98  on  a  test  set  containing  40%  of  the  data.  We  integrated  the  classification  model  into  an  open  source  command-line  tool  to  allow  practitioners  to  detect  which  accounts  in  a  given  Github  repository  actually  correspond  to  bots.
1	Hsp  a  hybrid  selection  and  prioritisation  of  regression  test  cases  based  on  information  retrieval  and  code  coverage  applied  on  an  industrial  case  study.  Abstract  The  usual  way  to  guarantee  quality  of  software  products  is  via  testing.  This  paper  presents  a  novel  strategy  for  selection  and  prioritisation  of  Test  Cases  (TC)  for  Regression  testing.  In  the  lack  of  code  artifacts  from  where  to  derive  Test  Plans,  this  work  uses  information  conveyed  by  textual  documents  maintained  by  Industry,  such  as  Change  Requests.  The  proposed  process  is  based  on  Information  Retrieval  techniques  combined  with  indirect  code  coverage  measures  to  select  and  prioritise  TCs.  The  aim  is  to  provide  a  high  coverage  Test  Plan  which  would  maximise  the  number  of  bugs  found.  This  process  was  implemented  as  a  prototype  tool  which  was  used  in  a  case  study  with  our  industrial  partner  (Motorola  Mobility).  Experiments  results  revealed  that  the  combined  strategy  provides  better  results  than  the  use  of  information  retrieval  and  code  coverage  independently.  Yet,  it  is  worth  mentioning  that  any  of  these  automated  options  performed  better  than  the  previous  manual  process  deployed  by  our  industrial  partner  to  create  test  plans.
1	A  systematic  review  on  the  functional  testing  of  semantic  web  services.  Semantic  web  services  are  gaining  more  attention  as  an  important  element  of  the  emerging  semantic  web.  Therefore,  testing  semantic  web  services  is  becoming  a  key  concern  as  an  essential  quality  assurance  measure.  The  objective  of  this  systematic  literature  review  is  to  summarize  the  current  state  of  the  art  of  functional  testing  of  semantic  web  services  by  providing  answers  to  a  set  of  research  questions.  The  review  follows  a  predefined  procedure  that  involves  automatically  searching  5  well-known  digital  libraries.  After  applying  the  selection  criteria  to  the  results,  a  total  of  34  studies  were  identified  as  relevant.  Required  information  was  extracted  from  the  studies  and  summarized.  Our  systematic  literature  review  identified  some  approaches  available  for  deriving  test  cases  from  the  specifications  of  semantic  web  services.  However,  many  of  the  approaches  are  either  not  validated  or  the  validation  done  lacks  credibility.  We  believe  that  a  substantial  amount  of  work  remains  to  be  done  to  improve  the  current  state  of  research  in  the  area  of  testing  semantic  web  services.
1	Designing  and  implementing  an  environment  for  software  start  up  education  patterns  and  anti  patterns.  Abstract  Today’s  students  are  prospective  entrepreneurs,  as  well  as  potential  employees  in  modern,  start-up-like  intrapreneurship  environments  within  established  companies.  In  these  settings,  software  development  projects  face  extreme  requirements  in  terms  of  innovation  and  attractiveness  of  the  end-product.  They  also  suffer  severe  consequences  of  failure  such  as  termination  of  the  development  effort  and  bankruptcy.  As  the  abilities  needed  in  start-ups  are  not  among  those  traditionally  taught  in  universities,  new  knowledge  and  skills  are  required  to  prepare  students  for  the  volatile  environment  that  new  market  entrants  face.  This  article  reports  experiences  gained  during  seven  years  of  teaching  start-up  knowledge  and  skills  in  a  higher-education  institution.  Using  a  design-based  research  approach,  we  have  developed  the  Software  Factory,  an  educational  environment  for  experiential,  project-based  learning.  We  offer  a  collection  of  patterns  and  anti-patterns  that  help  educational  institutions  to  design,  implement  and  operate  physical  environments,  curricula  and  teaching  materials,  and  to  plan  interventions  that  may  be  required  for  project-based  start-up  education.
1	Improving  the  communication  performance  of  distributed  animation  rendering  using  bittorrent  file  system.  We  propose  the  BitTorrent  file  system,  BTFS,  that  employs  the  BitTorrent  P2P  protocol  to  speedup  the  dissemination  of  a  large  dataset  among  client  machines.BTFS  provides  the  transparent  P2P  data  communication  mechanism  for  all  applications.We  address  the  data  security  as  well  as  its  scalability  issues  in  the  BTFS  design.Using  BTFS,  the  overall  rendering  time  in  the  volunteer-based  distributed  animation  rendering  is  up  to  3  times  less  than  those  of  traditional  network  file  systems.  Rendering  is  a  crucial  process  in  the  production  of  computer  generated  animation  movies.  It  executes  a  computer  program  to  transform  3D  models  into  series  of  still  images,  which  will  eventually  be  sequenced  into  a  movie.  Due  to  the  size  and  complexity  of  3D  models,  rendering  process  becomes  a  tedious,  time-consuming  and  unproductive  task  on  a  single  machine.  Accordingly,  animation  rendering  is  commonly  carried  out  in  a  distributed  computing  environment  where  numerous  computers  execute  in  parallel  to  speedup  the  rendering  process.  In  accordance  with  distribution  of  computing,  data  dissemination  to  all  computers  also  needs  certain  mechanisms  which  allow  large  3D  models  to  be  efficiently  moved  to  those  distributed  computers  to  ensure  the  reduction  of  time  and  cost  in  animation  production.  This  paper  presents  and  evaluates  BitTorrent  file  system  (BTFS)  for  improving  the  communication  performance  of  distributed  animation  rendering.  The  BTFS  provides  an  efficient,  secure  and  transparent  distributed  file  system  which  decouples  the  applications  from  complicated  communication  mechanism.  By  having  data  disseminated  in  a  peer-to-peer  manner  and  using  local  cache,  rendering  time  can  be  reduced.  Its  performance  comparison  with  a  production-grade  3D  animation  favorably  shows  that  the  BTFS  outperforms  traditional  distributed  file  systems  by  more  than  3  times  in  our  test  configuration.
1	A  comparative  study  on  simulation  vs  real  time  deployment  in  wireless  sensor  networks.  Increasing  deployment  density  and  shrinking  size  of  wireless  sensor  nodes  requires  small  equipped  battery  size.  This  means  emerging  wireless  sensor  nodes  must  compete  for  efficient  energy  utilization.  Medium  Access  Control  (MAC)  protocols  play  a  vital  role  in  energy  consumption  of  sensor  node  as  it  controls  the  radio  activities.  Customized  or  open  source  simulators  play  an  important  role  to  measure  the  performance  effectiveness  of  MAC  protocols  based  on  the  fact  that  they  are  flexible,  reduce  experimental  overhead  and  cost.  Nevertheless,  these  benefits  come  at  the  cost  of  results  accuracy.  In  this  paper,  we  investigate  differences  of  the  behaviour  of  our  agent  based  S-MAC  protocols  in  real  deployment  compared  to  the  results  produced  using  our  custom  based  simulator,  which  ignores  the  lower  layers  effects  such  as  packet  collision  and  overhearing.  We  use  network  simulator  2  (ns2),  an  open  source  simulator,  which  provides  a  complete  protocol  stack.  We  further  try  to  find  and  explain  the  rationale  of  the  variance  of  results  produced  by  real  deployment  and  that  of  simulators.
1	Quantitatively  measuring  a  large  scale  agile  transformation.  We  provide  a  quantitative  metrics  model  to  evaluate  the  impact  of  an  agile  and  lean  transformation.We  propose  eight  rigorously  described  metrics  within  that  model.We  establish  and  apply  the  model  in  a  large  international  telecommunication  organization  with  350  employees  in  two  sites.Our  findings  show  significant  improvement  in  six  of  the  eight  metrics  whereas  one  metric  showed  deteriorated  results.  Context:  Agile  software  development  continues  to  grow  in  popularity  and  is  being  adopted  by  more  and  more  organizations.  However,  there  is  a  need  for  empirical  evidence  on  the  impact,  benefits  and  drawbacks  of  an  agile  transformation  in  an  organization  since  the  cost  for  such  a  transformation  in  terms  of  money,  disrupted  working  routines  and  quality  of  development  can  become  considerable.  Currently,  such  evidence  exists  in  the  form  of  success  stores  and  case  studies,  mostly  of  qualitative  nature.Objective:  Provide  a  metrics  model  to  quantitatively  measure  the  impact  of  an  agile  transformation  in  a  software  development  organization.Method:  The  metrics  model  was  elicited  with  the  use  of  the  Goal  Question  Metric  approach.Results:  A  quantitative  metrics  model  containing  eight  rigorously  described  metrics  is  presented  and  followed  by  its  application  to  evaluate  an  agile  and  lean  transformation  in  a  large  international  telecommunication  organization  with  350  employees  in  two  sites.Conclusions:  The  metrics  model  was  sensitive  to  the  changes  that  occurred  in  the  organization  and  revealed  significant  improvements  in  six  of  the  eight  metrics  and  a  deterioration  in  one  of  the  metrics.
1	A  decision  support  framework  for  metrics  selection  in  goal  based  measurement  programs  gqm  dsfms.  Software  organizations  face  challenges  in  managing  and  sustaining  their  measurement  programs  over  time.  The  complexity  of  measurement  programs  increase  with  exploding  number  of  goals  and  metrics  to  collect.  At  the  same  time,  organizations  usually  have  limited  budget  and  resources  for  metrics  collection.  It  has  been  recognized  for  quite  a  while  that  there  is  the  need  for  prioritizing  goals,  which  then  ought  to  drive  the  selection  of  metrics.  On  the  other  hand,  the  dynamic  nature  of  the  organizations  requires  measurement  programs  to  adapt  to  the  changes  in  the  stakeholders,  their  goals,  information  needs  and  priorities.  Therefore,  it  is  crucial  for  organizations  to  use  structured  approaches  that  provide  transparency,  traceability  and  guidance  in  choosing  an  optimum  set  of  metrics  that  would  address  the  highest  priority  information  needs  considering  limited  resources.  This  paper  proposes  a  decision  support  framework  for  metrics  selection  (DSFMS)  which  is  built  upon  the  widely  used  Goal  Question  Metric  (GQM)  approach.  The  core  of  the  framework  includes  an  iterative  goal-based  metrics  selection  process  incorporating  decision  making  mechanisms  in  metrics  selection,  a  pre-defined  Attributes/Metrics  Repository,  and  a  Traceability  Model  among  GQM  elements.  We  also  discuss  alternative  prioritization  and  optimization  techniques  for  organizations  to  tailor  the  framework  according  to  their  needs.  The  evaluation  of  the  GQM-DSFMS  framework  was  done  through  a  case  study  in  a  CMMI  Level  3  software  company.
1	Non  parametric  statistical  fault  localization.  Abstract:  Fault  localization  is  a  major  activity  in  program  debugging.  To  automate  this  time-consuming  task,  many  existing  fault-localization  techniques  compare  passed  executions  and  failed  executions,  and  suggest  suspicious  program  elements,  such  as  predicates  or  statements,  to  facilitate  the  identification  of  faults.  To  do  that,  these  techniques  propose  statistical  models  and  use  hypothesis  testing  methods  to  test  the  similarity  or  dissimilarity  of  proposed  program  features  between  passed  and  failed  executions.  Furthermore,  when  applying  their  models,  these  techniques  presume  that  the  feature  spectra  come  from  populations  with  specific  distributions.  The  accuracy  of  using  a  model  to  describe  feature  spectra  is  related  to  and  may  be  affected  by  the  underlying  distribution  of  the  feature  spectra,  and  the  use  of  a  (sound)  model  on  inapplicable  circumstances  to  describe  real-life  feature  spectra  may  lower  the  effectiveness  of  these  fault-localization  techniques.  In  this  paper,  we  make  use  of  hypothesis  testing  methods  as  the  core  concept  in  developing  a  predicate-based  fault-localization  framework.  We  report  a  controlled  experiment  to  compare,  within  our  framework,  the  efficacy,  scalability,  and  efficiency  of  applying  three  categories  of  hypothesis  testing  methods,  namely,  standard  non-parametric  hypothesis  testing  methods,  standard  parametric  hypothesis  testing  methods,  and  debugging-specific  parametric  testing  methods.  We  also  conduct  a  case  study  to  compare  the  effectiveness  of  the  winner  of  these  three  categories  with  the  effectiveness  of  33  existing  statement-level  fault-localization  techniques.  The  experimental  results  show  that  the  use  of  non-parametric  hypothesis  testing  methods  in  our  proposed  predicate-based  fault-localization  model  is  the  most  promising.
1	Jstrace  fast  reproducing  web  application  errors.  Abstract      JavaScript  has  become  the  most  popular  language  for  client-side  web  applications.  Due  to  JavaScript's  highly-dynamic  and  event-driven  features,  it  is  challenging  to  diagnose  web  application  errors.  Record-replay  techniques  are  used  to  reproduce  errors  in  web  applications.  After  a  long  run,  these  techniques  will  record  a  long  event  trace  that  triggers  an  error.  Although  the  error-related  events  are  few,  they  are  interleaved  with  other  massive  error-irrelevant  events.  It  is  time-consuming  to  diagnose  errors  with  long  event  traces.    In  this  article,  we  present  JSTrace,  which  effectively  removes  error-irrelevant  events  from  the  long  event  trace,  and  further  facilitates  error  diagnosis.  Based  on  fine-grained  dependences  of  JavaScript  and  DOM  instructions,  we  develop  a  novel  dynamic  slicing  technique  that  can  remove  events  irrelevant  to  the  error.  We  further  present  rules  to  remove  irrelevant  events,  which  cannot  be  removed  by  dynamic  slicing.  In  this  process,  many  events  and  related  instructions  are  removed  without  losing  the  error  reproducing  accuracy.  Our  evaluation  on  13  real-world  web  application  errors  shows  that  the  reduced  event  traces  can  faithfully  reproduce  errors  with  an  average  reduction  rate  of  97%.  We  further  performed  case  studies  on  4  real-world  errors,  and  the  result  shows  that  JSTrace  is  useful  to  diagnose  web  application  errors.
1	An  exploratory  study  on  the  usage  of  common  interface  elements  in  android  applications.  The  number  of  mobile  applications  has  increased  drastically  in  the  past  few  years.  A  recent  study  has  shown  that  reusing  source  code  is  a  common  practice  for  Android  application  development.  However,  reuse  in  mobile  applications  is  not  necessarily  limited  to  the  source  code  (i.e.,  program  logic).  User  interface  (UI)  design  plays  a  vital  role  in  constructing  the  user-perceived  quality  of  a  mobile  application.  The  user-perceived  quality  reflects  the  users’  opinions  of  a  product.  For  mobile  applications,  it  can  be  quantified  by  the  number  of  downloads  and  raters.  In  this  study,  we  extract  commonly  used  UI  elements,  denoted  as  Common  Element  Sets  (CESs),  from  user  interfaces  of  applications.  Moreover,  we  highlight  the  characteristics  of  CESs  that  can  result  in  a  high  user-perceived  quality  by  proposing  various  metrics.  Through  an  empirical  study  on  1292  mobile  applications,  we  observe  that  (i)  CESs  of  mobile  applications  widely  occur  among  and  across  different  categories;  (ii)  certain  characteristics  of  CESs  can  provide  a  high  user-perceived  quality;  and  (iii)  through  a  manual  analysis,  we  recommend  UI  templates  that  are  extracted  and  summarized  from  CESs  for  developers.  Developers  and  quality  assurance  personnel  can  use  our  guidelines  to  improve  the  quality  of  mobile  applications.
1	Integration  between  requirements  engineering  and  safety  analysis.  We  analyze  activities  performed  in  safety  analysis  and  tool  support.We  present  challenges/problems  relating  to  the  integration  between  safety  and  RE.We  present  taxonomies  about  hazard  and  safety  analysis  techniques.Hazard  and  safety-related  information  are  classified  in  taxonomies.We  discuss  the  benefits  of  the  integration  between  RE  and  Safety  Engineering.  Context:  Safety-Critical  Systems  (SCS)  require  more  sophisticated  requirements  engineering  (RE)  approaches  as  inadequate,  incomplete  or  misunderstood  requirements  have  been  recognized  as  a  major  cause  in  many  accidents  and  safety-related  catastrophes.  Objective:  In  order  to  cope  with  the  complexity  of  specifying  SCS  by  RE,  we  investigate  the  approaches  proposed  to  improve  the  communication  or  integration  between  RE  and  safety  engineering  in  SCS  development.  We  analyze  the  activities  that  should  be  performed  by  RE  during  safety  analysis,  the  hazard/safety  techniques  it  could  use,  the  relationships  between  safety  information  that  it  should  specify,  the  tools  to  support  safety  analysis  as  well  as  integration  benefits  between  these  areas.  Method:  We  use  a  Systematic  Literature  Review  (SLR)  as  the  basis  for  our  work.  Results:  We  developed  four  taxonomies  to  help  RE  during  specification  of  SCS  that  classify:  techniques  used  in  (1)  hazard  analysis;  (2)  safety  analysis;  (3)  safety-related  information  and  (4)  a  detailed  set  of  information  regarding  hazards  specification.  Conclusions:  This  paper  is  a  step  towards  developing  a  body  of  knowledge  in  safety  concerns  necessary  to  RE  in  the  specification  of  SCS  that  is  derived  from  a  large-scale  SLR.  We  believe  the  results  will  benefit  both  researchers  and  practitioners.
1	Topic  based  software  defect  explanation.  Some  topics  are  more  defect-prone  than  others.Defect-prone  topics  are  likely  to  remain  so  over  time.Our  topic-based  metrics  provide  additional  defect  explanatory  to  baseline  metrics.Our  metrics  outperform  state-of-the-art  topic-based  cohesion  and  coupling  metrics.  Researchers  continue  to  propose  metrics  using  measurable  aspects  of  software  systems  to  understand  software  quality.  However,  these  metrics  largely  ignore  the  functionality,  i.e.,  the  conceptual  concerns,  of  software  systems.  Such  concerns  are  the  technical  concepts  that  reflect  the  systems  business  logic.  For  instance,  while  lines  of  code  may  be  a  good  general  measure  for  defects,  a  large  file  responsible  for  simple  I/O  tasks  is  likely  to  have  fewer  defects  than  a  small  file  responsible  for  complicated  compiler  implementation  details.  In  this  paper,  we  study  the  effect  of  concerns  on  software  quality.  We  use  a  statistical  topic  modeling  approach  to  approximate  software  concerns  as  topics  (related  words  in  source  code).  We  propose  various  metrics  using  these  topics  to  help  explain  the  file  defect-proneness.  Case  studies  on  multiple  versions  of  Firefox,  Eclipse,  Mylyn,  and  NetBeans  show  that  (i)  some  topics  are  more  defect-prone  than  others;  (ii)  defect-prone  topics  tend  to  remain  so  over  time;  (iii)  our  topic-based  metrics  provide  additional  explanatory  power  for  software  quality  over  existing  structural  and  historical  metrics;  and  (iv)  our  topic-based  cohesion  metric  outperforms  state-of-the-art  topic-based  cohesion  and  coupling  metrics  in  terms  of  defect  explanatory  power,  while  being  simpler  to  implement  and  more  intuitive  to  interpret.
1	On  the  relationships  between  qos  and  software  adaptability  at  the  architectural  level.  Modern  software  operates  in  highly  dynamic  and  often  unpredictable  environments  that  can  degrade  its  quality  of  service.  Therefore,  it  is  increasingly  important  having  systems  able  to  adapt  their  behavior.  However,  the  achievement  of  software  adaptability  can  influence  other  software  quality  attributes,  such  as  availability,  performance  or  cost.  This  paper  proposes  an  approach  for  analyzing  tradeoffs  between  the  system  adaptability  and  its  quality  of  service.  The  proposed  approach  is  based  on  a  set  of  metrics  that  allow  the  system  adaptability  evaluation.  The  approach  can  help  software  architects  to  guide  decisions  on  system  adaptation  for  fulfilling  system  quality  requirements.  The  application  and  effectiveness  of  the  approach  are  illustrated  through  examples  and  a  wide  set  of  experiments  carried  out  with  a  tool  we  have  developed.
1	Design  patterns  selection  an  automatic  two  phase  method.  Over  many  years  of  research  and  practices  in  software  development,  hundreds  of  software  design  patterns  have  been  invented  and  published.  Now,  a  question  which  naturally  arises  is  how  software  developers  select  the  right  design  patterns  from  all  relevant  patterns  to  solve  design  problems  in  the  software  design  phase.  To  address  this  issue,  in  this  paper,  we  propose  a  two-phase  method  to  select  a  right  design  pattern.  The  proposed  method  is  based  on  a  text  classification  approach  that  aims  to  show  an  appropriate  way  to  suggest  the  right  design  pattern(s)  to  developers  for  solving  each  given  design  problem.  There  are  two  advantages  of  the  proposed  method  in  comparison  to  previous  works.  First,  there  is  no  need  for  semi-formal  specifications  of  design  patterns  and  second,  the  suitable  design  patterns  are  suggested  with  their  degree  of  similarity  to  the  design  problem.  To  evaluate  the  proposed  method,  we  apply  it  on  real  problems  and  several  case  studies.  The  experimental  results  show  that  the  proposed  method  is  promising  and  effective.
1	Policy  based  awareness  management  pam  case  study  of  a  wireless  communication  system  at  a  hospital.  The  present  paper  evaluates  the  use  of  software  agents  to  identify  relevance  of  information,  called  awareness.  This  evaluation  is  based  on  existing  policies  and  scenarios  in  the  context  of  wireless  communication  of  a  hospital  in  Norway.  The  study  is  to  address  the  lack  of  literature  for  experimental  studies  on  a  method  to  employ  software  agents  for  awareness  identification.  Research  in  computer  supported  cooperative  work  indicates  the  significant  contributions  of  software  agents  to  assist  individuals.  There  are  bodies  of  work  that  show  awareness  provides  the  means  for  software  agents  in  which  effective  cooperation  can  take  place.  In  addition,  the  role  of  the  methods  to  identify  awareness  is  emphasized  in  the  literature  of  both  computer  supported  cooperative  work  and  software  agents.  This  paper  explains  a  step-wise  process,  called  Policy-based  Awareness  Management,  which  allows  agents  to  use  policies  as  a  source  to  identify  awareness  and  thus  change  their  behaviors  accordingly.  The  contribution  of  this  method  is  based  on  the  concepts  proposed  by  the  logic  of  general  awareness.  The  present  study  applies  Directory  Enabled  Networks-next  generation  as  the  policy  structure  for  the  method.  The  paper  evaluates  the  process  via  its  application  to  identify  the  relevance  of  information  in  wireless  communication  scenarios  in  a  hospital.  The  present  study  conducts  observations,  interviews  and  discussions  on  the  wireless  communication  system  of  the  hospital  to  identify  the  different  scenarios  happening  in  the  system.  The  paper  presents  a  set  of  simulations  on  these  scenarios  and  concludes  that  the  method  is  effective  and  cost-efficient.
1	Cala  classifying  links  automatically  based  on  their  url.  Abstract      Web  page  classification  refers  to  the  problem  of  automatically  assigning  a  web  page  to  one  or  more  classes  after  analysing  its  features.  Automated  web  page  classifiers  have  many  applications,  and  many  researchers  have  proposed  techniques  and  tools  to  perform  web  page  classification.  Unfortunately,  the  existing  tools  have  a  number  of  drawbacks  that  makes  them  unappealing  for  real-world  scenarios,  namely:  they  require  a  previous  extensive  crawling,  they  are  supervised,  they  need  to  download  a  page  before  classifying  it,  or  they  are  site-,  language-,  or  domain-dependent.  In  this  article,  we  propose  CALA,  a  tool  for  URL-based  web  page  classification.  The  strongest  features  of  our  tool  are  that  it  does  not  require  a  previous  extensive  crawling  to  achieve  good  classification  results,  it  is  unsupervised,  it  is  based  exclusively  on  URL  features,  which  means  that  pages  can  be  classified  without  downloading  them,  and  it  is  site-,  language-,  and  domain-independent,  which  makes  it  generally  applicable.  We  have  validated  our  tool  with  22  real-world  web  sites  from  multiple  domains  and  languages,  and  our  conclusion  is  that  CALA  is  very  effective  and  efficient  in  practice.
1	Investigating  security  threats  in  architectural  context.  Misuse  case  maps  (MUCM)  augment  use  case  maps  with  misuse  case  concepts.MUCMs  provide  integrated  views  of  security  issues  and  software  systems  architecture.MUCM  were  evaluated  in  controlled  experiments  with  complex  real-life  intrusions.Misuse  case  maps  lead  to  good  understanding  of  intrusions  and  ability  to  suggest  mitigations.Misuse  case  maps  were  perceived  more  positively  and  used  more  than  two  existing  techniques  used  as  alternative  treatment.  Many  techniques  have  been  proposed  for  eliciting  software  security  requirements  during  the  early  requirements  engineering  phase.  However,  few  techniques  so  far  provide  dedicated  views  of  security  issues  in  a  software  systems  architecture  context.  This  is  a  problem,  because  almost  all  requirements  work  today  happens  in  a  given  architectural  context,  and  understanding  this  architecture  is  vital  for  identifying  security  vulnerabilities  and  corresponding  mitigations.  Misuse  case  maps  attempt  to  provide  an  integrated  view  of  security  and  architecture  by  augmenting  use  case  maps  with  misuse  case  concepts.  This  paper  evaluates  misuse  case  maps  through  two  controlled  experiments  where  33  and  54  ICT  students  worked  on  complex  real-life  intrusions  described  in  the  literature.  The  students  who  used  misuse  case  maps  showed  significantly  better  understanding  of  intrusions  and  better  ability  to  suggest  mitigations  than  students  who  used  a  combination  of  two  existing  techniques  as  an  alternative  treatment.  Misuse  case  maps  were  also  perceived  more  favourably  overall  than  the  alternative  treatment,  and  participants  reported  using  misuse  case  maps  more  when  solving  their  tasks.
1	Modelling  equivalence  classes  of  feature  models  with  concept  lattices  to  assist  their  extraction  from  product  descriptions.  Abstract  Software  product  line  engineering  gathers  a  set  of  methods  to  help  create,  manage  and  maintain  a  collection  of  similar  software  systems.  Variability  modelling  is  a  focal  point  of  this  paradigm,  where  feature  models  (FMs)  are  the  prevalent  notation.  Migration  from  single  system  development  to  software  product  lines  is  a  spreading  topic  in  software  engineering.  To  ease  the  migration,  research  has  been  done  to  automatically  extract  FMs  from  software  descriptions,  but  most  of  these  approaches  are  defined  in  a  functional  manner  based  on  an  ad-hoc  variability  analysis.  In  this  paper,  we  propose  a  theoretical  view  on  FM  extraction  from  software  descriptions  based  on  Formal  Concept  Analysis  (FCA).  It  is  a  structural  framework  for  variability  representation  which  allows  to  lay  down  theoretical  foundation  to  variability  extraction.  We  propose  an  original  mapping  between  relationships  expressed  in  FMs  and  the  ones  emphasised  in  FCA  conceptual  structures.  We  show  that  conceptual  structures  represent  equivalence  classes  of  FMs  that  steer  the  user  choices  during  their  synthesis,  and  propose  a  reverse  engineering  method  based  on  them.  We  discuss  its  applicability  and  show  that  the  combinatorial  explosion  of  concept  lattices  can  be  avoided  by  the  use  of  two  sub-orders  embodying  the  necessary  information  concerning  variability.
1	The  effect  of  transactive  memory  systems  on  process  tailoring  in  software  projects  the  moderating  role  of  task  conflict  and  shared  temporal  cognitions.  Abstract  Contemporary  software  projects  are  unique  and  volatile,  leading  development  teams  to  modify  standard  development  processes  and  continue  to  make  adjustments  as  needed.  Adjusting  software  project  development  to  accommodate  the  variance  and  dynamics  is  called  software  process  tailoring  (SPT).  Because  SPT  critically  determines  how  projects  are  conducted,  its  performance  merits  investigation.  However,  the  extant  literature  lacks  empirical  evidence  of  the  underlying  effects  that  operate  and  influence  the  performance  of  SPT.  Specifically,  SPT  is  a  team-based  activity  that  requires  the  exchange  of  knowledge  and  opinions  among  members  to  yield  an  integrative  tailoring  solution;  SPT  is  also  a  highly  conflicting  process  involving  task  and  temporal  conflicts.  Given  these  characteristics,  teams’  operational  mechanisms  that  increase  SPT  performance  remain  unknown.  To  address  the  aforementioned  gaps,  this  study  adopts  the  transactive  memory  systems  (TMS)  theory  to  develop  a  research  model  to  explore  how  a  team's  TMS  affects  SPT  performance  with  task  conflict  and  shared  temporal  cognitions  (STC)  acting  as  moderators.  By  examining  102  software  project  teams,  we  found  that  TMS  has  a  positive  impact  on  SPT  performance.  Surprisingly,  task  conflict  reduces  the  effect  of  TMS  on  SPT  performance,  whereas  STC  amplifies  the  influence  of  TMS-SPT  performance.
1	Qmm  vanet  an  efficient  clustering  algorithm  based  on  qos  and  monitoring  of  malicious  vehicles  in  vehicular  ad  hoc  networks.  Abstract  Vehicular  ad  hoc  networks  (VANETs)  are  considered  as  a  subset  of  mobile  ad  hoc  networks  (MANETs)  that  can  be  used  in  the  transportation  field.  These  networks  considerably  improve  the  traffic  safety  and  accident  prevention.  Because  of  the  characteristics  of  VANETs  such  as  self-organization,  frequent  link  disconnections  and  rapid  topology  changes,  developing  efficient  routing  protocols  is  a  challenging  task.  To  address  this  issue,  clustering  is  an  appropriate  approach  in  a  mobile  environment.  Clustering  aims  to  partition  the  vehicles  into  a  number  of  clusters  based  on  some  predefined  metrics  such  as  velocity,  distance  and  location.  In  this  paper,  a  clustering  routing  protocol,  named  QMM-VANET,  which  considers  Quality  of  Service  (QoS)  requirements,  the  distrust  value  parameters  and  mobility  constraints,  is  proposed.  This  protocol  specifies  a  reliable  and  stable  cluster  and  increases  the  stability  and  connectivity  during  communications.  This  protocol  is  composed  of  three  parts:  (1)  computing  the  QoS  of  vehicles  and  electing  a  trustier  vehicle  as  a  cluster-head,  (2)  selecting  a  set  of  proper  neighboring  nodes  as  gateways  for  retransmitting  the  packets  and  (3)  using  gateway  recovery  algorithm  to  choose  another  gateway  in  case  of  failure  of  the  link.  NS-2  simulator  is  utilized  to  illustrate  the  performance  of  our  proposed  protocol  in  a  highway  scenario.  The  performance  analyses  display  that  the  QMM-VANET  protocol  can  achieve  low  end-to-end  delay  and  high  packet  delivery  ratio  and  maintain  the  network  stability.
1	Rigorous  component  based  system  design  using  the  bip  framework.  An  autonomous  robot  case  study  illustrates  the  use  of  the  behavior,  interaction,  priority  (BIP)  component  framework  as  a  unifying  semantic  model  to  ensure  correctness  of  essential  system  design  properties.
1	The  mobile  software  app  takeover.  Smartphones  aren't  very  &#x201C;smart&#x201D;  without  the  software  apps  that  give  them  their  usability  and  versatility.  Apps,  like  all  software,  need  some  degree  of  guidance,  regulation,  and  measurement  to  ensure  a  user  is  receiving  proper  functionality  and  quality  of  service.  This  problem  has  existed  in  software  engineering  and  software  development  from  Day  1,  and  apps  are  no  different.  The  void  has  led  to  a  recent  clamor  for  some  way  to  vet  apps  and  the  app  stores  in  which  they  will  reside  and  be  licensed.  The  guest  editors  of  this  special  issue  contend  that  this  will  be  a  huge  problem  for  the  mobile  app  market  as  it  continues  to  grow;  the  articles  they  feature  here  attempt  to  address  the  challenges  to  come.
1	Think  your  artificial  intelligence  software  is  fair  think  again.  Today,  machine-learning  software  is  used  to  help  make  decisions  that  affect  people's  lives.  Some  people  believe  that  the  application  of  such  software  results  in  fairer  decisions  because,  unlike  humans,  machine-learning  software  generates  models  that  are  not  biased.  Think  again.  Machine-learning  software  is  also  biased,  sometimes  in  similar  ways  to  humans,  often  in  different  ways.  While  fair  model-  assisted  decision  making  involves  more  than  the  application  of  unbiased  models-consideration  of  application  context,  specifics  of  the  decisions  being  made,  resolution  of  conflicting  stakeholder  viewpoints,  and  so  forth-mitigating  bias  from  machine-learning  software  is  important  and  possible  but  difficult  and  too  often  ignored.
1	Matching  open  source  software  licenses  with  corresponding  business  models.  When  open  source  stack  and  licensing  options  grow,  so  does  the  need  to  understand  the  interplay  among  licensing,  sourcing  decisions,  and  business  goals.
1	Guest  editors  introduction  software  engineering  for  the  cloud.  Cloud  computing  is  a  new  paradigm  for  software  systems  where  applications  are  divided  into  sets  of  composite  services  hosted  on  leased,  highly  distributed  platforms.  There  are  many  new  software  engineering  challenges  in  building  effective  cloud-based  software  applications.  This  special  issue  provides  a  set  of  practical  contributions  to  the  engineering  of  cloud  computing  applications  and  includes  software  processes,  architecture  and  design  approaches,  testing,  scalability  engineering,  security  engineering,  and  applications  of  highly  parallel  cloud-based  systems.
1	Lessons  from  developing  nonfunctional  requirements  for  a  software  platform.  Employing  a  software  platform  is  an  approach  to  achieve  a  higher  degree  of  software  reuse  by  enabling  multiple  software  products  to  share  the  platform-provided  services.  However,  platform  development  usually  involves  stakeholders  from  different  application  domains.  Their  application  situations  vary  widely  and  thus  nonfunctional  requirements  (NFRs)  for  the  software  platform  must  address  a  wider  range  of  needs  than  those  for  a  single  product.  This  article  describes  lessons  learned  in  developing  NFRs  for  a  large  software  platform,  the  challenging  issues,  and  the  techniques  used  to  address  them.  The  techniques  are  pragmatic  and  helped  with  NFR  reconciliation  and  management.  The  improved  quality  of  the  NFR  specifications  has  permitted  automation  of  platform  performance  testing  for  the  past  two  years.
1	Automated  synthesis  of  service  choreographies.  Future  Internet  research  promotes  the  production  of  a  distributed-computing  environment  that  will  be  increasingly  surrounded  by  a  virtually  infinite  number  of  software  services  that  can  be  composed  to  meet  user  needs.  Services  will  be  increasingly  active  entities  that,  communicating  peer-to-peer,  can  proactively  make  decisions  and  autonomously  perform  tasks.  Service  choreography  is  a  form  of  decentralized  service  composition  that  describes  peer-to-peer  message  exchanges  among  participant  services  from  a  global  perspective.  In  a  distributed  setting,  obtaining  the  coordination  logic  required  to  realize  a  choreography  is  nontrivial  and  error  prone.  So,  automatic  support  for  realizing  choreographies  is  needed.  For  this  purpose,  researchers  developed  a  choreography  synthesis  tool.  The  Web  extra  at  http://www.di.univaq.it/marco.autili/synthesis/shortdemo/demo.htm  is  a  short  demonstration  of  CHOReOSynt,  a  choreography  synthesis  tool.
1	Why  can  t  we  all  play  nice.  This  article  is  based  on  the  author's  research  in  stereotyping  and  collaboration&#x2014;the  two  opposing  forces  that  work  to  prevent  and  support  the  building  of  great  teams.  It  was  only  late  in  her  long  career  that  she  realized  how  important  the  &#x201C;people&#x201D;  side  is.  Tools,  programming  languages,  environments,  and  all  the  other  technical  stuff  are  important,  but  that  &#x201C;softer&#x201D;  side  can  be  really,  really  hard.
1	Run  time  variability  for  context  aware  smart  workflows.  In  variant-rich  workflow-based  systems,  a  major  concern  for  process  variability  is  the  context-aware  configuration  of  the  variants.  This  means  that  context  information,  not  users,  drives  process  configuration.  To  support  context-aware  process  configuration  in  a  dynamic  environment,  in  which  context  information  is  available  only  at  run  time,  smart  workflows  must  be  customized  at  run  time.  The  LateVa  (Late  Variability  for  Context-Aware  Smart  Workflows)  framework  lets  developers  model  and  manage  process  variability  by  composing  base  models,  fragments,  and  variability  models  and  by  deferring  binding  to  run  time.  Base  models  and  fragments  are  reusable,  thereby  reducing  the  modeling  effort  for  developing  variants.  LateVa  also  includes  an  automated  run-time-variability  mechanism  for  context-aware  smart  workflows.
1	Design  patterns  magic  or  myth.  A  mapping  study  of  design  pattern  literature  combined  with  two  follow-on  surveys  shows  only  limited  empirical  evidence  that  the  "Gang  of  Four"  patterns  provide  a  useful  way  of  transferring  design  knowledge  or  that  their  use  will  lead  to  better  designs.
1	25  years  of  software  engineering  in  brazil  beyond  an  insider  s  view.  The  software  engineering  area  is  facing  a  growing  number  of  challenges  due  to  the  continuing  increase  in  software  size  and  complexity.  The  challenges  are  addressed  by  the  very  relevant  and  high  quality  publications  of  the  Brazilian  Symposium  on  Software  Engineering  (SBES),  in  the  past  25  editions.  This  article  summarizes  the  findings  from  two  different  mapping  studies  about  these  25  SBES  editions.  It  also  reports  the  results  of  an  expert  opinion  survey  with  the  most  important  Brazilian  researchers  in  the  software  engineering  (SE)  area.  The  survey  reinforces  the  findings  of  the  mapping  studies.  It  also  provides  guidance  for  future  research.  In  addition,  the  studies  report  several  findings  that  confirmed  the  validity  of  the  research  methods  applied.  All  of  these  findings  are  important  input  to  the  current  Brazilian  SE  scenario.  Our  findings  also  suggest  that  greater  attention  should  be  given  to  the  SE  area,  by  improving  researchers'  interaction  with  industry  and  increasing  collaboration  between  researchers,  especially  internationally.
1	Service  oriented  approach  to  fault  tolerance  in  cpss.  Service-based  approach  to  build  dependable  and  resilient  cyber-physical  systems.Uses  middleware-based  RTSOA  for  heterogeneous  distributed  systems.Empirical  evaluation  of  the  fault  tolerance  capabilities  with  teleoperated  robot.Successfully  uses  services  as  aunit  of  fault  isolation  and  recovery.Resilience  supported  through  diversity  and  assessability.  Cyber-physical  systems  (CPSs)  are  open  and  interconnected  embedded  systems  that  control  or  interact  with  physical  processes.  Failures  in  CPSs  can  lead  to  loss  of  production  time,  damage  to  the  equipment  and  environment,  or  loss  of  life,  meaning  that  dependability  and  resilience  are  key  properties  for  their  design.  However,  existing  fault  tolerance  and  safety  approaches  are  inadequate  for  complex,  networked  and  dynamic  CPSs.  Service-orientation,  on  the  other  hand,  is  generally  considered  to  be  a  robust  architectural  style,  but  there  is  a  limited  amount  of  research  on  fault  tolerance  of  service-oriented  architecture  (SOA),  especially  on  distributed  real-time  systems.  We  propose  an  approach  that  utilizes  the  loosely  coupled  nature  of  services  to  implement  fault  tolerance  using  a  middleware-based  real-time  SOA  (RTSOA)  for  CPSs.  The  approach,  based  on  the  concepts  of  fault  isolation  and  recovery  at  the  service  level,  is  empirically  evaluated  using  a  demanding  bilateral  teleoperation  (remote  handling)  application.  The  empirical  evaluation  demonstrates  that  RTSOA  supports  real-time  fault  detection  and  recovery,  use  of  services  as  a  unit  of  fault  isolation,  and  it  provides  capability  to  implement  fault  tolerance  patterns  flexibly  and  without  significant  overhead.
1	Empirical  software  engineering  from  discipline  to  interdiscipline.  Abstract  Empirical  software  engineering  has  received  much  attention  in  recent  years  and  coined  the  shift  from  a  more  design-science-driven  engineering  discipline  to  an  insight-oriented,  and  theory-centric  one.  Yet,  we  still  face  many  challenges,  among  which  some  increase  the  need  for  interdisciplinary  research.  This  is  especially  true  for  the  investigation  of  social,  cultural  and  human-centric  aspects  of  software  engineering.  Although  we  can  already  observe  an  increased  recognition  of  the  need  for  more  interdisciplinary  research  in  (empirical)  software  engineering,  such  research  configurations  come  with  challenges  barely  discussed  from  a  scientific  point  of  view.  In  this  position  paper,  we  critically  reflect  upon  the  epistemological  setting  of  empirical  software  engineering  and  elaborate  its  configuration  as  an  Interdiscipline.  In  particular,  we  (1)  elaborate  a  pragmatic  view  on  empirical  research  for  software  engineering  reflecting  a  cyclic  process  for  knowledge  creation,  (2)  motivate  a  path  towards  symmetrical  interdisciplinary  research,  and  (3)  adopt  five  rules  of  thumb  from  other  interdisciplinary  collaborations  in  our  field  before  concluding  with  new  emerging  challenges.  This  supports  to  elevate  empirical  software  engineering  from  a  developing  discipline  moving  towards  a  paradigmatic  stage  of  normal  science  to  one  that  configures  interdisciplinary  teams  and  research  methods  symmetrically.
1	An  empirical  study  of  data  decomposition  for  software  parallelization.  Multi-core  programming  is  becoming  increasingly  important.Data  decomposition  is  a  key  challenge  during  parallelization  for  multi-core  CPUs.We  conduct  a  multi-method  study  to  better  understand  data  decomposition.We  derive  a  set  of  10  key  requirements  for  tools  to  support  parallelization.The  state-of-the-art  tooling  support  does  not  support  these  requirements.  Context:  Multi-core  architectures  are  becoming  increasingly  ubiquitous  and  software  professionals  are  seeking  to  leverage  the  capabilities  of  distributed-memory  architectures.  The  process  of  parallelizing  software  applications  can  be  very  tedious  and  error-prone,  in  particular  the  task  of  data  decomposition.  Empirical  studies  investigating  the  complexity  of  data  decomposition  and  communication  are  lacking.Objective:  Our  objective  is  threefold:  (i)  to  gain  an  empirical-based  understanding  of  the  task  of  data  decomposition  as  part  of  the  parallelization  of  software  applications;  (ii)  to  identify  key  requirements  for  tools  to  assist  developers  in  this  task,  and  (iii)  assess  the  current  state-of-the-art.Methods:  Our  empirical  investigation  employed  a  multi-method  approach,  using  an  interview  study,  participant-observer  case  study,  focus  group  study,  and  a  sample  survey.  The  empirical  investigation  involved  collaborations  with  three  industry  partners:  IBMs  High  Performance  Computing  Center,  the  Irish  Centre  for  High-End  Computing  (ICHEC),  and  JBA  Consulting.Results:  This  article  presents  data  decomposition  as  one  of  the  most  prevalent  tasks  of  parallelizing  applications  for  multi-core  architectures.  Based  on  our  studies,  we  identify  ten  key  requirements  for  tool  support  to  help  HPC  developers  in  this  area.  Our  evaluation  of  the  state-of-the-art  shows  that  none  of  the  extant  tool  support  implements  all  10  requirements.Conclusion:  While  there  is  a  considerable  body  of  research  in  the  area  of  HPC,  a  few  empirical  studies  exist  which  explicitly  focus  on  the  challenges  faced  by  practitioners  in  this  area;  this  research  aims  to  address  this  gap.  The  empirical  studies  in  this  article  provide  insights  that  may  help  researchers  and  tool  vendors  to  better  understand  the  needs  of  parallel  programmers.
1	A  multi  purpose  digital  image  watermarking  using  fractal  block  coding.  In  this  paper,  a  new  multi-purpose  watermarking  technique  is  presented  which  satisfies  both  verification  and  authentication  purposes  simultaneously  by  embedding  a  binary  watermark  into  the  image.  The  proposed  method  uses  a  special  type  of  fractal  block  coding  with  a  local  search  region  with  contrast  scaling  and  mean  of  range  block  as  its  parameters.  It  also  utilizes  Fuzzy  C-Mean  clustering  to  specify  the  watermark  bits.  To  overcome  the  high  computational  complexity  of  fractal  coding,  a  new  simple  coding  method  is  also  presented  which  improves  the  robustness  of  the  watermarking  and  decreases  the  run  time  of  fractal  block  coding  in  the  watermarking  procedure.  To  measure  the  fragility  and  robustness  of  the  method  to  signal  distortions  such  as  JPEG  compression,  median  filter,  and  additive  noise,  some  experiments  were  employed.  The  experimental  results  showed  that  the  proposed  method  has  provided  a  sensitive  authentication  and  a  reliable  verification.
1	Smart  a  novel  framework  for  addressing  range  queries  over  nonlinear  trajectories.  Abstract      A  spatiotemporal  database  is  a  database  that  manages  both  space  and  time  information.  Common  examples  include  tracking  of  moving  objects,  intelligent  transportation  systems,  cellular  communications  and  meteorology  monitoring.  A  spatiotemporal  query  determines  the  objects  included  in  a  region  at  a  specified  period  of  time  between  two  date-time  instants  referred  as  time  window.  In  the  context  of  this  work,  we  present  SMaRT:  A  novel    S  patiotemporal    M  ysql    R  e  T  rieval  framework,  based  on  MySQL  and  PostgreSQL  database  management  system.  Moreover,  we  propose  a  demo  user  interface  that  implements  all  of  its  capabilities,  in  order  to  help  user  determine  the  most  efficient  spatiotemporal  query  method  on  user-defined  2D  trajectories.  To  our  knowledge,  we  are  the  first  to  study  and  compare  methods  of  addressing  range  queries  on  nonlinear  moving  object  trajectories,  that  are  represented  both  in  dual  and  native  dimensional  space.  In  particular,  it  is  the  first  time  a  theoretically  efficient  dual  approach  was  implemented  for  nonlinear  trajectories  and  incorporated  into  a  well-known  open-source  RDBMS.  An  experimental  evaluation  is  included  that  shows  the  performance  and  efficiency  of  our  approach.
1	Quality  of  service  approaches  in  iot  a  systematic  mapping.  Abstract      In  an  Internet  of  Things  (IoT)  environment,  the  existence  of  a  huge  number  of  heterogeneous  devices,  which  are  potentially  resource-constrained  and/or  mobile  has  led  to  quality  of  service  (QoS)  concerns.  Quality  approaches  have  been  proposed  at  various  layers  of  the  IoT  architecture  and  take  into  consideration  a  number  of  different  QoS  factors.  This  paper  evaluates  the  current  state  of  the  art  of  proposed  QoS  approaches  in  the  IoT,  specifically:  (1)  What  layers  of  the  IoT  architecture  have  had  the  most  research  on  QoS?  (2)  What  quality  factors  do  the  quality  approaches  take  into  account  when  measuring  performance?  (3)  What  types  of  research  have  been  conducted  in  this  area?  We  have  conducted  a  systematic  mapping  using  a  number  of  automated  searches  from  the  most  relevant  academic  databases  to  address  these  questions.  This  mapping  has  identified  a  number  of  state  of  the  art  approaches  which  provides  a  good  reference  for  researchers.  The  paper  also  identifies  a  number  of  gaps  in  the  research  literature  at  specific  layers  of  the  IoT  architecture.  It  identifies  which  quality  factors,  research  and  contribution  facets  have  been  underutilised  in  the  state  of  the  art.
1	How  do  software  development  teams  manage  technical  debt  an  empirical  study.  Exploratory  case  study  with  empirical  data  from  eight  software  development  teams.Observation  of  various  different  strategies  for  technical  debt  management.Developed  technical  debt  management  framework.  Technical  debt  (TD)  is  a  metaphor  for  taking  shortcuts  or  workarounds  in  technical  decisions  to  gain  short-term  benefit  in  time-to-market  and  earlier  software  release.  In  this  study,  one  large  software  development  organization  is  investigated  to  gather  empirical  evidence  related  to  the  concept  of  technical  debt  management  (TDM).  We  used  the  exploratory  case  study  method  to  collect  and  analyze  empirical  data  in  the  case  organization  by  interviewing  a  total  of  25  persons  in  eight  software  development  teams.  We  were  able  to  identify  teams  where  the  current  strategy  for  TDM  was  only  to  fix  TD  when  necessary,  when  it  started  to  cause  too  much  trouble  for  development.  We  also  identified  teams  where  the  management  had  a  systematic  strategy  to  identify,  measure  and  monitor  TD  during  the  development  process.  It  seems  that  TDM  can  be  associated  with  a  similar  maturity  concept  as  software  development  in  general.  Development  teams  may  raise  their  maturity  by  increasing  their  awareness  and  applying  more  advanced  processes,  techniques  and  tools  in  TDM.  TDM  is  an  essential  part  of  sustainable  software  development,  and  companies  have  to  find  right  approaches  to  deal  with  TD  to  produce  healthy  software  that  can  be  developed  and  maintained  in  the  future.
1	Segmenting  large  traces  of  inter  process  communication  with  a  focus  on  high  performance  computing  systems.  An  approach  for  segmenting  traces  of  HPC  systems  is  proposed.  The  approach  fosters  the  automatic  detection  of  communication  patterns.  The  segmentation  mechanism  relies  on  a  technique  used  for  segmenting  DNA  sequences.  The  segmentation  process  is  applied  to  traces  of  hundreds  of  millions  of  events.  The  understanding  of  the  interactions  among  processes  of  a  High  Performance  Computing  (HPC)  system  can  be  made  easier  if  trace  analysis  is  used.  Traces,  however,  can  be  quite  large,  making  it  difficult  to  analyze  their  content  unless  some  abstraction  is  provided.  This  paper  presents  a  novel  trace  abstraction  approach  that  aims  to  facilitate  the  analysis  of  large  execution  traces  generated  from  HPC  applications.  Our  approach  allows  automatic  segmentation  of  large  traces  into  smaller  and  meaningful  clusters  that  reflect  the  various  execution  phases  of  the  traced  scenarios.  Our  approach  is  based  on  the  application  of  information  theory  principles  to  the  analysis  of  sequences  of  communication  patterns  extracted  from  traces  of  HPC  systems.  This  work  is  inspired  by  recent  studies  in  the  field  of  bioinformatics  where  several  techniques  have  been  proposed  to  segment  DNA  sequences  into  homogeneous  sub-domains,  where  each  sub-domain  exhibits  a  certain  degree  of  internal  homogeneity.  Trace  segments  can  be  used  in  a  number  of  applications  such  as  recovering  high-level  views  of  the  system  behavior  and  program  understanding.  We  demonstrate  the  usefulness  of  our  approach  by  applying  it  to  different  traces  of  hundreds  of  millions  of  events,  generated  from  two  HPC  systems.
1	Runtime  verification  of  train  control  systems  with  parameterized  modal  live  sequence  charts.  Abstract  With  the  growing  complexity  of  railway  control  systems,  it  is  required  to  preform  runtime  safety  checks  of  system  executions  that  go  beyond  conventional  runtime  monitoring  of  pre-programmed  safety  conditions.  Runtime  verification  is  a  lightweight  and  rigorous  formal  method  that  dynamically  analyses  execution  traces  against  some  formal  specifications.  A  challenge  in  applying  this  method  in  railway  systems  is  defining  a  suitable  monitoring  specification  language,  i.e.,  a  language  that  is  expressive,  of  reasonable  complexity,  and  easy  to  understand.  In  this  paper,  we  propose  parameterized  modal  live  sequence  charts  (PMLSCs)  by  introducing  the  alphabet  of  the  specification  into  charts  to  distinguish  between  silent  events  and  unexpected  events.  We  further  investigate  the  expressiveness  and  complexity  theories  of  the  language.  In  particular,  we  prove  that  PMLSCs  are  closed  under  negation  and  the  complexity  of  a  subclass  of  PMLSCs  is  linear,  which  allows  the  language  to  be  used  to  monitor  a  system  online.  Finally,  we  use  PMLSCs  to  monitor  an  RBC  system  in  the  Chinese  high-speed  railway  and  evaluate  the  performance.  The  experimental  results  show  that  the  PMLSC  has  high  monitoring  efficiency,  and  can  reduce  false  alarm  rate  by  introducing  alphabets  of  charts.
1	Security  analysis  of  image  cryptosystems  only  or  partially  based  on  a  chaotic  permutation.  The  paper  proposes  breaks  for  the  permutation  methods  adopted  in  the  chaos-based  image  cryptosystems.  By  a  careful  examination  on  the  most  chaotic  image  cryptosystems  we  can  find  that  the  permutation  process  constitute  the  main  step  or,  in  some  cases,  the  only  step  to  create  the  confusion.  It  can  be  applied  on  the  pixels  or  on  the  pixel  bits.  A  recently  proposed  image  encryption  scheme  based  on  shuffling  the  pixel  bits  inspired  from  other  works  is  treated  as  a  case  study.  By  applying  a  chosen  plaintext  attack,  we  demonstrate  that  a  hacker  can  determine  the  permutation  vectors  (matrixes)  used  to  permute  the  pixels  bits  or  the  pixels  themselves  and  exploit  them  to  reveal  the  plain  image.
1	Combinatorial  double  auction  based  resource  allocation  mechanism  in  cloud  computing  market.  Abstract  The  cloud  computing  environment  may  be  considered  as  market  for  computing  and  storage  resources.  Providers  rent  their  available  resources  in  the  form  of  Virtual  Machines  (VM)  and  charge  the  users  accordingly.  One  of  the  challenges  in  this  market  is  providing  a  mechanism  for  the  allocation  of  resources  and  their  pricing,  such  that  the  proper  benefit  of  both  users  and  providers  are  guaranteed.  In  this  paper,  a  combinatorial  double  auction-based  market  is  studied  in  which  a  broker  performs  the  allocation  of  the  providers’  VMs  according  to  the  users’  requests.  The  proposed  allocation  problem  is  formulated  as  an  integer  linear  programming  model  aiming  at  maximizing  the  total  profit  of  users  and  providers.  It  is  proved  that  the  proposed  model  satisfies  the  desirable  properties  including:  truthfulness,  fairness,  economic  efficiency  and  allocation  efficiency.  Furthermore,  due  to  the  high  complexity  of  the  proposed  model,  a  heuristic  resource  allocation  algorithm  with  a  quasi  linear  time  complexity  is  presented.  The  results  of  evaluations  confirm  the  good  agreement  of  the  heuristic  algorithm  with  the  optimization  model  in  terms  of  allocation  performance.  Moreover,  simulation  results  using  CloudSim  indicate  that,  compared  to  the  previous  works  in  literature,  the  proposed  algorithm  increases  the  profit  of  providers  and  users  and  reduces  the  resource  wastage.
1	A  survey  on  software  smells.  Abstract  Context  Smells  in  software  systems  impair  software  quality  and  make  them  hard  to  maintain  and  evolve.  The  software  engineering  community  has  explored  various  dimensions  concerning  smells  and  produced  extensive  research  related  to  smells.  The  plethora  of  information  poses  challenges  to  the  community  to  comprehend  the  state-of-the-art  tools  and  techniques.  Objective  We  aim  to  present  the  current  knowledge  related  to  software  smells  and  identify  challenges  as  well  as  opportunities  in  the  current  practices.  Method  We  explore  the  definitions  of  smells,  their  causes  as  well  as  effects,  and  their  detection  mechanisms  presented  in  the  current  literature.  We  studied  445  primary  studies  in  detail,  synthesized  the  information,  and  documented  our  observations.  Results  The  study  reveals  five  possible  defining  characteristics  of  smells  —  indicator,  poor  solution,  violates  best-practices,  impacts  quality,  and  recurrence.  We  curate  ten  common  factors  that  cause  smells  to  occur  including  lack  of  skill  or  awareness  and  priority  to  features  over  quality.  We  classify  existing  smell  detection  methods  into  five  groups  —  metrics,  rules/heuristics,  history,  machine  learning,  and  optimization-based  detection.  Challenges  in  the  smells  detection  include  the  tools’  proneness  to  false-positives  and  poor  coverage  of  smells  detectable  by  existing  tools.
1	Momm  multi  objective  model  merging.  Abstract      Nowadays,  software  systems  are  complex  and  large.  To  cope  with  this  situation,  teams  of  developers  have  to  cooperate  and  work  in  parallel  on  software  models.  Thus,  techniques  to  support  the  collaborative  development  of  models  are  a  must.  To  this  end,  several  approaches  exist  to  identify  the  change  operations  applied  in  parallel,  to  detect  conflicts  among  them,  as  well  as  to  construct  a  merged  model  by  incorporating  all  non-conflicting  operations.  Conflicts  often  denote  situations  where  the  application  of  one  operation  disables  the  applicability  of  another  one.  Consequently,  one  operation  has  to  be  omitted  to  construct  a  valid  merged  model  in  such  scenarios.  When  having  to  decide  which  operation  to  omit,  the  importance  of  its  application  has  to  be  taken  into  account  depending  on  the  operation  type  and  the  application  context.  However,  existing  works  treat  the  operations  to  merge  with  equal  importance.  We  introduce  in  this  paper,  for  the  first  time,  a  multi-objective  formulation  of  the  problem  of  model  merging,  based  on  NSGA-II,  that  aims  to  find  the  best  trade-off  between  minimizing  the  number  of  omitted  operations  and  maximizing  the  number  of  successfully  applied  important  operations.  We  evaluated  our  approach  using  seven  open  source  systems  and  compared  it  with  different  existing  model  merging  approaches.  The  merging  solutions  obtained  with  our  approach  were  found  in  all  of  the  scenarios  of  our  experiments  to  be  comparable  in  terms  of  minimizing  the  number  of  conflicts  to  those  suggested  by  existing  approaches  and  to  carry  a  high  importance  score  of  merged  operations.  Our  results  also  revealed  an  interesting  feature  concerning  the  trade-off  between  the  two  conflicting  objectives  that  demonstrates  the  practical  value  of  taking  the  importance  of  operations  into  account  in  model  merging  tasks.  In  fact,  the  shape  of  the  Pareto  front  represents  an  interesting  guidance  for  developers  to  select  best  solutions  based  on  their  preferences.
1	Finding  faults  a  scoping  study  of  fault  diagnostics  for  industrial  cyber  physical  systems.  Abstract  Context:  As  Industrial  Cyber–Physical  Systems  (ICPS)  become  more  connected  and  widely-distributed,  often  operating  in  safety-critical  environments,  we  require  innovative  approaches  to  detect  and  diagnose  the  faults  that  occur  in  them.  Objective:  We  profile  fault  identification  and  diagnosis  techniques  employed  in  the  aerospace,  automotive,  and  industrial  control  domains.  Each  of  these  sectors  has  adopted  particular  methods  to  meet  their  differing  diagnostic  needs.  By  examining  both  theoretical  presentations  as  well  as  case  studies  from  production  environments,  we  present  a  profile  of  the  current  approaches  being  employed  and  identify  gaps.  Methodology:  A  scoping  study  was  used  to  identify  and  compare  fault  detection  and  diagnosis  methodologies  that  are  presented  in  the  current  literature.  We  created  categories  for  the  different  diagnostic  approaches  via  a  pilot  study  and  present  an  analysis  of  the  trends  that  emerged.  We  then  compared  the  maturity  of  these  approaches  by  adapting  and  using  the  NASA  Technology  Readiness  Level  (TRL)  scale.  Results:  Fault  identification  and  analysis  studies  from  127  papers  published  from  2004  to  2019  reveal  a  wide  diversity  of  promising  techniques,  both  emerging  and  in-use.  These  range  from  traditional  Physics-based  Models  to  Data-Driven  Artificial  Intelligence  (AI)  and  Knowledge-Based  approaches.  Hybrid  techniques  that  blend  aspects  of  these  three  broad  categories  were  also  encountered.  Predictive  diagnostics  or  prognostics  featured  prominently  across  all  sectors,  along  with  discussions  of  techniques  including  Fault  trees,  Petri  nets  and  Markov  approaches.  We  also  profile  some  of  the  techniques  that  have  reached  the  highest  Technology  Readiness  Levels,  showing  how  those  methods  are  being  applied  in  real-world  environments  beyond  the  laboratory.  Conclusions:  Our  results  suggest  that  the  continuing  wide  use  of  both  Model-Based  and  Data-Driven  AI  techniques  across  all  domains,  especially  when  they  are  used  together  in  hybrid  configuration,  reflects  the  complexity  of  the  current  ICPS  application  space.  While  creating  sufficiently-complete  models  is  labor  intensive,  Model-free  AI  techniques  were  evidenced  as  a  viable  way  of  addressing  aspects  of  this  challenge,  demonstrating  the  increasing  sophistication  of  current  machine  learning  systems.  Connecting  ICPS  together  to  share  sufficient  telemetry  to  diagnose  and  manage  faults  is  difficult  when  the  physical  environment  places  demands  on  ICPS.  Despite  these  challenges,  the  most  mature  papers  present  robust  fault  diagnosis  and  analysis  techniques  which  have  moved  beyond  the  laboratory  and  are  proving  valuable  in  real-world  environments.
1	Distribution  data  deployment  software  architecture  convergence  in  big  data  systems.  Exponential  data  growth  from  the  Internet,  low-cost  sensors,  and  high-fidelity  instruments  have  fueled  the  development  of  advanced  analytics  operating  on  vast  data  repositories.  These  analytics  bring  business  benefits  ranging  from  Web  content  personalization  to  predictive  maintenance  of  aircraft  components.  To  construct  the  data  repositories  underpinning  these  systems,  rapid  innovation  has  occurred  in  distributed-data-management  technologies,  employing  schemaless  data  models  and  relaxing  consistency  guarantees  to  satisfy  scalability  and  availability  requirements.  These  big  data  systems  present  many  challenges  to  software  architects.  Distributed-software  architecture  quality  attributes  are  tightly  linked  to  both  the  data  and  deployment  architectures.  This  causes  a  consolidation  of  concerns,  and  designs  must  be  closely  harmonized  across  these  three  architectures  to  satisfy  quality  requirements.
1	Is  it  worth  responding  to  reviews  studying  the  top  free  apps  in  google  play.  Up  to  this  point,  researchers  have  not  explored  the  value  of  responding  to  user  reviews  of  mobile  apps.  An  analysis  of  reviews  and  responses  for  10,713  of  the  top  apps  in  Google  Play  showed  that  few  developers  responded  to  reviews.  However,  responding  can  have  positive  effects.  Users  changed  their  ratings  38.7  percent  of  the  time  following  a  response,  with  a  median  rating  increase  of  20  percent.
1	What  we  know  about  software  test  maturity  and  test  process  improvement.  In  many  companies,  software  testing  practices  and  processes  are  far  from  mature  and  are  usually  conducted  in  an  ad  hoc  fashion.  Such  immature  practices  lead  to  negative  outcomes—for  example,  testing  that  doesn’t  detect  all  the  defects  or  that  incurs  cost  and  schedule  overruns.  To  conduct  test  maturity  assessment  (TMA)  and  test  process  improvement  (TPI)  systematically,  researchers  and  practitioners  have  proposed  various  approaches  and  frameworks.  Motivated  by  a  recent  industrial  project  in  TMA  and  TPI  and  wanting  to  identify  the  state  of  the  art  and  practice  in  this  area,  researchers  conducted  a  review  of  both  the  scientific  literature  and  practitioners’  gray  literature  (for  example,  blog  posts).  The  review  identified  58  test  maturity  models  and  many  sources  with  varying  degrees  of  empirical  evidence.  The  review’s  results  can  serve  as  an  evidence-based  overview  of  and  index  to  the  vast  body  of  knowledge  in  this  important,  fast-growing  area.  Using  this  knowledge,  both  researchers  and  practitioners  should  be  able  to  assess  and  improve  the  maturity  of  test  processes.
1	Collaborative  repositories  in  model  driven  engineering.  Recently  proposed  model  repositories  aim  to  support  specific  needs--for  example,  collaborative  modeling,  the  ability  to  use  different  modeling  tools  in  software  life-cycle  management,  tool  interoperability,  increased  model  reuse,  and  the  integration  of  heterogeneous  models.
1	Database  refactoring  lessons  from  the  trenches.  Although  database  refactoring  has  been  advocated  as  an  important  area  of  database  development,  little  research  has  studied  its  implications.  A  small  software  development  firm  refactored  a  database  related  to  an  application  that  lets  clients  optimize  their  logistics  processes.  This  project  was  based  on  the  design  of  clear  database  development  conventions  and  the  need  to  package  documentation  in  the  database  itself.  The  experience  led  to  five  key  lessons  learned:  refactoring  should  be  automated  whenever  possible,  the  database  catalog  is  crucial,  refactoring  is  easier  when  it's  done  progressively,  refactoring  can  help  optimize  an  application  and  streamline  its  code  base,  and  refactoring  related  to  application  development  requires  a  complex  skill  set  and  must  be  applied  sensibly.  This  article  is  part  of  a  special  issue  on  Refactoring.
1	The  runtime  performance  of  invokedynamic  an  evaluation  with  a  java  library.  The  Java  7  platform  includes  the  invokedynamic  opcode  in  its  virtual  machine,  a  feature  that  lets  programmers  define,  and  dynamically  change,  the  linkage  of  method  call  sites  and  thereby  maintain  platform  optimizations.  The  authors  developed  a  library  that  lets  developers  use  this  new  JVM  feature,  present  a  comprehensive  evaluation  of  its  performance,  and  describe  how  to  use  the  developed  library  to  optimize  real  Java  applications,  including  two  mature  dynamic  languages.
1	Analyzing  ad  library  updates  in  android  apps.  Because  more  than  90  percent  of  mobile  apps  are  free,  advertising  on  them  is  a  key  revenue  source  for  their  developers.  Advertisements  are  served  on  apps  through  embedded  specialized  code  called  ad  libraries.  Unlike  with  other  types  of  libraries,  app  developers  can't  ignore  new  ad  libraries  or  new  versions  of  embedded  ad  libraries  without  risking  revenue  loss.  However,  updating  ad  libraries  incurs  costs,  which  can  become  problematic  as  these  updates  become  more  frequent.  Researchers  investigated  the  costs  of  updating  ad  libraries  and  explored  the  frequency  of  ad  library  updates  in  Android  apps.  An  analysis  of  numerous  versions  of  Android  apps  over  12  months  showed  that  almost  half  underwent  ad  library  updates  (an  ad  library  was  added,  removed,  or  updated).  Moreover,  in  nearly  14  percent  of  the  app  updates  with  at  least  one  ad  library  update,  no  changes  to  the  app's  API  occurred.  This  suggests  that  maintaining  the  ad  libraries  entailed  substantial  additional  effort  for  the  developers.
1	On  the  definition  of  microservice  bad  smells.  Code  smells  and  architectural  smells  (also  called  bad  smells)  are  symptoms  of  poor  design  that  can  hinder  code  understandability  and  decrease  maintainability.  Several  bad  smells  have  been  defined  in  the  literature  for  both  generic  architectures  and  specific  architectures.  However,  cloud-native  applications  based  on  microservices  can  be  affected  by  other  types  of  issues.  In  order  to  identify  a  set  of  microservice-specific  bad  smells,  researchers  collected  evidence  of  bad  practices  by  interviewing  72  developers  with  experience  in  developing  systems  based  on  microservices.  Then,  they  classified  the  bad  practices  into  a  catalog  of  11  microservice-specific  bad  smells  frequently  considered  harmful  by  practitioners.  The  results  can  be  used  by  practitioners  and  researchers  as  a  guideline  to  avoid  experiencing  the  same  difficult  situations  in  the  systems  they  develop.
1	Making  continual  code  quality  monitoring  and  control  processes  work  in  a  global  delivery  organization  cosmos.  Continual  monitoring  and  model  driven  quality  improvement  methods  are  becoming  critical  for  product  quality  within  software  development  organizations.  But,  setting  up  such  procedures  within  a  global  delivery  organization  poses  challenges  like  -  variation  in  the  engagement  models  across  different  client  projects,  variations  in  build  strategies  and  code  ownership  model,  process  related  hindrances  like  overlapping  quality  processes  of  the  delivery  and  the  product  organizations.  Moreover,  these  challenges  emerge  overtime  as  the  processes  become  more  understood  and  accepted.  We  present  evolution  and  design  of  COSMOS  toolkit  which  exemplifies  and  addresses  such  variations  in  the  delivery  projects  and  provides  a  consistent  continual  reporting  for  organizations.  The  toolkit  has  become  a  primary  enabler  for  adoption  of  a  continual  quality  control  process  and  CQMM,  a  model  based  quality  improvement  process.  It  addresses  among  others,  key  concerns  like  tools  &  build  variability,  legacy  code  &  code  ownership,  model  based  rules  selection  &  reporting,  variations  in  applicable  thresholds.
1	A  systematic  tertiary  study  of  communication  in  distributed  software  development  projects.  Distributed  Software  Development  (DSD)  is  characterized  by  physical  distance  and/or  different  time-zones  among  those  involved  in  the  process  of  developing  software.  This  physical  and  temporal  separation  entails  some  advantages,  but  it  also  brings  challenges  related  to  communication,  coordination  and  cooperation  in  carrying  out  complex  software  development  tasks.  Cultural  and  time-zone  differences,  lack  of  standardized  processes,  tools  and  infrastructure  incompatibility  are  examples  of  such  challenges.  Communication  is  among  one  of  the  main  challenges  faced  by  distributed  teams.  Therefore,  this  study  aims  at  moving  towards  a  consolidated  knowledge  about  communication  in  distributed  projects  by  developing  a  better  understanding  of  which  factors  influence  communication  processes  and  which  are  the  reported  effects  of  this  influence  in  DSD  projects.  To  attend  our  goal,  we  conducted  a  systematic  tertiary  literature  review  of  communication  in  distributed  projects.  The  research  questions  that  guided  our  study  are:  Q.1  Which  factors  influence  communication  in  Distributed  Software  Development  projects?;  Q.2  Which  are  the  effects  of  these  factors  in  communication  in  Distributed  Software  Development  projects?;  and  Q.3  Which  factors  identified  in  Q.1  are  related  to  the  effects  identified  in  Q.2  in  Distributed  Software  Development  projects?  We  briefly  present  below  the  result  of  the  identification,  analysis,  and  synthesis  of  the  effects  of  communication  process  in  DSD  projects  and  the  factors  in  the  distributed  context  that  affect  communication  effectiveness.
1	Game  governance  for  agile  management  of  enterprises  a  management  model  for  agile  governance.  Agility  at  the  business  and  organizational  levels  presents  a  challenge  for  many  enterprises.  Business  agility  demands  the  ability  to  sense  and  respond  to  changes  in  competitive  environments,  whereas  organizational  agility  demands  the  dexterity  to  sense  broader  market  opportunities  and  respond  with  changes  that  are  organization-wide.  These  challenges  require  an  information  and  communication  technologies  (ICT  or  IT)  environment  flexible  and  customizable  simultaneously  with  the  coordination  across  multiple  organization  units,  also  demands  effective  and  responsive  governance  in  order  to  deliver  faster,  better,  and  cheaper  value  to  the  business.  Driven  by  these  challenges,  the  goals  of  the  candidate's  PhD  thesis  is  to  conceive,  define,  and  evaluate  a  management  model  for  agile  governance  on  global  development  environments.  Preliminary  insights  suggest  the  model's  format  should  be  a  corporative  game,  applying  constructs  of  gamification,  fun  theory  and  game  theory,  generating  a  practical  approach  to  facilitate  the  implementation  of  a  collaborative  and  adaptive  culture,  in  order  to  establish  relational  integration  mechanisms  as  well  as  a  better  understanding  of  how  these  arrangements  can  help  the  organizations  to  attain  greater  enterprise  agility  and  support  its  overall  strategy.
1	A  critical  evaluation  of  failure  in  a  nearshore  outsourcing  project  what  dilemma  analysis  can  tell  us.  Global  Software  Engineering  (GSE)  research  contains  few  examples  consciously  applying  what  Glass  and  colleagues  have  termed  an  'evaluative-critical'  approach.  In  this  study  we  apply  dilemma  analysis  to  conduct  a  critical  review  of  a  major  (and  ongoing)  near  shore  Business  Process  Outsourcing  project  in  New  Zealand.  The  project  has  become  so  troubled  that  a  Government  Minister  has  recently  been  assigned  responsibility  for  troubleshooting  it.  The  'Novo  pay'  project  concerns  the  implementation  of  a  nationwide  payroll  system  responsible  for  the  payment  of  some  110,000  teachers  and  education  sector  staff.  An  Australian  company  won  the  contract  for  customizing  and  implementing  the  Novo  pay  system,  taking  over  from  an  existing  New  Zealand  service  provider.  We  demonstrate  how  a  modified  form  of  dilemma  analysis  can  be  a  powerful  technique  for  highlighting  risks  and  stakeholder  impacts  from  empirical  data,  and  that  adopting  an  evaluative-critical  approach  to  such  projects  can  usefully  highlight  tensions  and  barriers  to  satisfactory  project  outcomes.
1	Process  focused  lessons  learned  from  a  multi  site  development  project  at  daimler  trucks.  Daimler  Trucks,  being  a  very  internationally  oriented  business  unit  within  Daimler  AG,  is  in  the  challenging  position  to  manage  multi-site  development  projects.  Therefore,  the  Engine  Control  Unit  including  comprehensive  embedded  Software  has  to  be  developed  for  several  markets,  brands  and  engines.  One  challenge  during  this  project  included  setting  up  a  development  process,  updating  this  process  continuously  and  training  it  on  the  different  sites.  In  this  paper,  the  decision  for  the  current  development  process  will  be  shown.  Also,  the  need  and  reasoning  to  update  this  process  for  example  during  an  improvement  cycle  will  be  demonstrated.  The  second  part  of  this  paper  will  focus  on  the  establishing  and  training  of  these  processes  and  process  improvements.  Especially  the  lessons  learned  within  this  project  lasting  several  years  will  be  pointed  out.
1	The  impact  of  multi  site  software  governance  on  knowledge  management.  Software  Development  Governance  (SDG)  is  an  emerging  field  of  research,  under  the  umbrella  of  information  technology  governance.  SDG  challenges  increase  when  software  development  activities  are  distributed  across  multiple  locations.  Coordination  of  knowledge  management  processes  requires  specific  attention  in  multi-site  development.  This  paper  outlines  a  multi-site  software  governance  structure,  based  on  three  aspects:  the  business  strategy  that  binds  the  relationship  of  the  remote  offices,  the  structure  and  composition  of  the  remote  teams  and  the  way  tasks  are  allocated  across  sites.  Knowledge  management  processes  (including  knowledge  creation,  knowledge  transfer  and  communication)  are  identified  and  the  influence  of  different  governance  structures  on  these  processes  is  discussed.  We  do  so  through  a  case  study  at  Oce,  a  multi-national  company  in  printing  systems.
1	Integrating  global  sites  into  the  lean  and  agile  transformation  at  ericsson.  Transforming  a  large  organization  from  a  plan-driven  process  to  agile  development  is  challenging.  Despite  this,  large  organizations  are  increasingly  adopting  agile  development  and  lean  thinking.  However,  there  is  little  research  on  how  to  conduct  a  successful  transformation  in  large  organizations,  which  often  are  globally  distributed.  In  this  paper  we  discuss  how  one  R&D  unit  of  Ericsson  integrated  three  global  sites  into  their  lean  and  agile  transformation  involving  400  persons  in  Finland,  Hungary  and  the  US.  We  describe  the  challenges  and  success  factors  in  integrating  the  global  sites.  We  collected  the  data  by  45  semi-structured  interviews  and  longitudinally  observing  the  transformation  during  over  20  site  visits.  Success  factors  include:  early  and  broad  involvement  of  global  sites  at  all  organizational  levels,  constant  communication  and  cross-site  visits,  and  creation  of  joint  infrastructure.  The  challenges  include:  creating  a  shared  understanding  of  the  change,  enabling  end-to-end  development,  bridging  cultural  differences  and  creating  transparency.
1	Communication  between  developers  and  testers  in  distributed  continuous  agile  testing.  Software  developers  and  testers  have  to  work  together  to  achieve  the  goals  of  software  development  projects.  In  globally  distributed  software  projects  the  development  and  testing  are  often  scattered  across  multiple  locations  forming  virtual  teams.  Further,  the  distributed  projects  are  so  complex  that  none  of  team  members  can  possibly  possess  all  the  knowledge  about  the  project  individually.  During  testing  in  such  teams,  developers  and  testers  need  to  coordinate  and  communicate  frequently.  However,  coordination  is  affected  by  the  availability  of  the  project  information,  which  is  distributed  among  different  project  members  and  organizational  structures.  Many  companies  are  facing  decisions  about  how  to  apply  agile  methods  in  their  distributed  projects.  These  companies  are  often  motivated  by  the  opportunities  of  solving  the  coordination  and  communication  difficulties  associated  with  global  software  development.  In  this  paper  we  investigate  the  communication  between  testers  and  developers  in  two  teams  from  two  software  companies  performing  continuous  agile  testing  in  a  distributed  setting.  We  describe  four  communication  practices  used  by  the  team:  handover  through  issue  tracker  system,  formal  meetings,  written  communication  and  coordination  by  mutual  adjustment.  We  also  discuss  communication  between  testers  and  developers  in  collocated  versus  distributed  testers  and  developers.  We  have  found  that  early  participation  of  the  testers  is  very  important  to  the  success  of  the  handover  between  testers  and  developers.  The  communication  between  developers  and  testers  is  not  sufficiently  effective  through  written  communication  and  that  it  changes  depending  on  the  type  of  the  tasks  and  experience  of  the  testers.
1	Replicating  an  onshore  capstone  computing  project  in  a  farshore  setting  an  experience  report.  The  internationalization  of  tertiary  education  is  giving  rise  to  a  number  of  'export  education'  models,  but  cross-cultural  collaborative  educational  programmes  pose  several  challenges.  Most  undergraduate  computing  degrees  include  a  capstone  project  as  a  means  of  consolidating  student  learning  within  an  integrative  final  learning  experience,  which  aims  to  prepare  them  for  further  study  or  varying  forms  of  professional  practice.  Typically  these  projects  are  coordinated  and  conducted  in  an  'onshore'  setting,  although  notable  exceptions  do  exist.  We  report  our  early  experiences  and  the  key  decisions  taken  in  replicating  an  'onshore'  capstone  course  from  a  New  Zealand  setting,  to  a  full  student  cohort  studying  our  Bachelor  of  Computer  and  Information  Sciences  in  a  'farshore'  context  in  Vietnam.  We  explore  the  myriad  of  issues  relating  to  the  oversight,  design  and  structuring  of  the  course,  when  tailoring  an  inherently  open  ended  team  based  project  to  a  radically  different  remote  setting.
1	Fake  smile  detection  using  linear  support  vector  machine.  Fake  smile  is  an  emotional  sign  on  the  face  that  can  be  used  as  information  for  non-verbal  communication.  One  of  its  functions  is  for  lie  detection  purpose  based  on  the  information  of  emotional  sign  generated  on  the  face.  The  emergence  of  fake  smile  indicates  that  there  are  negative  emotions,  uncomfortable  feeling,  and  something  hidden  in  a  person.  This  research  aims  to  detect  fake  smile.  In  fact,  real  smile  is  characterized  by  the  contraction  of  zygomatic  major  muscle  on  the  edge  of  mouth  corner  and  obicularis  oculli  muscle  on  the  eyelids.  However,  on  a  fake  smile,  zygomatic  major  muscle  experiences  contraction,  but  obicularis  oculli  muscle  doesn't  contract.  Contraction  of  the  zygomatic  major  muscle  is  identified  by  the  appearance  of  wrinkles  on  the  cheeks  corner  of  the  mouth,  whereas  obicularis  oculli  contraction  is  identified  by  the  feature  value  of  eye  elongation.  On  the  test  image,  segmentation  of  RoI  (Region  of  Interest)  is  done  on  cheeks  and  eyes.  On  the  RoI  (Region  of  Interest)  cheeks,  wrinkle  density  is  calculated;  whereas  elongation  value  is  calculated  on  the  RoI  (Region  of  Interest)  eyes.  Based  on  the  two  variables  above,  with  support  vector  machine  linear  for  its  classification,  smile  is  classified  into  two  classes,  i.e.  real  smile  and  fake  smile.  The  test  result  showed  that  the  accuracy  of  system  is  86  %,  whereas  the  error  rate  is  14%.
1	Modeling  the  requirements  for  big  data  application  using  goal  oriented  approach.  In  this  research,  we  propose  generic  requirement  model  for  Big  Data  application.  The  model  offer  the  use  of  i×  Framework  and  Knowledge  Acquisition  automated  System  (KAOS),  which  are  part  of  Goal  Oriented  Requirement  Engineering  (GORE).  Big  Data  applications  handle  flood  of  data  that  occurs  from  anything  such  as  climate  data,  genomes,  even  just  software  logs  or  facebook  status.  To  build  such  applications  demands  gathering  special  requirements  specific  for  Big  Data.  A  generic  requirement  model  is  proposed  using  i×  and  KAOS  model.  The  model  is  constructed  from  analyzing  the  requirements  based  on  the  characteristics  of  Big  Data  and  its  challenges.  This  generic  model  is  then  applied  to  a  case  study  in  an  Indonesian's  government  agency  for  development  planning  of  West  Java  (UPTB  Pusdalisbang  Jawa  Barat).  The  result  of  this  application  has  demonstrated  that  the  model  can  be  used  to  generate  a  valid  software  requirement  specification.
1	Overlay  cloud  networking  through  meta  code.  We  propose  a  new  structuring  mechanism  and  architecture  for  construction  of  overlay  networks  spanning  multiple  clouds.  We  have  designed  and  implemented  Suorgi  for  this  purpose,  a  run-time  that  supports  extensibility  of  cloud  infrastructures.  Novice  users  can  now  construct  their  private  cloud  overlay  networks  without  explicit  programming  knowledge.  Key  to  support  this  is  meta-code,  preprogrammed  code  modules  extending  and  augmenting  the  cloud  infrastructure  to  fit  specific  user  needs.  Meta-code  can  implement,  for  instance,  fine-level  trust  policies,  replication  management,  auditing,  and  provenance  mechanisms.
1	An  architecture  for  cloud  service  testing  and  real  time  management.  Cloud  service  testing  ensures  that  services  run  properly  and  meet  the  Service  Level  Agreement  (SLA)  requirements.  However,  performance  problems  of  a  cloud  service,  such  as  the  availability  and  reliability,  are  difficult  to  diagnose  because  these  issues  might  be  caused  from  different  system  components.  To  solve  these  problems,  this  study  proposes  an  architecture  for  testing  environment  configuration  and  quality  estimation  for  both  of  fault  diagnosis  and  bottleneck  detection.  The  proposed  system  has  two  components:  offline  testing  and  online  management.  In  offline  testing,  target  service  are  tested  by  using  the  proposed  testing  module  and  corresponding  metrics  are  collected  for  further  analysis.  The  analyzed  results  which  are  separated  between  fault  diagnosis  and  bottleneck  detection  will  be  stored  in  knowledge  databases.  Finally,  online  management  module  will  automatically  suggest  how  to  do  when  facing  this  kind  of  problem  according  to  knowledge  based  diagnosis.
1	Detecting  code  injection  attacks  in  internet  explorer.  Code  injection  vulnerabilities  are  a  major  threat  to  Internet  security.  The  ability  for  a  malicious  website  to  install  malware  on  a  host  using  these  vulnerabilities  without  its  knowledge  is  particularly  menacing.  In  this  paper,  we  approach  this  problem  from  a  new  perspective  by  constructing  a  Markov  chain  graph  from  the  system  calls  Internet  Explorer  executes  and  then  modeling  this  graph  over  time.  We  apply  a  Gaussian  process  change-point  algorithm  to  detect  code  injection  attacks.  To  show  the  efficacy  of  this  approach,  we  collect  a  novel  dataset  of  system  call  traces  of  6  code  injection  attacks  using  3  distinct  exploits  against  the  Internet  Explorer  browser.  Our  algorithm  was  able  to  detect  all  of  the  code  injection  attacks  with  a  limited  number  of  false  positives.
1	Consview  towards  application  specific  consistent  context  views.  Detecting  and  resolving  context  inconsistency  is  critical  to  pervasive  computing  applications  and  infrastructures.  Context  inconsistency  occurs  when  an  application  perceives  contexts  that  breach  predefined  consistency  constraints.  This  can  drive  an  application  to  behave  abnormally  or  even  cause  failure.  Existing  work  commonly  assumes  the  presence  of  a  single  application  suffering  from  context  inconsistency,  such  that  specific  repair  actions  can  be  taken  to  resolve  the  inconsistency  for  this  application.  However,  when  multiple  applications  run  on  the  same  infrastructure,  they  may  impose  conflicting  requirements  on  resolving  context  inconsistency.  In  this  paper,  we  propose  a  novel  view-based  approach  ConsView  to  address  such  conflicting  requirements.  In  ConsView,  each  application  has  a  specific  view  to  its  own  contexts  that  satisfy  its  own  requirement  on  resolving  context  inconsistency.  Such  views  are  called  consistent  context  views.  We  discuss  the  challenges  of  doing  so  and  our  ideas  for  addressing  them.  We  implemented  a  prototype  infrastructure  supporting  consistent  context  views,  and  evaluated  it  experimentally  with  simulated  applications  of  real-life  settings.  The  results  confirmed  the  effectiveness  and  efficiency  of  our  ConsView  approach.
1	Saps  software  defined  network  aware  pub  sub  a  design  of  the  hybrid  architecture  utilizing  distributed  and  centralized  multicast.  Pub/Sub  communication  model  becomes  a  basis  of  various  applications,  e.g.  IoT/M2M,  SNS.  These  application  domains  require  new  properties  of  the  Pub/Sub  infrastructure,  for  example,  supporting  a  large  number  of  devices  with  widely  distributed  manner,  handling  emergency  messaging  with  priority  control  and  so  on.  In  order  to  meet  the  demands,  we  proposed  Software  Defined  Network  Aware  Pub/Sub  (SAPS)  which  utilize  the  both  Application  Layer  Multicast  (ALM)  and  SDN,  especially  Open  Flow  based  multicast  (OFM).  A  simulation  was  done  for  evaluating  the  hybrid  architecture  in  traffic  and  transmission  delay  reduction,  and  then  the  issues  to  be  solved  in  the  current  design  were  discussed.
1	Injecting  comments  to  detect  javascript  code  injection  attacks.  Most  web  programs  are  vulnerable  to  cross  site  scripting  (XSS)  that  can  be  exploited  by  injecting  JavaScript  code.  Unfortunately,  injected  JavaScript  code  is  difficult  to  distinguish  from  the  legitimate  code  at  the  client  side.  Given  that,  server  side  detection  of  injected  JavaScript  code  can  be  a  layer  of  defense.  Existing  server  side  approaches  rely  on  identifying  legitimate  script  code,  and  an  attacker  can  circumvent  the  technique  by  injecting  legitimate  JavaScript  code.  Moreover,  these  approaches  assume  that  no  JavaScript  code  is  downloaded  from  third  party  websites.  To  address  these  limitations,  we  develop  a  server  side  approach  that  distinguishes  injected  JavaScript  code  from  legitimate  JavaScript  code.  Our  approach  is  based  on  the  concept  of  injecting  comment  statements  containing  random  tokens  and  features  of  legitimate  JavaScript  code.  When  a  response  page  is  generated,  JavaScript  code  without  or  incorrect  comment  is  considered  as  injected  code.  Moreover,  the  valid  comments  are  checked  for  duplicity.  Any  presence  of  duplicate  comments  or  a  mismatch  between  expected  code  features  and  actually  observed  features  represents  JavaScript  code  as  injected.  We  implement  a  prototype  tool  that  automatically  injects  JavaScript  comments  and  deploy  injected  JavaScript  code  detector  as  a  server  side  filter.  We  evaluate  our  approach  with  three  JSP  programs.  The  evaluation  results  indicate  that  our  approach  detects  a  wide  range  of  code  injection  attacks.
1	Topic  shift  detection  in  online  discussions  using  structural  context.  Topic  shift  occurs  frequently  in  online  discussions,  and  automatically  detecting  topic  shift  can  help  to  better  capture  the  main  clues  and  obtain  relevant  answers  from  large  number  of  comments.  Traditional  topic-shift  detection  methods  calculate  text  similarity  and  have  limited  success  because  they  ignore  semantic  relatedness.  In  this  paper,  we  propose  a  new  topic  shift  detection  model  that  uses  conversational  structure  to  enrich  the  context  information  and  word  embedding  to  build  the  semantic  associations  for  each  comment  -  post  pair.  Experiments  show  that  the  proposed  model  leads  to  better  performance  in  terms  of  precision,  recall,  and  F1  score.
1	Modified  memetic  self  adaptive  firefly  algorithm  for  2d  fractal  image  reconstruction.  This  work  concerns  the  problem  of  2D  fractal  image  reconstruction  with  IFS:  given  a  2D  fractal  image,  the  goal  is  to  obtain  an  IFS  whose  attractor  approximates  the  input  image  accurately.  This  problem  is  known  to  be  a  difficult  multivariate  nonlinear  continuous  optimization  problem.  It  is  addressed  in  this  paper  through  a  modification  of  a  popular  nature-inspired  metaheuristics:  the  firefly  algorithm.  Our  approach,  called  memetic  modified  self-adaptive  firefly  algorithm  (MMSA-FFA),  enhances  the  original  firefly  algorithm  with  three  additional  features  for  better  performance:  the  use  of  self-adaptation  strategies  on  the  control  parameters,  a  new  population  model  based  on  elitism,  and  the  hybridization  with  the  Luus-Jaakola  local  search  heuristics.  The  method  is  applied  to  two  illustrative  examples  of  challenging  fractal  images  comprised  of  four  and  forty-four  contractive  functions,  respectively.  Our  experimental  results  show  that  the  method  performs  very  well,  being  able  to  capture  the  underlying  structure  of  the  fractal  images  with  good  visual  quality  and  reasonable  CPU  times  from  totally  random  initial  parameters.
1	Web  application  firewall  network  security  models  and  configuration.  Web  Application  Firewalls  (WAFs)  are  deployed  to  protect  web  applications  and  they  offer  in  depth  security  as  long  as  they  are  configured  correctly.  A  problem  arises  when  there  is  over-reliance  on  these  tools.  A  false  sense  of  security  can  be  obtained  with  the  implementation  of  a  WAF.  In  this  paper,  we  provide  an  overview  of  traffic  filtering  models  and  some  suggestions  to  avail  the  benefit  of  web  app  firewall.
1	Secure  mobile  ipc  software  development  with  vulnerability  detectors  in  android  studio.  The  security  threats  to  mobile  application  are  growing  explosively.  Mobile  app  flaws  and  security  defects  could  open  doors  for  hackers  to  easily  attack  mobile  apps.  Secure  software  development  must  be  addressed  earlier  in  the  development  lifecycle  rather  than  fixing  the  security  holes  after  attacking.  Early  elimination  of  possible  security  vulnerability  will  secure  our  software,  and  mitigate  the  security  risk  threats  by  potential  malicious  attacking.  However,  many  software  developer  professionals  lack  the  necessary  security  knowledge  and  skills  at  the  development  stagef.  In  this  paper  we  explore  the  Android  common  IPC  vulnerability  and  present  our  developed  IPC  flaw  detectors  with  open  source  FindSecurityBugs  plugin  in  Android  Studio  IDE.
1	Towards  an  architecture  for  customizable  drones.  Drones/UAVs  are  able  to  carry  out  air  operations  which  are  difficult  to  be  performed  by  manned  aircrafts.  In  addition,  their  use  brings  significant  economic  savings  and  environmental  benefits,  while  reducing  risks  to  human  life.  However,  current  generation  of  embedded  drone  architectures  consist  of  loosely  coupled  sub-systems  that  run  in  dedicated  monolithic  platforms.  Therefore,  this  approach  reaches  its  limits  when  the  demand  of  autonomy  increases  and  when  the  weight,  volume,  and  power  consumption  of  the  dedicated  sub-systems  encounter  the  drone  restrictions.  To  overcome  these  limitations,  we,  in  this  paper,  propose  a  modular  architectural  approach  inspired  from  the  very  famous  three-layered  architecture  within  the  mobile  robots'  domain.  Our  architecture  separates  the  different  system's  concerns  into  three  layers:  control,  flight  management,  and  planning.  Thus,  each  architecture  layer  can  be  evolved/customized  separately  without  affecting  the  other  layers.  To  ensure  the  applicability  of  our  architecture,  we  have  applied  it  to  a  drone  system  used  for  a  rescue  scenario.
1	Towards  dementia  friendly  smart  homes.  Care  costs  of  People  with  Dementia  (PwDs)  bear  a  tremendous  burden  on  healthcare  systems  around  the  world.  Smart  Homes  (SHs),  as  an  instance  of  the  ambient  assisted  living  technology,  can  help  reduce  these  expenses.  However,  only  few  of  the  existing  studies  in  the  literature  discuss  how  SHs  should  be  designed  by  considering  the  unique  requirements  of  PwDs.  Most  of  the  studies  view  dementia  care  as  a  straight  application  of  standard  SH  technology  without  accommodating  the  specific  needs  of  PwDs.  A  consequence  of  this  approach  is  the  inadequacy  and  unacceptability  of  generic  SH  systems  to  PwDs  as  well  as  other  stakeholders  in  the  sector  of  dementia  care.  The  present  research  reports  on  the  requirements  elicitation,  design  and  evaluation  of  a  dedicated  SH  prototype  for  PwDs.  In  contrast  to  most  of  the  existing  SH  systems  proposed  for  PwDs,  this  work  presents  a  tailored  prototype  that  is  based  on  a  user-centred  design  methodology.  The  preliminary  evaluation  by  a  sample  of  stakeholders  shows  the  suitability  of  the  proposed  methodology  and  consequently  the  resulting  prototype  for  reducing  care  difficulties  as  well  as  its  potential  of  deployment  in  the  real-world  environment.
1	Anvaya  an  algorithm  and  case  study  on  improving  the  goodness  of  software  process  models  generated  by  mining  event  log  data  in  issue  tracking  systems.  Issue  Tracking  Systems  (ITS)  such  as  Bugzilla  can  be  viewed  as  Process  Aware  Information  Systems  (PAIS)  generating  event-logs  during  the  life-cycle  of  a  bug  report.  Process  Mining  consists  of  mining  event  logs  generated  from  PAIS  for  process  model  discovery,  conformance  and  enhancement.  We  apply  process  map  discovery  techniques  to  mine  event  trace  data  generated  from  ITS  of  open  source  Firefox  browser  project  to  generate  and  study  process  models.  Bug  life-cycle  consists  of  diversity  and  variance.  Therefore,  the  process  models  generated  from  the  event-logs  are  spaghetti-like  with  large  number  of  edges,  inter-connections  and  nodes.  Such  models  are  complex  to  analyse  and  difficult  to  comprehend  by  a  process  analyst.  We  improve  the  Goodness  (fitness  and  structural  complexity)  of  the  process  models  by  splitting  the  event-log  into  homogeneous  subsets  by  clustering  structurally  similar  traces.  We  adapt  the  K-Medoid  clustering  algorithm  with  two  different  distance  metrics:  Longest  Common  Subsequence  (LCS)  and  Dynamic  Time  Warping  (DTW).  We  evaluate  the  goodness  of  the  process  models  generated  from  the  clusters  using  complexity  and  fitness  metrics.  We  study  back-forth  and  self-loops,  bug  reopening,  and  bottleneck  in  the  clusters  obtained  and  show  that  clustering  enables  better  analysis.  We  also  propose  an  algorithm  to  automate  the  clustering  process-the  algorithm  takes  as  input  the  event  log  and  returns  the  best  cluster  set.
1	Managing  web  content  using  linked  data  principles  combining  semantic  structure  with  dynamic  content  syndication.  Despite  the  success  of  the  emerging  Linked  Data  Web,  offering  content  in  a  machine-processable  way  and  --  at  the  same  time  --  as  a  traditional  Web  site  is  still  not  a  trivial  task.  In  this  paper,  we  present  the  OntoWiki-CMS  --  an  extension  to  the  collaborative  knowledge  engineering  toolkit  OntoWiki  for  managing  semantically  enriched  Web  content.  OntoWiki-CMS  is  based  on  OntoWiki  for  the  collaborative  authoring  of  semantically  enriched  Web  content,  vocabularies  and  taxonomies  for  the  semantic  structuring  of  the  Web  content  and  the  OntoWiki  Site  Extension,  a  template  and  dynamic  syndication  system  for  representing  the  semantically  enriched  content  as  a  Web  site  and  the  dynamic  integration  of  supplementary  content.  OntoWiki-CMS  facilitates  and  integrates  existing  content-specific  content  management  strategies  (such  as  blogs,  bibliographic  repositories  or  social  networks).  OntoWiki-CMS  helps  to  balance  between  the  creation  of  rich,  stable  semantic  structures  and  the  participatory  involvement  of  a  potentially  large  editor  and  contributor  community.  As  a  result  semantic  structuring  of  the  Web  content  facilitates  better  search,  browsing  and  exploration  as  we  demonstrate  with  a  use  case.
1	Prioritizing  interaction  test  suites  using  repeated  base  choice  coverage.  Combinatorial  interaction  testing  is  a  well-studied  testing  strategy  that  aims  at  constructing  an  effective  interaction  test  suite  (ITS)  of  a  specific  generation  strength  to  identify  interaction  faults  caused  by  the  interactions  among  factors.  Due  to  limited  testing  resources  in  practice,  for  example  in  combinatorial  interaction  regression  testing,  interaction  test  suite  prioritization  (ITSP)  has  been  proposed  to  improve  the  efficiency  of  testing.  An  intuitive  ITSP  strategy  that  has  been  widely  used  in  practice  is  fixed-strength  interaction  coverage  based  prioritization  (FICBP).  FICBP  makes  use  of  a  property  of  the  ITS:  interaction  coverage  at  a  fixed  prioritization  strength.  However,  a  challenge  facing  FICBP  is  that,  when  the  ITS  is  large,  the  prioritization  cost  can  be  very  high.  In  this  paper,  we  propose  a  new  FICBP  method  that,  by  repeatedly  using  base  choice  coverage  (i.e.,  one-wise  coverage)  during  the  prioritization  process,  improves  testing  efficiency  while  maintaining  testing  effectiveness.  The  empirical  studies  show  that  our  method  has  fault  detection  capability  comparable  to  current  FICBP  methods,  but  obtains  more  stable  results  in  many  cases.  Additionally,  our  method  requires  considerably  less  prioritization  time  than  other  FICBP  methods  at  different  prioritization  strengths.
1	Domain  independent  event  analysis  for  log  data  reduction.  Analyzing  the  run  time  behavior  of  large  software  systems  is  a  difficult  and  challenging  task.  Log  analysis  has  been  proposed  as  a  possible  solution.  However,  such  an  analysis  poses  unique  challenges,  mostly  due  to  the  volume  and  diversity  of  the  logged  data  that  is  collected,  thus  making  this  analysis  often  intractable  for  practical  purposes.  In  this  paper,  we  present  a  log  analysis  technique  that  aims  to  compute  a  smaller,  compared  to  the  original,  collection  of  events  that  relate  to  a  given  analysis  objective.  The  technique  is  based  on  computing  a  similarity  score  between  the  logged  events  and  a  collection  of  significant  events  that  we  refer  to  as  beacons.  The  major  novelties  of  the  proposed  technique  are  that  it  is  domain  independent  and  that  it  does  not  require  the  use  of  a  pre-existing  training  data  set.  The  technique  has  been  evaluated  against  the  DARPA  Intrusion  Detection  Evaluation  1999  and  the  KDD  1999  data  sets  with  promising  results.
1	Towards  predicting  risky  behavior  among  veterans  with  ptsd  by  analyzing  gesture  patterns.  Risky  behavior  including  violence  and  aggression,  self-injury,  anger  outburst,  domestic  violence  along  with  self-injury,  sexual  abuse,  rule-breaking,  use  of  drugs  and  alcohol,  suicide,  etc.  are  alarming  issues  among  US  military  veterans  who  return  from  combat  zone  deployment  in  Iraq  and  Afghanistan.  Veterans  are  exposed  to  trauma  in  war  zones  which  affect  most  of  them  with  post-traumatic  stress  disorder  (PTSD)  or  other  mental  health  problems  to  some  degree.  Studies  have  shown  that  veterans  have  much  higher  rates  of  PTSD  than  civilians  and  are  more  likely  to  engage  in  risky  behavior.  One  of  the  forms  of  displaying  and  engaging  in  risky  behaviors  is  through  gestures.  We  collaborated  with  veterans  and  social  scientists  to  find  the  list  of  13  gestures  that  are  often  used  by  veterans  engaged  in  risky  behaviors.  In  this  research  work,  we  have  collected  accelerometer  data  from  subjects  performing  the  gestures  mentioned  above  and  have  tried  to  detect  them  using  machine  learning  techniques.  This  paper  describes  identifying  gesture  clusters  from  the  accelerometer  coordinate  data  and  development  of  a  predictive  model  that  can  classify  the  gestures  resulting  in  the  prediction  of  risky  behaviors  among  the  veterans  who  suffer  from  PTSD.
1	Load  prediction  for  data  centers  based  on  database  service.  In  the  era  of  cloud  computing,  the  over-occupancy  of  data  center  resources  (CPU,  memory,  disk)  and  subsequent  machine  failure  have  resulted  in  great  loss  to  users  and  enterprises.  So  it  makes  sense  to  anticipate  the  server  workload  in  advance.  Previous  research  on  server  workloads  has  focused  on  trend  analysis  and  time  series  fitting.  We  propose  an  approach  to  forecast  the  workloads  of  servers  based  on  machine  learning.  And  our  data  comes  from  a  database-based  data  center  that  is  real,  large-scale,  and  enterprise-class.  We  use  the  servers'  historical  monitoring  data  for  our  models  to  predict  future  workloads  and  hence  provide  the  ability  to  automatically  warn  overload  and  reallocate  resources.  We  calculate  the  failure  detection  rate  and  false  alarm  rate  of  our  overload  detection  models,  as  well  as  put  forward  an  evaluation  based  on  the  overload  processing  cost.  Experimental  results  show  that  machine  learning  methods  especially  Random  Forest  can  better  predict  the  server  load  than  traditional  time  series  analysis  method.  We  use  the  forecast  results  to  propose  some  scheduling  strategies  to  prevent  server  overload,  achieve  intelligent  operation  and  maintenance,  and  failure  prediction.  Compared  with  the  traditional  time  series  analysis  method,  our  method  uses  less  data  and  lower  dimensions,  and  yields  more  accurate  predictions.
1	Towards  unified  vulnerability  assessment  with  open  data.  Continuous  and  comprehensive  vulnerability  management  is  a  difficult  task  for  administrators.  The  difficulties  are  not  because  of  a  lack  of  tools,  but  because  they  are  designed  without  service-oriented  architecture  viewpoint  and  there  is  insufficient  trustworthy  machine-readable  input  data.  This  paper  presents  a  service-oriented  architecture  for  vulnerability  assessment  systems  based  on  the  open  security  standards  and  related  contents.  If  the  functions  are  provided  as  a  service,  various  kinds  of  security  applications  can  be  interoperated  and  integrated  in  loosely-coupled  way.  We  also  studied  the  effectiveness  of  the  available  public  data  for  automated  vulnerability  assessment.  Despite  the  large  amount  of  efforts  that  goes  toward  describing  machine-readable  assessment  test  in  conformity  to  the  OVAL  standard,  the  evaluation  result  proves  inadequate  for  comprehensive  vulnerability  assessment.  Only  about  12%  of  all  the  known  vulnerabilities  are  covered  by  existing  OVAL  tests,  while  some  popular  client  applications  in  the  Top  30  with  most  unique  vulnerabilities  are  covered  more  than  90%.
1	Usefulness  evaluation  of  simulation  in  server  system  testing.  This  paper  evaluates  the  usefulness  of  different  simulators  developed  in  server  system  testing  for  a  mobile  application  during  5  years,  with  the  purpose  of  identifying  the  advantages  and  weaknesses  of  each  simulation  method  in  the  given  setup.  The  study  found  that  simulation  was  used  on  almost  every  interface  to  external  systems,  and  two  factors  that  affected  the  usefulness  of  simulators  over  long  term  were  the  functionality  supported  and  the  applicable  usage  scenarios.  Large-scale  simulation  did  not  keep  yielding  high  returns.  In  the  mobile  application  of  server/client  architecture,  emulating  external  servers  raised  the  testing  capability,  and  emulating  mobile  clients  led  to  higher  efficiency.  The  findings  in  this  study  can  be  used  as  references  for  software  teams  to  plan  simulation  in  QA  activities  properly  for  faster  QA  cycles,  deeper  understanding  of  the  system,  more  stable  test  results  and  lower  resource  costs.
1	Using  dependency  relations  to  improve  test  case  generation  from  uml  statecharts.  In  model-based  testing  the  size  of  the  used  model  has  a  great  impact  on  the  time  for  computing  test  cases.  In  model  checking,  dependence  relations  have  been  used  in  slicing  of  specifications  in  order  to  obtain  reduced  models  pertinent  to  criteria  of  interest.  In  specifications  described  using  state  based  formalisms  slicing  involves  the  removal  of  transitions  and  merging  of  states  thus  obtaining  a  structural  modified  specification.  Using  such  a  specification  for  model  based  test  case  generation  where  sequences  of  transitions  represent  test  cases  might  provide  traces  that  are  not  valid  on  a  correctly  behaving  implementation.  In  order  to  avoid  such  trouble,  we  suggest  the  use  of  control,  data  and  communication  dependences  for  identifying  parts  of  the  model  that  can  be  excluded  so  that  the  remaining  specification  can  be  safely  employed  for  test  case  generation.  This  information  is  included  in  test  purposes  which  are  then  used  in  the  test  case  generation  process.  We  present  also  first  empirical  results  obtained  by  using  several  models  from  industry  and  literature.
1	The  security  cost  of  content  distribution  network  architectures.  Content  Distribution  Network  (CDN)  architectures  face  a  wide  range  of  security  threats.  In  this  paper,  we  compare  the  cost  of  achieving  low  and  high  security  for  different  CDN  architectures.  We  reviewed  the  existing  and  emerging  systems,  identified  the  threats  that  they  face,  defined  the  general  security  requirements  and  considered  the  mechanisms  available  to  meet  the  requirements.  To  assess  the  security  cost,  we  first  defined  the  process  for  selecting  security  mechanisms,  and  then  defined  the  process  for  ranking  the  mechanisms  for  each  architecture.  The  security  comparison  result  clearly  shows  that  the  more  the  cost  of  providing  service  is  pushed  to  the  end  points,  the  higher  the  security  cost.  To  the  best  of  our  knowledge,  this  study  is  the  first  effort  to  assess  a  security  cost  comparison  of  different  CDN  architectures.  Our  work  is  orthogonal  to  other  studies  that  try  to  find  ways  of  reducing  the  content  distribution  service  cost,  rather  than  quantifying  the  cost  to  provide  service  security.
1	Future  directions  of  software  cybernetics  a  position  paper.  Software  Cybernetics  can  address  important  challenges  in  future  software  based  system.  To  achieve  this,  requires  interdisciplinary  work  and  research.  An  evaluation  of  how  Software  Cybernetics  can  interact  with  other  disciplines  is  called  for.
1	A  methodology  to  automate  the  selection  of  design  patterns.  Background:  Over  the  last  two  decades,  numerous  software  design  patterns  have  been  introduced  and  cataloged  on  the  basis  of  developer's  interest  and  skills.  Motivation:  In  software  design  phase,  inexperienced  designers  are  mostly  concerned  on  how  to  select  an  appropriate  design  pattern  from  the  catalog  of  relevant  patterns  in  order  to  solve  a  design  problem.  The  existing  automated  design  pattern  selection  methodologies  are  limited  to  the  need  of  formal  specification  of  design  patterns  or  an  appropriate  sample  size  to  make  the  learning  process  more  effective.  Method:  To  address  this  concern,  we  propose  a  three  step  methodology  to  automate  the  selection  process  of  design  pattern  for  a  design  problem.  The  steps  of  the  methodology  are  text  preprocessing,  use  of  an  unsupervised  learning  technique  (that  is  Fuzzy  c-Mean)  as  a  core  function  to  quantitatively  determine  the  resemblance  of  different  objects  and  selection  of  most  appropriate  pattern  for  a  design  problem.  We  evaluate  our  methodology  with  two  samples  that  is  Gang-of-Four  (GoF)  design  pattern  and  spoiled  pattern  collection,  and  three  object-oriented  related  design  problems.  Moreover,  we  used  Fuzzy  Silhouette  test,  Kappa  (k)  test,  Cosine  Similarity  and  argmax  function  to  measure  the  effectiveness  of  our  methodology.  Results:  In  case  of  GoF  pattern  collection,  we  validated  the  reliability  of  Fuzzy  c-Mean  (FCM)  results  using  a  classification  decision  tree,  and  observed  promising  results  compared  to  other  automation  techniques.  Conclusion:  From  the  comparison  results,  we  observed  11%,  4%  and  18%  improvement  in  the  performance  of  proposed  technique  as  compared  to  supervised  learning  techniques  of  Support  Vector  Machine,  Naive  Bayes  and  C4.5  respectively.
1	Developing  a  research  ideas  creation  system  through  reusing  knowledge  bases  for  ontology  construction.  Ideas  creation  is  an  important  activity  in  various  scenarios  while  research  is  one  of  them.  A  considerable  number  of  research  studies  have  been  made  in  the  past  years  in  improving  tools  and  methods  to  support  idea  generation.  However,  creativity  of  ideas  has  only  been  studied  in  sporadic  attempts,  and  most  of  them  only  share  little  similarity  in  conditions.  Moreover,  since  knowledge  is  the  basis  of  all  kinds  of  idea  generation,  knowledge  reuse  is  very  valuable  in  order  to  enhance  the  generation  process  and  the  results'  quality  including  creativity.  Besides  the  idea  generation  methods  and  techniques  have  recently  been  developed  in  our  earlier  work,  this  study  focuses  on  the  reuse  of  the  knowledge  bases  for  ontology  construction.  It  aims  to  enhance  the  effectiveness  and  innovation  of  generated  research  ideas.  Specifically,  this  paper  presents  an  ideas  creation  framework  with  knowledge  reuse  process  as  a  creative  computing.  In  particular,  ontologies  merging  techniques  with  supporting  algorithms  are  proposed  and  designed.  Furthermore,  a  case  study  is  presented  with  reused  and  new-formed  ontologies,  in  which  formal  algorithms  are  used  to  calculate  and  incorporate  the  knowledge  reuse.  The  main  contribution  of  this  paper  is  a  set  of  rules  to  support  knowledge  reuse  for  the  ontology  construction  in  the  ideas  creation  framework.
1	How  overlapping  community  structure  affects  epidemic  spreading  in  complex  networks.  Many  real-world  networks  exhibit  overlapping  community  structure  in  which  vertices  may  belong  to  more  than  one  community.  It  has  been  recently  shown  that  community  structure  plays  an  import  role  in  epidemic  spreading.  However,  the  effect  of  different  vertices  on  epidemic  behavior  was  still  unclear.  In  this  paper,  we  classify  vertices  into  overlapping  and  non-overlapping  ones,  and  investigate  in  detail  how  they  affect  epidemic  spreading  respectively.  We  propose  a  SIR  epidemic  model  named  ICP-SIR  (Inner-Community  Preferred  Susceptible-Infective-Recovered)  where  the  inner-community  and  inter-community  spreading  rates  are  different.  We  consider  the  case  where  epidemic  process  is  started  by  immunizing  and  infecting  multiple  overlapping  or  non-overlapping  vertices.  The  epidemic  model  is  applied  on  both  synthetic  and  real-world  networks.  Simulation  results  indicate  that  compared  to  non-overlapping  vertices,  overlapping  vertices  play  a  vital  role  in  spreading  the  epidemic  across  communities.  The  result  of  our  research  may  provide  some  reference  on  epidemic  immunization  in  the  future.
1	Aspect  oriented  security  and  exception  handling  within  an  object  oriented  system.  The  object-oriented  paradigm,  with  its  characteristics  of  inheritance,  modularity,  and  polymorphism,  has  facilitated  software  reuse.  However,  the  modularity  and  inheritance  characteristics  of  object-orientation,  has  posed  problems  in  the  area  of  exception  handling  in  terms  of  code  tangling,  repetitive  code,  and  the  possibility  of  spoofed  parent  classes  managing  the  exception.  We  propose  a  method  to  incorporate  parts  of  aspect-oriented  programming,  specifically  security  and  exception  handling  mechanisms,  into  an  object-oriented  system  in  order  to  increase  its  modularity  and  reuse,  centralize  its  security  and  exception  handling,  and  provide  enhanced  verification  and  security  of  resource  access.  At  the  end,  a  small  example  is  presented  to  illustrate  our  approach.  Our  goal  is  to  eventually  develop  a  methodology  and  tool  to  help  automate  the  migration  of  legacy  object-oriented  systems  to  the  aspect-oriented  paradigm,  particularly  in  regards  to  exception  handling  and  security.
1	Physiological  mouse  towards  an  emotion  aware  mouse.  Human-centered  computing  is  rapidly  becoming  a  major  research  direction  in  human-computer  interaction  research.  Among  the  various  research  issues,  we  believe  that  affective  computing,  or  the  ability  of  computers  to  react  according  to  what  a  user  feels,  is  very  important.  In  order  to  recognize  the  human  affect  (feeling),  one  can  rely  on  the  analysis  of  signal  inputs  captured  by  a  multitude  of  means.  In  this  paper,  we  propose  to  make  use  of  human  physiological  signals  as  a  new  form  of  modality  in  determining  human  affects,  in  a  non-intrusive  manner.  This  is  achieved  via  the  physiological  mouse,  as  a  first  step  towards  affective  computing.  We  augment  the  mouse  with  a  small  optical  component  for  capturing  user  photoplethysmographic  (PPG)  signal.  With  the  PPG  signal,  we  are  able  to  compute  and  derive  human  physiological  signals.  We  built  a  prototype  of  the  physiological  mouse  and  measured  raw  PPG  readings.  We  performed  experiments  to  study  the  accuracy  of  our  approach  in  determining  human  physiological  signals  from  the  mouse  PPG  data.  We  believe  that  our  research  will  provide  a  new  dimension  for  multimodal  affective  computing  research.
1	Logoptplus  learning  to  optimize  logging  in  catch  and  if  programming  constructs.  Software  logging  is  a  common  software  development  practice  which  is  used  to  log  program  execution  points.  This  execution  information  is  later  used  by  software  developers  for  debugging  purpose.  Software  logging  is  useful  but  it  has  cost  and  benefit  tradeoff.  Hence  it  is  important  to  optimize  the  number  of  log  statements  in  the  code.  However,  previous  studies  on  logging  show  that  optimal  logging  is  challenging  for  software  developers.  Hence  tools  and  techniques  which  can  help  developers  in  making  optimized  logging  decision  can  be  beneficial.  We  propose  LogOptPlus,  a  machine  learning  based  tool  to  help  software  developers  to  optimize  the  number  of  log  statements  in  the  source  code,  for  two  focused  code  construct  types.  LogOptPlus  is  a  significant  extension  of  our  previously  published  work  'LogOpt'.  LogOpt  is  designed  to  predict  logging  for  catch-blocks.  We  extend  the  functionality  of  LogOpt  to  predict  logging  for  both  if-blocks  and  catch-blocks.  We  identify  distinguishing  static  features  from  the  source  code  for  logging  prediction.  We  present  intuition  and  results  of  quantitative  analysis  of  all  the  features.  We  present  results  of  comprehensive  evaluation  of  LogOptPlus  with  five  different  machine  learning  algorithms  on  two  two  large  Open  Source  projects  (i.e.,  Apache  Tomcat  and  CloudStack).  Encouraging  experimental  results  on  two  Open  Source  projects  show  that  LogOptPlus  is  effective  in  logging  prediction  for  two  focused  code  construct  types.
1	A  scenario  based  approach  for  requirements  elicitation  for  software  systems  complying  with  the  utilization  of  ubiquitous  computing  technologies.  Ubiquitous  computing  aims  to  enhance  computer  use  by  not  only  making  many  computers  available  through  the  physical  environment,  but  making  them  effectively  invisible  to  the  user.  Scenarios  are  examples  of  interaction  sessions,  and  consist  of  descriptions  of  sequential  actions  which  relate  to  real-life  examples  rather  than  abstract  descriptions  of  the  functions.  This  paper  presents  a  scenario-based  approach  for  carrying  out  the  requirements  elicitation  process  for  software  systems  complying  with  the  utilization  of  ubiquitous  computing  technologies.  The  focus  is  on  the  elicitation  of  the  functional  requirements  in  a  user  requirement  form.  The  steps  followed  in  this  approach  to  carry  out  the  additional-requirements  elicitation  process  are  scenario  description  in  plain  text,  scenario  analysis  and  finally  scenario  modeling  in  Business  Process  Modeling  Notation  (BPMN).  The  proposed  approach  is  demonstrated  using  a  case  study  to  show  that  this  approach  is  feasible  and  promising  in  its  domain.
1	Selection  and  ordering  of  points  of  interest  in  large  scale  indoor  navigation  systems.  Indoor  navigation  systems  guide  users  through  complex  buildings,  which  they  do  not  know  in  advance.  The  complexity  of  public  buildings  such  as  airports,  train  stations  or  hospitals  leads  to  new  variants  of  well-studied  NP-hard  optimization  problems  such  as  the  Traveling  Salesman  Problem,  where  most  of  the  classical  approximations  are  not  directly  applicable  for  fundamental  geometric  reasons.  Fortunately  we  are  able  to  solve  the  upcoming  small  instances  of  these  problems  to  optimality  in  a  very  short  timeframe.  With  this  paper  we  define  the  Partially  Ordered  Traveling  Salesman  Problem,  explain  how  it  comes  up  in  indoor  navigation  applications  and  present  two  algorithms,  which  solve  or  approximate  the  Partially  Ordered  Traveling  Salesman  Problem  for  small  and  medium  size  instances  in  a  timeframe  of  less  than  one  second  and  hence  allow  for  a  real  time  and  context-aware  indoor  navigation  experience.  We  then  evaluate  both  algorithms  using  realistic  data  modelling  the  public  area  of  Munich  airport  spanning  nearly  200.000  square  meters.
1	Exploring  trusted  data  dissemination  in  a  vehicular  social  network  with  a  formal  compositional  approach.  Vehicular  Ad  hoc  Networks  (VANETs)  have  been  largely  impacted  by  social  characteristics  and  human  behaviours  with  the  wide  use  of  service  applications  in  VANETs.  Thus,  vehicular  communications  can  be  considered  as  a  social  network  for  auto-mobiles  in  which  drivers  can  share  data  with  other  neighbours.  In  this  context,  a  trusted  data  dissemination  claims  our  high  attention.  Besides  the  traditional  public  key  scheme,  this  paper  explores  security  issues  of  date  dissemination  through  associating  drivers'  social  relationships  with  vehicles  in  order  to  construct  a  trusted  vehicular  social  network.  Furthermore,  a  compositional  formal  approach  (Performance  Evaluation  Process  Algebra,  PEPA)  is  introduced  for  performance  modelling  of  such  a  network  due  to  its  outstanding  features.
1	Secure  and  long  lived  wireless  sensor  network  for  data  center  monitoring.  Reducing  energy  consumption  in  data  centers  is  one  of  the  important  issues  in  IT  industry.  Since  Power  Usage  Effectiveness  (PUE)  focuses  on  a  fraction  of  the  power  consumed  by  IT  equipment  such  as  servers  and  networking  devices,  the  power  consumption  of  environmental  control  systems  such  as  Computer  Room  Air  Conditioning  (CRAC)  is  "overhead"  in  the  sense  of  PUE.  In  order  to  improve  PUE,  dense  monitoring  of  data  center  environment  is  required  for  preventing  excessive  cooling.  One  of  the  promising  approaches  for  dense  environment  monitoring  is  utilizing  wireless  sensor  network  (WSN).  An  issue  in  applying  WSN  to  data  centers  is  message  encryption  and  authentication.  Compared  to  the  wired  network,  wireless  network  communication  might  be  monitored  from  the  outside  of  the  data  center  and  it  becomes  a  critical  issue  for  data  center  security.  In  this  paper,  we  aim  to  establish  secure  WSN  for  data  center  use  cases  and  several  preliminary  experiments  for  applying  message  encryption  and  authentication  will  be  discussed.
1	Spesc  a  specification  language  for  smart  contracts.  The  smart  contract  is  an  interdisciplinary  concept  that  concerns  business,  finance,  contract  law  and  information  technology.  Designing  and  developing  a  smart  contract  may  require  the  close  cooperation  of  many  experts  coming  from  different  fields.  How  to  support  such  collaborative  development  is  a  challenging  problem  in  blockchain-oriented  software  engineering.  This  paper  proposes  SPESC,  a  specification  language  for  smart  contracts,  which  can  define  the  specification  of  a  smart  contract  for  the  purpose  of  collaborative  design.  SPESC  can  specify  a  smart  contract  in  a  similar  form  to  real-world  contracts  using  a  natural-language-like  grammar,  in  which  the  obligations  and  rights  of  parties  and  the  transaction  rules  of  cryptocurrencies  are  clearly  defined.  The  preliminary  study  results  demonstrated  that  SPESC  can  be  easily  learned  and  understood  by  both  IT  and  non-IT  users  and  thus  has  greater  potential  to  facilitate  collaborative  smart  contract  development.
1	Modeling  restrained  epidemic  routing  on  complex  networks.  To  realize  an  efficient  DTN  (Delay/Disruption-Tolerant  Networking)  routing,  it  is  required  to  quickly  deliver  the  message  from  the  source  node  to  the  destination  node  as  well  as  to  quickly  delete  disused  message  replicas  from  the  network.  Epidemic  routing,  which  indefinitely  forwards  message  replicas  to  all  encountered  nodes,  realizes  the  near-optimal  message  delivery  delay  when  a  limited  number  of  messages  are  transferred.  However,  its  performance  is  significantly  degraded  when  a  number  of  messages  are  transferred  simultaneously.  In  our  previous  work,  we  have  proposed  a  simple  but  effective  extension  to  epidemic  routing  called  restrained  epidemic  routing,  which  intentionally  suppresses  message  forwardings  at  the  later  stage  of  epidemic-style  message  dissemination.  In  this  paper,  we  analyze  the  characteristics  of  restrained  epidemic  routing  when  the  contact  relation  between  nodes  is  given  by  a  general  contact  model  such  as  complex  networks.  Specifically,  we  describe  the  dynamics  of  restrained  epidemic  routing  on  a  complex  network  with  a  given  degree  distribution  as  differential  equations  using  the  degree-based  mean  field  approximation.
1	Authenticating  preference  oriented  multiple  users  spatial  queries.  Location-based  social  networks  (LBSNs)  are  attracting  significant  attentions,  which  make  location-aware  applications  prosperous.  We  proposed  the  Multiple  User-defined  Spatial  Query  (MUSQ)  in  [1].  However,  it  is  impractical  that  non-expert  users  provide  exact  vectors  to  denote  their  preferences  in  MUSQ.  In  this  paper,  we  design  a  group  users  weight  matrix  generation  algorithm  to  represent  users'  preferences  conveniently.  In  addition,  we  propose  a  refinement  method  to  improve  the  effectiveness  of  the  query  results.  Further,  considering  the  trust  issue  introduced  by  data  outsourcing,  an  authenticated  query  processing  framework  is  proposed.  A  set  of  experiments  are  conducted  to  show  the  effectiveness  and  scalability  of  our  methods  under  various  parameter  settings.
1	A  change  proposal  driven  approach  for  changeability  assessment  using  fca  based  impact  analysis.  Given  a  change  proposal,  how  can  we  evaluate  the  changeability  of  the  original  system  to  absorb  this  change  proposal  before  change  implementation?  Changes  to  software  often  have  unexpected  ripple  effects.  To  avoid  this  and  alleviate  the  risk  of  performing  undesirable  changes,  a  predictive  measurement  of  these  ripple  effects  should  be  conducted  and  a  decision  of  acceptance  or  rejection  should  be  made  on  this  change  proposal.  In  this  paper,  we  propose  an  approach  to  evaluate  a  software  system's  changeability  with  two  steps.  First,  our  approach  uses  formal  concept  analysis  to  perform  change  impact  analysis  ($CIA$),  which  estimates  the  ripple  effects  of  the  change  proposal.  Then,  we  propose  a  novel  impactness  metric  to  indicate  the  system's  changeability  to  absorb  this  change  proposal.  Case  studies  on  three  real-world  programs  show  the  effectiveness  of  our  changeability  assessment  approach.
1	Tool  supported  collaborative  requirements  prioritisation.  Automated  decision-making  techniques  are  useful  to  support  engineers  when  performing  requirements  engineering  tasks.  However,  to  be  effectively  used  in  practice  they  need  to  be  integrated  into  the  organisational  context,  in  which  stakeholder  engagement  becomes  a  critical  adoption  factor.  In  this  paper,  we  propose  a  tool-supported  collaborative  requirements  prioritisation  process,  called  GRP,  which  exploits  gamification  elements  to  engage  distributed  stakeholders  to  contribute  to  the  overall  decision-making  process.  Analytic  Hierarchy  Process  is  used  as  key  component  of  the  game  engine,  and  enables  an  iterative  prioritisation  process.  The  GRP  process  has  been  evaluated  through  an  exploratory  case  study,  which  has  been  conducted  at  a  small  software  company,  providing  us  with  preliminary  evidence  about  the  effectiveness  of  the  proposed  solution.  The  main  findings  and  lessons  learned  from  the  case  study  are  presented.
1	User  rank  a  user  influence  based  data  distribution  optimization  method  for  privacy  protection  in  cloud  storage  system.  The  spring  up  of  cloud  storage,  such  as  Hadoop  HDFS,  Open  Stack  Swift,  brings  us  more  intelligent  storage  solutions.  Nowadays,  the  most  commercial  version  of  cloud  storage  system  puts  more  emphasis  on  high-performance  and  high-availability,  very  little  attention  is  given  to  privacy  protection.  This  paper  proposes  a  user  influence-based  data  distribution  optimization  method,  User  Rank,  which  migrate  blocks  of  high-impact  data  from  threatened  nodes  to  secure  nodes.  Additionally,  a  reference  implementation  of  User  Rank  is  presented  to  optimize  cloud  storage  system.  The  simulation  experiments  results  verify  that  User  Rank  can  significantly  reduce  private  leakage  for  private  or  public  cloud.
1	Stateflow  to  extended  finite  automata  translation.  State  flow,  a  graphical  interface  tool  for  Matlab,  is  a  common  choice  for  design  of  event-driven  software  and  systems.  In  order  for  their  offline  analysis  (testing/verification)  or  online  analysis  (monitoring),  the  State  flow  model  must  be  converted  to  a  form  that  is  amenable  to  formal  analysis.  In  this  paper,  we  present  a  systematic  method,  which  translates  State  flow  into  a  formal  model,  called  Input/Output  Extended  Finite  Automata  (I/O-EFA).  The  translation  method  treats  each  state  of  the  State  flow  model  as  an  atomic  module,  and  applies  composition/refinement  rules  for  each  feature  (such  as  state-hierarchy,  local  events)  recursively  to  obtain  the  entire  model.  The  size  of  the  translated  model  is  linear  in  the  size  of  the  State  flow  chart.  Our  translation  method  is  sound  and  complete  in  the  sense  that  it  preserves  the  discrete  behaviors  as  observed  at  the  sample  times.  Further,  the  translation  method  has  been  implemented  in  a  Matlab  tool,  which  outputs  the  translated  I/O-EFA  model  that  can  itself  be  simulated  in  Matlab.
1	Supporting  the  specification  and  serialization  of  planned  architectures  in  architecture  driven  modernization  context.  Architecture-Driven  Modernization  (ADM)  intends  to  standardize  software  reengineering  by  relying  on  a  family  of  standard  metamodels.  Knowledge-Discovery  Metamodel  (KDM)  is  the  main  ADM  ISO  metamodel  aiming  at  representing  all  aspects  of  existing  legacy  systems.  One  of  the  internal  KDM  metamodels  is  called  Structure,  responsible  for  representing  architectural  abstractions  (Layers,  Components  and  Subsystems)  and  their  relationships.  Planned  Architecture  (PA)  is  an  artifact  that  involves  not  only  the  architectural  abstractions  of  the  system  but  also  the  access  rules  that  must  exist  between  them  and  be  maintained  over  time.  Although  PAs  are  frequently  used  in  Architecture-Conformance  Checking  processes,  up  to  this  moment,  there  is  no  contribution  showing  how  to  specify  and  serialize  PAs  in  ADM-based  modernization  projects.  Therefore,  in  this  paper  we  present  an  approach  that  i)  involves  a  DSL  (Domain-Specific  Language)  for  the  specification  of  PAs  using  the  Structure  metamodel  concepts,  and  ii)  proposes  a  strategy  for  the  serialization  of  PAs  as  a  Structure  metamodel  instance  without  modifying  it.  We  have  conducted  a  comparison  between  DCL-KDM  and  other  techniques  for  specifying  and  generating  PAs.  The  results  showed  that  DCL-KDM  is  an  efficient  alternative  to  to  generate  instances  of  the  Structure  metamodel  as  a  PA  and  to  serialize  it.
1	Automatic  detection  of  nosql  injection  using  supervised  learning.  With  the  advancement  in  big  data,  NoSQL  databases  are  enjoying  ever-growing  popularity.  The  increasing  use  of  this  technology  in  large  applications  also  brings  security  concerns  to  the  fore.  Historically,  SQL  injection  has  been  one  of  the  major  security  threats  over  the  years.  Recent  studies  reveal  that  NoSQL  databases  also  have  become  vulnerable  to  injections.  However,  NoSQL  security  is  yet  to  receive  the  attention  it  deserves  from  the  industry  or  academia.  In  this  work,  we  develop  a  tool  for  detecting  NoSQL  injections  using  supervised  learning.  To  the  best  of  our  knowledge,  our  developed  training  dataset  on  NoSQL  injection  is  the  first  of  its  kind.  We  manually  design  important  features  and  apply  various  supervised  learning  algorithms.  Our  tool  has  achieved  0.93  F2-score  as  established  by  10-fold  cross-validation.  We  also  apply  our  tool  to  a  NoSQL  injection  generating  tool,  NoSQLMap  and  find  that  our  tool  outperforms  Sqreen,  the  only  available  NoSQL  injection  detection  tool,  by  36.25%  in  terms  of  detection  rate.  The  proposed  technique  is  also  shown  to  be  database-agnostic  achieving  similar  performance  with  injection  on  MongoDB  and  CouchDB  databases.
1	Testing  java  exceptions  an  instrumentation  technique.  Quality  can  be  defined  as  the  level  of  adequacy  between  the  final  product  and  its  specification.  Software  industries  use  many  testing  methodologies  to  assure  the  high-quality  of  their  software.  Code  coverage  is  one  of  these  methodologies  usually  applied  to  test  the  expected  (normal)  behavior  of  software.  Exception  handling  structures  mainly  appear  in  software  code  to  deal  with  problems  caused  by  unexpected  behavior.  Because  of  this,  they  are  not  completely  covered  with  the  techniques  for  testing  programs  normal  behavior.  To  overcome  this  limit,  we  present  the  Verify  Ex,  a  Java  class  prototype,  together  with  a  Java  source  code  instrumentation  technique  to  exercise  exception  handling  structures.  The  result  is  an  increase  of  code  coverage  rate  due  to  the  inclusion  of  programs  exceptional  behavior  in  the  testing  execution  and  code  coverage  analysis.
1	A  method  for  detecting  unusual  defects  in  enterprise  system  using  model  checking  techniques.  This  paper  proposes  a  method  based  on  model  checking  for  detecting  hard-to-discover  defects  in  enterprise  systems.  Source  codes  are  transformed  into  an  appropriate  phased  abstract  model  so  that  we  can  observe  the  phenomena.  UPPAAL,  which  is  a  typical  model  checking  tool,  makes  an  exhaustive  checking  of  the  model  and  provides  a  result  whether  the  model  can  reach  the  specified  state  or  not.  We  have  developed  a  supporting  tool  to  narrow  the  range  of  model  checking  and  to  generate  UPPAAL  model  automatically.  We  discuss  our  method  in  detail  on  the  basis  of  the  results  of  a  case  study.
1	Perceptions  of  answer  quality  in  an  online  technical  question  and  answer  forum.  Software  developers  are  used  to  seeking  information  from  authoritative  texts,  such  as  a  technical  manuals,  or  from  experts  with  whom  they  are  familiar.  Increasingly,  developers  seek  information  in  online  question  and  answer  forums,  where  the  quality  of  the  information  is  variable.  To  a  novice,  it  may  be  challenging  to  filter  good  information  from  bad.  Stack  Overflow  is  a  Q&A  forum  that  introduces  a  social  reputation  element:  users  rate  the  quality  of  post-ed  answers,  and  answerers  can  accrue  points  and  rewards  for  writing  answers  that  are  rated  highly  by  their  peers.  A  user  that  consistently  authors  good  answers  will  develop  a  good  ‘reputation’  as  recorded  by  these  points.  While  this  system  was  designed  with  the  intent  to  incentivize  high-quality  answers,  it  has  been  suggested  that  information  seekers—and  particularly  technical  novices—may  rely  on  the  social  reputation  of  the  answerer  as  a  proxy  for  answer  quality.  In  this  paper,  we  investigate  the  role  that  this  social  factor—as  well  as  other  answer  characteristics—plays  in  the  information  filtering  process  of  technical  novices  in  the  context  of  Stack  Over-flow.  The  results  of  our  survey  conducted  on  Amazon.com’s  Mechanical  Turk  indicate  that  technical  novices  assess  information  quality  based  on  the  intrinsic  qualities  of  the  answer,  such  as  presentation  and  content,  suggesting  that  novices  are  wary  to  rely  on  social  cues  in  the  Q&A  context.
1	Mining  software  repositories  using  topic  models.  Software  repositories,  such  as  source  code,  email  archives,  and  bug  databases,  contain  unstructured  and  unlabeled  text  that  is  difficult  to  analyze  with  traditional  techniques.  We  propose  the  use  of  statistical  topic  models  to  automatically  discover  structure  in  these  textual  repositories.  This  discovered  structure  has  the  potential  to  be  used  in  software  engineering  tasks,  such  as  bug  prediction  and  traceability  link  recovery.  Our  research  goal  is  to  address  the  challenges  of  applying  topic  models  to  software  repositories.
1	Performance  evaluation  of  rpl  routing  protocol  in  6lowpan.  In  this  paper,  we  present  the  results  of  the  comparison  of  three  reactive  routing  protocols  for  Wireless  sensor  networks  (WSNs),  AODV,  DYMO  and  RPL.  For  the  purpose  of  performance  evaluation,  detailed  comparisons  are  made  with  AODV  and  DYMO.  Simulations  are  run  to  estimate  the  network  topology  change,  routing  overhead  and  average  packet  End-to-End  Delay.  We  simulate  RPL  with  COOJA  based  on  contiki  operating  system.  It  is  showed  that  RPL  routing  protocol  offers  adaptation  to  changing  network  topology  and  the  performances  of  RPL  are  superior  to  the  other  two  routing  protocol  in  6lowpan,  and  it  has  a  higher  stability  than  the  AODV,  DYMO.
1	Calibrating  use  case  points.  An  approach  to  calibrate  the  complexity  weights  of  the  use  cases  in  the  Use  Case  Points  (UCP)  model  is  put  forward.  The  size  metric  used  is  the  Use  Case  Points  (UCP)  which  can  be  calculated  from  the  use  case  diagram  along  with  its  use  case  scenario  as  described  in  the  UCP  model.  The  approach  uses  a  neural  network  with  fuzzy  logic  to  tune  the  complexity  weights.
1	How  and  when  to  transfer  software  engineering  research  via  extensions.  It  is  often  reported  that  there  is  a  large  gap  between  software  engineering  research  and  practice,  with  little  transfer  from  research  to  practice.  While  this  is  true  in  general,  one  transfer  technique  is  increasingly  breaking  down  this  barrier:  extensions  to  integrated  development  environments  (IDEs).  With  the  proliferation  of  app  stores  for  IDEs  and  increasing  transfer  effort  from  researchers  several  research-based  extensions  have  seen  significant  adoption.  In  this  talk  we'll  discuss  our  experience  transferring  code  search  research,  which  currently  is  in  the  top  5%  of  Visual  Studio  extensions  with  over  13,000  downloads,  as  well  as  other  research  techniques  transferred  via  extensions  such  as  NCrunch,  FindBugs,  Code  Recommenders,  Mylyn,  and  Instasearch.  We'll  use  the  lessons  learned  from  our  transfer  experience  to  provide  case  study  evidence  as  to  best  practices  for  successful  transfer,  supplementing  it  with  the  quantitative  evidence  offered  by  app  store  and  usage  data  across  the  broader  set  of  extensions.  The  goal  of  this  30  minute  talk  is  to  provide  researchers  with  a  realistic  view  on  which  research  techniques  can  be  transferred  to  practice  as  well  as  concrete  steps  to  execute  such  a  transfer.
1	Molveric  an  inspection  technique  for  molic  diagrams.  During  interaction  design,  interaction  models  are  developed  to  help  design  adequate  user  interaction  with  the  system.  MoLIC  (Modeling  Language  for  Interaction  as  Conversation)  is  a  language  used  to  represent  an  interaction  model,  which  can  then  be  used  as  a  basis  for  building  other  artifacts,  such  as  mockups.  However,  inspections  are  necessary  to  verify  whether  the  MoLIC  diagrams  are  complete,  consistent,  unambiguous,  and  contain  few  or  no  defects,  to  avoid  propagating  preventable  defects  to  derived  artifacts.  In  this  paper,  we  present  MoLVERIC,  a  technique  for  the  inspection  of  MoLIC  diagrams  that  uses  cards  with  verification  items  and  employs  principles  of  gamification.  Furthermore,  we  discuss  the  results  of  a  pilot  study  conducted  to  analyze  the  feasibility  of  this  technique.
1	Engineering  the  software  of  robotic  systems.  The  production  of  software  for  robotic  systems  is  often  case-specific,  without  fully  following  established  engineering  approaches.  Systematic  approaches,  methods,  models,  and  tools  are  pivotal  for  the  creation  of  robotic  systems  for  real-world  applications  and  turn-key  solutions.  Well-defined  (software)  engineering  approaches  are  considered  the  "make  or  break"  factor  in  the  development  of  complex  robotic  systems.  The  shift  towards  well-defined  engineering  approaches  will  stimulate  component  supply-chains  and  significantly  reshape  the  robotics  marketplace.      The  goal  of  this  technical  briefing  is  to  provide  an  overview  on  the  state  of  the  art  and  practice  concerning  solutions  and  open  challenges  in  the  engineering  of  software  required  to  develop  and  manage  robotic  systems.  Model-Driven  Engineering  (MDE)  is  discussed  as  a  promising  technology  to  raise  the  level  of  abstraction,  promote  reuse,  facilitate  integration,  boost  automation  and  promote  early  analysis  in  such  a  complex  domain.
1	Achieving  accuracy  and  scalability  simultaneously  in  detecting  application  clones  on  android  markets.  Besides  traditional  problems  such  as  potential  bugs,  (smartphone)  application  clones  on  Android  markets  bring  new  threats.  That  is,  attackers  clone  the  code  from  legitimate  Android  applications,  assemble  it  with  malicious  code  or  advertisements,  and  publish  these  ``purpose-added"  app  clones  on  the  same  or  other  markets  for  benefits.  Three  inherent  and  unique  characteristics  make  app  clones  difficult  to  detect  by  existing  techniques:  a  billion  opcode  problem  caused  by  cross-market  publishing,  gap  between  code  clones  and  app  clones,  and  prevalent  Type  2  and  Type  3  clones.          Existing  techniques  achieve  either  accuracy  or  scalability,  but  not  both.  To  achieve  both  goals,  we  use  a  geometry  characteristic,  called  centroid,  of  dependency  graphs  to  measure  the  similarity  between  methods  (code  fragments)  in  two  apps.  Then  we  synthesize  the  method-level  similarities  and  draw  a  Y/N  conclusion  on  app  (core  functionality)  cloning.  The  observed  ``centroid  effect"  and  the  inherent  ``monotonicity"  property  enable  our  approach  to  achieve  both  high  accuracy  and  scalability.  We  implemented  the  app  clone  detection  system  and  evaluated  it  on  five  whole  Android  markets  (including  150,145  apps,  203  million  methods  and  26  billion  opcodes).  It  takes  less  than  one  hour  to  perform  cross-market  app  clone  detection  on  the  five  markets  after  generating  centroids  only  once.
1	Reassert  a  tool  for  repairing  broken  unit  tests.  Successful  software  systems  continuously  change  their  requirements  and  thus  code.  When  this  happens,  some  existing  tests  get  broken  because  they  no  longer  reflect  the  intended  behavior,  and  thus  they  need  to  be  updated.  Repairing  broken  tests  can  be  time-consuming  and  difficult.      We  present  ReAssert,  a  tool  that  can  automatically  suggest  repairs  for  broken  unit  tests.  Examples  include  replacing  literal  values  in  tests,  changing  assertion  methods,  or  replacing  one  assertion  with  several.  Our  experiments  show  that  ReAssert  can  repair  many  common  test  failures  and  that  its  suggested  repairs  match  developers'  expectations.
1	Broker  based  secure  web  service  composition  using  star  topology.  Web  service  is  a  mechanism  of  providing  an  interoperating  communication  between  different  software  applications,  running  on  a  variety  of  platforms  and  frameworks.  Web  service  composition  can  be  processed  with  the  help  of  Business  Process  Execution  Language  (BPEL)  and  Ontology  Based  Web  Language  (OWL).  The  existing  business  processes  are  unable  to  support  human  user  interaction,  authorization,  encryption  and  decryption  mechanisms.  There  is  need  to  introduce  secure  broker  based  technique  for  web  service  composition.  In  this  paper,  we  have  proposed  a  broker  based  framework  for  securing  the  web  service  composition  by  introducing  a  broker's  layer  between  business  process  and  web  services.  The  secure  composition  of  web  services  will  be  performed  using  transposition  technique  based  on  star  topology  and  Intalio  business  process  designer.  The  proposed  framework  is  illustrated  with  the  example  of  different  hotels  running  in  different  places  in  secure  manner  over  the  internet.  The  proposed  technique  will  help  to  reduce  the  UDDI  load.
1	Scrum  for  cyber  physical  systems  a  process  proposal.  Agile  development  processes  and  especially  Scrum  are  chang-  ing  the  state  of  the  practice  in  software  development.  Many  companies  in  the  classical  IT  sector  have  adopted  them  to  successfully  tackle  various  challenges  from  the  rapidly  changing  environments  and  increasingly  complex  software  systems.  Companies  developing  software  for  embedded  or  cyber-physical  systems,  however,  are  still  hesitant  to  adopt  such  processes.  Despite  successful  applications  of  Scrum  and  other  agile  methods  for  cyber-physical  systems,  there  is  still  no  complete  process  that  maps  their  specific  challenges  to  practices  in  Scrum.  We  propose  to  fill  this  gap  by  treating  all  design  artefacts  in  such  a  development  in  the  same  way:  In  software  development,  the  final  design  is  already  the  product,  in  hardware  and  mechanics  it  is  the  starting  point  of  production.  We  sketch  the  Scrum  extension  Scrum  CPS  by  showing  how  Scrum  could  be  used  to  develop  all  design  artefacts  for  a  cyber  physical  system.  Hardware  and  mechanical  parts  that  might  not  be  available  yet  are  simulated.  With  this  approach,  we  can  directly  and  iteratively  build  the  final  software  and  produce  detailed  models  for  the  hardware  and  mechanics  production  in  parallel.  We  plan  to  further  detail  Scrum  CPS  and  apply  it  first  in  a  series  of  student  projects  to  gather  more  experience  before  testing  it  in  an  industrial  case  study.
1	Helping  programmers  improve  the  energy  efficiency  of  source  code.  This  paper  briefly  proposes  a  technique  to  detect  energy  inefficient  fragments  in  the  source  code  of  a  software  system.  Test  cases  are  executed  to  obtain  energy  consumption  measurements,  and  a  statistical  method,  based  on  spectrum-based  fault  localization,  is  introduced  to  relate  energy  consumption  to  the  system's  source  code.  The  result  of  our  technique  is  an  energy  ranking  of  source  code  fragments  pointing  developers  to  possible  energy  leaks  in  their  code.
1	Automated  refactoring  of  legacy  java  software  to  default  methods.  Java  8  default  methods,  which  allow  interfaces  to  contain  (instance)  method  implementations,  are  useful  for  the  skeletal  implementation  software  design  pattern.  However,  it  is  not  easy  to  transform  existing  software  to  exploit  default  methods  as  it  requires  analyzing  complex  type  hierarchies,  resolving  multiple  implementation  inheritance  issues,  reconciling  differences  between  class  and  interface  methods,  and  analyzing  tie-breakers  (dispatch  precedence)  with  overriding  class  methods  to  preserve  type-correctness  and  confirm  semantics  preservation.  In  this  paper,  we  present  an  efficient,  fully-automated,  type  constraint-based  refactoring  approach  that  assists  developers  in  taking  advantage  of  enhanced  interfaces  for  their  legacy  Java  software.  The  approach  features  an  extensive  rule  set  that  covers  various  corner-cases  where  default  methods  cannot  be  used.  To  demonstrate  applicability,  we  implemented  our  approach  as  an  Eclipse  plug-in  and  applied  it  to  19  real-world  Java  projects,  as  well  as  submitted  pull  requests  to  popular  GitHub  repositories.  The  indication  is  that  it  is  useful  in  migrating  skeletal  implementation  methods  to  interfaces  as  default  methods,  sheds  light  onto  the  pattern's  usage,  and  provides  insight  to  language  designers  on  how  this  new  construct  applies  to  existing  software.
1	Tool  choice  matters  javascript  quality  assurance  tools  and  usage  outcomes  in  github  projects.  Quality  assurance  automation  is  essential  in  modern  software  development.  In  practice,  this  automation  is  supported  by  a  multitude  of  tools  that  fit  different  needs  and  require  developers  to  make  decisions  about  which  tool  to  choose  in  a  given  context.  Data  and  analytics  of  the  pros  and  cons  can  inform  these  decisions.  Yet,  in  most  cases,  there  is  a  dearth  of  empirical  evidence  on  the  effectiveness  of  existing  practices  and  tool  choices.  We  propose  a  general  methodology  to  model  the  time-  dependent  effect  of  automation  tool  choice  on  four  outcomes  of  interest:  prevalence  of  issues,  code  churn,  number  of  pull  requests,  and  number  of  contributors,  all  with  a  multitude  of  controls.  On  a  large  data  set  of  npm  JavaScript  projects,  we  extract  the  adoption  events  for  popular  tools  in  three  task  classes:  linters,  dependency  managers,  and  coverage  reporters.  Using  mixed  methods  approaches,  we  study  the  reasons  for  the  adoptions  and  compare  the  adoption  effects  within  each  class,  and  sequential  tool  adoptions  across  classes.  We  find  that  some  tools  within  each  group  are  associated  with  more  beneficial  outcomes  than  others,  providing  an  empirical  perspective  for  the  benefits  of  each.  We  also  find  that  the  order  in  which  some  tools  are  implemented  is  associated  with  varying  outcomes.
1	Concolic  testing  for  high  test  coverage  and  reduced  human  effort  in  automotive  industry.  The  importance  of  automotive  software  has  been  rapidly  increasing  because  software  now  controls  many  components  in  motor  vehicles  such  as  window  controller,  smart-key  system,  and  tire  pressure  monitoring  system.  Consequently,  the  automotive  industry  spends  a  large  amount  of  human  effort  testing  automotive  software  and  is  interested  in  automated  software  testing  techniques  that  can  ensure  high-quality  automotive  software  with  reduced  human  effort.  In  this  paper,  we  report  our  industrial  experience  applying  concolic  testing  to  automotive  software  developed  by  Hyundai  Mobis.  We  have  developed  an  automated  testing  framework  MAIST  that  automatically  generates  the  test  driver,  stubs,  and  test  inputs  to  a  target  task  by  applying  concolic  testing.  As  a  result,  MAIST  has  achieved  90.5%  branch  coverage  and  77.8%  MC/DC  coverage  on  the  integrated  body  unit  (IBU)  software.  Furthermore,  it  reduced  the  cost  of  IBU  coverage  testing  by  reducing  the  manual  testing  effort  for  coverage  testing  by  53.3%.
1	Deobfuscating  android  native  binary  code.  In  this  paper,  we  propose  an  automated  approach  to  facilitate  the  deobfuscation  of  Android  native  binary  code.  Specifically,  given  a  native  binary  obfuscated  by  Obfuscator-LLVM  (the  most  popular  native  code  obfuscator),  our  deobfuscation  system  is  capable  of  recovering  the  original  Control  Flow  Graph.  To  the  best  of  our  knowledge,  it  is  the  first  work  that  aims  to  tackle  the  problem.  We  have  applied  our  system  in  different  scenarios,  and  the  experimental  results  demonstrate  the  effectiveness  of  our  system  based  on  generic  similarity  comparison  metrics.
1	Investigating  the  effects  of  gender  bias  on  github.  Diversity,  including  gender  diversity,  is  valued  by  many  software  development  organizations,  yet  the  field  remains  dominated  by  men.  One  reason  for  this  lack  of  diversity  is  gender  bias.  In  this  paper,  we  study  the  effects  of  that  bias  by  using  an  existing  framework  derived  from  the  gender  studies  literature.  We  adapt  the  four  main  effects  proposed  in  the  framework  by  posing  hypotheses  about  how  they  might  manifest  on  GitHub,  then  evaluate  those  hypotheses  quantitatively.  While  our  results  show  that  effects  of  gender  bias  are  largely  invisible  on  the  GitHub  platform  itself,  there  are  still  signals  of  women  concentrating  their  work  in  fewer  places  and  being  more  restrained  in  communication  than  men.
1	Class  imbalance  evolution  and  verification  latency  in  just  in  time  software  defect  prediction.  Just-in-Time  Software  Defect  Prediction  (JIT-SDP)  is  an  SDP  approach  that  makes  defect  predictions  at  the  software  change  level.  Most  existing  JIT-SDP  work  assumes  that  the  characteristics  of  the  problem  remain  the  same  over  time.  However,  JIT-SDP  may  suffer  from  class  imbalance  evolution.  Specifically,  the  imbalance  status  of  the  problem  (i.e.,  how  much  underrepresented  the  defect-inducing  changes  are)  may  be  intensified  or  reduced  over  time.  If  occurring,  this  could  render  existing  JIT-SDP  approaches  unsuitable,  including  those  that  re-build  classifiers  over  time  using  only  recent  data.  This  work  thus  provides  the  first  investigation  of  whether  class  imbalance  evolution  poses  a  threat  to  JIT-SDP.  This  investigation  is  performed  in  a  realistic  scenario  by  taking  into  account  verification  latency  --  the  often  overlooked  fact  that  labeled  training  examples  arrive  with  a  delay.  Based  on  10  GitHub  projects,  we  show  that  JIT-SDP  suffers  from  class  imbalance  evolution,  significantly  hindering  the  predictive  performance  of  existing  JIT-SDP  approaches.  Compared  to  state-of-the-art  class  imbalance  evolution  learning  approaches,  the  predictive  performance  of  JIT-SDP  approaches  was  up  to  97.2%  lower  in  terms  of  g-mean.  Hence,  it  is  essential  to  tackle  class  imbalance  evolution  in  JIT-SDP.  We  then  propose  a  novel  class  imbalance  evolution  approach  for  the  specific  context  of  JIT-SDP.  While  maintaining  top  ranked  g-means,  this  approach  managed  to  produce  up  to  63.59%  more  balanced  recalls  on  the  defect-inducing  and  clean  classes  than  state-of-the-art  class  imbalance  evolution  approaches.  We  thus  recommend  it  to  avoid  overemphasizing  one  class  over  the  other  in  JIT-SDP.
1	Some  reasons  why  actual  cross  fertilization  in  cross  functional  agile  teams  is  difficult.  Background:  Agile  teams  are  supposed  to  be  cross-functional  in  order  to  be  complete  (so  they  can  work  without  external  help).  Cross-functionality  is  also  supposed  to  produce  cross-fertilization:  Better  ideas  and  solutions,  problems  prevented  or  detected  earlier,  etc.  Question:  What  is  motivating  or  demotivating  team  members  to  work  in  a  cross-functional  manner?  Method:  We  conceptualize  observations  from  five  agile  teams  (work  observations,  interviews,  group  discussion)  and  from  interviews  with  five  agile  consultants/  coaches  by  applying  Grounded  Theory  Methodology.  Results:  The  inclination  to  interact  cross  functionally  is  moderated  by  at  least  six  factors  such  as  perceived  inefficiency,  a  sense  of  responsibility  for  one's  own  functional  domain,  or  the  difficulty  to  find  a  level  of  detail  that  is  suitable  for  the  collaboration.  Conclusion:  Cross-fertilization  is  harder  to  get  than  one  might  expect  and  teams  need  to  develop  good  judgment  to  succeed  at  it.
1	Poster  improving  formation  of  student  teams  a  clustering  approach.  Today's  courses  in  engineering  and  other  fields  frequently  involve  projects  done  by  teams  of  students.  An  important  aspect  of  these  team  assignments  is  the  formation  of  the  teams.  In  some  courses,  teams  select  different  topics  to  work  on.  Ideally,  team  formation  would  be  included  with  topic  selection,  so  teams  could  be  formed  from  students  interested  in  the  same  topics.  Intuitive  criteria  for  a  team  formation  algorithm  are  that  students  should  be  assigned  to  (1)  a  topic  which  they  have  interest  and  (2)  a  team  of  students  with  similar  interests  in  their  topic.  We  propose  an  approach  to  meeting  these  criteria  by  mining  student  preferences  for  topics  with  a  clustering  approach  and  then  matching  them  in  groups  to  topics  that  suit  their  shared  interests.  Our  implementation  is  based  on  hierarchical  k-means  clustering  and  a  weighting  formula  that  favors  increasing  overall  student  satisfaction  and  adding  members  until  the  maximum  allowable  team  size  is  reached.
1	Adding  sparkle  to  social  coding  an  empirical  study  of  repository  badges  in  the  npm  ecosystem.  In  fast-paced,  reuse-heavy,  and  distributed  software  development,  the  transparency  provided  by  social  coding  platforms  like  GitHub  is  essential  to  decision  making.  Developers  infer  the  quality  of  projects  using  visible  cues,  known  as  signals,  collected  from  personal  profile  and  repository  pages.  We  report  on  a  large-scale,  mixed-methods  empirical  study  of  npm  packages  that  explores  the  emerging  phenomenon  of  repository  badges,  with  which  maintainers  signal  underlying  qualities  about  their  projects  to  contributors  and  users.  We  investigate  which  qualities  maintainers  intend  to  signal  and  how  well  badges  correlate  with  those  qualities.  After  surveying  developers,  mining  294,941  repositories,  and  applying  statistical  modeling  and  time-series  analyses,  we  find  that  non-trivial  badges,  which  display  the  build  status,  test  coverage,  and  up-to-dateness  of  dependencies,  are  mostly  reliable  signals,  correlating  with  more  tests,  better  pull  requests,  and  fresher  dependencies.  Displaying  such  badges  correlates  with  best  practices,  but  the  effects  do  not  always  persist.
1	Understanding  the  factors  for  fast  answers  in  technical  q  a  websites  an  empirical  study  of  four  stack  exchange  websites.  Technical  questions  and  answers  (QA  ii)  the  current  incentive  system  does  not  recognize  non-frequent  answerers  who  often  answer  questions  which  frequent  answerers  are  not  able  to  answer  well.  Such  questions  that  are  answered  by  non-frequent  answerers  are  as  important  as  those  that  are  answered  by  frequent  answerers;  iii)  the  current  incentive  system  motivates  frequent  answerers  well,  but  such  frequent  answerers  tend  to  answer  short  questions.Our  findings  suggest  that  the  designers  of  Q&A  website  should  improve  their  incentive  systems  to  motivate  non-frequent  answerers  to  be  more  active  and  to  answer  questions  faster,  in  order  to  shorten  the  waiting  time  for  an  answer  (especially  for  questions  that  require  specific  knowledge  that  frequent  answerers  might  not  possess).  In  addition,  the  question  answering  incentive  system  needs  to  factor  in  the  value  and  difficulty  of  answering  the  questions  (e.g.,  by  providing  more  rewards  to  harder  questions  or  questions  that  remain  unanswered  for  a  long  period  of  time).
1	Poster  identifying  security  issues  in  software  development  are  keywords  enough.  Identifying  security  issues  before  attackers  do  has  become  a  critical  concern  for  software  development  teams  and  software  users.  While  methods  for  finding  programming  errors  (e.g.  fuzzers  1,  static  code  analysis  [3]  and  vulnerability  prediction  models  like  Scandariato  et  al.  [10])  are  valuable,  identifying  security  issues  related  to  the  lack  of  secure  design  principles  and  to  poor  development  processes  could  help  ensure  that  programming  errors  are  avoided  before  they  are  committed  to  source  code.  Typical  approaches  (e.g.  [4,  6--8])  to  identifying  security-related  messages  in  software  development  project  repositories  use  text  mining  based  on  pre-selected  sets  of  standard  security-related  keywords,  for  instance;  authentication,  ssl,  encryption,  availability,  or  password.  We  hypothesize  that  these  standard  keywords  may  not  capture  the  entire  spectrum  of  security-related  issues  in  a  project,  and  that  additional  project-specific  and/or  domain-specific  vocabulary  may  be  needed  to  develop  an  accurate  picture  of  a  project's  security.  For  instance,  Arnold  et  al.[1],  in  a  review  of  bug-fix  patches  on  Linux  kernel  version  2.6.24,  identified  a  commit  (commit  message:  "Fix  -  >vm_file  accounting,  mmap_region()  may  do  do_munmap()"  2)  with  serious  security  consequences  that  was  mis-classified  as  a  non-security  bug.  While  no  typical  security  keyword  is  mentioned,  memory  mapping  ('mmap')  in  the  domain  of  kernel  development  has  significance  from  a  security  perspective,  parallel  to  buffer  overflows  in  languages  like  C/C++.  Whether  memory  or  currency  is  at  stake,  identifying  changes  to  assets  that  the  software  manages  is  potentially  security-related.  The  goal  of  this  research  is  to  support  researchers  and  practitioners  in  identifying  security  issues  in  software  development  project  artifacts  by  defining  and  evaluating  a  systematic  scheme  for  identifying  project-specific  security  vocabularies  that  can  be  used  for  keyword-based  classification.  We  derive  three  research  questions  from  our  goal:  •  RQ1:  How  does  the  vocabulary  of  security  issues  vary  between  software  development  projects?  •  RQ2:  How  well  do  project-specific  security  vocabularies  identify  messages  related  to  publicly  reported  vulnerabilities?  •  RQ3:  How  well  do  existing  security  keywords  identify  project-specific  security-related  messages  and  messages  related  to  publicly  reported  vulnerabilities?  To  address  these  research  questions,  we  collected  developer  email,  bug  tracking,  commit  message,  and  CVE  record  project  artifacts  from  three  open  source  projects  :  Dolibarr,  Apache  Camel,  and  Apache  Derby.  We  manually  classified  5400  messages  from  the  three  project's  commit  messages,  bug  trackers,  and  emails,  and  linked  the  messages  to  each  project's  public  vulnerability  records,  Adapting  techniques  from  Bachmann  and  Bernstein  [2],  Schermann  et  al.  [11],  and  Guzzi  [5],  we  analyzed  each  project's  security  vocabulary  and  the  vocabulary's  relationship  to  the  project's  vulnerabilities.  We  trained  two  classifiers  (Model.A  and  Model.B)  on  samples  of  the  project  data,  and  used  the  classifiers  to  predict  security-related  messages  in  the  manually-classified  project  oracles.  Our  contributions  include:  •  A  systematic  scheme  for  linking  CVE  records  to  related  messages  in  software  development  project  artifacts  •  An  empirical  evaluation  of  project-specific  security  vocabulary  similarities  and  differences  between  project  artifacts  and  between  projects  To  summarize  our  findings  on  RQ1,  we  present  tables  of  our  qualitative  and  quantitative  results.  We  tabulated  counts  of  words  found  in  security-related  messages.  Traditional  security  keywords  (e.g.  password,  encryption)  are  present,  particularly  in  the  explicit  column,  but  each  project  also  contains  terms  describing  entities  unique  to  the  project,  for  example  'endpoint'  (Camel),  'blob'  (short  for  'Binary  Large  Object'),  'clob'  ('Character  Large  Object'),  'deadlock'  (Derby),  and  'invoice',  'order'  for  Dolibarr.  The  presence  of  these  terms  in  security-related  issues  suggests  that  they  are  assets  worthy  of  careful  attention  during  the  development  life  cycle.  Table  1  lists  the  statistics  for  security-related  messages  from  the  three  projects,  broken  down  by  security  class  and  security  property.  Explicit  security-related  messages  (messages  referencing  security  properties)  are  in  the  minority  in  each  project.  Implicit  messages  represent  the  majority  of  security-related  messages  in  each  project.  In  Table  2,  we  present  the  results  of  the  classifiers  built  using  the  various  project  and  literature  security  vocabularies  to  predict  security-related  messages  in  the  oracle  and  CVE  datasets.  We  have  marked  in  bold  the  highest  result  for  each  performance  measure  for  each  dataset.  Both  Models  A  and  B  have  a  high  performance  across  the  projects  when  predicting  for  the  oracle  dataset  of  the  project  for  which  they  were  built.  Further,  the  project-specific  models  have  higher  performance  than  the  literature-based  models  (Ray.vocab  [9]  and  Pletea.vocab  [7])  on  the  project  oracle  datasets.  Model  performance  is  not  sustained  and  is  inconsistent  when  applied  to  other  project's  datasets.  To  summarize  our  findings  on  RQ2,  Table  3  presents  performance  results  for  the  project  vocabulary  models  on  the  CVE  datasets  for  each  project.  We  have  marked  in  bold  the  highest  result  for  each  performance  measure  for  each  dataset.  Results  for  Model.A  shows  a  high  recall  for  Derby  and  Camel  and  a  worse  than  average  recall  for  Dolibarr.  However,  in  Model.B,  the  recall  is  above  60%  for  Dolibarr  and  over  85%  for  both  Derby  and  Camel.  We  reason  the  low  precision  is  due  to  our  approach  of  labeling  only  CVE-related  messages  as  security-related  and  the  rest  of  the  messages  are  labeled  to  be  not  security-related.  The  Dolibarr  results  are  further  complicated  by  the  low  proportion  of  security-related  messages  compared  with  the  other  two  projects  (as  reported  in  1).  To  summarize  our  findings  on  RQ3,  Table  2  and  Table  3  present  the  classifier  performance  results  for  two  sets  of  keywords,  Ray.vocab,  and  Pletea.vocab,  drawn  from  the  literature.  In  each  case,  the  project  vocabulary  model  had  the  highest  recall,  precision  and  F-Score  on  the  project's  oracle  dataset.  With  regards  to  the  CVE-dataset,  the  project  vocabulary  model  has  the  highest  recall.  However,  the  overall  performance,  as  measured  by  F-Score,  varied  by  dataset,  with  the  Ray  and  Pletea  keywords  scoring  higher  than  the  project  vocabulary  model.  The  low  precision  for  the  classifier  built  on  the  project's  vocabularies  follows  the  explanation  provided  under  RQ2.  Our  results  suggest  that  domain  vocabulary  model  show  recalls  that  outperform  standard  security  terms  across  our  datasets.  Our  conjecture,  supported  in  our  data,  is  that  augmenting  standard  security  keywords  with  a  project's  security  vocabulary  yields  a  more  accurate  security  picture.  In  future  work,  we  aim  to  refine  vocabulary  selection  to  improve  classifier  performance,  and  to  define  tools  implementing  the  approach  in  this  paper  to  aid  practitioners  and  researchers  in  identifying  software  project  security  issues.
1	Automated  program  repair  with  canonical  constraints.  Automated  program  repair  (APR)  seeks  to  improve  the  speed  and  decrease  the  cost  of  repairing  software  bugs.  Existing  APR  approaches  use  unit  tests  or  constraint  solving  to  find  and  validate  program  patches.  We  propose  Canonical  Search  And  Repair  (CSAR),  a  program  repair  technique  based  on  semantic  search  which  uses  a  canonical  form  of  the  path  conditions  to  characterize  buggy  and  patch  code  and  allows  for  easy  storage  and  retrieval  of  software  patches,  without  the  need  for  expensive  constraint  solving.  CSAR  uses  string  metrics  over  the  canonical  forms  to  cheaply  measure  semantic  distance  between  patches  and  buggy  code  and  uses  a  classifier  to  identify  situations  in  which  test  suite  executions  are  unnecessary-and  to  provide  a  finer-grained  means  of  differentiating  between  potential  patches.  We  evaluate  CSAR  on  the  IntroClass  benchmark,  and  show  that  CSAR  finds  more  correct  patches  (96%  increase)  than  previous  semantic  search  approaches,  and  more  correct  patches  (34%  increase)  than  other  previous  state-of-the-art  in  program  repair.
1	Understanding  devops  education  with  grounded  theory.  DevOps  stands  for  Development-Operations.  It  arises  from  the  IT  industry  as  a  movement  aligning  development  and  operations  teams.  DevOps  is  broadly  recognized  as  an  IT  standard,  and  there  is  high  demand  for  DevOps  practitioners  in  industry.  Therefore,  we  studied  whether  undergraduates  acquired  adequate  DevOps  skills  to  fulfill  the  demand  for  DevOps  practitioners  in  industry.  We  employed  Grounded  Theory  (GT),  a  social  science  qualitative  research  methodology,  to  study  DevOps  education  from  academic  and  industrial  perspectives.  In  academia,  academics  were  not  motivated  to  learn  or  adopt  DevOps,  and  we  did  not  find  strong  evidence  of  academics  teaching  DevOps.  Academics  need  incentives  to  adopt  DevOps,  in  order  to  stimulate  interest  in  teaching  DevOps.  In  industry,  DevOps  practitioners  lack  clearly  defined  roles  and  responsibilities,  for  the  DevOps  topic  is  diverse  and  growing  too  fast.  Therefore,  practitioners  can  only  learn  DevOps  through  hands-on  working  experience.  As  a  result,  academic  institutions  should  provide  fundamental  DevOps  education  (in  culture,  procedure,  and  technology)  to  prepare  students  for  their  future  DevOps  advancement  in  industry.  Based  on  our  findings,  we  proposed  five  groups  of  future  studies  to  advance  DevOps  education  in  academia.
1	Evosuite  at  the  sbst  2020  tool  competition.  EvoSuite  is  a  search-based  tool  that  automatically  generates  executable  unit  tests  for  Java  code  (JUnit  tests).  This  paper  summarizes  the  results  and  experiences  of  EvoSuite's  participation  at  the  eighth  unit  testing  competition  at  SBST  2020,  where  EvoSuite  achieved  the  highest  overall  score  (406.14  points)  for  the  seventh  time  in  eight  editions  of  the  competition.
1	Demystify  official  api  usage  directives  with  crowdsourced  api  misuse  scenarios  erroneous  code  examples  and  patches.  API  usage  directives  in  official  API  documentation  describe  the  contracts,  constraints  and  guidelines  for  using  APIs  in  natural  language.  Through  the  investigation  of  API  misuse  scenarios  on  Stack  Overflow,  we  identify  three  barriers  that  hinder  the  understanding  of  the  API  usage  directives,  i.e.,  lack  of  specific  usage  context,  indirect  relationships  to  cooperative  APIs,  and  confusing  APIs  with  subtle  differences.  To  overcome  these  barriers,  we  develop  a  text  mining  approach  to  discover  the  crowdsourced  API  misuse  scenarios  on  Stack  Overflow  and  extract  from  these  scenarios  erroneous  code  examples  and  patches,  as  well  as  related  API  and  confusing  APIs  to  construct  demystification  reports  to  help  developers  understand  the  official  API  usage  directives  described  in  natural  language.  We  apply  our  approach  to  API  usage  directives  in  official  Android  API  documentation  and  android-tagged  discussion  threads  on  Stack  Overflow.  We  extract  159,116  API  misuse  scenarios  for  23,969  API  usage  directives  of  3138  classes  and  7471  methods,  from  which  we  generate  the  demystification  reports.  Our  manual  examination  confirms  that  the  extracted  information  in  the  generated  demystification  reports  are  of  high  accuracy.  By  a  user  study  of  14  developers  on  8  API-misuse  related  error  scenarios,  we  show  that  our  demystification  reports  help  developer  understand  and  debug  API-misuse  related  program  errors  faster  and  more  accurately,  compared  with  reading  only  plain  API  usage-directive  sentences.
1	Eliminative  induction  a  basis  for  arguing  system  confidence.  Assurance  cases  provide  a  structured  method  of  explaining  why  a  system  has  some  desired  property,  e.g.,  that  the  system  is  safe.  But  there  is  no  agreed  approach  for  explaining  what  degree  of  confidence  one  should  have  in  the  conclusions  of  such  a  case.  In  this  paper,  we  use  the  principle  of  eliminative  induction  to  provide  a  justified  basis  for  assessing  how  much  confidence  one  should  have  in  an  assurance  case  argument.
1	New  approach  for  automatic  medical  image  annotation  using  the  bag  of  words  model.  In  this  paper,  we  present  a  new  approach  for  semantic  automatic  annotation  of  medical  images.  Indeed,  the  proposed  approach  uses  the  bag  of  words  model  to  represent  the  visual  content  of  the  medical  image  combined  with  text  descriptors  based  on  term  frequency-inverse  document  frequency  technique  and  reduced  by  latent  semantic  to  extract  the  co-occurrence  between  text  and  visual  terms.  In  a  first  phase,  we  are  interested  in  indexing  texts  and  extracting  all  relevant  terms  using  a  thesaurus  containing  medical  subject  headings  and  concepts.  In  a  second  phase,  medical  images  are  indexed  while  recovering  areas  of  interest  which  are  invariant  to  change  in  scale  such  as  light  and  tilt.  To  annotate  a  new  medical  image,  we  use  the  bag  of  words  model  to  recover  the  feature  vector.  Indeed,  we  use  the  vector  space  model  to  retrieve  similar  medical  images  from  the  training  database.  The  computation  of  the  relevance  value  of  an  image  according  to  a  query  image  is  based  on  the  cosine  function.  To  evaluate  the  performance  of  our  proposed  approach,  we  present  an  experiment  carried  out  on  five  types  of  radiological  imaging.  The  results  showed  that  our  approach  works  efficiently,  especially  with  more  images  taken  from  the  radiology  of  the  skull.
1	Easy  modelling  and  verification  of  unpredictable  and  preemptive  interrupt  driven  systems.  The  widespread  real-time  and  embedded  systems  are  mostly  interrupt-driven  because  their  heavy  interaction  with  the  environment  is  often  initiated  by  interrupts.  With  the  interrupt  arrival  being  unpredictable  and  the  interrupt  handling  being  preemptive,  a  large  number  of  possible  system  behaviours  are  generated,  which  makes  the  correctness  assurance  of  such  systems  difficult  and  costly.  Model  checking  is  considered  to  be  one  of  the  effective  methods  for  exhausting  behavioural  state  space  for  correctness.  However,  existing  modelling  approaches  for  interrupt-driven  systems  are  based  on  either  calculus  or  automata  theory,  and  have  a  steep  learning  curve.  To  address  this  problem,  we  propose  a  new  modelling  language  called  interrupt  sequence  diagram  (ISD).  By  extending  the  popular  UML  sequence  diagram  notations,  the  ISD  supports  the  modelling  of  interrupts'  essential  features  visually  and  concisely.  We  also  propose  an  automata-based  semantics  for  ISD,  based  on  which  ISD  can  be  transformed  to  a  subset  of  hybrid  automata  so  as  to  leverage  the  abundant  off-the-shelf  checkers.  Experiments  on  examples  from  both  real-world  and  existing  literature  were  conducted,  and  the  results  demonstrate  our  approach's  usability  and  effectiveness.
1	Sea  scale  agent  based  simulator  of  solea  solea  in  the  adriatic  sea.  DISPAS  is  an  agent-based  simulator  for  fish  stock  assessment  developed  as  a  decision  making  support  for  the  sustainable  management  of  fishery.  In  this  work  we  enlarge  the  underlying  model  of  DISPAS  allowing  it  to  model  and  simulate  a  multi-scale  scenario.  We  retain  the  currently  available  spatial  scale,  able  to  represent  a  limited  average  region  of  the  sea,  and  we  introduce  a  new  spatial  macro-scale,  able  to  represent  the  whole  sea.  At  the  macro-scale  a  single  agent  represents  an  area  of  five  square  nautical  miles  and  manages  groups  of  fish  in  different  age  classes.  The  interactions  among  the  macro  agents  permit  the  exchange  of  individuals  of  each  class  among  neighbor  areas.  A  case  study  regarding  the  Solea  solea  (Linnaeus,  1758;  Soleidae)  stock  of  the  northern  Adriatic  Sea  is  used  to  show  the  intended  approach,  taking  into  account  the  available  data,  coming  from  fishery  independent  scientific  surveys.
1	Towards  a  context  dependent  java  exceptions  hierarchy.  The  role  of  exceptions  is  crucial  for  the  robustness  of  modern  applications  and  critical  systems.  Despite  this,  there  is  a  long  debate  among  researchers,  programming  language  designers,  and  practitioners  regarding  the  usefulness  and  appropriateness  of  the  available  exception  types  and  their  classification.  In  this  paper,  we  examine  Java  exceptions  and  propose  a  new  class  hierarchy  and  compile-time  mechanisms  that  take  into  account  the  context  in  which  exceptions  can  arise.  We  believe  that  the  increased  specificity  of  exception  handling  based  on  our  proposal  can  boost  its  effectiveness  and  lead  to  fewer  application  failures.
1	Facilitating  communication  between  engineers  with  cares.  When  software  developers  need  to  exchange  information  or  coordinate  work  with  colleagues  on  other  teams,  they  are  often  faced  with  the  challenge  of  finding  the  right  person  to  communicate  with.  In  this  paper,  we  present  our  tool,  called  CARES  (Colleagues  and  Relevant  Engineers'  Support),  which  is  an  integrated  development  environment-based  (IDE)  tool  that  enables  engineers  to  easily  discover  and  communicate  with  the  people  who  have  contributed  to  the  source  code.  CARES  has  been  deployed  to  30  professional  developers,  and  we  interviewed  8  of  them  after  3  weeks  of  evaluation.  They  reported  that  CARES  helped  them  to  more  quickly  find,  choose,  and  initiate  contact  with  the  most  relevant  and  expedient  person  who  could  address  their  needs.
1	Feature  selection  using  feature  ranking  correlation  analysis  and  chaotic  binary  particle  swarm  optimization.  In  this  paper,  we  propose  a  multi-stage  feature  selection  algorithm,  which  focuses  on  the  reduction  of  redundant  features  and  the  improvement  of  classification  performance  using  feature  ranking  (FR),  correlation  analysis  (CA)  and  chaotic  binary  particle  swarm  optimization  (CBPSO).  In  the  first  stage,  with  the  purpose  of  selecting  the  most  effective  features  for  classification,  FR  is  introduced  to  select  the  top-ranked  features  according  to  the  classification  accuracies.  In  the  second  stage,  CA  is  used  to  measure  the  correlation  among  the  selected  top-ranked  features  for  reducing  redundant  features.  In  the  third  stage,  in  order  to  further  eliminate  redundant  features  and  improve  the  classification  performances,  CBPSO  is  adopted  to  search  the  optimal  feature  subset.  Ultimately,  feature  selection  can  be  completed  by  using  only  some  top-ranked  features  with  less  redundancy  for  classification.  Support  vector  machine  (SVM)  with  n-fold  cross-validation  is  adopted  to  assess  the  classification  performances  on  six  datasets  in  the  experiments.  Experimental  results  show  that  the  proposed  algorithm  can  achieve  better  performance  in  terms  of  classification  accuracy  and  the  number  of  features  than  benchmark  algorithms.
1	Network  education  video  recommendation  algorithm  based  on  context  and  trust  relationship.  With  the  development  of  information  technology  and  the  internet,  there  are  many  education  videos  on  the  network  for  people  to  study  in  their  spare  time.  However,  it  is  difficult  for  people  to  choose  education  videos  which  they  really  need.  To  solve  the  problem,  a  personalized  recommendation  algorithm  based  on  context  and  trust  relationship  is  proposed  in  this  paper.  Under  the  help  of  this  algorithm,  education  videos  interested  by  users  can  be  proactive  recommended.  The  algorithm  improves  traditional  filtering  recommendation  algorithm.  It  is  divided  into  three  parts.  One  candidate  video  set  is  firstly  obtained  according  to  user-rating  matrix  and  context,  and  then  another  set  is  obtained  according  to  trust  relationships  between  users.  Finally,  the  former  two  candidate  sets  are  combined  to  determine  the  recommendation  video  set.  Experiments  indicate  that  the  proposed  algorithm  is  more  accurate  than  traditional  collaborative  filtering  algorithm.
1	Product  assignment  recommender.  Effectiveness  of  software  development  process  depends  on  the  accuracy  of  data  in  supporting  tools.  In  particular,  a  customer  issue  assigned  to  a  wrong  product  team  takes  much  longer  to  resolve  (negatively  affecting  user-perceived  quality)  and  wastes  developer  effort.  In  Open  Source  Software  (OSS)  and  in  commercial  projects  values  in  issue-tracking  systems  (ITS)  or  Customer  Relationship  Management  (CRM)  systems  are  often  assigned  by  non-developers  for  whom  the  assignment  task  is  difficult.  We  propose  PAR  (Product  Assignment  Recommender)  to  estimate  the  odds  that  a  value  in  the  ITS  is  incorrect.  PAR  learns  from  the  past  activities  in  ITS  and  performs  prediction  using  a  logistic  regression  model.  Our  demonstrations  show  how  PAR  helps  developers  to  focus  on  fixing  real  problems,  and  how  it  can  be  used  to  improve  data  accuracy  in  ITS  by  crowd-sourcing  non-developers  to  verify  and  correct  low-accuracy  data.  http://youtu.be/IuykbzSTj8s
1	Quasi  crowdsourcing  testing  for  educational  projects.  The  idea  of  crowdsourcing  tasks  in  software  engineering,  especially  software  testing,  has  gained  popularity  in  recent  years.  Crowdsourcing  testing  and  educational  projects  are  natural  complementary.  One  of  the  challenges  of  crowdsourcing  testing  is  to  find  a  number  of  qualified  workers  with  low  cost.  Students  in  software  engineering  are  suitable  candidates  for  crowdsourcing  testing.  On  the  other  hand,  practical  projects  play  a  key  role  in  software  engineering  education.  In  order  to  enhance  educational  project  outcomes  and  achieve  industrial-strength  training,  we  need  to  provide  the  opportunity  for  students  to  be  exposed  to  commercial  software  development.          In  this  paper,  we  report  a  preliminary  study  on  crowdsourcing  testing  for  educational  projects.  We  introduce  three  commercial  software  products  as  educational  testing  projects,  which  are  crowdsourced  by  our  teaching  support  system.  We  call  this  "Quasi-Crowdsourcing  Test"  (QCT)  because  the  candidate  workers  are  students,  who  have  certain  social  relations.  The  investigation  results  are  encouraging  and  show  to  be  beneficial  to  both  the  students  and  industry  in  QCT  projects.
1	Guaranteeing  correct  evolution  of  software  product  lines  setting  up  the  problem.  The  research  question  that  we  posed  ourselves  and  which  has  led  to  this  paper  is:  how  can  we  guarantee  the  correct  functioning  of  products  of  an  SPL  when  core  components  evolve?  This  exploratory  paper  merely  proposes  an  overview  of  a  novel  approach  that,  by  extending  and  adapting  assume-guarantee  reasoning  to  evolving  SPLs,  guarantees  the  resilience  against  changes  in  the  environment  of  products  of  an  SPL.  The  idea  is  to  selectively  model  check  and  test  assume-guarantee  properties  on  those  SPL  components  affected  by  the  changes.
1	Improving  object  oriented  lack  of  cohesion  metric  by  excluding  special  methods.  Classes  are  the  basic  units  in  object-oriented  programs,  and  therefore,  their  quality  has  impact  on  the  overall  quality  of  the  software.  Class  cohesion  is  a  key  quality  factor,  and  it  refers  to  the  degree  of  relatedness  of  class  attributes  and  methods.  Software  developers  use  class  cohesion  measure  to  assess  the  quality  of  their  products  and  to  guide  the  restructuring  of  poorly  designed  classes.  Several  class  cohesion  metrics  are  proposed  in  the  literature,  and  the  impact  of  considering  the  special  methods  (i.e.,  constructors,  destructors,  and  access  methods)  in  cohesion  calculation  is  not  empirically  studied  for  most  of  them.  In  this  paper,  we  address  this  issue  for  one  of  the  most  popular  class  cohesion  metrics,  referenced  as  Lack  of  Cohesion  (LCOM).  Our  empirical  study  involves  applying  the  metric  with  and  without  considering  special  methods  on  classes  of  two  open  source  Java  applications  and  statistically  analyzing  the  results.  The  empirical  study  results  show  that  the  ability  of  LCOM  in  indicating  class  quality  slightly  improves  when  excluding  special  methods  from  the  LCOM  computation.
1	Toward  comprehensible  software  defect  prediction  models  using  fuzzy  logic.  Software  defect  prediction  is  a  discipline  that  predicts  the  defects  proneness  of  future  modules.  Software  metrics  are  used  for  this  kind  of  predication.  However,  the  predication  metrics  are  associated  with  uncertainty,  thus  the  metrics  need  to  be  expressed  in  linguistic  terms  to  overcome  ambiguity  and  uncertainty.  Two  types  of  knowledge  are  utilized  as  input  to  the  prediction  models:  software  metrics  and  expert's  opinions.  This  paper  proposes  a  framework  for  developing  fuzzy  logic-based  software  predication  model  using  different  set  of  software  metrics.  It  aims  to  provide  a  generic  set  of  metrics  to  be  used  for  software  defects  prediction.  The  performance  of  the  proposed  Fuzzy-based  models  has  been  validated  using  real  software  projects  data  where  Takagi-Sugeno  fuzzy  inference  engine  is  used  to  predict  software  defects.  Validation  results  are  satisfactory.
1	Values  first  se  research  principles  in  practice.  The  realization  that  software  has  a  far  reaching  impact  on  politics,  society  and  the  environment  is  not  new.  However,  only  recently  software  impact  has  been  explicitly  described  as  `systemic'  and  framed  around  complex  social  problems  such  as  sustainability.  We  argue  that  `wicked'  social  problems  are  consequences  of  the  interplay  between  complex  economical,  technical  and  political  interactions  and  their  underlying  value  choices.  Such  choices  are  guided  by  specific  sets  of  human  values  that  have  been  found  in  all  cultures  by  extensive  evidence-based  research.  The  aim  of  this  paper  is  to  give  more  visibility  to  the  interrelationship  between  values  and  SE  choices.  To  this  end,  we  first  introduce  the  concept  of  Values-First  SE  and  reflect  on  its  implications  for  software  development.  Our  contribution  to  SE  is  embedding  the  principles  of  values  research  in  the  SE  decision  making  process  and  extracting  lessons  learned  from  practice.
1	Mecc  memory  comparison  based  clone  detector.  In  this  paper,  we  propose  a  new  semantic  clone  detection  technique  by  comparing  programs'  abstract  memory  states,  which  are  computed  by  a  semantic-based  static  analyzer.      Our  experimental  study  using  three  large-scale  open  source  projects  shows  that  our  technique  can  detect  semantic  clones  that  existing  syntactic-  or  semantic-based  clone  detectors  miss.  Our  technique  can  help  developers  identify  inconsistent  clone  changes,  find  refactoring  candidates,  and  understand  software  evolution  related  to  semantic  clones.
1	Bottom  up  technologies  for  reuse  automated  extractive  adoption  of  software  product  lines.  Adopting  Software  Product  Line  (SPL)  engineering  principles  demands  a  high  up-front  investment.  Bottom-Up  Technologies  for  Reuse  (BUT4Reuse)  is  a  generic  and  extensible  tool  aimed  to  leverage  existing  similar  software  products  in  order  to  help  in  extractive  SPL  adoption.  The  envisioned  users  are  1)  SPL  adopters  and  2)  Integrators  of  techniques  and  algorithms  to  provide  automation  in  SPL  adoption  activities.  We  present  the  methodology  it  implies  for  both  types  of  users  and  we  present  the  validation  studies  that  were  already  conducted.  BUT4Reuse  tool  and  source  code  are  publicly  available  under  the  EPL  license.      Website:  http://but4reuse.github.io      Video:  https://www.youtube.com/watch?v=pa62Yc9LWyk
1	Metamorphic  testing  for  graphics  compilers.  We  present  strategies  for  metamorphic  testing  of  compilers  using  opaque  value  injection,  and  experiences  using  the  method  to  test  compilers  for  the  OpenGL  shading  language.
1	Inferring  method  specifications  from  natural  language  api  descriptions.  Application  Programming  Interface  (API)  documents  are  a  typical  way  of  describing  legal  usage  of  reusable  software  libraries,  thus  facilitating  software  reuse.  However,  even  with  such  documents,  developers  often  overlook  some  documents  and  build  software  systems  that  are  inconsistent  with  the  legal  usage  of  those  libraries.  Existing  software  verification  tools  require  formal  specifications  (such  as  code  contracts),  and  therefore  cannot  directly  verify  the  legal  usage  described  in  natural  language  text  of  API  documents  against  the  code  using  that  library.  However,  in  practice,  most  libraries  do  not  come  with  formal  specifications,  thus  hindering  tool-based  verification.  To  address  this  issue,  we  propose  a  novel  approach  to  infer  formal  specifications  from  natural  language  text  of  API  documents.  Our  evaluation  results  show  that  our  approach  achieves  an  average  of  92%  precision  and  93%  recall  in  identifying  sentences  that  describe  code  contracts  from  more  than  2500  sentences  of  API  documents.  Furthermore,  our  results  show  that  our  approach  has  an  average  83%  accuracy  in  inferring  specifications  from  over  1600  sentences  describing  code  contracts.
1	Prodirect  manipulation  bidirectional  programming  for  the  masses.  Software  interfaces  today  generally  fall  at  either  end  of  a  spectrum.  On  one  end  are  programmable  systems,  which  allow  expert  users  (i.e.  programmers)  to  write  software  artifacts  that  describe  complex  abstractions,  but  programs  are  disconnected  from  their  eventual  output.  On  the  other  end  are  domain-specific  graphical  user  interfaces  (GUIs),  which  allow  end  users  (i.e.  non-programmers)  to  easily  create  varied  content  but  present  insurmountable  walls  when  a  desired  feature  is  not  built-in.  Both  programmatic  and  direct  manipulation  have  distinct  strengths,  but  users  must  typically  choose  one  over  the  other  or  use  some  ad-hoc  combination  of  systems.  Our  goal,  put  simply,  is  to  bridge  this  divide.      We  envision  novel  software  systems  that  tightly  couple  programmatic  and  direct  manipulation  ---  a  combination  we  dub  prodirect  manipulation  ---  for  a  variety  of  use  cases.  This  will  require  advances  in  a  broad  range  of  software  engineering  disciplines,  from  program  analysis  and  program  synthesis  technology  to  user  interface  design  and  evaluation.  In  this  extended  abstract,  we  propose  two  general  strategies  ---  real-time  program  synthesis  and  domain-specific  synthesis  of  general-purpose  programs  ---  that  may  prove  fruitful  for  overcoming  the  technical  challenges.  We  also  discuss  metrics  that  will  be  important  in  evaluating  the  usability  and  utility  of  prodirect  manipulation  systems.
1	A  practical  guide  to  select  quality  indicators  for  assessing  pareto  based  search  algorithms  in  search  based  software  engineering.  Many  software  engineering  problems  are  multi-objective  in  nature,  which  has  been  largely  recognized  by  the  Search-based  Software  Engineering  (SBSE)  community.  In  this  regard,  Pareto-  based  search  algorithms,  e.g.,  Non-dominated  Sorting  Genetic  Algorithm  II,  have  already  shown  good  performance  for  solving  multi-objective  optimization  problems.  These  algorithms  produce  Pareto  fronts,  where  each  Pareto  front  consists  of  a  set  of  non-  dominated  solutions.  Eventually,  a  user  selects  one  or  more  of  the  solutions  from  a  Pareto  front  for  their  specific  problems.  A  key  challenge  of  applying  Pareto-based  search  algorithms  is  to  select  appropriate  quality  indicators,  e.g.,  hypervolume,  to  assess  the  quality  of  Pareto  fronts.  Based  on  the  results  of  an  extended  literature  review,  we  found  that  the  current  literature  and  practice  in  SBSE  lacks  a  practical  guide  for  selecting  quality  indicators  despite  a  large  number  of  published  SBSE  works.  In  this  direction,  the  paper  presents  a  practical  guide  for  the  SBSE  community  to  select  quality  indicators  for  assessing  Pareto-based  search  algorithms  in  different  software  engineering  contexts.  The  practical  guide  is  derived  from  the  following  complementary  theoretical  and  empirical  methods:  1)  key  theoretical  foundations  of  quality  indicators;  2)  evidence  from  an  extended  literature  review;  and  3)  evidence  collected  from  an  extensive  experiment  that  was  conducted  to  evaluate  eight  quality  indicators  from  four  different  categories  with  six  Pareto-based  search  algorithms  using  three  real  industrial  problems  from  two  diverse  domains.
1	An  extensible  regular  expression  based  tool  for  multi  language  mutant  generation.  Mutation  testing  is  widely  used  in  research  (even  if  not  in  practice).  Mutation  testing  tools  usually  target  only  one  programming  language  and  rely  on  parsing  a  program  to  generate  mutants,  or  operate  not  at  the  source  level  but  on  compiled  bytecode.  Unfortunately,  developing  a  robust  mutation  testing  tool  for  a  new  language  in  this  paradigm  is  a  difficult  and  time-consuming  undertaking.  Moreover,  bytecode/intermediate  language  mutants  are  difficult  for  programmers  to  read  and  understand.  This  paper  presents  a  simple  tool,  called  universalmutator,  based  on  regular-expression-defined  transformations  of  source  code.  The  primary  drawback  of  such  an  approach  is  that  our  tool  can  generate  invalid  mutants  that  do  not  compile,  and  sometimes  fails  to  generate  mutants  that  a  parser-based  tool  would  have  produced.  Additionally,  it  is  incompatible  with  some  approaches  to  improving  the  efficiency  of  mutation  testing.  However,  the  regexp-based  approach  provides  multiple  compensating  advantages.  First,  our  tool  is  easy  to  adapt  to  new  languages;  e.g.,  we  present  here  the  first  mutation  tool  for  Apple's  Swift  programming  language.  Second,  the  method  makes  handling  multi-language  programs  and  systems  simple,  because  the  same  tool  can  support  every  language.  Finally,  our  approach  makes  it  easy  for  users  to  add  custom,  project-specific  mutations.
1	Data  scientists  in  software  teams  state  of  the  art  and  challenges.  The  demand  for  analyzing  large  scale  telemetry,  machine,  and  quality  data  is  rapidly  increasing  in  software  industry.  Data  scientists  are  becoming  popular  within  software  teams.  For  example,  Face-book,  LinkedIn  and  Microsoft  are  creating  a  new  career  path  for  data  scientists.  In  this  paper,  we  present  a  large-scale  survey  with  793  professional  data  scientists  at  Microsoft  to  understand  their  educational  background,  problem  topics  that  they  work  on,  tool  usages,  and  activities.  We  cluster  these  data  scientists  based  on  the  time  spent  for  various  activities  and  identify  9  distinct  clusters  of  data  scientists  and  their  corresponding  characteristics.  We  also  discuss  the  challenges  that  they  face  and  the  best  practices  they  share  with  other  data  scientists.  Our  study  finds  several  trends  about  data  scientists  in  the  software  engineering  context  at  Microsoft,  and  should  inform  managers  on  how  to  leverage  data  science  capability  effectively  within  their  teams.
1	Assistive  computing  a  human  centered  approach  to  developing  computing  support  for  cognition.  The  growing  population  of  cognitively  impaired  individuals  calls  for  the  emergence  of  a  research  area  dedicated  to  developing  computing  systems  that  address  their  needs.  The  nature  of  this  research  area  requires  to  bridge  the  many  disciplines  needed  to  develop  human-centered,  assistive  computing  systems.  Such  bridging  may  seem  unattainable  considering  the  conceptual  and  practical  gaps  between  the  related  disciplines  and  the  challenges  of  propagating  human-related  concerns  throughout  the  many  stages  of  the  development  process  of  assistive  technologies.  As  a  consequence,  existing  assistive  technologies  lack  a  proper  needs  analysis;  their  development  is  often  driven  by  technology  concerns,  resulting  in  ill-designed  and  stereotype-biased  systems;  and,  most  of  them  are  not  tested  for  their  effectiveness  in  assisting  users.  In  this  paper,  we  propose  a  systematic  exploration  of  this  vast  challenge.  First,  we  define  Assistive  Computing  as  a  research  area  and  propose  key  principles  to  drive  its  study.  Then,  we  introduce  a  tool-based  methodology  dedicated  to  developing  assistive  computing  support,  integrating  a  range  of  disciplines  from  human-related  sciences  to  computer  science.  This  methodology  is  purposefully  pragmatic  in  that  it  leverages,  aggregates  and  revisits  numerous  research  results,  concretizing  it  with  a  range  of  examples.  More  generally,  our  goal  is  i)  to  provide  a  framework  to  conduct  research  in  the  area  of  Assistive  Computing  and  ii)  to  identify  the  necessary  bridges  between  disciplines  to  account  for  all  the  dimensions  of  such  systems.
1	Time  series  clustering  based  on  dynamic  time  warping.  In  general,  solving  prediction  problems  requires  a  series  of  operations  for  the  data  set  such  as  preprocessing,  partitioning,  and  structuring  features,  so  as  to  fit  a  better  prediction  model.  For  time  series  data,  it  is  divided  into  different  data  sets  according  to  certain  rules  to  achieve  the  effect  of  improving  the  accuracy  of  the  prediction  model.  This  paper  proposes  a  more  novel  clustering  method  which  the  traditional  Euclidean  distance  and  dynamic  time  planning  are  separately  weighted  and  combined  to  do  the  distance  calculation  method  in  clustering.  A  time  series  contains  both  a  time  dimension  and  a  spatial  dimension.  Euclidean  distance  is  mainly  used  for  spatial  distance  calculation.  Dynamic  time  warping  can  calculate  the  similarity  calculation  in  time  dimension,  similar  to  the  distance  calculation  in  the  spatial  dimension.  The  measure  of  similarity  of  time  series  is  a  measure  of  the  degree  of  similarity  between  two  time  series.  It  is  verified  by  experiments  that  under  the  same  prediction  model,  this  novel  clustering  method  is  better  than  the  Euclidean  distance  clustering  method  and  the  traditional  dynamic  time  warping  method.
1	When  to  extract  features  towards  a  recommender  system.  In  practice,  many  organizations  rely  on  cloning  to  implement  customer-specific  variants  of  a  system.  While  this  approach  can  have  several  disadvantages,  organizations  fear  to  extract  reusable  features  later  on,  due  to  the  corresponding  efforts  and  risks.  A  particularly  challenging  and  poorly  supported  task  is  to  decide  which  features  to  extract.  To  tackle  this  problem,  we  aim  to  develop  a  recommender  system  that  proposes  suitable  features  based  on  automated  analyses  of  the  cloned  legacy  systems.  In  this  paper,  we  sketch  this  recommender  and  its  empirically  derived  metrics,  which  comprise  cohesion,  impact,  and  costs  of  features  as  well  as  the  users'  previous  decisions.  Overall,  we  will  facilitate  the  adoption  of  systematic  reuse  based  on  an  integrated  platform.
1	Unrealizable  cores  for  reactive  systems  specifications  artifact.  This  document  describes  the  artifact  that  accompanies  the  ICSE'21  paper  "Unrealizable  Cores  for  Reactive  Systems  Specifications".  The  artifact  includes  the  specifications  that  were  used  in  the  experiments  that  are  described  in  the  paper.  It  further  includes  an  executable  that  allows  interested  readers  to  reproduce  these  experiments  and  inspect  their  results.  Additionally,  the  executable  is  applicable  to  any  specification  in  Spectra  format,  which  allows  conducting  similar  experiments  over  any  Spectra  specification.  We  hope  the  artifact  will  be  useful  for  researchers  who  are  interested  in  reactive  synthesis,  specifically  in  different  means  to  deal  with  unrealizable  specifications.
1	Hydiff  hybrid  differential  software  analysis.  Detecting  regression  bugs  in  software  evolution,  analyzing  side-channels  in  programs  and  evaluating  robustness  in  deep  neural  networks  (DNNs)  can  all  be  seen  as  instances  of  differential  software  analysis,  where  the  goal  is  to  generate  diverging  executions  of  program  paths.  Two  executions  are  said  to  be  diverging  if  the  observable  program  behavior  differs,  e.g.,  in  terms  of  program  output,  execution  time,  or  (DNN)  classification.  The  key  challenge  of  differential  software  analysis  is  to  simultaneously  reason  about  multiple  program  paths,  often  across  program  variants.  This  paper  presents  HyDiff,  the  first  hybrid  approach  for  differential  software  analysis.  HyDiff  integrates  and  extends  two  very  successful  testing  techniques:  Feedback-directed  greybox  fuzzing  for  efficient  program  testing  and  shadow  symbolic  execution  for  systematic  program  exploration.  HyDiff  extends  greybox  fuzzing  with  divergence-driven  feedback  based  on  novel  cost  metrics  that  also  take  into  account  the  control  flow  graph  of  the  program.  Furthermore  HyDiff  extends  shadow  symbolic  execution  by  applying  four-way  forking  in  a  systematic  exploration  and  still  having  the  ability  to  incorporate  concrete  inputs  in  the  analysis.  HyDiff  applies  divergence  revealing  heuristics  based  on  resource  consumption  and  control-flow  information  to  efficiently  guide  the  symbolic  exploration,  which  allows  its  efficient  usage  beyond  regression  testing  applications.  We  introduce  differential  metrics  such  as  output,  decision  and  cost  difference,  as  well  as  patch  distance,  to  assist  the  fuzzing  and  symbolic  execution  components  in  maximizing  the  execution  divergence.  We  implemented  our  approach  on  top  of  the  fuzzer  AFL  and  the  symbolic  execution  framework  Symbolic  PathFinder.  We  illustrate  HyDiff  on  regression  and  side-channel  analysis  for  Java  bytecode  programs,  and  further  show  how  to  use  HyDiff  for  robustness  analysis  of  neural  networks.
1	Tricorder  building  a  program  analysis  ecosystem.  Static  analysis  tools  help  developers  find  bugs,  improve  code  readability,  and  ensure  consistent  style  across  a  project.  However,  these  tools  can  be  difficult  to  smoothly  integrate  with  each  other  and  into  the  developer  workflow,  particularly  when  scaling  to  large  codebases.  We  present  T  ricorder  ,  a  program  analysis  platform  aimed  at  building  a  data-driven  ecosystem  around  program  analysis.  We  present  a  set  of  guiding  principles  for  our  program  analysis  tools  and  a  scalable  architecture  for  an  analysis  platform  implementing  these  principles.  We  include  an  empirical,  in-situ  evaluation  of  the  tool  as  it  is  used  by  developers  across  Google  that  shows  the  usefulness  and  impact  of  the  platform.
1	Cloud  user  security  based  on  rsa  and  md5  algorithm  for  resource  attestation  and  sharing  in  java  environment.  The  increased  degree  of  connectivity  and  the  increasing  amount  of  data  has  led  many  providers  and  in  particular  data  centers  to  employ  larger  infrastructures  with  dynamic  load  and  access  balancing.  This  lead  to  the  demand  of  cloud  computing.  But  there  are  some  security  concerns  when  we  handle  and  share  data  in  the  cloud  computing  environment.  In  this  paper  we  propose  a  new  cloud  computing  environment  where  we  approach  a  trusted  cloud  environment  which  is  controlled  by  both  the  client  and  the  cloud  environment  admin.  Our  approach  is  mainly  divided  into  two  parts.  First  part  is  controlled  by  the  normal  user  which  gets  permission  by  the  cloud  environment  for  performing  operation  and  for  loading  data.  Second  part  shows  a  secure  trusted  computing  for  the  cloud,  if  the  admin  of  the  cloud  want  to  read  and  update  the  data  then  it  take  permission  from  the  client  environment.  This  provides  a  way  to  hide  the  data  and  normal  user  and  can  protect  their  data  from  the  cloud  provider.  This  provides  a  two  way  security  protocol  which  helps  both  the  cloud  and  the  normal  user.  For  the  above  concept  we  apply  RSA  and  MD  5  algorithm.  When  the  cloud  user  upload  the  data  in  the  cloud  environment,  the  data  is  uploaded  in  encrypted  form  using  RSA  algorithm  and  the  cloud  admin  can  decrypt  using  their  own  private  key.  For  updating  the  data  in  the  cloud  environment  admin  request  the  user  for  a  secure  key.  Cloud  user  sends  a  secure  key  with  a  message  digest  tag  for  updating  data.  If  any  outsiders  perform  a  change  in  the  key,  the  tag  bit  is  also  changed  indicating  the  key  is  not  secure  and  correct.
1	Departures  from  optimality  understanding  human  analyst  s  information  foraging  in  assisted  requirements  tracing.  Studying  human  analyst's  behavior  in  automated  tracing  is  a  new  research  thrust.  Building  on  a  growing  body  of  work  in  this  area,  we  offer  a  novel  approach  to  understanding  requirements  analyst's  information  seeking  and  gathering.  We  model  analysts  as  predators  in  pursuit  of  prey  ---  the  relevant  traceability  information,  and  leverage  the  optimality  models  to  characterize  a  rational  decision  process.  The  behavior  of  real  analysts  with  that  of  the  optimal  information  forager  is  then  compared  and  contrasted.  The  results  show  that  the  analysts'  information  diets  are  much  wider  than  the  theory's  predictions,  and  their  residing  in  low-profitability  information  patches  is  much  longer  than  the  optimal  residence  time.  These  uncovered  discrepancies  not  only  offer  concrete  insights  into  the  obstacles  faced  by  analysts,  but  also  lead  to  principled  ways  to  increase  practical  tool  support  for  overcoming  the  obstacles.
1	Reran  timing  and  touch  sensitive  record  and  replay  for  android.  Touchscreen-based  devices  such  as  smartphones  and  tablets  are  gaining  popularity,  but  their  rich  input  capabilities  pose  new  development  and  testing  complications.  To  alleviate  this  problem,  we  present  an  approach  and  tool  named  Reran  that  permits  record-and-replay  for  the  Android  smartphone  platform.  Existing  GUI-level  record-and-replay  approaches  are  inadequate  due  to  the  expressiveness  of  the  smartphone  domain,  in  which  applications  support  sophisticated  GUI  gestures,  depend  on  inputs  from  a  variety  of  sensors  on  the  device,  and  have  precise  timing  requirements  among  the  various  input  events.  We  address  these  challenges  by  directly  capturing  the  low-level  event  stream  on  the  phone,  which  includes  both  GUI  events  and  sensor  events,  and  replaying  it  with  microsecond  accuracy.  Moreover,  Reran  does  not  require  access  to  app  source  code,  perform  any  app  rewriting,  or  perform  any  modifications  to  the  virtual  machine  or  Android  platform.  We  demonstrate  RERAN's  applicability  in  a  variety  of  scenarios,  including  (a)  replaying  86  out  of  the  Top-100  Android  apps  on  Google  Play;  (b)  reproducing  bugs  in  popular  apps,  e.g.,  Firefox,  Facebook,  Quickoffice;  and  (c)  fast-forwarding  executions.  We  believe  that  our  versatile  approach  can  help  both  Android  developers  and  researchers.
1	Run  time  verification  of  coboxes.  Run-time  verification  is  one  of  the  most  useful  techniques  for  detecting  faults.  In  this  paper  we  show  how  to  model  the  observable  behavior  of  concurrently  running  object  groups  coboxes  in  SAGA  Software  trace  Analysis  using  Grammars  and  Attributes  which  is  a  run-time  checker  that  provides  a  smooth  integration  of  the  specification  and  the  efficient  run-time  checking  of  both  data-  and  protocol-oriented  properties  of  message  sequences.  We  illustrate  the  effectiveness  of  our  method  by  an  industrial  case  study  from  the  eCommerce  software  company  Fredhopper.
1	A  framework  for  automated  testing  of  javascript  web  applications.  Current  practice  in  testing  JavaScript  web  applications  requires  manual  construction  of  test  cases,  which  is  difficult  and  tedious.  We  present  a  framework  for  feedback-directed  automated  test  generation  for  JavaScript  in  which  execution  is  monitored  to  collect  information  that  directs  the  test  generator  towards  inputs  that  yield  increased  coverage.  We  implemented  several  instantiations  of  the  framework,  corresponding  to  variations  on  feedback-directed  random  testing,  in  a  tool  called  Artemis.  Experiments  on  a  suite  of  JavaScript  applications  demonstrate  that  a  simple  instantiation  of  the  framework  that  uses  event  handler  registrations  as  feedback  information  produces  surprisingly  good  coverage  if  enough  tests  are  generated.  By  also  using  coverage  information  and  read-write  sets  as  feedback  information,  a  slightly  better  level  of  coverage  can  be  achieved,  and  sometimes  with  many  fewer  tests.  The  generated  tests  can  be  used  for  detecting  HTML  validity  problems  and  other  programming  errors.
1	Sound  empirical  evidence  in  software  testing.  Several  promising  techniques  have  been  proposed  to  automate  different  tasks  in  software  testing,  such  as  test  data  generation  for  object-oriented  software.  However,  reported  studies  in  the  literature  only  show  the  feasibility  of  the  proposed  techniques,  because  the  choice  of  the  employed  artifacts  in  the  case  studies  (e.g.,  software  applications)  is  usually  done  in  a  non-systematic  way.  The  chosen  case  study  might  be  biased,  and  so  it  might  not  be  a  valid  representative  of  the  addressed  type  of  software  (e.g.,  internet  applications  and  embedded  systems).  The  common  trend  seems  to  be  to  accept  this  fact  and  get  over  it  by  simply  discussing  it  in  a  threats  to  validity  section.  In  this  paper,  we  evaluate  search-based  software  testing  (in  particular  the  EvoSuite  tool)  when  applied  to  test  data  generation  for  open  source  projects.  To  achieve  sound  empirical  results,  we  randomly  selected  100  Java  projects  from  SourceForge,  which  is  the  most  popular  open  source  repository  (more  than  300,000  projects  with  more  than  two  million  registered  users).  The  resulting  case  study  not  only  is  very  large  (8,784  public  classes  for  a  total  of  291,639  bytecode  level  branches),  but  more  importantly  it  is  statistically  sound  and  representative  for  open  source  projects.  Results  show  that  while  high  coverage  on  commonly  used  types  of  classes  is  achievable,  in  practice  environmental  dependencies  prohibit  such  high  coverage,  which  clearly  points  out  essential  future  research  directions.  To  support  this  future  research,  our  SF100  case  study  can  serve  as  a  much  needed  corpus  of  classes  for  test  generation.
1	Detecting  android  malware  with  intensive  feature  engineering.  Nowadays,  the  amount  of  the  application  in  Android  App  Market  has  grown  fast,  and  the  android  malwares  have  been  introduced  fast  into  that  market,  too.  In  this  paper,  we  use  static  analysis  of  a  given  android  application  with  intensive  feature  engineering  which  we  focus  on  different  sources  and  different  levels.  It  means  that  we  not  only  extract  features  from  the  executable  file  classes.dex  but  also  from  the  other  android  resource  files  such  as  manifest  of  the  application,  more  over  we  expand  features  at  different  levels  of  abstraction  of  the  APK  application,  rather  than  using  more  features  at  the  single  level.  Finally,  we  combine  these  different  feature  sets  into  one  feature  set  which  is  used  by  the  classifiers  for  training/testing.  Our  method  is  compared  against  other  Android  malware  code  detection  and  found  to  be  more  efficient  in  terms  of  detection  accuracy  and  false  alarm  rate.
1	Product  line  architecture  recovery  an  approach  proposal  extended  abstract.  The  Product  Line  Architecture  (PLA)  is  an  important  asset  for  the  success  of  Software  Product  Line  (SPL)  projects.  Due  to  the  complexity  of  managing  the  architectural  variability,  maintain  the  PLA  up-to-date  and  synchronized  with  the  project  source  code  is  a  hard  problem.  The  systematic  use  of  Software  Architecture  Recovery  (SAR)  techniques  enables  the  PLA  recovery  and  keeps  the  PLA  aligned  with  the  development.  In  this  context,  we  present  our  initial  proposal  that  consists  of  an  approach  to  recover  PLAs  based  on  the  use  of  (bottom-up)  SAR  techniques.  We  performed  some  studies  (such  as  surveys,  literature  reviews,  and  exploratory  studies)  to  investigate  the  relationship  between  SAR  and  PLA  to  identify  gaps  and  define  the  research  area  state-of-the-art.  The  combination  of  SAR  and  PLA  is  an  important  strategy  to  address  some  issues  of  PLA  design.  We  identified  that  few  studies  address  architectural  variability,  PLA  variability  traceability,  and  empirical  evaluation  such  as  experiments,  surveys,  mixed-methods,  and  so  on.
1	Dependability  of  service  oriented  computing  time  probabilistic  failure  modelling.  In  the  paper  we  discuss  a  failure  and  servicing  model  of  software  applications  that  employ  the  service-oriented  paradigm  for  defining  cooperation  with  clients.  The  model  takes  into  account  a  time-probabilistic  relationship  between  different  servicing  outcomes  and  failures  modes.  We  put  forward  a  set  of  measures  for  estimating  dependability  of  service  provisioning  from  the  client's  viewpoint  and  present  analytical  models  to  be  used  for  the  assessment  of  the  mean  servicing  and  waiting  times  depending  on  client's  timeout  settings.
1	Does  personality  influence  the  usage  of  static  analysis  tools  an  explorative  experiment.  There  are  many  techniques  to  improve  software  quality.  One  is  using  automatic  static  analysis  tools.  We  have  observed,  however,  that  despite  the  low-cost  help  they  offer,  these  tools  are  underused  and  often  discourage  beginners.  There  is  evidence  that  personality  traits  influence  the  perceived  usability  of  a  software.  Thus,  to  support  beginners  better,  we  need  to  understand  how  the  workflow  of  people  with  different  prevalent  personality  traits  using  these  tools  varies.  For  this  purpose,  we  observed  users'  solution  strategies  and  correlated  them  with  their  prevalent  personality  traits  in  an  exploratory  study  with  student  participants  within  a  controlled  experiment.  We  gathered  data  by  screen  capturing  and  chat  protocols  as  well  as  a  Big  Five  personality  traits  test.  We  found  strong  correlations  between  particular  personality  traits  and  different  strategies  of  removing  the  findings  of  static  code  analysis  as  well  as  between  personality  and  tool  utilization.  Based  on  that,  we  offer  take-away  improvement  suggestions.  Our  results  imply  that  developers  should  be  aware  of  these  solution  strategies  and  use  this  information  to  build  tools  that  are  more  appealing  to  people  with  different  prevalent  personality  traits.
1	The  art  and  science  of  analyzing  software  data  quantitative  methods.  Using  the  tools  of  quantitative  data  science,  software  engineers  that  can  predict  useful  information  on  new  projects  based  on  past  projects.  This  tutorial  reflects  on  the  state-of-the-art  in  quantitative  reasoning  in  this  important  field.  This  tutorial  discusses  the  following:  (a)  when  local  data  is  scarce,  we  show  how  to  adapt  data  from  other  organizations  to  local  problems;  (b)  when  working  with  data  of  dubious  quality,  we  show  how  to  prune  spurious  information;  (c)  when  data  or  models  seem  too  complex,  we  show  how  to  simplify  data  mining  results;  (d)  when  the  world  changes,  and  old  models  need  to  be  updated,  we  show  how  to  handle  those  updates;  (e)  when  the  effect  is  too  complex  for  one  model,  we  show  to  how  reason  over  ensembles.
1	Second  international  workshop  on  managing  technical  debt  mtd  2011.  The  technical  debt  metaphor  is  gaining  significant  traction  in  the  software  development  community  as  a  way  to  understand  and  communicate  issues  of  intrinsic  quality,  value,  and  cost.  The  idea  is  that  developers  sometimes  accept  compromises  in  a  system  in  one  dimension  (e.g.,  modularity)  to  meet  an  urgent  demand  in  some  other  dimension  (e.g.,  a  deadline),  and  that  such  compromises  incur  a  "debt":  on  which  "interest"  has  to  be  paid  and  which  should  be  repaid  at  some  point  for  the  long-term  health  of  the  project.  Little  is  known  about  technical  debt,  beyond  feelings  and  opinions.  The  software  engineering  research  community  has  an  opportunity  to  study  this  phenomenon  and  improve  the  way  it  is  handled.  We  can  offer  software  engineers  a  foundation  for  managing  such  trade-offs  based  on  models  of  their  economic  impacts.  The  goal  of  this  second  workshop  is  to  discuss  managing  technical  debt  as  a  part  of  the  research  agenda  for  the  software  engineering  field.
1	A  brief  look  at  forward  error  correcting  codes.  Forward  Error  Correcting  codes  are  the  codes  applied  in  error  correction  in  the  communications,  by  adding  redundancies.  Without  a  feedback  channel,  it  is  an  attractive  alternate  of  ARQ  in  long  delay  path  and  one-way  transmission.  FECs  could  also  avoid  multicast  problems.  However,  in  practice,  FECs  could  be  relative  inefficient  due  to  the  expensive  cost  of  computation  and  requirement  of  over-transmission  sometimes.  Hence,  in  the  random  error  or  the  burst  error  channel,  particular  FEC  scheme  should  be  used  correspondently.
1	Automatic  test  oracle  for  image  processing  applications  using  support  vector  machines.  Software  testing  has  been  a  challenging  job  over  the  decades  and  possess  more  challenges  for  complex  inputs  such  as  images.  While  evaluating  correctness  of  the  output  images,  there  may  exist  a  large  number  of  correct  or  incorrect  images  with  insignificant  differences.  A  test  oracle  is  required  to  evaluate  the  correctness  of  output  images  which  may  not  be  available  in  most  of  the  cases.  Currently,  output  images  are  evaluated  by  domain  experts  such  as  medical  experts,  which  involves  manual  inspection  of  output  images  at  each  step  of  software  development.  In  this  paper,  we  have  proposed  a  mechanism  to  automate  the  test  oracle  using  support  vector  machine.  It  requires  a  few  correct  and  incorrect  images  for  the  training  and  is  capable  of  classification  of  correct  and  incorrect  output  images.  For  the  demonstration  purpose,  we  used  different  implementations  of  image  dilation  and  compared  the  results  with  statistical  oracle  and  metamorphic  testing.  The  results  in  our  initial  experiments  are  encouraging.
1	Exploring  api  embedding  for  api  usages  and  applications.  Word2Vec  is  a  class  of  neural  network  models  that  as  being  trained  from  a  large  corpus  of  texts,  they  can  produce  for  each  unique  word  a  corresponding  vector  in  a  continuous  space  in  which  linguistic  contexts  of  words  can  be  observed.  In  this  work,  we  study  the  characteristics  of  Word2Vec  vectors,  called    api  2  vec    or  API  embeddings,  for  the  API  elements  within  the  API  sequences  in  source  code.  Our  empirical  study  shows  that  the  close  proximity  of  the    api  2  vec    vectors  for  API  elements  reflects  the  similar  usage  contexts  containing  the  surrounding  APIs  of  those  API  elements.  Moreover,    api  2  vec    can  capture  several  similar  semantic  relations  between  API  elements  in  API  usages  via  vector  offsets.  We  demonstrate  the  usefulness  of    api  2  vec    vectors  for  API  elements  in  three  applications.  First,  we  build  a  tool  that  mines  the  pairs  of  API  elements  that  share  the  same  usage  relations  among  them.  The  other  applications  are  in  the  code  migration  domain.  We  develop    api  2  api  ,  a  tool  to  automatically  learn  the  API  mappings  between  Java  and  C#  using  a  characteristic  of  the    api  2  vec    vectors  for  API  elements  in  the  two  languages:  semantic  relations  among  API  elements  in  their  usages  are  observed  in  the  two  vector  spaces  for  the  two  languages  as  similar  geometric  arrangements  among  their    api  2  vec    vectors.  Our  empirical  evaluation  shows  that    api  2  api    relatively  improves  22.6%  and  40.1%  top-1  and  top-5  accuracy  over  a  state-of-the-art  mining  approach  for  API  mappings.  Finally,  as  another  application  in  code  migration,  we  are  able  to  migrate  equivalent  API  usages  from  Java  to  C#  with  up  to  90.6%  recall  and  87.2%  precision.
1	A  framework  for  adoption  of  machine  learning  in  industry  for  software  defect  prediction.  Machine  learning  algorithms  are  increasingly  being  used  in  a  variety  of  application  domains  including  software  engineering.  While  their  practical  value  have  been  outlined,  demonstrated  and  highlighted  in  number  of  existing  studies,  their  adoption  in  industry  is  still  not  widespread.  The  evaluations  of  machine  learning  algorithms  in  literature  seem  to  focus  on  few  attributes  and  mainly  on  predictive  accuracy.  On  the  other  hand  the  decision  space  for  adoption  or  acceptance  of  machine  learning  algorithms  in  industry  encompasses  much  more  factors.  Companies  looking  to  adopt  such  techniques  want  to  know  where  such  algorithms  are  most  useful,  if  the  new  methods  are  reliable  and  cost  effective.  Further  questions  such  as  how  much  would  it  cost  to  setup,  run  and  maintain  systems  based  on  such  techniques  are  currently  not  fully  investigated  in  the  industry  or  in  academia  leading  to  difficulties  in  assessing  the  business  case  for  adoption  of  these  techniques  in  industry.  In  this  paper  we  argue  for  the  need  of  framework  for  adoption  of  machine  learning  in  industry.  We  develop  a  framework  for  factors  and  attributes  that  contribute  towards  the  decision  of  adoption  of  machine  learning  techniques  in  industry  for  the  purpose  of  software  defect  predictions.  The  framework  is  developed  in  close  collaboration  within  industry  and  thus  provides  useful  insight  for  industry  itself,  academia  and  suppliers  of  tools  and  services.
1	Engineering  big  data  solutions.  Structured  and  unstructured  data  in  operational  support  tools  have  long  been  prevalent  in  software  engineering.  Similar  data  is  now  becoming  widely  available  in  other  domains.  Software  systems  that  utilize  such  operational  data  (OD)  to  help  with  software  design  and  maintenance  activities  are  increasingly  being  built  despite  the  difficulties  of  drawing  valid  conclusions  from  disparate  and  low-quality  data  and  the  continuing  evolution  of  operational  support  tools.  This  paper  proposes  systematizing  approaches  to  the  engineering  of  OD-based  systems.  To  prioritize  and  structure  research  areas  we  consider  historic  developments,  such  as  big  data  hype;  synthesize  defining  features  of  OD,  such  as  confounded  measures  and  unobserved  context;  and  discuss  emerging  new  applications,  such  as  diverse  and  large  OD  collections  and  extremely  short  development  intervals.  To  sustain  the  credibility  of  OD-based  systems  more  research  will  be  needed  to  investigate  effective  existing  approaches  and  to  synthesize  novel,  OD-specific  engineering  principles.
1	Distilling  neural  representations  of  data  structure  manipulation  using  fmri  and  fnirs.  Data  structures  permeate  many  aspects  of  software  engineering,  but  their  associated  human  cognitive  processes  are  not  thoroughly  understood.  We  leverage  medical  imaging  and  insights  from  the  psychological  notion  of  spatial  ability  to  decode  the  neural  representations  of  several  fundamental  data  structures  and  their  manipulations.  In  a  human  study  involving  76  participants,  we  examine  list,  array,  tree,  and  mental  rotation  tasks  using  both  functional  near-infrared  spectroscopy  (fNIRS)  and  functional  magnetic  resonance  imaging  (fMRI).  We  find  a  nuanced  relationship:  data  structure  and  spatial  operations  use  the  same  focal  regions  of  the  brain  but  to  different  degrees.  They  are  related  but  distinct  neural  tasks.  In  addition,  more  difficult  computer  science  problems  induce  higher  cognitive  load  than  do  problems  of  pure  spatial  reasoning.  Finally,  while  fNIRS  is  less  expensive  and  more  permissive,  there  are  some  computing-relevant  brain  regions  that  only  fMRI  can  reach.
1	Mining  software  defects  should  we  consider  affected  releases.  With  the  rise  of  the  Mining  Software  Repositories  (MSR)  field,  defect  datasets  extracted  from  software  repositories  play  a  foundational  role  in  many  empirical  studies  related  to  software  quality.  At  the  core  of  defect  data  preparation  is  the  identification  of  post-release  defects.  Prior  studies  leverage  many  heuristics  (e.g.,  keywords  and  issue  IDs)  to  identify  post-release  defects.  However,  such  the  heuristic  approach  is  based  on  several  assumptions,  which  pose  common  threats  to  the  validity  of  many  studies.  In  this  paper,  we  set  out  to  investigate  the  nature  of  the  difference  of  defect  datasets  generated  by  the  heuristic  approach  and  the  realistic  approach  that  leverages  the  earliest  affected  release  that  is  realistically  estimated  by  a  software  development  team  for  a  given  defect.  In  addition,  we  investigate  the  impact  of  defect  identification  approaches  on  the  predictive  accuracy  and  the  ranking  of  defective  modules  that  are  produced  by  defect  models.  Through  a  case  study  of  defect  datasets  of  32  releases,  we  find  that  that  the  heuristic  approach  has  a  large  impact  on  both  defect  count  datasets  and  binary  defect  datasets.  Surprisingly,  we  find  that  the  heuristic  approach  has  a  minimal  impact  on  defect  count  models,  suggesting  that  future  work  should  not  be  too  concerned  about  defect  count  models  that  are  constructed  using  heuristic  defect  datasets.  On  the  other  hand,  using  defect  datasets  generated  by  the  realistic  approach  lead  to  an  improvement  in  the  predictive  accuracy  of  defect  classification  models.
1	Recovering  variable  names  for  minified  code  with  usage  contexts.  To  avoid  the  exposure  of  original  source  code  in  a  Web  application,  the  variable  names  in  JS  code  deployed  in  the  wild  are  often  replaced  by  short,  meaningless  names,  thus  making  the  code  extremely  difficult  to  manually  understand  and  analysis.  This  paper  presents  JSNeat,  an  information  retrieval  (IR)-based  approach  to  recover  the  variable  names  in  minified  JS  code.  JSNeat  follows  a  data-driven  approach  to  recover  names  by  searching  for  them  in  a  large  corpus  of  open-source  JS  code.  We  use  three  types  of  contexts  to  match  a  variable  in  given  minified  code  against  the  corpus  including  the  context  of  the  properties  and  roles  of  the  variable,  the  context  of  that  variable  and  relations  with  other  variables  under  recovery,  and  the  context  of  the  task  of  the  function  to  which  the  variable  contributes.  We  performed  several  empirical  experiments  to  evaluate  JSNeat  on  the  dataset  of  more  than  322K  JS  files  with  1M  functions,  and  3.5M  variables  with  176K  unique  variable  names.  We  found  that  JSNeat  achieves  a  high  accuracy  of  69.1%,  which  is  the  relative  improvements  of  66.1%  and  43%  over  two  state-of-the-art  approaches  JSNice  and  JSNaughty,  respectively.  The  time  to  recover  for  a  file  or  a  variable  with  JSNeat  is  twice  as  fast  as  with  JSNice  and  4x  as  fast  as  with  JNaughty,  respectively.
1	Easing  iot  development  for  novice  programmers  through  code  recipes.  The  co-existence  of  various  kinds  of  devices,  protocols,  architectures,  and  programming  languages  make  Internet  of  Things  (IoT)  systems  complex  to  develop,  even  for  experienced  programmers.  Perforce,  Software  Engineering  challenges  are  even  more  difficult  to  address  by  novice  programmers.  Previous  research  focused  on  identifying  the  most  challenging  issues  that  novice  programmers  experience  when  developing  IoT  systems.  The  results  suggested  that  the  integration  of  heterogeneous  software  components  is  one  of  the  most  painful  issues,  mainly  due  to  the  lack  of  documentation  understandable  by  inexperienced  developers,  from  both  conceptual  and  technical  perspectives.  In  fact,  novice  programmers  devote  a  significant  effort  looking  for  documentation  and  code  samples  willing  to  understand  them  conceptually,  or  in  the  worst  case,  at  least  to  make  them  work.  Driven  by  the  research  question:  "How  do  the  lessons  learned  by  IoT  novice  programmers  can  be  captured,  so  they  become  an  asset  for  other  novice  developers?",  in  this  paper,  we  introduce  Code  Recipes.  They  consist  of  summarized  and  well-defined  documentation  modules,  independent  from  programming  languages  or  run-time  environments,  by  which  non-expert  programmers  can  smoothly  become  familiar  with  source  code,  written  by  other  developers  that  faced  similar  issues.  Through  a  use  case,  we  show  how  Code  Recipes  are  a  feasible  mechanism  to  support  novice  IoT  programmers  in  building  their  IoT  systems.
1	Natural  language  requirements  processing  from  research  to  practice.  Automated  manipulation  of  natural  language  requirements,  for  classification,  tracing,  defect  detection,  information  extraction,  and  other  tasks,  has  been  pursued  by  requirements  engineering  (RE)  researchers  for  more  than  two  decades.  Recent  technological  advancements  in  natural  language  processing  (NLP)  have  made  it  possible  to  apply  this  research  more  widely  within  industrial  settings.  This  technical  briefing  targets  researchers  and  practitioners,  and  aims  to  give  an  overview  of  what  NLP  can  do  today  for  RE  problems,  and  what  could  do  if  specific  research  challenges,  also  emerging  from  practical  experiences,  are  addressed.  The  talk  will:  survey  current  research  on  applications  of  NLP  to  RE  problems;  present  representative  industrially-ready  techniques,  with  a  focus  on  defect  detection  and  information  extraction  problems;  present  enabling  technologies  in  NLP  that  can  play  a  role  in  RE  research,  including  distributional  semantics  representations;  discuss  criteria  for  evaluation  of  NLP  techniques  in  the  RE  context;  outline  the  main  challenges  for  a  systematic  application  of  the  techniques  in  industry.  The  crosscutting  topics  that  will  permeate  the  talk  are  the  need  for  domain  adaptation,  and  the  essential  role  of  the  human-in-the-loop.
1	Enlightened  debugging.  Numerous  automated  techniques  have  been  proposed  to  reduce  the  cost  of  software  debugging,  a  notoriously  time-consuming  and  human-intensive  activity.  Among  these  techniques,  Statistical  Fault  Localization  (SFL)  is  particularly  popular.  One  issue  with  SFL  is  that  it  is  based  on  strong,  often  unrealistic  assumptions  on  how  developers  behave  when  debugging.  To  address  this  problem,  we  propose  Enlighten,  an  interactive,  feedback-driven  fault  localization  technique.  Given  a  failing  test,  Enlighten  (1)  leverages  SFL  and  dynamic  dependence  analysis  to  identify  suspicious  method  invocations  and  corresponding  data  values,  (2)  presents  the  developer  with  a  query  about  the  most  suspicious  invocation  expressed  in  terms  of  inputs  and  outputs,  (3)  encodes  the  developer  feedback  on  the  correctness  of  individual  data  values  as  extra  program  specifications,  and  (4)  repeats  these  steps  until  the  fault  is  found.  We  evaluated  Enlighten  in  two  ways.  First,  we  applied  Enlighten  to  1,807  real  and  seeded  faults  in  3  open  source  programs  using  an  automated  oracle  as  a  simulated  user;  for  over  96%  of  these  faults,  Enlighten  required  less  than  10  interactions  with  the  simulated  user  to  localize  the  fault,  and  a  sensitivity  analysis  showed  that  the  results  were  robust  to  erroneous  responses.  Second,  we  performed  an  actual  user  study  on  4  faults  with  24  participants  and  found  that  participants  who  used  Enlighten  performed  significantly  better  than  those  not  using  our  tool,  in  terms  of  both  number  of  faults  localized  and  time  needed  to  localize  the  faults.
1	How  gamification  affects  software  developers  cautionary  evidence  from  a  natural  experiment  on  github.  We  examine  how  the  behavior  of  software  developers  changes  in  response  to  removing  gamification  elements  from  GitHub,  an  online  platform  for  collaborative  programming  and  software  development.  We  find  that  the  unannounced  removal  of  daily  activity  streak  counters  from  the  user  interface  (from  user  profile  pages)  was  followed  by  significant  changes  in  behavior.  Long-running  streaks  of  activity  were  abandoned  and  became  less  common.  Weekend  activity  decreased  and  days  in  which  developers  made  a  single  contribution  became  less  common.  Synchronization  of  streaking  behavior  in  the  platform's  social  network  also  decreased,  suggesting  that  gamification  is  a  powerful  channel  for  social  influence.  Focusing  on  a  set  of  software  developers  that  were  publicly  pursuing  a  goal  to  make  contributions  for  100  days  in  a  row,  we  find  that  some  of  these  developers  abandon  this  quest  following  the  removal  of  the  public  streak  counter.  Our  findings  provide  evidence  for  the  significant  impact  of  gamification  on  the  behavior  of  developers  on  large  collaborative  programming  and  software  development  platforms.  They  urge  caution:  gamification  can  steer  the  behavior  of  software  developers  in  unexpected  and  unwanted  directions.
1	Deeplv  suggesting  log  levels  using  ordinal  based  neural  networks.  Developers  write  logging  statements  to  generate  logs  that  provide  valuable  runtime  information  for  debugging  and  maintenance  of  software  systems.  Log  level  is  an  important  component  of  a  logging  statement,  which  enables  developers  to  control  the  information  to  be  generated  at  system  runtime.  However,  due  to  the  complexity  of  software  systems  and  their  runtime  behaviors,  deciding  a  proper  log  level  for  a  logging  statement  is  a  challenging  task.  For  example,  choosing  a  higher  level  (e.g.,  error)  for  a  trivial  event  may  confuse  end  users  and  increase  system  maintenance  overhead,  while  choosing  a  lower  level  (e.g.,  trace)  for  a  critical  event  may  prevent  the  important  execution  information  to  be  conveyed  opportunely.  In  this  paper,  we  tackle  the  challenge  by  first  conducting  a  preliminary  manual  study  on  the  characteristics  of  log  levels.  We  find  that  the  syntactic  context  of  the  logging  statement  and  the  message  to  be  logged  might  be  related  to  the  decision  of  log  levels,  and  log  levels  that  are  further  apart  in  order  (e.g.,  trace  and  error)  tend  to  have  more  differences  in  their  characteristics.  Based  on  this,  we  then  propose  a  deep-learning  based  approach  that  can  leverage  the  ordinal  nature  of  log  levels  to  make  suggestions  on  choosing  log  levels,  by  using  the  syntactic  context  and  message  features  of  the  logging  statements  extracted  from  the  source  code.  Through  an  evaluation  on  nine  large-scale  open  source  projects,  we  find  that:  1)  our  approach  outperforms  the  state-of-the-art  baseline  approaches;  2)  we  can  further  improve  the  performance  of  our  approach  by  enlarging  the  training  data  obtained  from  other  systems;  3)  our  approach  also  achieves  promising  results  on  cross-system  suggestions  that  are  even  better  than  the  baseline  approaches  on  within-system  suggestions.  Our  study  highlights  the  potentials  in  suggesting  log  levels  to  help  developers  make  informed  logging  decisions.
1	An  interdisciplinary  model  for  graphical  representation.  The  paper  questions  whether  data-driven  and  problem-driven  models  are  sufficient  for  a  software  to  automatically  represent  a  meaningful  graphical  representation  of  scientific  findings.  The  paper  presents  descriptive  and  prescriptive  case  studies  to  understand  the  benefits  and  the  shortcomings  of  existing  models  that  aim  to  provide  graphical  representations  of  data-sets.  First,  the  paper  considers  data-sets  coming  from  the  field  of  software  metrics  and  shows  that  existing  models  can  provide  the  expected  outcomes  for  descriptive  scientific  studies.  Second,  the  paper  presents  data-sets  coming  from  the  field  of  human  mobility  and  sustainable  development,  and  shows  that  a  more  comprehensive  model  is  needed  in  the  case  of  prescriptive  scientific  fields  requiring  interdisciplinary  research.  Finally,  an  interdisciplinary  problem-driven  model  is  proposed  to  guide  the  software  users,  and  specifically  scientists,  to  produce  meaningful  graphical  representation  of  research  findings.  The  proposal  is  indeed  based  not  only  on  a  data-driven  and/or  problem-driven  model  but  also  on  the  different  knowledge  domains  and  scientific  aims  of  the  experts,  who  can  provide  the  information  needed  for  a  higher-order  structure  of  the  data,  supporting  the  graphical  representation  output.
1	Offside  learning  to  identify  mistakes  in  boundary  conditions.  Mistakes  in  boundary  conditions  are  the  cause  of  many  bugs  in  software.  These  mistakes  happen  when,  e.g.,  developers  make  use  of  ''  in  cases  where  they  should  have  used  '='.  Mistakes  in  boundary  conditions  are  often  hard  to  find  and  manually  detecting  them  might  be  very  time-consuming  for  developers.  While  researchers  have  been  proposing  techniques  to  cope  with  mistakes  in  the  boundaries  for  a  long  time,  the  automated  detection  of  such  bugs  still  remains  a  challenge.  We  conjecture  that,  for  a  tool  to  be  able  to  precisely  identify  mistakes  in  boundary  conditions,  it  should  be  able  to  capture  the  overall  context  of  the  source  code  under  analysis.  In  this  work,  we  propose  a  deep  learning  model  that  learn  mistakes  in  boundary  conditions  and,  later,  is  able  to  identify  them  in  unseen  code  snippets.  We  train  and  test  a  model  on  over  1.5  million  code  snippets,  with  and  without  mistakes  in  different  boundary  conditions.  Our  model  shows  an  accuracy  from  55%  up  to  87%.  The  model  is  also  able  to  detect  24  out  of  41  real-world  bugs;  however,  with  a  high  false  positive  rate.  The  existing  state-of-the-practice  linter  tools  are  not  able  to  detect  any  of  the  bugs.  We  hope  this  paper  can  pave  the  road  towards  deep  learning  models  that  will  be  able  to  support  developers  in  detecting  mistakes  in  boundary  conditions.
1	Seenomaly  vision  based  linting  of  gui  animation  effects  against  design  don  t  guidelines.  GUI  animations,  such  as  card  movement,  menu  slide  in/out,  snackbar  display,  provide  appealing  user  experience  and  enhance  the  usability  of  mobile  applications.  These  GUI  animations  should  not  violate  the  platform's  UI  design  guidelines  (referred  to  as  design-don't  guideline  in  this  work)  regarding  component  motion  and  interaction,  content  appearing  and  disappearing,  and  elevation  and  shadow  changes.  However,  none  of  existing  static  code  analysis,  functional  GUI  testing  and  GUI  image  comparison  techniques  can  "see"  the  GUI  animations  on  the  scree,  and  thus  they  cannot  support  the  linting  of  GUI  animations  against  design-don't  guidelines.  In  this  work,  we  formulate  this  GUI  animation  linting  problem  as  a  multi-class  screencast  classification  task,  but  we  do  not  have  sufficient  labeled  GUI  animations  to  train  the  classifier.  Instead,  we  propose  an  unsupervised,  computer-vision  based  adversarial  autoencoder  to  solve  this  linting  problem.  Our  autoencoder  learns  to  group  similar  GUI  animations  by  "seeing"  lots  of  unlabeled  real-application  GUI  animations  and  learning  to  generate  them.  As  the  first  work  of  its  kind,  we  build  the  datasets  of  synthetic  and  real-world  GUI  animations.  Through  experiments  on  these  datasets,  we  systematically  investigate  the  learning  capability  of  our  model  and  its  effectiveness  and  practicality  for  linting  GUI  animations,  and  identify  the  challenges  in  this  linting  problem  for  future  work.
1	Soft  skills  in  software  development  teams  a  survey  of  the  points  of  view  of  team  leaders  and  team  members.  Besides  technical  knowledge  and  experience,  the  so-called  "soft  skills"  of  team  members  are  also  an  important  factor  in  software  engineering  projects.  The  study  of  this  subject  is  gaining  the  attention  of  researchers  and  practitioners  in  recent  years.  In  this  paper  we  report  a  field  study  in  which  we  interviewed  35  software  engineering  practitioners  from  software  companies  in  Uruguay  to  know  their  points  of  view  about  what  are  the  soft  skills  they  consider  the  most  valued  to  have  by  the  leader  and  the  other  members  of  software  development  teams.  As  a  result,  Leadership,  Communication  skills,  Customer  orientation,  Interpersonal  skills,  and  Teamwork  are  the  most  valued  for  team  leaders,  while  Analytic,  problem-solving,  Commitment,  responsibility,  Eagerness  to  learn,  Motivation,  and  Teamwork  are  the  most  valued  ones  for  team  members.
1	Scalable  formal  verification  of  uml  models.  UML  (Unified  Modeling  Language)  has  been  used  for  years  in  diverse  domains.  Its  notations  usually  come  with  a  reasonably  well-defined  syntax,  but  its  semantics  is  left  under-specified  and  open  to  different  interpretations.  This  freedom  hampers  the  formal  verification  of  produced  specifications  and  calls  for  more  rigor  and  precision.  This  work  aims  to  bridge  this  gap  and  proposes  a  flexible  and  modular  formalization  approach  based  on  temporal  logic.  We  studied  the  different  interpretations  for  some  of  its  constructs,  and  our  framework  allows  one  to  assemble  the  semantics  of  interest  by  composing  the  selected  formalizations  for  the  different  pieces.  However,  the  formalization  per-se  is  not  enough.  The  verification  process,  in  general,  becomes  slow  and  impossible  --  as  the  model  grows  in  size.  To  tackle  the  scalability  problem,  this  work  also  proposes  a  bit-vector-based  encoding  of  LTL  formulae.  The  first  results  witness  a  significant  increase  in  the  size  of  analyzable  models,  not  only  for  our  formalization  of  UML  models,  but  also  for  numerous  other  models  that  can  be  reduced  to  bounded  satisfiability  checking  of  LTL  formulae.
1	Guided  dynamic  symbolic  execution  using  subgraph  control  flow  information.  Dynamic  symbolic  execution  (DSE)  is  an  efficient  SMT-based  path  enumeration  technique  used  in  software  testing.  In  this  work  in  progress,  we  consider  here  the  case  of  guided  DSE,  where  the  paths  to  enumerate  should  be  part  of  a  given  program  slice.  We  propose  a  new  path  selection  criterion,  which  aims  to  minimize  the  number  of  queries  to  the  SMT  solvers.  This  criterion  is  based  on  the  probability  of  a  path  to  exit  the  program  slice.  Experiments  show  that  this  information  can  be  computed  in  a  reasonable  time  for  DSE  purpose.
1	Deploying  an  online  software  engineering  education  program  in  a  globally  distributed  organization.  A  well-trained  software  engineering  workforce  is  a  key  to  success  in  a  highly  competitive  environment.  Changing  tools  and  technologies,  along  with  a  rapidly  changing  development  environment,  make  it  incumbent  on  organizations  to  invest  in  training.  In  this  paper,  we  describe  our  experience  in  deploying  an  online  training  program  in  a  globally  distributed  organization.  We  write  about  the  reasons  behind  ABB’s  Software  Development  Improvement  Program  (SDIP),  the  requirements  we  established  upfront,  the  people,  processes  and  technologies  we  used,  the  promotion  of  SDIP,  and  metrics  for  measuring  success.  Finally,  we  share  and  describe  results  and  lessons  learned  that  could  be  applied  to  many  organizations  with  similar  issues.  The  goal  of  this  paper  is  to  provide  a  set  of  replicable  best  practices  for  initiating  a  software  training  program  in  a  multi-national  organization.  The  first  SDIP  online  course  was  offered  in  June  2012.  Since  then,  we  have  had  more  than  10,000  enrollments  from  employees  in  54  countries.  Today,  our  training  library  contains  89  e-learning,  17  webinar,  video  and  virtual  lab  courses,  and  we  have  delivered  more  than  180  hosted  webinars.  Following  each  class,  we  ask  students  to  evaluate  the  class.  Ninety-eight  percent  are  satisfied  with  the  classes.
1	Managing  non  functional  uncertainty  via  model  driven  adaptivity.  Modern  software  systems  are  often  characterized  by  uncertainty  and  changes  in  the  environment  in  which  they  are  embedded.  Hence,  they  must  be  designed  as  adaptive  systems.  We  propose  a  framework  that  supports  adaptation  to  non-functional  manifestations  of  uncertainty.  Our  framework  allows  engineers  to  derive,  from  an  initial  model  of  the  system,  a  finite  state  automaton  augmented  with  probabilities.  The  system  is  then  executed  by  an  interpreter  that  navigates  the  automaton  and  invokes  the  component  implementations  associated  to  the  states  it  traverses.  The  interpreter  adapts  the  execution  by  choosing  among  alternative  possible  paths  of  the  automaton  in  order  to  maximize  the  system's  ability  to  meet  its  non-functional  requirements.  To  demonstrate  the  adaptation  capabilities  of  the  proposed  approach  we  implemented  an  adaptive  application  inspired  by  an  existing  worldwide  distributed  mobile  application  and  we  discussed  several  adaptation  scenarios.
1	Network  mobility  solution  based  on  predictive  fast  handover  in  pmipv6  domain.  In  this  paper,  a  novel  NEtwork  MObility(NEMO)  solution  based  on  predictive  fast  handover  is  proposed  to  solve  the  problem  of  long  handover  delay  and  lack  of  effective  inter-domain  handover  scheme  in  Proxy  Mobile  IPv6(PMIPv6)  network.  With  the  help  of  L2  scheme  ,NEMO  can  perform  pre-handover  before  getting  registered  so  that  relevant  information  is  acknowledged  in  advance.  Inter-domain  handover  is  supported  by  extending  signaling  messages  to  make  entities  communicate  between  different  domains.  The  packet  loss  problem  is  eliminated  by  introducing  double  buffering  scheme.  Analysis  and  simulation  shows  that  this  solution  can  reduce  the  delay  during  handover  and  improve  communication  efficiency.
1	Towards  an  agent  based  methodology  for  developing  agro  ecosystem  simulations.  Agro-ecosystems  are  ecological  systems  subject  to  human  interaction  whose  simulation  is  of  interest  to  several  disciplines  (e.g.  agronomy,  ecology  and  sociology).  The  agent-based  modeling  approach  appears  as  a  suitable  tool  for  modeling  this  kind  of  complex  system,  along  with  a  corresponding  agentoriented  software  engineering  (AOSE)  methodology  for  the  construction  of  the  simulation.  Nevertheless,  existing  AOSE  methodologies  are  general-purpose,  have  not  yet  accomplished  widespread  use,  and  clear  examples  of  applications  to  agro-ecosystems  are  hard  to  find.  This  article  sets  the  ground  for  an  AOSE  methodology  devised  specifically  for  developing  agro-ecosystem  simulations.  The  methodology  framework  is  based  upon  other  general-purpose  AOSE  methodologies,  and  it  relies  on  the  Unified  Modeling  Language  for  an  easy  uptake  from  interdisciplinary  teams.  As  a  first  proof  of  concept,  it  is  applied  to  a  real  case  study:  the  evolution  of  the  strategies  followed  by  cattle  producers  of  the  basalt-region  of  north  Uruguay  against  severe  draughts.
1	A  fast  sao  algorithm  based  on  coding  unit  partition  for  hevc.  Sample  adaptive  offset  (SAO)  filter  algorithm  is  introduced  by  HEVC  as  a  completely  new  stage  to  improve  video  quality.  It  is  located  after  deblocking  filter,  adding  an  offset  value  to  each  sample  based  on  the  reconstructed  data  and  the  original  YUV.  In  order  to  solve  the  problem  of  high  computational  complexity  of  SAO,  a  fast  S  AO  algorithm  based  on  coding  unit  partition  is  proposed  in  this  paper.  Evaluating  the  texture  complexity  of  coding  unit  by  the  depth  of  block  partitioning,  and  only  the  complex  coding  unit  performed  the  S  AO  filter.  Simulation  results  show  that,  compared  with  the  HEVC  conference  software  HM9.0,  the  proposed  algorithm  can  reduce  about  44.55%  encoding  time  up  to  83.68%,  while  it  suffers  from  negligible  on  PS  NR  performance.
1	Compiler  error  notifications  revisited  an  interaction  first  approach  for  helping  developers  more  effectively  comprehend  and  resolve  error  notifications.  Error  notifications  and  their  resolutions,  as  presented  by  modern  IDEs,  are  still  cryptic  and  confusing  to  developers.  We  propose  an  interaction-first  approach  to  help  developers  more  effectively  comprehend  and  resolve  compiler  error  notifications  through  a  conceptual  interaction  framework.  We  propose  novel  taxonomies  that  can  serve  as  controlled  vocabularies  for  compiler  notifications  and  their  resolutions.  We  use  preliminary  taxonomies  to  demonstrate,  through  a  prototype  IDE,  how  the  taxonomies  make  notifications  and  their  resolutions  more  consistent  and  unified.
1	Using  dynamic  analysis  to  generate  disjunctive  invariants.  Program  invariants  are  important  for  defect  detection,  program  verification,  and  program  repair.  However,  existing  techniques  have  limited  support  for  important  classes  of  invariants  such  as  disjunctions,  which  express  the  semantics  of  conditional  statements.  We  propose  a  method  for  generating  disjunctive  invariants  over  numerical  domains,  which  are  inexpressible  using  classical  convex  polyhedra.  Using  dynamic  analysis  and  reformulating  the  problem  in  non-standard  ``max-plus''  and  ``min-plus''  algebras,  our  method  constructs  hulls  over  program  trace  points.  Critically,  we  introduce  and  infer  a  weak  class  of  such  invariants  that  balances  expressive  power  against  the  computational  cost  of  generating  nonconvex  shapes  in  high  dimensions.          Existing  dynamic  inference  techniques  often  generate  spurious  invariants  that  fit  some  program  traces  but  do  not  generalize.  With  the  insight  that  generating  dynamic  invariants  is  easy,  we  propose  to  verify  these  invariants  statically  using  k-inductive  SMT  theorem  proving  which  allows  us  to  validate  invariants  that  are  not  classically  inductive.          Results  on  difficult  kernels  involving  nonlinear  arithmetic  and  abstract  arrays  suggest  that  this  hybrid  approach  efficiently  generates  and  proves  correct  program  invariants.
1	Proving  termination  of  programs  with  bitvector  arithmetic  by  symbolic  execution.  In  earlier  work,  we  developed  an  approach  for  automated  termination  analysis  of  C  programs  with  explicit  pointer  arithmetic,  which  is  based  on  symbolic  execution.  However,  similar  to  many  other  termination  techniques,  this  approach  assumed  the  program  variables  to  range  over  mathematical  integers  instead  of  bitvectors.  This  eases  mathematical  reasoning  but  is  unsound  in  general.  In  this  paper,  we  extend  our  approach  in  order  to  handle  fixed-width  bitvector  integers.  Thus,  we  present  the  first  technique  for  termination  analysis  of  C  programs  that  covers  both  byte-accurate  pointer  arithmetic  and  bit-precise  modeling  of  integers.  We  implemented  our  approach  in  the  automated  termination  prover  AProVE  and  evaluate  its  power  by  extensive  experiments.
1	Comparative  causality  explaining  the  differences  between  executions.  We  propose  a  novel  fine-grained  causal  inference  technique.  Given  two  executions  and  some  observed  differences  between  them,  the  technique  reasons  about  the  causes  of  such  differences.  The  technique  does  so  by  state  replacement,  i.e.  replacing  part  of  the  program  state  at  an  earlier  point  to  observe  whether  the  target  differences  can  be  induced.  It  makes  a  number  of  key  advances:  it  features  a  novel  execution  model  that  avoids  undesirable  entangling  of  the  replaced  state  and  the  original  state;  it  properly  handles  differences  of  omission  by  symmetrically  analyzing  both  executions;  it  also  leverages  a  recently  developed  slicing  technique  to  limit  the  scope  of  causality  testing  while  ensuring  that  no  relevant  state  causes  can  be  missed.  The  application  of  the  technique  on  automated  debugging  shows  that  it  substantially  improves  the  precision  and  efficiency  of  causal  inference  compared  to  state  of  the  art  techniques.
1	Apisynth  a  new  graph  based  api  recommender  system.  Current  API  recommendation  tools  yield  either  good  recall  ratio  or  good  precision,  but  not  both.  A  tool  named  APISynth  is  proposed  in  this  paper  by  utilizing  a  new  graph  based  approach.  Preliminary  evaluation  demonstrates  that  APISynth  wins  over  the  state  of  the  art  with  respect  to  both  the  two  criteria.
1	Modular  verification  of  information  flow  security  in  component  based  systems.  We  propose  a  novel  method  for  the  verification  of  information  flow  security  in  component-based  systems.  The  method  is  (a)  modular  w.r.t.  services  and  components,  i.e.,  overall  security  is  proved  to  follow  from  the  security  of  the  individual  services  provided  by  the  components,  and  (b)  modular  w.r.t.  attackers,  i.e.,  verified  security  properties  can  be  re-used  to  demonstrate  security  w.r.t.  different  kinds  of  attacks.
1	Guiding  dynamic  symbolic  execution  toward  unverified  program  executions.  Most  techniques  to  detect  program  errors,  such  as  testing,  code  reviews,  and  static  program  analysis,  do  not  fully  verify  all  possible  executions  of  a  program.  They  leave  executions  unverified  when  they  do  not  check  certain  properties,  fail  to  verify  properties,  or  check  properties  under  certain  unsound  assumptions  such  as  the  absence  of  arithmetic  overflow.      In  this  paper,  we  present  a  technique  to  complement  partial  verification  results  by  automatic  test  case  generation.  In  contrast  to  existing  work,  our  technique  supports  the  common  case  that  the  verification  results  are  based  on  unsound  assumptions.  We  annotate  programs  to  reflect  which  executions  have  been  verified,  and  under  which  assumptions.  These  annotations  are  then  used  to  guide  dynamic  symbolic  execution  toward  unverified  program  executions.  Our  main  technical  contribution  is  a  code  instrumentation  that  causes  dynamic  symbolic  execution  to  abort  tests  that  lead  to  verified  executions,  to  prune  parts  of  the  search  space,  and  to  prioritize  tests  that  cover  more  properties  that  are  not  fully  verified.  We  have  implemented  our  technique  for  the  .NET  static  analyzer  Clousot  and  the  dynamic  symbolic  execution  tool  Pex.  It  produces  smaller  test  suites  (by  up  to  19.2%),  covers  more  unverified  executions  (by  up  to  7.1%),  and  reduces  testing  time  (by  up  to  52.4%)  compared  to  combining  Clousot  and  Pex  without  our  technique.
1	Teaching  agile  model  driven  engineering  for  cyber  physical  systems.  Agile  development  methods,  model-driven  engineering,  and  cyber-physical  systems  are  important  topics  in  software  engineering  education.  It  is  not  obvious  how  to  teach  their  combination  while  respecting  individual  challenges  posed  to  students  and  educators.  We  have  devised  a  software  project  class  for  teaching  the  agile  MDE  for  CPS.  The  project  class  was  held  in  three  different  semesters.  In  this  paper,  we  report  on  the  setup  of  our  exploratory  study  and  its  goals  for  teaching.  We  base  our  evaluation  and  insights  on  interviews  and  questionnaires.  Our  results  show  the  feasibility  of  combination  of  agile  MDE  for  CPS  but  also  the  challenges  this  combination  poses  to  students  and  educators.
1	A  comparison  of  10  sampling  algorithms  for  configurable  systems.  Almost  every  software  system  provides  configuration  options  to  tailor  the  system  to  the  target  platform  and  application  scenario.  Often,  this  configurability  renders  the  analysis  of  every  individual  system  configuration  infeasible.  To  address  this  problem,  researchers  have  proposed  a  diverse  set  of  sampling  algorithms.  We  present  a  comparative  study  of  10  state-of-the-art  sampling  algorithms  regarding  their  fault-detection  capability  and  size  of  sample  sets.  The  former  is  important  to  improve  software  quality  and  the  latter  to  reduce  the  time  of  analysis.  In  a  nutshell,  we  found  that  sampling  algorithms  with  larger  sample  sets  are  able  to  detect  higher  numbers  of  faults,  but  simple  algorithms  with  small  sample  sets,  such  as  most-enabled-disabled,  are  the  most  efficient  in  most  contexts.  Furthermore,  we  observed  that  the  limiting  assumptions  made  in  previous  work  influence  the  number  of  detected  faults,  the  size  of  sample  sets,  and  the  ranking  of  algorithms.  Finally,  we  have  identified  a  number  of  technical  challenges  when  trying  to  avoid  the  limiting  assumptions,  which  questions  the  practicality  of  certain  sampling  algorithms.
1	Deploying  cogtool  integrating  quantitative  usability  assessment  into  real  world  software  development.  Usability  concerns  are  often  difficult  to  integrate  into  real-world  software  development  processes.  To  remedy  this  situation,  IBM  research  and  development,  partnering  with  Carnegie  Mellon  University,  has  begun  to  employ  a  repeatable  and  quantifiable  usability  analysis  method,  embodied  in  CogTool,  in  its  development  practice.  CogTool  analyzes  tasks  performed  on  an  interactive  system  from  a  storyboard  and  a  demonstration  of  tasks  on  that  storyboard,  and  predicts  the  time  a  skilled  user  will  take  to  perform  those  tasks.  We  discuss  how  IBM  designers  and  UX  professionals  used  CogTool  in  their  existing  practice  for  contract  compliance,  communication  within  a  product  team  and  between  a  product  team  and  its  customer,  assigning  appropriate  personnel  to  fix  customer  complaints,  and  quantitatively  assessing  design  ideas  before  a  line  of  code  is  written.  We  then  reflect  on  the  lessons  learned  by  both  the  development  organizations  and  the  researchers  attempting  this  technology  transfer  from  academic  research  to  integration  into  real-world  practice,  and  we  point  to  future  research  to  even  better  serve  the  needs  of  practice.
1	Mining  input  sanitization  patterns  for  predicting  sql  injection  and  cross  site  scripting  vulnerabilities.  Static  code  attributes  such  as  lines  of  code  and  cyclomatic  complexity  have  been  shown  to  be  useful  indicators  of  defects  in  software  modules.  As  web  applications  adopt  input  sanitization  routines  to  prevent  web  security  risks,  static  code  attributes  that  represent  the  characteristics  of  these  routines  may  be  useful  for  predicting  web  application  vulnerabilities.  In  this  paper,  we  classify  various  input  sanitization  methods  into  different  types  and  propose  a  set  of  static  code  attributes  that  represent  these  types.  Then  we  use  data  mining  methods  to  predict  SQL  injection  and  cross  site  scripting  vulnerabilities  in  web  applications.  Preliminary  experiments  show  that  our  proposed  attributes  are  important  indicators  of  such  vulnerabilities.
1	Witchdoctor  ide  support  for  real  time  auto  completion  of  refactorings.  Integrated  Development  Environments  (IDEs)  have  come  to  perform  a  wide  variety  of  tasks  on  behalf  of  the  programmer,  refactoring  being  a  classic  example.  These  operations  have  undeniable  benefits,  yet  their  large  (and  growing)  number  poses  a  cognitive  scalability  problem.  Our  main  contribution  is  WitchDoctor  --  a  system  that  can  detect,  on  the  fly,  when  a  programmer  is  hand-coding  a  refactoring.  The  system  can  then  complete  the  refactoring  in  the  background  and  propose  it  to  the  user  long  before  the  user  can  complete  it.  This  implies  a  number  of  technical  challenges.  The  algorithm  must  be  1)  highly  efficient,  2)  handle  unparseable  programs,  3)  tolerate  the  variety  of  ways  programmers  may  perform  a  given  refactoring,  4)  use  the  IDE's  proven  and  familiar  refactoring  engine  to  perform  the  refactoring,  even  though  the  the  refactoring  has  already  begun,  and  5)  support  the  wide  range  of  refactorings  present  in  modern  IDEs.  Our  techniques  for  overcoming  these  challenges  are  the  technical  contributions  of  this  paper.  We  evaluate  WitchDoctor's  design  and  implementation  by  simulating  over  5,000  refactoring  operations  across  three  open-source  projects.  The  simulated  user  is  faster  and  more  efficient  than  an  average  human  user,  yet  WitchDoctor  can  detect  more  than  90%  of  refactoring  operations  as  they  are  being  performed  --  and  can  complete  over  a  third  of  refactorings  before  the  simulated  user  does.  All  the  while,  WitchDoctor  remains  robust  in  the  face  of  non-parseable  programs  and  unpredictable  refactoring  scenarios.  We  also  show  that  WitchDoctor  is  efficient  enough  to  perform  computation  on  a  keystroke-by-keystroke  basis,  adding  an  average  overhead  of  only  15  milliseconds  per  keystroke.
1	Interactive  and  collaborative  source  code  annotation.  Software  documentation  plays  an  important  role  in  sharing  the  knowledge  behind  source  code  between  distributed  programmers.  Good  documentation  makes  source  code  easier  to  understand;  on  the  other  hand,  developers  have  to  constantly  update  the  documentation  whenever  the  source  code  changes.  Developers  will  benefit  from  an  automated  tool  that  simplifies  keeping  documentation  up-to-date  and  facilitates  collaborative  editing.  In  this  paper,  we  explore  the  concept  of  collaborative  code  annotation  by  combining  the  idea  from  crowdsourcing.  We  introduce  Cumiki,  a  web-based  collaborative  annotation  tool  that  makes  it  easier  for  crowds  of  developers  to  collaboratively  create  the  up-to-date  documentation.  This  paper  describes  the  user  interface,  the  mechanism,  and  its  implementation,  and  discusses  the  possible  usage  scenarios.
1	Information  security  risk  assessment  model  based  on  optimized  support  vector  machine  with  artificial  fish  swarm  algorithm.  Because  the  information  security  risk  assessment  have  the  problem  of  less  training  data  and  slow  convergence  rate,  we  put  forward  a  information  security  risk  assessment  model  based  on  support  vector  machine  (SVM)  using  artificial  fish  swarm  algorithm  (AFSA).  In  this  paper,  we  used  weekly  security  report  of  the  government  network  security  situation  from  China  National  Internet  Emergency  Center(CNCERT)  as  the  source  data  [1].  We  adopted  the  RBF  function  as  the  kernel  function  SVM,  then  optimized  the  penalty  coefficient  C  and  kernel  function  parameter  8  of  artificial  fish  swarm  algorithm.  At  the  end  of  this  paper,  we  established  the  optimal  evaluation  model  for  simulation.  Our  results  showed  that  the  information  security  risk  assessment  model  based  on  AFSA  SVM  has  higher  accuracy  and  faster  convergence  rate  than  the  one  of  cross-validation.
1	Deepperf  performance  prediction  for  configurable  software  with  deep  sparse  neural  network.  Many  software  systems  provide  users  with  a  set  of  configuration  options  and  different  configurations  may  lead  to  different  runtime  performance  of  the  system.  As  the  combination  of  configurations  could  be  exponential,  it  is  difficult  to  exhaustively  deploy  and  measure  system  performance  under  all  possible  configurations.  Recently,  several  learning  methods  have  been  proposed  to  build  a  performance  prediction  model  based  on  performance  data  collected  from  a  small  sample  of  configurations,  and  then  use  the  model  to  predict  system  performance  under  a  new  configuration.  In  this  paper,  we  propose  a  novel  approach  to  model  highly  configurable  software  system  using  a  deep  feedforward  neural  network  (FNN)  combined  with  a  sparsity  regularization  technique,  e.g.  the  L1  regularization.  Besides,  we  also  design  a  practical  search  strategy  for  automatically  tuning  the  network  hyperparameters  efficiently.  Our  method,  called  DeepPerf,  can  predict  performance  values  of  highly  configurable  software  systems  with  binary  and/or  numeric  configuration  options  at  much  higher  prediction  accuracy  with  less  training  data  than  the  state-of-the  art  approaches.  Experimental  results  on  eleven  public  real-world  datasets  confirm  the  effectiveness  of  our  approach.
1	Poster  property  specification  patterns  for  robotic  missions.  Engineering  dependable  software  for  mobile  robots  is  becoming  increasingly  important.  A  core  asset  in  engineering  mobile  robots  is  the  mission  specification---a  formal  description  of  the  goals  that  mobile  robots  shall  achieve.  Such  mission  specifications  are  used,  among  others,  to  synthesize,  verify,  simulate,  or  guide  the  engineering  of  robot  software.  Development  of  precise  mission  specifications  is  challenging.  Engineers  need  to  translate  the  mission  requirements  into  specification  structures  expressed  in  a  logical  language---a  laborious  and  error-prone  task.  To  mitigate  this  problem,  we  present  a  catalog  of  mission  specification  patterns  for  mobile  robots.  Our  focus  is  on  robot  movement,  one  of  the  most  prominent  and  recurrent  specification  problems  for  mobile  robots.  Our  catalog  maps  common  mission  specification  problems  to  recurrent  solutions,  which  we  provide  as  templates  that  can  be  used  by  engineers.  The  patterns  are  the  result  of  analyzing  missions  extracted  from  the  literature.  For  each  pattern,  we  describe  usage  intent,  known  uses,  relationships  to  other  patterns,  and---most  importantly---a  template  representing  the  solution  as  a  logical  formula  in  temporal  logic.  Our  specification  patterns  constitute  reusable  building  blocks  that  can  be  used  by  engineers  to  create  complex  mission  specifications  while  reducing  specification  mistakes.  We  believe  that  our  patterns  support  researchers  working  on  tool  support  and  techniques  to  synthesize  and  verify  mission  specifications,  and  language  designers  creating  rich  domain-specific  languages  for  mobile  robots,  incorporating  our  patterns  as  language  concepts.
1	Online  app  review  analysis  for  identifying  emerging  issues.  Detecting  emerging  issues  (e.g.,  new  bugs)  timely  and  precisely  is  crucial  for  developers  to  update  their  apps.  App  reviews  provide  an  opportunity  to  proactively  collect  user  complaints  and  promptly  improve  apps'  user  experience,  in  terms  of  bug  fixing  and  feature  refinement.  However,  the  tremendous  quantities  of  reviews  and  noise  words  (e.g.,  misspelled  words)  increase  the  difficulties  in  accurately  identifying  newly-appearing  app  issues.  In  this  paper,  we  propose  a  novel  and  automated  framework  IDEA,  which  aims  to  IDentify  Emerging  App  issues  effectively  based  on  online  review  analysis.  We  evaluate  IDEA  on  six  popular  apps  from  Google  Play  and  Apple's  App  Store,  employing  the  official  app  changelogs  as  our  ground  truth.  Experiment  results  demonstrate  the  effectiveness  of  IDEA  in  identifying  emerging  app  issues.  Feedback  from  engineers  and  product  managers  shows  that  88.9%  of  them  think  that  the  identified  issues  can  facilitate  app  development  in  practice.  Moreover,  we  have  successfully  applied  IDEA  to  several  products  of  Tencent,  which  serve  hundreds  of  millions  of  users.
1	Ways  of  applying  artificial  intelligence  in  software  engineering.  As  Artificial  Intelligence  (AI)  techniques  become  more  powerful  and  easier  to  use  they  are  increasingly  deployed  as  key  components  of  modern  software  systems.  While  this  enables  new  functionality  and  often  allows  better  adaptation  to  user  needs  it  also  creates  additional  problems  for  software  engineers  and  exposes  companies  to  new  risks.  Some  work  has  been  done  to  better  understand  the  interaction  between  Software  Engineering  and  AI  but  we  lack  methods  to  classify  ways  of  applying  AI  in  software  systems  and  to  analyse  and  understand  the  risks  this  poses.  Only  by  doing  so  can  we  devise  tools  and  solutions  to  help  mitigate  them.  This  paper  presents  the  AI  in  SE  Application  Levels  (AI-SEAL)  taxonomy  that  categorises  applications  according  to  their  point  of  application,  the  type  of  AI  technology  used  and  the  automation  level  allowed.  We  show  the  usefulness  of  this  taxonomy  by  classifying  15  papers  from  previous  editions  of  the  RAISE  workshop.  Results  show  that  the  taxonomy  allows  classification  of  distinct  AI  applications  and  provides  insights  concerning  the  risks  associated  with  them.  We  argue  that  this  will  be  important  for  companies  in  deciding  how  to  apply  AI  in  their  software  applications  and  to  create  strategies  for  its  use.
1	A  neuro  cognitive  perspective  of  program  comprehension.  Program  comprehension  is  the  cognitive  process  of  understanding  code.  Researchers  have  proposed  several  models  to  describe  program  comprehension.  However,  because  program  comprehension  is  an  internal  process  and  difficult  to  measure,  the  accuracy  of  the  existing  models  are  limited.  Neuro-imaging  methods,  such  as  functional  magnetic  resonance  imaging  (fMRI),  provide  a  novel  neuro-cognitive  perspective  to  program-comprehension  research.  With  my  thesis  work,  we  aim  at  establishing  fMRI  as  a  new  tool  for  program-comprehension  and  software-engineering  studies.  Furthermore,  we  seek  to  refine  our  existing  framework  for  conducting  fMRI  studies  by  extending  it  with  eye  tracking  and  improved  control  conditions.  We  describe  how  we  will  apply  our  upgraded  framework  to  extend  our  understanding  of  program  comprehension.  In  the  long-run,  we  would  like  to  contribute  insights  from  our  fMRI  studies  into  software-engineering  practices  by  providing  code-styling  guidelines  and  programming  tools,  which  reduce  the  required  cognitive  effort  to  comprehend  code.
1	Designing  a  next  generation  continuous  software  delivery  system  concepts  and  architecture.  Continuous  Integration  and  Continuous  Delivery  are  established  practices  in  modern  agile  software  development.  The  DevOps  movement  adapted  theses  practices  and  places  the  deployment  pipeline  at  its  heart  as  one  of  the  main  requirements  to  automate  the  software  development  process  and  to  deliver  and  operate  software  in  a  more  robust  way  with  higher  quality.  Over  the  time  a  lot  of  systems  and  tools  has  been  developed  to  implement  the  deployment  pipeline  and  to  support  continuous  delivery.  But  software  development  is  complex,  its  process  even  more  and  due  to  the  individual  organization  of  software  vendors  no  real  all-in-one  solution  for  CD  exists.  Literature  identified  a  lot  of  challenges  when  adopting  CD  and  DevOps  in  an  organization.  This  paper  presents  a  conceptual  model  and  fundamental  design  decisions  for  a  new  generation  of  software  delivery  systems  tackling  some  of  these  issues.  Our  approach  focuses  on  two  specific  challenges  for  adopting  CD.  The  first  is  the  lack  of  flexibility  and  maintainability  of  software  delivery  systems.  The  second  is  the  insufficient  user  support  to  model  and  manage  delivery  processes  and  pipelines.  We  introduce  an  automated  mechanism  to  ease  the  effort  for  developers  and  other  stakeholders.  Based  on  these  results  this  paper  introduces  an  architectural  proposal  for  a  next-generation  continuous  software  delivery  system."
1	Experimentation  in  the  operating  system  the  windows  experimentation  platform.  Online  controlled  experiments  are  the  gold  standard  for  evaluating  improvements  and  accelerating  innovations  in  online  and  app  worlds.  However,  little  is  known  about  applicability,  implementation,  and  efficacy  of  experimentation  for  operating  systems  (OS),  where  many  features  are  non-user-facing.  In  this  paper,  we  present  the  Windows  Experimentation  platform  (WExp),  and  insights  from  implementation  and  execution  of  real-world  experiments  in  the  OS.  We  start  by  discussing  the  need  for  experimentation  in  OS,  using  real  experiments  to  illustrate  the  benefits.  We  then  describe  the  architecture  of  WExp,  focusing  on  unique  considerations  in  its  engineering.  Finally,  we  discuss  learnings  and  challenges  from  conducting  real-world  experiments.  Our  experiences  and  insights  can  motivate  practitioners  to  start  experimenting  as  well  as  to  help  them  to  successfully  build  their  experimentation  platforms.  The  learnings  can  also  guide  experimenters  with  best-practices  and  highlight  promising  avenues  for  future  research.
1	Rule  based  code  generation  in  industrial  automation  four  large  scale  case  studies  applying  the  cayenne  method.  Software  development  for  industrial  automation  applications  is  a  growing  market  with  high  economic  impact.  Control  engineers  design  and  implement  software  for  such  systems  using  standardized  programming  languages  (IEC  61131-3)  and  still  require  substantial  manual  work  causing  high  engineering  costs  and  potential  quality  issues.  Methods  for  automatically  generating  control  logic  using  knowledge  extraction  from  formal  requirements  documents  have  been  developed,  but  so  far  only  been  demonstrated  in  simplified  lab  settings.  We  have  executed  four  case  studies  on  large  industrial  plants  with  thousands  of  sensors  and  actuators  for  a  rule-based  control  logic  generation  approach  called  CAYENNE  to  determine  its  practicability.  We  found  that  we  can  generate  more  than  70  percent  of  the  required  interlocking  control  logic  with  code  generation  rules  that  are  applicable  across  different  plants.  This  can  lead  to  estimated  overall  development  cost  savings  of  up  to  21  percent,  which  provides  a  promising  outlook  for  methods  in  this  class.
1	Mobile  robot  path  planning  based  on  improved  ddpg  reinforcement  learning  algorithm.  Mobile  robotics  has  a  wide  range  of  applications  and  path  planning  is  key  to  its  realization.  Mobile  robots  need  to  explore  the  environment  autonomously  to  find  their  destinations.  The  Deep  Deterministic  Policy  Gradient  (DDPG)  algorithm,  a  classical  algorithm  in  deep  reinforcement  learning,  has  a  large  advantage  in  continuous  control  problems.  However,  the  DDPG  algorithm  suffers  from  the  problems  of  low  training  efficiency  and  slow  convergence  caused  by  the  high  proportion  of  illegal  policies  due  to  the  lack  of  policy  action  filtering.  In  this  paper,  we  propose  a  mobile  robot  path  planning  method  based  on  an  improved  DDPG  reinforcement  learning  algorithm,  which  uses  a  small  amount  of  a  priori  knowledge  to  accelerate  the  training  of  deep  reinforcement  learning,  reduce  the  number  of  trial  and  error,  and  adopt  an  adaptive  exploration  method  based  on  the  $\varepsilon$  -greedy  algorithm.  Dynamically  adjust  the  exploration  factor  to  rationally  allocate  the  probability  of  exploration  and  exploitation.  The  adaptive  exploration  method  can  improve  the  exploration  efficiency,  reduce  the  exploration  duration  and  speed  up  the  convergence  of  the  algorithm.  Simulation  experiments  are  conducted  in  a  grid  environment,  and  the  results  show  that  the  proposed  algorithm  can  successfully  find  the  optimal  path.  Moreover,  the  comparison  experiments  between  Q-learning,  SARSA  and  the  proposed  algorithm  demonstrate  that  the  proposed  algorithm  has  better  path  planning  performance,  spends  the  least  computation  time  and  converges  the  fastest.
1	Pipelining  bottom  up  data  flow  analysis.  Bottom-up  program  analysis  has  been  traditionally  easy  to  parallelize  because  functions  without  caller-callee  relations  can  be  analyzed  independently.  However,  such  function-level  parallelism  is  significantly  limited  by  the  calling  dependence  -  functions  with  caller-callee  relations  have  to  be  analyzed  sequentially  because  the  analysis  of  a  function  depends  on  the  analysis  results,  a.k.a.,  function  summaries,  of  its  callees.  We  observe  that  the  calling  dependence  can  be  relaxed  in  many  cases  and,  as  a  result,  the  parallelism  can  be  improved.  In  this  paper,  we  present  Coyote,  a  framework  of  bottom-up  data  flow  analysis,  in  which  the  analysis  task  of  each  function  is  elaborately  partitioned  into  multiple  sub-tasks  to  generate  pipelineable  function  summaries.  These  sub-tasks  are  pipelined  and  run  in  parallel,  even  though  the  calling  dependence  exists.  We  formalize  our  idea  under  the  IFDS/IDE  framework  and  have  implemented  an  application  to  checking  null-dereference  bugs  and  taint  issues  in  C/C++  programs.  We  evaluate  Coyote  on  a  series  of  standard  benchmark  programs  and  open-source  software  systems,  which  demonstrates  significant  speedup  over  a  conventional  parallel  design.
1	Privacy  and  security  requirements  framework  for  the  internet  of  things  iot.  Capturing  privacy  and  security  requirements  in  the  very  early  stages  is  essential  for  creating  sufficient  public  confidence  in  order  to  facilitate  the  adaption  of  novel  systems  such  as  the  Internet  of  Things  (IoT).  However,  traditional  requirements  engineering  methods  and  frameworks  might  not  be  sufficiently  effective  when  dealing  with  new  types  of  IoT  heterogeneous  systems.  Therefore,  building  a  methodological  framework  to  model  the  privacy  and  security  requirements  specifications  for  IoT  is  necessary  in  order  to  deal  with  its  mission  critical  nature.  The  purpose  of  this  project  is  to  develop  such  a  requirements  engineering  framework  in  order  to  ensure  proper  development  of  IoT  with  security  and  privacy  taken  into  account  from  the  earliest  stages.
1	Csepm  a  continuous  software  engineering  process  metamodel.  Software  engineers  have  to  cope  with  uncertainties  and  changing  requirements.  Agile  methods  provide  flexibility  towards  changes  and  the  emergence  of  continuous  delivery  has  made  regular  feedback  loops  possible.  The  abilities  to  maintain  high  code  quality  through  reviews,  to  regularly  release  software,  and  to  collect  and  prioritize  user  feedback,  are  necessary  for  continuous  software  engineering  (CSE).  However,  there  exists  no  software  process  metamodel  that  handles  the  continuous  character  of  software  engineering.      In  this  paper,  we  describe  an  empirical  process  metamodel  for  continuous  software  engineering  called  CSEPM,  which  treats  development  activities  as  parallel  running  workflows  and  allows  tailoring  and  customization.  CSEPM  includes  static  aspects  that  describe  the  relations  between  specific  CSE  concepts  including  reviews,  releases,  and  feedback.  It  also  describes  the  dynamic  aspect  of  CSE,  how  development  workflows  are  activated  through  change  events.  We  show  how  CSEPM  allows  to  instantiate  linear,  iterative,  agile  and  continuous  process  models  and  how  it  enables  tailoring  and  customization.
1	Performance  analysis  using  subsuming  methods  an  industrial  case  study.  Large-scale  object-oriented  applications  consist  of  tens  of  thousands  of  methods  and  exhibit  highly  complex  runtime  behaviour  that  is  difficult  to  analyse  for  performance.  Typical  performance  analysis  approaches  that  aggregate  performance  measures  in  a  method-centric  manner  result  in  thinly  distributed  costs  and  few  easily  identifiable  optimisation  opportunities.  Subsuming  methods  analysis  is  a  new  approach  that  aggregates  performance  costs  across  repeated  patterns  of  method  calls  that  occur  in  the  application's  runtime  behaviour.  This  allows  automatic  identification  of  patterns  that  are  expensive  and  represent  practical  optimisation  opportunities.  To  evaluate  the  practicality  of  this  analysis  with  a  real  world  large-scale  object-oriented  application  we  completed  a  case  study  with  the  developers  of  letterboxd.com  ---  a  social  network  website  for  movie  goers.  Using  the  results  of  the  analysis  we  were  able  to  rapidly  implement  changes  resulting  in  a  54.8%  reduction  in  CPU  load  and  an  49.6%  reduction  in  average  response  time.
1	Build  waiting  time  in  continuous  integration  an  initial  interdisciplinary  literature  review.  In  this  position  paper,  we  present  and  demonstrate  the  idea  of  using  an  interdisciplinary  literature  review  to  accelerate  the  research  on  continuous  integration  practice.  A  common  suggestion  has  been  that  build  waiting  time  in  continuous  integration  cycle  should  be  less  than  10  minutes.  This  guideline  is  based  on  practitioners'  opinion  and  has  not  been  further  investigated.  The  objective  of  this  study  is  to  understand  the  effects  of  build  waiting  time  in  software  engineering  and  to  get  input  from  waiting  time  research  in  other  disciplines.  The  objective  is  met  by  performing  two  literature  reviews,  first  on  build  waiting  time  and  second  on  waiting  times  in  the  contexts  of  service  operation,  web  use  and  computer  use.  The  found  effects  of  build  waiting  time  were  categorized  into  continuous  integration  specific,  cognitive  and  emotional.  Two  minute  build  waiting  time  was  considered  optimal,  but  under  10  minutes  was  considered  acceptable.  Insight  from  other  waiting  time  research  suggests  that  the  perceptions  of  waiting  time  are  important  and  the  perceptions  can  be  lowered  by  providing  feedback  and  giving  developers  other  activities  during  the  integration.
1	Verifying  component  and  connector  models  against  crosscutting  structural  views.  The  structure  of  component  and  connector  (C&C)  models,  which  are  used  in  many  application  domains  of  software  engineering,  consists  of  components  at  different  containment  levels,  their  typed  input  and  output  ports,  and  the  connectors  between  them.  C&C  views,  which  we  have  presented  at  FSE'13,  can  be  used  to  specify  structural  properties  of  C&C  models  in  an  expressive  and  intuitive  way.          In  this  work  we  address  the  verification  of  a  C&C  model  against  a  C&C  view  and  present  efficient  (polynomial)  algorithms  to  decide  satisfaction.  A  unique  feature  of  our  work,  not  present  in  existing  approaches  to  checking  structural  properties  of  C&C  models,  is  the  generation  of  witnesses  for  satisfaction/non-satisfaction  and  of  short  natural-language  texts,  which  serve  to  explain  and  formally  justify  the  verification  results  and  point  the  engineer  to  its  causes.          A  prototype  tool  and  an  evaluation  over  four  example  systems  with  multiple  views,  performance  and  scalability  experiments,  as  well  as  a  user  study  of  the  usefulness  of  the  witnesses  for  engineers,  demonstrate  the  contribution  of  our  work  to  the  state-of-the-art  in  component  and  connector  modeling  and  analysis.
1	Quantifying  the  impact  of  different  non  functional  requirements  and  problem  domains  on  software  effort  estimation.  The  effort  estimation  techniques  used  in  the  software  industry  often  tend  to  ignore  the  impact  of  Non-functional  Requirements  (NFR)  on  effort  and  reuse  standard  effort  estimation  models  without  local  calibration.  Moreover,  the  effort  estimation  models  are  calibrated  using  data  of  previous  projects  that  may  belong  to  problem  domains  different  from  the  project  which  is  being  estimated.  Our  approach  suggests  a  novel  effort  estimation  methodology  that  can  be  used  in  the  early  stages  of  software  development  projects.  Our  proposed  methodology  initially  clusters  the  historical  data  from  the  previous  projects  into  different  problem  domains  and  generates  domain  specific  effort  estimation  models,  each  incorporating  the  impact  of  NFR  on  effort  by  sets  of  objectively  measured  nominal  features.  We  reduce  the  complexity  of  these  models  using  a  feature  subset  selection  algorithm.  In  this  paper,  we  discuss  our  approach  in  details,  and  we  present  the  results  of  our  experiments  using  different  supervised  machine  learning  algorithms.  The  results  show  that  our  approach  performs  well  by  increasing  the  correlation  coefficient  and  decreasing  the  error  rate  of  the  generated  effort  estimation  models  and  achieving  more  accurate  effort  estimates  for  the  new  projects.
1	Antminer  mining  more  bugs  by  reducing  noise  interference.  Detecting  bugs  with  code  mining  has  proven  to  be  an  effective  approach.  However,  the  existing  methods  suffer  from  reporting  serious  false  positives  and  false  negatives.  In  this  paper,  we  developed  an  approach  called  AntMiner  to  improve  the  precision  of  code  mining  by  carefully  preprocessing  the  source  code.  Specifically,  we  employ  the  program  slicing  technique  to  decompose  the  original  source  repository  into  independent  sub-repositories,  taking  critical  operations  (automatically  extracted  from  source  code)  as  slicing  criteria.  In  this  way,  the  statements  irrelevant  to  a  critical  operation  are  excluded  from  the  corresponding  sub-repository.  Besides,  various  semantics-equivalent  representations  are  normalized  into  a  canonical  form.  Eventually,  the  mining  process  can  be  performed  on  a  refined  code  database,  and  false  positives  and  false  negatives  can  be  significantly  pruned.  We  have  implemented  AntMiner  and  applied  it  to  detect  bugs  in  the  Linux  kernel.  It  reported  52  violations  that  have  been  either  confirmed  as  real  bugs  by  the  kernel  development  community  or  fixed  in  new  kernel  versions.  Among  them,  41  cannot  be  detected  by  a  widely  used  representative  analysis  tool  Coverity.  Besides,  the  result  of  a  comparative  analysis  shows  that  our  approach  can  effectively  improve  the  precision  of  code  mining  and  detect  subtle  bugs  that  have  previously  been  missed.
1	What  is  the  perception  of  female  and  male  software  professionals  on  performance  team  dynamics  and  job  satisfaction  insights  from  the  trenches.  Research  has  shown  that  gender  diversity  correlates  positively  with  innovation  and  productivity  in  many  professional  engineering  and  technology  domains.  Yet,  software  development  teams  are  dominated  by  males.  In  this  paper,  we  aim  at  understanding  whether  female  software  professionals,  compared  to  male,  have  different  perceptions  on  a)  team  performance  and  dynamics,  b)  their  own  personal  performance,  c)  their  immediate  supervisors,  and  d)  accomplishment,  recognition,  and  opportunities.  Understanding  perceptions  of  different  genders  can  help  software  professionals,  their  supervisors  and  those  responsible  for  staff  create  and  foster  environments  in  which  both  females  and  males  are  comfortable  and  perform  best.  To  achieve  this  aim,  we  conducted  a  survey  targeted  at  individual  software  professionals  in  technical  roles.  We  collected  and  analyzed  data  from  55  female  and  69  male  respondents.  Our  results  show  basic  differences  in  demographics  (e.g.,  males  tend  to  be  older,  have  more  senior  roles,  and  have  longer  tenure  with  their  employer).  While  we  did  find  some  differences  around  perceptions  of  spirit  of  team  work,  productivity,  sense  of  satisfaction  and  fairness  of  reviews  from  supervisors,  in  general,  females  and  males  do  not  seem  to  differ  significantly  in  their  perceptions.  Based  on  the  results  from  our  survey  and  insights  from  the  current  literature,  we  discuss  commonalities  and  differences  between  females  and  males,  and  explore  potential  implications  for  performance  reviews,  recognition,  and  career  progression.
1	The  co  evolution  of  socio  technical  structures  in  sustainable  software  development  lessons  from  the  open  source  software  communities.  Software  development  depends  on  many  factors,  including  technical,  human  and  social  aspects.  Due  to  the  complexity  of  this  dependence,  a  unifying  framework  must  be  defined  and  for  this  purpose  we  adopt  the  complex  networks  methodology.  We  use  a  data-driven  approach  based  on  a  large  collection  of  open  source  software  projects  extracted  from  online  project  development  platforms.  The  preliminary  results  presented  in  this  article  reveal  that  the  network  perspective  yields  key  insights  into  the  sustainability  of  software  development.
1	Combining  functional  and  imperative  programming  for  multicore  software  an  empirical  study  evaluating  scala  and  java.  Recent  multi-paradigm  programming  languages  combine  functional  and  imperative  programming  styles  to  make  software  development  easier.  Given  today's  proliferation  of  multicore  processors,  parallel  programmers  are  supposed  to  benefit  from  this  combination,  as  many  difficult  problems  can  be  expressed  more  easily  in  a  functional  style  while  others  match  an  imperative  style.  Due  to  a  lack  of  empirical  evidence  from  controlled  studies,  however,  important  software  engineering  questions  are  largely  unanswered.  Our  paper  is  the  first  to  provide  thorough  empirical  results  by  using  Scala  and  Java  as  a  vehicle  in  a  controlled  comparative  study  on  multicore  software  development.  Scala  combines  functional  and  imperative  programming  while  Java  focuses  on  imperative  shared-memory  programming.  We  study  thirteen  programmers  who  worked  on  three  projects,  including  an  industrial  application,  in  both  Scala  and  Java.  In  addition  to  the  resulting  39  Scala  programs  and  39  Java  programs,  we  obtain  data  from  an  industry  software  engineer  who  worked  on  the  same  project  in  Scala.  We  analyze  key  issues  such  as  effort,  code,  language  usage,  performance,  and  programmer  satisfaction.  Contrary  to  popular  belief,  the  functional  style  does  not  lead  to  bad  performance.  Average  Scala  run-times  are  comparable  to  Java,  lowest  run-times  are  sometimes  better,  but  Java  scales  better  on  parallel  hardware.  We  confirm  with  statistical  significance  Scala's  claim  that  Scala  code  is  more  compact  than  Java  code,  but  clearly  refute  other  claims  of  Scala  on  lower  programming  effort  and  lower  debugging  effort.  Our  study  also  provides  explanations  for  these  observations  and  shows  directions  on  how  to  improve  multi-paradigm  languages  in  the  future.
1	A  genetic  algorithm  for  detecting  significant  floating  point  inaccuracies.  It  is  well-known  that  using  floating-point  numbers  may  inevitably  result  in  inaccurate  results  and  sometimes  even  cause  serious  software  failures.  Safety-critical  software  often  has  strict  requirements  on  the  upper  bound  of  inaccuracy,  and  a  crucial  task  in  testing  is  to  check  whether  significant  inaccuracies  may  be  produced.      The  main  existing  approach  to  the  floating-point  inaccuracy  problem  is  error  analysis,  which  produces  an  upper  bound  of  inaccuracies  that  may  occur.  However,  a  high  upper  bound  does  not  guarantee  the  existence  of  inaccuracy  defects,  nor  does  it  give  developers  any  concrete  test  inputs  for  debugging.      In  this  paper,  we  propose  the  first  metaheuristic  search-based  approach  to  automatically  generating  test  inputs  that  aim  to  trigger  significant  inaccuracies  in  floating-point  programs.  Our  approach  is  based  on  the  following  two  insights:  (1)  with  FPDebug,  a  recently  proposed  dynamic  analysis  approach,  we  can  build  a  reliable  fitness  function  to  guide  the  search;  (2)  two  main  factors  ---  the  scales  of  exponents  and  the  bit  formations  of  significands  ---  may  have  significant  impact  on  the  accuracy  of  the  output,  but  in  largely  different  ways.  We  have  implemented  and  evaluated  our  approach  over  154  real-world  floating-point  functions.  The  results  show  that  our  approach  can  detect  significant  inaccuracies  in  the  subjects.
1	The  quamoco  product  quality  modelling  and  assessment  approach.  Published  software  quality  models  either  provide  abstract  quality  attributes  or  concrete  quality  assessments.  There  are  no  models  that  seamlessly  integrate  both  aspects.  In  the  project  Quamoco,  we  built  a  comprehensive  approach  with  the  aim  to  close  this  gap.          For  this,  we  developed  in  several  iterations  a  meta  quality  model  specifying  general  concepts,  a  quality  base  model  covering  the  most  important  quality  factors  and  a  quality  assessment  approach.  The  meta  model  introduces  the  new  concept  of  a  product  factor,  which  bridges  the  gap  between  concrete  measurements  and  abstract  quality  aspects.  Product  factors  have  measures  and  instruments  to  operationalise  quality  by  measurements  from  manual  inspection  and  tool  analysis.  The  base  model  uses  the  ISO  25010  quality  attributes,  which  we  refine  by  200  factors  and  600  measures  for  Java  and  C#  systems.          We  found  in  several  empirical  validations  that  the  assessment  results  fit  to  the  expectations  of  experts  for  the  corresponding  systems.  The  empirical  analyses  also  showed  that  several  of  the  correlations  are  statistically  significant  and  that  the  maintainability  part  of  the  base  model  has  the  highest  correlation,  which  fits  to  the  fact  that  this  part  is  the  most  comprehensive.  Although  we  still  see  room  for  extending  and  improving  the  base  model,  it  shows  a  high  correspondence  with  expert  opinions  and  hence  is  able  to  form  the  basis  for  repeatable  and  understandable  quality  assessments  in  practice.
1	Commit  bubbles.  Developers  who  use  version  control  are  expected  to  produce  systematic  commit  histories  that  show  well-defined  steps  with  logical  forward  progress.  Existing  version  control  tools  assume  that  developers  also  write  code  systematically.  Unfortunately,  the  process  by  which  developers  write  source  code  is  often  evolutionary,  or  as-needed,  rather  than  systematic.  Our  contribution  is  a  fragment-oriented  concept  called  Commit  Bubbles  that  will  allow  developers  to  construct  systematic  commit  histories  that  adhere  to  version  control  best  practices  with  less  cognitive  effort,  and  in  a  way  that  integrates  with  their  as-needed  coding  workflows.
1	Towards  better  program  obfuscation  optimization  via  language  models.  As  a  common  practice  in  software  development,  program  obfuscation  aims  at  deterring  reverse  engineering  and  malicious  attacks  on  released  source  or  binary  code.  Owning  ample  obfuscation  techniques,  we  have  relatively  little  knowledge  on  how  to  most  effectively  use  them.  The  biggest  challenge  lies  in  identifying  the  most  useful  combination  of  these  techniques.  We  propose  a  unified  framework  to  automatically  generate  and  optimize  obfuscation  based  on  an  obscurity  language  model  and  a  Monte  Carlo  Markov  Chain  (MCMC)  based  search  algorithm.  We  further  instantiate  it  for  JavaScript  programs  and  developed  the  Closure*  tool.  Compared  to  the  well-known  Google  Closure  Compiler,  Closure*  outperforms  its  default  setting  by  26%.  For  programs  which  have  already  been  well  obfuscated,  Closure*  can  still  outperform  by  22%.
1	Fusion  of  log  mel  spectrogram  and  glcm  feature  in  acoustic  scene  classification.  Acoustic  scene  classification  (ASC)  is  an  important  problem  of  computational  auditory  scene  analysis.  The  proposed  feature  is  extracted  from  the  fusion  of  the  Log-Mel  Spectrogram  (LMS)  and  the  Gray  Level  Co-occurrence  Matrix  (GLCM)  for  the  acoustic  scene  classification.  LMS  of  the  input  audio  file  is  calculated  and  then  GLCM  feature  is  extracted  from  LMS  to  detect  the  changes  of  audio  signal  in  time  and  frequency  domain.  Multi-class  Support  Vector  Machine  (SVM)  trains  this  feature  in  order  to  categorize  the  type  of  environment  for  audio  input  files.  The  main  contribution  of  this  paper  is  to  extract  the  effective  feature  from  the  combination  of  signal  processing  approach  and  image  processing  approach.  The  purpose  of  this  feature  is  to  reduce  computational  time  for  classification.  This  system  uses  Detection  and  Classification  of  Acoustic  Scenes  and  Events  (DCASE  2016)  challenges  to  show  the  robustness  of  the  proposed  feature.
1	Parallel  refinement  for  multi  threaded  program  verification.  Program  verification  is  one  of  the  most  important  methods  to  ensuring  the  correctness  of  concurrent  programs.  However,  due  to  the  path  explosion  problem,  concurrent  program  verification  is  usually  time  consuming,  which  hinders  its  scalability  to  industrial  programs.  Parallel  processing  is  a  mainstream  technique  to  deal  with  those  problems  which  require  mass  computing.  Hence,  designing  parallel  algorithms  to  improve  the  performance  of  concurrent  program  verification  is  highly  desired.  This  paper  focuses  on  parallelization  of  the  abstraction  refinement  technique,  one  of  the  most  efficient  techniques  for  concurrent  program  verification.  We  present  a  parallel  refinement  framework  which  employs  multiple  engines  to  refine  the  abstraction  in  parallel.  Different  from  existing  work  which  parallelizes  the  search  process,  our  method  achieves  the  effect  of  parallelization  by  refinement  constraint  and  learnt  clause  sharing,  so  that  the  number  of  required  iterations  can  be  significantly  reduced.  We  have  implemented  this  framework  on  the  scheduling  constraint  based  abstraction  refinement  method,  one  of  the  best  methods  for  concurrent  program  verification.  Experiments  on  SV-COMP  2018  show  the  encouraging  results  of  our  method.  For  those  complex  programs  requiring  a  large  number  of  iterations,  our  method  can  obtain  a  linear  reduction  of  the  iteration  number  and  significantly  improve  the  verification  performance.
1	Deepconcolic  testing  and  debugging  deep  neural  networks.  Deep  neural  networks  (DNNs)  have  been  deployed  in  a  wide  range  of  applications.  We  introduce  a  DNN  testing  and  debugging  tool,  called  DeepConcolic,  which  is  able  to  detect  errors  with  sufficient  rigour  so  as  to  be  applicable  to  the  testing  of  DNNs  in  safety-related  applications.  DeepConcolic  is  the  first  tool  that  implements  a  concolic  testing  technique  for  DNNs,  and  the  first  testing  tool  that  provides  users  with  the  functionality  of  investigating  particular  parts  of  a  DNN.  The  tool  has  been  made  publicly  available  and  a  demo  video  can  be  found  at  https://youtu.be/rliynbhoNLM.
1	Knn  based  overlapping  samples  filter  approach  for  classification  of  imbalanced  data.  Imbalanced  data  classification  is  one  of  the  most  interesting  problems  in  various  real-world  data  sets.  The  class  distribution  of  imbalanced  data  set  strongly  affects  the  classification  rate  of  learning  classifiers.  If  the  class  distribution  problems  can’t  be  solved  before  implementing  the  learning  algorithms,  the  predictions  of  learning  classifiers  tend  to  support  a  large  number  of  samples  (majority  class)  and  ignore  the  other  samples  (minority  class).  In  addition,  the  class  overlapping  problem  can  increase  the  difficulty  to  classify  the  minority  class  samples  correctly.  In  this  paper,  we  propose  an  effective  under-sampling  method  for  the  classification  of  imbalanced  and  overlapping  data  by  using  KNN-based  overlapping  samples  filter  approach.  Besides,  this  paper  summarizes  the  performance  analysis  of  three  ensemble-based  learning  classifiers  for  the  proposed  method.  Experimental  results  on  fifteen  imbalanced  data  sets  indicate  that  the  proposed  under-sampling  method  can  effectively  improve  the  five  representative  algorithms  in  terms  of  three  popular  metrics;  area  under  the  curve  (AUC),  G-mean  and  F-measure.
1	Towards  saving  money  in  using  smart  contracts.  Being  a  new  kind  of  software  leveraging  blockchain  to  execute  real  contracts,  smart  contracts  are  in  great  demand  due  to  many  advantages.  Ethereum  is  the  largest  blockchain  platform  that  supports  smart  contracts  by  running  them  in  its  virtual  machine.  To  ensure  that  a  smart  contract  will  terminate  eventually  and  prevent  abuse  of  resources,  Ethereum  charges  the  developers  for  deploying  smart  contracts  and  the  users  for  executing  smart  contracts.  Although  our  previous  work  shows  that  under-optimized  smart  contracts  may  cost  more  money  than  necessary,  it  just  lists  7  anti-patterns  and  the  detection  method  for  3  of  them.  In  this  paper,  we  conduct  the  first  in-depth  investigation  on  such  under-optimized  smart  contracts.  We  first  identify  24  anti-patterns  from  the  execution  traces  of  real  smart  contracts.  Then,  we  design  and  develop  GasReducer,  the  first  tool  to  automatically  detect  all  these  anti-patterns  from  the  bytecode  of  smart  contracts  and  replace  them  with  efficient  code  through  bytecode-to-bytecode  optimization.  Using  GasReducer  to  analyze  all  smart  contracts  and  their  execution  traces,  we  detect  9,490,768  and  557,565,754  anti-pattern  instances  in  deploying  and  invoking  smart  contracts,  respectively.
1	Poster  deciding  weak  monitorability  for  runtime  verification.  An  important  problem  in  runtime  verification  is  monitorability.  If  a  property  is  not  monitorable,  then  it  is  meaningless  to  check  it  at  runtime,  as  no  satisfaction  or  violation  will  be  reported  in  finite  steps.  In  this  paper,  we  revisit  the  classic  definition  of  monitorability,  and  show  that  it  is  too  restrictive  for  practical  runtime  verification.  We  propose  a  weaker  but  more  practical  definition  of  monitorability,  say  weak  monitorability,  and  show  how  to  decide  weak  monitorability  for  runtime  verification.
1	Is  better  data  better  than  better  data  miners  on  the  benefits  of  tuning  smote  for  defect  prediction.  We  report  and  fix  an  important  systematic  error  in  prior  studies  that  ranked  classifiers  for  software  analytics.  Those  studies  did  not  (a)  assess  classifiers  on  multiple  criteria  and  they  did  not  (b)  study  how  variations  in  the  data  affect  the  results.  Hence,  this  paper  applies  (a)  multi-performance  criteria  while  (b)  fixing  the  weaker  regions  of  the  training  data  (using  SMOTUNED,  which  is  an  auto-tuning  version  of  SMOTE).  This  approach  leads  to  dramatically  large  increases  in  software  defect  predictions  when  applied  in  a  5*5  cross-validation  study  for  3,681  JAVA  classes  (containing  over  a  million  lines  of  code)  from  open  source  systems,  SMOTUNED  increased  AUC  and  recall  by  60%  and  20%  respectively.  These  improvements  are  independent  of  the  classifier  used  to  predict  for  defects.  Same  kind  of  pattern  (improvement)  was  observed  when  a  comparative  analysis  of  SMOTE  and  SMOTUNED  was  done  against  the  most  recent  class  imbalance  technique.In  conclusion,  for  software  analytic  tasks  like  defect  prediction,  (1)  data  pre-processing  can  be  more  important  than  classifier  choice,  (2)  ranking  studies  are  incomplete  without  such  pre-processing,  and  (3)  SMOTUNED  is  a  promising  candidate  for  pre-processing.
1	Artifact  for  gentree  using  decision  trees  to  learn  interactions  for  configurable  software.  This  document  describes  the  artifact  package  accompanying  the  ICSE'21  paper  "GenTree:  Using  Decision  Trees  to  Learn  Interactions  for  Configurable  Software"  [1].  The  artifact  includes  GenTree  source  code,  pre-built  binaries,  benchmark  program  specifications,  and  scripts  to  replicate  the  data  presented  in  the  paper.  Furthermore,  GenTree  is  applicable  to  new  programs  written  in  supported  languages  (C,  C++,  Python,  Perl,  Ocaml),  or  can  be  extended  to  support  new  languages  easily.  GenTree  implementation  is  highly  modular  and  optimized,  hence,  it  can  also  be  used  as  a  framework  for  developing  and  testing  new  interaction  inference  algorithms.  We  hope  the  artifact  will  be  useful  for  researchers  who  are  interested  in  interaction  learning,  especially  iterative  and  data-driven  approaches.
1	Debugging  crashes  using  continuous  contrast  set  mining.  Facebook  operates  a  family  of  services  used  by  over  two  billion  people  daily  on  a  huge  variety  of  mobile  devices.  Many  devices  are  configured  to  upload  crash  reports  should  the  app  crash  for  any  reason.  Engineers  monitor  and  triage  millions  of  crash  reports  logged  each  day  to  check  for  bugs,  regressions,  and  any  other  quality  problems.  Debugging  groups  of  crashes  is  a  manually  intensive  process  that  requires  deep  domain  expertise  and  close  inspection  of  traces  and  code,  often  under  time  constraints.We  use  contrast  set  mining,  a  form  of  discriminative  pattern  mining,  to  learn  what  distinguishes  one  group  of  crashes  from  another.  Prior  works  focus  on  discretization  to  apply  contrast  mining  to  continuous  data.  We  propose  the  first  direct  application  of  contrast  learning  to  continuous  data,  without  the  need  for  discretization.  We  also  define  a  weighted  anomaly  score  that  unifies  continuous  and  categorical  contrast  sets  while  mitigating  bias,  as  well  as  uncertainty  measures  that  communicate  confidence  to  developers.  We  demonstrate  the  value  of  our  novel  statistical  improvements  by  applying  it  on  a  challenging  dataset  from  Facebook  production  logs,  where  we  achieve  40x  speedup  over  baseline  approaches  using  discretization.CCS  Concepts•  Software  and  its  engineering  $\rightarrow$Software  reliability.
1	Featurenet  diversity  driven  generation  of  deep  learning  models.  We  present  FeatureNET,  an  open-source  Neural  Architecture  Search  (NAS)  tool  1  that  generates  diverse  sets  of  Deep  Learning  (DL)  models.  FeatureNET  relies  on  a  meta-model  of  deep  neural  networks,  consisting  of  generic  configurable  entities.  Then,  it  uses  tools  developed  in  the  context  of  software  product  lines  to  generate  diverse  (maximize  the  differences  between  the  generated)  DL  models.  The  models  are  translated  to  Keras  and  can  be  integrated  into  typical  machine  learning  pipelines.  FeatureNET  allows  researchers  to  generate  seamlessly  a  large  variety  of  models.  Thereby,  it  helps  choosing  appropriate  DL  models  and  performing  experiments  with  diverse  models  (mitigating  potential  threats  to  validity).  As  a  NAS  method,  FeatureNET  successfully  generates  models  performing  equally  well  with  handcrafted  models.
1	How  do  companies  collaborate  in  open  source  ecosystems  an  empirical  study  of  openstack.  Open  Source  Software  (OSS)  has  come  to  play  a  critical  role  in  the  software  industry.  Some  large  ecosystems  enjoy  the  participation  of  large  numbers  of  companies,  each  of  which  has  its  own  focus  and  goals.  Indeed,  companies  that  otherwise  compete,  may  become  collaborators  within  the  OSS  ecosystem  they  participate  in.  Prior  research  has  largely  focused  on  commercial  involvement  in  OSS  projects,  but  there  is  a  scarcity  of  research  focusing  on  company  collaborations  within  OSS  ecosystems.  Some  of  these  ecosystems  have  become  critical  building  blocks  for  organizations  worldwide;  hence,  a  clear  understanding  of  how  companies  collaborate  within  large  ecosystems  is  essential.  This  paper  presents  the  results  of  an  empirical  study  of  the  OpenStack  ecosystem,  in  which  hundreds  of  companies  collaborate  on  thousands  of  project  repositories  to  deliver  cloud  distributions.  Based  on  a  detailed  analysis,  we  identify  clusters  of  collaborations,  and  identify  four  strategies  that  companies  adopt  to  engage  with  the  OpenStack  ecosystem.  We  alsofind  that  companies  may  engage  in  intentional  or  passive  collaborations,  or  may  work  in  an  isolated  fashion.  Further,  wefi  nd  that  a  company's  position  in  the  collaboration  network  is  positively  associated  with  its  productivity  in  OpenStack.  Our  study  sheds  light  on  how  large  OSS  ecosystems  work,  and  in  particular  on  the  patterns  of  collaboration  within  one  such  large  ecosystem.
1	Recommendation  of  move  method  refactoring  using  path  based  representation  of  code.  Software  refactoring  plays  an  important  role  in  increasing  code  quality.  One  of  the  most  popular  refactoring  types  is  the  Move  Method  refactoring.  It  is  usually  applied  when  a  method  depends  more  on  members  of  other  classes  than  on  its  own  original  class.  Several  approaches  have  been  proposed  to  recommend  Move  Method  refactoring  automatically.  Most  of  them  are  based  on  heuristics  and  have  certain  limitations  (e.g.,  they  depend  on  the  selection  of  metrics  and  manually-defined  thresholds).  In  this  paper,  we  propose  an  approach  to  recommend  Move  Method  refactoring  based  on  a  path-based  representation  of  code  called  code2vec  that  is  able  to  capture  the  syntactic  structure  and  semantic  information  of  a  code  fragment.  We  use  this  code  representation  to  train  a  machine  learning  classifier  suggesting  to  move  methods  to  more  appropriate  classes.  We  evaluate  the  approach  on  two  publicly  available  datasets:  a  manually  compiled  dataset  of  well-known  open-source  projects  and  a  synthetic  dataset  with  automatically  injected  code  smell  instances.  The  results  show  that  our  approach  is  capable  of  recommending  accurate  refactoring  opportunities  and  outperforms  JDeodorant  and  JMove,  which  are  state  of  the  art  tools  in  this  field.
1	From  boolean  relations  to  control  software.  Many  software  as  well  digital  hardware  automatic  synthesis  methods  define  the  set  of  implementations  meeting  the  given  system  specifications  with  a  boolean  relation  K.  In  such  a  context  a  fundamental  step  in  the  software  (hardware)  synthesis  process  is  finding  effective  solutions  to  the  functional  equation  defined  by  K.  This  entails  finding  a  (set  of)  boolean  function(s)  F  (typically  represented  using  OBDDs,  Ordered  Binary  Decision  Diagrams)  such  that:  1)  for  all  x  for  which  K  is  satisfiable,  K(x,F(x))  =  1  holds;  2)  the  implementation  of  F  is  efficient  with  respect  to  given  implementation  parameters  such  as  code  size  or  execution  time.  While  this  problem  has  been  widely  studied  in  digital  hardware  synthesis,  little  has  been  done  in  a  software  synthesis  context.  Unfortunately  the  approaches  developed  for  hardware  synthesis  cannot  be  directly  used  in  a  software  context.  This  motivates  investigation  of  effective  methods  to  solve  the  above  problem  when  F  has  to  be  implemented  with  software.  In  this  paper,  we  present  an  algorithm  that,  from  an  OBDD  representation  for  K,  generates  a  C  code  implementation  for  F  that  has  the  same  size  as  the  OBDD  for  F  and  a  worst  case  execution  time  linear  in  nr,  being  n  =  |x|  the  number  of  input  arguments  for  functions  in  F  and  r  the  number  of  functions  in  F.
1	A  dsl  for  multi  scale  and  autonomic  software  deployment.  In  this  paper,  we  present  an  ongoing  work  which  aims  at  defining  and  experimenting  a  Domain-Specific  Language  (DSL)  dedicated  to  multi-scale  and  autonomic  software  deployment.  Autonomic  software  deployment  in  open  environments  is  an  open  issue.  There,  the  topology  of  target  hosts  is  not  always  known  due  either  to  unforeseen  hardware  failures  or  limitations  (network  links,  hosts,  etc.)  or  to  device  arrival  and  disappearance.  In  a  previous  work,  we  proposed  to  describe  deployment  constraints  using  a  DSL  and  then  to  satisfy  them  using  a  middleware  for  autonomic  deployment,  rather  than  classically  building  and  executing  a  deployment  plan.  As  deployment  of  multi-scale  distributed  systems  demands  the  expression  of  specific  constraints  related  to  dimensions  and  scales,  it  is  necessary  to  think  over  and  define  a  new  Domain-Specific  Language.  In  this  paper,  we  propose  a  new  DSL  designed  to  support  the  expression  of  constraints  and  properties  related  to  multi-scale  and  autonomic  software  deployment.
1	A  systematic  mapping  study  on  domain  specific  languages.  Domain-Specific  Languages  (DSLs)  offer  substantial  gains  in  expressiveness  and  ease  of  use  compared  with  general  purpose  languages.  This  way,  DSLs  have  gained  significant  attention  in  industry  and  academy,  as  can  be  seen  by  the  increased  number  of  related  publications  in  key  conferences  and  journals.  This  paper  aims  to  provide  a  broad  view  of  the  DSL  research  field  by  performing  a  Systematic  Mapping  Study.  Adopting  a  detailed  search  strategy,  4450  studies  were  initially  identified,  and,  after  filtering,  1440  primary  studies  were  selected  and  categorized  using  a  particular  classification  scheme.  So,  this  work  presents  the  most  popular  application  domains  where  DSLs  have  been  applied,  identifies  different  tools  for  handling  DSLs,  including  language  workbenches,  and  enumerates  several  techniques,  methods  and/or  processes  for  dealing  with  DSLs.
1	Mining  opinions  in  user  generated  contents  to  improve  course  evaluation.  The  purpose  of  this  paper  is  to  show  how  opinion  mining  may  offer  an  alternative  way  to  improve  course  evaluation  using  students’  attitudes  posted  on  Internet  forums,  discussion  groups  and/or  blogs,  which  are  collectively  called  user-generated  content.  We  propose  a  model  to  mine  knowledge  from  students’  opinions  to  improve  teaching  effectiveness  in  academic  institutes.  Opinion  mining  is  used  to  evaluate  course  quality  in  two  steps:  opinion  classification  and  opinion  extraction.  In  opinion  classification,  machine  learning  methods  have  been  applied  to  classify  an  opinion  as  positive  or  negative  for  each  student’s  posts.  Then,  we  used  opinion  extraction  to  extract  features,  such  as  teacher,  exams  and  resources,  from  the  user-generated  content  for  a  specific  course.  Then  we  grouped  and  assigned  orientations  for  each  feature.
1	A  survey  of  query  expansion  query  suggestion  and  query  refinement  techniques.  The  ineffectiveness  of  information  retrieval  systems  often  caused  by  the  inaccurate  use  of  keywords  in  a  query.  In  order  to  solve  the  ineffectiveness  problem  in  information  retrieval  systems,  many  solutions  have  been  proposed  over  the  years.  The  most  common  techniques  are  revolving  around  query  modification  techniques  such  as  query  expansion,  query  refinement,  etc.  Due  to  the  high  similarity  in  these  query  modification  techniques,  people  are  often  confused  about  their  differences.  However,  few  existing  survey  papers  compare  their  differences.  Hence,  in  this  paper,  we  first  briefly  discuss  the  basic  technique  of  query  expansion,  query  suggestion  and  query  refinement,  and  then  make  a  detailed  comparison  between  these  three  techniques.  We  finally  show  the  promising  future  research  trend  in  the  field  of  query  modification.
1	Toward  a  methodological  knowledge  for  service  oriented  development  based  on  open  meta  model.  Situational  method  engineering  uses  a  repository  of  reusable  method  fragments  that  are  derived  from  existing  software  development  methodologies  and  industrial  best  practices  to  simplify  the  construction  of  any  project-specific  software  development  methodology  aligned  with  specific  characteristics  of  a  project  at  hand.  In  this  respect,  OPEN  is  a  well-established,  standardized  and  popular  approach  for  situational  method  engineering.  It  has  a  large  repository  of  reusable  method  fragments  called  OPF  that  method  engineers  can  select  and  assemble  them  according  to  the  requirements  of  a  project  to  construct  a  new  project-specific  software  development  methodology.  In  this  position  paper,  we  present  the  basic  concepts  and  foundations  of  OPEN  and  argue  for  an  urgent  need  for  new  extensions  to  OPEN  and  its  repository  in  support  of  service-oriented  software  development  practices.
1	Remote  user  authentication  scheme  with  hardware  based  attestation.  Many  previous  works  on  remote  user  authentication  schemes  are  related  to  remote  services  environment  such  as  online  banking  and  electronic  commerce.  However,  these  schemes  are  dependent  solely  on  one  parameter,  namely,  user  legitimacy  in  order  to  fulfill  the  authentication  process.  Furthermore,  most  of  the  schemes  rely  on  prearranged  shared  secret  key  or  server  secret  key  to  generate  session  key  in  order  to  secure  its  communication.  Consequently,  these  schemes  are  vulnerable  to  malicious  software  attacks  that  could  compromise  the  integrity  of  the  platform  used  for  the  communication.  As  a  result,  user  identity  or  shared  secret  key  potentially  can  be  exposed  due  to  limitation  of  the  scheme  in  providing  trust  or  evidence  of  claimed  platform  identity.  In  this  paper,  we  propose  a  remote  authentication  with  hardware  based  attestation  and  secure  key  exchange  protocol  to  resist  malicious  software  attack.  In  addition,  we  also  propose  pseudonym  identity  enhancement  in  order  to  improve  user  identity  privacy.
2	Hybreed  a  software  framework  for  developing  context  aware  hybrid  recommender  systems.  This  article  introduces  Hybreed,  a  software  framework  for  building  complex  context-aware  applications,  together  with  a  set  of  components  that  are  specifically  targeted  at  developing  hybrid,  context-aware  recommender  systems.  Hybreed  is  based  on  a  concept  for  processing  context  that  we  call  dynamic  contextualization.  The  underlying  notion  of  context  is  very  generic,  enabling  application  developers  to  exploit  sensor-based  physical  factors  as  well  as  factors  derived  from  user  models  or  user  interaction.  This  approach  is  well  aligned  with  context  definitions  that  emphasize  the  dynamic  and  activity-oriented  nature  of  context.  As  an  extension  of  the  generic  framework,  we  describe  Hybreed  RecViews,  a  set  of  components  facilitating  the  development  of  context-aware  and  hybrid  recommender  systems.  With  Hybreed  and  RecViews,  developers  can  rapidly  develop  context-aware  applications  that  generate  recommendations  for  both  individual  users  and  groups.  The  framework  provides  a  range  of  recommendation  algorithms  and  strategies  for  producing  group  recommendations  as  well  as  templates  for  combining  different  methods  into  hybrid  recommenders.  Hybreed  also  provides  means  for  integrating  existing  user  or  product  data  from  external  sources  such  as  social  networks.  It  combines  aspects  known  from  context  processing  frameworks  with  features  of  state-of-the-art  recommender  system  frameworks,  aspects  that  have  been  addressed  only  separately  in  previous  research.  To  our  knowledge,  Hybreed  is  the  first  framework  to  cover  all  these  aspects  in  an  integrated  manner.  To  evaluate  the  framework  and  its  conceptual  foundation,  we  verified  its  capabilities  in  three  different  use  cases.  The  evaluation  also  comprises  a  comparative  assessment  of  Hybreed's  functional  features,  a  comparison  to  existing  frameworks,  and  a  user  study  assessing  its  usability  for  developers.  The  results  of  this  study  indicate  that  Hybreed  is  intuitive  to  use  and  extend  by  developers.
2	Learning  with  intelligent  tutors  and  worked  examples  selecting  learning  activities  adaptively  leads  to  better  learning  outcomes  than  a  fixed  curriculum.  The  main  learning  activity  provided  by  intelligent  tutoring  systems  is  problem  solving,  although  several  recent  projects  investigated  the  effectiveness  of  combining  problem  solving  with  worked  examples.  Previous  research  has  shown  that  learning  from  examples  is  an  effective  learning  strategy,  especially  for  novice  learners.  A  worked  example  provides  step-by-step  explanations  of  how  a  problem  is  solved.  Many  studies  have  compared  learning  from  examples  to  unsupported  problem  solving,  and  suggested  presenting  worked  examples  to  students  in  the  initial  stages  of  learning,  followed  by  problem  solving  once  students  have  acquired  enough  knowledge.  This  paper  presents  a  study  in  which  we  compare  a  fixed  sequence  of  alternating  worked  examples  and  tutored  problem  solving  with  a  strategy  that  adapts  learning  tasks  to  students'  needs.  The  adaptive  strategy  determines  the  type  of  the  task  (a  worked  example,  a  faded  example  or  a  problem  to  be  solved)  based  on  how  much  assistance  the  student  received  on  the  previous  problem.  The  results  show  that  students  in  the  adaptive  condition  learnt  significantly  more  than  their  peers  who  were  presented  with  a  fixed  sequence  of  worked  examples  and  problem  solving.  Novices  from  the  adaptive  condition  learnt  faster  than  novices  from  the  control  group,  while  the  advanced  students  from  the  adaptive  condition  learnt  more  than  their  peers  from  the  control  group.
2	Cross  system  user  modeling  and  personalization  on  the  social  web.  In  order  to  adapt  functionality  to  their  individual  users,  systems  need  information  about  these  users.  The  Social  Web  provides  opportunities  to  gather  user  data  from  outside  the  system  itself.  Aggregated  user  data  may  be  useful  to  address  cold-start  problems  as  well  as  sparse  user  profiles,  but  this  depends  on  the  nature  of  individual  user  profiles  distributed  on  the  Social  Web.  For  example,  does  it  make  sense  to  re-use  Flickr  profiles  to  recommend  bookmarks  in  Delicious?  In  this  article,  we  study  distributed  form-based  and  tag-based  user  profiles,  based  on  a  large  dataset  aggregated  from  the  Social  Web.  We  analyze  the  completeness,  consistency  and  replication  of  form-based  profiles,  which  users  explicitly  create  by  filling  out  forms  at  Social  Web  systems  such  as  Twitter,  Facebook  and  LinkedIn.  We  also  investigate  tag-based  profiles,  which  result  from  social  tagging  activities  in  systems  such  as  Flickr,  Delicious  and  StumbleUpon:  to  what  extent  do  tag-based  profiles  overlap  between  different  systems,  what  are  the  benefits  of  aggregating  tag-based  profiles.  Based  on  these  insights,  we  developed  and  evaluated  the  performance  of  several  cross-system  user  modeling  strategies  in  the  context  of  recommender  systems.  The  evaluation  results  show  that  the  proposed  methods  solve  the  cold-start  problem  and  improve  recommendation  quality  significantly,  even  beyond  the  cold-start.
2	Lasimup  large  scale  multi  touch  support  integration  across  multiple  projections  on  arbitrary  surfaces.  Interactive  projections  account  for  a  great  way  of  user  interaction  and  can  be  used  in  applications  that  need  to  span  a  significant  amount  of  space.  However,  commercial  hardware  limit  the  number  of  the  displayed  projections  and  as  a  result  the  need  for  such  applications  cannot  be  satisfied  out  of  the  box.  This  paper  discusses  the  inner  workings  of  LASIMUP  (LArge  Scale  Interactive  MUlti-Projection),  a  platform  that  aims  to  bypass  that  limitation  by  developing  the  appropriate  software  and  the  use  of  low-cost  supplementary  hardware  which  can  enable  such  functionality.  The  platform  presented  has  the  benefits  of  being  cross-platform,  easy  to  install  and  affordable  to  acquire.  These  factors  enable  the  possibility  of  creating  large  scale  applications,  that  otherwise  would  be  impractical  to  deploy,  with  a  very  small  financial  and  operating  overhead.
2	Using  smart  mobile  devices  for  monitoring  in  assistive  environments.  Over  the  last  few  years,  the  vast  majority  of  smart  devices  has  been  equipped  with  a  variety  of  sensors,  including  accelation,  light  and  gravity  sensors,  magnetometers  etc.  Moreover,  mobile  smart  devices  possess  high  computational  power,  storage  in  the  order  of  gigabytes,  whereas  high  battery  capacity  and  high  bandwidth  are  available.  The  biggest  advantage  of  the  wide  presence  of  mobile  smart  devices  is  that  all  this  distributed  computing  power  is  already  at  hands  of  people,  being  idle  for  the  most  time.  This  fact  presents  a  chance  of  utilizing  this  distributed  computational  infrastructure  with  the  goal  of  building  participatory  sensing  systems  with  various  applications  for  enviromental  support,  like  health  or  structure  monitoring.  In  this  paper  we  are  presenting  a  generic  distributed  framework  consisting  only  of  mobile  smart  devices  and  operating  only  in  the  network.  We  utilize  a  scalable,  fault-tolerant  communication  protocol,  that  performs  best-effort  time  synchronization  of  the  nodes  and  present  a  first  approach  in  an  example  application  of  distributed  structural  health  monitoring  (SHM).
2	Comparing  conventional  and  augmented  reality  instructions  for  manual  assembly  tasks.  Augmented  Reality  (AR)  gains  increased  attention  as  a  means  to  provide  assistance  for  different  human  activities.  Hereby  the  suitability  of  AR  does  not  only  depend  on  the  respective  task,  but  also  to  a  high  degree  on  the  respective  device.  In  a  standardized  assembly  task,  we  tested  AR-based  in-situ  assistance  against  conventional  pictorial  instructions  using  a  smartphone,  Microsoft  HoloLens  and  Epson  Moverio  BT-200  smart  glasses  as  well  as  paper-based  instructions.  Participants  solved  the  task  fastest  using  the  paper  instructions,  but  made  less  errors  with  AR  assistance  on  the  Microsoft  HoloLens  smart  glasses  than  with  any  other  system.  Methodically  we  propose  operational  definitions  of  time  segments  and  other  optimizations  for  standardized  benchmarking  of  AR  assembly  instructions.
2	A  game  system  for  remote  rehabilitation  of  cerebral  palsy  patients.  Cerebral  Palsy  (CP)  is  a  condition  that  affects  many  people  and  causes  paralysis  or  disability  due  to  brain  dysfunctions.  Though  incurable,  physical  rehabilitation  has  proven  to  improve  the  patients'  quality  of  life.  Some  of  them,  however,  may  need  extra  motivation  to  go  through  the  rehabilitation  process.  Thus,  to  engage  them  in  rehabilitative  actions,  video  games,  virtual  reality  games  and  robots  that  combine  rehabilitation  with  play  have  been  developed.  We  propose  a  game  system  targeted  for  rehabilitation  of  young  CP  patients  and  a  graphical  Event  Recognition  (ER)  Authoring  Tool  with  which  a  therapist  can  define  ER  rules  that  allow  monitoring  the  rehabilitation  progress  of  a  patient  and  adjusting  the  therapy  accordingly.  Our  aim  is  to  develop  a  remote  CP  rehabilitation  platform,  that  combines  rehabilitative  actions  with  play  and  employs  inexpensive,  off  the  shelf  technologies  that  can  be  used  with  minimal  supervision.  This  platform  is  targeted  both  at  young  CP  patients  and  also  therapists.
2	Using  swrl  and  ontological  reasoning  for  the  personalization  of  context  aware  assistive  services.  The  prevalence  and  advancements  of  existing  context-aware  applications  are  limited  in  their  support  of  personalization  for  the  user.  The  increase  in  the  use  of  context-aware  technologies  has  sparked  growth  in  assistive  applications  and  there  is  now  a  need  to  enable  the  adaptation  of  such  technologies  to  reflect  the  changes  in  user  behaviors.  This  paper  describes  the  conceptualization  and  development  of  a  personalization  mechanism  that  can  be  integrated  into  a  context-aware  application  for  the  purposes  of  providing  an  adaptable,  mobile-based  service  to  a  user.  We  highlight  the  use  of  an  ontological  User  Profile  Model  to  provide  a  detailed  representation  of  a  user  for  use  within  adaptive  applications.  Special  emphasis  is  placed  on  the  use  of  rule-based  reasoning  using  the  Semantic  Web  Rule  Language  (SWRL).  The  paper  details  how  these  rules  are  created  and  used  alongside  the  User  Profile  for  the  purposes  of  application  personalization.  We  present  a  case  study  to  illustrate  the  use  of  SWRL  within  the  User  Profile  Model.  Specifically,  the  case  study  focuses  on  providing  personalized  travel  assistance  to  older  users,  with  the  use  of  self-service  ticket  machines  via  an  `on-demand'  context-aware  smart-phone.
2	Identification  of  post  meditation  perceptual  states  using  wearable  eeg  and  self  calibrating  protocols.  Since  antiquity  meditation  in  its  various  forms  has  been  robustly  associated  with  multiple  physical  and  mental  measurable  changes  of  state.  Obstacles  to  widespread  access  to  the  advantageous  effects  of  meditation  include  the  practice  time  necessary  to  become  an  expert  meditator  and  the  identification  of  a  personalized  beneficial  meditative  state  once  achieved.  This  work  leverages  previously  published  and  patented  results  towards  the  application  of  Self-Calibrating  Protocol  (SCP)-assisted  wearable  EEG  measurement  of  personalized  "perceptual  prints"  and  recognition  of  pre-  versus  post-meditation  perceptual  states.  We  describe  avenues  towards  harvesting  the  newly  enabled  technologies  towards  quantification  of  meditation  benefits  and  replication  of  beneficial  meditative  states  leveraging  technology  to  shorten  practice  times.
2	Why  age  is  not  that  important  an  ageing  perspective  on  computer  anxiety.  We  analyze  the  influence  of  age  on  mobile  computer  anxiety  in  a  sample  of  158  individuals  55+  by  means  of  path  analysis  modeling.  Taking  as  the  endogenous  variable  a  mobile  computer  anxiety  scale  MCAS,  Wang  2007,  models  include  demographic  and  socioeconomic  variables  and  a  computer  experience  scale  ---  based  on  the  familiarity  and  frequency  of  use  of  different  information  and  communication  technologies.  Results  confirm  a  positive  influence  of  age  on  mobile  computer  anxiety  which  is  mediated  by  both  socio-economic  variables  and  computer  experience.  The  influence  of  age  on  mobile  computer  anxiety  is  comparatively  low.  Age  is  not  the  relevant  dimension  to  explain  computer  anxiety,  as  socio-economic  background  and  computer  experience  have  higher  explanatory  capacity.  This  result  may  explain  the  inconsistent  results  regarding  the  direct  relationship  between  age  and  computer  anxiety  available  in  the  literature.
2	Intergenerational  play  between  young  people  and  old  family  members  patterns  benefits  and  challenges.  The  purpose  of  this  study  was  to  explore  the  patterns,  benefits  and  challenges  of  intergenerational  play  between  young  people  and  their  old  family  members  (i.e.,  parents,  grandparents,  and  uncles/aunts).  A  total  of  308  responses  were  collected  from  young  people  aged  15+  using  an  online  survey.  The  results  showed  that  young  people  generally  did  not  frequently  play  digital  games  with  family  members;  however  when  they  did,  they  reported  that  intergenerational  play  was  a  fun  way  for  them  to  bond  with  family  members.  In  addition,  skill  gaps,  different  cohort  knowledge,  and  old  adults’  physical  and  cognitive  declines  were  the  main  factors  that  had  negative  impacts  on  smooth  intergenerational  game  play.  The  biggest  challenge  that  participants  faced  was  explaining  game  rules  and  mechanisms  using  words  understandable  to  old  family  members.
2	Sensitv  smart  emotional  system  for  impaired  people  s  tv.  In  this  paper,  an  innovative  solution  is  presented:  a  smart  emotional  system  for  impaired  people's  TV.  It  aims  to  accompany  the  cognitive  information  contained  in  a  movie,  with  the  affective  content.  The  affect  is  then  communicated  to  the  movie  viewers  in  ways  compatible  for  people  with  hearing  and/or  visual  impairments,  to  let  them  experience  all  of  the  sensations  offered  by  the  movie.  To  do  so,  emotion  recognition  techniques  are  used  to  classify  movie  scenes  into  seven  basic  emotions.  These  emotions  are  then  represented,  in  realtime,  while  the  movie  is  playing,  to  the  viewers,  using  environmental  lights,  emotional  subtitles  and  a  second  screen  application  that  integrates  vibrations,  emoticons  and  background  music.
2	How  people  multitask  while  watching  tv.  We  often  think  of  TV  watching  as  the  activity  where  people  are  fully  engaged  and  immersed  in  the  TV  program.  However,  research  has  shown  that  there  is  a  continuum  of  levels  of  attention  while  watching  TV.  We  set  out  to  understand  multitasking  behaviors  as  well  as  users'  motivation  and  intention  behind  simultaneous  tasks  performed  in  front  of  the  television.  We  conducted  an  in-home  qualitative  research  methods  study  inside  ten  households  across  the  San  Francisco  Bay  Area  and  used  a  quantitative  method  for  analysis  of  the  large  amount  of  behavioral  data  we  gathered.  We  recorded  participants'  television  watching  behaviors  using  cameras  that  were  placed  in  their  homes  and  used  retrospective  interviews  to  gather  purpose  behind  events  that  were  observed  in  the  video  recordings.  We  defined  eye  gaze  elsewhere  than  on  the  TV  as  accounting  for  a  multitasking  event.  It  was  found  that  multitasking  occurred  almost  40%  of  the  time  when  people  were  seated  in  front  of  the  television.  Most  multitasking  occurred  during  TV  programs  --  not  during  the  interval  between  TV  programs.  Of  the  time  people  spent  multitasking,  36%  was  spent  on  a  device,  mostly  a  smartphone.  However,  only  10%  of  device-related  multitasking  was  related  to  the  content  being  played  on  the  TV.  With  our  study,  we  contribute  to  the  greater  body  of  foundational  knowledge  around  common  multitasking  behaviors  that  are  conducted  in  front  of  the  television.
2	Investigating  a  design  space  for  multidevice  environments.  There  has  been  significant  research  interest  over  recent  years  in  the  use  of  public  digital  displays  and  in  particular  their  capability  to  offer  both  interactivity  and  personalized  content.  Although  a  number  of  interaction  technologies  have  been  investigated,  a  promising  approach  has  been  the  use  of  the  ubiquitous  cell  phone,  which  not  only  offers  a  means  to  interact  with  displays  but  increasingly  offers  a  small  but  high-quality  screen  to  complement  the  larger  public  display.  However,  to  date  there  has  been  little  investigation  into  the  impact  on  users  when  interfaces  are  distributed  across  this  type  of  dual  screen  setup.  This  article  reports  on  a  series  of  experiments  carried  out  to  determine  if  there  is  a  significant  quantitative  or  qualitative  effect  on  user  performance  when  interaction  is  split  across  large  public  and  smaller  private  screens.
2	Theory  practice  and  policy  an  inquiry  into  the  uptake  of  hci  practices  in  the  software  industry  of  a  developing  country.  ABSTRACTWith  almost  four  decades  of  existence  as  a  community,  human–computer  interaction  (HCI)  practice  has  yet  to  diffuse  into  a  large  range  of  software  industries  globally.  A  review  of  existing  literature  suggests  that  the  diffusion  of  HCI  practices  in  software  organizations  lacks  theoretical  guidance.  Although  many  studies  have  tried  to  facilitate  HCI  uptake  by  the  software  industry,  there  are  scarce  studies  that  consider  HCI  practices  as  innovations  that  software  organizations  could  or  should  adopt.  Furthermore,  there  appears  to  be  a  lack  of  structure  in  the  facilitation  of  HCI  methodological  development  within  the  specialized  emerging  regions  field  such  as  Sub-Saharan  Africa.  In  order  to  address  this  gap,  an  exploratory  investigation  regarding  the  state  of  uptake  of  HCI  practices  in  Nigeria  is  conducted.  The  aim  of  this  article  is  to  improve  our  understanding  regarding  the  state  of  HCI  uptake  in  developing  countries  and  the  challenges  prevailing.  The  findings  show  that  HCI  practice  still  remains  with...
2	Perceived  time  as  a  measure  of  mental  workload  effects  of  time  constraints  and  task  success.  The  mental  workload  imposed  by  systems  is  important  to  their  operation  and  usability.  Consequently,  researchers  and  practitioners  need  reliable,  valid,  and  easy-to-administer  methods  for  measuring  mental  workload.  The  ratio  of  perceived  time  to  clock  time  appears  to  be  such  a  method,  yet  mental  workload  has  multiple  dimensions  of  which  the  perceived  time  ratio  has  mainly  been  linked  to  the  task-related  dimension.  This  study  investigates  how  the  perceived  time  ratio  is  affected  by  time  constraints,  which  make  time  an  explicit  concern  in  the  execution  of  tasks,  and  task  success,  which  is  a  performance-related  rather  than  task-related  dimension  of  mental  workload.  A  higher  perceived  time  ratio  is  found  for  timed  than  untimed  tasks.  According  to  subjective  workload  ratings  and  pupil-diameter  measurements,  the  timed  tasks  impose  higher  mental  workload.  This  finding  contradicts  the  prospective  paradigm,  which  asserts  that  perceived  time  decreases  with  increasing  mental  workload.  A  higher  perceived  time  ratio  wa...
2	Identifying  usability  issues  for  personalization  during  formative  evaluations  a  comparison  of  three  methods.  A  personalized  system  is  one  that  generates  unique  output  for  each  individual.  As  a  result,  personalization  has  transformed  the  interaction  between  the  user  and  the  system,  and  specific  new  usability  issues  have  arisen.  Methods  used  for  evaluating  personalized  systems  should  be  able  to  reveal  the  issues  and  problems  specifically  associated  with  personalization.  Therefore  this  study  evaluated  three  of  the  most  common  test  methods  used  to  detect  usability  problems  in  a  personalized  search  engine.  This  was  done  by  comparing  the  comments  generated  from  thinking-aloud,  questionnaires,  and  interviews.  Questionnaires  and  interviews  appear  to  be  more  useful  for  assessing  specific  usability  issues  for  personalization,  whereas  thinking-aloud  generates  more  comments  on  the  usefulness  of  the  system  in  the  intended  context  of  use  and  identifies  the  most  critical  and  serious  problems.  Interviews,  on  the  other  hand,  appear  to  yield  a  disproportionate  number  of  positive  comments.  During  the  formative  evaluation  of  a  personalized  system  it  is  best  to  use  a  combination  of  thinking-aloud  and  questionnaires.  This  article  concludes  with  a  summary  of  implications  for  practitioners.
2	A  human  cognitive  perspective  of  users  password  choices  in  recognition  based  graphical  authentication.  ABSTRACTGraphical  password  composition  is  an  important  part  of  graphical  user  authentication  which  affects  the  strength  of  the  chosen  password.  Considering  that  graphical  authentication  is  associat...
2	Sexist  ai  an  experiment  integrating  casa  and  elm.  This  study  employed  an  experiment  to  test  participants’  perceptions  of  an  artificial  intelligence  (AI)  recruiter.  It  used  a  2  (Specialist  AI/Generalist  AI)  ×  2  (Sexist/nonsexist)  design  to  test  the...
2	Hedonic  and  utilitarian  motivations  for  physical  game  systems  use  behavior.  With  the  recent  proliferation  of  physical  game  systems  in  entertainment  contexts,  gaining  a  better  understanding  of  why  users  are  willing  to  utilize  physical  game  systems  has  become  an  important  topic  for  practitioners  and  academics.  The  current  study  attempts  to  explore  the  determinants  of  behavioral  intention  pertaining  to  the  use  of  physical  tennis  games  from  the  perspective  of  both  hedonic  and  utilitarian  motivations.  A  research  model  is  proposed  based  on  the  existing  literature.  Data  collected  from  124  experienced  players  of  physical  tennis  games  are  tested  against  the  research  model  using  the  partial  least  squares  approach.  The  results  indicate  that  both  perceived  exercise  utility  and  perceived  enjoyment  have  a  significant  influence  on  behavioral  intention.  In  addition,  perceived  motion-sensing,  challenge,  interactivity,  ease  of  use,  and  design  aesthetics  are  found  to  have  a  significant  effect  on  perceived  enjoyment.  More  specifically,  perceived  motion-sensing  is  observed  to  be  the  antecedent  of  per...
2	Adapting  the  navigation  interface  of  smart  watches  to  user  movements.  ABSTRACTThe  high  mobility  of  smart  watches  can  easily  impair  interaction  performance,  and  many  applications  are  squeezed  into  an  extremely  tiny  screen,  which  causes  disorientations.  Therefore,  this  study  examines  the  extent  of  performance  impairment  caused  by  user  movements  and  proposes  navigation  aids  to  alleviate  the  impairment.  An  experiment  was  conducted  among  28  college  students  to  investigate  the  influence  of  user  movements  and  navigation  aids  on  users’  performance  and  subjective  feedback.  The  results  indicate  that  the  performance  of  using  smart  watches  in  walking  conditions  is  comparable  to  that  in  sitting  conditions.  However,  the  use  of  smart  watches  while  running  reduces  the  success  rates  of  operating,  perceived  ease  of  use,  perceived  usefulness,  and  flow  experience,  and  it  increases  subjective  cognitive  workload.  To  improve  user  experience,  the  effectiveness  of  providing  navigation  aids  for  smart  watches  is  confirmed.  Using  static  navigation  aids  while  sitting  and  walking  and  using  animated  navi...
2	Adaptive  social  robot  for  sustaining  social  engagement  during  long  term  children  robot  interaction.  ABSTRACTOne  of  the  known  challenges  in  Children–Robot  Interaction  (cHRI)  is  to  sustain  children’s  engagement  during  long-term  interactions  with  robots.  Researchers  have  hypothesized  that  robots  that  can  adapt  to  children’s  affective  states  and  can  also  learn  from  the  environment  can  result  in  sustaining  engagement  during  cHRI.  Recently,  researchers  have  conducted  a  range  of  studies  where  robots  portray  different  social  capabilities  and  have  shown  that  it  has  positively  influenced  children’s  engagement.  However,  despite  an  immense  body  of  research  on  implementation  of  different  adaptive  social  robots,  a  pivotal  question  remains  unanswered:  Which  adaptations  portrayed  by  a  robot  can  result  in  maintaining  long-term  social  engagement  during  cHRI?  In  other  words,  what  are  the  appropriate  and  effective  adaptations  portrayed  by  a  robot  that  will  sustain  social  engagement  for  an  extended  number  of  interactions?  In  this  article,  we  report  on  a  study  conducted  with  three  groups  of  children  who  played  a  snakes  and  l...
2	Gender  differences  in  motivations  for  identity  reconstruction  on  social  network  sites.  ABSTRACTSocial  network  sites  provide  people  a  unique  opportunity  for  self-presentation.  Due  to  various  reasons,  people  may  build  an  online  identity  that  is  partly  or  even  completely  different  from  their  identity  in  the  real  world.  Adopting  social  role  theory  as  the  theoretical  foundation,  the  current  study  investigated  gender  differences  in  the  motivations  for  virtual  identity  reconstruction  on  QQ,  a  social  network  site  based  in  China.  A  total  of  418  respondents  participated  in  the  study.  As  hypothesized,  the  results  showed  that  men  and  women  are  motivated  differently  when  reconstructing  their  identity  –  while  women  focus  more  on  physical  vanity,  men  emphasize  achievement  vanity.  The  authors  also  identified  gender  differences  in  other  motivations  for  online  identity  reconstruction:  bridging  social  capital,  disinhibition,  and  privacy  concerns.  The  results,  which  suggest  that  men  and  women  behave  in  gender-specific  ways,  are  in  line  with  the  propositions  made  in  social  role  theory.
2	Gender  and  personality  trait  measures  impact  degree  of  affect  change  in  a  hedonic  computing  paradigm.  To  date,  affective  computing  research  has  acknowledged  individual  differences  with  regard  to  detecting  affect,  yet  little  research  has  explored  how  these  individual  differences  may  determine  the  degree  to  which  affective  computing  is  successful  in  manipulating  the  affect  of  specific  computer  users.  The  current  study  used  individual  difference  measures  to  predict  how  much  an  individual  can  be  influenced  by  a  hedonic  computing  paradigm:  a  simple  trivia  game.  Female  participants  responded  in  a  greater  way  to  positive  affective  feedback  about  their  performance  than  did  men.  Moreover,  several  personality  traits,  including  neuroticism,  narcissism,  self-esteem,  and  extraversion,  augmented  the  degree  to  which  affect  changed  as  a  result  of  playing  the  game.  The  results  are  consistent  with  the  gender  differences  hypothesis,  and  the  authors  conclude  that  individual  differences,  particularly  gender  and  personality  traits,  play  a  large  role  in  the  potential  impact  of  computing  platforms  and  would  be  useful  in  personal...
2	Emotional  satisfaction  and  is  continuance  behavior  reshaping  the  expectation  confirmation  model.  This  research  develops  and  tests  an  extended  Expectation-Confirmation  Model  (ECM)  framework  to  investigate  IT  continuance  behavior  for  the  workplace  and  personal  use.  After  collecting  empirical  dat...
2	Takeover  transition  in  autonomous  vehicles  a  youtube  study.  Automated  driving  has  many  potential  benefits,  such  as  improving  driving  safety  and  reducing  drivers’  workload.  However,  from  a  human  factors’  perspective,  one  concern  is  that  drivers  become  increa...
2	Measuring  attitudes  towards  the  internet  the  general  internet  attitude  scale.  The  General  Internet  Attitude  Scale  (GIAS)  is  a  questionnaire  designed  to  explore  the  underlying  components  of  the  attitudes  of  individuals  to  the  Internet,  and  to  measure  individuals  on  these  attitude  components.  Previous  Internet  attitude  research  is  critiqued  for  its  lack  of  a  clear  definition  of  constructs.  GIAS  was  developed  starting  from  the  well-established  three-component  psychological  model  of  attitude  (affect,  behavior,  cognition)  into  which  applicable  statements  found  in  previous  Internet  attitude  measures  were  fitted.  GIAS  was  developed  using  an  iterative  psychometric  process  with  four  independent  samples  (N  =  2,200).  During  iterations,  the  wordings  of  the  items  were  refined,  and  exploratory  and  confirmatory  factor  analyses  identified  four  underlying  factors  in  the  scale:  Internet  Affect,  Internet  Exhilaration,  Social  Benefit  of  the  Internet,  and  Internet  Detriment,  all  of  which  had  acceptable  internal  reliabilities.  The  final  instrument  contains  21  items  and  demonstrates  strong  reliability  ac...
2	Seniors  and  information  technology  in  china.  Technology,  particularly  information  technology  (IT),  is  changing  rapidly  and  offers  many  advantages  for  users.  IT  has  the  potential  to  improve  one’s  quality  of  life,  but  only  if  used.  Research  has  shown  that,  in  most  developed  and  developing  nations,  senior  use  of  IT  lags  behind  that  of  other  segments  of  society.  This  “gray  divide”  is  a  concern  for  government,  business,  and  others.  Concern  about  senior  adoption  and  use  of  IT  has  been  the  motivation  for  numerous  studies  and  is  the  motivation  for  this  study  of  Chinese  seniors.  This  exploratory  study  examines  urban  Chinese  seniors’  IT  use,  computers  and  mobile  phones  in  particular,  but  also  their  attitudes  and  behaviors  as  they  relate  to  the  use  of  those  devices.  More  specifically  it  examines  several  potential  factors  that  motivate  or  hinder  senior’s  use  of  information  technology,  including  self-efficacy,  satisfaction,  comfort  with  technology,  and  both  positive  and  negative  attitudes.  This  study  provides  some  insight  into  the  quantity  and  diversity  of  Chine...
2	Optimal  view  angles  in  three  dimensional  objects  constructed  from  plane  figures  as  mental  images.  In  human–computer  interfaces,  such  as  computer-aided  design  and  clinical  assessment,  there  are  various  ways  to  create  a  mental  image  of  a  three-dimensional  (3D)  structure  from  plane  figures.  To  evaluate  the  specific  cognitive  activities  that  take  place  during  the  recognition  of  a  3D  structure,  the  act  of  mentally  framing  3D  images  from  plane  figures  and  the  ability  to  match  mental  images  to  actual  objects  were  investigated.  The  results  reveal  that  the  effects  of  depth  cues  such  as  shade  and  color  in  the  3D  images  significantly  facilitated  the  cognitive  process  in  human–computer  interfaces.  In  addition,  optimal  presentation  angles  for  the  matching  phase  exist;  these  findings  suggest  effective  methods  in  a  field  requiring  quick  judgment  based  on  high  cognitive  functions.
2	Exploring  patients  use  intention  of  personal  health  record  systems  implications  for  design.  AbstractPersonal  health  record  (PHR)  systems  offer  a  technology  for  personal  health  information  management  (PHIM)  activities.  Despite  efforts  to  increase  the  use  of  PHR  systems  as  a  mechanism  to  support  better  patient-centered  care  and  improve  information  management  across  the  continuum  of  care,  PHR  adoption  remains  low.  The  purpose  of  this  study  was  to  explore  how  to  design  a  PHR  system  that  can  adequately  support  personal  health  information  management  activities.  Using  a  mixed-methods  approach  (questionnaires  and  interviews),  we  identified  the  factors  affecting  a  person’s  intention  to  use  PHRs  and  also  described  the  personal  health  information  management  activities  among  people  from  a  wide  age  range  in  the  United  States.  Results  indicated  that  the  intention  to  use  PHR  systems  was  affected  by  system-related  factors,  such  as  perceived  usefulness,  health  information  understandability,  personalization,  and  patient–clinician  communication  support,  and  user-related  factors,  such  as  social  influence,  self-effi...
2	Understanding  and  improving  cross  cultural  decision  making  in  design  and  use  of  digital  media  a  research  agenda.  In  the  global  economy,  design  of  digital  media  often  involves  teams  of  individuals  from  a  variety  of  cultures  who  must  function  together.  Similarly,  products  must  be  designed  and  marketed  taking  specific  cultural  characteristics  into  account.  Much  is  known  about  decision  processes,  culture  and  cognition,  design  of  products  and  interfaces  for  human  interaction  with  machines,  and  organizational  processes,  but  this  knowledge  is  dispersed  across  several  disciplines  and  research  areas.  This  article  reviews  current  work  in  these  areas  and  proposes  a  research  agenda  for  fostering  increased  understanding  of  the  ways  in  which  cultural  differences  influence  decision  making  and  action  in  design  and  use  of  digital  media.
2	Older  adults  and  web  2  0  storytelling  technologies  probing  the  technology  acceptance  model  through  an  age  related  perspective.  Although  digital  storytelling  bears  significant  benefits  for  older  users,  much  remains  to  be  explored  regarding  their  psychosocial  attributes  that  could  affect  technology  acceptance.  The  Technology...
2	Interactive  visual  simulation  of  dynamic  ink  diffusion  effects.  This  paper  presents  an  effective  method  that  simulates  the  ink  diffusion  process  with  visual  plausible  effects  and  real-time  performance.  Our  algorithm  updates  the  dynamic  ink  volume  with  a  hybrid  grid-particle  representation:  the  fluid  velocity  field  is  calculated  with  a  low-resolution  grid  structure,  while  the  highly  detailed  ink  effects  are  controlled  and  visualized  with  the  particles.  We  propose  an  improved  ink  rendering  method  with  particle  sprites  and  motion  blur  techniques.  The  simulation  and  the  rendering  processes  are  efficiently  implemented  on  graphics  hardware  for  interactive  frame  rates.  Compared  to  traditional  simulation  methods  that  treat  water  and  ink  as  two  mixable  fluids,  our  method  is  simple  but  effective:  it  captures  various  ink  effects  such  as  pinned  boundary  [Chu  and  Tai  2005]  and  filament  pattern  [Shiny  et  al.  2010]  with  real-time  performance;  it  allows  easy  interaction  with  the  artists;  it  includes  basic  solid-fluid  interaction.  We  believe  that  our  method  is  attractive  for  industrial  animation  and  art  design.
2	Graph  cut  based  interactive  image  segmentation  with  texture  constraints.  In  the  paper,  we  present  a  method  of  interactive  image  segmentation  with  texture  constraints  in  the  framework  of  graph  cut.  Given  an  image,  we  first  gather  user-marked  information  to  establish  the  color  and  texture  prior  models  of  the  foreground  and  background.  Then,  we  formulate  an  energy  function  composed  of  color,  gradient  and  texture  terms.  At  last,  the  foreground  is  extracted  by  minimizing  the  energy  function  using  graph  cut.  In  the  energy  function,  the  texture  term  describes  the  difference  between  the  texture  prior  models  and  the  texture  descriptors  of  each  pixel  to  be  labeled.  The  foreground/background  texture  prior  model  is  represented  as  histograms  of  Local  Binary  Patterns  (LBP).  Every  pixel  to  be  labeled  in  the  image  has  a  foreground  and  a  background  texture  descriptor,  which  are  obtained  by  a  randomized  texton-searching  algorithm.  The  newly  added  texture  term  is  effective  to  overcome  the  difficulty  in  locating  real  boundaries  when  dealing  with  textured  foreground/background.  Experimental  results  demonstrate  that,  with  the  same  amount  of  user  interaction,  our  method  generates  better  results  than  traditional  ones.
2	The  art  of  metaphor  a  method  for  interface  design  based  on  mental  models.  People  tend  to  form  an  internal  model  of  systems  and  how  they  work.  An  accurate  mental  model  helps  promote  user  experience  and  performance.  As  a  result,  interface  designs  that  are  based  on  a  users  mental  model  have  been  discussed  frequently.  However,  the  concept  of  a  mental  model  has  varying  interpretations  and  is  considered  difficult  to  operationalize.  Against  this  background,  we  propose  a  framework  that  demonstrates  the  structure  of  a  mental  model  and  the  role  it  plays  in  human-computer  interactions  (HCI).  Accordingly,  a  method  is  proposd  for  interface  design.  This  method  is  supported  by  recent  cognitive  psychology  theory  and  is  practical  for  engineering  projects.
2	Line  based  single  view  3d  reconstruction  in  manhattan  world  for  augmented  reality.  Single  view  reconstruction  is  a  fundamental  issue  in  computer  vision.  Manhattan  world  assumption  is  often  used  for  reconstructing  man-made  environments,  which  assumes  that  planes  in  the  real  world  are  in  the  mutually  orthogonal  directions.  This  paper  deals  with  an  automatic  single  view  reconstruction  algorithm  of  Manhattan  worlds.  A  novel  method  for  estimating  directions  of  planes  using  graph  cut  optimization  is  proposed.  After  segmenting  an  image  into  planes  from  extracted  line  segments,  data  cost  function  and  smoothness  cost  function  for  graph  cut  optimization  is  defined  with  the  consideration  of  directions  of  line  segments  and  neighborhood  segments.  Further,  segments  having  same  depth  are  grouped  during  depth  estimation  step  using  minimum  spanning  tree  algorithm  with  proposed  weights.  Experimental  result  shows  that  the  proposed  method  is  not  only  able  to  identify  complex  Manhattan  structures  of  indoor  and  outdoor  scene  but  also  provide  exact  boundaries  and  intersections  of  planes  unlike  earlier  literature.
2	Using  eeg  data  analytics  to  measure  meditation.  This  paper  presents  the  study  we  have  done  to  detect  “meditation”  brain  state  by  analyzing  electroencephalographic  (EEG)  data.  We  firstly  discuss  what  is  “meditation”  state  and  some  prior  studies  on  meditation.  We  then  discuss  how  meditation  state  can  be  reflected  in  the  subject’s  brain  waves;  and  what  features  of  the  brain  waves  data  can  be  used  in  machine  learning  algorithms  to  classify  meditation  state  from  other  states.  We  studied  the  suitability  of  3  types  of  entropy:  Shannon  entropy,  approximate  entropy,  and  sample  entropy  in  different  circumstances.  We  found  that  overall  Sample  entropy  is  a  good  tool  to  extract  information  from  EEG  data.  Discretization  of  EEG  data  enhances  the  classification  rates  by  using  both  the  approximate  entropy  and  Shannon  entropy.
2	Feedback  based  self  training  system  of  patient  transfer.  In  this  paper,  we  propose  a  self-training  system  to  assist  nursing  students  to  learn  nursing  skills.  The  system  focuses  on  the  task  of  transferring  a  patient  from  a  bed  to  a  wheelchair.  In  the  system,  two  Kinect  sensors  were  applied  to  measure  the  posture  of  the  trainees  and  patients  and  an  automatic  evaluation  method  was  used  to  classify  the  trainees'  performance  in  each  skill  as  correct  or  incorrect.  A  feedback  interface  based  on  a  checklist  was  designed  to  help  the  trainees  check  whether  they  performed  correctly.  The  system  is  designed  for  the  trainees  to  operate  by  themselves.  A  control  test  was  performed  to  measure  the  training  effects  of  the  system.  The  results  show  that  the  growth  rate  of  the  group  that  trained  with  feedback  (79%)  was  higher  than  the  group  that  trained  without  feedback  (48%).
2	Ecg  identification  based  on  pca  rprop.  With  the  quick  development  of  information  technology,  people  pay  more  and  more  attention  to  information  security  and  property  safety,  where  identity  is  one  of  the  most  important  aspects  of  information  security.  Compared  with  the  traditional  means  of  identification,  biometrics  recognition  technology  offers  greater  security  and  convenience.  Among  which,  electrocardiogram  (ECG)  human  identification  has  been  attracted  great  attention  in  recent  years.  As  a  new  type  of  biometric  feature  authentication  technology,  the  feature  selection  and  classification  of  ECG  has  become  a  focus  of  the  research  community.  However,  there  exist  some  problems  that  can  impair  the  efficiency  and  accuracy  of  ECG  identification,  including  information  redundancy  and  high  dimensionality  in  feature  extraction,  and  insufficient  stability  in  classification.  In  order  to  solve  the  problems,  in  this  paper,  we  propose  a  recognition  method  based  on  PCA-RPROP.  In  this  method,  firstly,  only  R  points  are  located  to  get  the  original  single-cycle  waveforms.  Then,  PCA  and  whitening  are  used  to  process  original  data,  where  whitening  is  to  make  the  input  less  redundant  and  PCA  is  to  reduce  its  dimensionality.  Finally,  the  resilient  propagation  (RPROP)  algorithm  is  used  to  optimize  the  neural  network  and  establish  a  complete  recognition  model.  In  order  to  evaluate  the  effectiveness  of  the  algorithm,  we  compared  the  PCA  feature  with  the  wavelet  decomposition  and  multi-point  localization  features  in  an  ECG-ID  database,  and  also  compared  RPROP  with  traditional  BP  algorithm,  SVM  and  KNN.  The  experimental  results  show  that  this  method  can  improve  the  performance  compared  with  other  classifiers,  and  simultaneously  reduce  the  complexity  of  localization  and  the  redundancy  of  features.  It  is  superior  to  the  other  methods  both  speed  and  accuracy  in  recognition,  especially  when  compared  with  the  traditional  BP.  It  can  solve  the  problems  of  traditional  BP  with  2.4%  higher  recognition  accuracy  than  LIBSVM,  and  14  s  faster  than  KNN  in  terms  of  time  efficiency.  Therefore,  it  is  an  efficient,  simple  and  practical  recognition  algorithm.
2	Fast  collision  culling  in  large  scale  environments  using  gpu  mapping  function.  This  paper  presents  a  novel  and  efficient  GPU-based  parallel  algorithm  to  cull  non-colliding  object  pairs  in  very  large-scale  dynamic  simulations.  It  allows  to  cull  objects  in  less  than  25ms  with  more  than  100K  objects.  It  is  designed  for  many-core  GPU  and  fully  exploits  multi-threaded  capabilities  and  data-parallelism.  In  order  to  take  advantage  of  the  high  number  of  cores,  a  new  mapping  function  is  defined  that  enables  GPU  threads  to  determine  the  objects  pair  to  compute  without  any  global  memory  access.  These  new  optimized  GPU  kernel  functions  use  the  thread  indexes  and  turn  them  into  a  unique  pair  of  objects  to  test.  A  square  root  approximation  technique  is  used  based  on  Newton's  estimation,  enabling  the  threads  to  only  perform  a  few  atomic  operations.  A  first  characterization  of  the  approximation  errors  is  presented,  enabling  the  fixing  of  incorrect  computations.  The  I/O  GPU  streams  are  optimized  using  binary  masks.  The  implementation  and  evaluation  is  made  on  largescale  dynamic  rigid  body  simulations.  The  increase  in  speed  is  highlighted  over  other  recently  proposed  CPU  and  GPU-based  techniques.  The  comparison  shows  that  our  system  is,  in  most  cases,  faster  than  previous  approaches.
2	Contact  analog  warnings  on  windshield  displays  promote  monitoring  the  road  scene.  Drivers  attend  to  a  lot  of  information  at  various  locations  inside  and  outside  the  car  as  well  as  on  external  devices  (e.g.  smart  phones).  Head-Up  Displays  (HUDs)  support  keeping  drivers'  visual  focus  directed  towards  the  street;  as  they  present  virtual  information  in  the  windshield  area  on  top  of  the  physical  world  within  the  field  of  view  of  the  driver.  Displayed  information,  however,  is  often  spatially  dissociated  with  its  cause  in  the  physical  world:  for  example  a  warning  is  displayed,  yet  drivers  still  require  time  searching  for  the  hazard  causing  it.  Windshield  displays  (WSDs)  allow  virtual  warnings  being  displayed  at  the  position  of  the  hazard.  We  compared  HUD  and  WSD  with  the  baseline  no-display  and  found  that  drivers  demonstrate  a  calm  gaze  behavior  with  WSDs;  they  keep  their  visual  attention  in  average  1.5  s  longer  focused  on  the  leading  car.  However,  we  also  found  no  significant  faster  reaction  time  compared  to  HUDs.  We  discuss  our  findings  comparing  HUDs  to  WSDs,  present  potential  limitations  of  our  study  and  point  out  future  steps  in  order  to  further  investigate  the  advantages  of  WSDs.
2	Critical  analysis  on  the  nhtsa  acceptance  criteria  for  in  vehicle  electronic  devices.  We  tested  a  commercial  in-car  navigation  system  prototype  against  the  NHTSA  criteria  for  acceptance  testing  of  in-vehicle  electronic  devices,  in  order  to  see  what  types  of  in-car  tasks  fail  the  acceptance  test  and  why.  In  addition,  we  studied  the  visual  demands  of  the  driving  scenario  recommended  by  NHTSA  for  task  acceptance  testing.  In  the  light  of  the  results,  NHTSA  guidelines  and  acceptance  criteria  need  to  be  further  developed.  In  particular  visual  demands  of  the  driving  scenario  and  for  different  simulators  need  to  be  standardized  in  order  to  enable  fair  testing  and  comparable  test  results.  We  suggest  the  visual  occlusion  method  for  finding  a  driving  scenario  that  corresponds  better  with  real-life  driving  in  visual  demands  as  well  as  for  standardizing  the  visual  demands  of  the  scenario  when  applied  to  different  driving  simulators.  Furthermore,  the  acceptance  criteria  need  to  be  re-evaluated.  Especially  the  TEORT  limit's  applicability  to  a  variety  of  test  tasks  needs  to  be  validated  and  exceptions  for  certain  task  types  considered.  The  utility  of  the  average  glance  duration  criterion  should  be  reconsidered.
2	Relationship  between  drivers  self  reported  health  and  technology  perceptions  across  the  lifespan.  This  study  examined  relationships  between  individual  user  characteristics  and  perceptions  about  technology  --  experience,  adoption,  ability  to  learn,  and  trust.  Based  on  responses  from  a  diverse  sample  of  610  individuals  with  driving  experience,  it  was  found  that  perceived  health  and  well-being  were  strongly  associated  with  experience  and  perceptions  around  technology,  including  attitudes  toward  established  and  new  vehicle  technologies.  A  comparison  of  results  from  a  correlation  analysis  showed  the  effects  of  perceived  health  and  well-being  to  be  stronger  and  more  significant  compared  to  demographic  characteristics  and  medical  conditions.  The  findings  suggest  a  need  for  a  better  understanding  of  user  characteristics,  rather  than  relying  on  observable  traits.
2	Capture  the  car  qualitative  in  situ  methods  to  grasp  the  automotive  context.  In  terms  of  human  computer  interaction  (HCI),  the  car  interior  is  a  space,  which  can  be  divided  into  three  areas:  the  driver's  area,  the  front  seat  area,  and  the  back  seat  area.  So  far  HCI  researchers  have  primary  focused  on  the  driver,  and  how  in-car  electronic  devices  can  be  designed  to  assist  the  driver  in  order  to  increase  safety  and  comfort.  We  propose  that  for  investigating  interactive  technology  in  the  car  in  a  holistic  way,  all  three  areas  have  to  be  taken  into  account.  For  that  purpose  we  argue  for  an  increased  usage  of  qualitative  in-situ  studies,  which  have  been  hardly  applied  in  automotive  user  interface  research.  So  far  the  HCI  community  has  mainly  focused  on  laboratory  studies  utilizing  driving  simulators.  Despite  the  broad  range  of  available  field  study  methods,  such  as  ethnographic  and  self-reporting  studies,  the  adaption  of  these  methods  for  the  automotive  context  is  challenging  due  to  the  specific  characteristics  of  this  environment.  For  instance,  cars  provide  only  very  limited  space,  the  environment  is  constantly  changing  while  driving  and  the  driver  must  not  be  distracted  from  driving  safely.  As  a  consequence,  a  lack  of  experience  exists,  on  how  in-situ  methods  should  be  applied  to  cars.  In  this  paper  we  describe  three  qualitative  in-situ  studies,  we  conducted  to  research  the  driver,  the  front  seat  passenger,  and  the  rear  seat  passenger  spaces.  All  three  studies  used  a  different  method  tailored  to  fit  these  three  areas  best.  To  share  our  experiences  and  insights  we  discuss  the  strengths  and  pitfalls  of  each  method.
2	A  left  turn  driving  aid  using  projected  oncoming  vehicle  paths  with  augmented  reality.  Making  left  turns  across  oncoming  traffic  without  a  protected  left-turn  signal  is  a  significant  safety  concern  at  intersections.  In  a  left  turn  situation,  the  driver  typically  does  not  have  the  right  of  way  and  must  determine  when  to  initiate  the  turn  maneuver  safely.  It  has  been  reported  that  a  driver's  inability  to  correctly  judge  the  velocity  and  time  gap  of  the  oncoming  vehicles  is  a  major  cause  of  left  turn  crashes.  Although  the  position  and  velocity  of  surrounding  vehicles  is  available  using  camera  and  laser  based  vehicle  detection  and  tracking,  methods  on  how  to  effectively  communicate  such  information  to  help  the  driver  have  been  relatively  under-explored.  In  this  paper,  we  describe  a  left  turn  aid  that  displays  a  3  second  projected  path  of  the  oncoming  vehicle  in  the  driver's  environment  with  a  3D  Head-Up  Display  (3D-HUD).  Utilizing  the  abilities  of  our  3D-HUD  to  show  the  projected  path  in  Augmented  Reality  (AR)  could  help  increase  driver  intuition  and  alleviate  visual  distraction  as  compared  to  other  possible  non-AR  solutions.  Through  an  iterative  process  utilizing  early  user  feedback,  the  design  of  the  left  turn  aid  was  refined  to  interfere  less  with  the  driver  view  and  be  more  effective.  A  pilot  study  has  been  designed  for  a  driving  simulation  environment  and  can  be  used  to  evaluate  the  potential  of  the  proposed  AR  left  turn  aid  in  helping  the  driver  be  more  cautious  or  efficient  when  turning  left.
2	Estimation  of  drivers  emotional  states  based  on  neuroergonmic  equipment  an  exploratory  study  using  fnirs.  Research  has  shown  the  negative  effects  of  emotions  on  driving  performance  and  safety,  whereas  a  small  number  of  neuroimaging  studies  has  been  conducted  to  investigate  a  driver's  brain  activities  while  driving  with  emotions.  This  study  aims  to  explore  angry  drivers'  oxygen  concentration  level  using  functional  Near  Infrared  Spectroscopy  (fNIRS).  To  this  end,  ten  participants  drove  with  induced  anger  in  a  scenario  where  they  encountered  different  hazards.  Both  cluster  analysis  and  factor  analysis  showed  that  we  can  distinguish  their  anger  state  from  the  neutral  state  based  on  fNIRS  data.  Results  are  discussed  with  the  application  of  real-time  emotion  mitigation  system  for  drivers.
2	Ambient  light  and  its  influence  on  driving  experience.  In  modern  traffic,  measures  are  implemented  to  regulate  speeding,  which  may  annoy  drivers  who  pursue  an  exciting  driving  experience  and  make  them  exceed  speed  limits.  Others  prefer  a  more  relaxing  experience  resulting  in  socially  desired  driving  behavior.  This  paper  presents  a  study  investigating  the  capacity  of  ambient  light  to  alter  the  perception  of  speed  and  therefore  influence  the  driving  experience.  The  aim  of  this  study  was  to  determine  how  different  drivers  experience  the  concept  of  an  ambient  light  moving  along  the  a-pillar  inside  the  vehicle.  In  different  conditions,  the  light  moved  at  different  speeds.  The  outcomes  of  the  study  show  that  overall  the  ambient  light  used  in  this  study  had  a  positive  effect  on  the  driving  experience  but  that  the  attitude  towards  the  ambient  light  was  highly  individual.  The  majority  indicated  a  preference  towards  the  ambient  light  while  some  saw  it  more  as  a  distraction  or  even  inducing  more  stress.
2	Multimodal  interaction  in  the  car  combining  speech  and  gestures  on  the  steering  wheel.  Implementing  controls  in  the  car  becomes  a  major  challenge:  The  use  of  simple  physical  buttons  does  not  scale  to  the  increased  number  of  assistive,  comfort,  and  infotainment  functions.  Current  solutions  include  hierarchical  menus  and  multi-functional  control  devices,  which  increase  complexity  and  visual  demand.  Another  option  is  speech  control,  which  is  not  widely  accepted,  as  it  does  not  support  visibility  of  actions,  fine-grained  feedback,  and  easy  undo  of  actions.  Our  approach  combines  speech  and  gestures.  By  using  speech  for  identification  of  functions,  we  exploit  the  visibility  of  objects  in  the  car  (e.g.,  mirror)  and  simple  access  to  a  wide  range  of  functions  equaling  a  very  broad  menu.  Using  gestures  for  manipulation  (e.g.,  left/right),  we  provide  fine-grained  control  with  immediate  feedback  and  easy  undo  of  actions.  In  a  user-centered  process,  we  determined  a  set  of  user-defined  gestures  as  well  as  common  voice  commands.  For  a  prototype,  we  linked  this  to  a  car  interior  and  driving  simulator.  In  a  study  with  16  participants,  we  explored  the  impact  of  this  form  of  multimodal  interaction  on  the  driving  performance  against  a  baseline  using  physical  buttons.  The  results  indicate  that  the  use  of  speech  and  gesture  is  slower  than  using  buttons  but  results  in  a  similar  driving  performance.  Users  comment  in  a  DALI  questionnaire  that  the  visual  demand  is  lower  when  using  speech  and  gestures.
2	Drowsiness  detection  and  warning  in  manual  and  automated  driving  results  from  subjective  evaluation.  Drowsiness  is  a  main  cause  of  serious  traffic  accidents,  and  problematic  within  the  ongoing  automation  of  the  driving  task.  Several  approaches  for  drowsiness  detection  have  been  published  and  are  in  operation  in  production  cars  for  manual  driving.  To  assess  differences  in  the  development  of  drowsiness  between  manual  and  automated  driving,  and  to  further  investigate  the  potential  of  subjective  ratings,  we  conducted  a  driving  simulator  study  (N=30).  The  self-assessment  was  based  on  the  Karolinska  Sleepiness  Scale  (KSS),  during  and  after  driving.  Furthermore,  we  examined  the  impact  of  travel  time  and  driver  age  (20-25,  65-70  years).  Results  confirm  that  driving  mode  and  travel  time  have  a  significant  effect  on  the  development  of  drowsiness.  In  both  age  groups,  self-ratings  were  higher  for  automated  driving  and  particularly  by  younger  subjects.  All  subjects  estimated  themselves  drowsier  during  driving.  The  gained  knowledge  can  be  helpful  for  the  development  of  future  driver-vehicle  interfaces  in  driver  drowsiness  detection.
2	No  risk  no  trust  investigating  perceived  risk  in  highly  automated  driving.  When  evaluating  drivers'  trust  in  automated  systems,  perceived  risk  is  an  inevitable,  yet  underestimated  component,  especially  during  initial  interaction.  We  designed  two  experimental  studies  focusing  on  how  people  assess  risk  in  different  driving  environments  and  how  introductory  information  about  automation  reliability  influences  trust  and  risk  perception.  First,  we  designed  nine  driving  scenarios  to  determine  which  factors  influence  Perceived  Situational  Risk  (PSR)  and  Perceived  Relational  Risk  (PRR).  Results  showed  that  participants  identified  levels  of  risk  based  on  traffic  type  and  vehicles'  abnormal  behaviors.  We  then  evaluated  how  introductory  information  and  situational  risk  influence  trust  and  PRR.  Results  showed  that  participants  reported  the  highest  level  of  trust,  perceived  automation  reliability,  and  the  lowest  level  of  PRR  when  presented  with  information  about  a  highly  reliable  system,  and  when  driving  in  a  low-risk  situation.  These  results  highlight  the  importance  of  incorporating  perceived  risk  and  introductory  information  to  support  the  trust  calibration  in  automated  vehicles.
2	Interface  concepts  for  intent  communication  from  autonomous  vehicles  to  vulnerable  road  users.  This  paper  presents  six  interface  concepts  for  Autonomous  Vehicles  to  communicate  their  intention  to  Vulnerable  Road  Users.  The  concepts  were  designed  to  be  scalable  and  versatile,  and  attempt  to  address  some  of  the  limitations  of  existing  concepts  towards  an  unambiguous  communication.  The  interfaces  exist  currently  as  initial  concepts  generated  from  brainstorming  sessions  and  are  in  the  process  of  being  validated  through  prototype  development  and  controlled  studies.
2	Shift  interactive  smartphone  bumper  case.  This  paper  introduces  a  novel  smartphone  bumper  case  providing  an  interactive  touch  sense  and  a  realistic  physical  feeling.  The  thin  and  rapidly  responding  film-like  actuator  installed  in  the  case  simulates  a  physical  response.  A  battery  and  a  wireless  communication  controller  are  also  embedded  for  the  implementation  of  a  stand-alone  device.  As  the  case  is  designed  to  be  assembled  with  a  smartphone  and  the  assembly  does  not  require  any  hardware  revision  of  the  smartphone,  the  proposed  device  has  high  compatibility  with  current  consumer  products.  We  designed  a  software  structure  guaranteeing  a  real-time  physical  response.  The  API  can  be  used  to  provide  realistic  touch  response  and  a  user  test  is  conducted  for  a  comparison  with  the  conventionally  used  vibrator  in  a  mobile  device.
2	The  rice  haptic  rocker  comparing  longitudinal  and  lateral  upper  limb  skin  stretch  perception.  Skin  stretch,  when  mapped  to  joint  position,  provides  haptic  feedback  using  a  mechanism  similar  to  our  sense  of  proprioception  .  Rocker-type  skin  stretch  devices  typically  actuate  in  the  lateral  direction  of  the  arm,  though  during  limb  movement  stretch  about  joint  angles  is  in  the  longitudinal  direction.  In  this  paper,  human  perceptual  performance  in  a  target-hitting  task  is  compared  for  two  orientations  of  the  Rice  Haptic  Rocker.  The  longitudinal  direction  is  expected  to  be  more  intuitive  due  to  the  biological  similarities,  creating  a  more  effective  form  of  haptic  feedback.  The  rockers  are  placed  on  the  upper  arm,  and  convey  the  position  of  a  cursor  among  five  vertically  aligned  targets.  The  longitudinal  orientation  results  in  smaller  errors  compared  to  the  lateral  case.  Additionally,  the  outer  targets  were  reached  with  less  error  than  the  inner  targets  for  the  longitudinal  rocker.  This  result  suggests  longitudinal  stretch  is  more  easily  discerned  than  laterally  oriented  stretch.
2	Influence  of  different  types  of  prior  knowledge  on  haptic  exploration  of  soft  objects.  When  estimating  the  softness  of  an  object  by  active  touch,  humans  typically  indent  the  object’s  surface  several  times  with  their  finger,  applying  higher  peak  indentation  forces  when  they  expect  to  explore  harder  as  compared  to  softer  stimuli  [1].  Here,  we  compared  how  different  types  of  prior  knowledge  differentially  influence  exploratory  forces  in  softness  discrimination.  On  each  trial,  participants  successively  explored  two  silicone  rubber  stimuli  which  were  either  both  relatively  soft  or  both  relatively  hard,  and  judged  which  of  the  two  were  softer.  We  measured  peak  forces  of  the  first  indentation.  In  the  control  condition,  participants  obtained  no  information  about  whether  the  upcoming  stimulus  pair  would  be  from  the  hard  or  the  soft  category.  In  three  test  conditions,  participants  received  implicit  (pairs  from  the  same  category  were  blocked),  semantic  (the  words  soft  and  hard),  or  visual  prior  knowledge  about  the  softness  category.  Visual  information  was  provided  by  displaying  the  rendering  of  a  compliant  object  deformed  by  a  probe.  Given  implicit  information,  participants  again  used  significantly  more  force  in  their  first  touch  when  exploring  harder  as  compared  to  softer  objects.  Surprisingly,  when  given  visual  information,  participants  used  significantly  less  force  in  the  first  touch  when  exploring  harder  objects.  There  was  no  effect  when  participants  were  given  semantic  information.  We  conclude  that  different  types  of  prior  knowledge  influence  the  exploration  behavior  in  very  different  ways.  Thus,  the  mechanisms  through  which  prior  knowledge  is  integrated  in  the  exploration  process  might  be  more  complex  than  expected.
2	Evaluation  of  haptic  html  mappings  derived  from  a  novel  methodology.  As  levels  of  awareness  surrounding  accessibility  increase,  designers  often  look  towards  using  nonvisual  technologies  to  make  existing  graphical  interfaces  (e.g.  Web  pages)  more  inclusive.  As  existing  haptic  design  guidance  is  not  targeted  to  the  specific  needs  of  blind  Web  users,  inappropriate  touchable  representations  of  graphical  objects  may  be  developed  and  integrated  with  Web  interfaces,  thereby  reducing  the  quality  of  the  browsing  experience.  This  paper  describes  the  evaluation  of  haptic  HTML  mappings  that  were  developed  using  a  participatory-design  based  technique,  and  presented  using  the  Logitech  Wingman  force-feedback  mouse.  Findings  have  shown  that  participants  were  able  to  identify  objects  presented  haptically,  and  develop  a  structural  representation  of  layout  from  exploring  content.  Participants  were  able  to  perform  a  range  of  Web-based  tasks  that  were  previously  found  to  be  difficult  for  some  blind  users  when  using  a  screen  reader  alone.  The  haptic  HTML  mappings  are  presented,  along  with  recommendations  for  their  application  derived  from  a  validation  study.  The  design  guidance  presented  offers  a  standard  reference  tool  for  Web  designers  wanting  to  develop  an  accessible  browsing  application,  using  the  benefits  offered  by  a  force-feedback  mouse.
2	Speech  interaction  with  personal  assistive  robots  supporting  aging  at  home  for  individuals  with  alzheimer  s  disease.  Increases  in  the  prevalence  of  dementia  and  Alzheimer’s  disease  (AD)  are  a  growing  challenge  in  many  nations  where  healthcare  infrastructures  are  ill-prepared  for  the  upcoming  demand  for  personal  caregiving.  To  help  individuals  with  AD  live  at  home  for  longer,  we  are  developing  a  mobile  robot,  called  ED,  intended  to  assist  with  activities  of  daily  living  through  visual  monitoring  and  verbal  prompts  in  cases  of  difficulty.  In  a  series  of  experiments,  we  study  speech-based  interactions  between  ED  and  each  of  10  older  adults  with  AD  as  the  latter  complete  daily  tasks  in  a  simulated  home  environment.  Traditional  automatic  speech  recognition  is  evaluated  in  this  environment,  along  with  rates  of  verbal  behaviors  that  indicate  confusion  or  trouble  with  the  conversation.  Analysis  reveals  that  speech  recognition  remains  a  challenge  in  this  setup,  especially  during  household  tasks  with  individuals  with  AD.  Across  the  verbal  behaviors  that  indicate  confusion,  older  adults  with  AD  are  very  likely  to  simply  ignore  the  robot,  which  accounts  for  over  40p  of  all  such  behaviors  when  interacting  with  the  robot.  This  work  provides  a  baseline  assessment  of  the  types  of  technical  and  communicative  challenges  that  will  need  to  be  overcome  for  robots  to  be  used  effectively  in  the  home  for  speech-based  assistance  with  daily  living.
2	Game  tree  search  with  adaptive  resolution.  In  this  paper,  we  use  an  adaptive  resolution  R  to  enhance  the  min-max  search  with  alpha-beta  pruning  technique,  and  show  that  the  value  returned  by  the  modified  algorithm,  called  Negascout-with-resolution,  differs  from  that  of  the  original  version  by  at  most  R.  Guidelines  are  given  to  explain  how  the  resolution  should  be  chosen  to  obtain  the  best  possible  outcome.  Our  experimental  results  demonstrate  that  Negascout-with-resolution  yields  a  significant  performance  improvement  over  the  original  algorithm  on  the  domains  of  random  trees  and  real  game  trees  in  Chinese  chess.
2	Personalized  fitting  recommendation  based  on  support  vector  regression.  Collaborative  filtering  (CF)  is  a  popular  method  for  the  personalized  recommendation.  Almost  all  of  the  existing  CF  methods  rely  only  on  the  rating  data  while  ignoring  some  important  implicit  information  in  non-rating  properties  for  users  and  items,  which  has  a  significant  impact  on  the  preference.  In  this  study,  considering  that  the  average  rating  of  users  and  items  has  a  certain  stability,  we  firstly  propose  a  personalized  fitting  pattern  to  predict  missing  ratings  based  on  the  similarity  score  set,  which  combines  both  the  user-based  and  item-based  CF.  In  order  to  further  reduce  the  prediction  error,  we  use  the  non-rating  attributes,  such  as  a  user’s  age,  gender  and  occupation,  and  an  item’s  release  date  and  price.  Moreover,  we  present  the  deviation  adjustment  method  based  on  the  support  vector  regression.  Experimental  results  on  MovieLens  dataset  show  that  our  proposed  algorithms  can  increase  the  accuracy  of  recommendation  versus  the  traditional  CF.
2	A  crowdsourcing  method  for  online  social  networks  security  assessment  based  on  human  centric  computing.  Crowdsourcing  and  crowd  computing  are  a  trend  that  is  likely  to  be  increasingly  popular,  and  there  remain  a  number  of  research  and  operational  challenges  that  need  to  be  addressed.  The  human-centric  computational  abstraction  called  situation  may  be  used  to  cope  with  these  difficulties.  In  this  paper,  we  focus  on  one  such  challenge,  which  is  how  to  assign  crowd  assessment  tasks  about  security  and  privacy  in  online  social  networks  to  the  most  appropriate  users  efficiently,  effectively  and  accurately.  Specifically,  here  we  propose  a  novel  task  assignment  method  to  facilitate  crowd  assessment,  which  improves  the  security  and  trustworthiness  of  social  networking  platforms,  as  well  as  a  task  assignment  algorithm  based  on  SocialSitu,  which  is  a  social-domain-focused  situational  analytics.  Findings  from  our  crowd  assessment  experiments  on  a  real  world  social  network  Shareteches  show  that  the  precision  and  recall  of  the  proposed  method  and  algorithm  are  0.491  and  0.538  higher  than  those  of  a  random  algorithm’s,  as  well  as  0.336  and  0.366  higher  than  users’  theme-aware  algorithm’s,  respectively.  Moreover,  these  results  further  suggest  that  our  experimental  evaluation  enhance  the  security  and  privacy  of  online  social  networks.
2	Infrared  bundle  adjusting  and  clustering  method  for  head  mounted  display  and  leap  motion  calibration.  Leap  Motion  has  become  widely  used  due  to  its  ability  to  recognize  intuitive  hand  gestures  or  accurate  finger  positions.  Attaching  a  Leap  Motion  to  a  virtual  reality  head-mounted  display  (VR  HMD)  is  highly  interoperable  with  virtual  objects  in  virtual  reality.  However,  it  is  difficult  for  a  virtual  reality  application  to  identify  the  accurate  position  where  the  Leap  Motion  is  attached  to  the  HMD.  This  causes  errors  in  the  positions  of  the  actual  user’s  hands  and  the  virtual  hands,  which  makes  the  interaction  in  virtual  reality  difficult.  In  this  paper,  a  method  that  calibrates  an  output  area  in  VR  HMD  and  a  sensing  area  in  Leap  Motion  is  proposed.  The  difference  in  the  origin  coordinate  between  VR  HMD  and  Leap  Motion  is  derived  using  the  proposed  method.  The  position  of  the  Leap  Motion  attached  to  the  HMD  was  determined  through  an  experiment  using  the  proposed  calibration  technique,  and  the  error  was  approximately  0.757 cm.  Accordingly,  it  enables  more  intuitive  interactions  in  virtual  reality  applications.
2	Colshield  an  effective  and  collaborative  protection  shield  for  the  detection  and  prevention  of  collaborative  flooding  of  ddos  attacks  in  wireless  mesh  networks.  Wireless  mesh  networks  are  highly  susceptible  to  Distributed  Denial-of-Service  attacks  due  to  its  self-configuring  property.  Flooding  DDOS  attack  is  one  form  of  collaborative  attacks  and  the  transport  layer  of  such  networks  are  extremely  affected.  In  this  paper  we  propose  ColShield,  an  effective  and  collaborative  protection  shield  which  not  only  detects  flooding  attacks  but  also  prevents  the  flooding  attacks  through  clever  spoof  detection.  ColShield  consists  of  Intrusion  Protection  and  Detection  Systems  (IPDS)  located  at  various  points  in  the  network  which  collaboratively  defend  flooding  attacks.  ColShield  detects  the  attack  node  and  its  specific  port  number  under  attack.  In  order  to  reduce  the  burden  on  a  single  global  IPDS,  the  system  uses  several  local  IPDS  for  the  collaborative  mitigation  of  flooding  attacks.  The  evaluation  of  ColShield  is  done  using  extensive  simulations  and  is  proved  to  be  effective  in  terms  of  false  positive  ratio,  packet  delivery  ratio,  communication  overhead  and  attack  detection  time.
2	Fairness  scheme  for  energy  efficient  h  264  avc  based  video  sensor  network.  The  availability  of  advanced  wireless  sensor  nodes  enable  us  to  use  video  processing  techniques  in  a  wireless  sensor  network  (WSN)  platform.  Such  paradigm  can  be  used  to  implement  video  sensor  networks  (VSNs)  that  can  serve  as  an  alternative  to  existing  video  surveillance  applications.  However,  video  processing  requires  tremendous  resources  in  terms  of  computation  and  transmission  of  the  encoded  video.  As  the  most  widely  used  video  codec,  H.264/AVC  comes  with  a  number  of  advanced  encoding  tools  that  can  be  tailored  to  suit  a  wide  range  of  applications.  Therefore,  in  order  to  get  an  optimal  encoding  performance  for  the  VSN,  it  is  essential  to  find  the  right  encoding  configuration  and  setting  parameters  for  each  VSN  node  based  on  the  content  being  captured.  In  fact,  the  environment  at  which  the  VSN  is  deployed  affects  not  only  the  content  captured  by  the  VSN  node  but  also  the  node’s  performance  in  terms  of  power  consumption  and  its  life-time.  The  objective  of  this  study  is  to  maximize  the  lifetime  of  the  VSN  by  exploiting  the  trade-off  between  encoding  and  communication  on  sensor  nodes.  In  order  to  reduce  VSNs’  power  consumption  and  obtain  a  more  balanced  energy  consumption  among  VSN  nodes,  we  use  a  branch  and  bound  optimization  techniques  on  a  finite  set  of  encoder  configuration  settings  called  configuration  IDs  (CIDs)  and  a  fairness-based  scheme.  In  our  approach,  the  bitrate  allocation  in  terms  of  fairness  ratio  per  each  node  is  obtained  from  the  training  sequences  and  is  used  to  select  appropriate  encoder  configuration  settings  for  the  test  sequences.  We  use  real  life  content  of  three  different  possible  scenes  of  VSNs’  implementation  with  different  levels  of  complexity  in  our  study.  Performance  evaluations  show  that  the  proposed  optimization  technique  manages  to  balance  VSN’s  power  consumption  per  each  node  while  the  nodes’  maximum  power  consumption  is  minimized.  We  show  that  by  using  that  approach,  the  VSN’s  power  consumption  is  reduced  by  around  7.58%  in  average.
2	3d  face  recognition  a  survey.  3D  face  recognition  has  become  a  trending  research  direction  in  both  industry  and  academia.  It  inherits  advantages  from  traditional  2D  face  recognition,  such  as  the  natural  recognition  process  and  a  wide  range  of  applications.  Moreover,  3D  face  recognition  systems  could  accurately  recognize  human  faces  even  under  dim  lights  and  with  variant  facial  positions  and  expressions,  in  such  conditions  2D  face  recognition  systems  would  have  immense  difficulty  to  operate.  This  paper  summarizes  the  history  and  the  most  recent  progresses  in  3D  face  recognition  research  domain.  The  frontier  research  results  are  introduced  in  three  categories:  pose-invariant  recognition,  expression-invariant  recognition,  and  occlusion-invariant  recognition.  To  promote  future  research,  this  paper  collects  information  about  publicly  available  3D  face  databases.  This  paper  also  lists  important  open  problems.
2	Social  internet  of  things  vision  challenges  and  trends.  IoT  describes  a  new  world  of  billions  of  objects  that  intelligently  communicate  and  interact  with  each  other.  One  of  the  important  areas  in  this  field  is  a  new  paradigm-Social  Internet  of  Things  (SIoT),  a  new  concept  of  combining  social  networks  with  IoT.  SIoT  is  an  imitation  of  social  networks  between  humans  and  objects.  Objects  like  humans  are  considered  intelligent  and  social.  They  create  their  social  network  to  achieve  their  common  goals,  such  as  improving  functionality,  performance,  and  efficiency  and  satisfying  their  required  services.  Our  article’s  primary  purpose  is  to  present  a  comprehensive  review  article  from  the  SIoT  system  to  analyze  and  evaluate  the  recent  works  done  in  this  area.  Therefore,  our  study  concentrated  on  the  main  components  of  the  SIoT  (Architecture,  Relation  Management,  Trust  Management,  web  services,  and  information),  features,  parameters,  and  challenges.  To  gather  enough  information  for  better  analysis,  we  have  reviewed  the  articles  published  between  2011  and  December  2019.  The  strengths  and  weaknesses  of  each  article  are  examined,  and  effective  evaluation  parameters,  approaches,  and  the  most  used  simulation  tools  in  this  field  are  discussed.  For  this  purpose,  we  provide  a  scientific  taxonomy  for  the  final  SIoT  structure  based  on  the  academic  contributions  we  have  studied.  Ultimately  we  observed  that  the  evaluation  parameters  are  different  in  each  element  of  the  SIoT  ecosystem,  for  example  for  Relation  Management,  scalability  29%  and  navigability  22%  are  the  most  concentrated  metrics,  in  Trust  Management,  accuracy  25%,  and  resiliency  25%  is  more  important,  in  the  web  service  process,  time  23%  and  scalability  16%  are  the  most  mentioned  and  finally  in  information  processing,  throughput  and  time  25%  are  the  most  investigated  factor.  Also,  Java-based  tools  like  Eclipse  has  the  most  percentage  in  simulation  tools  in  reviewed  literature  with  28%,  and  SWIM  has  13%  of  usage  for  simulation.
2	Smartphone  based  system  for  sensorimotor  control  assessment  monitoring  improving  and  training  at  home.  This  article  proposes  an  innovative  Smartphone-based  architecture  designed  to  assess,  monitor,  improve  and  train  sensorimotor  abilities  at  home.  This  system  comprises  inertial  sensors  to  measure  orientations,  calculation  units  to  analyze  sensorimotor  control  abilities,  visual,  auditory  and  somatosensory  systems  to  provide  biofeedback  to  the  user,  screen  display  and  headphones  to  provide  test  and/or  training  exercises  instructions,  and  wireless  connection  to  transmit  data.  We  present  two  mobile  applications,  namely  “iBalance”  and  “iProprio”,  to  illustrate  concrete  realization  of  such  architecture  in  the  case  of  at-home  autonomous  assessment  and  rehabilitation  programs  for  balance  and  proprioceptive  abilities.  Our  findings  suggest  that  the  present  architecture  system,  which  does  not  involve  dedicated  and  specialized  equipment,  but  which  is  entirely  embedded  on  a  Smartphone,  could  be  a  suitable  solution  for  Ambient  Assisted  Living  technologies.
2	An  optimized  message  aggregation  method  to  resolve  funneling  effect  in  mobility  management.  In  infrastructure-based  mobile  ad  hoc  networks,  every  mobile  node  has  to  send  registration  message  periodically  to  register  with  an  Internet  Gateway  (IG)  or  update  its  presence.  Since  registration  messages  are  delivered  to  an  IG,  the  mobile  nodes  near  an  IG  can  be  congested,  thus  experiencing  a  lot  of  collisions  in  message  transmission.  This  phenomenon  is  called  “funneling  effect.”  In  this  paper,  we  employ  message  aggregation  technique  to  resolve  the  funneling  effect  and  use  a  skewed  time  synchronization  to  maximize  the  message  aggregation.  We  show  by  resorting  to  simulation  that  the  message  aggregation  technique  can  alleviate  the  funneling  effect  greatly.
2	Specifying  an  mqtt  tree  for  a  connected  smart  home.  Ambient  Assisted  Living  (AAL)  represents  one  of  the  most  promising  Internet  of  Things  applications  due  to  its  influence  on  the  quality  of  life  and  health  of  the  elderly  people.  However,  the  interoperability  is  one  of  the  major  issues  that  needs  to  be  addressed  to  promote  the  adoption  of  AAL  solutions  in  real  environments,  and  to  find  a  way  of  common  exchange  between  the  available  connected  tools  to  share  the  data  exchanged.  This  article  will  present  software  buses  needs  and  specify  an  API  based  on  a  MQTT  software  bus  treelike  architecture.  An  example  is  given  to  illustrate  the  efficiency  of  the  API  developed  in  a  smart  home.
2	Wireless  sensor  and  actor  networks  e  health  e  science  e  decisions.  Wireless  sensor  networks  permit  to  gather  information  about  the  world  and  to  act  on  it  at  an  unprecedentedly  detailed  level.  They  are  transforming  profoundly  the  way  we  live  as  individuals  and  interact  with  the  society  and  our  environment.  In  the  medical  context  they  can  be  used  at  following  levels:  (1)  direct  monitoring  and  supporting  the  health  of  a  specific  patient  [e-Health],  (2)  quality  assurance  of  the  applied  treatments  and  devices,  verification  of  medical  models  [e-Science],  (3)  taking  higher  level  decisions,  either  directly  or  by  advising  humans  (drug  approval,  combating  epidemics)  [e-Decisions].
2	Design  for  rituals  of  letting  go  an  embodiment  perspective  on  disposal  practices  informed  by  grief  therapy.  People  increasingly  live  their  lives  online,  accruing  large  collections  of  digital  possessions,  which  symbolically  represent  important  relationships,  events,  and  activities.  Most  HCI  research  on  bereavement  focuses  on  retaining  these  significant  digital  possessions  to  honor  the  departed.  However,  recent  work  suggests  that  significant  digital  possessions  may  complicate  moving  on;  they  function  as  both  comforting  and  painful  reminders  but  currently  provide  inflexible  methods  for  disposal.  A  few  works  have  investigated  the  disposal  of  digital  objects  as  a  means  of  letting  go.  To  better  understand  this  we  interviewed  10  psychotherapists  who  employ  rituals  of  letting  go  to  help  patients  overcome  loss  in  situations  such  as  a  divorce,  a  breakup,  or  a  stillbirth.  Patients  disposed  of  either  natural  artifacts  or  symbolic  personal  possessions  through  actions  such  as  burning,  burying,  or  placing  in  a  body  of  water.  Therapists  noted  that  people  increasingly  have  digital  possessions,  and  that  the  act  of  deletion  does  not  offer  the  same  cathartic  sense  of  release  as  disposal  of  material  artifacts.  Based  on  the  analysis  of  this  grief  therapy,  we  propose  a  new  conceptual  framework  for  rituals  of  letting  go  that  highlights  temporality,  visibility,  and  force.  It  provides  a  vocabulary  to  talk  about  disposal.  We  then  offer  design  implications  connecting  the  rituals  of  letting  go  to  the  disposal  of  digital  things.  Based  on  our  interviews  and  analytic  framework,  we  propose  novel  technologies  that  better  connect  the  embodied  nature  of  letting  go  rituals  to  the  process  of  digital  disposal.
2	Robotic  computer  as  a  mediator  in  smart  environments.  With  the  advance  of  IT  technologies,  a  new  type  of  computing  device  will  be  introduced  for  smart  environments  in  the  near  future.  In  this  paper,  we  outline  our  on-going  development  of  the  robotic  computer  that  naturally  interacts  with  users,  understands  the  current  situation  of  users  and  environments,  and  proactively  provides  users  with  services.  The  proposed  robotic  computer  consists  of  a  control  unit  and  an  agent  unit.  The  control  unit,  which  is  a  remote  processing  server  connected  to  the  host  server  of  the  smart  environment,  provides  memory  (high-level  information)  and  processing  (computing  resources)  capabilities  to  the  agent  unit.  The  agent  unit  is  a  portable  device  which  is  wirelessly  connected  to  the  control  unit,  and  interacts  with  users  and  physical  objects.  We  describe  the  system  architecture  and  the  implementation  of  a  proof-of-concept  prototype  of  the  proposed  robotic  computer.
2	Design  and  evaluation  of  a  command  recommendation  system  for  software  applications.  We  examine  the  use  of  modern  recommender  system  technology  to  aid  command  awareness  in  complex  software  applications.  We  first  describe  our  adaptation  of  traditional  recommender  system  algorithms  to  meet  the  unique  requirements  presented  by  the  domain  of  software  commands.  A  user  study  showed  that  our  item-based  collaborative  filtering  algorithm  generates  2.1  times  as  many  good  suggestions  as  existing  techniques.  Motivated  by  these  positive  results,  we  propose  a  design  space  framework  and  its  associated  algorithms  to  support  both  global  and  contextual  recommendations.  To  evaluate  the  algorithms,  we  developed  the  CommunityCommands  plug-in  for  AutoCAD.  This  plug-in  enabled  us  to  perform  a  6-week  user  study  of  real-time,  within-application  command  recommendations  in  actual  working  environments.  We  report  and  visualize  command  usage  behaviors  during  the  study,  and  discuss  how  the  recommendations  affected  users  behaviors.  In  particular,  we  found  that  the  plug-in  successfully  exposed  users  to  new  commands,  as  unique  commands  issued  significantly  increased.
2	Self  care  technologies  in  hci  trends  tensions  and  opportunities.  Many  studies  show  that  self-care  technologies  can  support  patients  with  chronic  conditions  and  their  carers  in  understanding  the  ill  body  and  increasing  control  of  their  condition.  However,  many  of  these  studies  have  largely  privileged  a  medical  perspective  and  thus  overlooked  how  patients  and  carers  integrate  self-care  into  their  daily  lives  and  mediate  their  conditions  through  technology.  In  this  review,  we  focus  on  how  patients  and  carers  use  and  experience  self-care  technology  through  a  Human-Computer  Interaction  (HCI)  lens.  We  analyse  studies  of  self-care  published  in  key  HCI  journals  and  conferences  using  the  Grounded  Theory  Literature  Review  (GTLR)  method  and  identify  research  trends  and  design  tensions.  We  then  draw  out  opportunities  for  advancing  HCI  research  in  self-care,  namely,  focusing  further  on  patients'  everyday  life  experience,  considering  existing  collaborations  in  self-care,  and  increasing  the  influence  on  medical  research  and  practice  around  self-care  technology.
2	On  the  naturalness  of  touchless  putting  the  interaction  back  into  nui.  Norman's  critique  is  indicative  of  the  issue  that  while  using  the  word  natural  might  have  become  natural,  it  is  coming  at  a  cost.  In  other  words,  precisely  because  the  notion  of  naturalness  has  become  so  commonplace  in  the  scientific  lexicon  of  HCI,  so  it  is  becoming  increasingly  important,  it  seems  that  there  is  a  critical  examination  of  the  conceptual  work  being  performed  when  it  is  used.  There  is  a  need  to  understand  the  key  assumptions  implicit  within  it  and  how  these  frame  approaches  to  design  and  engineering  in  particular  ways.  A  second  significant  element  of  this  perspective  comes  from  Wittgenstein,  and  his  claim  that,  through  action,  people  create  shared  meanings  with  others,  and  these  shared  meanings  are  the  essential  common  ground  that  enable  individual  perception  to  be  cohered  into  socially  organized,  understood,  and  coordinated  experiences.
2	The  design  and  evaluation  of  interfaces  for  navigating  gigapixel  images  in  digital  pathology.  This  article  describes  the  design  and  evaluation  of  two  generations  of  an  interface  for  navigating  datasets  of  gigapixel  images  that  pathologists  use  to  diagnose  cancer.  The  interface  design  is  innovative  because  users  panned  with  an  overview:detail  view  scale  difference  that  was  up  to  57  times  larger  than  established  guidelines,  and  1  million  pixel  “thumbnail”  overviews  that  leveraged  the  real  estate  of  high-resolution  workstation  displays.  The  research  involved  experts  performing  real  work  (pathologists  diagnosing  cancer),  using  datasets  that  were  up  to  3,150  times  larger  than  those  used  in  previous  studies  that  involved  navigating  images.  The  evaluation  provides  evidence  about  the  effectiveness  of  the  interfaces  and  characterizes  how  experts  navigate  gigapixel  images  when  performing  real  work.  Similar  interfaces  could  be  adopted  in  applications  that  use  other  types  of  high-resolution  images  (e.g.,  remote  sensing  or  high-throughput  microscopy).
2	Social  haystack  dynamic  quality  assessment  of  citizen  generated  content  during  emergencies.  People  all  over  the  world  are  regularly  affected  by  disasters  and  emergencies.  Besides  official  emergency  services,  ordinary  citizens  are  getting  increasingly  involved  in  crisis  response  work.  They  are  usually  present  on-site  at  the  place  of  incident  and  use  social  media  to  share  information  about  the  event.  For  emergency  services,  the  large  amount  of  citizen-generated  content  in  social  media,  however,  means  that  finding  high-quality  information  is  similar  to  “finding  a  needle  in  a  haystack”.  This  article  presents  an  approach  to  how  a  dynamic  and  subjective  quality  assessment  of  citizen-generated  content  could  support  the  work  of  emergency  services.  First,  we  present  results  of  our  empirical  study  concerning  the  usage  of  citizen-generated  content  by  emergency  services.  Based  on  our  literature  review  and  empirical  study,  we  derive  design  guidelines  and  describe  a  concept  for  dynamic  quality  measurement  that  is  implemented  as  a  service-oriented  web-application  “Social  Haystack.”  Finally,  we  outline  findings  of  its  evaluation  and  implications  thereof.
2	Embodied  cognition  and  the  magical  future  of  interaction  design.  The  theory  of  embodied  cognition  can  provide  HCI  practitioners  and  theorists  with  new  ideas  about  interaction  and  new  principles  for  better  designs.  I  support  this  claim  with  four  ideas  about  cognition:  (1)  interacting  with  tools  changes  the  way  we  think  and  perceive  --  tools,  when  manipulated,  are  soon  absorbed  into  the  body  schema,  and  this  absorption  leads  to  fundamental  changes  in  the  way  we  perceive  and  conceive  of  our  environments;  (2)  we  think  with  our  bodies  not  just  with  our  brains;  (3)  we  know  more  by  doing  than  by  seeing  --  there  are  times  when  physically  performing  an  activity  is  better  than  watching  someone  else  perform  the  activity,  even  though  our  motor  resonance  system  fires  strongly  during  other  person  observation;  (4)  there  are  times  when  we  literally  think  with  things.  These  four  ideas  have  major  implications  for  interaction  design,  especially  the  design  of  tangible,  physical,  context  aware,  and  telepresence  systems.
2	Effects  of  communication  partner  instruction  on  the  communication  of  individuals  using  aac  a  meta  analysis.  The  purpose  of  this  study  was  to  conduct  a  systematic  review  and  meta-analysis  of  the  augmentative  and  alternative  communication  (AAC)  partner  instruction  intervention  literature  to  determine  (a)  the  overall  effects  of  partner  interventions  on  the  communication  of  individuals  using  AAC,  and  (b)  any  possible  moderating  variables  relating  to  participant,  intervention,  or  outcome  characteristics.  Seventeen  single-case  experimental  design  studies  (53  participants)  met  the  inclusion  criteria  and  were  advanced  to  the  full  coding  and  analysis  phase  of  the  investigation.  Descriptive  analyses  and  effect  size  estimations  using  the  Improvement  Rate  Difference  (IRD)  metric  were  conducted.  Overall,  communication  partner  interventions  were  found  to  be  highly  effective  across  a  range  of  participants  using  AAC,  intervention  approaches,  and  outcome  measure  characteristics,  with  more  evidence  available  for  participants  less  than  12  years  of  age,  most  of  whom  had  a  diagnosis  of  autism  spectrum  disorder  or  intellectual/devel...
2	Reconstructing  the  voice  of  an  individual  following  laryngectomy.  This  case  study  describes  the  generation  of  a  synthetic  voice  resembling  that  of  an  individual  before  she  underwent  a  laryngectomy.  Recordings  of  this  person  (6–7  min)  speaking  prior  to  the  operation  were  used  to  create  the  voice.  Synthesis  was  based  on  statistical  speech  models  and  this  method  allows  models  pre-trained  on  many  speakers  to  be  adapted  to  resemble  an  individual  voice.  The  results  of  a  listening  test  in  which  participants  were  asked  to  judge  the  similarity  of  the  synthetic  voice  to  the  pre-operation  (target)  voice  are  reported.  Members  of  the  patient’s  family  were  asked  to  make  a  similar  judgment.  These  experiments  show  that,  for  most  listeners,  the  voice  is  quite  convincing  despite  the  low  quality  and  small  quantity  of  adaptation  data.
2	Terminology  and  notation  in  written  representations  of  conversations  with  augmentative  and  alternative  communication.  There  is  a  need  for  a  continuous  discussion  about  what  terms  one  should  use  within  the  field  of  augmentative  and  alternative  communication.  When  talking  and  thinking  about  people  in  their  role  as  users  of  alternative  communication  forms,  the  terms  should  reflect  their  communicative  ways  and  means,  their  achievements  and  what  they  are  doing,  rather  than  focus  on  what  they  cannot  do.  There  are  rather  few  articles  analyzing  utterance  construction  and  dialogue  processes  involving  children  and  adults  using  manual  and  graphic  communication  systems.  The  aim  of  this  paper  was  to  contribute  to  reviving  the  discussion  of  terminology  and  to  more  analyses  of  signing  and  aided  communication  and  an  increase  in  the  use  of  conversation  excerpts  in  the  AAC  Journal  and  elsewhere.
2	A  stochastic  network  calculus  approach  for  the  end  to  end  delay  analysis  of  lte  networks.  The  last  decade  has  seen  an  explosive  growth  in  wireless  network  services  and  consequently  an  increasing  demand  of  high  data  rates  on  wireless  networks,  and  the  demand  of  users  on  real-time  traffic  is  growing.  The  quality  of  service  (QoS)  requirements  of  different  applications  are  different,  for  example,  the  real-time  applications  are  delay-sensitive,  yet  can  tolerant  certain  data  loss.  An  intrinsic  characteristic  of  radio  communication  is  that  the  instantaneous  radio-channel  quality  varies  in  time.  Since  stochastic  network  calculus  takes  the  stochastic  characteristics  of  traffic  and  service  processes  into  consideration  to  make  better  use  of  their  statistical  multiplexing  gains,  so  we  take  the  3GPP  long  term  evolution  (LTE)  as  the  background,  and  construct  a  framework  of  analyzing  the  performance  of  the  LTE  networks  by  modeling  the  arrival  model  and  the  service  model  using  stochastic  network  calculus.  Numerical  analyses  are  conducted  to  prove  the  effectiveness  of  the  framework.
2	Reaching  analysis  of  wheelchair  users  using  motion  planning  methods.  For  an  environment  to  be  well  suited  for  wheelchair  use  not  only  should  it  be  sufficiently  clear  of  obstacles  so  that  the  wheelchair  can  navigate  it,  it  should  also  be  properly  designed  so  that  critical  devices  such  as  light  switches  can  be  reached  by  the  hand  of  the  wheelchair  user.  Given  a  goal  location,  explicitly  calculating  a  path  of  the  wheelchair  and  the  person  sitting  in  it  so  that  they  can  reach  the  goal  is  not  a  trivial  task.  In  this  paper,  we  augment  a  Rapidly-exploring  Random  Tree  (RRT)  planner  with  a  goal  region  generation  stage  that  encourages  the  RRT  to  grow  toward  configurations  from  which  the  wheelchair  user  can  reach  the  goal  point.  The  approach  is  demonstrated  on  simulated  3D  environments.
2	The  iconicity  of  picture  communication  symbols  for  children  with  english  additional  language  and  mild  intellectual  disability.  AbstractThe  purpose  of  this  study  was  to  examine  the  iconicity  of  16  Picture  Communication  Symbols  (PCS)  presented  on  a  themed  bed-making  communication  overlay  for  South  African  children  with  English  as  an  additional  language  and  mild  intellectual  disability.  The  survey  involved  30  participants.  The  results  indicated  that,  overall,  the  16  symbols  were  relatively  iconic  to  the  participants.  The  authors  suggest  that  the  iconicity  of  picture  symbols  could  be  manipulated,  enhanced,  and  influenced  by  contextual  effects  (other  PCS  used  simultaneously  on  the  communication  overlay).  In  addition,  selection  of  non-target  PCS  for  target  PCS  were  discussed  in  terms  of  postulated  differences  in  terms  of  distinctiveness.  Potential  clinical  implications  and  limitations  of  the  study,  as  well  as  recommendations  for  future  research,  are  discussed.
2	A  task  taxonomy  for  network  evolution  analysis.  Visualization  has  proven  to  be  a  useful  tool  for  understanding  network  structures.  Yet  the  dynamic  nature  of  social  media  networks  requires  powerful  visualization  techniques  that  go  beyond  static  network  diagrams.  To  provide  strong  temporal  network  visualization  tools,  designers  need  to  understand  what  tasks  the  users  have  to  accomplish.  This  paper  describes  a  taxonomy  of  temporal  network  visualization  tasks.  We  identify  the  1)  entities,  2)  properties,  and  3)  temporal  features,  which  were  extracted  by  surveying  53  existing  temporal  network  visualization  systems.  By  building  and  examining  the  task  taxonomy,  we  report  which  tasks  are  well  covered  by  existing  systems  and  make  suggestions  for  designing  future  visualization  tools.  The  feedback  from  12  network  analysts  helped  refine  the  taxonomy.
2	3d  hand  tracking  in  the  presence  of  excessive  motion  blur.  We  present  a  sensor-fusion  method  that  exploits  a  depth  camera  and  a  gyroscope  to  track  the  articulation  of  a  hand  in  the  presence  of  excessive  motion  blur.  In  case  of  slow  and  smooth  hand  motions,  the  existing  methods  estimate  the  hand  pose  fairly  accurately  and  robustly,  despite  challenges  due  to  the  high  dimensionality  of  the  problem,  self-occlusions,  uniform  appearance  of  hand  parts,  etc.  However,  the  accuracy  of  hand  pose  estimation  drops  considerably  for  fast-moving  hands  because  the  depth  image  is  severely  distorted  due  to  motion  blur.  Moreover,  when  hands  move  fast,  the  actual  hand  pose  is  far  from  the  one  estimated  in  the  previous  frame,  therefore  the  assumption  of  temporal  continuity  on  which  tracking  methods  rely,  is  not  valid.  In  this  paper,  we  track  fast-moving  hands  with  the  combination  of  a  gyroscope  and  a  depth  camera.  As  a  first  step,  we  calibrate  a  depth  camera  and  a  gyroscope  attached  to  a  hand  so  as  to  identify  their  time  and  pose  offsets.  Following  that,  we  fuse  the  rotation  information  of  the  calibrated  gyroscope  with  model-based  hierarchical  particle  filter  tracking.  A  series  of  quantitative  and  qualitative  experiments  demonstrate  that  the  proposed  method  performs  more  accurately  and  robustly  in  the  presence  of  motion  blur,  when  compared  to  state  of  the  art  algorithms,  especially  in  the  case  of  very  fast  hand  rotations.
2	Model  estimation  and  selection  towardsunconstrained  real  time  tracking  and  mapping.  We  present  an  approach  and  prototype  implementation  to  initialization-free  real-time  tracking  and  mapping  that  supports  any  type  of  camera  motion  in  3D  environments,  that  is,  parallax-inducing  as  well  as  rotation-only  motions.  Our  approach  effectively  behaves  like  a  keyframe-based  Simultaneous  Localization  and  Mapping  system  or  a  panorama  tracking  and  mapping  system,  depending  on  the  camera  movement.  It  seamlessly  switches  between  the  two  modes  and  is  thus  able  to  track  and  map  through  arbitrary  sequences  of  parallax-inducing  and  rotation-only  camera  movements.  The  system  integrates  both  model-based  and  model-free  tracking,  automatically  choosing  between  the  two  depending  on  the  situation,  and  subsequently  uses  the  “Geometric  Robust  Information  Criterion”  to  decide  whether  the  current  camera  motion  can  best  be  represented  as  a  parallax-inducing  motion  or  a  rotation-only  motion.  It  continues  to  collect  and  map  data  after  tracking  failure  by  creating  separate  tracks  which  are  later  merged  if  they  are  found  to  overlap.  This  is  in  contrast  to  most  existing  tracking  and  mapping  systems,  which  suspend  tracking  and  mapping  and  thus  discard  valuable  data  until  relocalization  with  respect  to  the  initial  map  is  successful.  We  tested  our  prototype  implementation  on  a  variety  of  video  sequences,  successfully  tracking  through  different  camera  motions  and  fully  automatically  building  combinations  of  panoramas  and  3D  structure.
2	Perception  based  evaluation  of  projection  methods  for  multidimensional  data  visualization.  Similarity-based  layouts  generated  by  multidimensional  projections  or  other  dimension  reduction  techniques  are  commonly  used  to  visualize  high-dimensional  data.  Many  projection  techniques  have  been  recently  proposed  addressing  different  objectives  and  application  domains.  Nonetheless,  very  little  is  known  about  the  effectiveness  of  the  generated  layouts  from  a  user’s  perspective,  how  distinct  layouts  from  the  same  data  compare  regarding  the  typical  visualization  tasks  they  support,  or  how  domain-specific  issues  affect  the  outcome  of  the  techniques.  Learning  more  about  projection  usage  is  an  important  step  towards  both  consolidating  their  role  inhigh-dimensional  data  analysis  and  taking  informed  decisions  when  choosing  techniques.  This  work  provides  a  contribution  towards  this  goal.  We  describe  the  results  of  an  investigation  on  the  performance  of  layouts  generated  by  projection  techniques  as  perceived  by  their  users.  We  conducted  a  controlled  user  study  to  test  against  the  following  hypotheses:  (1)  projection  performance  istask-dependent;  (2)  certain  projections  perform  better  on  certain  types  of  tasks;  (3)  projection  performance  depends  on  the  nature  of  the  data;  and  (4)  subjects  prefer  projections  with  good  segregation  capability.  We  generated  layouts  of  high-dimensional  data  with  five  techniques  representative  of  different  projection  approaches.  As  application  domains  we  investigated  image  and  document  data.  We  identified  eight  typical  tasks,  three  of  them  related  to  segregation  capability  of  the  projection,  three  related  to  projection  precision,  and  two  related  to  incurred  visual  cluttering.  Answers  to  questions  were  compared  for  correctness  against  ‘ground  truth’  computed  directly  from  the  data.  We  also  looked  at  subject  confidence  and  task  completion  times.  Statistical  analysis  of  the  collected  data  resulted  in  Hypotheses  1  and  3  being  confirmed,  Hypothesis  2  being  confirmed  partially  and  Hypotheses  4  could  not  be  confirmed.  We  discuss  our  findings  in  comparison  with  some  numerical  measures  of  projection  layout  quality.  Our  results  offer  interesting  insight  on  the  use  ofprojection  layouts  in  data  visualization  tasks  and  provide  a  departing  point  for  further  systematic  investigations.
2	Edge  compression  techniques  for  visualization  of  dense  directed  graphs.  We  explore  the  effectiveness  of  visualizing  dense  directed  graphs  by  replacing  individual  edges  with  edges  connected  to  'modules'-or  groups  of  nodes-such  that  the  new  edges  imply  aggregate  connectivity.  We  only  consider  techniques  that  offer  a  lossless  compression:  that  is,  where  the  entire  graph  can  still  be  read  from  the  compressed  version.  The  techniques  considered  are:  a  simple  grouping  of  nodes  with  identical  neighbor  sets;  Modular  Decomposition  which  permits  internal  structure  in  modules  and  allows  them  to  be  nested;  and  Power  Graph  Analysis  which  further  allows  edges  to  cross  module  boundaries.  These  techniques  all  have  the  same  goal-to  compress  the  set  of  edges  that  need  to  be  rendered  to  fully  convey  connectivity-but  each  successive  relaxation  of  the  module  definition  permits  fewer  edges  to  be  drawn  in  the  rendered  graph.  Each  successive  technique  also,  we  hypothesize,  requires  a  higher  degree  of  mental  effort  to  interpret.  We  test  this  hypothetical  trade-off  with  two  studies  involving  human  participants.  For  Power  Graph  Analysis  we  propose  a  novel  optimal  technique  based  on  constraint  programming.  This  enables  us  to  explore  the  parameter  space  for  the  technique  more  precisely  than  could  be  achieved  with  a  heuristic.  Although  applicable  to  many  domains,  we  are  motivated  by-and  discuss  in  particular-the  application  to  software  dependency  analysis.
2	The  connected  scatterplot  for  presenting  paired  time  series.  The    connected  scatterplot    visualizes  two  related  time  series  in  a  scatterplot  and  connects  the  points  with  a  line  in  temporal  sequence.  News  media  are  increasingly  using  this  technique  to  present  data  under  the  intuition  that  it  is  understandable  and  engaging.  To  explore  these  intuitions,  we  (1)  describe  how  paired  time  series  relationships  appear  in  a  connected  scatterplot,  (2)  qualitatively  evaluate  how  well  people  understand  trends  depicted  in  this  format,  (3)  quantitatively  measure  the  types  and  frequency  of  misinter  pretations,  and  (4)  empirically  evaluate  whether  viewers  will  preferentially  view  graphs  in  this  format  over  the  more  traditional  format.  The  results  suggest  that  low-complexity  connected  scatterplots  can  be  understood  with  little  explanation,  and  that  viewers  are  biased  towards  inspecting  connected  scatterplots  over  the  more  traditional  format.  We  also  describe  misinterpretations  of  connected  scatterplots  and  propose  further  research  into  mitigating  these  mistakes  for  viewers  unfamiliar  with  the  technique.
2	Topotag  a  robust  and  scalable  topological  fiducial  marker  system.  Fiducial  markers  have  been  playing  an  important  role  in  augmented  reality  (AR),  robot  navigation,  and  general  applications  where  the  relative  pose  between  a  camera  and  an  object  is  required.  Here  we  introduce  TopoTag,  a  robust  and  scalable  topological  fiducial  marker  system,  which  supports  reliable  and  accurate  pose  estimation  from  a  single  image.  TopoTag  uses  topological  and  geometrical  information  in  marker  detection  to  achieve  higher  robustness.  Topological  information  is  extensively  used  for  2D  marker  detection,  and  further  corresponding  geometrical  information  for  ID  decoding.  Robust  3D  pose  estimation  is  achieved  by  taking  advantage  of  all  TopoTag  vertices.  Without  sacrificing  bits  for  higher  recall  and  precision  like  previous  systems,  TopoTag  can  use  full  bits  for  ID  encoding.  TopoTag  supports  tens  of  thousands  unique  IDs  and  easily  extends  to  millions  of  unique  tags  resulting  in  massive  scalability.  We  collected  a  large  test  dataset  including  in  total  169,713  images  for  evaluation,  involving  in-plane  and  out-of-plane  rotation,  image  blur,  different  distances,  and  various  backgrounds,  etc.  Experiments  on  the  dataset  and  real  indoor  and  outdoor  scene  tests  with  a  rolling  shutter  camera  both  show  that  TopoTag  significantly  outperforms  previous  fiducial  marker  systems  in  terms  of  various  metrics,  including  detection  accuracy,  vertex  jitter,  pose  jitter  and  accuracy,  etc.  In  addition,  TopoTag  supports  occlusion  as  long  as  the  main  tag  topological  structure  is  maintained  and  allows  for  flexible  shape  design  where  users  can  customize  internal  and  external  marker  shapes.  Code  for  our  marker  design/generation,  marker  detection,  and  dataset  are  available  at  http://herohuyongtao.github.io/research/publications/topo-tag/  .
2	Sirius  dual  symmetric  interactive  dimension  reductions.  Much  research  has  been  done  regarding  how  to  visualize  and  interact  with  observations  and  attributes  of  high-dimensional  data  for  exploratory  data  analysis.  From  the  analyst's  perceptual  and  cognitive  perspective,  current  visualization  approaches  typically  treat  the  observations  of  the  high-dimensional  dataset  very  differently  from  the  attributes.  Often,  the  attributes  are  treated  as  inputs  (e.g.,  sliders),  and  observations  as  outputs  (e.g.,  projection  plots),  thus  emphasizing  investigation  of  the  observations.  However,  there  are  many  cases  in  which  analysts  wish  to  investigate  both  the  observations  and  the  attributes  of  the  dataset,  suggesting  a  symmetry  between  how  analysts  think  about  attributes  and  observations.  To  address  this,  we  define  SIRIUS  (Symmetric  Interactive  Representations  In  a  Unified  System),  a  symmetric,  dual  projection  technique  to  support  exploratory  data  analysis  of  high-dimensional  data.  We  provide  an  example  implementation  of  SIRIUS  and  demonstrate  how  this  symmetry  affords  additional  insights.
2	Indexed  points  parallel  coordinates  visualization  of  multivariate  correlations.  We  address  the  problem  of  visualizing  multivariate  correlations  in  parallel  coordinates.  We  focus  on  multivariate  correlation  in  the  form  of  linear  relationships  between  multiple  variables.  Traditional  parallel  coordinates  are  well  prepared  to  show  negative  correlations  between  two  attributes  by  distinct  visual  patterns.  However,  it  is  difficult  to  recognize  positive  correlations  in  parallel  coordinates.  Furthermore,  there  is  no  support  to  highlight  multivariate  correlations  in  parallel  coordinates.  In  this  paper,  we  exploit  the  indexed  point  representation  of  p  -flats  (planes  in  multidimensional  data)  to  visualize  local  multivariate  correlations  in  parallel  coordinates.  Our  method  yields  clear  visual  signatures  for  negative  and  positive  correlations  alike,  and  it  supports  large  datasets.  All  information  is  shown  in  a  unified  parallel  coordinates  framework,  which  leads  to  easy  and  familiar  user  interactions  for  analysts  who  have  experience  with  traditional  parallel  coordinates.  The  usefulness  of  our  method  is  demonstrated  through  examples  of  typical  multidimensional  datasets.
2	Topic  based  exploration  and  embedded  visualizations  for  research  idea  generation.  This  work  analyzes  sensemaking  frameworks  and  experiments  with  an  iteratively  designed  visual  analysis  tool  to  identify  design  implications  for  facilitating  research  idea  generation  using  visualizations.  Our  tool,  ThoughtFlow,  structures  and  visualizes  literature  collections  using  topic  models  to  bridge  the  information  gap  between  core  activities  during  research  ideation.  To  help  users  stay  focused  on  a  topic  while  discovering  relevant  documents,  we  designed  and  analyzed  usage  patterns  for  two  types  of  embedded  visualization  that  help  determine  document  relevance  while  minimizing  distraction.  We  analyzed  how  research  ideation  outcomes  and  processes  differ  when  using  ThoughtFlow  and  conventional  search  engines  by  augmenting  insight-based  evaluation  with  concept-map  analysis.  Our  results  suggest  that  operations  afforded  by  topic  models  match  well  with  later  ideation  stages  when  coherent  topics  have  emerged,  but  not  with  early  stages  when  users  are  still  relying  heavily  on  individual  keywords  to  gather  background  knowledge.  We  also  present  qualitative  evidence  that  citation  sparklines  encourage  more  exploration  of  recommended  references,  and  that  a  preference  for  paper  thumbnails  may  depend  on  the  consistency  between  the  evidence  and  the  current  mental  frame.
2	Seamless  multi  projection  revisited.  This  paper  introduces  a  novel  photometric  compensation  technique  for  inter-projector  luminance  and  chrominance  variations.  Although  it  sounds  as  a  classical  technical  issue,  to  the  best  of  our  knowledge  there  is  no  existing  solution  to  alleviate  the  spatial  non-uniformity  among  strongly  heterogeneous  projectors  at  perceptually  acceptable  quality.  Primary  goal  of  our  method  is  increasing  the  perceived  seamlessness  of  the  projection  system  by  automatically  generating  an  improved  and  consistent  visual  quality.  It  builds  upon  the  existing  research  of  multi-projection  systems,  but  instead  of  working  with  perceptually  non-uniform  color  spaces  such  as  CIEXYZ,  the  overall  computation  is  carried  out  using  the  RLab  [10,  pp.  243–254]  color  appearance  model  which  models  the  color  processing  in  an  adaptive,  perceptual  manner.  Besides,  we  propose  an  adaptive  color  gamut  acquisition,  spatially  varying  gamut  mapping,  and  optimization  framework  for  edge  blending.  The  paper  describes  the  overall  workflow  and  detailed  algorithm  of  each  component,  followed  by  an  evaluation  validating  the  proposed  method.  The  experimental  results  both  qualitatively  and  quantitatively  show  the  proposed  method  significant  improved  the  visual  quality  of  projected  results  of  a  multi-projection  display  with  projectors  with  severely  heterogeneous  color  processing.
2	Transformation  of  an  uncertain  video  search  pipeline  to  a  sketch  based  visual  analytics  loop.  Traditional  sketch-based  image  or  video  search  systems  rely  on  machine  learning  concepts  as  their  core  technology.  However,  in  many  applications,  machine  learning  alone  is  impractical  since  videos  may  not  be  semantically  annotated  sufficiently,  there  may  be  a  lack  of  suitable  training  data,  and  the  search  requirements  of  the  user  may  frequently  change  for  different  tasks.  In  this  work,  we  develop  a  visual  analytics  systems  that  overcomes  the  shortcomings  of  the  traditional  approach.  We  make  use  of  a  sketch-based  interface  to  enable  users  to  specify  search  requirement  in  a  flexible  manner  without  depending  on  semantic  annotation.  We  employ  active  machine  learning  to  train  different  analytical  models  for  different  types  of  search  requirements.  We  use  visualization  to  facilitate  knowledge  discovery  at  the  different  stages  of  visual  analytics.  This  includes  visualizing  the  parameter  space  of  the  trained  model,  visualizing  the  search  space  to  support  interactive  browsing,  visualizing  candidature  search  results  to  support  rapid  interaction  for  active  learning  while  minimizing  watching  videos,  and  visualizing  aggregated  information  of  the  search  results.  We  demonstrate  the  system  for  searching  spatiotemporal  attributes  from  sports  video  to  identify  key  instances  of  the  team  and  player  performance.
2	Data  driven  synthetic  modeling  of  trees.  In  this  paper,  we  develop  a  data-driven  technique  to  model  trees  from  a  single  laser  scan.  A  multi-layer  representation  of  the  tree  structure  is  proposed  to  guide  the  modeling  process.  In  this  process,  a  marching  cylinder  algorithm  is  first  developed  to  construct  visible  branches  from  the  laser  scan  data.  Three  levels  of  crown  feature  points  are  then  extracted  from  the  scan  data  to  synthesize  three  layers  of  non-visible  branches.  Based  on  the  hierarchical  particle  flow  technique,  the  branch  synthesis  method  has  the  advantage  of  producing  visually  convincing  tree  models  that  are  consistent  with  scan  data.  User  intervention  is  extremely  limited.  The  robustness  of  this  technique  has  been  validated  on  both  conifer  and  broadleaf  trees.
2	Asymmetric  tensor  field  visualization  for  surfaces.  Asymmetric  tensor  field  visualization  can  provide  important  insight  into  fluid  flows  and  solid  deformations.  Existing  techniques  for  asymmetric  tensor  fields  focus  on  the  analysis,  and  simply  use  evenly-spaced  hyperstreamlines  on  surfaces  following  eigenvectors  and  dual-eigenvectors  in  the  tensor  field.  In  this  paper,  we  describe  a  hybrid  visualization  technique  in  which  hyperstreamlines  and  elliptical  glyphs  are  used  in  real  and  complex  domains,  respectively.  This  enables  a  more  faithful  representation  of  flow  behaviors  inside  complex  domains.  In  addition,  we  encode  tensor  magnitude,  an  important  quantity  in  tensor  field  analysis,  using  the  density  of  hyperstreamlines  and  sizes  of  glyphs.  This  allows  colors  to  be  used  to  encode  other  important  tensor  quantities.  To  facilitate  quick  visual  exploration  of  the  data  from  different  viewpoints  and  at  different  resolutions,  we  employ  an  efficient  image-space  approach  in  which  hyperstreamlines  and  glyphs  are  generated  quickly  in  the  image  plane.  The  combination  of  these  techniques  leads  to  an  efficient  tensor  field  visualization  system  for  domain  scientists.  We  demonstrate  the  effectiveness  of  our  visualization  technique  through  applications  to  complex  simulated  engine  fluid  flow  and  earthquake  deformation  data.  Feedback  from  domain  expert  scientists,  who  are  also  co-authors,  is  provided.
2	Screenx  public  immersive  theatres  with  uniform  movie  viewing  experiences.  This  paper  introduces  ScreenX,  which  is  a  novel  movie  viewing  platform  that  enables  ordinary  movie  theatres  to  become  multi-projection  movie  theatres.  This  enables  the  general  public  to  enjoy  immersive  viewing  experiences.  The  left  and  right  side  walls  are  used  to  form  surrounding  screens.  This  surrounding  display  environment  delivers  a  strong  sense  of  immersion  in  general  movie  viewing.  However,  naive  display  of  the  content  on  the  side  walls  results  in  the  appearance  of  distorted  images  according  to  the  location  of  the  viewer.  In  addition,  the  different  dimensions  in  width,  height,  and  depth  among  theatres  may  lead  to  different  viewing  experiences.  Therefore,  for  successful  deployment  of  this  novel  platform,  an  approach  to  providing  similar  movie  viewing  experiences  across  target  theatres  is  presented.  The  proposed  image  representation  model  ensures  minimum  average  distortion  of  the  images  displayed  on  the  side  walls  when  viewed  from  different  locations.  Furthermore,  the  proposed  model  assists  with  determining  the  appropriate  variation  of  the  content  according  to  the  diverse  viewing  environments  of  different  theatres.  The  theatre  suitability  estimation  method  excludes  outlier  theatres  that  have  extraordinary  dimensions.  In  addition,  the  content  production  guidelines  indicate  appropriate  regions  to  place  scene  elements  for  the  side  wall,  depending  on  their  importance.  The  experiments  demonstrate  that  the  proposed  method  improves  the  movie  viewing  experiences  in  ScreenX  theatres.  Finally,  ScreenX  and  the  proposed  techniques  are  discussed  with  regard  to  various  aspects  and  the  research  issues  that  are  relevant  to  this  movie  viewing  platform  are  summarized.
2	Modeling  data  driven  dominance  traits  for  virtual  characters  using  gait  analysis.  We  present  a  data-driven  algorithm  for  generating  gaits  of  virtual  characters  with  varying  dominance  traits.  Our  formulation  utilizes  a  user  study  to  establish  a  data-driven  dominance  mapping  between  gaits  and  dominance  labels.  We  use  our  dominance  mapping  to  generate  walking  gaits  for  virtual  characters  that  exhibit  a  variety  of  dominance  traits  while  interacting  with  the  user.  Furthermore,  we  extract  gait  features  based  on  known  criteria  in  visual  perception  and  psychology  literature  that  can  be  used  to  identify  the  dominance  levels  of  any  walking  gait.  We  validate  our  mapping  and  the  perceived  dominance  traits  by  a  second  user  study  in  an  immersive  virtual  environment.  Our  gait  dominance  classification  algorithm  can  classify  the  dominance  traits  of  gaits  with  ˜73  percent  accuracy.  We  also  present  an  application  of  our  approach  that  simulates  interpersonal  relationships  between  virtual  characters.  To  the  best  of  our  knowledge,  ours  is  the  first  practical  approach  to  classifying  gait  dominance  and  generate  dominance  traits  in  virtual  characters.
2	Text  readability  in  head  worn  displays  color  and  style  optimization  in  video  versus  optical  see  through  devices.  Efficient  text  visualization  in  head-worn  augmented  reality  (AR)  displays  is  critical  because  it  is  sensitive  to  display  technology,  text  style  and  color,  ambient  illumination  and  so  on.  The  main  problem  for  the  developer  is  to  know  the  optimal  text  style  for  the  specific  display  and  for  applications  where  color  coding  must  be  strictly  followed  because  it  is  regulated  by  laws  or  internal  practices.  In  this  work,  we  experimented  the  effects  on  readability  of  two  head-worn  devices  (optical  and  video  see-through),  two  backgrounds  (light  and  dark),  five  colors  (white,  black,  red,  green,  and  blue),  and  two  text  styles  (plain  text  and  billboarded  text).  Font  type  and  size  were  kept  constant.  We  measured  the  performance  of  15  subjects  by  collecting  about  5,000  measurements  using  a  specific  test  application  and  followed  by  qualitative  interviews.  Readability  turned  out  to  be  quicker  on  the  optical  see-through  device.  For  the  video  see-through  device,  background  affects  readability  only  in  case  of  text  without  billboard.  Finally,  our  tests  suggest  that  a  good  combination  for  indoor  augmented  reality  applications,  regardless  of  device  and  background,  could  be  white  text  and  blue  billboard,  while  a  mandatory  color  should  be  displayed  as  billboard  with  a  white  text  message.
2	Selecting  the  aspect  ratio  of  a  scatter  plot  based  on  its  delaunay  triangulation.  Scatter  plots  are  diagrams  that  visualize  two-dimensional  data  as  sets  of  points  in  the  plane.  They  allow  users  to  detect  correlations  and  clusters  in  the  data.  Whether  or  not  a  user  can  accomplish  these  tasks  highly  depends  on  the  aspect  ratio  selected  for  the  plot,  i.e.,  the  ratio  between  the  horizontal  and  the  vertical  extent  of  the  diagram.  We  argue  that  an  aspect  ratio  is  good  if  the  Delaunay  triangulation  of  the  scatter  plot  at  this  aspect  ratio  has  some  nice  geometric  property,  e.g.,  a  large  minimum  angle  or  a  small  total  edge  length.  More  precisely,  we  consider  the  following  optimization  problem.  Given  a  set  Q  of  points  in  the  plane,  find  a  scale  factor  s  such  that  scaling  the  x-coordinates  of  the  points  in  Q  by  s  and  the  y-coordinates  by  1=s  yields  a  point  set  P(s)  that  optimizes  a  property  of  the  Delaunay  triangulation  of  P(s),  over  all  choices  of  s.  We  present  an  algorithm  that  solves  this  problem  efficiently  and  demonstrate  its  usefulness  on  real-world  instances.  Moreover,  we  discuss  an  empirical  test  in  which  we  asked  64  participants  to  choose  the  aspect  ratios  of  18  scatter  plots.  We  tested  six  different  quality  measures  that  our  algorithm  can  optimize.  In  conclusion,  minimizing  the  total  edge  length  and  minimizing  what  we  call  the  'uncompactness'  of  the  triangles  of  the  Delaunay  triangulation  yielded  the  aspect  ratios  that  were  most  similar  to  those  chosen  by  the  participants  in  the  test.
2	Combining  single  and  packet  ray  tracing  for  arbitrary  ray  distributions  on  the  intel  mic  architecture.  Wide-SIMD  hardware  is  power  and  area  efficient,  but  it  is  challenging  to  efficiently  map  ray  tracing  algorithms  to  such  hardware  especially  when  the  rays  are  incoherent.  The  two  most  commonly  used  schemes  are  either  packet  tracing,  or  relying  on  a  separate  traversal  stack  for  each  SIMD  lane.  Both  work  great  for  coherent  rays,  but  suffer  when  rays  are  incoherent:  The  former  experiences  a  dramatic  loss  of  SIMD  utilization  once  rays  diverge;  the  latter  requires  a  large  local  storage,  and  generates  multiple  incoherent  streams  of  memory  accesses  that  present  challenges  for  the  memory  system.  In  this  paper,  we  introduce  a  single-ray  tracing  scheme  for  incoherent  rays  that  uses  just  one  traversal  stack  on  16-wide  SIMD  hardware.  It  uses  a  bounding-volume  hierarchy  with  a  branching  factor  of  four  as  the  acceleration  structure,  exploits  four-wide  SIMD  in  each  box  and  primitive  intersection  test,  and  uses  16-wide  SIMD  by  always  performing  four  such  node  or  primitive  tests  in  parallel.  We  then  extend  this  scheme  to  a  hybrid  tracing  scheme  that  automatically  adapts  to  varying  ray  coherence  by  starting  out  with  a  16-wide  packet  scheme  and  switching  to  the  new  single-ray  scheme  as  soon  as  rays  diverge.  We  show  that  on  the  Intel  Many  Integrated  Core  architecture  this  hybrid  scheme  consistently,  and  over  a  wide  range  of  scenes  and  ray  distributions,  outperforms  both  packet  and  single-ray  tracing.
2	A  heuristic  approach  to  value  driven  evaluation  of  visualizations.  Recently,  an  approach  for  determining  the  value  of  a  visualization  was  proposed,  one  moving  beyond  simple  measurements  of  task  accuracy  and  speed.  The  value  equation  contains  components  for  the  time  savings  a  visualization  provides,  the  insights  and  insightful  questions  it  spurs,  the  overall  essence  of  the  data  it  conveys,  and  the  confidence  about  the  data  and  its  domain  it  inspires.  This  articulation  of  value  is  purely  descriptive,  however,  providing  no  actionable  method  of  assessing  a  visualization's  value.  In  this  work,  we  create  a  heuristic-based  evaluation  methodology  to  accompany  the  value  equation  for  assessing  interactive  visualizations.  We  refer  to  the  methodology  colloquially  as  ICE-T,  based  on  an  anagram  of  the  four  value  components.  Our  approach  breaks  the  four  components  down  into  guidelines,  each  of  which  is  made  up  of  a  small  set  of  low-level  heuristics.  Evaluators  who  have  knowledge  of  visualization  design  principles  then  assess  the  visualization  with  respect  to  the  heuristics.  We  conducted  an  initial  trial  of  the  methodology  on  three  interactive  visualizations  of  the  same  data  set,  each  evaluated  by  15  visualization  experts.  We  found  that  the  methodology  showed  promise,  obtaining  consistent  ratings  across  the  three  visualizations  and  mirroring  judgments  of  the  utility  of  the  visualizations  by  instructors  of  the  course  in  which  they  were  developed.
2	Srvis  towards  better  spatial  integration  in  ranking  visualization.  Interactive  ranking  techniques  have  substantially  promoted  analysts'  ability  in  making  judicious  and  informed  decisions  effectively  based  on  multiple  criteria.  However,  the  existing  techniques  cannot  satisfactorily  support  the  analysis  tasks  involved  in  ranking  large-scale  spatial  alternatives,  such  as  selecting  optimal  locations  for  chain  stores,  where  the  complex  spatial  contexts  involved  are  essential  to  the  decision-making  process.  Limitations  observed  in  the  prior  attempts  of  integrating  rankings  with  spatial  contexts  motivate  us  to  develop  a  context-integrated  visual  ranking  technique.  Based  on  a  set  of  generic  design  requirements  we  summarized  by  collaborating  with  domain  experts,  we  propose  SRVis,  a  novel  spatial  ranking  visualization  technique  that  supports  efficient  spatial  multi-criteria  decision-making  processes  by  addressing  three  major  challenges  in  the  aforementioned  context  integration,  namely,  a)  the  presentation  of  spatial  rankings  and  contexts,  b)  the  scalability  of  rankings'  visual  representations,  and  c)  the  analysis  of  context-integrated  spatial  rankings.  Specifically,  we  encode  massive  rankings  and  their  cause  with  scalable  matrix-based  visualizations  and  stacked  bar  charts  based  on  a  novel  two-phase  optimization  framework  that  minimizes  the  information  loss,  and  the  flexible  spatial  filtering  and  intuitive  comparative  analysis  are  adopted  to  enable  the  in-depth  evaluation  of  the  rankings  and  assist  users  in  selecting  the  best  spatial  alternative.  The  effectiveness  of  the  proposed  technique  has  been  evaluated  and  demonstrated  with  an  empirical  study  of  optimization  methods,  two  case  studies,  and  expert  interviews.
2	A  perception  driven  approach  to  supervised  dimensionality  reduction  for  visualization.  Dimensionality  reduction  (DR)  is  a  common  strategy  for  visual  analysis  of  labeled  high-dimensional  data.  Low-dimensional  representations  of  the  data  help,  for  instance,  to  explore  the  class  separability  and  the  spatial  distribution  of  the  data.  Widely-used  unsupervised  DR  methods  like  PCA  do  not  aim  to  maximize  the  class  separation,  while  supervised  DR  methods  like  LDA  often  assume  certain  spatial  distributions  and  do  not  take  perceptual  capabilities  of  humans  into  account.  These  issues  make  them  ineffective  for  complicated  class  structures.  Towards  filling  this  gap,  we  present  a  perception-driven  linear  dimensionality  reduction  approach  that  maximizes  the  perceived  class  separation  in  projections.  Our  approach  builds  on  recent  developments  in  perception-based  separation  measures  that  have  achieved  good  results  in  imitating  human  perception.  We  extend  these  measures  to  be  density-aware  and  incorporate  them  into  a  customized  simulated  annealing  algorithm,  which  can  rapidly  generate  a  near  optimal  DR  projection.  We  demonstrate  the  effectiveness  of  our  approach  by  comparing  it  to  state-of-the-art  DR  methods  on  93  datasets,  using  both  quantitative  measure  and  human  judgments.  We  also  provide  case  studies  with  class-imbalanced  and  unlabeled  data.
2	Sketching  uncertainty  into  simulations.  In  a  variety  of  application  areas,  the  use  of  simulation  steering  in  decision  making  is  limited  at  best.  Research  focusing  on  this  problem  suggests  that  most  user  interfaces  are  too  complex  for  the  end  user.  Our  goal  is  to  let  users  create  and  investigate  multiple,  alternative  scenarios  without  the  need  for  special  simulation  expertise.  To  simplify  the  specification  of  parameters,  we  move  from  a  traditional  manipulation  of  numbers  to  a  sketch-based  input  approach.  Users  steer  both  numeric  parameters  and  parameters  with  a  spatial  correspondence  by  sketching  a  change  onto  the  rendering.  Special  visualizations  provide  immediate  visual  feedback  on  how  the  sketches  are  transformed  into  boundary  conditions  of  the  simulation  models.  Since  uncertainty  with  respect  to  many  intertwined  parameters  plays  an  important  role  in  planning,  we  also  allow  the  user  to  intuitively  setup  complete  value  ranges,  which  are  then  automatically  transformed  into  ensemble  simulations.  The  interface  and  the  underlying  system  were  developed  in  collaboration  with  experts  in  the  field  of  flood  management.  The  real-world  data  they  have  provided  has  allowed  us  to  construct  scenarios  used  to  evaluate  the  system.  These  were  presented  to  a  variety  of  flood  response  personnel,  and  their  feedback  is  discussed  in  detail  in  the  paper.  The  interface  was  found  to  be  intuitive  and  relevant,  although  a  certain  amount  of  training  might  be  necessary.
2	Toward  objective  evaluation  of  working  memory  in  visualizations  a  case  study  using  pupillometry  and  a  dual  task  paradigm.  Cognitive  science  has  established  widely  used  and  validated  procedures  for  evaluating  working  memory  in  numerous  applied  domains,  but  surprisingly  few  studies  have  employed  these  methodologies  to  assess  claims  about  the  impacts  of  visualizations  on  working  memory.  The  lack  of  information  visualization  research  that  uses  validated  procedures  for  measuring  working  memory  may  be  due,  in  part,  to  the  absence  of  cross-domain  methodological  guidance  tailored  explicitly  to  the  unique  needs  of  visualization  research.  This  paper  presents  a  set  of  clear,  practical,  and  empirically  validated  methods  for  evaluating  working  memory  during  visualization  tasks  and  provides  readers  with  guidance  in  selecting  an  appropriate  working  memory  evaluation  paradigm.  As  a  case  study,  we  illustrate  multiple  methods  for  evaluating  working  memory  in  a  visual-spatial  aggregation  task  with  geospatial  data.  The  results  show  that  the  use  of  dual-task  experimental  designs  (simultaneous  performance  of  several  tasks  compared  to  single-task  performance)  and  pupil  dilation  can  reveal  working  memory  demands  associated  with  task  difficulty  and  dual-tasking.  In  a  dual-task  experimental  design,  measures  of  task  completion  times  and  pupillometry  revealed  the  working  memory  demands  associated  with  both  task  difficulty  and  dual-tasking.  Pupillometry  demonstrated  that  participants'  pupils  were  significantly  larger  when  they  were  completing  a  more  difficult  task  and  when  multitasking.  We  propose  that  researchers  interested  in  the  relative  differences  in  working  memory  between  visualizations  should  consider  a  converging  methods  approach,  where  physiological  measures  and  behavioral  measures  of  working  memory  are  employed  to  generate  a  rich  evaluation  of  visualization  effort.
2	Visualizing  a  moving  target  a  design  study  on  task  parallel  programs  in  the  presence  of  evolving  data  and  concerns.  Common  pitfalls  in  visualization  projects  include  lack  of  data  availability  and  the  domain  users'  needs  and  focus  changing  too  rapidly  for  the  design  process  to  complete.  While  it  is  often  prudent  to  avoid  such  projects,  we  argue  it  can  be  beneficial  to  engage  them  in  some  cases  as  the  visualization  process  can  help  refine  data  collection,  solving  a  “chicken  and  egg”  problem  of  having  the  data  and  tools  to  analyze  it.  We  found  this  to  be  the  case  in  the  domain  of  task  parallel  computing  where  such  data  and  tooling  is  an  open  area  of  research.  Despite  these  hurdles,  we  conducted  a  design  study.  Through  a  tightly-coupled  iterative  design  process,  we  built  Atria,  a  multi-view  execution  graph  visualization  to  support  performance  analysis.  Atria  simplifies  the  initial  representation  of  the  execution  graph  by  aggregating  nodes  as  related  to  their  line  of  code.  We  deployed  Atria  on  multiple  platforms,  some  requiring  design  alteration.  We  describe  how  we  adapted  the  design  study  methodology  to  the  “moving  target”  of  both  the  data  and  the  domain  experts'  concerns  and  how  this  movement  kept  both  the  visualization  and  programming  project  healthy.  We  reflect  on  our  process  and  discuss  what  factors  allow  the  project  to  be  successful  in  the  presence  of  changing  data  and  user  needs.
2	Cope  interactive  exploration  of  co  occurrence  patterns  in  spatial  time  series.  Spatial  time  series  is  a  common  type  of  data  dealt  with  in  many  domains,  such  as  economic  statistics  and  environmental  science.  There  have  been  many  studies  focusing  on  finding  and  analyzing  various  kinds  of  events  in  time  series;  the  term  ‘event’  refers  to  significant  changes  or  occurrences  of  particular  patterns  formed  by  consecutive  attribute  values.  We  focus  on  a  further  step  in  event  analysis:  discover  temporal  relationship  patterns  between  event  locations,  i.e.,  repeated  cases  when  there  is  a  specific  temporal  relationship  (same  time,  before,  or  after)  between  events  occurring  at  two  locations.  This  can  provide  important  clues  for  understanding  the  formation  and  spreading  mechanisms  of  events  and  interdependencies  among  spatial  locations.  We  propose  a  visual  exploration  framework  COPE  (Co-Occurrence  Pattern  Exploration),  which  allows  users  to  extract  events  of  interest  from  data  and  detect  various  co-occurrence  patterns  among  them.  Case  studies  and  expert  reviews  were  conducted  to  verify  the  effectiveness  and  scalability  of  COPE  using  two  real-world  datasets.
2	Visualizing  request  flow  comparison  to  aid  performance  diagnosis  in  distributed  systems.  Distributed  systems  are  complex  to  develop  and  administer,  and  performance  problem  diagnosis  is  particularly  challenging.  When  performance  degrades,  the  problem  might  be  in  any  of  the  system's  many  components  or  could  be  a  result  of  poor  interactions  among  them.  Recent  research  efforts  have  created  tools  that  automatically  localize  the  problem  to  a  small  number  of  potential  culprits,  but  research  is  needed  to  understand  what  visualization  techniques  work  best  for  helping  distributed  systems  developers  understand  and  explore  their  results.  This  paper  compares  the  relative  merits  of  three  well-known  visualization  approaches  (side-by-side,  diff,  and  animation)  in  the  context  of  presenting  the  results  of  one  proven  automated  localization  technique  called  request-flow  comparison.  Via  a  26-person  user  study,  which  included  real  distributed  systems  developers,  we  identify  the  unique  benefits  that  each  approach  provides  for  different  problem  types  and  usage  modes.
2	Ambient  occlusion  effects  for  combined  volumes  and  tubular  geometry.  This  paper  details  a  method  for  interactive  direct  volume  rendering  that  computes  ambient  occlusion  effects  for  visualizations  that  combine  both  volumetric  and  geometric  primitives,  specifically  tube-shaped  geometric  objects  representing  streamlines,  magnetic  field  lines  or  DTI  fiber  tracts.  The  algorithm  extends  the  recently  presented  the  directional  occlusion  shading  model  to  allow  the  rendering  of  those  geometric  shapes  in  combination  with  a  context  providing  3D  volume,  considering  mutual  occlusion  between  structures  represented  by  a  volume  or  geometry.  Stream  tube  geometries  are  computed  using  an  effective  spline-based  interpolation  and  approximation  scheme  that  avoids  self-intersection  and  maintains  coherent  orientation  of  the  stream  tube  segments  to  avoid  surface  deforming  twists.  Furthermore,  strategies  to  reduce  the  geometric  and  specular  aliasing  of  the  stream  tubes  are  discussed.
2	Axisketcher  interactive  nonlinear  axis  mapping  of  visualizations  through  user  drawings.  Visual  analytics  techniques  help  users  explore  high-dimensional  data.  However,  it  is  often  challenging  for  users  to  express  their  domain  knowledge  in  order  to  steer  the  underlying  data  model,  especially  when  they  have  little  attribute-level  knowledge.  Furthermore,  users'  complex,  high-level  domain  knowledge,  compared  to  low-level  attributes,  posits  even  greater  challenges.  To  overcome  these  challenges,  we  introduce  a  technique  to  interpret  a  user's  drawings  with  an  interactive,  nonlinear  axis  mapping  approach  called  AxiSketcher.  This  technique  enables  users  to  impose  their  domain  knowledge  on  a  visualization  by  allowing  interaction  with  data  entries  rather  than  with  data  attributes.  The  proposed  interaction  is  performed  through  directly  sketching  lines  over  the  visualization.  Using  this  technique,  users  can  draw  lines  over  selected  data  points,  and  the  system  forms  the  axes  that  represent  a  nonlinear,  weighted  combination  of  multidimensional  attributes.  In  this  paper,  we  describe  our  techniques  in  three  areas:  1)  the  design  space  of  sketching  methods  for  eliciting  users'  nonlinear  domain  knowledge;  2)  the  underlying  model  that  translates  users'  input,  extracts  patterns  behind  the  selected  data  points,  and  results  in  nonlinear  axes  reflecting  users'  complex  intent;  and  3)  the  interactive  visualization  for  viewing,  assessing,  and  reconstructing  the  newly  formed,  nonlinear  axes.
2	Protosteer  steering  deep  sequence  model  with  prototypes.  Recently  we  have  witnessed  growing  adoption  of  deep  sequence  models  (e.g.  LSTMs)  in  many  application  domains,  including  predictive  health  care,  natural  language  processing,  and  log  analysis.  However,  the  intricate  working  mechanism  of  these  models  confines  their  accessibility  to  the  domain  experts.  Their  black-box  nature  also  makes  it  a  challenging  task  to  incorporate  domain-specific  knowledge  of  the  experts  into  the  model.  In  ProtoSteer  (Prototype  Steering),  we  tackle  the  challenge  of  directly  involving  the  domain  experts  to  steer  a  deep  sequence  model  without  relying  on  model  developers  as  intermediaries.  Our  approach  originates  in  case-based  reasoning,  which  imitates  the  common  human  problem-solving  process  of  consulting  past  experiences  to  solve  new  problems.  We  utilize  ProSeNet  (Prototype  Sequence  Network),  which  learns  a  small  set  of  exemplar  cases  (i.e.,  prototypes)  from  historical  data.  In  ProtoSteer  they  serve  both  as  an  efficient  visual  summary  of  the  original  data  and  explanations  of  model  decisions.  With  ProtoSteer  the  domain  experts  can  inspect,  critique,  and  revise  the  prototypes  interactively.  The  system  then  incorporates  user-specified  prototypes  and  incrementally  updates  the  model.  We  conduct  extensive  case  studies  and  expert  interviews  in  application  domains  including  sentiment  analysis  on  texts  and  predictive  diagnostics  based  on  vehicle  fault  logs.  The  results  demonstrate  that  involvements  of  domain  users  can  help  obtain  more  interpretable  models  with  concise  prototypes  while  retaining  similar  accuracy.
2	Efficient  representation  and  optimization  for  tpms  based  porous  structures.  In  this  approach,  we  present  an  efficient  topology  and  geometry  optimization  of  Triply  Periodic  Minimal  Surfaces  (TPMS)  based  porous  shell  structures,  which  can  be  represented,  analyzed,  optimized  and  stored  directly  using  functions.  The  proposed  framework  is  directly  executed  on  functions  instead  of  remeshing  (tetrahedral/hexahedral),  and  this  framework  substantially  improves  the  controllability  and  efficiency.  Specifically,  a  valid  TPMS-based  porous  shell  structure  is  first  constructed  by  function  expressions.  The  porous  shell  permits  continuous  and  smooth  changes  of  geometry  (shell  thickness)  and  topology  (porous  period).  The  porous  structures  also  inherit  several  of  the  advantageous  properties  of  TPMS,  such  as  smoothness,  full  connectivity  (no  closed  hollows),  and  high  controllability.  Then,  the  problem  of  filling  an  object's  interior  region  with  porous  shell  can  be  formulated  into  a  constraint  optimization  problem  with  two  control  parameter  functions.  Finally,  an  efficient  topology  and  geometry  optimization  scheme  is  presented  to  obtain  optimized  scale-varying  porous  shell  structures.  In  contrast  to  traditional  heuristic  methods  for  TPMS,  our  work  directly  optimize  both  the  topology  and  geometry  of  TPMS-based  structures.  Various  experiments  have  shown  that  our  proposed  porous  structures  have  obvious  advantages  in  terms  of  efficiency  and  effectiveness.
2	Virtual  reality  genres  comparing  preferences  in  immersive  experiences  and  games.  Even  though  virtual  reality  (VR)  shares  features  with  video  games,  it  offers  a  wider  range  of  experiences.  There  is  currently  no  cohesive  classification  for  commercial  VR  offerings.  As  a  first  step  to  account  for  this  deficiency,  the  work  in  progress  considers  the  relationship  between  game  genres  and  users?  ratings  and  downloads  of  VR  experiences.  We  found  Action,  Shooter,  and  Simulation  to  be  the  most  frequently  downloaded  genres;  Action  and  Music/Rhythm  the  most  highly  rated;  and  Simulation  and  Music/Rhythm  to  occur  at  a  statistically  higher  rate  in  VR  compared  to  non-VR.  Finally,  we  learned  that  VR  experiences  are  less  likely  to  receive  positive  ratings  than  2D  games.  The  findings  can  inform  developers?  marketing  decisions  based  on  demand.
2	Optimal  camera  placement  for  motion  capture  systems.  Optical  motion  capture  is  based  on  estimating  the  three-dimensional  positions  of  markers  by  triangulation  from  multiple  cameras.  Successful  performance  depends  on  points  being  visible  from  at  least  two  cameras  and  on  the  accuracy  of  the  triangulation.  Triangulation  accuracy  is  strongly  related  to  the  positions  and  orientations  of  the  cameras.  Thus,  the  configuration  of  the  camera  network  has  a  critical  impact  on  performance.  A  poor  camera  configuration  may  result  in  a  low  quality  three-dimensional  (3D)  estimation  and  consequently  low  quality  of  tracking.  This  paper  introduces  and  compares  two  methods  for  camera  placement.  The  first  method  is  based  on  a  metric  that  computes  target  point  visibility  in  the  presence  of  dynamic  occlusion  from  cameras  with  “good”  views.  The  second  method  is  based  on  the  distribution  of  views  of  target  points.  Efficient  algorithms,  based  on  simulated  annealing,  are  introduced  for  estimating  the  optimal  configuration  of  cameras  for  the  two  metrics  and  a  given  distribution  of  target  points.  The  accuracy  and  robustness  of  the  algorithms  are  evaluated  through  both  simulation  and  empirical  measurement.  Implementations  of  the  two  methods  are  available  for  download  as  tools  for  the  community.
2	A  divide  and  conquer  approach  to  quad  remeshing.  Many  natural  and  man-made  objects  consist  of  simple  primitives,  similar  components,  and  various  symmetry  structures.  This  paper  presents  a  divide-and-conquer  quadrangulation  approach  that  exploits  such  global  structural  information.  Given  a  model  represented  in  triangular  mesh,  we  first  segment  it  into  a  set  of  submeshes,  and  compare  them  with  some  predefined  quad  mesh  templates.  For  the  submeshes  that  are  similar  to  a  predefined  template,  we  remesh  them  as  the  template  up  to  a  number  of  subdivisions.  For  the  others,  we  adopt  the  wave-based  quadrangulation  technique  to  remesh  them  with  extensions  to  preserve  symmetric  structure  and  generate  compatible  quad  mesh  boundary.  To  ensure  that  the  individually  remeshed  submeshes  can  be  seamlessly  stitched  together,  we  formulate  a  mixed-integer  optimization  problem  and  design  a  heuristic  solver  to  optimize  the  subdivision  numbers  and  the  size  fields  on  the  submesh  boundaries.  With  this  divider-and-conquer  quadrangulation  framework,  we  are  able  to  process  very  large  models  that  are  very  difficult  for  the  previous  techniques.  Since  the  submeshes  can  be  remeshed  individually  in  any  order,  the  remeshing  procedure  can  run  in  parallel.  Experimental  results  showed  that  the  proposed  method  can  preserve  the  high-level  structures,  and  process  large  complex  surfaces  robustly  and  efficiently.
2	Corneal  imaging  calibration  for  optical  see  through  head  mounted  displays.  In  recent  years  optical  see-through  head-mounted  displays  (OST-HMDs)  have  moved  from  conceptual  research  to  a  market  of  mass-produced  devices  with  new  models  and  applications  being  released  continuously.  It  remains  challenging  to  deploy  augmented  reality  (AR)  applications  that  require  consistent  spatial  visualization.  Examples  include  maintenance,  training  and  medical  tasks,  as  the  view  of  the  attached  scene  camera  is  shifted  from  the  user's  view.  A  calibration  step  can  compute  the  relationship  between  the  HMD-screen  and  the  user's  eye  to  align  the  digital  content.  However,  this  alignment  is  only  viable  as  long  as  the  display  does  not  move,  an  assumption  that  rarely  holds  for  an  extended  period  of  time.  As  a  consequence,  continuous  recalibration  is  necessary.  Manual  calibration  methods  are  tedious  and  rarely  support  practical  applications.  Existing  automated  methods  do  not  account  for  user-specific  parameters  and  are  error  prone.  We  propose  the  combination  of  a  pre-calibrated  display  with  a  per-frame  estimation  of  the  user's  cornea  position  to  estimate  the  individual  eye  center  and  continuously  recalibrate  the  system.  With  this,  we  also  obtain  the  gaze  direction,  which  allows  for  instantaneous  uncalibrated  eye  gaze  tracking,  without  the  need  for  additional  hardware  and  complex  illumination.  Contrary  to  existing  methods,  we  use  simple  image  processing  and  do  not  rely  on  iris  tracking,  which  is  typically  noisy  and  can  be  ambiguous.  Evaluation  with  simulated  and  real  data  shows  that  our  approach  achieves  a  more  accurate  and  stable  eye  pose  estimation,  which  results  in  an  improved  and  practical  calibration  with  a  largely  improved  distribution  of  projection  error.
2	Comparing  visual  interactive  labeling  with  active  learning  an  experimental  study.  Labeling  data  instances  is  an  important  task  in  machine  learning  and  visual  analytics.  Both  fields  provide  a  broad  set  of  labeling  strategies,  whereby  machine  learning  (and  in  particular  active  learning)  follows  a  rather  model-centered  approach  and  visual  analytics  employs  rather  user-centered  approaches  (visual-interactive  labeling).  Both  approaches  have  individual  strengths  and  weaknesses.  In  this  work,  we  conduct  an  experiment  with  three  parts  to  assess  and  compare  the  performance  of  these  different  labeling  strategies.  In  our  study,  we  (1)  identify  different  visual  labeling  strategies  for  user-centered  labeling,  (2)  investigate  strengths  and  weaknesses  of  labeling  strategies  for  different  labeling  tasks  and  task  complexities,  and  (3)  shed  light  on  the  effect  of  using  different  visual  encodings  to  guide  the  visual-interactive  labeling  process.  We  further  compare  labeling  of  single  versus  multiple  instances  at  a  time,  and  quantify  the  impact  on  efficiency.  We  systematically  compare  the  performance  of  visual  interactive  labeling  with  that  of  active  learning.  Our  main  findings  are  that  visual-interactive  labeling  can  outperform  active  learning,  given  the  condition  that  dimension  reduction  separates  well  the  class  distributions.  Moreover,  using  dimension  reduction  in  combination  with  additional  visual  encodings  that  expose  the  internal  state  of  the  learning  model  turns  out  to  improve  the  performance  of  visual-interactive  labeling.
2	Torso  crowds.  We  present  a  novel  dense  crowd  simulation  method.  In  real  crowds  of  high  density,  people  manoeuvring  the  crowd  need  to  twist  their  torso  to  pass  between  others.  Our  proposed  method  does  not  use  the  traditional  disc-shaped  agent,  but  instead  employs  capsule-shaped  agents,  which  enables  us  to  plan  such  torso  orientations.  Contrary  to  other  crowd  simulation  systems,  which  often  focus  on  the  movement  of  the  entire  crowd,  our  method  distinguishes  between  active  agents  that  try  to  manoeuvre  through  the  crowd,  and  passive  agents  that  have  no  incentive  to  move.  We  introduce  the  concept  of  a    focus  point    to  influence  crowd  agent  orientation.  Recorded  data  from  real  human  crowds  are  used  for  validation,  which  shows  that  our  proposed  model  produces  equivalent  paths  for  85  percent  of  the  validation  set.  Furthermore,  we  present  a  character  animation  technique  that  uses  the  results  from  our  crowd  model  to  generate  torso-twisting  and  side-stepping  characters.
2	Computational  phase  modulated  eyeglasses.  We  present  computational  phase-modulated  eyeglasses,  a  see-through  optical  system  that  modulates  the  view  of  the  user  using  phase-only  spatial  light  modulators  (PSLM).  A  PSLM  is  a  programmable  reflective  device  that  can  selectively  retardate,  or  delay,  the  incoming  light  rays.  As  a  result,  a  PSLM  works  as  a  computational  dynamic  lens  device.  We  demonstrate  our  computational  phase-modulated  eyeglasses  with  either  a  single  PSLM  or  dual  PSLMs  and  show  that  the  concept  can  realize  various  optical  operations  including  focus  correction,  bi-focus,  image  shift,  and  field  of  view  manipulation,  namely  optical  zoom.  Compared  to  other  programmable  optics,  computational  phase-modulated  eyeglasses  have  the  advantage  in  terms  of  its  versatility.  In  addition,  we  also  presents  some  prototypical  focus-loop  applications  where  the  lens  is  dynamically  optimized  based  on  distances  of  objects  observed  by  a  scene  camera.  We  further  discuss  the  implementation,  applications  but  also  discuss  limitations  of  the  current  prototypes  and  remaining  issues  that  need  to  be  addressed  in  future  research.
2	Does  helping  hurt  aiming  assistance  and  skill  development  in  a  first  person  shooter  game.  In  multiplayer  First-Person  Shooter  (FPS)  games,  experience  can  suffer  if  players  have  different  skill  levels  --  novices  can  become  frustrated,  and  experts  can  become  bored.  An  effective  solution  to  this  problem  is  aiming-assistance-based  player  balancing,  which  gives  weaker  players  assistance  to  bring  them  up  to  the  level  of  stronger  players.  However,  it  is  unknown  how  assistance  affects  skill  development.  The  guidance  hypothesis  suggests  that  players  will  become  overly  reliant  on  the  assistance  and  will  not  learn  aiming  skills  as  well  as  they  would  without  it.  In  order  to  determine  whether  aiming  assistance  hinders  FPS  skill  development,  we  carried  out  a  study  that  compared  performance  gains  and  experiential  measures  for  an  assisted  group  and  an  unassisted  group,  over  14  game  sessions  over  five  days.  Our  results  show  that  although  aim  assistance  did  significantly  improve  performance  and  perceived  competence  when  it  was  present,  there  were  no  significant  differences  in  performance  gains  or  experiential  changes  between  the  assisted  and  unassisted  groups  (and  on  one  measure,  assisted  players  improved  significantly  more).  These  results  go  against  the  prediction  of  the  guidance  hypothesis,  and  suggest  instead  that  the  value  of  aiming  assistance  outweighs  concerns  about  skill  development  --  removing  one  of  the  remaining  barriers  that  designers  may  see  in  using  player  balancing  techniques.
2	The  first  hour  experience  how  the  initial  play  can  engage  or  lose  new  players.  The  first  time  a  player  sits  down  with  a  game  is  critical  for  their  engagement.  Games  are  a  voluntary  activity  and  easy  to  abandon.  If  the  game  cannot  hold  player  attention,  it  will  not  matter  how  much  fun  the  game  is  later  on  if  the  player  quits  early.  Worse,  if  the  initial  experience  was  odious  enough,  the  player  will  dissuade  others  from  playing.  Industry  advice  is  to  make  the  game  fun  from  the  start  to  hook  the  player.  In  our  analysis  of  over  200  game  reviews  and  interviews  with  industry  professionals,  we  advance  an  alternative,  complementary  solution.  New  design  terminology  is  introduced  such  as  "holdouts"  (what  keeps  players  playing  despite  poor  game  design)  and  the  contrast  between  momentary  fun  vs.  intriguing  experiences.  Instead  of  prioritizing  fun,  we  assert  that  intrigue  and  information  should  be  seen  as  equally  valuable  for  helping  players  determine  if  they  want  to  continue  playing.  The  first  sustained  play  session  (coined  "first  hour"),  when  inspected  closely,  offers  lessons  for  game  development  and  our  understanding  of  how  players  evaluate  games  as  consumable  products.
2	Beam  me  round  scotty  studying  asymmetry  and  interdependence  in  a  prototype  cooperative  game.  In  "Beam  Me  'Round,  Scotty!",  pairs  of  players  engage  with  asymmetric  gameplay  mechanics  and  interfaces  (e.g.  leading  vs.  support,  action  vs.  strategy,  gamepad  vs.  mouse  interaction)  in  a  cooperative  adventure  to  escape  a  hostile  alien  world.  "Beam  Me  'Round,  Scotty!"  presents  a  multi-faceted  play  experience  designed  to  bridge  differences  in  player  skills,  styles,  and  interests.  By  introducing  deliberate  interdependence  through  asymmetry,  different  types  of  players  can  come  together  and  have  fun  overcoming  obstacles,  defeating  enemies,  and  escaping  the  alien  planet  via  their  unique  contributions.
2	Eyeplay  applications  for  gaze  in  games.  What  new  challenges  does  the  combination  of  games  and  eye-tracking  present?  The  EyePlay  workshop  brings  together  researchers  and  industry  specialists  from  the  fields  of  eye-tracking  and  games  to  address  this  question.  Eye-tracking  been  investigated  extensively  in  a  variety  of  domains  in  human-computer  Interaction,  but  little  attention  has  been  given  to  its  application  for  gaming.  As  eye-tracking  technology  is  now  an  affordable  commodity,  its  appeal  as  a  sensing  technology  for  games  is  set  to  become  the  driving  force  for  novel  methods  of  player-computer  interaction  and  games  evaluation.  This  workshop  presents  a  forum  for  eye-based  gaming  research,  with  a  focus  on  identifying  the  opportunities  that  eye-tracking  brings  to  games  design  and  research,  on  plotting  the  landscape  of  the  work  in  this  area,  and  on  formalising  a  research  agenda  for  EyePlay  as  a  field.  Possible  topics  are,  but  not  limited  to,  novel  interaction  techniques  and  game  mechanics,  usability  and  evaluation,  accessibility,  learning,  and  serious  games  contexts.
2	Hide  and  seek  exploring  interaction  with  smart  wallpaper.  Displays  are  getting  larger,  thinner,  and  are  consuming  less  power.  The  logical  conclusion  of  this  trend  is  a  future  in  which  wall-size  displays  are  common  in  homes,  a  concept  we  have  described  as  "smart  wallpaper"  for  the  purposes  of  evaluating  its  HCI  implications.  What  new  kinds  of  experience  could  smart  wallpaper  provide?  This  paper  describes  a  case  study  on  one  potential  smart  wallpaper  experience:  an  interactive  children's  game.  Our  results  suggest  that  children  and  their  families  see  substantial  value  in  both  the  game  and  the  concept  of  smart  wallpaper.  They  valued  the  immersion,  the  physical  activity  and  the  shared  nature  of  the  experience.  On  the  basis  of  this  study,  we  conclude  that  there  is  substantial  potential  for  wall-size  displays  to  enable  valuable  experiences  for  children  and  their  families,  and  that  smartphones  can  be  used  as  an  intuitive  means  of  interacting  with  them.
2	Magia  transformo  designing  for  mixed  reality  transformative  play.  This  work-in-progress  paper  presents  a  first  look  at  Magia  Transformo:  a  mixed  reality  game  for  transformative  play.  Drawing  on  techniques  of  backleading  and  outside->in  transformation  from  theater  practice,  Magia  Transformo  uses  costumes,  props,  sets,  narration,  and  physical  enactment  to  transform  the  bodies  and  environment  of  its  players.  We  argue  that  these  design  techniques,  although  still  in  their  early  development,  can  broadly  inform  the  work  of  designers  and  scholars  seeking  to  create  experiences  of  character  identification  and  transformation  in  games.
2	Beyond  human  animals  as  an  escape  from  stereotype  avatars  in  virtual  reality  games.  Virtual  reality  setups  are  particularly  suited  to  create  a  tight  bond  between  players  and  their  avatars  up  to  a  degree  where  we  start  perceiving  the  virtual  representation  as  our  own  body.  We  hypothesize  that  such  an  illusion  of  virtual  body  ownership  (IVBO)  has  a  particularly  high,  yet  overlooked  potential  for  nonhumanoid  avatars.  To  validate  our  claim,  we  use  the  example  of  three  very  different  creatures---a  scorpion,  a  rhino,  and  a  bird---to  explore  possible  avatar  controls  and  game  mechanics  based  on  specific  animal  abilities.  A  quantitative  evaluation  underpins  the  high  game  enjoyment  arising  from  embodying  such  nonhuman  morphologies,  including  additional  body  parts  and  obtaining  respective  superhuman  skills,  which  allows  us  to  derive  a  set  of  novel  design  implications.  Furthermore,  the  experiment  reveals  a  correlation  between  IVBO  and  game  enjoyment,  which  is  a  further  indication  that  nonhumanoid  creatures  offer  a  meaningful  design  space  for  VR  games  worth  further  investigation.
2	Towards  experiencing  eating  as  a  form  of  play.  There  is  an  increasing  trend  in  interaction  design  to  engage  with  food.  We  note  that  most  prior  work  targets  instrumental  benefits  (for  example  see  food  tracking  apps  to  manage  nutritional  intake).  In  contrast,  in  this  article,  we  highlight  the  potential  of  technology  to  support  eating  as  a  form  of  play.  We  reflect  on  our  own  work  to  articulate  two  design  strategies  for  game  designers  on  how  they  can  facilitate  playful  eating  experiences  using  novel  technologies.  Ultimately,  with  our  work,  we  aim  to  facilitate  a  more  playful  engagement  around  the  way  we  eat.
2	Increasing  donating  behavior  through  a  game  for  change  the  role  of  interactivity  and  appreciation.  Games  for  change  have  attracted  the  interest  of  humanitarian  aid  organizations  and  researchers  alike.  However,  their  effectiveness  to  promote  behavior  such  as  donating  remains  unclear.  Furthermore,  little  is  known  about  how  key  game  properties  interactivity  and  presentation  mode  impact  the  effectiveness  of  these  games,  or  how  player  attitudes  and  experiences  relate  to  the  interplay  between  game  properties  and  donating  behavior.  In  this  study,  experimental  conditions  were  systematically  varied  in  their  interactivity  and  presentation  mode.  Thereby,  234  participants  played,  watched,  or  read  through  one  of  six  variations  of  the  narrative  of  the  game  Darfur  is  Dying.  Following  this,  they  were  asked  to  choose  the  percentage  of  an  unexpected  bonus  to  donate  to  a  charity.  While  interactivity  increased  donating  by  an  average  of  12%,  presentation  mode  had  no  significant  impact  on  the  percentage  donated.  Thus,  between  presentation  mode  and  interactivity,  interactivity  was  found  to  be  the  more  impactful  game  property.  Moreover,  appreciation  fully  mediated  the  relationship  between  interactivity  and  donating,  hinting  at  its  relevance  for  the  evaluation  of  the  effectiveness  of  games  for  change.
2	Tai  chi  in  the  clouds  using  micro  uav  s  to  support  tai  chi  practice.  Tai  Chi  uses  smooth  movement  and  a  focussed  state  of  mind  to  support  mental  and  physical  health.  Tai  Chi  teachers  use  metaphoric  imagery  such  as  "wave  hands  like  clouds"  to  help  students  integrate  smooth  movements  with  a  focussed  mind.  Current  interactive  technologies  applied  to  Tai  Chi  take  a  very  literal  approach,  focussing  on  body  position  and  centre  of  gravity.  In  contrast,  "Tai  Chi  In  The  Clouds"  is  a  system  which  uses  micro  unmanned  aerial  vehicles  (UAVs)  as  "clouds"  to  lead  or  follow  the  movements  of  the  hands,  giving  live  feedback  on  smoothness  of  movement  via  LEDs.  We  used  UAVs  to  aid  the  experience  of  living  out  the  metaphoric  imagery  used  in  Tai  Chi.  With  our  work  we  aim  to  contribute  to  new  design  language  to  support  movement  based,  mind-body  practices.
2	Spotcheck  an  efficient  defense  against  information  exposure  cheats.  A  lot  of  hidden  information  is  present  in  client  programs  of  most  existing  online  multi-player  games.  This  hidden  information  is  necessary  for  clients  to  render  a  player's  view  of  the  game.  However,  the  same  hidden  information  can  be  exploited  by  cheaters  to  gain  an  unfair  advantage  over  other  players.  Eliminating  hidden  information  from  the  game  client  comes  at  a  significant  cost  to  the  server,  since  it  must  now  send  the  data  required  to  render  a  client's  view  on-demand.  Consequently,  the  burden  of  tracking  a  player's  view  shifts  from  the  client  to  the  server,  hindering  scalability  and  degrading  game  performance.  We  propose  SpotCheck,  a  more  scalable  approach  for  detecting  information  exposure  cheats.  The  key  idea  is  that  servers  still  disseminate  game  state  information  on-demand,  but  clients  retain  the  burden  of  tracking  a  player's  view.  After  each  move,  clients  must  submit  a  descriptor  pertaining  to  the  player's  view.  The  server  then  randomly  chooses  to  validate  the  descriptor  and  sends  back  relevant  game  state  information.  Our  experimental  results  show,  that  SpotCheck  can  reduce  the  server  CPU  overhead  by  as  much  as  half  when  compared  to  the  alternative,  while  still  being  an  effective  defense  against  information  exposure  cheats.
2	Cooperation  and  interdependence  how  multiplayer  games  increase  social  closeness.  Games  have  long  been  used  as  a  bonding  activity;  however,  research  on  establishing  and  maintaining  social  closeness  through  games  uses  different  terms,  different  mechanics  and  controls,  and  different  contexts  of  use.  As  a  result,  designers  have  little  guidance  on  which  multiplayer  game  mechanics  are  most  effective.  We  summarize  literature  on  game  design  for  social  closeness  into  a  framework  with  two  collaborative  mechanics:  cooperation  and  interdependence.  We  then  created  four  versions  of  an  online  game  to  independently  test  the  effects  of  these  game  mechanics  on  relationship  formation  between  paired  online  strangers,  showing  that  cooperation  and  interdependence  are  two  distinct  factors  that  both  can  be  used  to  improve  play  experience  and  increase  social  bonds.  Additionally,  we  unpack  the  effect  of  interdependence,  showing  that  the  improved  social  closeness  can  be  explained  by  the  increase  in  conversational  turns.
2	Systematic  review  and  validation  of  the  game  experience  questionnaire  geq  implications  for  citation  and  reporting  practice.  Despite  lacking  a  formal  peer-reviewed  publication,  the  Game  Experience  Questionnaire  (GEQ)  is  widely  applied  in  games  research,  which  might  risk  the  proliferation  of  erroneous  study  implications.  This  concern  motivated  us  to  conduct  a  systematic  literature  review  of  73  publications,  analysing  how  and  why  the  GEQ  and  its  variants  have  been  employed  in  current  research.  Besides  inconsistent  reporting  of  psychometric  properties,  we  found  that  misleading  citation  practices  with  regards  to  the  source,  rationale  and  number  of  items  reported  were  prevalent,  which  in  part  seem  to  stem  from  confusion  over  the  "manuscript  in  preparation"  status.  Additionally,  we  present  the  results  of  a  validation  study  (N  =  633),  which  found  no  evidence  for  the  originally  postulated  7-factor  structure  of  the  GEQ.  Based  on  these  findings,  we  discuss  the  challenges  inherent  to  the  "manuscript  in  preparation"  status  and  provide  recommendations  for  authors,  researchers,  educators,  and  reviewers  on  how  to  improve  reporting,  citation  and  publication  practices.
2	Stochastic  modeling  of  light  weight  floating  objects.  Light-weight  objects  floating  inside  a  flow  play  a  significant  role  in  the  liveliness  of  our  world  (e.g.,  leaves,  dust,  snowflakes,  bubbles).  They  follow  the  flow  and  show  complex  and  chaotic  motion.  First,  animators  usually  add  simple  random  noise  to  the  streaming  path.  However,  this  method  yields  low-quality  floating  behavior  since  the  random  noise  does  not  take  into  account  the  spatial  and  temporal  distribution  of  underlying  flow  turbulence.  For  example,  obstacle-induced  oscillation  cannot  be  easily  created  as  a  major  source  of  the  unique  motion.  Second,  floating  objects  can  be  passively  advected  by  flow  velocities  from  physically-based  simulation.  A  critical  challenge  is  that  modeling  the  important  jiggling  motion  requires  turbulent  flow  field,  which  is  hard  to  achieve  by  direct  numerical  simulation  (DNS)  due  to  limited  computational  resources  and  numerical  dissipation.  This  situation  deteriorates  severely  when  realtime  performance  and  interactivity  are  demanded  in  a  3D  gaming  environment.  Moreover,  the  floating  motion  has  intrinsic  stochastic  nature,  i.e.,  the  repeated  executions  result  in  nonidentical  dynamics,  which  is  not  achievable  with  deterministic  simulation.  Third,  adding  noise  to  fluid  solvers  can  introduce  chaotic  flow  velocities  (e.g.  [Pfaff  et  al.  2010]).  In  an  approach,  special  Langevin  particles  affect  fluid  solver  with  forces  [Chen  et  al.  2011].  However,  such  methods  rely  on  DNS  and  apply  chaotic  addition  to  the  whole  fluid  domain,  which  is  inefficient  in  handling  a  group  of  floating  objects  inside.
2	Attention  and  selection  in  online  choice  tasks.  The  task  of  selecting  one  among  several  items  in  a  visual  display  is  extremely  common  in  daily  life  and  is  executed  billions  of  times  every  day  on  the  Web.  Attention  is  vital  for  selection,  but  the  end-to-end  process  of  what  draws  and  sustains  attention,  and  how  that  influences  selection,  remains  poorly  understood.  We  study  this  in  a  complex  multi-item  selection  setting,  where  participants  selected  one  among  eight  news  articles  presented  in  a  grid  layout  on  a  screen.  By  varying  the  position,  saliency,  and  topic  of  the  news  items,  we  identify  the  relative  importance  of  these  visual  and  semantic  factors  in  attention  and  selection.  We  present  a  simple  model  of  attention  that  predicts  many  key  features  such  as  attention  shifts  and  dwell  time  per  item.  Potential  applications  of  our  findings  include  optimizing  visual  displays  to  drive  user  attention.
2	Eliciting  affective  recommendations  to  support  distance  learning  students.  Affective  support  can  be  provided  through  personalized  recommendations  integrated  within  learning  management  systems  (LMS).  We  have  applied  the  TORMES  user  centered  engineering  approach  to  involve  educators  in  a  recommendation  elicitation  process  in  a  distance  learning  (DL)  context.
2	Making  it  game  like  topolor  2  and  gamified  social  e  learning.  This  paper  briefly  introduces  Topolor  2,  a  social  personalised  adaptive  e-learning  environment  with  novel  gamification  features,  aiming  at  reducing  undesirable  'noise'  effects  of  social  interaction  and  at  further  improving  the  learning  experience.  The  goal  of  this  paper  is  to  showcase  the  main  gamified  social  interaction  features.
2	Integrating  context  similarity  with  sparse  linear  recommendation  model.  Context-aware  recommender  systems  extend  traditional  recommender  systems  by  adapting  their  output  to  users’  specific  contextual  situations.  Most  of  the  existing  approaches  to  context-aware  recommendation  involve  directly  incorporating  context  into  standard  recommendation  algorithms  (e.g.,  collaborative  filtering,  matrix  factorization).  In  this  paper,  we  highlight  the  importance  of  context  similarity  and  make  the  attempt  to  incorporate  it  into  context-aware  recommender.  The  underlying  assumption  behind  is  that  the  recommendation  lists  should  be  similar  if  their  contextual  situations  are  similar.  We  integrate  context  similarity  with  sparse  linear  recommendation  model  to  build  a  similarity-learning  model.  Our  experimental  evaluation  demonstrates  that  the  proposed  model  is  able  to  outperform  several  state-of-the-art  context-aware  recommendation  algorithms  for  the  top-N  recommendation  task.
2	Scrutable  user  models  and  personalised  item  recommendation  in  mobile  lifestyle  applications.  This  paper  presents  our  work  on  supporting  scrutable  user  models  for  use  in  mobile  applications  that  provide  personalised  item  recommendations.  In  particular,  we  describe  a  mobile  lifestyle  application  in  the  fine-dining  domain,  designed  to  recommend  meals  at  a  particular  restaurant  based  on  a  person’s  user  model.  The  contributions  of  this  work  are  three-fold.  First  is  the  mobile  application  and  its  personalisation  engine  for  item  recommendation  using  a  content  and  critique-based  hybrid  recommender.  Second,  we  illustrate  the  control  and  scrutability  that  a  user  has  in  configuring  their  user  model  and  browsing  a  content  list.  Thirdly,  this  is  validated  in  a  user  experiment  that  illustrates  how  new  digital  features  may  revolutionise  the  way  that  paper-based  systems  (like  restaurant  menus)  currently  work.  Although  this  work  is  based  on  restaurant  menu  recommendations,  its  approach  to  scrutability  and  mobile  client-side  personalisation  carry  across  to  a  broad  class  of  commercial  applications.
2	Rating  based  preference  elicitation  for  recommendation  of  stress  intervention.  In  recent  years,  recommender  systems  have  emerged  as  a  key  component  for  personalization  in  health  applications.  Central  in  the  development  of  recommender  systems  is  rating-based  preference  elicitation,  based  both  on  single-criterion  and  multi-criteria  rating.  Though  its  use  has  already  been  studied  in  various  domains  of  recommender  systems,  far  too  little  attention  has  been  paid  to  preference  elicitation  in  health  recommender  systems~(HRS).  The  purpose  of  this  paper  is  to  develop  a  better  understanding  of  this  preference  elicitation  by  studying  the  criteria  that  users  consider  when  they  rate  a  health  promotion  recommendation  from  HRS,  and  accordingly,  to  offer  a  design  solution  as  a  functional  feedback  model  for  mobile  health  applications.  This  paper  investigates  the  user-perceived  importance  of  various  criteria,  as  well  as  latent  factors  for  eliciting  user  feedback  on  the  recommendations.  It  also  reports  the  relationship  of  explanation  and  trust  to  the  overall  rating.  By  aggregating  a  list  of  all  possible  criteria,  we  further  discover  that  not  all  criteria  are  equally  important  to  users,  and  that  the  effectiveness  of  a  recommendation  plays  a  dominant  role.
2	Who  shares  fake  news  in  online  social  networks.  Today  more  and  more  people  use  social  networks  and  so  the  differences  in  personalities  of  users  become  more  diversified.  The  same  holds  true  for  available  news  content.  To  test  if  regular  news  and  fake  news  are  distributed  similarly  and  to  what  extent  this  depends  on  the  personality  and  behavior  of  individuals,  we  conducted  a  mixed-method  study.  Through  an  online  questionnaire  we  measured  personality  traits  of  individuals  in  social  networks,  how  they  behave,  and  how  they  are  connected  to  each  other.  Using  this  data,  we  developed  an  agent-based  model  of  an  online  social  network.  Using  our  model,  an  average  of  92%  of  regular  news  and  98%  of  fake  news  were  disseminated  to  the  whole  network.  Network  density  turned  out  to  be  more  important  for  dissemination  than  the  differences  in  personality  and  behavior  of  individuals.  Thus  the  spread  of  fake  news  can  not  only  be  addressed  by  focusing  on  the  personality  of  individual  users  and  their  associated  behavior.  Systemic  approaches---integrating  both  human  and  algorithm---must  be  considered  to  effectively  combat  fake  news.
2	Providing  control  and  transparency  in  a  social  recommender  system  for  academic  conferences.  A  social  recommender  system  aims  to  provide  useful  suggestion  to  the  user  and  prevent  social  overload  problem.  Most  of  the  research  efforts  are  spent  on  push  high  relevant  item  on  top  of  the  ranked  list,  using  a  weight  ensemble  approach.  However,  we  argue  the  ``learned''  static  fusion  is  not  enough  to  specific  contexts.  In  this  paper,  we  develop  a  series  visual  recommendation  components  and  control  panel  for  the  user  to  interact  with  the  recommendation  result  of  an  academic  conference.  The  system  offers  a  better  recommendation  transparency  and  user-driven  fusion  through  recommended  sources.  The  experiment  result  shows  the  user  did  fuse  the  different  recommended  sources  and  exploration  patterns  among  tasks.  The  post-study  survey  is  positively  associated  with  the  system  and  explanation  function  effectiveness.  This  finding  shed  light  on  the  future  research  of  design  a  recommender  system  with  human  intervention  and  the  interface  beyond  the  static  ranked  list.
2	Personalization  of  parliamentary  document  retrieval  using  different  user  profiles.  Owing  to  the  information  overload  we  are  faced  with  nowadays,  personalization  approaches  are  becoming  almost  a  must,  in  order  to  provide  relevant  information  for  users.  These  personalization  techniques  retrieve  results  closer  to  the  user  interests  and  preferences,  by  using  the  information  stored  in  the  user  profile.  We  have  carried  out  a  comparative  study  between  six  different  user  profile  representation  approaches,  based  on  the  content  of  the  documents  of  the  Andalusian  Parliament,  obtaining  quite  good  personalized  performance  results  and  some  interesting  conclusions  about  the  goodnesses  of  these  content-based  approaches.
2	A  two  stage  item  recommendation  method  using  probabilistic  ranking  with  reconstructed  tensor  model.  In  a  tag-based  recommender  system,  the  multi-dimensional      correlation  should  be  modeled  effectively  for  finding  quality  recommendations.  Recently,  few  researchers  have  used  tensor  models  in  recommendation  to  represent  and  analyze  latent  relationships  inherent  in  multi-dimensions  data.  A  common  approach  is  to  build  the  tensor  model,  decompose  it  and,  then,  directly  use  the  reconstructed  tensor  to  generate  the  recommendation  based  on  the  maximum  values  of  tensor  elements.  In  order  to  improve  the  accuracy  and  scalability,  we  propose  an  implementation  of  the  n-mode  block-striped  (matrix)  product  for  scalable  tensor  reconstruction  and  probabilistically  ranking  the  candidate  items  generated  from  the  reconstructed  tensor.  With  testing  on  real-world  datasets,  we  demonstrate  that  the  proposed  method  outperforms  the  benchmarking  methods  in  terms  of  recommendation  accuracy  and  scalability.
2	Visual  variables  in  adaptive  visualizations.  Visualizations  provide  various  variables  for  the  adaptation  to  the  usage  context  and  the  users.  Today’s  adaptive  visualizations  make  use  of  various  visual  variables  to  order  or  filter  information  or  visualizations.  However,  the  capabilities  of  visual  variables  in  context  of  human  information  processing  and  tasks  are  not  comprehensively  exploited.  This  paper  discusses  the  value  of  the  different  visual  variables  providing  beneficial  and  more  accurately  adapted  information  visualizations.
2	Employing  relation  between  reading  and  writing  skills  on  age  based  categorization  of  short  estonian  texts.  In  this  paper,  we  present  results  of  our  study  on  age-based  categorization  of  short  texts  as  85  words  per  author.  We  introduce  a  novel  set  of  features  that  will  reliably  work  with  short  texts,  and  is  easy  to  extract  from  the  text  itself  without  any  outside  databases.  These  features  were  formerly  known  as  variables  in  readability  formulas.  We  tested  datasets  presented  two  age  groups  children  and  teens  up  to  age  15  and  adults  20  years  and  older.  Besides  readability  features,  we  also  tested  widely  used  n-gram  features.  Models  trained  on  readability  features  performed  better  or  as  well  as  models  trained  on  n-gram  features.  Model  generated  by  Support  Vector  Machine  with  readability  features  yield  to  f-score  0.953.
2	Incorporating  student  response  time  and  tutor  instructional  interventions  into  student  modeling.  Bayesian  Knowledge  Tracing  (BKT)  is  one  of  the  most  widely  adopted  student-modeling  methods.  It  uses  performance  (incorrect,correct)  to  infer  student  knowledge  state  (unlearned,  learned).  However,  performance  can  be  noisy  and  thus  we  explored  another  type  of  observations  --  student  response  time.  Furthermore,  we  proposed  Intervention  Bayesian  Knowledge  Tracing  (Intervention-BKT)  which  can  incorporate  multiple  types  of  instructional  interventions  into  the  conventional  BKT  model.  Our  results  show  that  for  next-step  performance  predictions,  Intervention-BKT  is  more  effective  than  BKT;  whereas  to  predict  students'  post-test  scores,  including  student  response  time  would  yield  better  result  than  using  performance  alone.
2	What  makes  an  image  tagger  fair.  Image  analysis  algorithms  have  been  a  boon  to  personalization  in  digital  systems  and  are  now  widely  available  via  easy-to-use  APIs.  However,  it  is  important  to  ensure  that  they  behave  fairly  in  applications  that  involve  processing  images  of  people,  such  as  dating  apps.  We  conduct  an  experiment  to  shed  light  on  the  factors  influencing  the  perception  of  "fairness."  Participants  are  shown  a  photo  along  with  two  descriptions  (human-  and  algorithm-generated).  They  are  then  asked  to  indicate  which  is  "more  fair"  in  the  context  of  a  dating  site,  and  explain  their  reasoning.  We  vary  a  number  of  factors,  including  the  gender,  race  and  attractiveness  of  the  person  in  the  photo.  While  participants  generally  found  human-generated  tags  to  be  more  fair,  API  tags  were  judged  as  being  more  fair  in  one  setting  -  where  the  image  depicted  an  "attractive,"  white  individual.  In  their  explanations,  participants  often  mention  accuracy,  as  well  as  the  objectivity/subjectivity  of  the  tags  in  the  description.  We  relate  our  work  to  the  ongoing  conversation  about  fairness  in  opaque  tools  like  image  tagging  APIs,  and  their  potential  to  result  in  harm.
2	Predicting  emotions  from  multimodal  users  data.  Prediction  of  emotions  is  important  for  understanding  human  be-havior  and  modeling  users  in  learning  environments.  In  this  paper,we  present  a  deep  multi-modal  architecture  for  emotions  predic-tion,  which  takes  advantage  of  deep  learning,  user  multimodal  dataand  the  hierarchy  of  human  memory.  The  architecture  consists  ofthe  combination  of  Long  Short-Term  memory  (LSTMs).  One  of  thenovelty  of  our  approach  is  that,  we  enhance  the  LSTM  with  anexplicit  memory  since  in  brain  studies,  the  memory  is  often  dividedinto  two  further  main  types:  explicit  (or  declarative)  memory  andimplicit  (or  procedural)  memory,  the  last  one  being  the  main  pur-pose  of  LSTMs  architectures.  The  resulting  model  has  been  testedon  a  public  multi-modal  dataset.
2	User  model  in  a  box  cross  system  user  model  transfer  for  resolving  cold  start  problems.  Recommender  systems  face  difficulty  in  cold-start  scenarios  where  a  new  user  has  provided  only  few  ratings.  Improving  cold-start  performance  is  of  great  interest.  At  the  same  time,  the  growing  number  of  adaptive  systems  makes  it  ever  more  likely  that  a  new  user  in  one  system  has  already  been  a  user  in  another  system  in  related  domains.  To  what  extent  can  a  user  model  built  by  one  adaptive  system  help  address  a  cold  start  problem  in  another  system?  We  compare  methods  of  cross-system  user  model  transfer  across  two  large  real-life  systems:  we  transfer  user  models  built  for  information  seeking  of  scientific  articles  in  the  SciNet  exploratory  search  system,  operating  over  tens  of  millions  of  articles,  to  perform  cold-start  recommendation  of  scientific  talks  in  the  CoMeT  talk  management  system,  operating  over  hundreds  of  talks.  Our  user  study  focuses  on  transfer  of  novel  explicit  open  user  models  curated  by  the  user  during  information  seeking.  Results  show  strong  improvement  in  cold-start  talk  recommendation  by  transferring  open  user  models,  and  also  reveal  why  explicit  open  models  work  better  in  cross-domain  context  than  traditional  hidden  implicit  models.
2	Opinion  driven  matrix  factorization  for  rating  prediction.  Rating  prediction  is  a  well-known  recommendation  task  aiming  to  predict  a  user’s  rating  for  those  items  which  were  not  rated  yet  by  her.  Predictions  are  computed  from  users’  explicit  feedback,  i.e.  their  ratings  provided  on  some  items  in  the  past.  Another  type  of  feedback  are  user  reviews  provided  on  items  which  implicitly  express  users’  opinions  on  items.  Recent  studies  indicate  that  opinions  inferred  from  users’  reviews  on  items  are  strong  predictors  of  user’s  implicit  feedback  or  even  ratings  and  thus,  should  be  utilized  in  computation.  As  far  as  we  know,  all  the  recent  works  on  recommendation  techniques  utilizing  opinions  inferred  from  users’  reviews  are  either  focused  on  the  item  recommendation  task  or  use  only  the  opinion  information,  completely  leaving  users’  ratings  out  of  consideration.  The  approach  proposed  in  this  paper  is  filling  this  gap,  providing  a  simple,  personalized  and  scalable  rating  prediction  framework  utilizing  both  ratings  provided  by  users  and  opinions  inferred  from  their  reviews.  Experimental  results  provided  on  a  dataset  containing  user  ratings  and  reviews  from  the  real-world  Amazon  Product  Review  Data  show  the  effectiveness  of  the  proposed  framework.
2	Susceptibility  to  persuasive  strategies  a  comparative  analysis  of  nigerians  vs  canadians.  Personalizing  persuasive  technologies  (PTs)  is  one  of  the  hallmarks  of  a  successful  PT  intervention.  However,  there  is  a  lack  of  understanding  of  how  Africans  and  North  Americans  differ  or  are  similar  in  the  susceptibility  to  persuasive  strategies.  To  bridge  this  gap,  we  conducted  a  cross-cultural  study  among  284  subjects  to  investigate  the  moderating  effect  of  culture  on  the  susceptibility  of  users  to  Cialdini's  principles  of  persuasion.  Specifically,  using  Nigeria  and  Canada  as  a  case  study,  we  investigated  how  both  groups  vary  in  their  levels  of  susceptibility  to  Authority,  Commitment,  Consensus,  Liking,  Reciprocity  and  Scarcity.  The  results  of  our  analysis  show  that  Nigerians  are  more  susceptible  to  Authority  and  Scarcity  than  Canadians,  while  Canadians  are  more  susceptible  to  Reciprocity,  Liking  and  Consensus  than  Nigerians.  However,  both  groups  do  not  differ  with  respect  to  Commitment  (the  most  persuasive  strategy).  Finally,  we  discussed  our  findings  and  mapped  the  most  persuasive  Cialdini's  principles  in  each  group  to  implementable  persuasive  strategies  in  the  PT  domain.
2	Beyond  algorithmic  fairness  in  recommender  systems.  Fairness  is  one  of  the  crucial  aspects  of  modern  Recommender  Systems  which  has  recently  drawn  substantial  attention  from  the  community.  Many  recent  works  have  addressed  this  aspect  by  studying  the  fairness  of  the  recommendation  through  different  forms  of  evaluation  methodologies  and  metrics.  However,  the  majority  of  these  works  have  mainly  concentrated  on  the  recommendation  algorithms  and  hence  measured  the  fairness  from  the  algorithmic  viewpoint.  While  such  viewpoint  may  still  play  an  important  role,  it  does  not  necessarily  project  a  comprehensive  picture  of  how  the  users  may  perceive  the  overall  fairness  of  a  recommender  system.  This  paper  extends  the  prior  works  and  goes  beyond  the  algorithmic  fairness  in  recommender  systems  by  highlighting  the  non-algorithmic  viewpoint  on  the  fairness  in  these  systems.  The  paper  proposes  an  evaluation  methodology  that  can  be  used  to  assess  the  fairness  of  a  recommender  system  perceived  by  its  users.  We  have  adopted  a  well-known  model  and  re-formulated  it  to  suit  the  particular  characteristics  of  the  recommender  systems,  and  accordingly,  their  corresponding  users.  Our  proposed  methodology  can  be  used  in  order  to  elicit  the  feedback  of  the  users,  along  with  three  important  dimensions,  i.e.,  Engagement,  Representation,  and  Action  &  Expression.  We  have  formed  a  set  of  survey  questions  that  address  the  aforementioned  dimensions,  as  a  set  of  examples  to  assess  the  fairness  in  a  recommender  system.
2	Can  animated  agents  help  us  create  better  conversational  moods  an  experiment  on  the  nature  of  optimal  conversations.  We  describe  a  method  using  animated  agents  to  investigate  how  humans  recognize  conversational  moods.  Conversational  moods  are  usually  generated  by  conversation  participants  through  verbal  cues  as  well  as  nonverbal  cues,  such  as  facial  expressions,  eye  movements,  and  nods.  Identifying  specific  rules  of  conversational  moods  would  enable  us  to  construct  conversational  robots  and  agents  that  are  not  only  able  to  converse  naturally,  but  pleasantly  and  excitedly.  Additionally,  these  robots  and  agents  would  be  able  to  assist  us  with  proper  action  to  generate  improved  conversational  moods  in  different  situations.  We  propose  methods  for  developing  agents  that  can  help  improve  the  quality  of  our  conversations  and  facilitate  greater  enjoyment  of  life.
2	User  interfaces  for  older  adults.  Needs  and  wishes  regarding  the  interaction  with  ICT  solutions  change  over  time  and  vary  between  older  adults.  They  depend  on  the  user's  physical  and  mental  capabilities  and  preferences.  Thus  the  user  interface,  which  is  considered  critical  to  the  success  or  failure  of  an  ICT  product  or  service,  should  be  adaptable.  AALuis  provides  an  open  middleware  layer  to  guarantee  accessible  and  usable  user  interfaces  for  Ambient  Assisted  Living  services.  The  general  idea  is  to  foster  a  detachment  of  the  user  interface  from  the  service  and  its  functionality,  respectively.  Furthermore  an  input  fusion  and  output  fission  regarding  I/O  modalities  based  on  the  user's  preferences  is  striven  for.  At  the  heart  of  AALuis  lays  a  dynamically  adapted,  personalized  interaction  between  an  older  adult  and  the  service,  with  various  I/O  devices.  The  first  results  of  the  project  look  promising  to  achieve  flexibility  in  the  creation  and  usage  of  interfaces.  The  chosen  approach  allows  further  developments  expanding  the  functionalities  and  improving  the  generated  user  interfaces.
2	Myosl  a  framework  for  measuring  usability  of  two  arm  gestural  electromyography  for  sign  language.  Several  Sign  Language  (SL)  systems  have  been  developed  using  various  technologies:  Kinect,  armbands,  and  gloves.  Majority  of  these  studies  never  considered  user  experience  as  part  of  their  approach.  With  that,  we  propose  a  new  framework  that  eases  usability  by  employing  two-arm  gestural  electromyography  instead  of  typical  vision-based  systems  see  Fig.  5.  Interactions  can  be  considered  seamless  and  natural  with  this  way.  In  this  preliminary  study,  we  conducted  focus  group  discussions  and  usability  tests  with  signers.  Based  on  the  results  of  the  usability  tests,  90%  of  respondents  found  the  armband  comfortable.  The  respondents  also  stated  that  the  armband  was  not  intrusive  when  they  tried  to  perform  their  sign  gestures.  At  the  same  time,  they  found  it  aesthetically  pleasing.  Additionally,  we  produced  an  initial  prototype  from  this  experiment  setup  and  tested  them  on  several  conversational  scenarios.  By  using  this  approach,  we  enable  an  agile  framework  that  caters  the  needs  of  the  signer-user.
2	Using  pupil  size  variation  during  visual  emotional  stimulation  in  measuring  affective  states  of  non  communicative  individuals.  In  this  paper  we  describe  an  exploratory  experiment  conducted  in  the  early  stages  of  the  design  and  development  of  a  screening  program  for  non-communicative  individuals.  The  system  incorporates  eye-tracking  and  biosensor  technologies  for  objectively  measuring  affective  states  by  combining  measures  of  ElectroDermal  Activity  (EDA),  temperature  and  eye  gaze  metrics  of  pupil  dilation.  The  objective  is  to  provide  individuals  with  cognitive  disabilities  who  experience  difficulties  in  expressing  their  feelings  with  an  alternate  form  of  communicating  their  emotional  states  in  clinical  screening  and  occupational  therapy  sessions.  The  interactive  computer-based  screening  program  is  comprised  of  stimuli-sets  designed  to  elicit  emotional  response  from  viewers  with  a  trend  analysis  component  designed  with  an  interface  for  health  care  providers.  The  focus  of  this  paper  is  the  feasibility  of  including  pupil  dilation  as  a  measure  reflecting  affective  states  of  individuals  in  the  overall  emotional  intelligence  screening  system.
2	Canihelp  a  platform  for  inclusive  collaboration.  Technology  plays  a  key  role  in  daily  life  of  people  with  special  needs,  being  a  mean  of  integration  or  even  communication  with  society.  By  built  up  experience,  we  find  that  support  tools  play  a  crucial  part  in  empowerment  of  persons  with  special  needs  and  small  advances  may  represent  shifts  and  opportunities.  The  diversity  of  solutions  and  the  need  for  dedicated  hardware  to  each  feature  represents  a  barrier  to  its  use,  compromising  the  success  of  the  solutions  against,  among  others,  problems  of  usability  and  scale.  This  paper  aims  to  explore  the  concept  of  inclusive  collaboration  to  enhance  the  mutual  interaction  and  assistance.  The  proposed  approach  combines  and  generalizes  the  usage  of  human  computation  in  a  collaborative  environment  with  assistive  technologies  creating  redundancy  and  complementarity  in  the  solutions  provided,  contributing  to  enhance  the  quality  of  life  of  people  with  special  needs  and  the  elderly.  The  CanIHelp  platform  is  an  embodiment  of  the  concept  as  a  result  from  an  orchestrated  model  using  mechanisms  of  collective  intelligence  through  social  inclusion  initiatives.  The  platform  features  up  for  integrating  assistive  technologies,  collaborative  tools  and  multiple  multimedia  communication  channels,  accessible  through  multimodal  interfaces  for  universal  access.  A  discussion  of  the  impacts  of  fostering  collaboration  and  broadening  from  the  research  concepts  to  the  societal  impacts  is  presented.  As  final  remarks  a  set  of  future  research  challenges  and  guidelines  are  identified.
2	Are  internet  and  social  network  usage  associated  with  wellbeing  and  social  inclusion  of  seniors  the  third  age  online  survey  on  digital  media  use  in  three  european  countries.  Research  on  the  psychosocial  effects  of  Internet  and  social  network  usage  in  seniors  is  either  contradictory  or  sparse.  As  part  of  the  Third  Age  Online  project,  this  paper  reports  a  cross-sectional  survey  conducted  in  Germany,  the  Netherlands  and  in  Switzerland.  The  survey,  utilizing  regression  analysis,  examined  whether  or  not  social  inclusion  and  mental  wellbeing  were  predictors  of  Internet  usage  and  social  network  usage.  Results  showed  that  social  inclusion  variables  were  associated  with  both  Internet  usage  and  social  network  usage.  Internet  usage  was  associated  with  respondents  who  were  both  less  and  more  socially  included.  Mental  wellbeing  was  positively  related  to  Internet  usage  but  not  to  social  network  usage.  In  further  studies,  longitudinal  designs  are  needed  to  reveal  the  directions  of  causality  between  Internet/social  network  usage  and  mental  wellbeing/social  inclusion.
2	Collaborative  editing  for  all  the  google  docs  example.  Collaborative  software  tools  allow  people  to  share  documents  and  knowledge  via  Internet,  in  a  simple,  economic  and  efficient  way.  Unfortunately  collaborative  software  often  relies  heavily  on  visual  features  and  dynamic  technologies  with  user  interfaces  that  are  difficult  to  use  via  screen  reader,  or  are  sometimes  even  inaccessible  for  the  blind.    In  this  paper  we  illustrate  and  discuss  results  of  an  accessibility  inspection  of  the  main  collaborative  functions  of  Google  Docs  using  the  JAWS  screen  reader.  Results  highlight  several  difficulties  encountered  when  interacting  with  elements  of  the  Google  Docs  interfaces.  Content  perception  is  often  incomplete,  since  many  elements  or  changes  occurring  in  the  collaborative  environment  are  not  intercepted  by  the  screen  reader  and  announced  to  the  user.  In  addition,  the  behavior  of  the  collaborative  functions  analyzed  (as  well  as  the  rendering)  changes  from  one  web  browser  to  another.  Some  general  guidelines  are  discussed,  for  designing  user  interfaces  of  collaborative  editors  that  are  more  usable  when  interacting  via  screen  reader.
2	Metagnostic  deductive  question  answering  with  explanation  from  texts.  The  present  paper  presents  a  system  called  AMYNTAS  for  "metagnostic"  deductive  question  answering  from  texts.  This  system  can  logically  combine  information  from  texts  and  answer  questions  generating  explanations  for  its  operation  exhibiting  "self-awareness".  The  deductions  are  performed  directly  with  the  natural  language  text  without  previous  translation  into  a  formal  representation.  The  "metagnostic"  effect  is  accomplished  by  representing  and  processing  the  state  of  linguistic  processing  and  reasoning  of  the  system.  The  system  is  implemented  in  Prolog  and  uses  a  text  grammar  to  parse  sentences  that  contain  the  information  being  sought.  The  system  uses  reasoning  rules,  lexicon,  ontology,  prerequisite  knowledge  and  the  history  of  its  state.  The  system  may  easily  be  adapted  to  completely  different  domains  such  as  biomedical  texts  and  texts  of  the  proofs  of  theorems  of  Euclidean  geometry.  An  evaluation  performed  with  real  sentences  from  these  two  completely  different  domains  gave  satisfactory  results  of  accuracy  and  facility  of  domain  adaptation.
2	Immersive  journalism  in  vr  four  theoretical  domains  for  researching  a  narrative  design  framework.  A  major  focus  of  research  in  Virtual  Reality  (VR)  media  examines  the  technological  affordances  for  creating  immersion,  which  in  turn  can  generate  presence  –  the  feeling  of  being  there  –  in  a  virtual  environment.  This  research  has  given  rise  to  an  emerging  form  of  fact-based  storytelling  called  immersive  journalism,  a  term  used  to  describe  digitally  produced  stories  designed  to  provide  a  first-person,  interactive  experience  with  news  events.  This  paper  examines  the  concept  of  immersive  journalism  and  discusses  both  its  potential  and  its  limitations  as  a  narrative  and  journalistic  genre.  Immersive  journalism  will  require  a  new  narrative  design  framework,  and  four  theoretical  domains  are  discussed  as  underscoring  this  framework.  The  four  are  VR  presence,  narrative,  cognition  and  journalistic  ethics.
2	An  asymmetric  bimanual  gestural  interface  for  immersive  virtual  environments.  In  this  paper,  a  3D  bimanual  gestural  interface  using  data  gloves  is  presented.  We  build  upon  past  contributions  on  gestural  interfaces  and  bimanual  interactions  to  create  an  efficient  and  intuitive  gestural  interface  that  can  be  used  in  a  wide  variety  of  immersive  virtual  environments.  Based  on  real  world  bimanual  interactions,  the  proposed  interface  uses  the  hands  in  an  asymmetric  style,  with  the  left  hand  providing  the  mode  of  interaction  and  the  right  hand  acting  on  a  finer  level  of  detail.  To  validate  the  efficiency  of  this  interface  design,  a  comparative  study  between  the  proposed  two-handed  interface  and  a  one-handed  variant  was  conducted  on  a  group  of  right-handed  users.  The  results  of  the  experiment  support  the  bimanual  interface  as  more  efficient  than  the  unimanual  one.  It  is  expected  that  this  interface  and  the  conclusions  drawn  from  the  experiments  will  be  useful  as  a  guide  for  efficient  design  of  future  bimanual  gestural  interfaces.
2	A  new  approach  for  indoor  navigation  using  semantic  webtechnologies  and  augmented  reality.  Indoor  navigation  is  an  important  research  topic  nowadays.  The  complexity  of  larger  buildings,  supermarkets,  museums,  etc.  makes  it  necessary  to  use  applications  which  can  facilitate  the  orientation.  While  for  outdoor  navigation  already  exist  tried  and  tested  solutions,  but  few  reliable  ones  are  available  for  indoor  navigation.  In  this  paper  we  investigate  the  possible  technologies  for  indoor  navigation.  Then,  we  present  a  general,  cost  effective  system  as  a  solution.  This  system  uses  the  advantages  of  semantic  web  to  store  data  and  to  compute  the  possible  paths  as  well.  Furthermore  it  uses  Augmented  Reality  techniques  and  map  view  to  provide  interaction  with  the  users.  We  made  a  prototype  based  on  client-server  architecture.  The  server  runs  in  a  cloud  and  provides  the  appropriate  data  to  the  client,  which  can  be  a  smartphone  or  a  tablet  with  Android  operation  system.
2	A  mark  up  language  and  interpreter  for  interactive  scenes  for  embodied  conversational  agents.  Our  research  seeks  to  provide  embodied  conversational  agents  (ECAs)  with  behaviors  that  enable  them  to  build  and  maintain  rapport  with  human  users.  To  conduct  this  research,  we  need  to  build  agents  and  systems  that  can  maintain  high  levels  of  engagement  with  humans  over  multiple  interaction  sessions.  These  sessions  can  potentially  extend  to  longer  periods  of  time  to  examine  long-term  effects  of  the  virtual  agent’s  behaviors.  Our  current  ECA  interacts  with  humans  in  a  game  called  “Survival  on  Jungle  Island.”  Throughout  this  game,  users  interact  with  our  agent  across  several  scenes.  Each  scene  is  composed  of  a  collection  of  speech  input,  speech  output,  gesture  input,  gesture  output,  scenery,  triggers,  and  decision  points.  Our  prior  system  was  developed  with  procedural  code,  which  did  not  lend  itself  to  rapid  extension  to  new  game  scenes.  So  to  enable  effective  authoring  of  the  scenes  for  the  “Jungle”  game,  we  adopted  a  declarative  approach.  We  developed  ECA  middleware  that  parses,  interprets,  and  executes  XML  files  that  define  the  scenes.  This  paper  presents  the  XML  coding  scheme  and  its  implementation  and  describes  the  functional  back-end  enabled  by  the  scene  scripts.
2	Sign  search  and  sign  synthesis  made  easy  to  end  user  the  paradigm  of  building  a  sl  oriented  interface  for  accessing  and  managing  educational  content.  Accessibility  of  electronic  content  by  deaf  and  hard-of-hearing  WWW  users  is  crucially  depending  on  the  possibility  to  acquire  information  that  can  be  presented  in  their  native  sign  language  (SL),  from  the  vast  amounts  of  text  sources  being  constantly  uploaded.  Similarly  crucial  is  the  ability  to  easily  create  new  electronic  content  that  can  enable  dynamic  message  exchange,  covering  various  communication  needs.
2	Designing  accessible  visualizations  the  case  of  designing  a  weather  map  for  blind  users.  Major  strides  have  been  made  to  improve  the  accessibility  of  text-based  documents  for  blind  users,  however,  visualizations  still  remain  largely  inaccessible.  The  AISP  framework  represents  an  attempt  to  streamline  the  design  process  by  aligning  the  information  seeking  behaviors  of  a  blind  user  with  those  of  a  sighted  user  utilizing  auditory  feedback.  With  the  recent  popularity  of  touch-based  devices,  and  the  overwhelming  success  of  the  talking  tactile  tablet,  we  therefore  suggest  that  the  AISP  framework  be  extended  to  include  the  sense  of  touch.  This  research-in-progress  paper  proposes  such  an  extended  design  framework,  MISD.  In  addition,  the  article  also  presents  the  preliminary  work  done  in  designing  an  accessible  weather  map  based  on  our  theory-driven  design.  A  discussion  and  an  outline  of  future  work  conclude  the  manuscript.
2	Cognitive  impact  evaluation  of  multimodal  interfaces  for  blind  people  towards  a  systematic  review.  Visual  disability  has  a  major  impact  on  people’s  quality  of  life.  Although  there  are  many  technologies  to  assist  people  who  are  blind,  most  of  them  do  not  necessarily  guarantee  the  effectiveness  of  the  intended  use.  Then,  we  have  conducted  a  systematic  literature  review  concerning  the  cognitive  impact  evaluation  of  multimodal  interfaces  for  blind  people.  We  report  in  this  paper  the  preliminary  results  of  the  systematic  literature  review  with  the  purpose  of  understanding  how  the  cognitive  impact  is  currently  evaluated  when  using  multimodal  interfaces  for  blind  people.  Among  twenty-five  papers  retrieved  from  the  systematic  review,  we  found  a  high  diversity  of  experiments.  Some  of  them  do  not  present  the  data  clearly  and  do  not  apply  a  statistical  method  to  guarantee  the  results.  Besides  this,  other  points  related  to  the  experiments  are  analyzed.  We  conclude  that  there  is  a  need  to  better  plan  and  present  data  from  experiments  on  technologies  for  cognition  of  blind  people.  Moreover,  as  the  next  step  in  this  research,  we  will  investigate  these  preliminary  results  with  a  qualitative  analysis.
2	Behavioural  intentions  of  using  virtual  reality  in  learning  perspectives  of  acceptance  of  information  technology  and  learning  style.  The  use  of  virtual  reality  (VR)  has  become  a  viable  alternative  to  conventional  learning  methods  in  various  knowledge  domains.  Wearable  head-mounted  displays  (HMDs)  are  devices  that  provide  users  with  an  immersive  VR  experience.  To  investigate  the  direct  determinants  affecting  students’  reasons  for  HMD  use  in  learning,  hypotheses  relating  to  information  technology  acceptance  and  Kolb’s  learning  styles  were  proposed  and  tested  in  this  study.  Participants  were  recruited  through  stratified  random  sampling  according  to  the  population  ratio  of  colleges  at  a  university  in  Taiwan.  Students  were  shown  a  video  on  VR  applications  in  learning,  after  which  an  online  survey  was  completed.  In  total,  387  questionnaires  were  collected  of  which  376  were  valid.  An  inference  analysis  of  the  samples  was  performed  by  structural  equation  modelling  with  eight  exogenous  latent  variables,  namely  the  four  constructs  of  the  unified  theory  of  acceptance  and  use  of  technology  (UTAUT)  and  the  four  modes  of  Kolb’s  learning  styles.  All  eight  variables  pointed  to  one  endogenous  latent  variable:  behavioural  intention.  The  results  showed  all  four  constructs  of  the  UTAUT  to  have  a  positive  and  significant  effect  on  students’  behavioural  intention  to  use  HMDs  in  learning  and  only  the  concrete  experience  mode  of  Kolb’s  learning  styles  to  have  a  positive  and  significant  effect.  Based  on  these  findings,  this  study  provides  suggestions  on  how  to  encourage  HMDs  use  in  learning  to  VR  developers  and  educational  institutions.
2	Experimental  evaluation  of  a  coplanar  airborne  separation  display.  Two  experiments,  an  active  conflict  resolution  task  and  a  passive  situation  awareness  assessment,  were  conducted  that  compared  two  versions  of  a  constraint-based  coplanar  airborne  separation  assistance  display.  A  baseline  display  showed  a  maneuver  space  based  on  2-D  projections  of  traffic  and  performance  constraints.  A  second  augmented  display  also  incorporated  cutting  planes  that  take  the  dimension  orthogonal  to  the  projection  into  account,  thereby  providing  a  more  precise  visualization  of  traffic  constraints.  Results  showed  that  although  pilots  performed  well  with  either  display,  the  augmented  display  scored  consistently  better  in  terms  of  performance,  efficiency  of  conflict  resolutions,  the  amount  of  errors  in  the  initial  resolutions,  and  the  level  of  situation  awareness  compared  with  the  baseline  display.  On  the  other  hand,  more  losses  of  separation  were  found  with  the  augmented  display,  as  pilots  tried  to  maximize  the  maneuvering  efficiency  according  to  the  precision  with  which  constraints  were  visualized.
2	Ar  image  generation  using  view  dependent  geometry  modification  and  texture  mapping.  Augmented  reality  (AR)  applications  often  require  virtualized  real  objects,  i.e.,  virtual  objects  that  are  built  based  on  real  objects  and  rendered  from  an  arbitrary  viewpoint.  In  this  paper,  we  propose  a  method  for  real  object  virtualization  and  AR  image  generation  based  on  view-dependent  geometry  modification  and  texture  mapping.  The  proposed  method  is  a  hybrid  of  model-  and  image-based  rendering  techniques  that  uses  multiple  input  images  of  the  real  object  as  well  as  the  object's  three-dimensional  (3D)  model  obtained  by  an  automatic  3D  reconstruction  technique.  Even  with  state-of-the-art  technology,  the  reconstructed  3D  model's  accuracy  can  be  insufficient,  resulting  in  such  visual  artifacts  as  false  object  boundaries.  The  proposed  method  generates  a  depth  map  from  a  3D  model  of  a  virtualized  real  object  and  expands  its  region  in  the  depth  map  to  remove  the  false  object  boundaries.  Since  such  expansion  reveals  the  background  pixels  in  the  input  images,  which  is  particularly  undesirable  for  AR  applications,  we  preliminarily  extract  object  regions  and  use  them  for  texture  mapping.  With  our  GPU  implementation  for  real-time  AR  image  generation,  we  experimentally  demonstrated  that  using  expanded  geometry  reduces  the  number  of  required  input  images  and  maintains  visual  quality.
2	A  review  of  the  evidence  for  training  effectiveness  with  virtual  reality  technology.  Prior  to  adopting  new  technologies  for  training,  evaluations  must  be  executed  to  demonstrate  their  benefit.  Specifically,  the  appeal  of  virtual  reality  has  led  to  applications  across  domains.  While  many  evaluations  have  been  conducted  on  their  effectiveness,  there  has  yet  been  a  review  to  summarize  and  categorize  the  evidence  on  training  outcomes.  To  assess  the  benefits  these  new  technologies  may  bring  to  the  trainee,  a  review  of  the  research  on  the  training  effectiveness  with  virtual  reality  (VR)  technology  that  was  conducted.  The  goal  for  this  review  was  to  take  a  domain-agnostic  perspective  to  identify  the  knowledge,  skills,  and  abilities  (KSAs)  that  have  been  trained  effectively  or  enhanced  with  the  use  of  VR.  This  review  searched  the  related  literature  within  multiple  databases  and  found  publications  that  met  the  search  criteria  from  1992  to  2019.  A  discussion  of  previous  VR  training  reviews  is  first  presented,  followed  by  an  in-depth  evaluation  of  the  literature  that  met  the  inclusion  criteria.  Three  distinct  categories  of  KSAs  were  identified  consistently:  psychomotor  performance,  knowledge  acquisition,  and  spatial  ability.  Recommendations  to  support  achievement  of  training  outcomes  utilizing  VR  training  systems  are  provided.
2	Holoscopic  3d  microgesture  recognition  by  deep  neural  network  model  based  on  viewpoint  images  and  decision  fusion.  Finger  microgestures  have  been  widely  used  in  human  computer  interaction  (HCI),  particularly  for  interactive  applications,  such  as  virtual  reality  (VR)  and  augmented  reality  (AR)  technologies,  to  provide  immersive  experience.  However,  traditional  2D  image-based  microgesture  recognition  suffers  from  low  accuracy  due  to  the  limitations  of  2D  imaging  sensors,  which  have  no  depth  information.  In  this  article,  we  proposed  an  innovative  3D  microgesture  recognition  system  based  on  a  holoscopic  3D  imaging  sensor.  Due  to  the  lack  of  holoscopic  3D  datasets,  a  comprehensive  holoscopic  3D  microgesture  (HoMG)  database  is  created  and  used  to  develop  a  robust  3D  microgesture  recognition  method.  Then,  a  fast  algorithm  is  proposed  to  extract  multiviewpoint  images  from  one  holoscopic  image.  Furthermore,  we  applied  a  CNN  model  with  an  attention-based  residual  block  to  each  viewpoint  image  to  improve  the  algorithm  performance.  Finally,  bagging  classification  tree  decision-level  fusion  is  applied  to  combine  the  predictions.  The  experimental  results  demonstrate  that  the  proposed  method  outperforms  state-of-the-art  methods  and  delivers  a  better  accuracy  than  existing  methods.
2	Modeling  human  guidance  behavior  based  on  patterns  in  agent  environment  interactions.  This  paper  presents  the  foundations  for  the  analysis  and  modeling  of  human  guidance  behavior  that  is  based  on  the  emergent  patterns  in  the  closed-loop  agent-environment  dynamics.  The  central  hypothesis  is  that  these  patterns,  which  can  be  explained  in  terms  of  invariants  inherent  to  the  closed-loop  dynamics,  provide  the  building  blocks  for  the  organization  of  human  guidance  behavior.  The  concept  of  interaction  patterns  is  first  introduced  using  a  toy  example  and  then  detailed  formally  using  dynamical  system  and  control  principles.  This  paper  then  demonstrates  the  existence  and  significance  of  interaction  patterns  in  human  guidance  behavior  that  is  based  on  data  collected  using  guidance  experiments  with  a  miniature  helicopter.  The  results  confirm  that  human  guidance  behavior  indeed  exhibits  invariances  as  defined  by  interaction  patterns.  The  trajectories  that  are  associated  with  each  interaction  pattern  are  then  further  decomposed  by  applying  piecewise  linear  identification.  The  resulting  elements  are  then  combined  under  a  hierarchical  model  that  provides  a  natural  and  formal  description  of  human  guidance  behavior.
2	Eye  gaze  tracking  with  a  web  camera  in  a  desktop  environment.  This  paper  addresses  the  eye  gaze  tracking  problem  using  a  low  cost  and  more  convenient  web  camera  in  a  desktop  environment,  as  opposed  to  gaze  tracking  techniques  requiring  specific  hardware,  e.g.,  infrared  high-resolution  camera  and  infrared  light  sources,  as  well  as  a  cumbersome  calibration  process.  In  the  proposed  method,  we  first  track  the  human  face  in  a  real-time  video  sequence  to  extract  the  eye  regions.  Then,  we  combine  intensity  energy  and  edge  strength  to  obtain  the  iris  center  and  utilize  the  piecewise  eye  corner  detector  to  detect  the  eye  corner.  We  adopt  a  sinusoidal  head  model  to  simulate  the  3-D  head  shape,  and  propose  an  adaptive  weighted  facial  features  embedded  in  the  pose  from  the  orthography  and  scaling  with  iterations  algorithm,  whereby  the  head  pose  can  be  estimated.  Finally,  the  eye  gaze  tracking  is  accomplished  by  integration  of  the  eye  vector  and  the  head  movement  information.  Experiments  are  performed  to  estimate  the  eye  movement  and  head  pose  on  the  BioID  dataset  and  pose  dataset,  respectively.  In  addition,  experiments  for  gaze  tracking  are  performed  in  real-time  video  sequences  under  a  desktop  environment.  The  proposed  method  is  not  sensitive  to  the  light  conditions.  Experimental  results  show  that  our  method  achieves  an  average  accuracy  of  around      $1.28  ^\circ$      without  head  movement  and      $2.27  ^\circ$      with  minor  movement  of  the  head.
2	Hand  gesture  recognition  using  multiple  acoustic  measurements  at  wrist.  This  article  investigates  the  use  of  acoustic  signals  recorded  at  the  human  wrist  for  hand  gesture  recognition.  The  prototype  consists  of  40  microphones  to  be  worn  at  the  wrist.  The  gesture  recognition  performance  is  evaluated  through  the  identification  of  36  gestures  in  American  sign  language  (ASL),  including  26  ASL  alphabetical  characters  and  10  ASL  numbers.  The  optimal  area  for  sensor  band  placement  (distal/proximal)  is  examined  to  reveal  the  location  of  the  highest  discrimination  accuracy.  Ten  subjects  are  recruited  to  perform  over  ten  trials  for  each  set  of  hand  gestures.  Using  mutual  information-based  feature  selection  methods,  two  time-domain  features:  difference  absolute  mean  value  and  log-energy  entropy,  is  selected  from  35  commonly  used  time-domain  features  for  the  hand  gesture  classification.  As  a  proof-of-concept,  two  common  classification  methods,  linear  discriminant  analysis  and  support  vector  machine,  is  used  to  classify  hand  gestures  from  the  acoustic  measurements  recorded  by  each  subject.  Results  show  the  intrasubject  average  classification  accuracy  above  90%  using  the  two  features  with  all  40  microphones,  while  the  average  classification  accuracy  exceeding  84%  is  obtained  using  ten  microphones.  These  results  indicate  that  acoustic  signatures  from  the  human  wrist  can  be  utilized  for  hand  gesture  recognition,  while  the  use  of  few,  simple  features,  with  low  computational  requirements  is  sufficient  to  characterize  some  hand  gestures.  The  proposed  technique  demonstrates  a  promising  means  for  developing  a  low-cost  wearable  hand  gesture  recognition  device  using  microphones.
2	The  semaine  database  annotated  multimodal  records  of  emotionally  colored  conversations  between  a  person  and  a  limited  agent.  SEMAINE  has  created  a  large  audiovisual  database  as  a  part  of  an  iterative  approach  to  building  Sensitive  Artificial  Listener  (SAL)  agents  that  can  engage  a  person  in  a  sustained,  emotionally  colored  conversation.  Data  used  to  build  the  agents  came  from  interactions  between  users  and  an  "operator”  simulating  a  SAL  agent,  in  different  configurations:  Solid  SAL  (designed  so  that  operators  displayed  an  appropriate  nonverbal  behavior)  and  Semi-automatic  SAL  (designed  so  that  users'  experience  approximated  interacting  with  a  machine).  We  then  recorded  user  interactions  with  the  developed  system,  Automatic  SAL,  comparing  the  most  communicatively  competent  version  to  versions  with  reduced  nonverbal  skills.  High  quality  recording  was  provided  by  five  high-resolution,  high-framerate  cameras,  and  four  microphones,  recorded  synchronously.  Recordings  total  150  participants,  for  a  total  of  959  conversations  with  individual  SAL  characters,  lasting  approximately  5  minutes  each.  Solid  SAL  recordings  are  transcribed  and  extensively  annotated:  6-8  raters  per  clip  traced  five  affective  dimensions  and  27  associated  categories.  Other  scenarios  are  labeled  on  the  same  pattern,  but  less  fully.  Additional  information  includes  FACS  annotation  on  selected  extracts,  identification  of  laughs,  nods,  and  shakes,  and  measures  of  user  engagement  with  the  automatic  system.  The  material  is  available  through  a  web-accessible  database.
2	Experience  driven  procedural  content  generation.  Procedural  content  generation  (PCG)  is  an  increasingly  important  area  of  technology  within  modern  human-computer  interaction  (HCI)  design.  Personalization  of  user  experience  via  affective  and  cognitive  modeling,  coupled  with  real-time  adjustment  of  the  content  according  to  user  needs  and  preferences  are  important  steps  toward  effective  and  meaningful  PCG.  Games,  Web  2.0,  interface,  and  software  design  are  among  the  most  popular  applications  of  automated  content  generation.  The  paper  provides  a  taxonomy  of  PCG  algorithms  and  introduces  a  framework  for  PCG  driven  by  computational  models  of  user  experience.  This  approach,  which  we  call  Experience-Driven  Procedural  Content  Generation  (EDPCG),  is  generic  and  applicable  to  various  subareas  of  HCI.  We  employ  games  as  an  example  indicative  of  rich  HCI  and  complex  affect  elicitation,  and  demonstrate  the  approach's  effectiveness  via  dissimilar  successful  studies.
2	Virtual  character  facial  expressions  influence  human  brain  and  facial  emg  activity  in  a  decision  making  game.  We  examined  the  effects  of  the  emotional  facial  expressions  of  a  virtual  character  (VC)  on  human  frontal  electroencephalographic  (EEG)  asymmetry  (putatively  indexing  approach/withdrawal  motivation),  facial  electromyographic  (EMG)  activity  (emotional  expressions),  and  social  decision  making  (cooperation/defection).  In  a  within-subjects  design,  the  participants  played  the  Iterated  Prisoner's  Dilemma  game  with  VCs  with  different  dynamic  facial  expressions  (predefined  or  dependent  on  the  participant's  electrodermal  and  facial  EMG  activity).  In  general,  VC  facial  expressions  elicited  congruent  facial  muscle  activity.  However,  both  frontal  EEG  asymmetry  and  facial  EMG  activity  elicited  by  an  angry  VC  facial  expression  varied  as  a  function  of  preceding  interactional  events  (human  collaboration/defection).  Pre-decision  inner  emotional-motivational  processes  and  emotional  facial  expressions  were  dissociated,  suggesting  that  human  goals  influence  pre-decision  frontal  asymmetry,  whereas  display  rules  may  affect  (pre-decision)  emotional  expressions  in  human-VC  interaction.  An  angry  VC  facial  expression,  high  pre-decision  corrugator  EMG  activity,  and  relatively  greater  left  frontal  activation  predicted  the  participant's  decision  to  defect.  Both  post-decision  frontal  asymmetry  and  facial  EMG  activity  were  related  to  reciprocal  cooperation.  The  results  suggest  that  the  justifiability  of  VC  emotional  expressions  and  the  perceived  fairness  of  VC  actions  influence  human  emotional  responses.
2	Multimodal  spatiotemporal  representation  for  automatic  depression  level  detection.  Physiological  studies  have  shown  that  there  are  some  differences  in  speech  and  facial  activities  between  depressive  and  healthy  individuals.  Based  on  this  fact,  we  propose  a  novel  Spatio-Temporal  Attention  (STA)  network  and  a  Multimodal  Attention  Feature  Fusion  (MAFF)  strategy  to  obtain  the  multimodal  representation  of  depression  cues  for  predicting  the  individual  depression  level.  Specifically,  we  firstly  divide  the  speech  amplitude  spectrum/video  into  fixed-length  segments  and  input  these  segments  into  the  STA  network,  which  not  only  integrates  the  spatial  and  temporal  information  through  attention  mechanism,  but  also  emphasizes  the  audio/video  frames  related  to  depression  detection.  The  audio/video  segment-level  feature  is  obtained  from  the  output  of  the  last  full  connection  layer  of  the  STA  network.  Secondly,  this  paper  employs  the  eigen  evolution  pooling  method  to  summarize  the  changes  of  each  dimension  of  the  audio/video  segment-level  features  to  aggregate  them  into  the  audio/video  level  feature.  Thirdly,  the  multimodal  representation  with  modal  complementary  information  is  generated  using  the  MAFF  and  inputs  into  the  support  vector  regression  predictor  for  estimating  depression  severity.  Experimental  results  on  the  AVEC2013  and  AVEC2014  depression  databases  illustrate  the  effectiveness  of  our  method.
2	Estimating  affective  taste  experience  using  combined  implicit  behavioral  and  neurophysiological  measures.  We  trained  a  model  to  distinguish  an  extreme  high  arousal,  unpleasant  drink  from  regular  drinks  based  on  a  range  of  implicit  behavioral  and  physiological  responses  to  naturalistic  tasting.  The  trained  model  predicted  arousal  ratings  of  regular  drinks,  highlighting  the  possibility  to  estimate  affective  experience  without  having  to  rely  on  subjective  ratings.
2	Prospect  theoretic  modeling  of  customer  affective  cognitive  decisions  under  uncertainty  for  user  experience  design.  In  order  to  incorporate  both  affective  and  cognitive  factors  in  the  decision-making  process,  a  user  experience  (UX)  evaluation  function  based  on  cumulative  prospect  theory  is  proposed  for  three  different  affective  states  and  two  different  types  of  products  (affect-rich  versus  affect-poor).  In  order  to  tackle  multiple  parameters  involved  in  the  UX  evaluation  function,  a  hierarchical  Bayesian  model  is  proposed  with  a  technique  called  “Markov  chain  Monte  Carlo.”  It  estimates  parameters  that  represent  different  cognitive  tendencies  and  affective  influences  for  customers  at  the  individual  and  group  levels  by  generating  posterior  probability  density  functions  of  the  parameters  to  incorporate  inherent  uncertainty.  An  experiment  with  four  hypotheses  was  designed  to  test  the  proposed  model.  We  found  that:  1)  anxious  participants  tend  to  be  more  risk-averse  than  those  in  joy  and  excitement;  2)  joyful  and  excited  participants  tend  to  be  more  risk-seeking  than  those  in  anxiety  in  UX-related  choice  decision  making;  3)  all  participants  tend  to  be  averse  to  unpleasant  UX;  and  4)  participants  tend  to  value  by  feeling  for  affect-rich  products  and  value  by  calculation  for  affect-poor  products.  Furthermore,  the  models  of  five  different  types  can  predict  choice  decision  making  between  product  profiles  with  around  80%  accuracy.  In  summary,  the  results  explain  affective-cognitive  decision-making  behavior  in  the  complex  domain  of  UX  design  and,  thus,  illustrate  the  potential  and  feasibility  of  the  proposed  method.
2	Web  design  attributes  in  building  user  trust  satisfaction  and  loyalty  for  a  high  uncertainty  avoidance  culture.  In  this  study,  we  attempt  to  evaluate  the  user  preferences  for  web  design  attributes  (i.e.,  typography,  color,  content  quality,  interactivity,  and  navigation)  to  determine  the  trust,  satisfaction,  and  loyalty  for  uncertainty  avoidance  cultures.  Content  quality  and  navigation  have  been  observed  as  strong  factors  in  building  user  trust  with  e-commerce  websites.  In  contrast,  interactivity,  color,  and  typography  have  been  observed  as  strong  determinants  of  user  satisfaction.  The  most  relevant  and  interesting  finding  is  related  to  typography,  which  has  been  rarely  discussed  in  e-commerce  literature.  A  questionnaire  was  designed  to  collect  data  to  corroborate  the  proposed  model  and  hypotheses.  Furthermore,  the  partial  least-squares  method  was  adopted  to  analyze  the  collected  data  from  the  students  who  participated  in  the  test  (    $n$      =  558).  Finally,  the  results  of  this  study  provide  strong  support  to  the  proposed  model  and  hypotheses.  Therefore,  all  the  web  design  attributes  were  observed  as  important  design  features  to  develop  user  trust  and  satisfaction  for  uncertainty  avoidance  cultures.  Although  both  factors  seem  to  be  relevant,  the  relationship  between  trust  and  loyalty  was  observed  to  be  stronger  than  between  satisfaction  and  loyalty;  thus,  trust  seems  to  be  a  stronger  determinant  of  loyalty  for  risk/high  uncertainty  avoidance  cultures.
2	Deep  models  for  engagement  assessment  with  scarce  label  information.  Task  engagement  is  defined  as  loadings  on  energetic  arousal  (affect),  task  motivation,  and  concentration  (cognition)    [1]  .  It  is  usually  challenging  and  expensive  to  label  cognitive  state  data,  and  traditional  computational  models  trained  with  limited  label  information  for  engagement  assessment  do  not  perform  well  because  of  overfitting.  In  this  paper,  we  proposed  two  deep  models  (i.e.,  a  deep  classifier  and  a  deep  autoencoder)  for  engagement  assessment  with  scarce  label  information.  We  recruited  15  pilots  to  conduct  a  4-h  flight  simulation  from  Seattle  to  Chicago  and  recorded  their  electroencephalograph  (EEG)  signals  during  the  simulation.  Experts  carefully  examined  the  EEG  signals  and  labeled  20  min  of  the  EEG  data  for  each  pilot.  The  EEG  signals  were  preprocessed  and  power  spectral  features  were  extracted.  The  deep  models  were  pretrained  by  the  unlabeled  data  and  were  fine-tuned  by  a  different  proportion  of  the  labeled  data  (top  1%,  3%,  5%,  10%,  15%,  and  20%)  to  learn  new  representations  for  engagement  assessment.  The  models  were  then  tested  on  the  remaining  labeled  data.  We  compared  performances  of  the  new  data  representations  with  the  original  EEG  features  for  engagement  assessment.  Experimental  results  show  that  the  representations  learned  by  the  deep  models  yielded  better  accuracies  for  the  six  scenarios  (77.09%,  80.45%,  83.32%,  85.74%,  85.78%,  and  86.52%),  based  on  different  proportions  of  the  labeled  data  for  training,  as  compared  with  the  corresponding  accuracies  (62.73%,  67.19%,  73.38%,  79.18%,  81.47%,  and  84.92%)  achieved  by  the  original  EEG  features.  Deep  models  are  effective  for  engagement  assessment  especially  when  less  label  information  was  used  for  training.
2	A  text  driven  conversational  avatar  interface  for  instant  messaging  on  mobile  devices.  In  this  letter,  we  investigate  the  use  of  conversational  avatars  as  a  means  to  improve  the  user  experience  on  instant  messaging  (IM)  for  mobile  devices.  We  describe  the  design  and  implementation  of  an  interface  for  IM  featuring  a  3-D  facial  avatar  that  is  driven  by  the  text  messages  being  exchanged  between  chatting  participants.  Our  design  is  affordable  under  the  limited  computational  capacity  of  current  generation  mobiles.  We  evaluate  user  acceptance  and  reaction  via  user  studies,  by  comparing  it  to  a  more  conventional  IM  interface,  and  provide  recommendations  for  the  effective  design  of  conversational  avatar  interfaces  for  mobile  applications.
2	Measuring  affective  cognitive  experience  and  predicting  market  success.  We  present  a  new  affective-behavioral-cognitive  (ABC)  framework  to  measure  the  usual  cognitive  self-report  information  and  behavioral  information,  together  with  affective  information  while  a  customer  makes  repeated  selections  in  a  random-outcome  two-option  decision  task  to  obtain  their  preferred  product.  The  affective  information  consists  of  human-labeled  facial  expression  valence  taken  from  two  contexts:  one  where  the  facial  valence  is  associated  with  affective  wanting,  and  the  other  with  affective  liking.  The  new  “affective  wanting”  measure  is  made  by  setting  up  a  condition  where  the  person  shows  desire  to  receive  one  of  two  products,  and  we  measure  if  the  face  looks  satisfied  or  disappointed  when  each  of  the  products  arrives.  The  “affective  liking”  measure  captures  facial  expressions  after  sampling  a  product.  The  ABC  framework  is  tested  in  a  real-world  beverage  taste  experiment,  comparing  two  similar  products  that  actually  went  to  market,  where  we  know  the  market  outcomes.  We  find  that  the  affective  measure  provides  significant  improvement  over  the  cognitive  measure,  increasing  the  discriminability  between  the  two  similar  products,  making  it  easier  to  tell  which  is  most  preferred  using  a  small  number  of  people.  We  also  find  that  the  new  facial  valence  “affective  wanting”  measure  provides  a  significant  boost  in  discrimination  and  accuracy.
2	Emobed  strengthening  monomodal  emotion  recognition  via  training  with  crossmodal  emotion  embeddings.  Despite  remarkable  advances  in  emotion  recognition,  they  are  severely  restrained  from  either  the  essentially  limited  property  of  the  employed  single  modality,  or  the  synchronous  presence  of  all  involved  multiple  modalities.  Motivated  by  this,  we  propose  a  novel  crossmodal  emotion  embedding  framework  called  EmoBed,  which  aims  to  leverage  the  knowledge  from  other  auxiliary  modalities  to  improve  the  performance  of  an  emotion  recognition  system  at  hand.  The  framework  generally  includes  two  main  learning  components,  i.e.,  joint  multimodal  training  and  crossmodal  training.  Both  of  them  tend  to  explore  the  underlying  semantic  emotion  information  but  with  a  shared  recognition  network  or  with  a  shared  emotion  embedding  space,  respectively.  In  doing  this,  the  enhanced  system  trained  with  this  approach  can  efficiently  make  use  of  the  complementary  information  from  other  modalities.  Nevertheless,  the  presence  of  these  auxiliary  modalities  is  not  demanded  during  inference.  To  empirically  investigate  the  effectiveness  and  robustness  of  the  proposed  framework,  we  perform  extensive  experiments  on  the  two  benchmark  databases  RECOLA  and  OMG-Emotion  for  the  tasks  of  dimensional  emotion  regression  and  categorical  emotion  classification,  respectively.  The  obtained  results  show  that  the  proposed  framework  significantly  outperforms  related  baselines  in  monomodal  inference,  and  are  also  competitive  or  superior  to  the  recently  reported  systems,  which  emphasises  the  importance  of  the  proposed  crossmodal  learning  for  emotion  recognition.
2	Brain  dynamics  during  arousal  dependent  pleasant  unpleasant  visual  elicitation  an  electroencephalographic  study  on  the  circumplex  model  of  affect.  Emotion  regulation  to  pleasant  and  unpleasant  stimuli  involves  several  brain  areas,  such  as  the  prefrontal  cortex,  amygdala,  and  insular  cortex.  However,  how  a  specific  arousal  level  affects  such  brain  dynamics  is  not  fully  understood.  To  this  effect,  we  propose  an  electroencephalography  (EEG)-based  study,  where  22  healthy  subjects  were  emotionally  elicited  through  affective  pictures  gathered  from  the  International  Affective  Picture  System.  Based  on  the  circumplex  model  of  affect,  we  used  four  arousing  levels,  each  with  two  valence  levels  (i.e.,  pleasant  and  unpleasant).  Considering  these  levels,  we  investigated  the  EEG  power  spectra  and  functional  connectivity  among  channels.  We  then  used  this  information  to  build  an  automatic  valence  classifier.  The  experimental  results  showed  that  the  functional  connectivity  at  the  highest  frequency  bands  (i.e.,  >  30  Hz)  was  most  sensitive  to  arousal  modulation.  Specifically,  high  connectivity  over  the  right  hemisphere  occurred  during  pleasant  elicitation,  whereas  that  over  the  left  hemisphere  occurred  during  negative  elicitation.  In  addition,  short-range  connections  in  the  frontal  regions  became  weaker  with  increased  arousal  level,  whereas  long-range  connections  were  enhanced.  Concerning  the  spectral  analysis,  the  most  significant  valence-dependent  changes  were  found  at  intermediate  arousing  elicitations  over  the  prefrontal  and  occipital  regions.  The  automatic  valence  classification  showed  a  recognition  accuracy  of  up  to  86.37  percent.
2	Exploring  user  experience  with  image  schemas  sentiments  and  semantics.  Although  the  concept  of  user  experience  includes  two  key  aspects,  experience  of  meaning  (usability)  and  experience  of  emotion  (affect),  the  empirical  work  that  measures  both  the  usability  and  affective  aspects  of  user  experience  is  currently  limited.  This  is  particularly  important  considering  that  affect  could  significantly  influence  a  user's  perception  of  usability.  This  paper  uses  image  schemas  to  quantitatively  and  systematically  evaluate  both  these  aspects.  It  proposes  a  method  for  evaluating  user  experience  that  is  based  on  using  image  schemas,  sentiment  analysis,  and  computational  semantics.  The  aim  is  to  link  the  sentiments  expressed  by  users  during  their  interactions  with  a  product  to  the  specific  image  schemas  used  in  the  designs.  The  method  involves  semantic  and  sentiment  analysis  of  the  verbal  responses  of  the  users  to  identify  (i)  task-related  words  linked  to  the  task  for  which  a  certain  image  schema  has  been  used  and  (ii)  affect-related  words  associated  with  the  image  schema  employed  in  the  interaction.  The  main  contribution  is  in  linking  image  schemas  with  interaction  and  affect.  The  originality  of  the  method  is  twofold.  First,  it  uses  a  domain-specific  ontology  of  image  schemas  specifically  developed  for  the  needs  of  this  study.  Second,  it  employs  a  novel  ontology-based  algorithm  that  extracts  the  image  schemas  employed  by  the  user  to  complete  a  specific  task  and  identifies  and  links  the  sentiments  expressed  by  the  user  with  the  specific  image  schemas  used  in  the  task.  The  proposed  method  is  evaluated  using  a  case  study  involving  40  participants  who  completed  a  set  task  with  two  different  products.  The  results  show  that  the  method  successfully  links  the  users’  experiences  to  the  specific  image  schemas  employed  to  complete  the  task.  This  method  facilitates  significant  improvements  in  product  design  practices  and  usability  studies  in  particular,  as  it  allows  qualitative  and  quantitative  evaluation  of  designs  by  identifying  specific  image  schemas  and  product  design  features  that  have  been  positively  or  negatively  received  by  the  users.  This  allows  user  experience  to  be  assessed  in  a  systematic  way,  which  leads  to  a  better  understanding  of  the  value  associated  with  particular  design  features.
2	Personalized  multitask  learning  for  predicting  tomorrow  s  mood  stress  and  health.  While  accurately  predicting  mood  and  wellbeing  could  have  a  number  of  important  clinical  benefits,  traditional  machine  learning  (ML)  methods  frequently  yield  low  performance  in  this  domain.  We  posit  that  this  is  because  a  one-size-fits-all  machine  learning  model  is  inherently  ill-suited  to  predicting  outcomes  like  mood  and  stress,  which  vary  greatly  due  to  individual  differences.  Therefore,  we  employ  Multitask  Learning  (MTL)  techniques  to  train  personalized  ML  models  which  are  customized  to  the  needs  of  each  individual,  but  still  leverage  data  from  across  the  population.  Three  formulations  of  MTL  are  compared:  i)  MTL  deep  neural  networks,  which  share  several  hidden  layers  but  have  final  layers  unique  to  each  task;  ii)  Multi-task  Multi-Kernel  learning,  which  feeds  information  across  tasks  through  kernel  weights  on  feature  types;  and  iii)  a  Hierarchical  Bayesian  model  in  which  tasks  share  a  common  Dirichlet  Process  prior.  We  offer  the  code  for  this  work  in  open  source.  These  techniques  are  investigated  in  the  context  of  predicting  future  mood,  stress,  and  health  using  data  collected  from  surveys,  wearable  sensors,  smartphone  logs,  and  the  weather.  Empirical  results  demonstrate  that  using  MTL  to  account  for  individual  differences  provides  large  performance  improvements  over  traditional  machine  learning  methods  and  provides  personalized,  actionable  insights.
2	Nonlinear  appraisal  modeling  an  application  of  machine  learning  to  the  study  of  emotion  production.  Appraisal  theory  of  emotion  claims  that  emotions  are  not  caused  by  “raw”  stimuli,  as  such,  but  by  the  subjective  evaluation  (appraisal)  of  those  stimuli.  Studies  that  analyzed  this  relation  have  been  dominated  by  linear  models  of  analysis.  These  methods  are  not  ideally  suited  to  examine  a  basic  assumption  of  many  appraisal  theories,  which  is  that  appraisal  criteria  interact  to  differentiate  emotions,  and  hence  show  nonlinear  effects.  Studies  that  did  model  interactions  were  either  limited  in  scope  or  exclusively  theory-driven  simulation  attempts.  In  the  present  study,  we  improve  on  these  approaches  using  data-driven  methods  from  the  field  of  machine  learning.  We  modeled  a  categorical  emotion  response  as  a  function  of  25  appraisal  predictors,  using  a  large  data  set  on  recalled  emotion  experiences  (5,901  cases).  A  systematic  comparison  of  machine  learning  models  on  these  data  supported  the  interactive  nature  of  the  appraisal-emotion  relationship,  with  the  best  nonlinear  model  significantly  outperforming  the  best  linear  model.  The  interaction  structure  was  found  to  be  moderately  hierarchical.  Strong  main  effects  of  intrinsic  valence  and  goal  compatibility  appraisal  differentiated  positive  from  negative  emotions,  while  more  specific  emotions  (e.g.,  pride,  irritation,  despair)  were  differentiated  by  interactions  involving  agency  appraisal  and  norm  appraisal.
2	Partial  reinforcement  in  game  biofeedback  for  relaxation  training.  This  paper  investigates  the  effect  of  reinforcement  schedules  on  biofeedback  games  for  stress  self-regulation.  In  particular,  it  examines  whether  partial  reinforcement  can  improve  resistance  to  extinction  of  relaxation  behaviors,  i.e.,  once  biofeedback  is  removed.  Namely,  we  compare  two  types  of  reinforcement  schedules  (partial  and  continuous)  in  a  mobile  biofeedback  game  that  encourages  players  to  slow  their  breathing  during  gameplay.  The  game  uses  a  negative-reinforcement  instrumental  conditioning  paradigm,  removing  an  aversive  stimulus  (random  actions  in  the  game)  if  players  slows  down  their  breathing.  We  conducted  an  experimental  trial  with  24  participants  to  compare  the  two  reinforcement  schedules  against  a  control  condition.  Our  results  indicate  that  partial  reinforcement  improves  resistance  to  extinction,  as  measured  by  breathing  rate  and  skin  conductance  post-treatment.  In  addition,  based  on  linear  regression  and  correlation  analysis  we  found  that  participants  in  the  partial  reinforcement  learned  to  slow  their  breathing  at  the  same  pace  as  those  under  continuous  reinforcement.  The  article  discusses  the  implications  of  these  results  and  directions  for  future  work.
2	Exploring  cross  modality  affective  reactions  for  audiovisual  emotion  recognition.  Psycholinguistic  studies  on  human  communication  have  shown  that  during  human  interaction  individuals  tend  to  adapt  their  behaviors  mimicking  the  spoken  style,  gestures,  and  expressions  of  their  conversational  partners.  This  synchronization  pattern  is  referred  to  as  entrainment.  This  study  investigates  the  presence  of  entrainment  at  the  emotion  level  in  cross-modality  settings  and  its  implications  on  multimodal  emotion  recognition  systems.  The  analysis  explores  the  relationship  between  acoustic  features  of  the  speaker  and  facial  expressions  of  the  interlocutor  during  dyadic  interactions.  The  analysis  shows  that  72  percent  of  the  time  the  speakers  displayed  similar  emotions,  indicating  strong  mutual  influence  in  their  expressive  behaviors.  We  also  investigate  the  cross-modality,  cross-speaker  dependence,  using  mutual  information  framework.  The  study  reveals  a  strong  relation  between  facial  and  acoustic  features  of  one  subject  with  the  emotional  state  of  the  other  subject.  It  also  shows  strong  dependence  between  heterogeneous  modalities  across  conversational  partners.  These  findings  suggest  that  the  expressive  behaviors  from  one  dialog  partner  provide  complementary  information  to  recognize  the  emotional  state  of  the  other  dialog  partner.  The  analysis  motivates  classification  experiments  exploiting  cross-modality,  cross-speaker  information.  The  study  presents  emotion  recognition  experiments  using  the  IEMOCAP  and  SEMAINE  databases.  The  results  demonstrate  the  benefit  of  exploiting  this  emotional  entrainment  effect,  showing  statistically  significant  improvements.
2	Facial  action  unit  detection  using  attention  and  relation  learning.  Attention  mechanism  has  recently  attracted  increasing  attentions  in  the  field  of  facial  action  unit  (AU)  detection.  By  finding  the  region  of  interest  of  each  AU  with  the  attention  mechanism,  AU-related  local  features  can  be  captured.  Most  of  the  existing  attention  based  AU  detection  works  use  prior  knowledge  to  predefine  fixed  attentions  or  refine  the  predefined  attentions  within  a  small  range,  which  limits  their  capacity  to  model  various  AUs.  In  this  paper,  we  propose  an  end-to-end  deep  learning  based  attention  and  relation  learning  framework  for  AU  detection  with  only  AU  labels,  which  has  not  been  explored  before.  In  particular,  multi-scale  features  shared  by  each  AU  are  learned  firstly,  and  then  both  channel-wise  and  spatial  attentions  are  adaptively  learned  to  select  and  extract  AU-related  local  features.  Moreover,  pixel-level  relations  for  AUs  are  further  captured  to  refine  spatial  attentions  so  as  to  extract  more  relevant  local  features.  Without  changing  the  network  architecture,  our  framework  can  be  easily  extended  for  AU  intensity  estimation.  Extensive  experiments  show  that  our  framework  (i)  soundly  outperforms  the  state-of-the-art  methods  for  both  AU  detection  and  AU  intensity  estimation  on  the  challenging  BP4D,  DISFA,  FERA  2015  and  BP4D+  benchmarks,  (ii)  can  adaptively  capture  the  correlated  regions  of  each  AU,  and  (iii)  also  works  well  under  severe  occlusions  and  large  poses.
2	A  task  based  taxonomy  of  erroneous  human  behavior.  Abstract      Unexpected,  erroneous  human  interactions  often  contribute  to  failures  in  complex  systems.  Human  factors  engineers  and  researchers  have  developed  taxonomies  that  allow  engineers,  designers,  and  practitioners  to  think  about  and  model  erroneous  behavior  to  improve  the  safety  of  human-interactive  systems.  However,  the  two  leading  erroneous  behavior  taxonomies  are  based  on  incompatible  phenomenological  and  genotypical  perspectives.  Further,  neither  of  these  are  formulated  in  terms  of  task  analytic  methods,  analysis  and  modeling  techniques  human  factors  engineers  use  for  documenting  how  humans  normatively  achieve  goals  when  interacting  with  a  system.  In  this  work,  we  introduce  a  new  erroneous  human  behavior  taxonomy  based  on  where  and  how  human  behavior  diverges  from  task  analytic  models  of  human  behavior.  By  describing  where  a  human  diverges  from  a  normative  task,  and  by  identifying  what  information  the  human  failed  to  properly  attend  to  that  produced  the  divergence,  this  taxonomy  seeks  to  unify  the  phenomenological  and  genotypical  perspectives.  We  describe  the  theory  behind  this  taxonomy  and  the  different  erroneous  behavior  classifications  that  result  from  it.  We  then  show  how  it  is  compatible  with  the  leading  phenomenological  and  genotypical  taxonomies.  Finally,  we  discuss  the  implications  of  this  new  taxonomy  and  avenues  of  future  research.
2	Evaluating  touch  screen  vibration  modality  for  blind  users  to  access  simple  shapes  and  graphics.  Accessing  visual  information  becomes  a  central  need  for  all  kinds  of  tasks  and  users  (from  accessing  graphics  and  charts  in  news  articles,  to  viewing  images  of  items  on  sale  on  e-commerce  sites),  especially  for  blind  users.  In  this  context,  digital  tools  of  assistance,  using  adapted  software  (screen  readers,  talking  browsers,  etc.),  hardware  (force  feedback  mouse,  piezo-electric  pins,  etc.),  and  more  recently  touch-screen  technology  (using  smart  phones  or  smart  tablets)  have  been  increasingly  helping  blind  persons  access  and  manipulate  information.  While  effective  with  textual  information,  yet  existing  solutions  remain  limited  when  handling  visual  information.  In  this  context,  the  goal  of  our  study  is  to  shed  light  on  how  the  vibration  modality  can  be  perceived  by  blind  users  when  accessing  simple  contour-based  images  and  visual  graphics  on  a  touch-screen.  In  this  paper,  we  target  the  vibration-only  modality,  compared  with  audio-kinesthetic  or  multimodal  vibro-audio  solutions.  Our  main  motivation  is  that  the  potentials  and  limitations  of  touch-screen  vibration-only  feedback  need  to  be  fully  studied  and  understood  prior  to  integrating  other  modalities  (such  as  sound,  human  speech,  or  other  forms  of  haptic  feedback).  This  could  prove  very  useful  in  a  range  of  applications:  allowing  blind  people  to  access  geographic  maps,  to  navigate  autonomously  inside  and  outside  buildings,  as  well  as  to  access  graphs  and  mathematical  charts  (for  blind  students).  To  achieve  our  goal,  we  develop  a  dedicated  experimental  protocol,  titled  EVIAC,  testing  a  blind  user's  capacity  in  learning,  distinguishing,  identifying,  and  recognizing  basic  shapes  and  geometric  objects  presented  on  a  vibrating  touch-screen.  Extensive  tests  were  conducted  on  blindfolded  and  blind  candidates,  using  a  battery  of  evaluation  metrics  including:  i)  accuracy  of  shape  recognition,  ii)  testers  average  response  time,  iii)  number  and  duration  of  finger  strokes,  iv)  surface  area  covered  by  the  testers  finger  path  trails,  as  well  as  iv)  finger  path  correlation  with  the  surface  of  the  target  shape.  Results  show  that  blind  users  are  generally  capable  of  accessing  simple  shapes  and  graphics  presented  on  a  vibrating  touch-screen.  However,  results  also  underline  various  issues,  ranging  over:  prolonged  response  time  (e.g.,  blind  users  require  1min  and  22s  on  average  to  recognize  a  basic  shape),  reduced  touch-screen  surface  coverage,  and  low  correlation  between  the  surface  of  the  target  shape  and  the  tester's  vibration  trails.  The  latter  issues  need  to  be  further  investigated  to  produce  optimal  recipes  for  using  touch-screen  technology  to  support  image  accessibility  for  blind  users.
2	Perceived  fit  and  satisfaction  on  web  learning  performance  is  continuance  intention  and  task  technology  fit  perspectives.  Virtual  learning  system  (VLS)  is  an  information  system  that  facilitates  e-learning  have  been  widely  implemented  by  higher  education  institutions  to  support  face-to-face  teaching  and  self-managed  learning  in  the  virtual  learning  and  education  environment  (VLE).  This  is  referred  to  a  blended  learning  instruction.  By  adopting  the  VLS,  students  are  expected  to  enhance  learning  by  getting  access  to  course-related  information  and  having  full  opportunities  to  interact  with  instructors  and  peers.  However,  there  are  mixed  findings  revealed  in  the  literature  with  respect  to  the  learning  outcomes  in  adopting  VLS.  In  this  study,  we  argue  that  the  link  between  the  precedents  of  leading  students  to  continue  to  use  VLSs  and  their  impacts  on  learning  effectiveness  and  productivity  are  overlooked  in  the  literature.  This  paper  aims  to  tackle  this  question  by  integrating  information  system  (IS)  continuance  theory  with  task-technology  fit  (TTF)  to  extend  our  understandings  of  the  precedents  of  the  intention  to  continue  VLS  and  their  impacts  on  learning.  By  doing  it,  factors  of  technology-acceptance-to-performance,  based  on  TAM  (technology  acceptance  model)  and  TTF  and  post-technology-acceptance,  based  on  expectation-confirmation  theory,  models  can  be  included  to  test  in  one  study.  The  results  reveal  that  perceived  fit  and  satisfaction  are  important  precedents  of  the  intention  to  continue  VLS  and  individual  performance.  Later,  a  discussion  and  conclusions  are  provided.  This  study  sheds  light  on  learning  system  design  as  assisted  by  IS  in  VLE  and  can  serve  as  a  basis  for  promoting  VLS  in  assisting  learning.
2	Interrupted  by  my  car  implications  of  interruption  and  interleaving  research  for  automated  vehicles.  Abstract  As  vehicles  of  the  future  take  on  more  of  the  driving  responsibility  and  the  role  of  the  driver  transitions  into  more  of  a  monitoring  capacity,  the  traditional  notions  of  interruption  and  attention  management  needs  to  be  reconsidered  for  automated  vehicles.  We  argue  that  the  transfer  of  control  between  the  automated  vehicle  and  the  human  driver  can  be  considered  as  an  interruption  handling  process,  and  that  this  process  goes  through  a  series  of  ten  explicit  stages.  Each  stage  has  its  own  characteristics  and  implications  for  practice  and  future  research.  Therefore,  in  this  paper  we  identify  for  each  stage  what  is  known  from  theory,  together  with  important  implications  for  safety,  design,  and  future  research,  especially  for  human-machine  interaction.  More  generally,  the  framework  makes  explicit  that  it  is  not  appropriate  to  think  of  transfer  of  control  as  a  single  event  or  even  small  set  of  events.  The  framework  also  highlights  that  it  might  not  be  realistic  to  expect  human  drivers  to  immediately  respond  correctly  to  a  system  initiated  request  to  transfer  control,  given  that  humans  interleave  their  attention  between  non-driving  and  driving  tasks,  and  given  that  a  transition  constitutes  of  multiple  stages.  These  nuances  are  accounted  for  in  the  framework.
2	Design  features  of  embodied  conversational  agents  in  ehealth  a  literature  review.  Abstract  Embodied  conversational  agents  (ECAs)  are  gaining  interest  to  elicit  user  engagement  and  stimulate  actual  use  of  eHealth  applications.  In  this  literature  review,  we  identify  the  researched  design  features  for  ECAs  in  eHealth,  the  outcome  variables  that  were  used  to  measure  the  effect  of  these  design  features  and  what  the  found  effects  for  each  variable  were.  Searches  were  performed  in  Scopus,  ACM  Digital  Library,  PsychINFO,  Pubmed  and  IEEE  Xplore  Digital  Library,  resulting  in  1284  identified  articles  of  which  33  articles  were  included.  The  agents  speech  and/or  textual  output  and  its  facial  and  gaze  expressions  were  the  most  common  design  features.  Little  research  was  performed  on  the  agent's  looks.  The  measured  effect  of  these  design  features  was  often  on  the  perception  of  the  agent's  and  user’s  characteristics,  relation  with  the  agent,  system  usage,  intention  to  use,  usability  and  behaviour  change.  Results  show  that  emotion  and  relational  behaviour  seem  to  positively  affect  the  perception  of  the  agents  characteristics  and  that  relational  behaviour  also  seems  to  positively  affect  the  relation  with  the  agent,  usability  and  intention  to  use.  However,  these  design  features  do  not  necessarily  lead  to  behaviour  change.  This  review  showed  that  consensus  on  design  features  of  ECAs  in  eHealth  is  far  from  established.  Follow-up  research  should  include  more  research  on  the  effects  of  all  design  features,  especially  research  on  the  effects  in  a  long-term,  daily  life  setting,  and  replication  of  studies  on  the  effects  of  design  features  performed  in  other  contexts  than  eHealth.
2	Psychological  needs  and  virtual  worlds  case  second  life.  The  most  advanced  contemporary  virtual  worlds  provide  their  users  with  a  possibility  for  living  versatile  virtual  lives  together  with  other  users.  A  growing  number  of  users  worldwide  are  utilizing  this  possibility.  The  aim  of  this  research  was  to  study  active  virtual  world  users'  satisfaction  of  psychological  needs  both  inworld  and  outworld.  A  global  online  survey  for  the  users  of  Second  Life  was  constructed  based  on  a  model  of  ten  psychological  needs.  The  results  based  on  258  responses  indicated  that  self-esteem,  autonomy  and  physical  thriving  were  the  most  highly  satisfied  needs  inworld.  Furthermore,  the  results  indicated  that  autonomy,  physical  thriving,  and  money-luxury  were  needs,  which  were  satisfied  to  a  significantly  larger  extent  in  the  virtual  world  than  in  the  users'  real  lives  (when  not  using  a  computer).  On  the  other  hand,  the  needs  for  competence,  relatedness,  security,  and  popularity-influence  were  more  extensively  satisfied  in  the  users'  daily  lives  than  when  in  Second  Life.  The  qualitative  findings  highlighted  relatedness  needs  as  motivations  for  Second  Life  usage  and  revealed  five  central  themes  in  the  motivations  for  Second  Life  usage:  Second  Life  as  self-therapy,  as  a  source  of  instant  pleasures,  as  liberation  from  social  norms,  as  a  tool  for  self-expression,  and  as  exploration  and  novelty.  In  all,  the  findings  suggest  that  the  use  of  advanced  virtual  worlds  is  driven  by  a  variety  of  different  psychological  needs.  Virtual  world  usage  is  also  related  to  need  satisfaction  in  the  users'  lives  outside  the  virtual  world.
2	Smartphone  help  contents  re  organization  considering  user  specification  via  conditional  gan.  Abstract  There  are  various  help  systems  embedded  in  smartphones  that  are  intended  to  provide  assistance  to  users.  These  systems  should  be  conveniently  accessible  in  locations  where  users  may  need  assistance.  Moreover,  when  help  contents  are  provided  without  regard  to  the  users  interest,  it  makes  it  difficult  for  the  user  to  find  relevant  content.  Thus,  the  present  study  provides  a  new  method  of  re-organizing  help  content  by  considering  each  users  interests  and  preferences  using  their  app  usage  sequence.  Based  on  the  user  specification  derived  from  the  app  usage  sequence,  help  contents  usage  prediction  is  generated  for  each  of  them  with  conditional  generative  adversarial  network  (GAN)  architecture  in  a  new  way.  Further,  another  method  to  pre-process  data  in  applying  conditional  GAN,  originally  devised  to  generate  image  data,  is  proposed  in  our  problem.  The  experiment  result  showed  a  higher  absolute  performance  level  of  help  contents  usage  prediction  and  better  performance  of  effectiveness  in  re-organization  of  top-k  contents  compared  to  the  existing  benchmark  method.  Thus,  the  proposed  method  reflects  the  users  interest  and  provides  appropriate  help  contents  for  each  user  effectively.
2	Rational  security  modelling  everyday  password  use.  To  inform  the  design  of  security  policy,  task  models  of  password  behaviour  were  constructed  for  different  user  groups-Computer  Scientists,  Administrative  Staff  and  Students.  These  models  identified  internal  and  external  constraints  on  user  behaviour  and  the  goals  for  password  use  within  each  group.  Data  were  drawn  from  interviews  and  diaries  of  password  use.  Analyses  indicated  password  security  positively  correlated  with  the  sensitivity  of  the  task,  differences  in  frequency  of  password  use  were  related  to  password  security  and  patterns  of  password  reuse  were  related  to  knowledge  of  security.  Modelling  revealed  Computer  Scientists  viewed  information  security  as  part  of  their  tasks  and  passwords  provided  a  way  of  completing  their  work.  By  contrast,  Admin  and  Student  groups  viewed  passwords  as  a  cost  incurred  when  accessing  the  primary  task.  Differences  between  the  models  were  related  to  differences  in  password  security  and  used  to  suggest  six  recommendations  for  security  officers  to  consider  when  setting  password  policy.
2	Development  of  scales  for  the  measurement  of  principles  of  design.  Principles  of  design  have  been  widely  applied  by  practitioners  such  as  photographers,  artists,  architects,  designers,  and  others  for  many  years.  This  paper  examines  whether  the  principles  advocated  by  practitioners  systematically  and  reliably  influence  perceived  aesthetics  and  perceived  ease  of  use  constructs  that  are  important  for  the  evaluation  of  websites.  To  examine  these  relationships,  the  paper  offers  an  operational  definition,  develops  and  validates  a  measurement  tool  for  assessing  the  principles.  To  provide  a  comprehensive  and  broad  definition  of  the  principles,  over  20  books  written  by  practitioners  were  reviewed  and  more  than  100  questionnaire  items  were  extracted.  These  items  were  further  refined  through  two  rounds  of  content  analysis  using  emergent  and  a  priori  coding  with  10  judges  and  2  experts.  The  remaining  items  from  the  two  rounds  of  coding  were  examined  using  surveys.  Exploratory  and  confirmatory  factor  analyses  were  performed.  The  results  show  that  the  items  under  6  constructs  have  adequate  convergent  and  discriminant  validity.  Lastly,  a  field  study  testing  effects  of  the  principles  on  perceived  ease  of  use  and  aesthetics  was  conducted.  The  results  show  that  principles  of  design  predict  perceived  ease  of  use  better  than  perceived  aesthetics.  Implications  for  practice  and  research  are  discussed.
2	Adaptation  and  validation  of  the  itc  sense  of  presence  inventory  for  the  portuguese  language.  Abstract  This  investigation  concerns  the  translation  and  validation  of  the  ITC  -  Sense  of  Presence  Inventory  (ITC-SOPI)  for  the  Portuguese-speaking  population  (in  Europe),  estimating  the  validity  of  the  content  and  concepts  and  the  maintenance  of  an  equivalent  semantics.  It  also  sought  to  verify  its  psychometric  properties,  namely  its  factorial  validity  and  internal  consistency.  The  sample  consisted  of  459  individuals,  274  males  and  185  females.  The  fidelity  of  the  subscales  varied  between  0.67  and  0.89.  Confirmatory  factor  analysis  revealed  a  theoretical  model  of  35  items,  divided  by  four  factors.  After  fixing  some  of  the  residual  errors  between  items,  the  following  adjustment  indexes  were  calculated:  χ2/df = 2.301;  goodness  fit  index = 0.860;  comparative  fitness  index = 0.889;  root  mean  square  error  of  approximation = 0.053;  Akaike’s  information  criterion  =  1420.  Based  on  the  observed  results  and  the  robustness  of  the  sample  size  used,  the  obtained  theoretical  model  shows  that  the  ITC-SOPI  is  recommended  to  measure  presence  in  virtual  reality  research  projects  with  samples  of  Portuguese  language  speakers.
2	Feeling  connected  to  smart  objects  a  moderated  mediation  model  of  locus  of  agency  anthropomorphism  and  sense  of  connectedness.  Abstract  In  human-IoT  interaction,  both  users  and  smart  objects  can  exercise  agency.  This  study  examines  how  the  relationship  between  the  locus  of  agency  (user  vs.  object)  and  interaction  outcomes  is  mediated  through  sense  of  connectedness  and  how  this  mediated  relationship  is  moderated  by  anthropomorphic  cues.  Results  from  a  lab  experiment  (N = 94)  indicate  that  users  perceive  their  interactions  more  positively  when  they  exercise  their  own  agency  than  when  the  objects  have  agency.  In  addition,  when  the  objects  exert  own  agency,  anthropomorphism  elicits  more  positive  user  responses  by  increasing  sense  of  connectedness.
2	Factors  affecting  perception  of  information  security  and  their  impacts  on  it  adoption  and  security  practices.  The  gap  between  the  perceived  security  of  an  information  system  and  its  real  security  level  can  influence  people'  decisions  and  behavior.  The  objective  of  this  study  is  to  find  effective  ways  to  adjust  people's  perception  of  information  security,  in  order  to  enhance  their  intention  to  adopt  IT  appliances  and  compliance  to  security  practices.  Two  separate  experiments  were  conducted.  In  experiment  I,  64  participants  were  asked  to  transfer  money  through  an  e-banking  system.  Their  intention  to  adopt  e-banking  was  measured  by  a  questionnaire.  In  experiment  II,  64  participants  were  asked  to  register  on  an  online  forum.  Their  subjective  intention  to  create  a  strong  password  was  measured  by  a  questionnaire,  and  the  objective  strength  of  the  passwords  they  created  was  calculated.  Results  of  the  ANOVA  and  the  path  models  derived  from  the  path  analysis  indicated  that  people's  adoption  intention,  such  as  their  intention  to  adopt  e-banking,  can  be  enhanced  by  changing  their  perceived  Knowledge,  Controllability  and  Awareness,  while  changing  the  perceived  Controllability  is  most  effective.  The  results  also  indicated  that  people's  compliance  to  security  practices,  such  as  setting  strong  passwords  for  IT  systems,  can  be  enhanced  by  changing  their  perceived  Knowledge,  Severity  and  Possibility,  while  changing  their  perceived  Knowledge  and  Severity  is  most  effective.  Implications  for  further  research  and  practice  were  also  discussed.
2	Data  driven  smart  home  system  for  elderly  people  based  on  web  technologies.  The  proportion  of  elderly  people  over  65  years  old  has  rapidly  increased,  and  social  costs  related  to  aging  population  problems  have  grown  globally.  The  governments  want  to  reduce  these  social  costs  through  advanced  technologies.  The  physician  or  medical  center  evaluates  health  conditions  from  the  reports  of  elderly  people.  However,  self-reports  are  often  inaccurate,  and  sometimes  reports  by  family  or  caregivers  can  be  more  accurate.  To  solve  these  problems,  an  evaluated  objective  method  based  on  sensor  data  is  needed.  In  this  paper,  we  propose  a  data-driven  smart  home  system  that  uses  web  technologies  for  connecting  sensors  and  actuators.  The  proposed  system  provides  a  method  of  monitoring  elderly  people’s  daily  activities  using  commercial  sensors  to  register  recognizable  activities  easily.  In  addition,  it  controls  actuators  in  the  home  by  using  user-defined  rules  and  shows  a  summary  of  elderly  people’s  activities  to  monitor  them.
2	Vision  based  mid  air  unistroke  character  input  using  polar  signatures.  Hand-gesture-based  commands  may  replace  touch  and  electromechanical  input  panels  to  make  public  interactive  response  systems  (IRS)  more  accessible.  This  work  presents  a  prototype  framework  for  vision-based  mid-air  unistroke  character  input,  which  can  be  adapted  as  an  interface  for  the  IRS.  At  first,  we  developed  an  acquisition  module  that  effectively  spots  the  legitimate  gesture  trajectory  by  implementing  pen-up  and  pen-down  actions  using  depth  thresholding  and  velocity  tracking.  The  extracted  trajectory  is  recognized  through  a  novel,  fast,  and  easy  to  implement  the  equipolar  signature  (EPS)  technique.  Apart  from  resistance  to  rotation,  scale,  and  translation  variations,  EPS  exhibits  neutrality  to  stroking  directions  as  well.  On  the  three  self-collected  datasets  comprising  of  digits,  alphabets,  and  symbols,  the  EPS  scheme  obtains  over  96.5%  accurate  results  with  an  average  of  30-ms  running  time.  The  proposed  scheme  is  also  validated  on  an  open  dataset  DAIR  (Dataset  for  AIR  Handwriting),  where  it  achieves  95.5%  mean  accuracy  with  24.3-ms  recognition  time  per  gesture.  The  developed  approach  is  compared  with  benchmark  schemes  to  justify  its  accuracy  and  speed.
2	A  user  centered  segmentation  method  for  complex  historical  manuscripts  based  on  document  graphs.  In  historical  manuscripts,  humans  can  detect  handwritten  words,  lines,  and  decorations  with  lightness  even  if  they  do  not  know  the  language  or  the  script.  Yet  for  automatic  processing  this  task  has  proven  elusive,  especially  in  the  case  of  handwritten  documents  with  complex  layouts,  which  is  why  semiautomatic  methods  that  integrate  the  human  user  into  the  process  are  needed.  In  this  paper,  we  introduce  a  user-centered  segmentation  method  based  on  document  graphs  and  scribbling  interaction.  The  graphs  capture  a  sparse  representation  of  the  document's  structure  that  can  then  be  edited  by  the  user  with  a  stylus  on  a  touch-sensitive  screen.  We  evaluate  the  proposed  method  on  a  newly  introduced  database  of  historical  manuscripts  with  complex  layout  and  demonstrate,  first,  that  the  document  graphs  are  already  close  to  the  desired  segmentation  and,  second,  that  scribbling  allows  a  natural  and  efficient  interaction.
2	Sentiview  sentiment  analysis  and  visualization  for  internet  popular  topics.  There  would  be  value  to  several  domains  in  discovering  and  visualizing  sentiments  in  online  posts.  This  paper  presents  SentiView,  an  interactive  visualization  system  that  aims  to  analyze  public  sentiments  for  popular  topics  on  the  Internet.  SentiView  combines  uncertainty  modeling  and  model-driven  adjustment.  By  searching  and  correlating  frequent  words  in  text  data,  it  mines  and  models  the  changes  of  the  sentiment  on  public  topics.  In  addition,  using  a  time-varying  helix  together  with  an  attribute  astrolabe  to  represent  sentiments,  it  can  visualize  the  changes  of  multiple  attributes  and  relationships  among  demographics  of  interest  and  the  sentiments  of  participants  on  popular  topics.  The  relationships  of  interest  among  different  participants  are  presented  in  a  relationship  map.  Using  a  new  evolution  model  that  is  based  on  cellular  automata,  it  is  able  to  compare  the  time-varying  features  for  sentiment-driven  forums  on  both  simulated  and  real  data.  Adaptable  for  different  social  networking  platforms,  such  as  Twitter,  blog  and  forum,  the  methods  demonstrate  the  effectiveness  of  SentiView  in  analyzing  and  visualizing  public  sentiments  on  the  Web.
2	Evaluation  of  haptic  and  visual  cues  for  repulsive  or  attractive  guidance  in  nonholonomic  steering  tasks.  Remote  control  of  vehicles  is  a  difficult  task  for  operators.  Support  systems  that  present  additional  task  information  may  assist  operators,  but  their  usefulness  is  expected  to  depend  on  several  factors  such  as  1)  the  nature  of  conveyed  information,  2)  what  modality  it  is  conveyed  through,  and  3)  the  task  difficulty.  In  an  exploratory  experiment,  these  three  factors  were  manipulated  to  quantify  their  effects  on  operator  behavior.  Subjects  (      $n  =  {{15}}$    )  used  a  haptic  manipulator  to  steer  a  virtual  nonholonomic  vehicle  through  abstract  environments,  in  which  obstacles  needed  to  be  avoided.  Both  a  simple  support  conveying  near-future  predictions  of  the  trajectory  of  the  vehicle  and  a  more  elaborate  support  that  continuously  suggests  the  path  to  be  taken  were  designed  (factor  1).  These  types  of  information  were  offered  either  with  visual  or  haptic  cues  (factor  2).  These  four  support  systems  were  tested  in  four  different  abstracted  environments  with  decreasing  amount  of  allowed  variability  in  realized  trajectories  (factor  3).  The  results  show  improvements  for  the  simple  support  only  when  this  information  was  presented  visually,  but  not  when  offered  haptically.  For  the  elaborate  support,  equally  large  improvements  for  both  modalities  were  found.  This  suggests  that  the  elaborate  support  is  better:  additional  information  is  key  in  improving  performance  in  nonholonomic  steering  tasks.
2	A  comparative  study  between  a  virtual  reality  heart  anatomy  system  and  traditional  medical  teaching  modalities.  The  aim  of  using  virtual  reality  (VR)  as  a  medical  training  tool  is  to  offer  additional  means  to  teach  students  and  to  improve  the  quality  of  medical  skills.  A  novel  system  was  developed  to  fulfil  the  requirements  of  modern  medical  education  and  overcome  the  challenges  faced  by  both  students  and  lecturers  in  the  process  of  knowledge  transfer.  A  heart  three-dimensional  model  presented  in  a  virtual  reality  (VR)  environment  has  been  implemented  in  order  to  facilitate  a  new  educational  modality.  This  paper  reports  the  outcome  of  a  comparative  study  between  traditional  medical  teaching  modalities  and  virtual  reality  technology.  This  study  was  conducted  in  the  Faculty  of  Medicine  in  the  University  of  Jordan.  The  participants  were  asked  to  perform  system  trials  and  experiment  with  the  system  by  navigating  through  the  system  interfaces,  as  well  as  being  exposed  to  the  traditional  physical  model  of  the  human  heart  that  is  currently  used  in  the  faculty  during  practical  anatomy  sessions.  Afterwards,  they  were  asked  to  provide  feedback  via  a  comparative  questionnaire.  The  participants’  replies  to  the  questions  regarding  the  Physical  Heart  Model  and  VR  heart  anatomy  system  were  assessed  for  reliability  using  Cronbach’s  alpha.  The  first  group’s  (Physical  Heart  Model  questions)  α  value  was  0.689.  The  second  group’s  (VR  heart  anatomy  system  questions)  α  value  was  0.791.  Comparing  students’  experience  results  between  the  traditional  method  (Physical  Heart  Model)  and  the  VR  heart  anatomy  system,  the  mean  scores  showed  a  distinct  increase  in  the  values.  This  indicates  that  the  developed  system  enhanced  their  experience  in  anatomy  learning  and  the  provided  tools  improved  their  understanding  of  heart  anatomy.  Results  demonstrated  the  usefulness  of  the  system  by  showing  a  higher  satisfaction  rate  for  the  provided  tools  regarding  structure  and  visualisation.
2	Virtual  experience  real  consequences  the  potential  negative  emotional  consequences  of  virtual  reality  gameplay.  As  virtual  reality  (VR)  technology  enters  mainstream  markets,  it  is  imperative  that  we  understand  its  potential  impacts  on  users,  both  positive  and  negative.  In  the  present  paper,  we  build  on  the  extant  literature’s  focus  on  the  physical  side  effects  of  VR  gameplay  (e.g.,  cybersickness)  by  focusing  on  VR’s  potential  to  intensify  users’  experiences  of  negative  emotions.  We  first  conducted  a  preliminary  survey  to  assess  users’  emotional  responses  during  VR  gameplay,  with  the  results  suggesting  that  certain  VR  situations  can  in  fact  produce  intense  negative  emotional  experiences.  We  then  designed  an  interactive  scenario  intended  to  elicit  low  to  moderate  amounts  of  negative  emotion,  wherein  participants  played  out  the  scenario  in  either  VR  (using  the  HTC  Vive)  or  on  a  laptop  computer.  Compared  to  the  participants  who  enacted  the  scenario  on  the  laptop,  those  in  the  VR  condition  reported  higher  levels  of  absorption,  which  in  turn  increased  the  intensity  of  their  negative  emotional  response  to  the  scenario.  A  follow-up  questionnaire  administered  several  hours  later  revealed  that  the  intensified  negative  emotions  resulting  from  VR  had  a  significant  positive  correlation  with  negative  rumination  (i.e.,  harmful  self-related  thoughts  related  to  distress).  These  results  show  that  VR  gameplay  has  the  potential  to  elicit  strong  negative  emotional  responses  that  could  be  harmful  for  users  if  not  managed  properly.  We  discuss  the  practical  and  policy  implications  of  our  findings.
2	Multi  touch  gestures  for  pre  kindergarten  children.  Abstract      The  direct  manipulation  interaction  style  of  multi-touch  technology  makes  it  the  ideal  mechanism  for  learning  activities  from  pre-kindergarteners  to  adolescents.  However,  most  commercial  pre-kindergarten  applications  only  support  tap  and  drag  operations.  This  paper  investigates  pre-kindergarteners׳  (2–3  years  of  age)  ability  to  perform  other  gestures  on  multi-touch  surfaces.  We  found  that  these  infants  could  effectively  perform  additional  gestures,  such  as  one-finger  rotation  and  two-finger  scale  up  and  down,  just  as  well  as  basic  gestures,  despite  gender  and  age  differences.  We  also  identified  cognitive  and  precision  issues  that  may  have  an  impact  on  the  performance  and  feasibility  of  several  types  of  interaction  (double  tap,  long  press,  scale  down  and  two-finger  rotation)  and  propose  a  set  of  design  guidelines  to  mitigate  the  associated  problems  and  help  designers  envision  effective  interaction  mechanisms  for  this  challenging  age  range.
2	A  novel  method  based  on  ompgw  method  for  feature  extraction  in  automatic  music  mood  classification.  Music  mood  is  useful  for  music-related  applications  such  as  music  retrieval  or  recommendation,  which  represents  the  inherent  emotional  expression  of  music  signals.  In  this  paper,  a  novel  technique  is  proposed  for  music  signal  analysis  in  the  view  of  emotions,  which  is  based  on  the  orthogonal  matching  pursuit,  Gabor  functions,  and  the  Wigner  distribution  function.  The  technique,  called  the  OMPGW  method,  consists  of  three-level  schemes:  the  low-level,  the  middle-level  and  the  high-level  schemes.  For  the  low-level  schemes,  the  orthogonal  matching  pursuit  combined  with  Gabor  functions  is  proposed  to  provide  an  adaptive  time-frequency  decomposition  of  music  signals.  Compared  with  other  algorithms  for  signal  analysis,  the  proposed  algorithm  can  achieve  a  higher  spatial  and  temporal  resolution  and  give  a  better  interpret  of  the  music  signal  structures.  In  the  middle-level  schemes,  the  Wigner  distribution  function  is  applied  to  obtain  the  time-frequency  energy  distribution  of  the  results  from  the  low-level  schemes.  High-level  schemes  are  used  to  describe  the  modeling  of  audio  features,  the  procedure  of  music  mood  classification.  A  classifier  based  on  support  vector  machines  is  utilized  to  model  the  extracted  features  with  the  proposed  technique  regarding  the  emotion  models.  Several  experiments  are  conducted  with  four  datasets,  and  better  results  are  achieved  with  the  proposed  method.  In  music  mood  classification  experiments,  music  clips  are  classified  into  different  kinds  of  mood  clusters,  and  mean  accuracy  of  69.53  percent  on  our  dataset  can  be  achieved  using  the  OMPGW  method.
2	Affective  impression  sentiment  awareness  poi  suggestion  via  embedding  in  heterogeneous  lbsns.  In  this  study,  we  present  a  novel  framework  called  Community-based  sentiment  Extraction  and  NeTwork  Embedding  for  POI  Recommendation  (CENTER)  in  order  to  suggesting  impressive  POIs  in  an  effective  and  efficient  fashion.  The  CENTER  framework  integrates  two  essential  techniques:  (1)  a  latent  probabilistic  generative  model  called  Community-based  Sentiment  Extraction  (CSE),  which  can  accurately  capture  the  review  contents  in  LBSNs  by  taking  into  consideration  the  characteristics  of  social  communities.  The  parameters  of  the  CSE  model  are  inferred  effectively  by  the  Gibbs  sampling  method.  The  primary  sentiments  are  obtained  based  on  the  probability  distribution  of  sentiments;  (2)  a  network  embedding  model  called  Sentiment-aware  Nework  Embedding  for  POI  Recommendation  (SNER)  is  employed  to  learn  the  representation  of  important  factors  including  POIs,  users  and  textual  sentiments  in  a  low-dimensional  embedding  space.  The  joint  training  is  utilized  to  alternatively  sample  all  sets  of  edges  in  a  heterogeneous  network.  Extensive  experiments  were  conducted  on  two  large-scale  real  datasets,  in  order  to  evaluate  the  performance  of  CENTER.  The  results  demonstrate  that  CENTER  outperforms  the  state-of-the-art  baseline  methods  in  the  effectiveness  and  efficiency  of  POI  suggestion.
2	Developing  intuitive  gestures  for  spatial  interaction  with  large  public  displays.  Freehand  gestures  used  in  gestural-based  interactive  systems  are  often  designed  around  technical  limitations  of  gesture  capturing  technologies,  resulting  in  gestures  that  may  not  be  intuitive  to  users.  In  this  paper,  we  investigated  freehand  gestures  that  are  intuitive  to  users  with  common  technical  knowledge.  We  conducted  a  gesture  solicitation  study  with  30  participants,  who  were  asked  to  complete  21  tasks  on  a  large  display  using  freehand  gestures.  All  gestures  in  the  study  were  video-recorded.  We  conducted  in-depth  interviews  with  each  participant  to  ask  about  the  gestures  they  had  chosen  and  why  they  had  chosen  them.  We  found  that  a  large  proportion  of  intuitive  freehand  gestures  had  metaphoric  origins  from  daily  uses  of  two-dimensional  surface  displays,  such  as  smart  phones  and  tablets.  However,  participants  may  develop  new  gestures,  particularly  when  objects  they  are  manipulating  deviated  from  those  commonly  seen  in  surface  technologies.  In  this  paper,  we  discuss  when  and  why  participants  developed  new  gestures  rather  than  reusing  gestures  of  similar  tasks  on  two-dimension  surface  displays.  We  suggest  design  implications  for  gestures  for  large  public  displays.
2	Pilots  latency  of  first  fixation  and  dwell  among  regions  of  interest  on  the  flight  deck.  The  purpose  of  this  pilot  study  is  to  investigate  the  differences  of  eye  movements  among  three  different  flight  backgrounds.  There  were  eleven  participants  (2  military  pilots  with  average  2,250  flying  hours,  6  commercial  pilots  with  average  5,360  flying  hours,  and  3  novices).  All  participants  wear  a  mobile  eye  tracker  during  the  experiment  operating  a  Boeing  747  flight  simulator  for  landing.  The  eye  tracker  recorded  all  participants’  eye  movement  data  automatically.  The  average  values  of  the  latency  of  first  fixation  (LFF)  and  the  total  contact  time  (TCT)  for  five  regions  of  interest  (ROIs)  are  used  to  examine  proposed  hypotheses.  The  findings  include:  (1)  participants  of  different  flight  backgrounds  have  different  sequences  of  viewing  ROIs;  (2)  participants  of  military  pilots  and  novices  spent  most  of  time  viewing  the  outside  of  cockpit  (ROI-3);  however,  participants  of  commercial  pilots  spent  most  of  time  viewing  the  Primary  Flight  Display  (ROI-1).  Current  research  findings  might  be  applied  for  developing  conversion  training  for  military  pilots  conversed  to  civil  airlines  pilots.  The  fundamental  reasons  of  why  pilots  viewing  ROIs  in  different  sequence  and  spending  significant  different  time  on  the  ROIs  needed  to  be  studied  further  in  the  future.
2	An  auditory  display  to  convey  urgency  information  in  industrial  control  rooms.  Auditory  warning  signals  are  common  features  in  industrial  control  rooms.  Finding  sound  signals  that  convey  higher  degrees  of  urgency  while  keeping  the  potential  for  annoyance  low  is  challenging.  In  the  present  study,  evaluations  were  performed  on  four  different  types  of  auditory  displays.  The  displays  were  all  designed  to  convey  three  levels  of  urgency.  The  examination  focused  on  the  following  questions:  1  "How  reliably  can  the  operators  identify  the  three  levels  of  urgency?"  and  2  "How  annoying  do  the  operators  find  the  sound  signals?".  Fourteen  operators  participated  in  the  study.  For  every  signal  within  each  auditory  display,  the  participants  were  asked  to  rate  the  level  of  urgency  and  annoyance.  The  results  show  that  one  can  design  auditory  displays  that  employ  appropriate  urgency  mapping  while  the  perceived  annoyance  is  kept  at  a  low  level.  The  work  also  suggests  that  involving  the  end  users  in  the  design  process  could  be  advantageous.
2	Visuospatial  processing  and  learning  effects  in  virtual  reality  based  mental  rotation  and  navigational  tasks.  Visuospatial  function  and  performance  in  interactions  between  humans  and  computers  involve  the  human  identification  and  manipulation  of  computer  generated  stimuli  and  their  location.  The  impact  of  learning  on  mental  rotation  has  been  demonstrated  in  studies  relating  everyday  spatial  activities  and  spatial  abilities.  An  aspect  of  visuospatial  learning  in  virtual  environments  that  has  not  been  widely  studied  is  the  impact  of  threat  on  learning  in  a  navigational  task.  In  fact,  to  our  knowledge,  the  combined  assessment  of  learning  during  mental  rotation  trials  and  learning  in  an  ecologically  valid  virtual  reality-based  navigational  environment  (that  has  both  high  and  low  threat  zones)  has  not  been  adequately  studied.  Results  followed  expectation:  1)  learning  occurred  in  the  virtual  reality  based  mental  rotation  test.  Although  there  was  a  relation  between  route  learning  and  practice,  a  primacy  effect  was  observed  as  participants  performed  more  poorly  when  going  from  the  first  zone  to  the  last.
2	Investigation  of  factors  affecting  the  usability  evaluation  of  an  adaptive  cruise  control  system.  In  this  study,  we  investigate  the  factors  affecting  the  usability  evaluation  of  an  adaptive  cruise  control  (ACC)  system.  In  this  experiment,  the  participants  drove  a  Toyota  Prius  car  with  an  ACC  on  a  highway.  We  sampled  215  types  of  driving  data  recorded  at  a  frequency  of  60  Hz  during  driving.  At  each  of  the  six  designated  stop  points  on  the  driving  course,  the  participants  stopped  their  cars  and  evaluated  the  usability  of  the  ACC  system  by  answering  the  usability  questionnaire  for  automation  systems.  The  participants’  driving  styles  were  measured  using  the  driving  style  questionnaire.  The  multiple  regression  analyses  showed  that  the  participants’  driving  styles,  the  ACC’s  driving  control,  and  the  participants’  intervention  in  the  driving  control  of  the  ACC  influenced  the  usability  evaluation.  The  results  were  discussed  in  terms  of  the  human–automation  interactions  and  the  design  principles  of  an  ACC.
2	Passive  force  feedback  gloves  with  joint  based  variable  impedance  using  layer  jamming.  Force  feedback  gloves  have  a  great  potential  in  enhancing  the  fidelity  of  virtual  reality  and  teleoperation  systems.  It  is  a  challenge  to  develop  multifinger  and  lightweight  force  feedback  gloves.  In  this  paper,  we  propose  a  solution  using  layer  jamming  sheet  (LJS)  on  each  finger  joint.  In  simulating  free  space,  the  LJS  is  soft  and  easy  to  deform,  which  allows  the  finger  joints  to  move  freely  with  a  small  resistance  force.  In  simulating  constrained  space,  the  LJS  becomes  stiff,  which  provides  resistance  torques  to  prevent  the  rotation  of  finger  joints.  Possible  solutions  for  mounting  the  LJS  on  finger  joints  are  investigated.  Mechanical  models  of  the  LJS  are  derived  by  quantifying  the  relationship  between  the  bending  stiffness  and  the  pressure,  material,  and  geometry  of  the  layer.  Experiments  are  performed  to  characterize  the  mechanical  behavior  of  the  LJS  actuator  and  to  validate  the  performance  of  the  different  design  solutions  in  simulating  free  space  and  constrained  space.  Experimental  results  indicate  the  potential  of  the  proposed  joint-based  LJS-actuated  approach  in  developing  lightweight  force  feedback  gloves.
2	Analysis  of  operational  comfort  in  manual  tasks  using  human  force  manipulability  measure.  This  paper  proposes  a  scheme  for  human  force  manipulability  (HFM)  based  on  the  use  of  isometric  joint  torque  properties  to  simulate  the  spatial  characteristics  of  human  operation  forces  at  an  end-point  of  a  limb  with  feasible  magnitudes  for  a  specified  limb  posture.  This  is  also  applied  to  the  evaluation/prediction  of  operational  comfort  (OC)  when  manually  operating  a  human-machine  interface.  The  effectiveness  of  HFM  is  investigated  through  two  experiments  and  computer  simulations  of  humans  generating  forces  by  using  their  upper  extremities.  Operation  force  generation  with  maximum  isometric  effort  can  be  roughly  estimated  with  an  HFM  measure  computed  from  information  on  the  arm  posture  during  a  maintained  posture.  The  layout  of  a  human-machine  interface  is  then  discussed  based  on  the  results  of  operational  experiments  using  an  electric  gear-shifting  system  originally  developed  for  robotic  devices.  The  results  indicate  a  strong  relationship  between  the  spatial  characteristics  of  the  HFM  and  OC  levels  when  shifting,  and  the  OC  is  predicted  by  using  a  multiple  regression  model  with  HFM  measures.
2	Power  optimization  of  ultrasonic  friction  modulation  tactile  interfaces.  Ultrasonic  friction-modulation  devices  provide  rich  tactile  sensation  on  flat  surfaces  and  have  the  potential  to  restore  tangibility  to  touchscreens.  To  date,  their  adoption  into  consumer  electronics  has  been  in  part  limited  by  relatively  high  power  consumption,  incompatible  with  the  requirements  of  battery-powered  devices.  This  paper  introduces  a  method  that  optimizes  the  energy  efficiency  and  performance  of  this  class  of  devices.  It  considers  optimal  energy  transfer  to  the  impedance  provided  by  the  finger  interacting  with  the  surface.  Constitutive  equations  are  determined  from  the  mode  shape  of  the  interface  and  the  piezoelectric  coupling  of  the  actuator.  The  optimization  procedure  employs  a  lumped  parameter  model  to  simplify  the  treatment  of  the  problem.  Examples  and  an  experimental  study  show  the  evolution  of  the  optimal  design  as  a  function  of  the  impedance  of  the  finger.
2	Perceptual  analysis  of  vibrotactile  flows  on  a  mobile  device.  "Vibrotactile  flow"  refers  to  a  continuously  moving  sensation  of  vibrotactile  stimulation  applied  by  a  few  actuators  directly  onto  the  skin  or  through  a  rigid  medium.  Research  demonstrated  the  effectiveness  of  vibrotactile  flow  for  conveying  intuitive  directional  information  on  a  mobile  device.  In  this  paper,  we  extend  previous  research  by  investigating  the  perceptual  characteristics  of  vibrotactile  flows  rendered  on  a  mobile  device  and  proposing  a  synthesis  framework  for  vibrotactile  flows  with  desired  perceptual  properties.
2	A  haptic  shared  control  architecture  for  guided  multi  target  robotic  grasping.  Although  robotic  telemanipulation  has  always  been  a  key  technology  for  the  nuclear  industry,  little  advancement  has  been  seen  over  the  last  decades.  Despite  complex  remote  handling  requirements,  simple  mechanically  linked  master–slave  manipulators  still  dominate  the  field.  Nonetheless,  there  is  a  pressing  need  for  more  effective  robotic  solutions  able  to  significantly  speed  up  the  decommissioning  of  legacy  radioactive  waste.  This  paper  describes  a  novel  haptic  shared-control  approach  for  assisting  a  human  operator  in  the  sort  and  segregation  of  different  objects  in  a  cluttered  and  unknown  environment.  A  three-dimensional  scan  of  the  scene  is  used  to  generate  a  set  of  potential  grasp  candidates  on  the  objects  at  hand.  These  grasp  candidates  are  then  used  to  generate  guiding  haptic  cues,  which  assist  the  operator  in  approaching  and  grasping  the  objects.  The  haptic  feedback  is  designed  to  be  smooth  and  continuous  as  the  user  switches  from  a  grasp  candidate  to  the  next  one,  or  from  one  object  to  another  one,  avoiding  any  discontinuity  or  abrupt  changes.  To  validate  our  approach,  we  carried  out  two  human–subject  studies,  enrolling  15  participants.  We  registered  an  average  improvement  of  20.8%,  20.1%,  and  32.5%  in  terms  of  completion  time,  linear  trajectory,  and  perceived  effectiveness,  respectively,  between  the  proposed  approach  and  standard  teleoperation.
2	A  method  to  reveal  workload  weak  resilience  signals  at  a  rail  control  post.  Reorganization  of  a  rail  control  post  may  affect  its  ability  to  cope  with  unexpected  disruptions.  The  term  'resilience',  the  ability  to  manage  spare  adaptive  capacity  when  unexpected  events  occur,  encapsulates  this  situation.  This  paper  focuses  on  the  workload  adaptive  capacity  through  a  method  for  revealing  workload  weak-resilience-signals  WRS.  Three  different  workload  measurements  are  adapted  to  identify  structural  changes  in  workload.  The  first,  executed  cognitive  task  load,  targets  system  activities.  The  second,  integrated  workload  scale,  is  a  subjective  measure.  The  last,  heart  rate  variability,  identifies  physiological  arousal  because  of  workload.  An  experiment  is  designed  to  identify  the  workload  change  and  distribution  across  group  members  during  disruptions.  A  newly  defined  Stretch,  the  reaction  of  the  system  to  an  external  cluster-event,  is  used  to  reveal  a  workload  WRS.  The  method  is  suitable  for  real-time  usage  and  provides  the  means  for  the  rail  signaler  to  influence  the  system  through  his  subjective  workload  perception.
2	The  rooms  creating  immersive  experiences  through  projected  augmented  reality.  In  this  paper,  we  describe  The  Rooms  –  a  horror  game  that  uses  projected  augmented  reality  to  enable  a  new  type  of  immersive  gaming  experience.  Interviews  and  questionnaires  from  20  players  of  the  game  showed  that  projected  augmented  reality  games  can  support  immersion  and  can  avoid  the  problem  of  nausea  that  VR  experiences  can  invoke.  However,  since  players  experienced  different  types  of  immersion  the  study  suggests  that  a  model  that  allows  different  and  parallel  types  of  immersion  may  provide  more  informative  results  when  used  in  play  tests.  The  Rooms  also  suggested  that  a  new  type  of  immersion,  spatial  immersion,  might  be  a  relevant  addition  to  such  models.
2	Compositional  procedural  content  generation.  We  consider  the  strengths  and  drawbacks  of  various  procedural  content  generation  methods,  and  how  they  could  be  combined  to  hybrid  methods  that  retain  the  advantages  and  avoid  the  disadvantages  of  their  constituent  methods.  One  answer  is  composition,  where  one  method  is  nestled  inside  another.  As  an  example,  we  present  a  hybrid  evolutionary-ASP  dungeon  generator.
2	Semantic  constraints  for  procedural  generation  of  virtual  worlds.  Procedural  generation  of  virtual  worlds  is  a  promising  alternative  to  classical  manual  modelling  approaches,  which  usually  require  a  large  amount  of  effort  and  expertise.  However,  it  suffers  from  a  number  of  issues;  most  importantly,  the  lack  of  user  control  over  the  generation  process  and  its  outcome.  Because  of  this,  the  result  of  a  procedural  method  is  highly  unpredictable,  rendering  it  almost  unusable  for  virtual  world  designers.      This  paper  focuses  on  providing  user  control  to  deliver  an  outcome  consistent  with  designer's  intent.  For  this,  we  introduce  semantic  constraints,  a  flexible  concept  to  express  high-level  designer's  intent  in  intuitive  terms  as  e.g.  line  of  sight.  Our  constraint  evaluation  method  is  capable  of  detecting  the  context  in  which  such  a  constraint  is  specified,  automatically  adapting  to  surrounding  features  of  the  virtual  world.  From  experiments  performed  within  our  prototype  modelling  system,  we  can  conclude  that  semantic  constraints  are  another  step  forward  in  making  procedural  generation  of  virtual  worlds  more  controllable  and  accessible  to  non-specialist  designers.
2	Creative  contextual  dialog  adaptation  in  an  open  world  rpg.  Role  playing  games  rely  typically  on  hand-written  dialog  that  has  no  flexibility  in  adapting  to  the  game  state  such  as  the  level  of  the  player.  This  is  an  even  bigger  problem  for  open  world  RPGs  that  make  it  possible  to  complete  the  game  quests  and  objectives  virtually  in  any  given  order.  We  present  a  computationally  creative  method  for  adapting  Fallout  4  dialog  to  the  changes  in  the  game  state  using  word  embeddings  for  semantics  and  a  BRNN  for  sequence-to-sequence  paraphrasing  of  syntax.
2	Salient  time  steps  selection  from  large  scale  time  varying  data  sets  with  dynamic  time  warping.  Empowered  by  rapid  advance  of  high  performance  computer  architectures  and  software,  it  is  now  possible  for  scientists  to  perform  high  resolution  simulations  with  unprecedented  accuracy.  Nowadays,  the  total  size  of  data  from  a  large-scale  simulation  can  easily  exceed  hundreds  of  terabytes  or  even  petabytes,  distributed  over  a  large  number  of  time  steps.  The  sheer  size  of  data  makes  it  difficult  to  perform  post  analysis  and  visualization  after  the  computation  is  completed.  Frequently,  large  amounts  of  valuable  data  produced  from  simulations  are  discarded,  or  left  in  disk  unanalyzed.  In  this  paper,  we  present  a  novel  technique  that  can  retrieve  the  most  salient  time  steps,  or  key  time  steps,  from  large  scale  time-varying  data  sets.  To  achieve  this  goal,  we  develop  a  new  time  warping  technique  with  an  efficient  dynamic  programming  scheme  to  map  the  whole  sequence  into  an  arbitrary  number  of  time  steps  specified  by  the  user.  A  novel  contribution  of  our  dynamic  programming  scheme  is  that  the  mapping  between  the  whole  time  sequence  and  the  key  time  steps  is  globally  optimal,  and  hence  the  information  loss  is  minimum.  We  propose  a  high  performance  algorithm  to  solve  the  dynamic  programming  problem  that  makes  the  selection  of  key  times  run  in  real  time.  Based  on  the  technique,  we  create  a  visualization  system  that  allows  the  user  to  browse  time  varying  data  at  arbitrary  levels  of  temporal  detail.  Because  of  the  low  computational  complexity  of  this  algorithm,  the  tool  can  help  the  user  explore  time  varying  data  interactively  and  hierarchically.  We  demonstrate  the  utility  of  our  algorithm  by  showing  results  from  different  time-varying  data  sets.
2	Group  dynamics  in  scientific  visualization.  The  ability  to  visually  extract  and  track  features  is  appealing  to  scientists  in  many  simulations  including  flow  fields.  However,  as  the  resolution  of  the  simulation  becomes  higher,  the  number  of  features  to  track  increases  and  so  does  the  cost  in  large-scale  simulations.  Since  many  of  these  features  act  in  groups,  it  seems  more  cost-effective  to  follow  groups  of  features  rather  than  individual  ones.  Very  little  work  has  been  done  for  tracking  groups  of  features.  In  this  paper,  we  present  the  first  full  group  tracking  framework  in  which  we  track  groups  (clusters)  of  features  in  time-varying  3D  fluid  flow  simulations.  Our  framework  uses  a  clustering  algorithm  to  group  interacting  features.  We  demonstrate  the  use  of  our  framework  on  data  output  from  a  3D  simulation  of  wall  bounded  turbulent  flow.
2	Reflectance  and  illumination  estimation  for  realistic  augmentations  of  real  scenes.  The  acquisition  of  surface  material  properties  and  lighting  conditions  is  a  fundamental  step  for  photo-realistic  Augmented  Reality  (AR).  In  this  paper,  we  present  a  new  method  for  the  estimation  of  diffuse  and  specular  reflectance  properties  of  indoor  real  static  scenes.  Using  an  RGB-D  sensor,  we  further  estimate  the  3D  position  of  light  sources  responsible  for  specular  phenomena  and  propose  a  novel  photometry-based  classification  for  all  the  3D  points.  Our  algorithm  allows  convincing  AR  results  such  as  realistic  virtual  shadows  as  well  as  proper  illumination  and  specularity  occlusions.
2	Determining  detection  thresholds  for  fixed  positional  offsets  for  virtual  hand  remapping  in  virtual  reality.  Virtual  reality  commonly  makes  use  of  tracked  hand  interactions  for  user  input.  Interaction  techniques  sometimes  alter  the  mapping  between  the  real  and  virtual  coordinate  systems  to  modify  interaction  possibilities.  This  paper  studies  fixed  positional  offsets  applied  to  the  location  of  the  virtual  hand.  We  present  a  controlled  experiment  in  which  users’  hands  were  subject  to  fixed  positional  offsets  of  varying  magnitudes  while  completing  target-touching  tasks.  The  study  provides  estimations  for  detection  thresholds  for  positional  hand  offsets  in  six  directions  relative  to  the  real-world  location  of  the  hand  and  provides  evidence  performance  using  offset  virtual  hands  can  vary  based  on  offset  parameters.  Significant  differences  in  offset  detection  were  identified  based  on  offset  direction,  indicating  that  positional  adjustments  made  to  virtual  hands  should  consider  directionality  when  limiting  techniques  rather  than  just  a  constant  value.  Hand  offsets  kept  within  the  threshold  value  resulted  in  comparable  performance  to  unmodified  hand  registration,  while  offsets  beyond  the  threshold  resulted  in  larger  completion  times.
2	Foveated  instant  radiosity.  Foveated  rendering  distributes  computational  resources  based  on  visual  acuity,  more  in  the  foveal  regions  of  our  eyes  and  less  in  the  periphery.  The  traditional  rasterization  method  can  be  adapted  into  the  foveated  rendering  framework  in  a  quite  straightforward  way,  but  it’s  difficult  for  estimating  global  illumination.  Instant  Radiosity  is  an  efficient  global  illumination  method.  It  generates  Virtual  Point  Lights  (VPLs)  on  the  surface  of  the  virtual  scenes  from  light  sources  and  uses  these  VPLs  to  simulate  light  bounces.  However,  instant  radiosity  can  not  be  adapted  into  the  foveated  rendering  pipeline  directly,  and  is  too  slow  for  virtual  reality  experience.  What’s  more,  instant  radiosity  does  not  consider  temporal  coherence,  therefore  it  lacks  temporal  stability  for  dynamic  scenes.  In  this  paper,  we  propose  a  foveated  rendering  method  for  instant  radiosity  with  more  accurate  global  illumination  effects  in  the  foveal  region  and  less  accurate  global  illumination  in  the  peripheral  region.  We  define  a  foveated  importance  for  each  VPL,  and  use  it  to  smartly  distribute  the  VPLs  to  guarantee  the  rendering  precision  of  the  foveal  region.  Meanwhile,  we  propose  a  novel  VPL  reuse  scheme,  which  updates  only  a  small  fraction  of  VPLs  over  frames,  which  ensures  temporal  coherence  and  improves  time  efficiency.  Our  method  supports  dynamic  scenes  and  achieves  high  quality  in  the  foveal  regions  at  interactive  frame  rates.
2	Exploring  the  hearthstone  deck  space.  A  significant  issue  in  game  balancing  is  understanding  the  game  itself.  For  simple  games  end-to-end  optimization  approaches  can  help  explore  the  game's  design  space,  but  for  more  complex  games  it  is  necessary  to  isolate  and  explore  its  parts.  Hearthstone,  Blizzard's  popular  two-player  turn-taking  adversarial  card  game,  has  two  distinct  game-playing  challenges:  choosing  when  and  how  to  play  cards,  and  selecting  which  cards  a  player  can  access  during  the  game  (deckbuilding).  Focusing  on  deckbuilding,  four  experiments  are  conducted  to  computationally  explore  the  design  of  Hearthstone.  They  address  the  difficulty  of  constructing  good  decks,  the  specificity  and  generality  of  decks,  and  the  transitivity  of  decks.  Results  suggest  it  is  possible  to  find  decks  with  an  Evolution  Strategy  (ES)  that  convincingly  beat  other  decks  available  in  the  game,  but  that  they  also  exhibit  some  generality  (i.e.  they  perform  well  against  unknown  decks).  Interestingly,  a  second  ES  experiment  is  performed  where  decks  are  evolved  against  opponents  playing  the  originally  evolved  decks.  Since  the  originally  evolved  decks  beat  the  starter  decks,  and  the  twice  evolved  decks  beat  the  originally  evolved  decks,  some  degree  of  transitivity  of  the  deck  space  is  shown.  While  only  a  preliminary  study  with  restrictive  conditions,  this  paper  paves  the  way  for  future  work  computationally  identifying  properties  of  cards  important  for  different  gameplay  strategies  and  helping  players  build  decks  to  fit  their  personal  playstyles  without  the  need  for  in-depth  domain  knowledge.
2	Data  agent.  This  paper  introduces  DATA  Agent,  a  system  which  creates  murder  mystery  adventures  from  open  data.  In  the  game,  the  player  takes  on  the  role  of  a  detective  tasked  with  finding  the  culprit  of  a  murder.  All  characters,  places,  and  items  in  DATA  Agent  games  are  generated  using  open  data  as  source  content.  The  paper  discusses  the  general  game  design  and  user  interface  of  DATA  Agent,  and  provides  details  on  the  generative  algorithms  which  transform  linked  data  into  different  game  objects.  Findings  from  a  user  study  with  30  participants  playing  through  two  games  of  DATA  Agent  show  that  the  game  is  easy  and  fun  to  play,  and  that  the  mysteries  it  generates  are  straightforward  to  solve.
2	Player  centered  ai  for  automatic  game  personalization  open  problems.  Computer  games  represent  an  ideal  research  domain  for  the  next  generation  of  personalized  digital  applications.  This  paper  presents  a  player-centered  framework  of  AI  for  game  personalization,  complementary  to  the  commonly  used  system-centered  approaches.  Built  on  the  Structure  of  Actions  theory,  the  paper  maps  out  the  current  landscape  of  game  personalization  research  and  identifies  eight  open  problems  that  need  further  investigation.  These  problems  require  deep  collaboration  between  technological  advancement  and  player  experience  design.
2	Cloudbits  supporting  conversations  through  augmented  zero  query  search  visualization.  The  retrieval  of  additional  information  from  public  (e.g.,  map  data)  or  private  (e.g.,  e-mail)  information  sources  using  personal  smart  devices  is  a  common  habit  in  today's  co-located  conversations.  This  behavior  of  users  imposes  challenges  in  two  main  areas:  1)  cognitive  focus  switching  and  2)  information  sharing.      In  this  paper,  we  explore  a  novel  approach  for  conversation  support  through  augmented  information  bits,  allowing  users  to  see  and  access  information  right  in  front  of  their  eyes.  To  that  end,  we  investigate  the  requirements  for  the  design  of  a  user  interface  to  support  conversations  through  proactive  information  retrieval  in  an  exploratory  study.  Based  on  the  results,  we  2)  present  CloudBits:  A  set  of  visualization  and  interaction  techniques  to  provide  mutual  awareness  and  enhance  coupling  in  conversations  through  augmented  zero-query  search  visualization  along  with  its  prototype  implementation.  Finally,  we  3)  report  the  findings  of  a  qualitative  evaluation  and  conclude  with  guidelines  for  the  design  of  user  interfaces  for  conversation  support.
2	Motivational  game  design  patterns  of  ville  games.  The  phenomenal  growth  of  social  network  games  in  the  last  five  years  has  left  many  game  designers,  game  scholars,  and  long-time  game  players  wondering  how  these  games  so  effectively  engage  their  audiences.  Without  a  strong  understanding  of  the  sources  of  appeal  of  social  network  games,  and  how  they  relate  to  the  appeal  of  past  games  and  other  human  activities,  it  has  proven  difficult  to  interpret  the  phenomenon  accurately  or  build  upon  its  successes.  In  this  paper  we  propose  and  employ  a  particular  approach  to  this  challenge,  analyzing  the  motivational  game  design  patterns  in  the  popular  'Ville  style  of  game  using  the  lenses  of  behavioral  economics  and  behavioral  psychology,  explaining  ways  these  games  engage  and  retain  players.  We  show  how  such  games  employ  strategies  in  central,  visible  ways  that  are  also  present  (if  perhaps  harder  to  perceive)  in  games  with  very  different  mechanics  and  audiences.  Our  conclusions  point  to  lessons  for  game  design,  game  interpretation,  and  the  design  of  engaging  software  of  any  type.
2	Visual  cues  to  restore  student  attention  based  on  eye  gaze  drift  and  application  to  an  offshore  training  system.  Drifting  student  attention  is  a  common  problem  in  educational  environments.  We  demonstrate  8  attention-restoring  visual  cues  for  display  when  eye  tracking  detects  that  student  attention  shifts  away  from  critical  objects.  These  cues  include  novel  aspects  and  variations  of  standard  cues  that  performed  well  in  prior  work  on  visual  guidance.  Our  cues  are  integrated  into  an  offshore  training  system  on  an  oil  rig.  While  students  participate  in  training  on  the  oil  rig,  we  can  compare  our  various  cues  in  terms  of  performance  and  student  preference,  while  also  observing  the  impact  of  eye  tracking.  We  demonstrate  experiment  software  with  which  users  can  compare  various  cues  and  tune  selected  parameters  for  visual  quality  and  effectiveness.
2	An  interactive  augmented  reality  coloring  book.  Creating  entertaining  and  educational  books  not  only  requires  providing  visually  stimulating  content  but  also  means  for  students  to  interact,  create,  and  express  themselves.  In  this  paper  we  present  a  new  type  of  mixed-reality  book  experience,  which  augments  an  educational  coloring  book  with  user-generated  three  dimensional  content.  We  explore  a  “pop-up  book”  metaphor  and  describe  a  process  by  which  children's  drawing  and  coloring  is  used  as  input  to  generate  and  change  the  appearance  of  the  book  content.  Our  system  is  based  on  natural  feature  tracking  and  image  processing  techniques  that  can  be  easily  exploited  for  other  AR  publishing  applications.
2	Assessing  upper  extremity  motor  dysfunction  using  an  augmented  reality  game.  An  old  but  still  ongoing  subject  of  debate  among  augmented  reality  (AR)  experts  is  about  which  see-through  paradigm  is  best  in  wearable  AR  displays.  Video  see-through  (VST)  and  optical  see-through  (OST)  paradigms  have  both  their  own  strengths  and  shortcomings  with  respect  to  technological  and  human-factor  aspects.  The  major  difference  between  these  see-through  paradigms  is  in  providing  an  aided  (VST)  or  unaided  (OST)  view  of  the  real  world.  In  this  work,  we  present  a  novel  approach  for  the  development  of  AR  stereoscopic  head-mounted  displays  (HMDs)  that  can  provide  both  the  see-through  mechanisms.  Our  idea  is  to  dynamically  modify  the  transparency  of  the  display  through  a  liquid  crystal  (LC)-based  electro-optical  shutter  applied  on  the  top  of  a  standard  OST  device  opportunely  modified  for  housing  a  pair  of  external  cameras.  A  plane-induced  homography  transformation  is  used  for  consistently  warping  the  video  images,  hence  reducing  the  parallax  between  cameras  and  displays.  An  externally  applied  drive  voltage  is  used  for  smoothly  controlling  the  light  transmittance  of  the  LC  shutters  so  as  to  allow  an  easy  transition  between  the  unaided  and  the  camera-mediated  view  of  the  real  scene.  Our  tests  have  proven  the  efficacy  of  the  proposed  solution  under  worst-case  lighting  conditions.
2	Single  view  augmentation  of  3d  elastic  objects.  This  paper  proposes  an  efficient  method  to  capture  and  augment  highly  elastic  objects  from  a  single  view.  3D  shape  recovery  from  a  monocular  video  sequence  is  an  underconstrained  problem  and  many  approaches  have  been  proposed  to  enforce  constraints  and  resolve  the  ambiguities.  State-of-the  art  solutions  enforce  smoothness  or  geometric  constraints,  consider  specific  deformation  properties  such  as  inextensibility  or  ressort  to  shading  constraints.  However,  few  of  them  can  handle  properly  large  elastic  deformations.  We  propose  in  this  paper  a  real-time  method  which  makes  use  of  a  me  chanical  model  and  is  able  to  handle  highly  elastic  objects.  Our  method  is  formulated  as  a  energy  minimization  problem  accounting  for  a  non-linear  elastic  model  constrained  by  external  image  points  acquired  from  a  monocular  camera.  This  method  prevents  us  from  formulating  restrictive  assumptions  and  specific  constraint  terms  in  the  minimization.  The  only  parameter  involved  in  the  method  is  the  Young's  modulus  where  we  show  in  experiments  that  a  rough  estimate  of  its  value  is  sufficient  to  obtain  a  good  reconstruction.  Our  method  is  compared  to  existing  techniques  with  experiments  conducted  on  computer-generated  and  real  data  that  show  the  effectiveness  of  our  approach.  Experiments  in  the  context  of  minimally  invasive  liver  surgery  are  also  provided.
2	Window  shaping  3d  design  ideation  in  mixed  reality.  We  present,  Window-Shaping,  a  mobile,  markerless,  mixed-reality  (MR)  interface  for  creative  design  ideation  that  allows  for  the  direct  creation  of  3D  shapes  on  and  around  physical  objects.  Using  the  sketch-and-inflate  scheme,  we  present  a  design  workflow  where  users  can  create  dimensionally  consistent  and  visually  coherent  3D  models  by  borrowing  visual  and  dimensional  attributes  from  existing  physical  objects.
2	Transform  beyond  tangible  bits  towards  radical  atoms.  Whereas  today's  mainstream  Human  Computer  Interaction  (HCI)  research  addresses  functional  concerns  -  the  needs  of  users,  practical  applications,  and  usability  evaluation  -  Tangible  Bits  and  Radical  Atoms  are  driven  by  vision.  This  is  because  today's  technologies  will  become  obsolete  in  one  year,  and  today's  applications  will  be  replaced  in  10  years,  but  true  visions,  we  believe,  can  last  longer  than  100  years.  Tangible  Bits  seeks  to  realize  seamless  interfaces  between  humans,  digital  information,  and  the  physical  environment  by  giving  physical  form  to  digital  information,  making  bits  directly  manipulable  and  perceptible.  Our  goal  is  to  invent  new  design  media,  taking  advantage  of  the  richness  of  human  senses  and  skills  as  developed  through  our  lifetime  of  interaction  with  the  physical  world  as  well  as  the  computational  reflection  enabled  by  real-time  sensing  and  digital  feedback.  Radical  Atoms  takes  a  leap  beyond  Tangible  Bits  by  assuming  a  hypothetical  generation  of  materials  that  can  change  form  and  properties  dynamically  and  computationally,  becoming  as  reconfigurable  as  pixels  on  a  screen.  Radical  Atoms  is  the  future  material  that  can  transform  its  shape,  conform  to  constraints,  and  inform  the  users  of  their  affordances.  Radical  Atoms  is  a  vision  for  the  future  of  human-material  interaction,  in  which  all  digital  information  has  a  tangible  manifestation  for  interactions.  I  present  the  trajectory  of  our  vision-driven  design  research  from  Tangible  Bits  towards  Radical  Atoms,  and  a  variety  of  design  projects  presented  and  exhibited  in  arts,  design,  and  science  communities  in  the  past  20  years.
2	Out  of  reach  a  novel  ar  interface  approach  for  motor  rehabilitation.  Mixed  reality  rehabilitation  systems  and  games  are  demonstrating  potential  as  innovative  adjunctive  therapies  for  health  professionals  in  their  treatment  of  various  hand  and  upper  limb  motor  impairments.  Unilateral  motor  deficits  of  the  arm,  for  example,  are  commonly  experienced  post  stroke.  Our  TheraMem  system  provides  an  augmented  reality  game  environment  that  contributes  to  this  increasingly  rich  area  of  research.  We  present  a  prototype  system  which  “fools  the  brain”  by  visually  amplifying  users'  hand  movements  —  small  actual  hand  movements  lead  to  perceived  larger  movements.  We  validate  the  usability  of  our  system  in  an  empirical  study  with  forty-five  non-clinical  participants.  In  addition,  we  present  early  qualitative  evidence  for  the  utility  of  our  approach  and  system  for  stroke  recovery  and  motor  rehabilitation.  Future  uses  of  the  system  are  considered  by  way  of  conclusion.
2	Introducing  augmented  reality  to  optical  coherence  tomography  in  ophthalmic  microsurgery.  Augmented  Reality  (AR)  in  microscopic  surgery  has  been  subject  of  several  studies  in  the  past  two  decades.  Nevertheless,  AR  has  not  found  its  way  into  everyday  microsurgical  workflows.  The  introduction  of  new  surgical  microscopes  equipped  with  Optical  Coherence  Tomography  (OCT)  enables  the  surgeons  to  perform  multimodal  (optical  and  OCT)  imaging  in  the  operating  room.  Taking  full  advantage  of  such  elaborate  source  of  information  requires  sophisticated  intraoperative  image  fusion,  information  extraction,  guidance  and  visualization  methods.  Medical  AR  is  a  unique  approach  to  facilitate  utilization  of  multimodal  medical  imaging  devices.  Here  we  propose  a  novel  medical  AR  solution  to  the  long-known  problem  of  determining  the  distance  between  the  surgical  instrument  tip  and  the  underlying  tissue  in  ophthalmic  surgery  to  further  pave  the  way  of  AR  into  the  surgical  theater.  Our  method  brings  augmented  reality  to  OCT  for  the  first  time  by  augmenting  the  surgeon's  view  of  the  OCT  images  with  an  estimated  instrument  cross-section  shape  and  distance  to  the  retinal  surface  using  only  information  from  the  shadow  of  the  instrument  in  intraoperative  OCT  images.  We  demonstrate  the  applicability  of  our  method  in  retinal  surgery  using  a  phantom  eye  and  evaluate  the  accuracy  of  the  augmented  information  using  a  micromanipulator.
2	Walking  in  place  for  vr  navigation  independent  of  gaze  direction  using  a  waist  worn  inertial  measurement  unit.  Many  techniques  have  been  proposed  for  navigation  using  head-mounted-displays  (HMDs)  in  virtual  reality  (VR).  A  walking-in-place  (WIP)  interface  for  virtual  locomotion  provides  a  high  presence  and  an  immersive  experience  in  a  virtual  environment  (VE).  However,  most  of  the  WIP  techniques  can  only  navigate  users  in  the  direction  of  their  gaze.  Other  WIP  methods  considering  virtual  locomotion  direction  have  complex  configurations  or  are  only  feasible  when  the  tracking  spaces  are  not  limited.  This  paper  proposes  a  WIP  interface  independent  of  gaze  direction  based  on  the  data  analysis  of  waist-mounted  inertial  sensors.  Our  method  can  navigate  in  the  locomotion  direction  by  calculating  the  orientation  of  the  pelvis.  We  experimentally  compared  two  WIP  methods  using  a  navigation  task  that  required  participants  to  periodically  observe  the  surrounding  VE:  (1)  Conventional  WIP  (gaze-based  direction)  (2)  Proposed  WIP  (pelvis-based  direction).  While  there  was  no  difference  in  learnability  or  cybersickness  between  the  two  methods,  the  proposed  method  had  shorter  task  time  and  higher  efficiency.
2	Seeing  is  believing  improving  the  perceived  trust  in  visually  embodied  alexa  in  augmented  reality.  Voice-activated  Intelligent  Virtual  Assistants  (IVAs)  such  as  Amazon  Alexa  offer  a  natural  and  realistic  form  of  interaction  that  pursues  the  level  of  social  interaction  among  real  humans.  The  user  experience  with  such  technologies  depends  to  a  large  degree  on  the  perceived  trust  in  and  reliability  of  the  IVA.  In  this  poster,  we  explore  the  effects  of  a  three-dimensional  embodied  representation  of  Amazon  Alexa  in  Augmented  Reality  (AR)  on  the  user's  perceived  trust  in  her  being  able  to  control  Internet  of  Things  (IoT)  devices  in  a  smart  home  environment.  We  present  a  preliminary  study  and  discuss  the  potential  of  positive  effects  in  perceived  trust  due  to  the  embodied  representation  compared  to  a  voice-only  condition.
2	Big  foot  using  the  size  of  a  virtual  foot  to  scale  gap  width.  Spatial  perception  research  in  the  real  world  and  in  virtual  environments  suggests  that  the  body  (e.g.,  hands)  plays  a  role  in  the  perception  of  the  scale  of  the  world.  However,  little  research  has  closely  examined  how  varying  the  size  of  virtual  body  parts  may  influence  judgments  of  action  capabilities  and  spatial  layout.  Here,  we  questioned  whether  changing  the  size  of  virtual  feet  would  affect  judgments  of  stepping  over  and  estimates  of  the  width  of  a  gap.  Participants  viewed  their  disembodied  virtual  feet  as  small  or  large  and  judged  both  their  ability  to  step  over  a  gap  and  the  size  of  gaps  shown  in  the  virtual  world.  Foot  size  affected  both  affordance  judgments  and  size  estimates  such  that  those  with  enlarged  virtual  feet  estimated  they  could  step  over  larger  gaps  and  that  the  extent  of  the  gap  was  smaller.  Shrunken  feet  led  to  the  perception  of  a  reduced  ability  to  step  over  a  gap  and  smaller  estimates  of  width.  The  results  suggest  that  people  use  their  visually  perceived  foot  size  to  scale  virtual  spaces.  Regardless  of  foot  size,  participants  felt  that  they  owned  the  feet  rendered  in  the  virtual  world.  Seeing  disembodied,  but  motion-tracked,  virtual  feet  affected  spatial  judgments,  suggesting  that  the  presentation  of  a  single  tracked  body  part  is  sufficient  to  produce  similar  effects  on  perception,  as  has  been  observed  with  the  presence  of  fully  co-located  virtual  self-avatars  or  other  body  parts  in  the  past.
2	Sketchtop  design  collaboration  on  a  multi  touch  tabletop.  Computer  mediated  group  collaboration,  particularly  in  the  design  and  engineering  disciplines,  is  in  need  of  better  applications  that  suit  the  needs  of  effective  exchange  of  information.  Multi-touch  surfaces  offer  the  capabilities  to  augment  and  better  enable  face-to-face  interaction  with  digital  content  and  applications.  This  paper  presents  the  design,  development  and  testing  of  SketchTop,  a  multi-touch  sketching  application  for  collocated  design  collaboration.
2	Perception  and  prediction  of  social  intentions  from  human  body  motion.  Better  understanding  of  how  social  traits  and  intentions  are  conveyed  through  human  body  motion  is  central  in  creating  believable,  life-like  virtual  characters.  Body  motion  conveys  socially  relevant  information  on  which  people  make  judgments,  such  as  other  people's  personality  or  gender.  [e.g.  Koppensteiner  and  Grammer  2011,  Thoresen  et  al.  2012].  However,  it  is  unclear  how  the  body  motion  features  are  related  to  the  perceived  social  traits  and  intentions  of  others.  The  main  goal  of  this  study  is  to  examine  how  visual  information  from  human  body  motion  is  related  to  social  traits  and  intention  perception.  Furthermore,  we  also  investigated  if  body  motion  features  can  be  used  as  predictors  of  social  intentions.
2	Comparison  of  two  methods  for  improving  distance  perception  in  virtual  reality.  Distance  is  commonly  underperceived  in  virtual  environments  (VEs)  compared  to  real  environments.  Past  work  suggests  that  displaying  a  replica  VE  based  on  the  real  surrounding  environment  leads  to  more  accurate  judgments  of  distance,  but  that  work  has  lacked  the  necessary  control  conditions  to  firmly  make  this  conclusion.  Other  research  indicates  that  walking  through  a  VE  with  visual  feedback  improves  judgments  of  distance  and  size.  This  study  evaluated  and  compared  those  two  methods  for  improving  perceived  distance  in  VEs.  All  participants  experienced  a  replica  VE  based  on  the  real  lab.  In  one  condition,  participants  visually  previewed  the  real  lab  prior  to  experiencing  the  replica  VE,  and  in  another  condition  they  did  not.  Participants  performed  blind-walking  judgments  of  distance  and  also  judgments  of  size  in  the  replica  VE  before  and  after  walking  interaction.  Distance  judgments  were  more  accurate  in  the  preview  compared  to  no  preview  condition,  but  size  judgments  were  unaffected  by  visual  preview.  Distance  judgments  and  size  judgments  increased  after  walking  interaction,  and  the  improvement  was  larger  for  distance  than  for  size  judgments.  After  walking  interaction,  distance  judgments  did  not  differ  based  on  visual  preview,  and  walking  interaction  led  to  a  larger  improvement  in  judged  distance  than  did  visual  preview.  These  data  suggest  that  walking  interaction  may  be  more  effective  than  visual  preview  as  a  method  for  improving  perceived  space  in  a  VE.
2	The  effects  of  peripheral  vision  and  light  stimulation  on  distance  judgments  through  hmds.  Egocentric  distances  are  often  underestimated  in  virtual  environments  through  head-mounted  displays  (HMDs).  Previous  studies  suggest  that  peripheral  vision  can  influence  distance  perception.  Specifically,  light  in  the  periphery  may  improve  distance  judgments  in  HMDs.  In  this  study,  we  conducted  a  series  of  experiments  with  varied  peripheral  treatments  around  the  viewport.  First,  we  found  that  the  peripheral  brightness  significantly  influences  distance  judgments  when  the  periphery  is  brighter  than  a  certain  threshold,  and  found  a  possible  range  where  the  threshold  was  in.  Second,  we  extended  our  previous  research  by  changing  the  size  of  the  peripheral  treatment.  A  larger  visual  field  (field  of  view  of  the  HMD)  resulted  in  significantly  more  accurate  distance  judgments  compared  to  our  original  experiments  with  black  peripheral  treatment.  Last,  we  found  that  applying  a  pixelated  peripheral  treatment  can  also  improve  distance  judgments.  The  result  implies  that  augmenting  peripheral  vision  with  secondary  low-resolution  displays  may  improve  distance  judgments  in  HMDs.
2	Balansar  using  spatial  augmented  reality  to  train  children  s  balancing  skills  in  physical  education.  Balancing  is  one  of  the  essential  motor  skills  to  adopt  and  maintain  an  active  lifestyle  in  child-  and  adulthood.  Visual  perception  plays  an  important  role  in  learning  balancing  skills.  Current  balancing  exercises,  however,  offer  few  opportunities  for  practicing  visual  perception.      A  technology  we  consider  promising  for  Physical  Education  (PE)  is  Spatial  Augmented  Reality  (SAR),  the  projection  of  (interactive)  animations  onto  three-dimensional  surfaces.  We  present  BalanSAR,  a  novel  combination  of  SAR,  existing  PE  balancing  exercises  and  traditional  PE  gym  equipment.  By  projecting  animations  onto  the  gym  floor  and  existing  equipment,  visual  perception  in  balancing  exercises  can  be  practiced  more  effectively.      We  propose  a  study  to  investigate  the  feasibility  and  the  effect  of  using  SAR  on  the  efficiency  and  effectiveness  of  PE  in  terms  of  children's  motor  skills  and  their  motivation  in  PE  and  physical  activity  in  general.
2	Desu  100  about  the  temptation  to  destroy  a  robot.  Whilst  previous  research  examined  the  behaviour  of  humans  instructed  to  destroy  robots,  this  paper  is  concerned  with  the  question  "Are  humans  tempted  to  destroy  robots?"  For  this  purpose,  an  interactive  installation  was  created,  consisting  of  the  robot  DESU  100  and  a  button  on  a  pedestal.  The  visitors  of  this  installation  were  faced  with  the  opportunity  to  push  the  button,  and  herewith  forcing  the  robot  to  hit  itself.  Despite  feeling  sympathy  for  DESU  100,  most  of  the  visitors  yielded  to  the  temptation  and  pressed  the  button  at  least  once,  experiencing  satisfaction.
2	Qook  enhancing  information  revisitation  for  active  reading  with  a  paper  book.  Revisiting  information  on  previously  accessed  pages  is  a  common  activity  during  active  reading.  Both  physical  and  digital  books  have  their  own  benefits  in  supporting  such  activity  according  to  their  manipulation  natures.  In  this  paper,  we  introduce  QOOK,  a  paper-book  based  interactive  reading  system,  which  integrates  the  advanced  technology  of  digital  books  with  the  affordances  of  physical  books  to  facilitate  people's  information  revisiting  process.  The  design  goals  of  QOOK  are  derived  from  the  literature  survey  and  our  field  study  on  physical  and  digital  books  respectively.  QOOK  allows  page  flipping  just  like  on  a  real  book  and  enables  people  to  use  electronic  functions  such  as  keyword  searching,  highlighting  and  bookmarking.  A  user  study  is  conducted  and  the  study  results  demonstrate  that  QOOK  brings  faster  information  revisiting  and  better  reading  experience  to  readers.
2	Interacting  with  electroactive  polymers  in  responsive  environments.  In  this  paper,  we  discuss  the  opportunities  and  challenges  of  creating  responsive  environments  with  Electroactive  Polymers  (EAPs).  Our  previous  research  on  tools  and  methods  for  EAPs  enabled  us  to  develop  two  public  installations:  SOLO  and  Electric  Animal  Plant.  Going  beyond  the  demonstration  of  EAPs,  these  projects  explore  the  aesthetics  and  interactivity  of  such  shape-changing  materials.  We  explain  actuating  and  sensing  capabilities  of  EAPs  in  these  works,  and  we  reflect  on  the  response  of  the  participants  to  the  installations.
2	On  the  relationship  between  the  force  jnd  and  the  stiffness  jnd  in  haptic  perception.  A  large  variation  of  the  haptic  Just  Noticeable  difference  (JND)  in  stiffness  is  found  in  literature.  But  no  underlying  model  that  explains  this  variation  was  found,  limiting  the  practical  use  of  the  stiffness  JND  in  the  evaluation  work  of  control  loading  system  (CLS).  To  this  end,  we  investigated  the  cause  of  this  variation  from  humans'  strategy  for  stiffness  discrimination,  by  two  experiments  in  which  a  configurable  manipulator  was  used  to  generate  an  elastic  force  proportional  to  its  angular  displacement  (deflection).  In  a  first  experiment,  the  stiffness  JND  was  measured  for  three  stiffness  levels,  and  an  invariant  Weber  fraction  was  obtained.  We  found  that  for  stiffness  discrimination,  subjects  reproduced  the  same  amount  of  the  manipulator  deflection  and  used  the  difference  in  the  terminal  forces  as  the  indication  of  the  stiffness  difference.  We  demonstrated  that  the  stiffness  Weber  fraction  and  the  force  Weber  fraction  could  be  related  by  a  systematic  bias  in  the  deflection  reproduction,  which  was  caused  by  the  difference  in  the  manipulator  stiffness.  A  second  experiment  with  two  conditions  was  done  to  verify  this  model.  In  one  condition,  we  measured  the  stiffness  JND  while  asking  subjects  to  move  the  manipulator  to  a  target  angular  displacement.  Thus  the  bias  in  the  deflection  reproduction  was  eliminated,  and  this  resulted  a  stiffness  Weber  fraction  that  equaled  the  force  Weber  fraction.  In  the  other  condition,  the  stiffness  JND  was  measured  without  the  deflection  target,  and  a  bias  in  deflection  reproduction  was  again  observed.  This  bias  related  the  measurements  for  the  two  conditions  by  the  formulation  obtained  from  the  first  experiment.  This  suggests  that  the  accuracy  of  reproducing  the  manipulator  position  for  stiffness  discrimination,  which  may  be  susceptible  to  experimental  setting,  can  be  used  to  explain  the  variation  of  stiffness  JND  in  literature.  Suggestions  are  given  for  CLS  evaluation  and  applications  requiring  precise  manipulator  motion  control.
2	Blob  manipulation.  This  paper  introduces  Blob  Manipulation,  the  interaction  technique  with  fluidic  soft  matter.  Most  of  the  soft  matters  are  substances  between  liquid  and  solid  and  possess  viscoelasticity.  We  focus  on  this  materiality  and  propose  a  novel  interaction  technique.  A  stirring  rod  is  used  as  the  input  tool.  When  the  system  detects  a  user  input  such  as  touching,  rubbing  or  tapping,  the  corresponding  transformation  will  be  executed.  Six  basic  operations  were  designed  to  transform  fluidic  soft  matter  geometrically  and  topologically.  Rheological  user  interface  associated  with  metamorphose  is  expected  to  pioneer  new  possibilities  for  design,  education  and  entertainment.
2	Bimodal  task  facilitation  in  a  virtual  traffic  scenario  through  spatialized  sound  rendering.  Audio  rendering  is  generally  used  to  increase  the  realism  of  virtual  environments  (VE).  In  addition,  audio  rendering  may  also  improve  the  performance  in  specific  tasks  carried  out  in  interactive  applications  such  as  games  or  simulators.  In  this  article  we  investigate  the  effect  of  the  quality  of  sound  rendering  on  task  performance  in  a  task  which  is  inherently  vision-dominated.  The  task  is  a  virtual  traffic  gap-crossing  scenario  with  two  elements:  first,  to  discriminate  crossable  and  uncrossable  gaps  in  oncoming  traffic,  and  second,  to  find  the  right  timing  to  start  crossing  the  street  without  an  accident.  A  study  was  carried  out  with  48  participants  in  an  immersive  virtual  environment  setup  with  a  large  screen  and  headphones.  Participants  were  grouped  into  three  different  scenarios.  In  the  first  one,  spatialized  audio  rendering  with  head-related  transfer  function  (HRTF)  filtering  was  used.  The  second  group  was  tested  with  conventional  stereo  rendering,  and  the  remaining  group  ran  the  experiment  in  a  mute  condition.  Our  results  give  a  clear  evidence  that  spatialized  audio  improves  task  performance  compared  to  the  unimodal  mute  condition.  Since  all  task-relevant  information  was  in  the  participants'  field-of-view,  we  conclude  that  an  enhancement  of  task  performance  results  from  a  bimodal  advantage  due  to  the  integration  of  visual  and  auditory  spatial  cues.
2	Conformance  relations  for  labeled  event  structures.  We  propose  a  theoretical  framework  for  testing  concurrent  systems  from  true  concurrency  models  like  Petri  nets  or  networks  of  automata.  The  underlying  model  of  computation  of  such  formalisms  are  labeled  event  structures,  which  allow  to  represent  concurrency  explicitly.  The  activity  of  testing  relies  on  the  definition  of  a  conformance  relation  that  depends  on  the  observable  behaviors  on  the  system  under  test,  which  is  given  for  sequential  systems  by  ioco  type  relations.  However,  these  relations  are  not  capable  of  capturing  and  exploiting  concurrency  of  non  sequential  behavior.  We  study  different  conformance  relations  for  labeled  event  structures,  relying  on  different  notions  of  observation,  and  investigate  their  properties  and  connections.
2	Inter  institutional  protocol  describing  the  use  of  three  dimensional  printing  for  surgical  planning  in  a  patient  with  childhood  epilepsy  from  3d  modeling  to  neuronavigation.  This  study  is  the  first  step  in  an  effort  to  develop  three-dimensional  (3D)  printing  for  use  in  pediatric  surgical  planning.  In  order  to  accomplish  this,  we  established  an  effective  collaboration  between  Ribeirao  Preto  Clinics  Hospital  (HCRP)  and  Renato  Archer  Center  for  Information  Technology  (CTI).  Printed  biomodels  can  be  used  to  support  discussions,  decision-making,  and  neuronavigation  before  surgery.  The  main  purpose  of  3D  printing  for  specific  case  handling  is  to  reduce  damage  by  enhancing  knowledge  of  orientation  during  surgical  planning  and  personnel  training  before  surgery.  Here,  we  produced  an  object  that  represented  the  brain  and  face  segment  of  a  patient  via  additive  manufacturing  technology  based  on  magnetic  resonance  imaging  (MRI)  data.  Specific  landmarks  were  measured  by  three  distinct  methods:  manual  caliper,  an  InVesalius  software  measurement  tool,  and  neuronavigation  coordinate  detection.  The  mean  coefficient  of  variation  was  7.17%  between  all  methods  and  landmarks  measured.  Our  results  validate  the  combined  use  of  biomodels  with  InVesalius  software  tools  for  the  assessment  of  individual  brain  anatomy  facilitating  manual  handling  and  visualization  of  3D  models.  The  establishment  of  communication  protocols  between  the  teams  involved,  as  well  as  navigation  protocols  for  quality  control,  presents  the  possibility  of  developing  long  term  training  programs,  and  promotes  the  congregation  of  individuals  from  research  areas  in  Medical  Physics,  Medical  Sciences,  and  Neuroscience.
2	A  reliable  efficient  routing  protocol  for  dynamic  topology  in  wireless  body  area  networks  using  min  max  multi  commodity  flow  model.  WBSNs  (wireless  body  sensor  network)  like  any  other  sensor  networks  suffers  limited  energy  and  are  the  highly  distributed  network,  in  which  its  nodes  are  the  organizer  itself  and  each  of  them  has  the  flexibility  of  collecting  and  transmitting  patient  biomedical  information  to  a  sink.  When  knowledge  sent  to  sink  from  a  path  that  doesn't  have  a  definite  basis,  the  routing  is  a  crucial  challenge  in  Wireless  Body  Area  Sensor  Networks,  additionally  reliability  and  routing  delay  are  the  considerable  factors  in  these  type  of  networks.  Most  of  the  attention  should  be  given  to  the  energy  routing  where  energy  awareness  is  an  essential  consideration  in  WBSNs  and  the  frequent  topology  change  increases  the  dynamics  of  network  topology,  and  complicates  the  process  of  relay  selection  in  cooperative  communications.  In  this  paper,  we  propose  a  Min-Max  multi-commodity  flow  model  for  WBSNs  which  allows  to  prevent  sensor  node  saturation  and  take  best  action  against  reliability  and  the  path  loss,  by  imposing  an  equilibrium  use  of  sensors  during  the  routing  process.  Simulation  results  show  that  the  algorithm  balances  the  energy  consumption  of  nodes  effectively  and  maximize  the  network  lifetime.  It  will  meet  the  enhanced  WBSNs  requirements,  including  better  delivery  ratio,  less  reliable  routing  overhead.
2	Fpga  based  preliminary  cad  for  kidney  on  iot  enabled  portable  ultrasound  imaging  system.  Ultrasound  imaging  has  been  widely  used  for  preliminary  diagnosis  as  it  is  non-invasive  and  has  good  scope  for  the  doctors  to  analyze  many  diseases.  Lack  of  trained  sonographers  make  ultrasound  imaging  diagnosis  time  consuming  to  detect  any  abnormality.  Sometimes  the  problem  cannot  exactly  be  identified  which  may  lead  to  error  in  diagnosis.  Hence  in  this  paper  we  present  computer  aided  automatic  detection  of  abnormality  in  kidney  on  the  ultrasound  system  itself,  to  decrease  the  time  for  reports  and  not  to  depend  on  the  sonographer.  We  classified  the  kidney  as  normal  and  abnormal  case.  Segment  the  kidney  region  and  extract  Intensity  histogram  features  and  Haralick  features  from  Gray  Level  Cooccurnace  Matrix  (GLCM).  These  features  are  calculated  for  a  set  of  large  data  containing  both  normal  and  abnormal  cases.  Abnormal  case  includes  kidney  stone,  cyst  and  bacterial  infection.  Standard  deviation  for  each  parameter  is  observed,  considered  only  those  features  with  less  deviation  and  implemented  on  FPGA  Kintex  board.  If  the  range  of  mean  value  is  1.08  to  1.336,  skewness  is  2.882  to  7.708,  Kurtosis  is  1.06  to  71.152,  Cluster  Shade  is  72  to  243,  Homogeneity  is  0.993  to  0.998,  the  observed  kidney  image  is  normal  otherwise  abnormal.
2	Software  validation  via  model  animation.  This  paper  explores  a  new  approach  to  validating  software  implementations  that  have  been  produced  from  formally-verified  algorithms.  Although  visual  inspection  gives  some  confidence  that  the  implementations  faithfully  reflect  the  formal  models,  it  does  not  provide  complete  assurance  that  the  software  is  correct.  The  proposed  approach,  which  is  based  on  animation  of  formal  specifications,  compares  the  outputs  computed  by  the  software  implementations  on  a  given  suite  of  input  values  to  the  outputs  computed  by  the  formal  models  on  the  same  inputs,  and  determines  if  they  are  equal  up  to  a  given  tolerance.  The  approach  is  illustrated  on  a  prototype  air  traffic  management  system  that  computes  simple  kinematic  trajectories  for  aircraft.  Proofs  for  the  mathematical  models  of  the  system’s  algorithms  are  carried  out  in  the  Prototype  Verification  System  (PVS).  The  animation  tool  PVSio  is  used  to  evaluate  the  formal  models  on  a  set  of  randomly  generated  test  cases.  Output  values  computed  by  PVSio  are  compared  against  output  values  computed  by  the  actual  software.  This  comparison  improves  the  assurance  that  the  translation  from  formal  models  to  code  is  faithful  and  that,  for  example,  floating  point  errors  do  not  greatly  affect  correctness  and  safety  properties.
2	Analysis  of  neural  correlates  of  saccadic  eye  movements.  In  a  concurrent  electroencephalography  (EEG)  and  eye-tracking  study,  we  explore  the  specific  neural  responses  associated  with  saccadic  eye  movements.  We  hypothesise  that  there  is  a  distinct  saccade-related  neural  response  that  occurs  well  before  a  physical  saccade  and  that  this  response  is  different  for  free,  natural  saccades  versus  forced  saccades.  Our  results  show  a  distinct  and  measurable  brain  response  approximately  200  ms  before  a  physical  saccade  actually  occurs.  This  response  is  distinctly  different  for  free  saccades  versus  forced  saccades.  Our  results  open  up  possibilities  of  predicting  saccades  based  on  neural  data.  This  is  of  particular  relevance  for  creating  effective  gaze  guidance  mechanisms  within  a  virtual  reality  (VR)  environment  and  for  creating  faster  brain  computer  interfaces  (BCI).
2	Exploring  the  power  of  feedback  loops  in  wearables  computers.  Touch,  sight,  smell,  hearing  and  taste  -  our  senses  link  us  to  the  outside  world.  Reflexes  react  to  all  stimuli  arriving  simultaneously  to  our  sensory  environment.  But  there  are  lapses  in  awareness  of  seemingly  obvious  stimuli  to  temporary  losses  of  attention  and  lapses  that  we  are  not  aware  of  in  the  form  of  reflexes.  The  main  motivation  of  this  research  is  to  plug  these  lapses  with  the  power  of  feedback  loops  in  environments  where  human  and  wearable  computers  are  intertwined  and  explore  their  application  as  tools  for  self-modification  and  sustainable  change.  This  work  proposes  a  combination  of  body  worn  objects  and  hidden  technology  to  create  compelling,  aesthetic  solutions  that  not  only  appeal  to  our  senses,  but  which  fuse  seamlessly  with  our  everyday  lives.  In  order  to  exemplify  this  exploration,  we  created  Blinklifier,  a  wearable  device  that  senses  our  reflex  of  blinking  through  conductive  makeup  and  metalized  eyelashes,  and  amplifies  it.
2	The  atb  framework  quantifying  and  classifying  epistemic  strategies  in  tangible  problem  solving  tasks.  In  task  performance,  pragmatic  actions  refer  to  behaviors  that  make  direct  progress,  while  epistemic  actions  involve  altering  the  world  so  that  cognitive  processes  are  faster,  more  reliable  or  less  taxing.  Epistemic  actions  are  frequently  presented  as  a  beneficial  consequence  of  interacting  with  tangible  systems.  However,  we  currently  lack  tools  to  measure  epistemic  behaviors,  making  substantiating  such  claims  highly  challenging.  This  paper  addresses  this  problem  by  presenting  ATB,  a  video-coding  framework  that  enables  the  identification  and  measurement  of  different  epistemic  actions  during  problem-solving  tasks.  The  framework  was  developed  through  a  systematic  literature  review  of  78  papers,  and  analyzed  through  a  study  involving  a  jigsaw  puzzle  --  a  classical  spatial  problem  --  involving  60  participants.  In  order  to  assess  the  framework's  value  as  a  metric,  we  analyze  the  study  with  respect  to  its  reliability,  validity  and  predictive  power.  The  broadly  supportive  results  lead  us  to  conclude  that  the  ATB  framework  enables  the  use  of  observed  epistemic  behaviors  as  a  performance  metric  for  tangible  systems.  We  believe  that  the  development  of  metrics  focused  explicitly  on  the  properties  of  tangible  interaction  are  currently  required  to  gain  insight  into  the  genuine  and  unique  benefits  of  tangible  interaction.  The  ATB  framework  is  a  step  towards  this  goal.
2	When  is  it  not  craft  materiality  and  mediation  when  craft  and  computing  meet.  Craft  has  emerged  as  an  important  reference  point  for  HCI.  To  avoid  a  misrepresenting,  all-encompassing  application  of  craft  to  interaction  design,  this  position  paper  first  discerns  craft  from  HCI.  It  develops  material  engagement  and  mediation  as  differentiating  factors  to  reposition  craft  in  relation  to  tangible  interaction  design.  The  aim  is  to  clarify  craft's  relation  to  interaction  design  and  to  open  up  new  opportunities  and  questions  that  follow  from  this  repositioning.
2	Optimizing  pressure  matrices  interdigitation  and  interpolation  methods  for  continuous  position  input.  This  paper  provides  resources  and  design  recommendations  for  optimizing  position  input  for  pressure  sensor  matrices,  a  sensor  design  often  used  in  eTextiles.  Currently  applications  using  pressure  matrices  for  precise  continuous  position  control  are  rare.  One  reason  designers  opt  against  using  these  sensors  for  continuous  position  control  is  that  when  the  finger  transitions  from  one  sensing  electrode  to  the  next,  jerky  motion,  jumps  or  other  non-linear  artifacts  appear.  We  demonstrate  that  interdigitation  can  improve  transition  behavior  and  discuss  interpolation  algorithms  to  best  leverage  such  designs.  We  provide  software  for  reproducing  our  sensors  and  experiment,  as  well  as  a  dataset  consisting  of  1122  swipe  gestures  performed  on  17  sensors.
2	Gesture  based  distributed  user  interaction  system  for  a  reconfigurable  self  organizing  smart  wall.  We  describe  user  interactions  with  the  self-organized  amorphous  wall,  a  modular,  fully  distributed  system  of  computational  building  blocks  that  communicate  locally  for  creating  smart  surfaces  and  functional  room  dividers.  We  describe  a  menu  and  a  widget-based  approach  in  which  functions  are  color-coded  and  can  be  selected  by  dragging  them  from  module  to  module  on  the  surface  of  the  wall.  We  also  propose  an  on-off  switch  gesture  and  a  dial  gesture  each  spanning  multiple  units  as  canonical  input  mechanisms  that  are  realized  in  a  fully  distributed  way.
2	Sdn  tap  an  sdn  based  traffic  aware  protocol  for  wireless  sensor  networks.  Congestion  control  is  a  challenging  issue  in  wireless  sensor  networks  with  limited  channel  bandwidth.  Thus,  many  protocols  have  been  designed  to  provide  a  distributed  traffic  control  during  packet  forwarding.  However,  all  these  approaches  are  applied  to  single-hop  communication  networks,  ignoring  the  multi-hop  restrictions.  In  this  work,  we  take  advantage  of  software  defined  networking  paradigm  by  devising  a  controller  node  in  such  a  way  that  it  collects  all  the  necessary  information  from  wireless  sensor  network  nodes.  Thus,  based  on  hop  count  and  local  traffic  information,  controller  decides  for  possible  flow  path  changes  to  evenly  distribute  the  traffic.  The  evaluations  revealed  that  the  SDN-TAP  outperforms  conventional  routing  protocols  by  reducing  packet  loss  rate  up  to  46%.
2	A  clinical  decision  and  support  system  with  automatically  ecg  classification  in  telehealthcare.  Telehealthcare  is  a  global  trend  affecting  clinical  practice  in  the  world.  With  the  progress  and  development  of  telecommunication  technologies,  telecom  facilities  have  afforded  telehealthcare  a  new  approach  for  chronic  disease  management.  The  electrocardiogram  (ECG)  is  commonly  used  to  detect  abnormal  heart  rhythms  and  to  investigate  the  cause  of  heart  abnormalities.  To  reduce  the  cardiologists'  loading  and  to  provide  a  continuously  telehealthcare,  we  developed  a  clinical  decision  and  support  system  (CDSS)  with  automatic  recognition  of  the  ECG  in  real-time  analysis.  In  addition,  we  adopted  the  approach  of  noise  reduction  and  feature  extraction  for  support  vector  machine  (SVM)  implementation  with  automatic  learning  algorithms.  The  automatic  interpretation  of  ECG  could  provide  assistance  to  physicians  in  decision-making,  especially  with  large  volumes  of  data.  According  to  the  preliminary  results  of  automatic  classification  models,  we  acquired  88.4%  sensitivity,  for  noise  detection  model,  85.9%  specificity  for  sinus  classification  model  and  89.1%  sensitivity  for  disease  classification  model,  respectively.  However,  it  is  not  reliable  enough  to  obviate  the  need  for  physician's  diagnosis  and  confirmation.  We  should  put  much  effort  on  enhancing  the  performance  of  ECG  interpretation  in  the  future.
2	Rat  cortical  layers  classification  extracting  evoked  local  field  potential  images  with  implanted  multi  electrode  sensor.  One  of  the  most  ambitious  goals  of  neuroscience  and  its  neuroprosthetic  applications  is  to  interface  intelligent  electronic  devices  with  the  biological  brain  to  cure  neurological  diseases.  This  emerging  research  field  builds  on  our  growing  understanding  of  brain  circuits  and  on  recent  technological  advances  in  miniaturization  of  implantable  multi-electrode-arrays  (MEAs)  to  record  brain  signals  at  high  spatiotemporal  resolution.  Data  processing  is  needed  to  extract  useful  information  from  the  recorded  neural  activity  to  better  understand  the  function  of  underlying  neural  circuits  and,  in  perspective,  to  operate  neuroprosthetic  devices.  In  this  context,  machine  learning  approaches  are  increasingly  used  in  many  application  scenarios.  This  paper  focuses  on  processing  data  of  evoked  local  field  potentials  (LFPs)  recorded  from  the  rat  barrel  cortex  using  a  miniaturized  16×16  MEA.  We  evaluated  machine  learning  algorithms  and  trained  an  optimized  classifier  to  detect  at  which  cortical  depth  the  neural  activity  is  measured.  We  demonstrate  with  experimental  results  that  machine  learning  can  be  applied  successfully  to  noisy  single-trial  LFPs  offering  up  to  99.11%  of  test  accuracy  in  classifying  signals  acquired  from  different  cortical  layers.  As  such,  the  method  is  a  very  promising  starting  point  toward  real-time  decoding  of  cerebral  activities  with  low  power  consumption  digital  processors  for  brain-machine  interfacing  and  neuroprosthetic  applications.
2	Real  time  physiological  stream  processing  for  health  monitoring  services.  We  introduce  an  algorithmic  framework  that  uses  nonparametric  Bayesian  models  to  process  real-time  physiological  data  in  the  context  of  developing  and  testing  personalized  wellness  monitors  and  tailored  intervention  strategies.  A  wearable  device  aggregates  signals  from  various  sensors  while  periodically  transmitting  the  collected  data  to  a  backend  server.  The  server  performs  the  computationally  challenging  model  inference  tasks  offline  and  builds  custom  user  profiles  based  on  inferred  hidden  Markov  states.  We  discuss  how  these  user  profiles  can  be  used  to  detect  possible  physiological  changes  in  a  simple  application  based  on  a  two-week  study  hosted  at  Jaslok  Hospital,  where  relaxation  therapy  is  given  to  eight  healthy  male  subjects  as  intervention  against  stress  from  the  workday.  A  heuristic  is  introduced  to  enable  real-time  state  identification  using  the  modest  processing  capabilities  of  the  wearable  device.
2	Keyword  based  sentiment  mining  using  twitter.  Big  Data  are  the  new  frontier  for  businesses  and  governments  alike.  Dealing  with  big  data  and  extracting  valuable  and  actionable  knowledge  from  it  poses  one  of  the  biggest  challenges  in  computing  and,  simultaneously,  provides  one  of  the  greatest  opportunities  for  business,  government  and  society  alike.  The  content  produced  by  the  social  media  community  and  in  particular  the  micro  blogging  community  reflects  one  of  the  most  opinion-and  knowledge-rich,  real-time  accessible,  expressive  and  diverse  data  sources,  both  in  terms  of  content  itself  as  well  as  context  related  knowledge  such  as  user  profiles  including  user  relations.  Harnessing  the  embedded  knowledge  and  in  particular  the  underlying  opinion  about  certain  topics  and  gaining  a  deeper  understanding  of  the  overall  context  will  provide  new  opportunities  in  the  inclusion  of  user  opinions  and  preferences.  This  paper  discusses  a  keyword-based  classifier  for  short  message  based  sentiment  mining.  It  outlines  a  simple  classification  mechanism  that  has  the  potential  to  be  extended  to  include  additional  sentiment  dimensions.  Eventually,  this  could  provide  a  deeper  understanding  about  user  preferences,  which  in  turn  could  actively  and  in  almost  real  time  influence  further  development  activities  or  marketing  campaigns.
2	A  computational  model  for  texture  analysis  in  images  with  fractional  differential  filter  for  texture  detection.  This  paper  is  dedicated  to  the  modelling  of  textured  images  influenced  by  fractional  derivatives  for  texture  detection.  As  most  of  the  images  contain  textures,  texture  analysis  becomes  the  most  important  for  image  understanding  and  it  is  a  key  solution  for  many  computer  vision  applications.  Hence,  texture  must  be  suitably  detected  and  represented.  Nevertheless,  existing  texture  detection  algorithms  consider  texture  as  a  unique  feature  from  edges.  The  proposed  model  explores  a  novel  way  of  developing  texture  detection  algorithm  by  mimicking  edge  detection  algorithms.  The  method  assumes  that  texture  feature  is  analogous  to  edges  and  thus,  the  time  complexity  is  reduced  significantly.  The  model  proposed  in  this  work  is  based  on  Gaussian  kernel  smoothing,  Fractional  partial  derivatives  and  a  statistical  approach.  It  is  justified  to  be  robust  to  noisy  images  and  possesses  statistical  interpretation.  The  model  is  validated  by  the  classification  experiments  on  different  types  of  textured  images  from  Brodatz  album.  It  achieves  higher  classification  accuracy  than  the  existing  methods.
2	An  event  ontology  model  research  for  environmental  pollution  emergencies.  In  the  modeling  process,  event  ontology  design  pattern  (EODP)  is  a  solution  to  recurring  the  same  error  scheme.  It  can  speed  up  the  construction  of  event  ontology  and  avoid  design  ambiguity.  A  gene...
2	Time  and  space  reasoning  for  ambient  systems.  This  paper  presents  an  algebraic  language,  called  Time-AgLOTOS,  to  describe  time-dependent  behavior  of  intelligent  agent  for  the  design  of  Ambient  Intelligence  systems.  This  specification  model  provides  a  theoretical  foundation  for  performing  planning  under  timing  constraints.  Based  on  a  true-concurrency  semantics,  a  contextual  model,  called  Spatio-Temporal  Planning  System  STPS,  is  developed  to  capture  all  possible  evolutions  of  an  agent  plan  including  context  changes.  The  STPS  provides  formal  description  of  possible  actions  to  perform  supporting  timing  constraints,  action  duration  and  spatial  information.  This  structure  offers  new  possibilities  and  strategies  for  taking  agent  real-time  decisions  in  context-awareness  manner.
2	How  to  manage  persons  taken  malaise  at  the  steering  wheel  using  haaas  in  a  vehicular  cloud  computing  environment.  These  last  years,  an  increasingly  significant  number  of  traffic  accidents  caused  by  a  malaise  at  the  steering  wheel  was  observed.  The  management  of  this  kind  of  phenomenon  is  difficult  and  presents  a  real  challenge.  Given  this  fact,  the  authors  try,  in  this  paper,  to  propose  a  work  which,  thanks  to  new  technologies,  let's  take  care  of  people  taken  malaise  while  driving.  Thus,  they  will  provide  a  new  service  called  HAaaS  "Health-Assistance  as  a  Service"  which  will  allow  the  detection  of  malaise  and  the  management  of  the  driver  and  the  vehicle.  Moreover,  the  aspect  of  co-operation  will  be  guaranteed  thanks  to  a  mechanism  that  the  authors  will  use  and  that  they  call  the  Help/rescue  mechanism.
2	Sensitive  self  disclosures  responses  and  social  support  on  instagram  the  case  of  depression.  People  can  benefit  from  disclosing  negative  emotions  or  stigmatized  facets  of  their  identities,  and  psychologists  have  noted  that  imagery  can  be  an  effective  medium  for  expressing  difficult  emotions.  Social  network  sites  like  Instagram  offer  unprecedented  opportunity  for  image-based  sharing.  In  this  paper,  we  investigate  sensitive  self-disclosures  on  Instagram  and  the  responses  they  attract.  We  use  visual  and  textual  qualitative  content  analysis  and  statistical  methods  to  analyze  self-disclosures,  associated  comments,  and  relationships  between  them.  We  find  that  people  use  Instagram  to  engage  in  social  exchange  and  story-telling  about  difficult  experiences.  We  find  considerable  evidence  of  social  support,  a  sense  of  community,  and  little  aggression  or  support  for  harmful  or  pro-disease  behaviors.  Finally,  we  report  on  factors  that  influence  engagement  and  the  type  of  comments  these  disclosures  attract.  Personal  narratives,  food  and  beverage,  references  to  illness,  and  self-appearance  concerns  are  more  likely  to  attract  positive  social  support.  Posts  seeking  support  attract  significantly  more  comments.  CAUTION:  This  paper  includes  some  detailed  examples  of  content  about  eating  disorders  and  self-injury  illnesses.
2	Question  temporality  identification  and  uses.  In  this  paper,  we  introduce  the  concept  of  question  temporality  as  a  measure  of  the  usefulness  of  the  answers  provided  on  the  questions  asked  in  the  Question  Answering  sites  (QA).  We  define  question  temporality  based  on  when  the  answers  provided  on  the  questions  would  expire.  We  use  classification  methods  to  show  that  the  question  temporality  can  be  assessed  automatically.  Our  regression  analysis  highlights  features  that  predict  temporality  of  the  questions.  Our  research  can  be  instructive  for  interface  designers  to  design  temporality-aware  interfaces  and  influence  selection  of  questions  and  answers  for  display.
2	Structuring  aggregating  and  evaluating  crowdsourced  design  critique.  Feedback  is  an  important  component  of  the  design  process,  but  gaining  access  to  high-quality  critique  outside  a  classroom  or  firm  is  challenging.  We  present  CrowdCrit,  a  web-based  system  that  allows  designers  to  receive  design  critiques  from  non-expert  crowd  workers.  We  evaluated  CrowdCrit  in  three  studies  focusing  on  the  designer's  experience  and  benefits  of  the  critiques.  In  the  first  study,  we  compared  crowd  and  expert  critiques  and  found  evidence  that  aggregated  crowd  critique  approaches  expert  critique.  In  a  second  study,  we  found  that  designers  who  got  crowd  feedback  perceived  that  it  improved  their  design  process.  The  third  study  showed  that  designers  were  enthusiastic  about  crowd  critiques  and  used  them  to  change  their  designs.  We  conclude  with  implications  for  the  design  of  crowd  feedback  services.
2	Cats  collection  and  analysis  of  tweets  made  simple.  Twitter  presents  an  unparalleled  opportunity  for  researchers  from  various  fields  to  gather  valuable  and  genuine  textual  data  from  millions  of  people.  However,  the  collection  pro-cess,  as  well  as  the  analysis  of  these  data  require  different  kinds  of  skills  (e.g.  programing,  data  mining)  which  can  be  an  obstacle  for  people  who  do  not  have  this  background.  In  this  paper  we  present  CATS,  an  open  source,  scalable,  Web  application  designed  to  support  researchers  who  want  to  carry  out  studies  based  on  tweets.  The  purpose  of  CATS  is  twofold:  (i)  allow  people  to  collect  tweets  (ii)  enable  them  to  analyze  these  tweets  thanks  to  efficient  tools  (e.g.  event  detection,  named-entity  recognition,  topic  modeling,  word-clouds).  What  is  more,  CATS  relies  on  a  distributed  imple-mentation  which  can  deal  with  massive  data  streams.
2	Measuring  urban  deprivation  from  user  generated  content.  Measuring  socioeconomic  deprivation  of  cities  in  an  accurate  and  timely  fashion  has  become  a  priority  for  governments  around  the  world,  as  the  massive  urbanization  process  we  are  witnessing  is  causing  high  levels  of  inequalities  which  require  intervention.  Traditionally,  deprivation  indexes  have  been  derived  from  census  data,  which  is  however  very  expensive  to  obtain,  and  thus  acquired  only  every  few  years.  Alternative  computational  methods  have  been  proposed  in  recent  years  to  automatically  extract  proxies  of  deprivation  at  a  fine  spatio-temporal  level  of  granularity;  however,  they  usually  require  access  to  datasets  (e.g.,  call  details  records)  that  are  not  publicly  available  to  governments  and  agencies.  To  remedy  this,  we  propose  a  new  method  to  automatically  mine  deprivation  at  a  fine  level  of  spatio-temporal  granularity  that  only  requires  access  to  freely  available  user-generated  content.  More  precisely,  the  method  needs  access  to  datasets  describing  what  urban  elements  are  present  in  the  physical  environment;  examples  of  such  datasets  are  Foursquare  and  OpenStreetMap.  Using  these  datasets,  we  quantitatively  describe  neighborhoods  by  means  of  a  metric,  called  Offering  Advantage,  that  reflects  which  urban  elements  are  distinctive  features  of  each  neighborhood.  We  then  use  that  metric  to  (i)  build  accurate  classifiers  of  urban  deprivation  and  (ii)  interpret  the  outcomes  through  thematic  analysis.  We  apply  the  method  to  three  UK  urban  areas  of  different  scale  and  elaborate  on  the  results  in  terms  of  precision  and  recall.
2	Moral  and  affective  differences  in  u  s  immigration  policy  debate  on  twitter.  Understanding  ideological  conflict  has  been  a  topic  of  interest  in  CSCW,  for  example  in  Value  Sensitive  Design  research.  More  specifically,  understanding  ideological  conflict  is  important  for  studying  social  media  platforms  like  Twitter,  which  provide  the  ability  for  people  to  freely  express  their  thoughts  and  opinions  on  contentious  political  events.  In  this  work,  we  examine  Twitter  data  to  understand  the  moral,  affective,  and  cognitive  differences  in  language  use  between  two  opposing  sides  of  the  political  debate  over  immigration  related  issues  in  the  United  States  in  the  year  since  the  2016  presidential  election.  In  total,  we  analyzed  and  compared  the  language  of  45,045  pro-immigration  tweets  and  11,213  anti-immigration  tweets  spread  across  this  period.  Based  on  Moral  Foundations  Theory  used  to  understand  ideological  conflict,  we  found  pro-immigration  tweets  to  contain  more  language  associated  with  moral  foundations  of  harm,  fairness,  and  loyalty.  Anti-immigration  tweets  contained  more  language  associated  with  moral  foundations  of  authority,  more  words  associated  with  cognitive  rigidity  and  more  3rd  person  pronouns  and  negative  emotion.  We  discuss  the  implications  of  our  research  for  political  communication  over  social  media,  and  for  incorporating  Moral  Foundations  Theory  into  other  CSCW  research.  We  discuss  the  potential  application  of  this  theory  for  Value  Sensitive  Design  research.
2	Freaky  collaborative  enactments  of  emotion.  The  field  of  CSCW  is  increasingly  drawing  on  theories  and  approaches  from  feminist  philosophy  of  science.  To  date  such  efforts  have  focused  on  understanding  users  and  their  practices.  We  present  a  research  prototype  showing  that  feminist  theories  can  lead  to  novel  design  solutions.  Freaky  is  a  mobile,  interactive  system  that  collaborates  with  its  users  in  the  enactment  of  emotion.  Informed  by  the  feminist  literature,  the  system  introduces  a  novel  approach  to  emotion:  designing  for  human-machine  co-production  of  emotion.
2	Collaboration  across  professional  boundaries  the  emergence  of  interpretation  drift  and  the  collective  creation  of  project  jargon.  Collaboration  among  people  with  different  knowledge  backgrounds  can  raise  complex  issues.  The  risk  of  communication  breakdowns  increases  due  to  the  different  worldviews  and  languages  each  professional  brings  to  such  a  collective  project.  To  solve  this  intercultural  collaboration  issue,  previous  research  suggested  a  numbers  of  support  systems  that  seek  to  diminish  differences,  understand  others'  social  worlds  and  achieve  mutual  understanding.  However,  interestingly,  the  cases  introduced  in  this  paper  indicate  that  a  key  in  such  collaboration  support  resides  in  the  collective  creation  process  rather  than  the  minimizing  of  differences  or  understanding  of  one  another's  social  world.  Professionals  in  design  often  maintain  different  social  worlds  and  their  understanding  of  others  within  them,  even  though  they  collaborate  for  a  collective  concern.  In  this  paper,  by  introducing  and  analyzing  two  empirical  collaborative  design  cases,  we  address  the  interpretation  drift  of  common  terms  as  an  unavoidable  integral  aspect  of  the  design  process,  and  the  ongoing  collective  creation  of  local  languages  or  project  jargon  during  the  collaboration  period  as  facilitators  to  enable  collaborators  to  work  beyond  interpretation  drift  and  breakdowns.  By  referring  to  our  cases,  we  show  that  communicative  difficulties  diminish  but  do  not  entirely  disappear,  and  continuous  drift  in  interpretations  and  breakdowns  are  observed.  At  the  same  time,  in  such  a  process,  collectively  created  and  maintained  project  jargon  contributes  largely  to  facilitate  professionals  with  different  knowledge  backgrounds.
2	Using  design  patterns  in  collaborative  interaction  design  processes.  This  work  aims  at  investigating  and  measuring  the  impact  a  collection  of  design  patterns  would  have  on  collaborative  interaction  design  processes.  An  initial  case  study  was  conducted  involving  18  teams  of  students  in  Computer  Science.  Making  use  of  a  collection  of  design  patterns  for  the  design  of  synchronous  applications,  they  were  asked  to  design  the  GUI  and  the  interaction  process  of  applications  which  support  synchronous  collaboration  in  activities  such  as  drawing,  text  editing,  game  solving,  and  searching.  The  results  obtained  through  the  case  study  focused  on  understanding:  a)  whether  the  format  and  the  content  of  a  collection  of  design  patterns  are  easy  to  understand  for  novice  designers,  b)  the  strategies  novice  designers  develop  in  working  with  a  collection  of  design  patterns,  and  c)  the  overall  impact  of  using  design  patterns  in  collaborative  design  processes.
2	Tumblr  fandoms  community  culture.  A  growing  trend  is  the  participation  in  online  fandom  communities  through  the  support  of  the  blogging  platform  Tumblr.  We  investigated  Tumblr  fandom  users'  motivations  behind  participating  in  fandoms,  and  how  they  interacted  within  the  Tumblr  community.  Our  results  show  that  fandom  users  feel  their  Tumblr  experience  is  "always-on"  where  they  participate  at  nearly  any  point  in  the  day.  They  have  also  adopted  a  unique  set  of  jargon  and  use  of  animated  GIFs  to  match  their  desired  fandom  activities.  Overall,  our  results  show  that  Tumblr  fandom  users  present  a  unique  culture,  much  different  from  other  social  networking  sites.
2	Information  needs  for  integration  decisions  in  the  release  process  of  large  scale  parallel  development.  Version  control  branching  allows  an  organization  to  parallelize  its  development  efforts.  Releasing  a  software  system  developed  in  this  manner  requires  release  managers,  and  other  project  stakeholders,  to  make  decisions  about  how  to  integrate  the  branched  work.  This  group  decision-making  process  becomes  very  complex  in  the  case  of  large-scale  parallel  development.  To  better  understand  the  information  needs  of  release  managers  in  this  context,  we  conducted  an  interview  study  at  a  large  software  company.  Our  analysis  of  the  interviews  provides  a  view  into  how  release  managers  make  integration  decisions,  organized  around  ten  key  factors.  Based  on  these  factors,  we  discuss  specific  information  needs  for  release  managers  and  how  the  needs  can  be  met  in  future  work.
2	Public  curation  of  a  historic  collection  a  means  for  speaking  safely  in  public.  We  showcase  the  Voices  from  the  Rwanda  Tribunal  project  and  associated  website  that  provides  online  public  access  to  a  set  of  historic  video  interviews  with  personnel  from  the  International  Criminal  Tribunal  for  Rwanda.  The  demonstration  emphasizes  the  careful  design  process  needed  for  a  project  of  this  sensitivity,  including  two  technical  features  that  democratize  curation  of  the  collection  -  (1)  public  suggestions  for  video  highlights  and  (2)  public  contribution  of  keywords  in  Kinyarwanda,  English  or  French.
2	How  did  you  feel  during  our  conversation  retrospective  analysis  of  intercultural  and  same  culture  instant  messaging  conversations.  Research  has  shown  that  intercultural  communication  can  be  more  problematic  than  same-culture  communication.  In  this  study  we  use  a  technique  called  retrospective  analysis  in  order  to  examine  these  problems  in  greater  detail.  American  and  Chinese  participants  discussed  a  crime  story  with  either  an  American  or  a  Chinese  partner  using  Instant  Messaging  (IM).  After  the  session,  each  participant  reviewed  the  IM  conversation  in  two-minute  segments  and  rated  it  on  several  dimensions.  Chinese  participants  reported  significantly  more  problems  than  American  participants,  and  partner  culture  affected  all  participants'  feelings  of  annoyance.  An  analysis  of  the  communication  problems  participants  reported  showed  four  themes:  mismatched  communication  styles,  differences  in  conversational  focus,  relationship-building  issues,  and  problems  with  the  IM  medium.  The  results  show  how  differences  in  communication  styles  can  affect  intercultural  conversations  and  provide  design  suggestions  for  new  tools  to  improve  intercultural  collaboration.
2	Does  collectivism  inhibit  individual  creativity  the  effects  of  collectivism  and  perceived  diversity  on  individual  creativity  and  satisfaction  in  virtual  ideation  teams.  One  particular  problem  CSCW  and  HCI  scholars  have  sought  to  address  through  the  design  of  collaborative  systems  is  the  issues  associated  with  diversity  and  creativity.  Diversity  can  promote  creativity  by  exposing  individuals  to  different  perspectives  and  at  the  same  time  make  it  difficult  for  teams  to  leverage  their  differences  to  be  more  creative.  This  paper  asserts  that  through  the  promotion  of  cooperation,  collectivism  will  help  ideation  team  members  overcome  the  challenges  associated  with  diversity  and  promote  creativity.  To  examine  this  assertion,  we  conducted  an  experimental  study  involving  107  individuals  in  33  idea-generation  teams.  Collectivism  was  promoted  through  priming.  The  results  confirm  our  assertion:  collectivism  created  conditions  that  facilitated  creativity  when  teams  were  high  in  perceived  diversity.  Collectivism  also  facilitated  more  satisfaction  among  teammates  by  offsetting  negative  perceptions  of  diversity.  These  results  offer  new  insights  on  collectivism,  perceived  diversity  and  creativity.
2	Remixers  understandings  of  fair  use  online.  How  do  online  content  creators  make  decisions  about  copyright  law?  In  the  course  of  day-to-day  online  activities,  Internet  users  are  forced  to  make  subtle  judgments  about  one  of  the  most  confusing  and  nuanced  areas  of  law,  copyright  and  fair  use.  In  this  study,  we  conducted  semi-structured  interviews  with  eleven  content  creators  who  participate  in  remix  and  fan  creation  activities  online,  to  try  to  probe  their  legal  understandings  and  attitudes.  We  found  that  social  norms  that  emerge  among  these  content  creators  do  not  always  track  to  what  the  law  actually  says,  but  are  often  guided  more  by  ethical  concerns.  Our  participants  showed  surprisingly  similar  patterns  of  understandings  and  confusions,  impacting  technology  use  and  interaction  online.
2	Environmental  jolts  impact  of  exogenous  factors  on  online  community  participation.  Few  studies  of  online  communities  take  exogenous  factors  into  account  while  explaining  community  participation.  We  present  preliminary  results  from  a  study  investigating  the  impact  of  steward  companies'  actions  on  online  community  participation.  We  identified  two  events:  (1)  open  sourcing  of  Java  by  Sun  and  (2)  acquisition  of  Sun  (and  consequently  of  Java)  by  Oracle,  and  examined  participation  in  their  developer  online  communities.  We  found  significant  change  in  participation  levels  around  each  event  with  both  significant  increases  and  decreases.  We  conjecture  that  participation  increased  if  the  action  was  perceived  as  supportive  by  developers  (e.g.  Sun's  open  sourcing  of  Java)  whereas  it  decreased  if  the  action  was  perceived  as  detrimental  by  developers  (e.g.  Oracle's  acquisition  of  Sun).
2	From  the  matrix  to  a  model  of  coordinated  action  moca  a  conceptual  framework  of  and  for  cscw.  The  CSCW  community  is  reliant  upon  technology-centric  models  of  groupware  and  collaboration  that  frame  how  we  examine  and  design  for  cooperative  work.  This  paper  both  reviews  the  CSCW  literature  to  examine  existing  models  of  collaborative  work  and  proposes  a  new,  expanded  conceptual  model:  the  Model  of  Coordinated  Action  (MoCA).  MoCA  is  a  broader  framework  for  describing  complex  collaborative  situations  and  environments  including,  but  not  limited  to,  collaborations  that  have  diverse,  high-turnover  memberships  or  emerging  practices.  We  introduce  MoCA's  seven  dimensions  of  coordinative  action  and  illustrate  their  connection  to  past  and  current  CSCW  research.  Finally,  we  discuss  some  ramifications  of  MoCA  for  our  understanding  of  CSCW  as  a  sociotechnical  design  space.
2	Analyzing  the  flow  of  knowledge  in  computer  mediated  teams.  In  this  article,  we  present  an  analysis  of  communication  transcripts  from  computer-mediated  teams  that  illustrates  how  different  kinds  of  decision  support  impact  collaborative  knowledge  construction.  Our  analysis  introduces  an  algorithmic  technique  called  Topic  Evolution  Analysis  (TEvA),  which  tracks  clusters  of  words  in  conversation,  and  illustrates  how  these  clusters  change  and  merge  over  time.  This  analysis  is  combined  with  measurements  of  group  dynamics  to  distinguish  between  teams  using  different  kinds  of  decision  support.      Our  analysis  offers  evidence  that  some  kinds  of  decision  support  improve  the  apparent  rationality  of  a  team,  but  at  the  cost  of  collaborative  knowledge  construction.  This  result  is  not  apparent  when  simply  measuring  team  decision  performance.  We  use  this  finding  to  motivate  the  utility  and  importance  of  the  approach  when  assessing  the  impact  of  technology  on  collaborative  knowledge  processing.
2	Exploring  the  ownership  and  persistent  value  of  facebook  content.  In  this  paper,  we  present  the  results  of  a  study  examining  244  participants'  attitudes  about  the  value,  ownership,  and  control  of  social  network  data.  We  use  Facebook-based  scenarios  to  elicit  reactions  to  hypothetical  statements  about  saving  social  network  content  that  belongs  to  others,  reusing,  repurposing,  and  monetizing  social  network  data,  and  removing  social  network  content  that  is  not  specifically  one's  own.  Participants  also  report  on  their  own  practices  in  each  of  these  areas.  Findings  not  only  address  issues  related  to  ownership,  but  also  explore  the  use  of  social  networks  as  documentary  records,  and  the  discrepancies  between  participants'  perceptions  of  how  they  would  like  their  social  network  content  to  be  used,  and  how  it  is  actually  used.
2	Taking  notes  or  playing  games  understanding  multitasking  in  video  communication.  This  paper  presents  a  detailed  examination  of  factors  that  affect  perceptions  of  and  attitudes  towards  multitasking  in  video  conferencing.  We  first  report  findings  from  interviews  with  15  professional  users  of  videoconferencing.  Our  interviews  revealed  the  roles  and  potential  link  of  technology  and  activity.  We  then  report  results  from  a  controlled  online  experiment  with  397  participants  based  in  the  United  States.  Our  results  show  that  the  technology  used  for  multitasking  has  a  significant  effect  on  others'  assumptions  of  what  secondary  activity  the  multitasker  is  likely  engaged  in,  and  that  this  assumed  activity  in  turn  affects  evaluations  of  politeness  and  appropriateness.  We  also  show  that  different  layouts  of  the  video  conferencing  UI  can  affect  perception  of  engagement  in  the  meeting  and  in  turn  ratings  of  polite  or  impolite  behavior.  We  propose  a  conceptual  model  that  captures  our  results  and  use  the  model  to  discuss  implications  for  behavior  and  for  the  design  of  video  communication  tools.
2	Community  based  data  validation  practices  in  citizen  science.  Technology-supported  citizen  science  has  created  huge  volumes  of  data  with  increasing  potential  to  facilitate  scientific  progress,  however,  verifying  data  quality  is  still  a  substantial  hurdle  due  to  the  limitations  of  existing  data  quality  mechanisms.  In  this  study,  we  adopted  a  mixed  methods  approach  to  investigate  community-based  data  validation  practices  and  the  characteristics  of  records  of  wildlife  species  observations  that  affected  the  outcomes  of  collaborative  data  quality  management  in  an  online  community  where  people  record  what  they  see  in  the  nature.  The  findings  describe  the  processes  that  both  relied  upon  and  added  to  information  provenance  through  information  stewardship  behaviors,  which  led  to  improved  reliability  and  informativity.  The  likelihood  of  community-based  validation  interactions  were  predicted  by  several  factors,  including  the  types  of  organisms  observed  and  whether  the  data  were  submitted  from  a  mobile  device.  We  conclude  with  implications  for  technology  design,  citizen  science  practices,  and  research.
2	Let  s  talk  about  the  quantified  workplace.  Over  the  past  decades  the  advances  in  pervasive  technology  have  enabled  new  ways  of  understanding  human  behaviour  in  the  workplace.  This  trend  merged  with  the  new  rise  in  the  Quantified-Self  movement  has  engendered  a  new  paradigm  of  Quantified  Workplace,  where  sensing  solutions  as  well  as  participatory  inputs  could  be  used  to  model,  quantify  and  visualise  dynamics  of  the  workplace.  In  this  workshop  we  want  to  start  a  new  dialogue  to  discuss  challenges,  insights  and  reflections  on  this  topic.  To  this  aim  we  are  looking  for  original  submissions  which  offer  new  insights  or  propose  new  techniques  for  Quantified  Workplace.  We  welcome  technical  research  papers,  qualitative  research  studies,  case  studies  as  well  as  work-in-progress  papers  which  could  trigger  discussions  around  the  workshop  topics.
2	Exploring  the  ecosystem  of  software  developers  on  github  and  other  platforms.  GitHub  provides  various  social  features  for  developers  to  collaborate  with  others.  Those  features  are  important  for  developers  to  coordinate  their  work  (Dabbish  et  al.,  2012;  Marlow  et  al.,  2013).  We  hypothesized  that  the  social  system  of  GitHub  users  was  bound  by  system  interactions  such  that  contributing  to  similar  code  repositories  would  lead  to  users  following  one  another  on  GitHub  or  vice  versa.  Using  a  quadratic  assignment  procedure  (QAP)  correlation,  however,  only  a  weak  correlation  among  followship  and  production  activities  (code,  issue,  and  wiki  contributions)  was  found.  Survey  with  GitHub  users  revealed  an  ecosystem  on  the  Internet  for  software  developers,  which  includes  many  platforms,  such  as  Forrst,  Twitter,  and  Hacker  News,  among  others.  Developers  make  social  introductions  and  other  interactions  on  these  platforms  and  engage  with  one  anther  on  GitHub.  Due  to  these  preliminary  findings,  we  describe  GitHub  as  a  part  of  a  larger  ecosystem  of  developer  interactions.
2	The  impacts  of  platform  quality  on  gig  workers  autonomy  and  job  satisfaction.  Gig  economy  jobs  rely  heavily  on  the  use  of  platforms  including  mobile  applications.  Even  though  such  platforms  are  necessary  to  participate  in  the  gig  economy,  we  know  very  little  about  how  the  quality  of  these  platforms  affects  gig  workers.  Drawing  from  a  survey  of  Uber  drivers,  in  this  paper  we  examine  the  impacts  of  platform  quality  on  gig  workers'  job  autonomy  and  job  satisfaction.  Preliminary  results  suggest  that  gig  workers  working  in  the  high  quality  of  platforms  are  more  likely  to  have  greater  job  autonomy  and  satisfaction.  This  study  contributes  to  the  literature  by  identifying  platform  quality  as  an  important  factor  of  gig  workers'  job  autonomy  and  satisfaction  and  suggesting  possible  applications  of  the  preliminary  findings  in  future  research.
2	Integrating  optical  waveguides  for  display  and  sensing  on  pneumatic  soft  shape  changing  interfaces.  We  introduce  the  design  and  fabrication  process  of  integrating  optical  fiber  into  pneumatically  driven  soft  composite  shape  changing  interfaces.  Embedded  optical  waveguides  can  provide  both  sensing  and  illumination,  and  add  one  more  building  block  to  the  design  of  designing  soft  pneumatic  shape  changing  interfaces.
2	Geltouch  localized  tactile  feedback  through  thin  programmable  gel.  We  present  GelTouch,  a  gel-based  layer  that  can  selectively  transition  between  soft  and  stiff  to  provide  tactile  multi-touch  feedback.  It  is  flexible,  transparent  when  not  activated,  and  contains  no  mechanical,  electromagnetic,  or  hydraulic  components,  resulting  in  a  compact  form  factor  (a  2mm  thin  touchscreen  layer  for  our  prototype).  The  activated  areas  can  be  morphed  freely  and  continuously,  without  being  limited  to  fixed,  predefined  shapes.  GelTouch  consists  of  a  poly(N-isopropylacrylamide)  gel  layer  which  alters  its  viscoelasticity  when  activated  by  applying  heat  (>32  C).  We  present  three  different  activation  techniques:  1)  Indium  Tin  Oxide  (ITO)  as  a  heating  element  that  enables  tactile  feedback  through  individually  addressable  taxels;  2)  predefined  tactile  areas  of  engraved  ITO,  that  can  be  layered  and  combined;  3)  complex  arrangements  of  resistance  wire  that  create  thin  tactile  edges.  We  present  a  tablet  with  6x4  tactile  areas,  enabling  a  tactile  numpad,  slider,  and  thumbstick.  We  show  that  the  gel  is  up  to  25  times  stiffer  when  activated  and  that  users  detect  tactile  features  reliably  (94.8%).
2	Hachistick  simulating  haptic  sensation  on  tablet  pc  for  musical  instruments  application.  In  this  paper,  we  propose  a  novel  stick-type  interface,  the  "HaCHIStick,"  for  musical  performance  on  a  tablet  PC.  The  HaCHIStick  is  composed  of  a  stick  with  an  embedded  vibrotactile  actuator,  a  visual  display,  and  an  elastic  sheet  on  the  display.  By  combining  the  kinesthetic  sensation  induced  by  striking  the  elastic  sheet  with  vibrotactile  sensation,  the  system  provides  natural  haptic  cues  that  enable  the  user  to  feel  what  they  strike  with  the  stick,  such  as  steel  or  wood.  This  haptic  interaction  would  enrich  the  user's  experience  when  playing  the  instruments.  The  interface  is  regarded  as  a  type  of  haptic  augmented  reality  (AR)  system,  with  a  relatively  simple  setup.
2	Tmotion  embedded  3d  mobile  input  using  magnetic  sensing  technique.  We  present  TMotion,  a  self-contained  3D  input  that  enables  spatial  interactions  around  mobile  using  a  magnetic  sensing  technique.  Using  a  single  magnetometer  from  the  mobile  device,  we  can  track  the  3D  position  of  the  permanent  magnet  embedded  in  the  prototype  along  with  an  inertial  measurement  unit.  By  numerically  solving  non-linear  magnetic  field  equations  with  known  orientation  from  inertial  measurement  unit  (IMU),  we  attain  a  tracking  rate  greater  than  30Hz  based  solely  on  the  mobile  device  computation.  We  describe  the  working  principle  of  TMotion  and  example  applications  illustrating  its  capability.
2	Post  literate  programming  linking  discussion  and  code  in  software  development  teams.  The  literate  programming  paradigm  presents  a  program  interleaved  with  natural  language  text  explaining  the  code's  rationale  and  logic.  While  this  is  great  for  program  readers,  the  labor  of  creating  literate  programs  deters  most  program  authors  from  providing  this  text  at  authoring  time.  Instead,  as  we  determine  through  interviews,  developers  provide  their  design  rationales  after  the  fact,  in  discussions  with  collaborators.  We  propose  to  capture  these  discussions  and  incorporate  them  into  the  code.  We  have  prototyped  a  tool  to  link  online  discussion  of  code  directly  to  the  code  it  discusses.  Incorporating  these  discussions  incrementally  creates  post-literate  programs  that  convey  information  to  future  developers.
2	Tip  tap  battery  free  discrete  2d  fingertip  input.  We  describe  Tip-Tap,  a  wearable  input  technique  that  can  be  implemented  without  batteries  using  a  custom  RFID  tag.  It  recognizes  2-dimensional  discrete  touch  events  by  sensing  the  intersection  between  two  arrays  of  contact  points:  one  array  along  the  index  fingertip  and  the  other  along  the  thumb  tip.  A  formative  study  identifies  locations  on  the  index  finger  that  are  reachable  by  different  parts  of  the  thumb  tip,  and  the  results  determine  the  pattern  of  contacts  points  used  for  the  technique.  Using  a  reconfigurable  3x3  evaluation  device,  a  second  study  shows  eyes-free  accuracy  is  86%  after  a  very  short  period,  and  adding  bumpy  or  magnetic  passive  haptic  feedback  to  contacts  is  not  necessary.  Finally,  two  battery-free  prototypes  using  a  new  RFID  tag  design  demonstrates  how  Tip-Tap  can  be  implemented  in  a  glove  or  tattoo  form  factor.
2	Haptic  pivot  on  demand  handhelds  in  vr.  We  present  PIVOT,  a  wrist-worn  haptic  device  that  renders  virtual  objects  into  the  user's  hand  on  demand.  Its  simple  design  comprises  a  single  actuated  joint  that  pivots  a  haptic  handle  into  and  out  of  the  user's  hand,  rendering  the  haptic  sensations  of  grasping,  catching,  or  throwing  an  object  anywhere  in  space.  Unlike  existing  hand-held  haptic  devices  and  haptic  gloves,  PIVOT  leaves  the  user's  palm  free  when  not  in  use,  allowing  users  to  make  unencumbered  use  of  their  hand.  PIVOT  also  enables  rendering  forces  acting  on  the  held  virtual  objects,  such  as  gravity,  inertia,  or  air-drag,  by  actively  driving  its  motor  while  the  user  is  firmly  holding  the  handle.  When  wearing  a  PIVOT  device  on  both  hands,  they  can  add  haptic  feedback  to  bimanual  interaction,  such  as  lifting  larger  objects.  In  our  user  study,  participants  (n=12)  evaluated  the  realism  of  grabbing  and  releasing  objects  of  different  shape  and  size  with  mean  score  5.19  on  a  scale  from  1  to  7,  rated  the  ability  to  catch  and  throw  balls  in  different  directions  with  different  velocities  (mean=5.5),  and  verified  the  ability  to  render  the  comparative  weight  of  held  objects  with  87%  accuracy  for  ~100g  increments.
2	Attaching  objects  to  smartphones  back  side  for  a  modular  interface.  This  paper  proposes  a  new  approach  to  enhancing  interaction  with  general  applications  on  smartphones.  Physical  objects  held  on  back  surface  of  the  smartphone,  which  can  be  captured  by  the  rear  camera  with  a  mirror,  work  as  input  devices  or  controllers.  It  does  not  require  any  additional  electronic  devices  but  offers  tactile  feedback.  The  occlusion  problem  does  not  occur  when  using  smartphone's  back  side,  in  terms  of  both  display  and  camera  viewing.  We  implemented  on  an  Android  smartphone  and  confirmed  that  it  provides  richer  interaction  and  low  latency  (100  ms).
2	Fit  your  hand  personalized  user  interface  considering  physical  attributes  of  mobile  device  users.  We  present  a  mobile  user  interface  which  dynamically  reformulates  the  layout  based  on  the  touch  input  pattern  of  users.  By  analyzing  the  touch  input,  it  infers  users'  physical  characteristics  such  as  handedness,  finger  length,  or  usage  habits,  thereby  calculates  the  optimal  touch  area  for  the  user.  The  user  interface  is  gradually  adapted  to  each  user  by  automatically  rearranging  graphic  objects  such  as  application  icons  to  the  most  easy-to-touch  positions.  To  compute  the  optimal  touch  area,  we  designed  software  architecture  and  implemented  an  Android  application  which  analyzes  touch  input  and  determines  the  touch  frequency  in  specific  screen  areas,  the  handedness  and  hand  size  of  users.  As  proof  of  concept,  this  research  prototype  shows  acceptable  performance  and  accuracy.  To  decide  which  items  should  be  placed  in  the  optimal  touch  area,  we  plan  to  integrate  our  machine  learning  algorithm  which  prioritizes  applications  according  to  the  context  of  users  into  the  proposed  system.
2	Sensortape  modular  and  programmable  3d  aware  dense  sensor  network  on  a  tape.  SensorTape  is  a  modular  and  dense  sensor  network  in  a  form  factor  of  a  tape.  SensorTape  is  composed  of  interconnected  and  programmable  sensor  nodes  on  a  flexible  electronics  substrate.  Each  node  can  sense  its  orientation  with  an  inertial  measurement  unit,  allowing  deformation  self-sensing  of  the  whole  tape.  Also,  nodes  sense  proximity  using  time-of-flight  infrared.  We  developed  network  architecture  to  automatically  determine  the  location  of  each  sensor  node,  as  SensorTape  is  cut  and  rejoined.  Also,  we  made  an  intuitive  graphical  interface  to  program  the  tape.  Our  user  study  suggested  that  SensorTape  enables  users  with  different  skill  sets  to  intuitively  create  and  program  large  sensor  network  arrays.  We  developed  diverse  applications  ranging  from  wearables  to  home  sensing,  to  show  low  deployment  effort  required  by  the  user.  We  showed  how  SensorTape  could  be  produced  at  scale  using  current  technologies  and  we  made  a  2.3-meter  long  prototype.
2	Vermeer  direct  interaction  with  a  360  viewable  3d  display.  We  present  Vermeer,  a  novel  interactive  360°  viewable  3D  display.  Like  prior  systems  in  this  area,  Vermeer  provides  viewpoint-corrected,  stereoscopic  3D  graphics  to  simultaneous  users,  360°  around  the  display,  without  the  need  for  eyewear  or  other  user  instrumentation.  Our  goal  is  to  over-come  an  issue  inherent  in  these  prior  systems  which  -  typically  due  to  moving  parts  -  restrict  interactions  to  outside  the  display  volume.  Our  system  leverages  a  known  optical  illusion  to  demonstrate,  for  the  first  time,  how  users  can  reach  into  and  directly  touch  3D  objects  inside  the  display  volume.  Vermeer  is  intended  to  be  a  new  enabling  technology  for  interaction,  and  we  therefore  describe  our  hardware  implementation  in  full,  focusing  on  the  challenges  of  combining  this  optical  configuration  with  an  existing  approach  for  creating  a  360°  viewable  3D  display.  Initially  we  demonstrate  direct  involume  interaction  by  sensing  user  input  with  a  Kinect  camera  placed  above  the  display.  However,  by  exploiting  the  properties  of  the  optical  configuration,  we  also  demonstrate  novel  prototypes  for  fully  integrated  input  sensing  alongside  simultaneous  display.  We  conclude  by  discussing  limitations,  implications  for  interaction,  and  ideas  for  future  work.
2	Reinventing  the  wheel  transforming  steering  wheel  systems  for  autonomous  vehicles.  In  this  paper,  we  introduce  two  different  transforming  steering  wheel  systems  that  can  be  utilized  to  augment  user  experience  for  future  partially  autonomous  and  fully  autonomous  vehicles.  The  first  one  is  a  robotic  steering  wheel  that  can  mechanically  transform  by  using  its  actuators  to  move  the  various  components  into  different  positions.  The  second  system  is  a  LED  steering  wheel  that  can  visually  transform  by  using  LEDs  embedded  along  the  rim  of  wheel  to  change  colors.  Both  steering  wheel  systems  contain  onboard  microcontrollers  developed  to  interface  with  our  driving  simulator.  The  main  function  of  these  two  systems  is  to  provide  emergency  warnings  to  drivers  in  a  variety  of  safety  critical  scenarios,  although  the  design  space  that  we  propose  for  these  steering  wheel  systems  also  includes  the  use  as  interactive  user  interfaces.  To  evaluate  the  effectiveness  of  the  emergency  alerts,  we  conducted  a  driving  simulator  study  examining  the  performance  of  participants  (N=56)  after  an  abrupt  loss  of  autonomous  vehicle  control.  Drivers  who  experienced  the  robotic  steering  wheel  performed  significantly  better  than  those  who  experienced  the  LED  steering  wheel.  The  results  of  this  study  suggest  that  alerts  utilizing  mechanical  movement  are  more  effective  than  purely  visual  warnings.
2	The  affective  experience  of  handling  digital  fabrics  tactile  and  visual  cross  modal  effects.  In  the  textile  sector,  emotions  are  often  associated  with  both  physical  touch  and  manipulation  of  the  product.  Thus  there  is  the  need  to  recreate  the  affective  experiences  of  touching  and  interacting  with  fabrics  using  commonly  available  internet  technology.  New  digital  interactive  representations  of  fabrics  simulating  handling  have  been  proposed  with  the  idea  of  bringing  the  digital  experience  of  fabrics  closer  to  the  reality.  This  study  evaluates  the  contribution  of  handling  real  fabrics  to  viewing  digital  interactive  animations  of  said  fabrics  and  vice  versa.  A  combination  of  self-report  and  physiological  measures  was  used.  Results  showed  that  having  previous  physical  handling  experience  of  the  fabrics  significantly  increased  pleasure  and  engagement  in  the  visual  experience  of  the  digital  handling  of  the  same  fabrics.  Two  factors  mediated  these  experiences:  gender  and  interoceptive  awareness.  Significant  results  were  not  found  for  the  opposite  condition.
2	Herme  yet  another  interactive  conversational  robot.  Herme,  the  talking  robot  developed  at  the  Speech  Communication  Lab  in  Trinity  College  Dublin  collected  almost  500  signed  consent  forms  from  casual  visitors  to  the  TCD  Science  Gallery  over  a  three-month  period  in  2012.  Herme  is  a  small  LEGO  device  supporting  a  web  cam  and  programmed  to  talk  to  any  people  who  come  within  her  range  of  view.  She  films  the  interaction  and  after  chatting  with  her  new  'friends'  for  about  three  minutes,  tries  to  persuade  them  to  sign  an  ethics  committee  consent  form  allowing  the  researchers  in  her  lab  to  analyse  the  audio-video  recordings  of  the  conversation  in  order  to  better  understand  how  people  might  interact  with  machines  in  the  future.  This  demonstration  showcases  work  being  carried  out  at  the  Speech  Communication  Lab  under  SFI  funding  on  the  FASTNET  project,  with  a  Focus  on  Actions  in  Social  Talk,  linking  computer  speech  synthesis  with  different  forms  of  speech  and  image  processing  to  make  inferences  about  the  success  of  an  ongoing  conversation.  The  corpus  of  recordings  of  conversations  collected  at  the  Science  Gallery  provides  essential  insights  into  how  human  social  interaction  follows  clearly  defined  patterns  that  can  be  taken  advantage  of  in  advanced  speech  technology.  The  demonstration  presented  at  ACII  2013  will  allow  participants  to  engage  with  Herme  and  judge  her  competence  for  themselves.
2	Sensorsnaps  integrating  wireless  sensor  nodes  into  fabric  snap  fasteners  for  textile  interfaces.  Adding  electronics  to  textiles  can  be  time-consuming  and  requires  technical  expertise.  We  introduce  SensorSnaps,  low-power  wireless  sensor  nodes  that  seamlessly  integrate  into  caps  of  fabric  snap  fasteners.  SensorSnaps  provide  a  new  technique  to  quickly  and  intuitively  augment  any  location  on  the  clothing  with  sensing  capabilities.  SensorSnaps  securely  attach  and  detach  from  ubiquitous  commercial  snap  fasteners.  Using  inertial  measurement  units,  the  SensorSnaps  detect  tap  and  rotation  gestures,  as  well  as  track  body  motion.  We  optimized  the  power  consumption  for  SensorSnaps  to  work  continuously  for  45  minutes  and  up  to  4  hours  in  capacitive  touch  standby  mode.  We  present  applications  in  which  the  SensorSnaps  are  used  as  gestural  interfaces  for  a  music  player  controller,  cursor  control,  and  motion  tracking  suit.  The  user  study  showed  that  SensorSnap  could  be  attached  in  around  71  seconds,  similar  to  attaching  off-the-shelf  snaps,  and  participants  found  the  gestures  easy  to  learn  and  perform.  SensorSnaps  could  allow  anyone  to  effortlessly  add  sophisticated  sensing  capacities  to  ubiquitous  snap  fasteners.
2	Potential  clinical  impact  of  positive  affect  in  robot  interactions  for  autism  intervention.  While  interactive  technologies  frequently  are  designed  to  be  enjoyable,  there  are  particular  reasons  to  prioritize  this  for  technologies  intended  to  support  autism  interventions.  Most  broadly,  enjoyment  of  activities  or  materials  used  in  interventions  has  been  associated  with  heightened  improvements  in  the  behaviors  targeted  by  the  interventions.  In  the  largest  group  study  to  date  of  school-aged  children  with  high-functioning  autism  (N=24),  we  present  evidence  of  more  positive  affect  elicited  with  a  robot  than  with  an  adult,  during  matched  triadic  interactions  designed  to  facilitate  social  and  conversational  interaction  with  a  clinician.  Robot-mediated  increases  in  positive  affect  were  found  to  be  associated  with  production  of  spoken  language  directed  to  the  clinician  during  robot  interaction.  We  further  found  that  robot-mediated  increases  in  positive  affect  were  associated  with  greater  autism  severity,  particularly  in  the  social  affect  domain,  and  with  lower  nonverbal  IQ.  Our  findings  suggest  that  robots  may  have  a  unique  advantage  in  interventions  for  children  with  autism  spectrum  disorders  by  eliciting  more  positive  affect,  and  that  we  should  explore  robot  support  for  interventions  with  lower-functioning,  affected  individuals.
2	Multi  modal  affect  induction  for  affective  brain  computer  interfaces.  Reliable  applications  of  affective  brain-computer  interfaces  (aBCI)  in  realistic,  multi-modal  environments  require  a  detailed  understanding  of  the  processes  involved  in  emotions.  To  explore  the  modalityspecific  nature  of  affective  responses,  we  studied  neurophysiological  responses  (i.e.,  EEG)  of  24  participants  during  visual,  auditory,  and  audiovisual  affect  stimulation.  The  affect  induction  protocols  were  validated  by  participants'  subjective  ratings  and  physiological  responses  (i.e.,  ECG).  Coherent  with  literature,  we  found  modality-specific  responses  in  the  EEG:  posterior  alpha  power  decreases  during  visual  stimulation  and  increases  during  auditory  stimulation,  anterior  alpha  power  tends  to  decrease  during  auditory  stimulation  and  to  increase  during  visual  stimulation.  We  discuss  the  implications  of  these  results  for  multimodal  aBCI.
2	Development  of  a  binary  fmri  bci  for  alzheimer  patients  a  semantic  conditioning  paradigm  using  affective  unconditioned  stimuli.  With  the  aim  of  developing  a  brain-computer  interface  for  the  communication  of  basic  mental  states,  a  classical  conditioning  paradigm  with  affective  stimuli  was  used,  assessing  the  possibility  to  discriminate  between  affirmative  and  negative  thinking  in  an  fMRI-BCI  setting.  6  Alzheimer  patients  and  7  healthy  control  subjects  participated  to  the  study.  Congruent  and  incongruent  word-pairs  were  respectively  associated  to  pleasant  (baby  laughter)  and  unpleasant  (scream)  affective  stimuli.  A  Support  Vector  Machine  classifier  focusing  on  insula,  amygdala  and  anterior  cingulate  cortex  was  used  to  discriminate  between  the  activations  relative  to  congruent  and  incongruent  word-pairs  (eliciting  respectively  affirmative  and  negative  thinking),  following  the  conditioning  process.  Classification  accuracy  was  on  average  71%  for  Alzheimer  patients,  reaching  85%,  and  on  average  69%  for  control  subjects,  reaching  83%.  This  study  shows  that  it  is  possible  to  extract  information  on  individuals'  mental  states  by  exploiting  affective  responses,  overcoming  the  typical  obstacles  of  traditional  BCIs,  which  generally  require  time-consuming  trainings  and  intact  cognition.
2	Depend  augmented  handwriting  system  using  ferromagnetism  of  a  ballpoint  pen.  This  paper  presents  dePENd,  a  novel  interactive  system  that  assists  in  sketching  using  regular  pens  and  paper.  Our  system  utilizes  the  ferromagnetic  feature  of  the  metal  tip  of  a  regular  ballpoint  pen.  The  computer  controlling  the  X  and  Y  positions  of  the  magnet  under  the  surface  of  the  table  provides  entirely  new  drawing  experiences.  By  controlling  the  movements  of  a  pen  and  presenting  haptic  guides,  the  system  allows  a  user  to  easily  draw  diagrams  and  pictures  consisting  of  lines  and  circles,  which  are  difficult  to  create  by  free-hand  drawing.  Moreover,  the  system  also  allows  users  to  freely  edit  and  arrange  prescribed  pictures.  This  is  expected  to  reduce  the  resistance  to  drawing  and  promote  users'  creativity.  In  addition,  we  propose  a  communication  tool  using  two  dePENd  systems  that  is  expected  to  enhance  the  drawing  skills  of  users.  The  functions  of  this  system  enable  users  to  utilize  interactive  applications  such  as  copying  and  redrawing  drafted  pictures  or  scaling  the  pictures  using  a  digital  pen.  Furthermore,  we  implement  the  system  and  evaluate  its  technical  features.  In  this  paper,  we  describe  the  details  of  the  design  and  implementations  of  the  device,  along  with  applications,  technical  evaluations,  and  future  prospects.
2	An  aligned  rank  transform  procedure  for  multifactor  contrast  tests.  Data  from  multifactor  HCI  experiments  often  violates  the  assumptions  of  parametric  tests  (i.e.,  nonconforming  data).  The  Aligned  Rank  Transform  (ART)  has  become  a  popular  nonparametric  analysis  in  HCI  that  can  find  main  and  interaction  effects  in  nonconforming  data,  but  leads  to  incorrect  results  when  used  to  conduct  post  hoc  contrast  tests.  We  created  a  new  algorithm  called  ART-C  for  conducting  contrast  tests  within  the  ART  paradigm  and  validated  it  on  72,000  synthetic  data  sets.  Our  results  indicate  that  ART-C  does  not  inflate  Type  I  error  rates,  unlike  contrasts  based  on  ART,  and  that  ART-C  has  more  statistical  power  than  a  t-test,  Mann-Whitney  U  test,  Wilcoxon  signed-rank  test,  and  ART.  We  also  extended  an  open-source  tool  called  ARTool  with  our  ART-C  algorithm  for  both  Windows  and  R.  Our  validation  had  some  limitations  (e.g.,  only  six  distribution  types,  no  mixed  factorial  designs,  no  random  slopes),  and  data  drawn  from  Cauchy  distributions  should  not  be  analyzed  with  ART-C.
2	Multiple  instance  learning  for  classification  of  human  behavior  observations.  Analysis  of  audiovisual  human  behavior  observations  is  a  common  practice  in  behavioral  sciences.  It  is  generally  carried  through  by  expert  annotators  who  are  asked  to  evaluate  several  aspects  of  the  observations  along  various  dimensions.  This  can  be  a  tedious  task.  We  propose  that  automatic  classification  of  behavioral  patterns  in  this  context  can  be  viewed  as  a  multiple  instance  learning  problem.  In  this  paper,  we  analyze  a  corpus  of  married  couples  interacting  about  a  problem  in  their  relationship.  We  extract  features  from  both  the  audio  and  the  transcriptions  and  apply  the  Diverse  Density-Support  Vector  Machine  framework.  Apart  from  attaining  classification  on  the  expert  annotations,  this  framework  also  allows  us  to  estimate  salient  regions  of  the  complex  interaction.
2	Automated  mental  stress  recognition  through  mobile  thermal  imaging.  Mental  stress  is  a  critical  problem  in  our  modern  society.  This  form  of  stress  strongly  affects  our  well  being,  and  technology  is  needed  to  help  us  to  manage  health  problems.  The  ability  to  automatically  recognize  a  person's  mental  stress  can  be  fundamental  in  supporting  stress  and  health  management.  This  research  focuses  on  the  use  of  mobile  thermal  imaging,  a  new  and  less  explored  sensor,  to  merge  the  measurement  of  multiple  physiological  signatures  into  one  sensor  and  to  build  a  reliable  mental  stress  automatic  recognition  model.  Mobile  thermal  imaging  has  greater  potentials  for  real-world  applications  given  that  it  is  small  and  light  weight,  and  requires  low  computation  cost.  To  make  mobile  thermal  imaging  a  robust  multimodal  stress  sensor,  we  have  so  far  contributed:  i)  a  new  robust  respiration  tracking  method;  and  ii)  a  novel  respiration-based  automatic  stress  recognition  model  that  works  in  ubiquitous  settings.  We  are  currently  investigating  new  thermal  signatures  from  underexplored  body  regions  (i.e.  trapezius  muscle)  and  formulating  a  research  framework  to  fuse  multiple  thermal  signatures  for  more  reliable  stress  recognition  outcomes.
2	Modeling  doctor  patient  communication  with  affective  text  analysis.  We  present  a  method  of  automatic  analysis  of  doctor-patient  communication  and  present  findings  after  applying  this  methodology  in  a  post  hoc  study  of  communication  between  oncologists  and  their  cancer  patients  (N=122).  We  analyzed  several  features  of  each  participant  in  the  conversation  including  the  number  of  words  spoken,  the  average  positive/negative  sentiment  expressed,  the  number  of  questions  asked,  and  the  word  diversity  (unique  word  count).  We  found  that  the  number  of  words  spoken  by  the  doctor  is  correlated  with  the  highest  doctor  communication  ability  ratings  made  by  patients.  We  additionally  found  that  unsupervised  clustering  of  conversation  features  into  “styles”  identified  that  certain  styles  are  associated  with  higher  communication  ratings.  Two  well-defined  styles  emerged  when  clustering  based  on  doctor  word  diversity  and  doctor  sentiment:  a  high  word  diversity-neutral  sentiment  style,  which  was  associated  with  higher  ratings,  and  a  low  word  diversity-positive  sentiment  style  with  lower  average  ratings.  Machine  learning  models  were  trained  to  automatically  predict  whether  a  doctor-patient  interaction  will  be  rated  high  or  not  with  a  best-performing  71%  test  set  accuracy.
2	The  effects  of  physicality  on  the  child  s  imagination.  This  paper  investigates  the  effects  of  physical  objects  as  support  for  imagination  in  the  context  of  enactive  storytelling.  More  specifically,  we  target  nine-year-old  children  because  of  their  general  disengagement  from  creative  activity,  a  phenomenon  known  as  the  Fourth-grade  Slump  that  arises  from  a  demotivational  spiral  brought  on  by  social  awareness.  We  study  how  enactment  using  physical  objects  may  allow  the  child  to  better  engage  in  story  imagination.  Our  study  compares  the  richness  of  the  imagination  under  three  main  enactment  conditions  with  objects  that  have  varying  degrees  of  fidelity  to  referent  objects:  Cultural  objects  (physical  visual  resemblance);  Physical  objects  (similar  physical  affordances);  Arbitrary  objects  (minimal  physical  and  visual  affordances).  We  employ  a  mixed-methods  analysis  to  gauge  the  child's  level  of  broader  imagination  from  three  data  sources:  Enactment  videos,  drawings  and  interviews  with  the  children.  We  found  that  the  object  types  significantly  differ  in  their  support  of  the  imagination,  with  the  object  of  highest  specificity  being  most  effective.  Our  findings  can  inform  the  design  of  embodied  creativity-support  systems  for  children.
2	Moment  by  moment  creating  movement  sketches  with  camera  stillframes.  While  mobile  authoring  applications  are  proliferating,  choreographic  tools  that  support  the  generation  and  transformation  of  user-created  movement  `samples'  are  less  readily  available.  iDanceForms  is  a  novel  mobile  choreographic  application  that  generates  unique  movement  choices  through  a  camera  stillframing  technique  to  provoke  movement  catalysts.  In  keeping  with  the  principles  of  whole  body  interaction  (and  principles  of  `defamiliarization'),  the  design  of  iDanceForms  supports  opportunities  for  surprise,  unexpected  movement  choices  and  meaning-making.  This  paper  presents  data  collected  from  an  observational  study  of  choreographers  using  iDanceForms.  In  the  study  we  found  that  choreographers  appropriated  the  intended  functionality  of  iDanceForms  to  create  highly  individualized  and  unexpected  movement  sequences.  They  found  inspiration  in  exploring  unexpected  framing  of  form  and  content,  which  resulted  in  creative  explorations  that  produced  unique  movement  possibilities  provided  by  the  system.  Drawing  from  our  observations  we  discuss  possible  roles  that  sensor-enabled  mobile  devices  could  play  in  movement  generation  through  personal  meaning-making,  creative  choreographic  strategies  and  discovery,  and  in  provoking  whole  body  interaction  through  principles  of  `defamiliarization'  in  the  context  of  HCI.
2	Mischief  humor  from  games  to  playable  cities.  Playable  cities  are  smart  cities  that  allow  artists,  urban  designers,  and  city  dwellers  to  introduce  sensors  and  actuators  or  use  already  in-place  sensors  and  actuators  for  playful  applications.  These  applications  allow  users  to  interact  with  street  furniture  or  with  and  in  public  buildings.  Sensors  and  actuators  can  be  embedded  in  street  furniture  and  public  spaces,  they  can  be  addressed  directly  (or  they  address  the  city  dweller)  or  interaction  is  mediated  by  using  wearables.  In  this  paper  we  argue  that  there  will  be  a  convergence  of  videogame  environments,  augmented  reality  environments  and  digitally  enhanced  physical  (smart)  environments.  As  a  consequence  we  may  expect  that  the  various  forms  of  social  and  multi-player  videogame  behavior  will  also  emerge  in  playable  cities.  In  particular  we  pay  attention  to  the  various  forms  of  mischief  humor  in  multiplayer  videogame  environments  and  our  expectations  about  having  them  appear  in  future  playable  cities.
2	Real  time  robust  recognition  of  speakers  emotions  and  characteristics  on  mobile  platforms.  We  demonstrate  audEERING's  sensAI  technology  running  natively  on  low-resource  mobile  devices  applied  to  emotion  analytics  and  speaker  characterisation  tasks.  A  showcase  application  for  the  Android  platform  is  provided,  where  au-dEERING's  highly  noise  robust  voice  activity  detection  based  on  Long  Short-Term  Memory  Recurrent  Neural  Networks  (LSTM-RNN)  is  combined  with  our  core  emotion  recognition  and  speaker  characterisation  engine  natively  on  the  mobile  device.  This  eliminates  the  need  for  network  connectivity  and  allows  to  perform  robust  speaker  state  and  trait  recognition  efficiently  in  real-time  without  network  transmission  lags.  Real-time  factors  are  benchmarked  for  a  popular  mobile  device  to  demonstrate  the  efficiency,  and  average  response  times  are  compared  to  a  server  based  approach.  The  output  of  the  emotion  analysis  is  visualized  graphically  in  the  arousal  and  valence  space  alongside  the  emotion  category  and  further  speaker  characteristics.
2	Affective  multi  agent  system  for  simulating  mechanisms  of  social  effects  of  emotions.  The  goal  of  the  paper  is  to  present  a  multi-agent  based  approach  for  simulating  social  effects  of  emotion  in  group  with  no  common  tangible  goal.  The  paper  presents  formalization  of  mechanisms  that  are  needed  to  implement  social  effects  of  emotions  in  a  group  as  well  as  describes  design  and  implementation  of  such  mechanisms  in  affective  multi-agent  system  for  believable  human  simulation.  When  fully  developed,  it  would  enable  multiple  applications,  including  simulating  user  interactions  for  analyzing  media  impact  on  human  groups.
2	Compact  ultrasound  device  for  noncontact  interaction.  This  paper  introduces  a  compact  device  for  noncontact  interaction.  It  can  push  objects  from  a  distance  by  utilizing  focused  ultrasound.  The  maximum  output  force  at  the  focal  point  is  16  mN.  The  position  of  the  focal  point  can  be  moved  quickly  and  precisely.  The  device  is  small  (19×19×5  cm3),  light  (0.6  kg),  and  compact  so  that  one  can  pick  it  up  with  one  hand  and  install  it  at  various  places.  This  easy-to-use  device  would  lead  to  a  wide  variety  of  applications.
2	A  survey  of  players  opinions  on  interface  customization  in  world  of  warcraft.  Massively  multiplayer  online  role-playing  games,  such  as  World  of  Warcraft,  have  become  very  popular  in  recent  years.  These  types  of  games  often  provide  the  player  with  a  wide  range  of  game  abilities,  weapons,  tools,  options,  stats,  etc.  which  grow  in  number  as  the  player  progresses  through  the  game.  This  in  turn  makes  the  user  interface  of  the  game  more  complex  and  difficult  to  interact  with.  Games  such  as  World  of  Warcraft  attempt  to  combat  this  by  providing  mechanisms  (e.g.  add-ons)  for  interface  customization  by  the  player.  However,  it  is  unclear  which  aspects  of  the  game  interface  players  prefer  to  customize,  or  what  effects  those  customizations  have  on  their  gameplay  experience.  In  this  paper  we  present  a  survey  of  World  of  Warcraft  players  to  identify  their  opinions  on  game  interface  customization  preferences.  The  results  of  this  survey  are  likely  to  apply  to  other  massively  multiplayer  online  role-playing  games.
2	Playing  to  beat  the  blues.  This  study  examined  the  effects  of  exposure  to  specific  message  features  on  the  usage  of  mental  health  video  games  applications.  Based  on  linguistic  agency  and  disease  causality,  participants  with  mild  depression  read  messages  assigning  agency  to  depression  or  to  humans  and  depicting  depression  as  external  or  internal  before  accessing  a  video  game  designed  for  handling  depression  symptoms.  Assigning  internal  causality  language  led  to  greater  game  usability  and  higher  intentions  of  using  the  games.  Game  performance  and  time  spent  playing  the  games,  was  higher  for  external  causality  language.  Overall,  findings  from  this  study  demonstrate  the  effectiveness  of  specific  message  prompt  features  to  promote  video  game  experience  for  mental  health  apps.  Participants  with  mild  depression  read  message  prompts  before  using  a  health  game  app.Endogenous  causality  language  in  message  prompts  led  to  increased  app  engagement.Endogenous  causality  language  in  message  prompts  increased  intentions  to  use  the  app.Exogenous  causality  prompts  led  to  more  time  spent  with  the  app  and  game  performance.
2	Peer  attachment  sexual  experiences  and  risky  online  behaviors  as  predictors  of  sexting  behaviors  among  undergraduate  students.  The  current  study  created  a  predictive  risk  model  for  sexting  behaviors  based  on  prior  sexual  experiences,  online  environments,  and  peer  attachment  styles  (trust,  alienation,  and  ambivalence).  Eighty-eight  undergraduate  students  completed  an  anonymous  online  survey  regarding  their  sexting  behaviors,  sexual  experiences,  Internet  usage,  and  peer  attachment  styles.  61%  Of  the  sample  reported  sexting.  The  final  predictive  model  for  sexting  behavior  included  the  following  variables:  ambivalence,  unprotected  sex,  Internet  adult  pornography  use,  and  web-based  video  chatting  with  strangers.  In  terms  of  individual  relationships,  unprotected  sex,  adult  pornography  use,  and  web-based  chatting  with  strangers  were  significantly  related  to  sexting  (see  Table  5).  Individuals  who  have  had  unprotected  sex  were  4.5  times  more  likely  to  sext,  and  individuals  who  viewed  adult  pornography  were  4  times  more  likely  to  sext.  Finally,  individuals  who  had  engaged  in  web-based  video  chatting  with  strangers  were  2.4  times  more  likely  to  sext.  Future  research  suggestions  and  study  limitations  are  discussed.
2	An  empirical  study  on  trust  in  mobile  banking.  Trust  is  essential  for  Mobile  Banking  (MB)  adoption  and  usage.  MB  technology  has  the  potential  to  improve  people's  quality  of  life  and  to  bring  efficiency  to  banks.  In  this  paper,  MB  trust  was  addressed  in  Brazil,  a  developing  country  that  has  an  enormous  potential  for  expansion  of  banking  services.  We  used  Confirmatory  Factor  Analysis  and  Structural  Equation  Modeling  to  analyze  the  database,  which  was  composed  of  1077  questionnaires.  In  this  study  sample,  determinants  of  trust  had  similar  behavior  when  compared  to  determinants  of  trust  previously  pointed  out  in  the  literature.  Our  discussion  indicated  a  kind  of  information  asymmetry  that  could  be  mitigated  in  order  to  build  trust  in  MB  and  promote  its  adoption.  However,  we  observed  a  negative  relationship  between  trust  in  MB  and  undergraduate  course  area  (dummy  variable  for  undergraduate  courses  in  technology).  The  inclusion  and  analysis  of  this  new  variable,  especially  in  developing  countries,  may  contribute  with  the  literature  on  MB  adoption.  We  studied  mobile  banking  trust  in  Brazil  where  has  a  high  potential  for  expansion  of  banking  services.We  analyzed  the  database,  which  was  composed  of  1077  questionnaires  using  structural  equation  model.We  found  the  information  asymmetry  for  trust  in  mobile  banking  and  promote  its  adoption.We  observed  a  negative  relationship  between  trust  and  undergraduate  course  area.Theoretical  and  practical  implications  are  discussed  in  the  paper.
2	Determinants  of  consumers  attitudes  toward  mobile  advertising.  Mobile  advertising  is  booming,  due  to  the  popularity  of  mobile  devices.  Drawing  on  motivation  theory,  this  study  examines  the  factors  that  influence  consumers'  attitudes  toward  mobile  advertising.  The  authors  identify  timeliness,  localization,  and  personalization  of  the  advertisement  message  as  antecedents  of  extrinsic  motivation,  as  well  as  consumer  innovativeness  and  perceived  enjoyment  as  antecedents  of  intrinsic  motivation.  Using  structural  equation  modeling,  the  study  analyzes  the  conceptual  model  with  a  sample  of  218  mobile  phone  users.  Both  intrinsic  and  extrinsic  motivations  mediate  the  effects  of  the  advertising  message's  characteristics  on  mobile  phone  users'  attitudes  toward  mobile  advertising.  Extrinsic  motivation  factors  are  timeliness,  localization  and  personalization.Intrinsic  motivation  factors  are  consumer  innovativeness  and  perceived  enjoyment.Both  intrinsic  motivation  and  extrinsic  motivation  have  mediate  effects.
2	Life  span  differences  in  the  uses  and  gratifications  of  tablets.  Tablet  uses:  information  seeking,  relationship  maintenance,  style,  amusement,  &  organization.There  are  life-span  differences  in  tablet  uses  and  gratifications.Tablet  uses  and  gratifications  predict  hours  of  tablet  use  across  four  generations.Life-span  tablet  use  disparities  have  practical  applications,  especially  for  health  management.  This  study  extends  Uses  and  Gratifications  theory  by  examining  the  uses  and  gratifications  of  a  new  technological  device,  the  tablet  computer,  and  investigating  the  differential  uses  and  gratifications  of  tablet  computers  across  the  life-span.  First,  we  utilized  a  six-week  tablet  training  intervention  to  adapt  and  extend  existing  measures  to  the  tablet  as  a  technological  device.  Next,  we  used  paper-based  and  online  surveys  (N=847),  we  confirmed  four  main  uses  of  tablets:  (1)  information  seeking,  (2)  relationship  maintenance,  (3)  style,  (4)  amusement  and  killing  time,  and  added  one  additional  use  category  (5)  organization.  We  discovered  differences  among  the  five  main  uses  of  tablets  across  the  life-span,  with  older  adults  using  tablets  the  least  overall.  Builders,  Boomers,  GenX  and  GenY  all  reported  the  highest  means  for  information  seeking.  Finally,  we  used  a  structural  equation  model  to  examine  how  uses  and  gratifications  predicts  hours  of  tablet  use.  The  study  provides  limitations  and  suggestions  for  future  research  and  marketers.  In  particular,  this  study  offers  insight  to  the  relevancy  of  theory  as  it  applies  to  particular  information  and  communication  technologies  and  consideration  of  how  different  periods  in  the  life-span  affect  tablet  motivations.
2	Compulsive  internet  use  and  relations  between  social  connectedness  and  introversion.  People  with  poor  social  connectedness  have  a  greater  risk  of  developing  compulsive  Internet  use.More  introverted  people  appear  to  be  more  vulnerable  to  compulsive  Internet  use.Using  the  Internet  can  become  problematic  and  lead  to  poor  social  connectedness.Compulsive  Internet  Use  Scale  is  a  reliable  and  valid  scale  in  a  sample  of  students.  This  study  aims  to  further  understand  factors  involved  in  compulsive  Internet  use,  with  specific  focus  on  the  relation  between  social  connectedness,  the  trait  introversion,  and  compulsive  Internet  use.  While  Internet  use  can  enhance  social  connectedness,  compulsive  Internet  use  has  been  associated  with  poor  social  connectedness.  The  factors  that  make  a  person  vulnerable  to  compulsive  Internet  use  and  its  negative  effects  remain  unclear.  The  personality  trait  introversion  has  been  associated  with  poor  social  connectedness,  and  there  is  disagreement  on  whether  or  not  social  interaction  on  the  Internet  is  beneficial  for  people  high  in  this  trait.  Australian  university  students  (N=168)  participated  in  an  online  survey.  Standardised  scales  were  used  to  measure  social  connectedness,  introversion,  and  compulsive  Internet  use.  Results  show  that  introverted  adults  report  more  compulsive  Internet  use  symptoms  than  extroverts.  In  addition,  introversion  partially  mediated  the  relation  between  compulsive  Internet  use  and  social  connectedness.  The  results  raise  questions  for  future  research  into  factors  involved  in  the  development  of  compulsive  Internet  use  and  its  effect  on  social  connectedness,  especially  in  those  who  are  introverted.
2	African  american  female  students  in  online  collaborative  learning  activities.  The  aim  of  the  present  study  was  to  explore  how  African  American  female  students  perceived  their  experience  of  online  collaborative  learning.  A  qualitative  study  was  conducted  in  a  university  in  Southeastern  United  States.  We  conducted  semi-structured  interviews  with  nine  African  American  female  students  in  an  online  multimedia  instructional  design  course.  Data  triangulation  was  achieved  by  including  the  analysis  of  the  students'  emails,  their  posts  on  online  chat  room  and  bulletin  boards.  Results  revealed  that  the  perspectives  of  African  American  women  towards  online  collaborative  learning  could  be  categorized  into  three  themes:  (a)  peer  support  perceived  as  a  give-and-take  process  with  a  sense  of  fairness,  (b)  group  member  role  as  a  formation  of  identity,  and  c)  "frustration"  as  a  common  response  to  differing  levels  of  peer  participation  and  interaction.  These  emerging  factors  reflected  African  American  women's  perceptions  towards  online  collaborative  learning  and  provided  profound  implications  for  future  research  and  practice.  We  examined  African  American  female  students'  online  collaborative  learning.This  qualitative  study  examined  from  a  socio-emotional-cognitive  perspective.African  American  women  perceived  peer  supports  as  a  give-and-take  process.African  American  women  viewed  group  member  role  as  a  formation  of  identity."Frustration"  was  the  common  response  to  peer  participation  and  interaction.
2	To  boldly  go  where  no  group  has  gone  before  an  analysis  of  online  group  identity  and  validation  of  a  measure.  Abstract      Online  groups  have  become  more  popular  in  recent  decades,  in  both  research  and  practice.  Many  authors  have  proposed  important  outcomes  of  group  membership,  and  some  have  even  investigated  some  preliminary  dynamics  of  these  online  groups.  Unfortunately,  no  validated  measure  of  online  group  identity  exists,  causing  these  researchers  to  employ  measures  with  poor  psychometric  properties  or  concerning  construct  validity.  For  these  reasons,  the  current  article  undergoes  a  multiple  study  process  to  validate  a  measure  of  online  group  identity.  In  doing  so,  several  aspects  of  online  group  identity  are  discovered,  largely  based  on  propositions  previously  posed  for  offline  groups.  Study  1  demonstrates  that  individuals  from  a  general  sample  identify  with  online  groups,  and  the  measure  has  an  identifiable  factor  structure.  Using  a  naturally  occurring  online  group,  Study  2  shows  the  measure’s  concurrent,  convergent,  and  divergent  validity,  while  simultaneously  revealing  many  novel  relationships  of  online  group  identity.  Lastly,  Study  3  investigates  the  effect  of  The  United  States  President,  Barack  Obama,  creating  an  account  on  the  website  studied  in  Study  2  and  interacting  with  group  members.  The  results  of  Study  3  reveal  that  online  group  identity  did  not  change  after  this  historic  event,  although  members  had  notable  emotional  responses.  In  all,  the  current  study  illustrates  the  validity  of  an  online  group  identity  measure,  and  discovers  many  important  relationships  previously  unknown  in  regards  to  online  group  identity.  It  is  believed  that  the  investigated  measure  of  online  group  identity  will  become  an  important  tool  in  future  research,  especially  when  further  probing  the  relationships  analyzed  in  the  current  study.  Further  implications  and  suggestions  for  future  studies  are  discussed.
2	Ecrm  2  0  applications  and  trends  the  use  and  perceptions  of  greek  tourism  firms  of  social  networks  and  intelligence.  Although  previous  CRM  studies  treat  eCRM  as  a  synonymous  with  1:1  communications  and  personalised  service  at  an  individual  basis,  web  2.0  further  enables  firms  and  users  to  generate  customer  value  and  build  customer  relations  through  social  networking,  co-learning,  co-production  and  collaboration.  The  paper  advocates  the  development  of  eCRM  2.0  strategies  aiming  at  exploiting  both  the  networking  and  social/customer  intelligence  of  web  2.0  by  integrating  and  engaging  customers  and  communities  along  firms'  value  chain  operations.  The  usage  and  readiness  of  Greek  tourism  firms  to  embark  on  eCRM  2.0  strategies  was  examined  by  conducting  an  e-mail  survey  and  focused  groups  with  tourism  professionals.  Despite  the  low  adoption  rates  of  eCRM  2.0,  respondents  were  aware  and  greatly  concerned  about  the  practical  implications  of  this  field,  which  in  turn  elucidated  an  agenda  for  future  research  studies.
2	Eye  hand  coordination  strategies  during  active  video  game  playing.  All  participants  used  multiple  eye-hand  strategies  while  playing  digital  games.Different  active  video  games  induced  different  latencies  and  gaze  points.Children  had  different  latencies  and  fixation  than  adults  during  game  playing.Eye-tracking  technology  may  explore  eye-hand  coordination  in  game-based  learning.  The  purpose  of  this  exploratory  study  was  to  examine  the  eye-hand  coordination  patterns  while  playing  two  virtual-reality  active  video  games  in  healthy  children  and  adults.  Eleven  children  (mean  age  8.09years)  and  ten  adults  participated  in  the  study.  Each  participant  played  two  digital  games,  Slap  Stream  and  Kung  Foo,  from  EyeToy  Play  software.  Eye  movements  were  recorded  using  Mobile  Eye  eyetracker.  Eye-hand  coordination  strategies  and  the  time  when  virtual  object  appeared,  the  gaze  shifted  to  the  object,  the  reach  started,  the  gaze  shifted  away,  and  the  reach  ended  were  coded  from  the  video.  The  latencies  between  these  events  were  computed  and  compared  between  adults  and  children  and  between  games.  The  fixation  duration,  number  of  fixations,  and  number  of  gaze  points  were  also  computed  for  each  game's  areas  of  interests.  Results  showed  that  (1)  all  participants  used  multiple  eye-hand  strategies  while  playing  active  video  games  with  some  strategies  more  than  others;  (2)  the  Kung  Foo  game  (with  one  target  appearing  on  the  screen)  and  the  Slap  Stream  game  (with  potentially  multiple  targets  appearing  on  the  screen)  induced  different  latencies  and  gaze  points  between  children  and  adults;  and  (3)  children  had  longer  latencies  and  shorter  fixation  durations  than  adults.  The  study  thus  provides  in-depth  understanding  of  different  patterns  of  eye-hand  coordination  in  relations  to  active  video  game  playing.  The  significant  differences  in  coordinative  control  strategies  found  between  adults  and  children  as  well  as  between  game  types  provide  a  basis  for  further  research  in  both  child  development  and  game-based  learning  fields.
2	Why  are  children  attracted  to  the  internet  the  role  of  need  satisfaction  perceived  online  and  perceived  in  daily  real  life.  From  the  perspective  of  self-determination  theory,  this  study  investigated  the  motivations  for  children's  Internet  use  by  examining  how  basic  psychological  need  satisfaction,  perceived  online  and  perceived  in  daily  real  life,  affects  children's  Internet  use  outcomes.  A  total  of  637  elementary  school  students  from  China  took  participated  in  this  study.  The  results  from  structural  equation  modeling  showed  that  need  satisfaction  perceived  online  predicted  higher  levels  of  Internet  use  and  more  positive  affect  experienced  online,  whereas  need  satisfaction  perceived  in  daily  real  life  predicted  less  time  spent  online,  less  negative  affect,  and  more  positive  affect.  Both  inherent  properties  of  the  experiences  provided  by  the  Internet  and  children's  social  backgrounds  contribute  to  Internet  use  outcomes.  This  study  supports  self-determination  theory  in  explaining  children's  Internet  use  motivations.  Implications  for  efforts  to  encourage  appropriate  Internet  use  and  directions  for  future  research  are  discussed.
2	An  application  of  brand  personality  to  advergames.  The  purpose  of  this  study  is  to  explore  the  advergame  personality  (AP)  dimensions  and  to  explicate  the  underlying  relationships  of  the  AP  dimensions  with  company  attributes,  product  categories,  and  consumers'  behavioral  intentions.  A  series  of  surveys  with  convenience  samples  indicates  that  consumers  ascribe  personality  characteristics  to  advergames  and  that  the  perceived  AP  is  five-dimensional,  specifically:  vibrancy,  competence,  intelligence,  activeness,  and  excitement.  A  path  model  shows  that  company  attributes  (i.e.,  size,  reputation,  relevance)  influenced  each  AP  dimension  in  various  ways,  depending  on  product  category  (i.e.,  hedonic  vs.  utilitarian),  which  in  turn  affected  consumers  intentions  to  play  an  advergame  and  to  purchase  a  product.  This  study  produces  valuable  insights  into  the  effectiveness  of  advergames  and  into  ways  to  strategically  lead  to  behavioral  intentions  to  play  an  advergame  and  purchase  a  product.  Consumers  assign  human  personality  traits  to  advergames.Consumers'  perceived  advertising  personality  is  five-dimensional.Company  attributes  and  product  category  influence  advergame  personality  dimensions.Relationships  between  behavioral  intentions  (advergaming,  purchase)  and  advergame  personality  are  identified.
2	What  can  we  learn  from  advertisements  of  logistics  firms  on  youtube  a  cross  cultural  perspective.  The  current  leading  third  party  logistics  industry  players  have  all  experienced  a  shift  towards  an  increasingly  retail  consumer  point  of  contact,  therefore  facilitating  the  need  to  appeal  to  the  retail  consumer  through  advertising  and  brand  management.  With  the  rise  in  word-of-mouth  (WOM)  advertising  in  online,  Web  2.0  contexts,  this  empirical  work  represented  a  first  attempt  to  investigate  the  correlation  between  the  placement  of  corporate  ads  on  a  user-generated  Web  2.0  platform  with  the  bottom  line  of  the  logistics  firms  involved,  with  a  focus  on  the  express  package  industry.  The  study  further  investigated  whether  there  were  common  characteristics  of  effective  advertisements  in  Web  2.0  environments  as  rated  by  viewers,  and  whether  such  assessments  would  hold  across  cultural  and  demographic  boundaries,  given  the  global  nature  of  Web  2.0  content.  It  was  found  that  both  page  hit  popularity  and  respondent  agreement  on  effective  advertisement  characteristicsrelated  positively  to  sales,  with  results  being  consistent  cross-culturally.  Firms  are  strongly  advised  to  take  note  of  the  massive  potential  for  highly  low  cost  or  free  advertising  such  platforms  can  provide.  Conversely,  firms  must  become  aware  of  both  the  benefits  and  risks  of  Web  2.0  environments,  including  damage  caused  by  potential  saboteurs  to  their  brand  control  and  image.
2	Relational  dialectics  and  social  networking  sites  the  role  of  facebook  in  romantic  relationship  escalation  maintenance  conflict  and  dissolution.  Abstract      Due  to  their  prevalence  and  unique  affordances,  social  networking  sites  such  as  Facebook  have  the  potential  to  influence  offline  relationships.  This  study  employed  Baxter’s  (2011)  refinement  of  relational  dialectics  theory  to  explore  Facebook’s  role  in  emerging  adults’  romantic  relationships.  Data  from  ten  focus  groups  revealed  that  Facebook  contributes  to  and  provides  a  forum  for  discursive  struggles  related  to  the  integration–separation,  expression–privacy,  and  stability–change  dialectics.  Romantic  partners  are  able  to  connect  with  each  other  and  integrate  their  social  networks  on  Facebook,  but  some  struggle  to  maintain  privacy  and  independence.  As  such,  SNSs  can  be  a  site  of  and  trigger  for  romantic  conflict.  Participants’  responses  indicated  that  Facebook  is  interwoven  with  the  experience  of  these  dialectics  due  to  its  affordances,  specifically  the  semi-public  nature  of  relationship  activities  on  Facebook  and  the  shift  in  control  over  relational  information  from  individuals  to  network  members.
2	Defining  sociability  and  social  presence  in  social  tv.  Social  TV,  a  new  interactive  television  service,  has  been  rapidly  developing.  With  the  conceptual  model  of  sociability,  this  study  empirically  investigates  the  effects  of  perceived  sociability  on  the  motivations  and  attitudes  toward  Social  TV.  A  model  is  created  to  validate  the  relationship  of  perceived  sociality  to  social  presence,  usability,  and  intention.  Empirical  findings  show  the  key  influence  of  sociability  on  users'  acceptance  and  intent  to  continue  using  Social  TV.  Implications  of  the  findings  are  discussed  in  terms  of  building  a  theory  of  sociability  and  providing  practical  insights  into  developing  a  meaningful  sociable  TV  interface.
2	The  diffusion  of  misinformation  on  social  media.  This  study  examines  dynamic  communication  processes  of  political  misinformation  on  social  media  focusing  on  three  components:  the  temporal  pattern,  content  mutation,  and  sources  of  misinformation.  We  traced  the  lifecycle  of  17  popular  political  rumors  that  circulated  on  Twitter  over  13  months  during  the  2012  U.S.  presidential  election.  Using  text  analysis  based  on  time  series,  we  found  that  while  false  rumors  (misinformation)  tend  to  come  back  multiple  times  after  the  initial  publication,  true  rumors  (facts)  do  not.  Rumor  resurgence  continues,  often  accompanying  textual  changes,  until  the  tension  around  the  target  dissolves.  We  observed  that  rumors  resurface  by  partisan  news  websites  that  repackage  the  old  rumor  into  news  and,  gain  visibility  by  influential  Twitter  users  who  introduce  such  rumor  into  the  Twittersphere.  In  this  paper,  we  argue  that  media  scholars  should  consider  the  mutability  of  diffusing  information,  temporal  recurrence  of  such  messages,  and  the  mechanism  by  which  these  messages  evolve  over  time.  False  political  rumors  tend  to  resurface  multiple  times  after  the  initial  publication.False  political  rumors  often  turn  into  a  more  intense  and  extreme  version  over  time.Resurged  old  political  rumors  tend  to  be  presented  as  news.None-traditional  partisan  media  are  often  behind  the  constant  generation  of  false  news.
2	Girls  video  gaming  behaviour  and  undergraduate  degree  selection  a  secondary  data  analysis  approach.  Abstract  Girls'  uptake  of  physical  science,  technology,  engineering  and  mathematics  (PSTEM)  degrees  continues  to  be  poor.  Identifying  and  targeting  interventions  for  girl  groups  that  are  likely  to  go  into  STEM  degrees  may  be  a  possible  solution.  This  paper,  using  a  self-determination  theory  and  self-socialisation  framework,  determines  whether  one  girl  group's,  “geek  girls”,  video  gaming  behaviour  is  associated  with  their  choice  of  undergraduate  degree  by  using  two  secondary  datasets:  a  cross-sectional  study  of  the  Net  Generation  (n = 814)  and  the  Longitudinal  Study  of  Young  People  in  England  (LSYPE)  dataset  (n = 7342).  Chi-square  analysis  shows  that  girls  who  were  currently  studying  a  PSTEM  degree  were  more  likely  to  be  gamers  and  engage  in  multiplayer  gamers.  Further,  using  logistic  regressions,  girls  who  were  heavy  gamers  (>9 h/wk)  at  13–14  years  were  found  to  be  more  likely  to  pursue  a  PSTEM  degree  but  this  was  influenced  by  their  socio-economic  status.  Similar  associations  with  boys  and  PSTEM  degrees  were  not  found  or  were  weak.  Therefore,  girls  were  self-socialising  or  self-determining  their  identity  groups  through  gaming.  This  research  can  provide  the  basis  for  whether  encouraging  gaming  in  adolescent  girls  can  help  them  onto  PSTEM  pathways.
2	Exploring  children  s  learning  experience  in  constructionism  based  coding  activities  through  design  based  research.  Abstract  Over  the  last  few  years,  the  integration  of  coding  activities  for  children  in  K-12  education  has  flourished.  In  addition,  novel  technological  tools  and  programming  environments  have  offered  new  opportunities  and  increased  the  need  to  design  effective  learning  experiences.  This  paper  presents  a  design-based  research  (DBR)  approach  conducted  over  two  years,  based  on  constructionism-based  coding  experiences  for  children,  following  the  four  stages  of  DBR.  Three  iterations  (cycles)  were  designed  and  examined  in  total,  with  participants  aged  8–17  years  old,  using  mixed  methods.  Over  the  two  years,  we  conducted  workshops  in  which  students  used  a  block-based  programming  environment  (i.e.,  Scratch)  and  collaboratively  created  a  socially  meaningful  artifact  (i.e.,  a  game).  The  study  identifies  nine  design  principles  that  can  help  us  to  achieve  higher  engagement  during  the  coding  activity.  Moreover,  positive  attitudes  and  high  motivation  were  found  to  result  in  the  better  management  of  cognitive  load.  Our  contribution  lies  in  the  theoretical  grounding  of  the  results  in  constructionism  and  the  emerging  design  principles.  In  this  way,  we  provide  both  theoretical  and  practical  evidence  of  the  value  of  constructionism-based  coding  activities.
2	Depressed  and  swiping  my  problems  for  later  the  moderation  effect  between  procrastination  and  depressive  symptomatology  on  internet  addiction.  Abstract  Based  on  insights  from  the  model  of  compensatory  internet  use  and  emotion  regulation  theory,  this  study  aimed  to  explore  two  possible  mechanisms  explaining  the  reliable  association  between  depressive  symptomatology  and  internet  addiction:  procrastination  on  the  internet  and  flow  experiences  online.  Data  were  collected  from  529  high  school  students,  with  a  mean  age  of  15.2  years  (SD = 1.30),  enrolled  in  six  schools  in  a  metropolitan  region  of  Chile.  Voluntary  participants  completed  self-reported  measures  of  internet  addiction,  depressive  symptomatology,  procrastination  on  the  internet,  and  flow  experiences  online.  A  three-level  hierarchical  linear  model  was  calculated  to  evaluate  the  potential  moderator  effect  of  flow  and  procrastination  on  the  relationship  between  depressive  symptomatology  and  internet  addiction.  Results  revealed  that  procrastination  moderated  this  relationship  while  flow  experiences  online  did  not.  The  findings  are  discussed  in  terms  of  their  implications  for  research  and  clinical  practice,  highlighting  the  importance  of  considering  the  intentions  behind  internet  usage.
2	Do  individuals  with  problematic  and  non  problematic  internet  game  use  differ  in  cooperative  behaviors  with  partners  of  different  social  distances  evidence  from  the  prisoner  s  dilemma  and  chicken  game.  Abstract  Numerous  studies  have  reported  deficits  in  non-social  decision-making  tasks  and  reward-related  impulsivity  in  individuals  with  problematic  Internet  game  use  (PIGU).  However,  those  findings  may  not  generalize  to  situations  that  involve  social  interactions.  Therefore,  this  study  aimed  to  examine  how  PIGU  may  relate  to  behavior  during  cooperative  dilemmas  and  the  potential  moderating  influences  of  social  distance.  Twenty-three  subjects  with  PIGU  and  23  with  occasional  Internet  game  use  (OIGU)  were  recruited  to  participate  in  the  experiment.  Participants  played  League  of  Legends  cooperatively  with  one  of  their  real-life  friends  and  one  strange  confederate  game-mate  under  non-face-to-face  conditions,  and  then  completed  Prisoner's  Dilemma  (PD)  and  Chicken  Game  (CG)  tasks  with  their  friend,  game-mate,  and  stranger,  respectively.  Results  showed  no  between-group  difference  on  cooperation  in  the  PD,  but  participants  with  PIGU  (as  compared  to  those  with  OIGU)  exhibited  lower  cooperation  during  the  CG.  Compared  with  friends  and  stranger  partners,  short-time  cooperation  in  online  gaming  did  not  improve  cooperation  toward  game-mates  in  the  PD,  but  elevated  cooperation  in  the  PIGU  group  toward  game-mates  in  the  CG.  The  results  of  this  work  suggest  that  risk-taking-related  preferences  in  individuals  with  PIGU  may  extend  to  some  aspects  of  social  decision-making.
2	Determination  of  optimal  paths  to  task  goals  using  expert  system  based  on  goms  model.  Website  users  often  experience  several  difficulties  while  trying  to  access  or  navigate  a  website.  This  is  mostly  due  to  their  inability  to  familiarize  themselves  with  the  structures  in  the  website  or  as  a  result  of  complex  procedures  which  prevent  them  from  reaching  their  goals.  It  is  therefore,  important  to  develop  a  methodology  or  guidance  technique  for  assisting  website  users  to  achieve  their  goals.  A  type  of  expert  system  that  provides  the  needed  guidance  necessary  in  order  to  achieve  these  goals  was  proposed  in  this  paper.  A  sample  website  was  initially  designed,  and  the  analysis  of  website  menu  structure  was  conducted.  The  rules  to  find  the  optimal  path  are  established  based  on  the  Goals,  Operators,  Methods,  and  Selection  rules  (GOMS)  model  by  considering  individual  preferences  on  input  devices.  Derivatives  of  the  GOMS  model  such  as  the  Cognitive  Perceptual  Model  GOMS,  Natural  GOMS  Language,  GOMS  Language  and  GOMS  Language  Evaluation  and  Analysis  were  reviewed.  The  Card,  Moran  and  Newell  (CMN)  GOMS  technique  was  selected  as  the  primary  inference  engine  of  the  proposed  expert  system.  This  was  primarily  based  on  the  highly  efficient  and  exemplary  capability  of  the  CMN-GOMS  to  predict  both  operator  sequence  and  execution  time.  The  expert  system  was  finally  constructed  from  the  result  of  the  acquired  knowledge  base  and  other  applicable  rules.
2	Problem  solving  through  digital  game  design.  Project  Tech  engages  secondary  students  (ages  14  to  17)  in  the  process  of  digital  game  design  in  a  variety  of  in-school,  after-school,  and  secure  settings.  The  goal  of  Project  Tech  is  to  leverage  students  interests  in  games  and  design  to  foster  their  problem-solving  in  a  supportive  environment  where  they  learn  to  create  games  about  a  social  issue  they  have  experienced  personally.  The  present  study  compares  the  in-school  special  needs  version  of  Project  Tech  (n=11)  to  examine  problem  solving.  Students  enrolled  in  Project  Tech  were  guided  in  the  process  of  designing  digital  games  aimed  at  teaching  younger  students  (ages  1215)  about  social  issues  facing  teenagers.  A  quantitative  content  analysis  was  conducted  on  35  iterations  of  a  directed  design  game  and  35  iterations  of  a  free  design  game  created  by  special  needs  young  people  and  director  notes.  The  purpose  of  the  study  was  to  draw  from  the  game  iterations  a  list  of  empirically  grounded  problem  solving  attributes  that  are  associated  with  digital  game  design  in  a  special  needs  classroom.  The  findings  of  the  study  resulted  in  the  understanding  of  problem  solving  with  special  needs  young  people  in  four  areas:  representative  characteristics,  planning  characteristics,  executing  characteristics  and  evaluation  characteristics.  Game  design  promotes  problem-solving  skills  in  special  needs  young  people.Ill-structured  problem  solving  is  supported  by  free  design  more  than  directed  design.Free  design  promotes  representation,  planning  and  evaluating  characteristics  for  problem  solving.Directed  design  promotes  executing  characteristics  for  problem  solving.
2	Overspecified  references.  We  study  the  utility  of  overspecification  in  language  learning.We  compare  two  groups  of  learners  receiving  different  degrees  of  specification.Giving  more  specification  in  practise  exercises  improves  lexical  acquisition  rates.Overspecification  affects  the  design  of  language  learning  materials  and  software.  This  study  explores  the  role  of  quantity  of  information  on  vocabulary  acquisition  in  a  virtual  world.  Previous  studies  have  shown  that,  although  it  makes  interpretation  more  lengthy,  speakers  include  more  information  in  their  referring  expressions  than  is  strictly  necessary  to  identify  an  object  -  they  overspecify.  We  aim  to  study  the  impact  of  this  kind  of  overspecification  on  the  acquisition  of  new  lexemes  in  a  foreign  language.  Our  hypothesis  is  that  using  overspecified  expressions  during  the  practice  of  recently  acquired  vocabulary  will  help  learners  to  better  remember  the  lexemes  and  to  exploit  them  more  efficiently  later  on.  In  this  paper,  we  describe  an  experimental  study  designed  to  evaluate  this  hypothesis,  comparing  two  groups  of  learners  who  received  overspecified  and  minimally  specified  referring  expressions  while  practising  newly  acquired  lexemes  in  the  context  of  a  language  learning  game  in  Russian.  The  game  is  situated  in  a  virtual  environment  and  the  interaction  is  similar  to  that  of  a  video  game.  Our  results,  based  on  experimental  data  from  participants'  performance  as  well  as  a  post-experiment  questionnaire,  support  the  claim  that  overspecification  improves  lexical  acquisition  rates  compared  to  minimal  specification.  Some  pedagogical  suggestions  are  provided  for  the  design  of  referring  expression  generation  algorithms  in  Technology-Enhanced  Language  Learning  (TELL)  and  Computer  Assisted  Language  Learning  (CALL)  Systems.
2	Does  game  rules  work  as  a  game  changer  analyzing  the  effect  of  rule  orientation  on  brand  attention  and  memory  in  advergames.  Abstract  In  advergames,  marketers  typically  use  a  brand  execution  strategy  of  enacting  the  game  rules  and  embedding  the  targeted  brands  in  these  rules  to  achieve  the  brand  effectiveness.  Despite  the  extensive  use  of  this  mode  of  gameplay,  the  current  understanding  in  this  area  lacks  clarity.  In  this  context,  the  present  research  examines  the  importance  of  rule  orientation  as  a  mode  of  gameplay,  its  boundary  conditions  which  strengthen  this  rule  orientation,  and  the  mechanism  through  which  the  application  of  rule  orientation  enhances  gamers'  brand  attention  and  memory.  Results  from  two  experimental  studies  conclusively  prove  that  use  of  rule  orientation  positively  influences  gamers'  brand  attention  and  memory.  Results  also  suggest  that  rule  orientation  boosts  the  gamers'  brand  attention  and  memory  when  it  is  presented  in  a  high  brand-game  goal  congruent  and  in  a  high  game  autonomy  mode.  In  addition,  the  results  also  support  that  rule  orientation  enhance  the  gamers'  attention  and  memory  when  it  designed  as  high  brand-game  goal  congruent  along  with  a  highly  brand  integrated  mode.  The  effect  of  rule  orientation  and  its  boundary  conditions  creates  brrand  attention  and  memory  through  the  gamers'  perception  of  the  flow  experience.  Thus,  the  study  findings  suggest  that  marketers  can  use  rule  orientation  along  with  high  brand-game  goal  congruence,  high  autonomy,  and  high  brand  integration  because  this  mode  of  gameplay  creates  a  very  strong  form  of  brand  attention  and  memory.
2	I  don  t  want  to  be  known  for  that  the  role  of  temporality  in  online  self  presentation  of  young  gay  and  bisexual  males.  Abstract  Self-presentation,  the  social  process  by  which  people  reveal  information  about  themselves  and  perform  the  social  roles  that  structure  everyday  interactions,  has  been  significantly  altered  by  today's  social  technologies.  Where  Goffman's  influential  self-presentation  framework  focused  on  in-person,  real-time  role  performances,  today's  technologies  routinely  involve  viewing  text  and  visual  content  aggregated  over  time  and  linked  to  an  individual's  persistent  online  identity,  the  capacity  for  both  ephemeral  and  more  permanent  content,  and  novel  modes  of  audience  engagement  such  as  liking  and  commenting.  These  new  socio-technical  affordances  not  only  allow  for  novel  behaviors,  but  also  alter  the  temporal  dynamics  of  the  self-presentation  process  in  ways  that  are  not  well  understood.  In  this  paper  we  address  this  gap  through  an  interview  study  of  32  young  gay  and  bisexual  male  (GBM)  Instagram  users  in  the  United  States.  Results  extend  our  understanding  of  the  relationship  between  social  technology  affordances  and  the  temporal  dynamics  of  self-presentation.  We  show  how  participants  drew  on  Instagram's  affordances  for  identity  persistence,  content  persistence  and  audience  engagement  to  slow  down  or  speed  up  their  role  performances  to  particular  audiences,  to  increase  the  visibility  of  their  performances  without  appearing  overtly  to  seek  attention,  and  to  make  their  audience  at  any  given  point  in  time  a  part  of  the  performance  seen  by  future  audiences.
2	The  relations  between  youtube  addiction  social  anxiety  and  parasocial  relationships  with  youtubers  a  moderated  mediation  model  based  on  a  cognitive  behavioral  framework.  Abstract  YouTube  is  a  popular  video-sharing  platform  where  viewers  can  watch  videos  made  by  media  performers  called  YouTubers.  YouTube  is  a  social  media  site  conducive  to  the  development  of  parasocial  relationships,  which  consists  in  asymmetrical  relationships  between  media  users  and  media  performers.  The  aim  of  this  study  is  to  identify  the  determinants  of  YouTube  addiction  by  examining  the  relationships  between  social  anxiety,  parasocial  relationships  with  YouTubers  and  YouTube  addiction  based  on  a  cognitive-behavioral  theoretical  framework.  Data  from  932  participants  were  collected  through  an  online  survey.  Multiple  regression  analyses  and  structural  equation  modeling  using  a  bootstrap  procedure  reveal  that  1)  social  anxiety  and  2)  parasocial  relationships  with  YouTubers  are  predictors  of  YouTube  addiction,  3)  social  anxiety  is  a  predictor  of  parasocial  relationships,  4)  social  anxiety  moderates  the  relation  between  parasocial  relationships  and  YouTube  addiction,  5)  parasocial  relationships  mediate  the  relation  between  social  anxiety  and  YouTube  addiction  and  6)  social  anxiety  also  moderates  this  mediated  effect.  The  proposed  moderated-mediation  model  of  YouTube  addiction  fits  well  the  data.  The  findings  of  this  study  contribute  to  the  literature  on  the  parasocial  compensation  hypothesis  while  providing  useful  information  for  prevention  and  intervention  for  YouTube  addiction  and  social  anxiety  disorder.
2	Lessons  learned  applying  learning  analytics  to  assess  serious  games.  Abstract  Serious  Games  have  already  proved  their  advantages  in  different  educational  environments.  Combining  them  with  Game  Learning  Analytics  can  further  improve  the  life-cycle  of  serious  games,  by  informing  decisions  that  shorten  development  time  and  reduce  development  iterations  while  improving  their  impact,  therefore  fostering  their  adoption.  Game  Learning  Analytics  is  an  evidence-based  methodology  based  on  in-game  user  interaction  data,  and  can  provide  insight  about  the  game-based  educational  experience  promoting  aspects  such  as  a  better  assessment  of  the  learning  process.  In  this  article,  we  review  our  experiences  and  results  applying  Game  Learning  Analytics  for  serious  games  in  three  different  scenarios:  (1)  validating  and  deploying  a  game  to  raise  awareness  about  cyberbullying,  (2)  validating  the  design  of  a  game  to  improve  independent  living  of  users  with  intellectual  disabilities  and  (3)  improving  the  evaluation  of  a  game  on  first  aid  techniques.  These  experiences  show  different  uses  of  game  learning  analytics  in  the  context  of  serious  games  to  improve  their  design,  evaluation  and  deployment  processes.  Building  up  from  these  experiences,  we  discuss  the  results  obtained  and  provide  lessons  learnt  from  these  different  applications,  to  provide  an  approach  that  can  be  generalized  to  improve  the  design  and  application  of  a  wide  range  of  serious  games  in  different  educational  settings.
2	Self  disclosure  on  social  media  the  role  of  perceived  network  responsiveness.  Abstract  Social  media  sites  allow  users  to  instantaneously  self-disclose  to  their  entire  social  network.  This  creates  an  opportunity  to  engage  in  self-expression  that  is  farther-reaching  than  ever  before,  but  also  a  new  challenge:  managing  the  risk  inherent  in  self-disclosing  to  a  large  and  diverse  set  of  people.  What  guides  decisions  about  how  openly  to  self-disclose  in  such  contexts?  Building  on  theoretical  and  empirical  evidence  linking  perceived  partner  responsiveness  to  open  self-disclosure  in  face-to-face  dyadic  interactions,  we  hypothesized  that  perceptions  of  a  Facebook  network's  responsiveness  would  shape  people's  self-disclosure  on  Facebook.  We  also  examined  whether  observers  can  infer  people's  perceived  network  responsiveness  from  thin-slices  of  self-disclosure.  Across  two  studies,  people  who  perceived  their  Facebook  network  as  more  (vs.  less)  responsive  self-disclosed  more  openly  on  Facebook.  Furthermore,  observers  could  infer  participants'  perceived  network  responsiveness  with  some  accuracy  on  the  basis  of  disclosure  openness.  Implications  for  the  self-disclosure  and  person  perception  literatures  are  discussed.
2	Teachers  acceptance  and  use  of  digital  learning  environments  after  hours  implications  for  work  life  balance  and  the  role  of  integration  preference.  While  a  growing  number  of  teachers  use  information  and  communication  technology  (ICT)  for  work  tasks  outside  the  formal  working  hours  and  premises,  research  is  inconclusive  how  this  relates  to  their  work-life  balance.  Following  calls  to  examine  the  antecedents  and  moderating  mechanisms  of  such  behavior,  the  present  study  aims  to  examine  how  technology  acceptance  relates  to  work-related  ICT  use  after  hours  (WIA)  and  work-life  balance,  as  well  as  how  employees'  integration  preference  affects  these  relationships.  Data  was  collected  among  288  secondary  school  teachers  in  Flanders  (Belgium)  concerning  their  use  of  digital  learning  environments  (DLE)  beyond  school  grounds  and  school  hours.  Structural  equation  modelling  shows  that  social  influence  reduces  teachers'  work-life  balance  mediated  by  WIA.  While  there  was  no  support  for  other  technology  acceptance  factors  or  the  moderating  role  of  integration  preference,  performance  expectancy  of  the  DLE  and  integration  preference  were  associated  with  a  higher  work-life  balance.  Hereby,  this  study  contributes  to  research  on  WIA  by  integrating  the  technology  acceptance  framework  with  boundary  theory  and  work-life  research.  Overall,  the  findings  show  that  DLE  have  an  impact  on  teachers’  work-life  balance  independent  of  technological  factors  or  their  personal  preference,  underscoring  the  importance  of  school  policies  that  cement  the  use  of  DLE  in  the  private  domain.
2	Examining  actual  consumer  usage  of  e  wallet  a  case  study  of  big  data  analytics.  Abstract  A  systematic  review  of  previous  mobile  payment  studies  revealed  that  studies  mainly  focused  on  consumer  adoptions  via  conducting  surveys,  yet  the  low  adoption  of  mobile  payment  services  still  exists.  Despite  a  number  of  studies  investigated  barriers  to  mobile  payment  consumer  adoptions,  little  explanation  has  been  provided  without  taking  ecosystem  factors  of  mobile  payment  into  account.  Given  the  promising  future  of  digital  payments,  we  believe  it  is  more  relevant  to  study  the  actual  usage  of  e-wallets  via  a  novel  approach.  The  objective  of  the  study  is  to  identify  and  categorize  related  themes  of  usages  of  e-wallets  using  big  data  analytics.  With  a  large  dataset  of  18,149  user  posts  extracted  from  social  media  platforms,  we  apply  the  text  mining  method  to  analyze  e-wallet  users'  behaviors.  Our  major  findings  are  (i)  whilst  contradicting  with  user  adoption  factors  (e.g.,  usefulness),  users  are  attracted  to  use  e-wallets  to  gain  cashback  and  accumulate  reward  points;  (ii)  successful  measures  for  e-wallet  business  models  include  a  user-friendly  interface,  promotional  campaigns,  and  customer  service  with  real-time  problem-solving;  (iii)  an  intense  competition  between  bank  e-wallets  and  third  party  e-wallets  is  compounded  with  stricter  government  regulations;  (iv)  low  rates  of  merchant  adoption  contribute  to  the  lack  of  critical  mass  use  of  e-wallets.  The  big  data  analytics  of  actual  usage  of  e-wallets  produce  more  relevant  and  accurate  understandings  of  the  mobile  payment  mechanism.  It  reflects  the  complexity  of  human-computer  interactions.  This  study  establishes  a  predictive  model  of  measuring  successful  e-wallet  business  and  provides  a  holistic  view  of  the  mobile  payment  ecosystem  with  empirical  evidence.  The  business  must  go  beyond  consumer  adoption,  learn  actual  user  behavior,  and  take  consumers’  pulse  to  achieve  a  sustainable  business  model.  It  is  suggested  that  governments  reform  and  upgrade  the  monitoring  framework  to  accommodate  the  development  of  payment  systems.
2	Factors  affecting  the  intention  to  use  a  web  based  learning  system  among  blue  collar  workers  in  the  automotive  industry.  This  study  aims  to  identify,  using  an  extended  Technology-Acceptance  Model  (TAM),  the  factors  affecting  the  decision  of  using  a  web-based  learning  system  among  blue-collar  workers  in  the  automotive  industry.  A  structural  equation-modeling  approach  was  applied  to  identify  the  variables  that  significantly  affect  the  decision  of  using  the  system.  Using  LISREL  8.54,  data  collected  from  546  blue-collar  workers  were  used  to  test  the  proposed  research  model.  Empirical  testing  of  the  extended  TAM  found  all  paths  to  be  significant  in  the  hypothesized  directions,  that  is,  the  results  of  the  study  strongly  support  the  application  of  extended  TAM  in  predicting  the  blue-collar  workers'  intention  to  use  a  web-based  learning  system.  Among  the  factors,  social  influence  is  a  much  stronger  predictor  of  user  intention  compared  to  others.  The  study  concludes  with  the  implications  of  this  study  for  managers  and  recommendations  for  possible  future  research.
2	Unconscious  goal  pursuit  primes  attitudes  towards  technology  usage  a  virtual  reality  experiment.  Abstract      Several  approaches  in  technology  adoption,  such  as  the  Technology  Acceptance  Model  (TAM),  ask  future  users  to  provide  evaluations  of  technology.  Such  evaluations  are  expected  to  predict  actual  use  behavior.  For  example,  users’  evaluations  in  terms  of  perceived  usefulness  and  perceived  ease  of  use  are  considered  meaningful  indicators  of  intention  to  use  the  technology,  and  future  usage.  However,  these  approaches  still  show  limited  reliability  and  do  not  consider  other  critical  aspects,  such  as  situated,  unconscious  goals  and  the  tendency  to  perceive  related  affordances.  In  order  to  test  the  hypothesis  that  technology  evaluation  may  be  influenced  by  unconscious  goals,  forty  participants  were  split  in  two  groups.  The  experimental  session  included  two  phases.  In  the  first  phase,  each  group  explored  a  virtual  environment  that  primed  a  specific  goal.  In  the  second  phase,  participants  were  asked  to  evaluate  the  usefulness  and  the  easiness  of  use  of  two  versions  of  the  same  technology  (a  mobile  devices  interface).  Results  showed  that  each  group  evaluated  as  more  useful  the  version  of  the  technology  which  featured  an  affordance  related  to  the  respective  primed  goal.  Discussion  deals  with  the  possible  unconscious  influences  on  attitudes  towards  technology  adoption,  and  provides  operative  guidelines  to  account  for  them  in  technology  adoption  research.
2	Relationships  among  smartphone  addiction  stress  academic  performance  and  satisfaction  with  life.  Results  of  several  studies  have  suggested  that  smartphone  addiction  has  negative  effects  on  mental  health  and  well-being.  To  contribute  to  knowledge  on  this  topic,  our  study  had  two  aims.  One  was  to  investigate  the  relationship  between  risk  of  smartphone  addiction  and  satisfaction  with  life  mediated  by  stress  and  academic  performance.  The  other  aim  was  to  explore  whether  satisfaction  with  life  mediated  by  stress  and  academic  performance  facilitates  smartphone  addiction.  To  identify  test  subjects,  systematic  random  sampling  was  implemented.  A  total  of  300  university  students  completed  an  online  survey  questionnaire  that  was  posted  to  the  student  information  system.  The  survey  questionnaire  collected  demographic  information  and  responses  to  scales  including  the  Smartphone  Addiction  Scale  -  Short  Version,  the  Perceived  Stress  Scale,  and  the  Satisfaction  with  Life  Scale.  Data  analyses  included  Pearson  correlations  between  the  main  variables  and  multivariate  analysis  of  variances.  The  results  showed  that  smartphone  addiction  risk  was  positively  related  to  perceived  stress,  but  the  latter  was  negatively  related  to  satisfaction  with  life.  Additionally,  a  smartphone  addiction  risk  was  negatively  related  to  academic  performance,  but  the  latter  was  positively  related  to  satisfaction  with  life.  Stress  mediates  the  relationship  between  smartphone  addiction  and  satisfaction  with  life.Academic  performance  mediates  the  relationship  b/w  smartphone  addiction  &  satisfaction  with  life.There  is  a  zero  order  correlation  between  smartphone  addiction  and  satisfaction  with  life.
2	Accomplishing  authenticity  in  a  labor  exposing  space.  The  present  work,  through  an  ethnographic  study  of  MySpace  (N=96),  examines  the  ways  in  which  authenticity  is  accomplished  within  a  labor-exposing  space.  To  maintain  authenticity,  actors  must  make  invisible  the  extensive  labor  of  self-presentation.  Certain  online  spaces,  such  as  social  network  sites  and  personal  interactive  homepages,  can  be  thought  of  as  labor-exposing  spaces,  in  that  they  give  actors  clear  and  explicit  control  over  self-representations,  making  impressions  of  spontaneity  difficult  to  accomplish  (Davis,  2010;  Gatson,  2011a;  Marwick  &  boyd,  2010).  I  discuss  and  delineate  several  strategies  used  by  participants  to  maintain  authenticity  on  MySpace.  I  conclude  that  while  the  priorities  of  identity  processes  remain  stable  over  time,  the  ways  in  which  we  accomplish  identity  are  culturally,  historically  and  materially  contingent.
2	The  relationship  between  internet  parenting  styles  and  internet  usage  of  children  and  adolescents.  Parenting  styles,  which  are  known  to  have  a  significant  effect  on  children's  development,  also  have  a  significant  effect  on  children's  Internet  use.  This  study  was  designed  to  demonstrate  how  parenting  styles  in  relation  to  Internet  use  are  perceived  by  children  and  parents  and  how  these  styles  affect  children's  Internet  use.  Both  qualitative  and  quantitative  data-collection  techniques  were  used  in  the  study.  The  quantitative  data-collection  process  was  carried  out  through  the  participation  of  1289  students,  and  the  qualitative  data-collection  process  was  carried  out  with  20  parents  and  23  children.  The  result  of  the  study  showed  that  the  Internet  parenting  styles  could  be  categorized  as  laissez-faire,  permissive,  authoritative,  and  authoritarian.  A  significant  relationship  was  shown  between  Internet  parenting  style  and  child's  gender  and  mothers'  education  level.  It  was  also  found  that  as  the  students'  age  and  grade  level  increased,  Internet  parenting  styles  that  were  interpreted  as  initially  authoritative  were  later  interpreted  as  laissez-faire.  As  the  grade  level  increased,  the  Internet  parenting  style  tended  to  lean  toward  laissez-faire.  A  laissez-faire  Internet  parenting  style  is  the  style  most  commonly  preferred  by  parents.The  relationship  between  child/parent  characteristics  and  Internet  parenting  style  is  significant.Internet  parenting  styles  significantly  affect  the  child's  Internet  usage.Parent's  Internet  style  and  the  child's  view  of  the  parent's  style  do  not  exactly  coincide  with  each  other.
2	Text  based  communication  influences  self  esteem  more  than  face  to  face  or  cellphone  communication.  Diary  data  was  collected  for  76  participants  over  6day  intervals.3649  social  interactions  were  randomly  sampled  across  communication  channels.Self-esteem  was  measured  at  baseline  and  follow-up.Face-to-face  communication  was  the  most  frequent  form  of  communication.Text-based  communication  was  most  influential  on  self-esteem.  Meaningful  social  interactions  are  positively  associated  with  improvements  in  self-esteem,  but  this  phenomenon  has  largely  been  unexplored  in  digital  media  despite  the  prevalence  of  new,  text-based  communication  (e.g.  Facebook,  texting,  email,  etc.).  To  address  this  gap  in  the  literature  the  frequency  and  quality,  or  meaningfulness,  of  communication  was  measured  in  mediated  and  non-mediated  channels  across  a  random  sample  of  3649  social  interactions  using  Experience  Sampling  Methods.  Results  revealed  that  most  communication  took  place  face-to-face  (62%),  with  less  text-based  (about  22%)  and  cell  phone  voice  (14%)  communication.  Meaningful  face-to-face  and  text-based  communication  were  associated  with  changes  in  self-esteem  according  to  a  marginally  significant  and  significant  finding,  respectively.  Text-based  communication  was  more  important  for  self-esteem  than  face-to-face  or  phone  communication,  which  is  consistent  with  research  on  the  magnifying  effect  of  text-based  communication  on  interpersonal  processes.  According  to  the  Internet  enhanced  self-disclosure  hypothesis,  the  psychological  benefits  of  text-based  communication  stems  from  enhanced  self-disclosure,  which  is  also  supported  in  the  data.  Additional  work  is  needed  to  better  understand  the  mechanisms  underlying  the  positive  relationship  between  meaningful  text-based  interactions  and  self-esteem,  but  findings  point  to  the  important  role  of  digital  communication  for  psychological  health.
2	Impact  of  using  classroom  response  systems  on  students  entrepreneurship  learning  experience.  Abstract      Technology-based  teaching  devices  that  promote  interaction  and  communication  between  instructors  and  learners  benefit  active  learning.  Emerging  technologies  for  Classroom  Response  Systems  (CRS)  and  mobile  devices  can  potentially  help  instructors  create  a  student-centered  interactive  classroom.  In  this  study,  the  authors  aim  to  evaluate  students'  experiences  of  using  mobile-based  CRS  technology  in  the  context  of  an  entrepreneurship  course.  This  study  involves  22  graduate  students  enrolled  in  an  18-week  course  in  Entrepreneurship  Management.  This  study  examines  how  their  learning  could  be  supported  and  enhanced  by  CRS  technology.  Results  indicate  that  mobile-based  CRS  technology  is  a  useful  and  effective  tool  for  facilitating  interaction  among  learners  and  content,  enhancing  students'  engagement  with  entrepreneurial  knowledge  acquisition,  and  improving  students'  motivation  toward  increased  entrepreneurial  capability.  In  particular,  students  experience  innovative,  active,  and  deep  learning  in  a  mobile-based  and  CRS-supported  classroom  regardless  of  time  and  location.
2	Enjoyment  fosters  media  use  frequency  and  determines  its  relational  outcomes.  Previous  work  has  begun  to  incorporate  psychological  constructs  into  our  understanding  of  media  multiplexity  (i.e.,  use  of  multiple  media  in  a  single  interpersonal  relationship).  Following  this  approach,  this  investigation  examined  how  a  user's  enjoyment  of  a  communication  medium  might  both  predict  medium  use  frequency  and  alter  the  extent  to  which  closeness  is  an  outcome  of  such  use.  Results  supported  this  model  across  six  popular  interpersonal  communication  media  (voice  telephone,  text  messaging,  e-mail,  instant  messaging,  video  chat,  and  social  networking  sites)  in  relationships  with  extended  family  members.  Specifically,  at  low  levels  of  medium  enjoyment,  use  of  a  medium  was  not  associated  with  relational  closeness.  Practically,  these  results  suggest  that  communicating  via  an  unsatisfying  medium  may  not  foster  relational  closeness;  theoretically,  these  results  argue  for  including  psychological  processes  to  strengthen  MMT  as  a  theory  of  interpersonal  media  choice.  We  used  conditional  process  analysis  to  study  medium  enjoyment  in  extended  families.Enjoyment  of  a  medium  predicted  medium  use  frequency  in  extended  families.Medium  use  frequency  positively  predicts  relational  closeness.Medium  enjoyment  moderates  the  effect  media  use  has  on  relational  closeness.
2	Full  length  articlecommunication  in  the  face  of  a  school  crisis  examining  the  volume  and  content  of  social  media  mentions  during  active  shooter  incidents.  Little  is  known  about  the  effectiveness  of  social  media  in  delivering  information  during  active  shooter  incidents  at  the  P-12  level.  This  study  analyzed  social  media  activity  that  occurred  during  and  after  two  active  shooter  events  on  September  30,  2014.  Over  5000  social  media  posts  from  Facebook,  Twitter,  blogs,  and  mainstream  news  outlets  were  analyzed.  Social  media  analysis  outlined  the  scope  of  online  communication  during  the  first  week  following  the  incidents,  revealed  social  media  frequency,  increases  in  conversation,  misinformation,  and  differences  between  parent  and  student  posts.  Results  revealed  spikes  in  social  media  chatter  following  the  release  of  the  identities  of  shooters  and  victims.  Consistent  with  media  dependency  theory  and  the  high  levels  of  uncertainty  characteristic  of  the  incident,  users’  social  media  posts  contained  more  information  than  affect  displays  during  the  active  shooter  event.  Implications  for  scholars  and  P-12  administrators  are  discussed.
2	It  s  complicated  a  systematic  review  of  associations  between  social  network  site  use  and  romantic  relationships.  Social  network  site  (SNS)  use  may  have  important  implications  for  romantic  relationships.  This  systematic  literature  review  aims  to  (a)  identify  theory-based  approaches  for  studying  associations  between  SNS  use  and  romantic  relationships,  (b)  identify  key  romantic  relationship  constructs  measured  in  relation  to  SNS  use,  (c)  synthesize  the  mechanisms  by  which  SNS  use  may  influence  and  be  influenced  by  romantic  relationships,  and  (d)  discuss  improved  methods  for  guiding  future  research.  Twenty-six  peer-reviewed  articles  published  in  English  from  2000  to  2015  that  include  measures  of  a  romantic  relationship  construct  as  an  outcome  or  predictor  of  SNS-related  behavior  for  someone  in  a  romantic  relationship  comprise  this  review.  Studies  are  categorized  as  focusing  on  individual  characteristics,  relationship  characteristics,  or  behavioral  actions.  Overall,  findings  indicate  underdevelopment  of  SNS-related  theory,  and  suggest  that  SNS  behaviors  may  both  influence  and  be  influenced  by  individual  and  relationship  characteristics  such  as  adult  attachment  style,  SNS-induced  jealousy,  relationship  satisfaction  and  commitment,  and  partner  identity  overlap.  Gender  appears  to  influence  associations  between  SNS  use  and  relationship  constructs,  particularly  in  relation  to  interpreting  ambiguous  information  about  a  partner.  Further,  SNSs  may  serve  a  maintenance  function  within  romantic  relationships.  Directions  for  future  research  include  assessing  multiple  SNSs  in  diverse  samples  and  standardizing  measurement  of  SNS  behaviors.  Review  synthesizes  existing  SNS  use  and  romantic  relationship  literature.Identifies  theory-based  approaches  for  studying  associations.Shows  the  need  for  standardizing  measurement  in  this  area  of  research.Gender  influences  the  interplay  between  SNS  use  and  romantic  relationships.SNS  use  may  both  benefit  and  harm  romantic  relationships.
2	Violent  video  game  exposure  and  moral  disengagement  in  early  adolescence.  Previous  research  has  suggested  that  violent  video  game  exposure  might  induce  moral  disengagement;  however,  few  studies  have  examined  the  underlying  mechanisms.  Based  on  the  social  cognitive  model  of  morality,  we  examined  the  interaction  between  how  violent  video  game  exposure  and  moral  identity  predict  moral  disengagement.  Participants  comprised  761  adolescents  (53%  boys)  in  the  seventh  grade  from  five  Chinese  junior  middle  schools  (Mage=13.17,  SD=0.65  years).  The  correlation  analysis  revealed  that  violent  video  game  exposure  was  positively  related  to  moral  disengagement  and  negatively  related  to  moral  identity,  and  moral  identity  was  negatively  correlated  with  moral  disengagement.  When  controlling  for  gender  and  age,  violent  video  game  exposure  interacted  with  moral  identity  to  predict  moral  disengagement.  For  adolescents  with  a  higher  moral  identity,  moral  disengagement  decreased  following  less  exposure  to  violent  video  games;  however,  no  significant  decrease  occurred  in  adolescents  with  a  lower  moral  identity.  The  results  revealed  a  moderating  effect  of  moral  identity  between  violent  video  game  exposure  and  moral  disengagement  in  early  adolescence.  This  supported  the  social  cognitive  model  of  morality  that  proposes  that  situational  variables  can  interact  with  moral  identity  to  predict  moral  outcomes  in  early  adolescence.  Based  on  a  social  cognitive  model  of  morality.Participants  comprised  individuals  in  early  adolescence.Violent  video  game  exposure  positively  predicted  moral  disengagement.Moral  identity  moderated  violent  video  game  exposure  predicting  moral  disengagement.Moral  disengagement  decreased  following  less  exposure  to  violent  video  games.
2	Research  reportgender  and  texting  masculinity  femininity  and  gender  role  ideology.  Texting,  although  one  of  the  newer  forms  of  computer-mediated  communication,  has  become  very  popular,  especially  among  teens.  This  research,  using  self-report  measures,  explored  college  students’  perceptions  of  texting,  including  texting’s  leading  to  relationship  conflict  and  interfering  with  classes,  as  well  as  how  attitudes  towards  texting  were  related  to  masculinity,  femininity,  and  gender-linked  (traditional)/gender-transcendent  (nontraditional)  attitudes.  Our  participants  (n  =  183)  more  frequently  used  emoticons  than  abbreviations,  especially  vulgar  abbreviations.  Over  70%  reported  at  least  minimal  texting  interference  with  classes/college  preparation,  and  over  60%  indicated  that  their  own  or  their  significant  other’s  texting  contributed  to  relationship  conflict.  The  only  significant  male–female  difference  was  in  sexually  explicit  messages  received,  but  positive  associations  were  found  between  more  traditional  gendered  attitudes  and  texting  interfering  with  studying/school,  with  relationship  conflict,  and,  for  men  only,  “sexting”  and  using  vulgar  abbreviations.  Other  findings  included  gender  transcendence  being  negatively  associated  with  the  reported  number  of  messages  sent  as  well  as  being  bothered  by  texting;  femininity  also  predicted  frequency  of  emoticon  use.  Our  research  suggests  that  individual  differences  in  texting  may  be  related  to  variables  associated  with  gendered  self-perceptions  and  traditional  gender  roles.
2	The  role  of  social  media  metrics  in  online  news  evaluations  revised  manuscript  submitted  to  computers  in  human  behavior.  This  study  examines  how  social  media  metrics,  as  compared  to  the  credibility  of  news  organizations,  affect  online  news  evaluations.  In  a  2  x  2  between-subjects  experiment  (N  =  202),  participants  read  a  news  story  that  was  reported  either  by  a  high  credibility  or  a  low  credibility  news  organization,  with  either  an  absence  or  presence  of  social  media  metrics.  The  results  indicate  that  (a)  social  media  metrics  reduce  the  effects  of  media  credibility  on  online  news  evaluations;  (b)  the  effects  of  social  media  metrics  on  online  news  evaluations  hold  only  when  the  news  story  is  from  a  low  credibility  news  organization;  and  (c)  the  personal  relevance  of  the  issue  moderates  the  effects  of  media  credibility  and  social  media  metrics.  These  findings  suggest  that  social  media  metrics  may  work  as  an  updated  alternative  to  the  two-step  flow  model  of  communication.  The  findings  also  reemphasize  the  need  for  media  organizations  to  maintain  their  credibility  in  the  generational  shift  of  news  consumption.
2	Mobile  technology  adoption  across  the  lifespan  a  mixed  methods  investigation  to  clarify  adoption  stages  and  the  influence  of  diffusion  attributes.  We  conducted  a  multi-study,  mixed-methods,  longitudinal  investigation  to  examine  how  mobile  technology  diffuses  across  the  lifespan,  in  real  time,  within  a  multi-generational  population,  while  seeking  local  knowledge  through  community-based  participatory  research.  Using  qualitative  methods  (QUAL),  we  examined  technology  adoption  within  and  across  three  iterations  (16  weeks)  of  a  nine-wave  longitudinal  community  technology-training  workshop,  situated  within  a  15-wave  study.  In  parallel,  we  interrogated  existing  conceptualization  and  operationalization  of  diffusion  of  technology  variables,  then  deductively  evaluated  the  dominant  DOI-related  variables  re-conceptualized  through  the  community  study  in  a  large  cross-sectional  quantitative  (QUAN)  investigation.  We  interpreted  our  results  consistently  and  iteratively  with  a  mixed-methods  approach  that  included  conceptualization,  operationalization,  and  empirical  testing.  We  discovered  that  oft-conflated  concepts  of  knowledge,  use,  and  ownership  represent  distinct  stages  of  adoption.  Our  findings  suggest  constant  feedback/permeable  boundaries  between  these  stages,  and  that  DOI  attributes  may  influence  mobile  technology  adoption  stages  differentially.  We  suggest  that  innovators  seeking  to  facilitate  mobile  technology  adoption  should  focus  on  reducing  complexity,  and  establishing  calibration  of  complexity  perceptions.  We  propose  a  lifespan  mobile  technology  diffusion  model,  and  call  to  question  the  language  used  in  investigations  related  to  the  digital  divide.  We  strive  to  clarify  labels  that  may  stereotype  vulnerable  populations,  such  as  older  adults.  Our  research  contributes  to  theories  of  technology  adoption  -  particularly  after  the  introduction  of  digital  communication  -  the  diffusion  of  innovations  in  the  community  over  time,  and  technology  adoption  process  as  affected  by  interpersonal  communication  and  relationships,  including  among  the  technologically  undercapitalized  and  the  digitally  privileged.
2	Sig  blocks.  This  paper  presents  the  SIG-Blocks  system  developed  for  automated  cognitive  assessment  via  tangible  geometric  games  (TAG-Games).  Computerized  game  administration  and  real-time  cognitive  and  behavior  assessments  were  realized  by  wireless  self-synchronization  in  communication,  decentralized  hybrid-sensing,  assembly  and  motion  detection,  and  graphical  visualization.  The  measurable  performance  data  included  time  and  accuracy  at  each  manipulation  step,  overall  speed  of  manipulative  motions,  and  the  total  number  of  rotational  motions.  For  preliminary  evaluation,  three  types  of  TAG-Games  were  designed:  TAG-GameA  for  assembly,  TAG-GameS  for  shape  matching,  and  TAG-GameM  for  memory.  As  a  part  of  the  game  design,  a  computational  measure  of  play  complexity  was  defined  for  each  TAG-Game  based  on  the  geometric  properties  and  the  number  of  blocks  in  the  item.  An  evaluation  with  86  participants  assessed  both  reliability  of  the  TAG-Game  items  using  split-half  and  test-retest  reliability  tests  and  validity  of  the  proposed  complexity  measures  by  comparing  the  results  with  three  subtests  of  the  Wechsler  Adults  Intelligence  Scale  4th  Edition  (WAIS-IV),  i.e.  Block  Design  (BD),  Matrix  Reasoning  (MR),  and  Digit  Span  (DS).  The  high  reliability  coefficients  showed  that  TAG-Games  were  reliable.  Regarding  validity,  correlations  were  found  between  TAG-GameA  and  BD  and  between  TAG-GameS  and  MR.  Behavioral  analysis  also  showed  that  the  TAG-Game  performance  was  positively  correlated  with  the  manipulation  speed,  but  not  correlated  with  the  total  number  of  rotations  applied  to  the  blocks.  A  new  tangible  game  technology,  called  SIG-Blocks,  is  presented.TAG-Games,  using  SIG-Blocks  as  game  control,  are  designed  for  automated  cognitive  assessment.Performance  data  in  TAG-Games  were  correlated  with  selected  standardized  measures.SIG-Blocks  and  TAG-Games  enable  automated,  remote  assessment  of  cognitive  skills.
2	Sexting  profiles  in  the  united  states  and  canada.  Recent  advances  in  mobile  technology  have  allowed  individuals  to  engage  in  sexting  (i.e.,  sharing  sexual  words  and  images  via  technology).  Researchers  have  examined  the  prevalence  and  correlates  of  sexting,  but  differences  in  samples  and  definitions  make  it  difficult  to  develop  a  cohesive  picture  of  adult  sexting.  This  study  extends  our  understanding  of  sexting  behavior  by  using  binationally-representative  data  from  615  Americans  and  Canadians  in  committed  heterosexual  and  same-sex  couple  relationships  from  The  Couple  Well-Being  Project.  Using  latent  profile  analysis,  we  explored  how  individuals'  patterns  of  sending  and  receiving  explicit  word  and/or  picture  text  messages  illustrated  distinct  profiles  of  sexting  behavior.  The  analyses  revealed  4  distinct  groups  of  sexters:  non-sexters  (71.5%),  word-only  sexters  (14.5%),  frequent  sexters  (8.5%),  and  hyper  sexters  (5.5%).  We  then  compared  these  groups  on  various  relationship  factors,  indicators  of  individual  well-being,  and  technology-related  behaviors.  Frequent  and  hyper  sexters  reported  greater  sexual  satisfaction  but  were  not  significantly  different  from  non-sexters  or  word-only  sexters  in  relationship  satisfaction.  Further,  frequent  and  hyper  sexters  scored  more  poorly  on  other  relationship  variables  (i.e.,  attachment  security,  commitment,  ambivalence,  and  conflict)  than  non-sexters  or  word-only  sexters  and  showed  greater  media  and  pornography  viewing,  technoference  in  face-to-face  interactions  with  their  partner,  and  infidelity-related  behaviors  on  social  media.  There  were  4  types  of  sexters:  non-sexters,  word-only,  frequent,  and  hyper  sexters.Non-sexters  were  the  most  prevalent  type  of  sexter  (71.5%).Sexters  scored  higher  in  sexual  satisfaction.Sexters  were  no  different  in  overall  relationship  satisfaction.Frequent  and  hyper  sexters  scored  more  poorly  on  other  relationship  variables.
2	Use  of  multiple  social  media  platforms  and  symptoms  of  depression  and  anxiety.  IntroductionWhile  increased  time  spent  on  social  media  (TSSM)  has  been  associated  with  depression  and  anxiety,  the  independent  role  of  using  multiple  social  media  (SM)  platforms  is  unclear.  MethodsWe  surveyed  a  nationally-representative  sample  of  1787  U.S.  young  adults  ages  1932.  Depression  and  anxiety  symptoms  were  measured  using  the  Patient-Reported  Outcomes  Measurement  Information  System  (PROMIS).  We  assessed  use  of  multiple  SM  platforms  with  an  adapted  Pew  Internet  Research  scale.  We  used  ordered  logistic  regression  models  to  assess  associations  between  use  of  multiple  SM  platforms  and  mental  health  outcomes  while  controlling  for  eight  covariates,  including  overall  TSSM.  ResultsCompared  to  those  who  used  02  social  media  platforms,  participants  who  used  711  social  media  platforms  had  substantially  higher  odds  of  having  increased  levels  of  both  depression  (Adjusted  Odds  Ratio  [AOR]=3.0,  95%  CI=1.94.8)  and  anxiety  symptoms  (AOR=3.2,  95%  CI=2.05.1).  Associations  were  linear  (p<0.001  for  all)  and  robust  to  all  sensitivity  analyses.  ConclusionsUse  of  multiple  SM  platforms  is  independently  associated  with  symptoms  of  depression  and  anxiety,  even  when  controlling  for  overall  TSSM.  These  associations  are  strong  enough  that  it  may  be  valuable  for  clinicians  to  ask  individuals  with  depression  and  anxiety  about  multiple  platform  use  and  to  counsel  regarding  this  potential  contributing  factor.  We  found  a  linear  association  between  the  number  of  platforms  used  and  depression.We  found  a  linear  association  between  the  number  of  platforms  used  and  anxiety.Associations  remained  strong  after  controlling  for  total  time  of  social  media  use.
2	The  impact  of  distractions  on  the  usability  and  intention  to  use  mobile  devices  for  wireless  data  services.  Mobile  technology  has  quickly  become  ingrained  in  society  due  to  the  flexibility  of  anywhere/anytime  usage.  However,  factors  associated  with  and  that  impact  mobility,  mobile  users,  and  mobile  use  of  products  and  services  are  still  poorly  understood.  For  example,  even  though  distractions  are  ever  present  during  everyday  use  of  mobile  devices,  the  nature  and  extent  to  which  user  perceptions  and  performance  are  affected  by  their  presence  is  unknown.  An  empirical  study  was  undertaken  to  investigate  the  impact  of  distractions  and  confirmation  of  pre-trial  expectations  on  usability  and  its  subsequent  effect  on  consumers'  behavioral  intention  toward  using  a  mobile  device  for  wireless  data  services.  Distractions  were  simulated  in  this  study  in  the  form  of  either  user  motion  or  environmental  noise  (i.e.  background  auditory  and  visual  stimuli).  A  Structural  Equation  Modelling  (SEM)  analysis  confirmed  the  impacts  of  distractions  on  efficiency  and  effectiveness,  and  in  turn  the  users'  satisfaction  and  behavioral  intention  to  use  a  mobile  device  for  wireless  data  services.  Support  was  also  obtained  for  a  mediating  effect  of  post-trial  confirmation  of  expectations  between  perceived  performance  and  satisfaction.  Implications  of  these  findings  for  theory,  practice,  and  future  research  are  outlined.
2	Reinforcing  inspiration  for  technology  acceptance  improving  memory  and  software  training  results  through  neuro  physiological  performance.  This  paper  investigates  the  phenomenon  of  reinforcing  inspiration  for  technology  acceptance  by  improving  memory  and  software  training  results  Neuro-physiological  performance.  Monitoring  of  cortisol  levels  provided  feedback  for  a  decision  support  system  that  measured  errors  and  elapsed  time  for  training  tasks  completed  by  end-users  of  a  health  care  application.  The  training  success  was  measured  utilizing  statistics,  SEM  and  a  Fuzzy  approach.  The  predictive  model  was  implemented  by  comparing  the  regression,  fuzzy  logic  and  SEM  results.  Data  collected  from  338  health  care  workers  were  used  to  test  a  proposed  model  that  inspiration,  memory,  and  inspirational  memory  affect  end  user  intention  to  adopt  a  digitized  patient  record  software  application.  Structural  equation  modeling  showed  that,  as  expected,  inspiration  affected  the  individual  behavior  of  the  end  users.  Inspiration  had  an  interactive  impact  through  memory  on  collective  acceptance  of  the  technology,  thereby  affecting  subsequent  evaluations  and  behavior.  The  proposed  model  was  nomologically  validated  through  the  use  of  a  portable  platform  loaded  with  software  for  the  electronic  collection  of  operational-level  health  care  data.  Embedded  metrics  measured  participants'  memory  as  operationalized  by  task  completion  time,  number  of  errors,  and  completeness  of  the  data.  In  order  to  triangulate  the  results,  salivary  cortisol  levels  collected  from  74  health  care  workers  were  used  to  measure  whether  inspiration  improves  memory  and  affects  end  user  intention  to  adopt  the  application  through  reduced  errors  and  decreased  completion  times.  This  paper  contributes  to  the  literature  by  introducing  inspiration  as  a  key  driver  that  improves  memory  to  affect  end  user  intention  to  use  digitized  patient  record  technology.
2	Comparative  study  of  soft  computing  techniques  for  mobile  robot  navigation  in  an  unknown  environment.  Robot  navigation  and  obstacle  avoidance  using  fuzzy  logic  controller  is  presented.Soft  computing  techniques  are  used  to  optimize  the  performance  of  fuzzy  logic.The  automatic  tuning  was  done  by  using  three  soft  computing  techniques:  GA,  PSO,  and  NN.The  best  performance  in  terms  of  travelling  time  and  speed  is  based  on  GA-Fuzzy.The  PSO-Fuzzy  and  Neuro-Fuzzy  methods  have  better  performance  in  terms  of  distance  travelled.  An  autonomous  mobile  robot  operating  in  an  unstructured  environment  must  be  able  to  deal  with  dynamic  changes  of  the  environment.  Navigation  and  control  of  a  mobile  robot  in  an  unstructured  environment  are  one  of  the  most  challenging  problems.  Fuzzy  logic  control  is  a  useful  tool  in  the  field  of  navigation  of  mobile  robot.  In  this  research,  fuzzy  logic  controller  is  optimized  by  integrating  fuzzy  logic  with  other  soft  computing  techniques  like  genetic  algorithm,  neural  networks,  and  Particle  Swarm  Optimization  (PSO).  Soft  computing  techniques  are  used  in  this  work  to  tune  the  membership  function  parameters  of  fuzzy  logic  controller  to  improve  the  navigation  performance.  Four  methods  have  been  designed  and  implemented:  manually  constructed  fuzzy  logic  (M-Fuzzy),  fuzzy  logic  with  genetic  algorithm  (GA-Fuzzy),  fuzzy  logic  with  neural  network  (Neuro-Fuzzy),  and  fuzzy  logic  with  PSO  (PSO-Fuzzy).  The  performances  of  these  approaches  are  compared  through  computer  simulations  and  experiment  number  of  scenarios  using  Khepera  III  mobile  robot  platform.  Hybrid  fuzzy  logic  controls  with  soft  computing  techniques  are  found  to  be  most  efficient  for  mobile  robot  navigation.  The  GA-Fuzzy  technique  is  found  to  perform  better  than  the  other  techniques  in  most  of  the  test  scenarios  in  terms  of  travelling  time  and  average  speed.  The  performances  of  both  PSO-Fuzzy  and  Neuro-Fuzzy  are  found  to  be  better  than  the  other  methods  in  terms  of  distance  travelled.  In  terms  of  bending  energy,  the  PSO-Fuzzy  and  Neuro-Fuzzy  are  found  to  be  better  in  simulation  results.  Although,  the  M-Fuzzy  is  found  to  be  better  using  real  experimental  results.  Hence,  the  most  important  system  parameter  will  dictate  which  of  the  four  methods  to  use.
2	Emotion  understanding  and  performance  during  computer  supported  collaboration.  Individuals  collaborating  around  and  through  computers  benefit  from  receiving  information  that  helps  them  understand  one  another,  which  is  often  termed  awareness.  This  article  explores  what  collaborators  understand  about  each  other's  emotions  and  the  potential  benefits  for  performance  that  might  come  from  raising  this  understanding.  In  Experiment  1  co-located  collaborators  judged  each  other's  emotions  after  playing  a  game  that  required  cooperative  data  collection  and  analysis.  Their  judgements  were  largely  inaccurate  and  based  on  their  own  emotions,  suggesting  limited  emotion  understanding.  Experiment  2  explored  if  this  could  be  overcome  by  making  collaborators  aware  of  each  other's  emotions.  Co-located  and  remote  collaborators  played  a  cooperative  puzzle-solving  game  under  conditions  of  awareness  or  no  awareness.  Awareness  was  manipulated  by  making  collaborators  share  their  self-reported  emotions  during  key  moments  of  their  game  play.  Both  remote  and  co-located  collaborators  improved  their  performance  after  sharing  their  emotions.  However,  unlike  co-located  collaborators,  remote  collaborators  also  improved  their  understanding  of  each  other's  emotions  and  experienced  more  positive  affect.  We  conclude  by  discussing  the  content  of  collaborators'  emotion  understanding  and  the  probable  mechanisms  underlying  the  observed  effects  of  being  made  aware  of  a  partner's  emotions.
2	Cognitive  demand  digital  screens  and  blink  rate.  Varying  cognitive  demand  produced  a  significant  reduction  in  blink  rate.Switching  to  digital  displays  from  print  does  not  change  blink  rate.Contemporary  screens  may  be  closer  to  print  than  older  displays.Dry  eye  symptoms  may  be  related  to  the  number  of  incomplete  blinks.  PurposeMany  subjects  experience  ocular  and  visual  symptoms  when  viewing  digital  electronic  screens.  Previous  studies  have  reported  a  reduced  blink  rate  during  computer  operation  and  suggested  that  this  may  account  for  some  of  the  symptoms  experienced  during  such  tasks.  However,  it  is  unclear  whether  these  changes  in  blink  rate  are  related  to  the  screen  display  or  to  differences  in  the  mental  requirements  of  the  task.  Accordingly,  the  present  study  compared  blink  rates  when  reading  material  having  low  or  high  cognitive  demand  from  a  tablet  computer  or  hard  copy  printed  text.  MethodsSubjects  (N=16)  were  required  to  perform  a  continuous  10min  reading  task  either  from  a  tablet  computer  or  a  printed  hard  copy  page  at  a  viewing  distance  of  30cm.  Two  sets  of  text,  which  varied  in  their  level  of  cognitive  demand,  were  used.  Target  size,  contrast  and  viewing  angle  were  similar  for  all  conditions.  Subjects  were  video-recorded  during  the  task  to  determine  their  blink  rate.  ResultsVarying  cognitive  demand  resulted  in  a  significant  reduction  in  blink  rate.  While  the  method  of  presentation  (tablet  versus  print)  did  not  produce  a  significant  change  in  blink  rate,  the  interaction  of  cognition  with  the  method  of  presentation  was  statistically  significant.  ConclusionsThese  results  indicate  that  a  change  in  the  cognitive  demand  of  the  task  has  a  larger  effect  on  mean  blink  rate  than  varying  the  method  of  presentation.  Contemporary  screens  may  be  closer  in  format  to  printed  materials  than  older  displays.  Therefore,  it  seems  unlikely  that  the  ocular  and  visual  symptoms  commonly  experienced  when  viewing  digital  screens  are  produced  by  a  reduced  blink  rate.
2	Understanding  the  continued  use  of  intra  organizational  blogs.  The  continued  use  of  internal  blogs  is  driven  by  habituation.The  use  is  also  adapted  by  network  externalities  and  managerial  incentives.Actual  usage  data  are  collected  from  an  in-practice  system  to  test  the  model.The  work  provides  a  behavior  pattern  perspective  for  studying  continued  use.The  findings  help  understand  and  leverage  the  benefits  of  internal  social  media.  As  intra-organizational  blogs  are  expected  to  provide  a  flexible,  intra-organizational  networking  platform  that  can  effectively  facilitate  knowledge  sharing,  it  is  worthwhile  to  address  why  employees  embrace  a  blogging  system  and  engage  in  continual  blogging.  Drawing  upon  the  existing  literature,  this  paper  proposes  a  conceptual  model  that  suggests  the  continued  use  of  internal  blogs  by  employees  is  strongly  driven  by  the  force  of  habituation,  which  is  repeatedly  adapted  by  the  effects  of  network  externalities  and  managerial  incentives.  To  empirically  test  this  "adaptive  habituation  model",  actual  usage  data  are  collected  from  the  internal  blogging  platform  of  a  large  telecommunication  company  to  measure  all  related  constructs.  Statistical  results  from  regression  analyses  illustrate  that  the  proposed  model  effectively  explains  why  employees  continue  to  use  corporate  internal  blogging  systems.  Practically,  our  findings  suggest  that  while  managerial  measures,  such  as  a  ranking  mechanism,  may  help  leverage  the  benefits  of  intra-organizational  social  networking  applications,  it  may  take  some  time  for  these  effects  to  emerge.
2	Why  do  we  multitask  with  media  predictors  of  media  multitasking  among  internet  users  in  the  united  states  and  taiwan.  Survey  conducted  in  America  and  Taiwan  explored  antecedents  of  media  multitasking.Media  ownership,  polychronicity,  motivations  positively  predicted  multitasking.Four  media  multitasking  motivations  mediated  the  effect  of  media  ownership.Americans  scored  higher  on  polychronicity  and  media  multitasking  than  Taiwanese.The  effects  of  media  ownership,  polychronicity,  and  motivations  varied  by  country.  The  study  explored  how  media  and  audience  factors,  such  as  country  of  residence,  media  ownership,  polychronicity,  or  the  preference  to  do  multiple  things  at  the  same  time,  predict  media  multitasking  behaviors  and  if  different  motivations  to  multitask  mediate  the  effects  of  these  factors.  The  study  is  based  on  a  cross-cultural  survey  (N=1972)  that  included  respondents  from  the  United  States  and  Taiwan.  The  findings  indicated  that  media  ownership,  polychronicity,  and  four  motivations  (control,  entertainment,  connection,  and  addiction)  positively  predicted  media  multitasking  behaviors.  The  four  motivations  were  also  found  to  mediate  the  effect  of  media  ownership.  American  respondents  were  higher  polychronics  and  heavier  multitaskers  than  their  Taiwanese  counterparts.  In  the  Taiwanese  sample,  polychronicity  and  motivations  increased  the  effects  of  media  ownership  on  media  multitasking.  In  the  American  sample,  polychronicity  contributed  little  to  the  effect  of  media  ownership,  and  the  mediating  role  of  motivations  decreased  with  the  increase  in  the  level  of  polychronicity.
2	The  effects  of  social  media  on  students  behaviors  facebook  as  a  case  study.  Social  networks  are  one  of  the  most  used  communication  methods  of  today's  world.  Their  use  in  different  fields  has  been  examined  in  several  research  studies.  This  study  aims  to  examine  the  effects  of  social  media  on  student's  behaviors  which  will  mainly  focus  on  Facebook.  Whether  there  is  a  positive  relationship  between  confidence,  social  media  participation  and  social  media  related  behaviors  will  also  be  assed  with  regard  to  using  Facebook.  In  order  to  collect  the  primary  data,  a  general  scanning  model  was  used  to  observe  attitudes  of  high  school  students.  The  participants  chosen  were  362  high  school  students  from  level  9  to  12.  The  findings  highlight  that  Facebook  is  used  for  communication  entertainment  and  sharing  news,  pictures  and  songs.  In  addition,  their  Facebook  profile  picture  is  alone  and  students  were  aware  that  swearing  is  considered  a  form  of  misconduct,  which  is  a  good  sign.  The  study  also  indicates  that  students  were  aware  of  protecting  their  social  identity  as  their  Facebook  shares  are  not  public.  Furthermore,  they  respect  privacy  as  they  do  not  use  their  friend's  Facebook  account.  Students  reflect  their  mood  on  social  media  which  creates  a  chance  for  consultancy.Nice  Facebook  comments  increase  students'  confidence.Students  know  how  to  control  their  privacy.Students'  Facebook  use  shows  indications  of  narcissism.
2	Motivation  determines  facebook  viewing  strategy.  Individuals'  Social  Networking  Site  (SNS)  profiles  are  central  to  online  impression  formation.  Distinct  profile  elements  (e.g.,  Profile  Picture)  experimentally  manipulated  in  isolation  can  alter  perception  of  profile  owners,  but  it  is  not  known  which  elements  are  focused  on  and  attributed  most  importance  when  profiles  are  viewed  naturally.  The  current  study  recorded  the  eye  movement  behaviour  of  70  participants  who  viewed  experimenter-generated  Facebook  timelines  of  male  and  female  targets  carefully  controlled  for  content.  Participants  were  instructed  to  process  the  targets  either  as  potential  friends  or  as  potential  employees.  Target  timelines  were  delineated  into  Regions  of  Interest  (RoIs)  prior  to  data  collection.  We  found  pronounced  effects  of  target  gender,  viewer  motivation  and  interactions  between  these  factors  on  processing.  Global  processing  patterns  differed  based  on  whether  a  'social'  or  a  'professional'  viewing  motivation  was  used.  Both  patterns  were  distinct  to  the  'F'-shaped  patterns  observed  in  previous  research.  When  viewing  potential  employees  viewers  focused  on  the  text  content  of  timelines  and  when  viewing  potential  friends  image  content  was  more  important.  Viewing  patterns  provide  insight  into  the  characteristics  and  abilities  of  targets  most  valued  by  viewers  with  distinct  motivations.  These  results  can  inform  future  research,  and  allow  new  perspectives  on  previous  findings.  Viewer  motivation  and  profile  gender  influences  processing  of  social  network  sites.When  viewing  potential  employees,  text  content  is  focused  more  than  image  content.When  viewing  potential  friends,  image  content  is  focused  more  than  text  content.Female  profiles  viewed  as  employees  were  evaluated  on  physical  attractiveness.Male  profiles  viewed  as  employees  were  evaluated  on  information  content.
2	Excessive  use  of  mobile  social  networking  sites.  While  pervasive  technologies,  such  as  mobile  social  networking  sites  (SNS),  can  contribute  to  increased  enjoyment  and  convenience,  their  pervasive  nature  can  also  result  in  excessive  use  and  consequently  may  arise  several  negative  outcomes.  We  applied  cognitive  behavioral  model  and  social  cognitive  theory  to  explain  the  negative  consequences  related  to  family,  personal  and  professional  life  of  excessive  use  of  mobile  SNS.  The  research  model  was  empirically  tested  with  490  mobile  SNS  users.  Our  findings  significantly  contribute  to  the  domain  of  dark  side  of  information  technology  by  theoretically  and  empirically  investigating  the  negative  outcomes  of  excessive  use,  and  further  examining  their  inter-relationships.  We  examined  the  negative  outcomes  of  excessive  use  of  mobile  social  networking  sites.Three  types  of  conflicts  (viz.  technology-family,  technology-personal,  and  technology-work)  were  examined.Excessive  use  enhanced  cognitive  preoccupation.Excessive  use  and  cognitive  preoccupation  were  associated  with  the  three  types  of  conflicts.The  relationships  and  the  roles  of  these  three  conflicts  on  technostress  were  also  examined.
2	Understanding  families  motivations  for  sustainable  behaviors.  We  analyzed  eco-friendly  motivations  for  sustainable  behaviors  in  the  context  of  families.Motivations  varied  according  to  family  type  and  their  emotional  needs.Routines,  children  age  and  parental  responsibilities  determined  family  motivations.Motivations  reflected  their  emotional  needs,  sense  of  security  and  identity.  While  interest  in  eco-feedback  technologies  has  peaked  over  the  last  decade,  research  increasingly  highlights  that  simply  providing  information  to  individuals  regarding  their  consumption  behaviors  does  not  guarantee  behavior  change.  This  has  lead  to  an  increasing  body  of  work  that  attempts  to  characterize  individuals'  latent  motivations  that  drive  sustainable  behaviors.  With  this  paper  we  aim  at  expanding  this  body  of  work  by  analyzing  such  motivations  in  the  context  of  families.  We  report  findings  from  interviews  with  15  families  who  used  an  eco-feedback  interface  over  a  period  of  2years.  Our  study  reveals  that  motivations  for  sustainable  behavior  were  not  only  rooted  in  individuals'  environmental  concerns  and  need  for  expense  management  but  they  also  regarded:  (i)  individuals'  and  families'  need  for  a  sense  of  control  and  security,  (ii)  parents'  self-perceived  responsibility  of  their  role  as  parents  and  (iii)  the  perception  of  individual  as  well  as  family  identity.  We  argue  that  in  order  for  eco-feedback  technologies  to  attain  long-lasting  behavioral  changes  in  the  domestic  environment  they  need  to  address  basic  family  needs  that  go  beyond  individual  ideals  of  pro-environmental  behavior.
2	User  sentiment  analysis  based  on  social  network  information  and  its  application  in  consumer  reconstruction  intention.  Abstract  Due  to  the  increasingly  fierce  competition  in  the  consumer  goods  market,  customer  retention  strategies  are  of  great  importance  for  enterprises  to  maintain  their  dominance  and  long-term  and  stable  earnings.  Understanding  customer  repurchase  intention  and  re-patronage  is  the  prerequisite  and  foundation  for  business  and  retailers.  Online  reviews  include  ratings  and  emotional  information  from  many  customers  on  brands  and  online  stores.  Consumer  repurchase  intention  can  be  measured  by  mining  the  attitudes  and  emotions  of  consumers  in  the  reviews.  Based  on  online  reviews,  this  work  used  textual  sentiment  calculation  and  fuzzy  mathematics  to  study  the  online  repurchase  intention  of  online  consumers.  Taking  satisfaction,  trust  and  promotion  efforts  as  the  antecedents,  and  consumer  repurchase  intention  as  the  consequent,  a  model  was  established  based  on  emotional  computing  and  fuzzy  reasoning.  Through  the  satisfaction,  trust  and  promotion  effort  of  five  sportswear  brands  in  Taobao,  we  verified  the  reasoning  for  determining  consumer  repurchase  intention  of  products.  Meanwhile,  the  relationship  between  the  initial  purchase  intention  and  repurchase  intention  of  consumers  was  compared,  thus  providing  the  basis  for  online  stores  to  formulate  their  marketing  strategy  and  brand  segmentation.
2	Hashtags  motivational  drivers  their  use  and  differences  between  influencers  and  followers.  Abstract  Hashtags  have  become  a  ubiquitous  and  seminal  feature  of  social  media;  however,  a  comprehensive  understanding  of  what  motivates  and  predicts  their  use  is  yet  to  be  addressed.  To  fill  this  gap,  this  research  investigates  motives  of  hashtag  use  and  their  effect  on  behavioral  outcomes  based  on  the  Uses  and  Gratifications  (U&G)  approach.  Through  a  two-phase  mixed  method  data  collection,  we  distill  six  motives  of  hashtag  use  in  the  context  of  Instagram:  Self-presentation,  Chronicling,  Inventiveness,  Information  Seeking,  Venting,  and  Etiquette.  We  find  drivers  for  platform  use  to  affect  these  motives,  which,  in  turn,  influence  the  frequency  of  clicking  and  adding  hashtags,  and  the  number  of  hashtags  a  user  may  employ  in  a  post.  Furthermore,  we  find  potential  influencers  to  be  heavy  users  of  hashtags,  primarily  driven  by  motives  of  self-presentation,  and  to  score  higher  on  narcissism,  extraversion  and  self-monitoring  than  followers.  We  further  assert  the  need  for  U&G  studies  to  explicitly  acknowledge  the  nature  of  social  media  that  allows  users  to  both  consume  and  produce  content.  The  findings  hold  important  implications  for  social  media  managers  and  designers.
2	Sens  network  analytics  to  combine  social  and  cognitive  perspectives  of  collaborative  learning.  Abstract  In  this  paper,  we  propose  a  novel  approach  to  the  analysis  of  collaborative  learning.  The  approach  posits  that  different  dimensions  of  collaborative  learning  emerging  from  social  ties  and  content  analysis  of  discourse  can  be  modeled  as  networks.  As  such,  the  combination  of  social  network  analysis  (SNA)  and  epistemic  network  analysis  (ENA)  analysis  can  detect  information  about  a  learner's  enactment  of  what  the  literature  on  collaborative  learning  has  described  as  a  role:  an  ensemble  of  cognitive  and  social  dimensions  that  is  marked  by  interacting  with  the  appropriate  people  about  appropriate  content.  The  proposed  approach  is  named  social  epistemic  network  signature  (SENS)  and  is  defined  as  a  combination  of  these  two  complementary  network  analytic  techniques.  The  proposed  SENS  approach  is  examined  on  data  produced  in  collaborative  activities  performed  in  a  massive  open  online  course  (MOOC)  delivered  via  a  major  MOOC  platform.  The  results  of  a  study  conducted  on  a  data  set  collected  in  a  MOOC  suggest  SNA  and  ENA  produce  complementary  results  which  can  i)  explain  collaboration  processes  that  shaped  the  creation  of  social  ties  and  that  were  associated  with  different  network  roles;  ii)  describe  differences  between  low  and  high  performing  groups  of  learners;  and  iii)  show  how  combined  properties  derived  from  SNA  and  ENA  predict  academic  performance.
2	The  development  of  students  computational  thinking  practices  in  elementary  and  middle  school  classes  using  the  learning  game  zoombinis.  Abstract  This  paper  reports  on  a  research  study  of  45  classes  in  US  schools  (grades  3–8)  using  Zoombinis,  a  popular  Computational  Thinking  (CT)  learning  game  for  ages  8  to  adult.  The  study  examined  the  relationship  among  student  gameplay,  related  classroom  activity,  and  the  development  of  students’  CT  practices  in  Zoombinis  classes.  A  combination  of  research  methods,  including  educational  data-mining  on  game  data  logs,  cluster  analysis  on  teacher  logs  of  classroom  activity,  and  multilevel  modeling,  was  used  to  determine  the  impact  of  the  duration  and  nature  of  student  gameplay,  as  well  as  the  extent  and  nature  of  classroom  activity,  on  student  CT  practices.  Automated  detectors  of  gameplay  CT  practices  built  for  this  research  were  significant  predictors  of  external  post-assessment  scores,  and  thus  show  promise  as  implicit  assessments  of  CT  practices  within  gameplay.  Students  with  high  duration  of  gameplay  and  high  gameplay  CT  practices  scored  highest  on  external  post-assessment  of  CT  practices,  when  accounting  for  pre-assessment  scores.  This  research  suggests  that  Zoombinis  is  an  effective  CT  learning  tool  and  CT  assessment  tool  for  elementary-  and  middle-school  students.
2	The  impact  of  learner  metacognition  and  goal  orientation  on  problem  solving  in  a  serious  game  environment.  Abstract  To  understand  the  impact  of  learner  metacognition  and  goal  orientation  on  problem-solving,  this  study  investigated  159  undergraduate  students’  metacognition,  goal  orientations,  and  their  problem-solving  performances  and  processes  in  a  laboratory  setting  using  a  Serious  Game  (SG)  environment  that  adopts  problem-based  learning  (PBL)  pedagogy  to  teach  space  science.  Utilizing  cluster  analysis,  multiple  regression,  similarity  measure  and  data  visualization,  this  study  analyzed  multiple  data  sources,  including  computer  log  data,  problem-solving  performance  scores,  and  survey  data.  The  results  show  that  both  learner  metacognition  and  goal  orientation  affected  problem-solving.  The  findings  of  this  study  offer  insights  of  how  learner  characteristics  impact  on  problem-solving  in  SG  environments  with  PBL  pedagogy.  It  also  contributes  to  understanding  the  design  of  SG  environments  to  benefit  learners  based  on  their  metacognitive  levels.
2	Effects  of  gain  versus  loss  framed  performance  feedback  on  the  use  of  fitness  apps.  Grounded  in  prospect  theory  and  self-efficacy  theory,  this  study  examines  the  effect  of  message  framing  on  users'  intentions  to  adopt  fitness  applications  (apps).  Through  the  use  of  a  laboratory  experiment  employing  a  specially  designed  fitness  app,  we  tested  the  effectiveness  of  gain-framed  performance  feedback  in  the  adoption  of  the  fitness  app  as  well  as  in  enhancing  exercise  self-efficacy  and  outcome  expectations  of  exercise.  Results  of  this  study  show  the  advantage  of  gain-framed  messages  over  loss-framed  messages  in  increasing  user's  intentions  to  use  the  app.  A  mediation  analysis  using  a  bootstrap  method  revealed  that  the  effect  of  the  gain-framed  messages  on  users'  intentions  to  use  the  fitness  app  was  mediated  through  exercise  self-efficacy  and  outcome  expectations  of  exercise.  We  examined  the  effect  of  message  framing  on  intentions  to  use  a  fitness  app  (IUFA).The  gain-framed  message  had  a  positive  effect  on  exercise  self-efficacy  (ESE).Exercise  self-efficacy  made  a  significant  positive  impact  on  outcome  expectations  of  exercise.Outcome  expectations  of  exercise  (OEE)  had  a  strong  effect  on  intentions  to  use  a  fitness  app.The  effect  of  gain-framed  messages  on  IUFA  was  fully  mediated  by  ESE  and  OEE.
2	A  note  of  caution  regarding  anthropomorphism  in  hci  agents.  Universal  usability  is  an  important  component  of  HCI,  particularly  as  companies  promote  their  products  in  increasingly  global  markets  to  users  with  diverse  cultural  backgrounds.  Successful  anthropomorphic  agents  must  have  appropriate  computer  etiquette  and  nonverbal  communication  patterns.  Because  there  are  differences  in  etiquette,  tone,  formality,  and  colloquialisms  across  different  user  populations,  it  is  unlikely  that  a  generic  anthropomorphic  agent  would  be  universally  appealing.  Additionally,  because  anthropomorphic  characters  are  depicted  as  capable  of  human  reasoning  and  possessing  human  motivations,  users  may  ascribe  undue  trust  in  these  agents.  Trust  is  a  complex  construct  that  exerts  an  important  role  in  a  user's  interactions  with  an  interface  or  system.  Feelings  and  perceptions  about  an  anthropomorphic  agent  may  impact  the  construction  of  a  mental  model  about  a  system,  which  may  lead  to  inappropriate  calibrations  of  automation  trust  that  is  based  on  an  emotional  connection  with  the  anthropomorphic  agent  rather  than  on  actual  system  performance.
2	Computer  mediated  communication  and  risk  taking  behaviour.  In  an  unregulated  environment  Internet  use  is  not  without  risk,  and  video  has  been  proposed  to  influence  riskiness  and  trust  behaviour.  This  experiment  exploredthe  differences  in  willingness  to  take  risks  on  events  portrayed  over  the  Internet  via  a  videolink,  relative  to  events  occurring  in  close  proximity  (collocated).  Thirty-four  participants  played  a  roulette  game  on  a  computer,  wagering  points  upon  the  outcomes  of  spins  of  a  real  roulette  wheel.  The  amounts,  types  of  bets  and  the  time  to  place  them  were  analysed.  It  was  found  that  confidence  (points  wagered)  did  not  change,  but  people  went  for  lower  risk  (when  more  was  at  stake),  or  thought  more  about  the  risks  they  took  (when  more  was  at  stake)  over  a  videolink.  People  accepted  greater  risk  on  outcomes  occurring  in  close  proximity,  than  those  events  portrayed  over  a  videolink.  Variations  in  perceived  risk  in  response  to  online  versus  offline  events  probably  reflect  differences  in  the  potential  to  influence  outcomes.
2	Getting  to  know  you  face  to  face  versus  online  interactions.  It  is  an  open  question  as  to  how  impressions  formed  via  computer-mediated  communication  (CMC)  differ  from  those  formed  face-to-face  (FtF).  Some  research  suggests  that  judgments  of  others  formed  while  interacting  over  CMC  are  more  favorable  than  judgments  formed  in  FtF,  while  other  researchers  argue  the  pattern  is  in  the  opposite  direction.  We  sought  to  settle  this  conflict  by  examining  impressions  formed  via  each  communication  mode  while  controlling  for  the  other.  Participants  interacted  with  a  partner  twice:  once  FtF  and  once  CMC.  When  controlling  for  each  communication  mode,  participants  interacting  FtF,  formed  more  positive  impressions  of  their  partner  than  did  those  in  the  other  sequence.  Furthermore,  FtF  participants  had  greater  self-other  agreement  then  those  who  interacted  via  CMC.  Implications  for  impressions  formed  over  the  Internet  are  discussed.
2	The  effects  of  social  media  based  brand  communities  on  brand  community  markers  value  creation  practices  brand  trust  and  brand  loyalty.  Social  media  based  brand  communities  are  communities  initiated  on  the  platform  of  social  media.  In  this  article,  we  explore  whether  brand  communities  based  on  social  media  (a  special  type  of  online  brand  communities)  have  positive  effects  on  the  main  community  elements  and  value  creation  practices  in  the  communities  as  well  as  on  brand  trust  and  brand  loyalty.  A  survey  based  empirical  study  with  441  respondents  was  conducted.  The  results  of  structural  equation  modeling  show  that  brand  communities  established  on  social  media  have  positive  effects  on  community  markers  (i.e.,  shared  consciousness,  shared  rituals  and  traditions,  and  obligations  to  society),  which  have  positive  effects  on  value  creation  practices  (i.e.,  social  networking,  community  engagement,  impressions  management,  and  brand  use).  Such  communities  could  enhance  brand  loyalty  through  brand  use  and  impression  management  practices.  We  show  that  brand  trust  has  a  full  mediating  role  in  converting  value  creation  practices  into  brand  loyalty.  Implications  for  practice  and  future  research  opportunities  are  discussed.
2	Objective  facebook  behaviour.  The  aim  of  the  study  was  to  test  whether,  and  how  much,  specific  objective  Facebook  behaviours  are  more  frequent  in  problematic  than  non-problematic  Facebook  users.  Differences  between  problematic  and  non-problematic  Facebook  users  in  objective  Facebook  behaviours  were  examined  using  frequentist  and  Bayesian  t-tests.  Participants  were  undergraduate  students  (n=297,  80.8%  female,  age  mean=21.05,  standard  deviation=1.88).  Problematic  Facebook  use  was  assessed  using  fifteen  items  adapted  from  the  scale  developed  and  validated  for  the  measurement  of  Generalized  Problematic  Internet  Use.  A  specific  R  package  was  developed  to  obtain  information  about  objective  Facebook  behaviours  (friendship  activities,  events,  wall  activities,  and  text  messages).  T-tests  indicated  that  non-problematic  and  problematic  users  significantly  differ  in  several  objective  Facebook  behaviours.  Bayesian  analyses  confirmed  t-test  results  and  supported  that  Problematic  users  scored  higher  than  non-problematic  users  in  several  dependent  variables,  such  as  number  of  friendships  established,  number  of  events  attended,  all  wall  activities  (e.g.  number  of  like),  and  private  messages  sent.  The  analysis  of  data  about  objective  Facebook  behaviours  goes  beyond  the  self-reported  information  about  such  activities,  and  helps  to  understand  the  role  of  its  potentially  addictive  activities  in  predicting  PFU.  Objective  Facebook  behaviours  were  used  overcoming  self-report  measures.The  relation  between  objective  data  and  problematic  Facebook  use  was  showed.Problematic  Facebook  users  scored  higher  than  non-problematic  in  several  activities.All  wall  activities  and  private  messages  were  more  frequent  in  problematic  users.Objective  Facebook  behaviours  should  be  considered  in  tackling  PFU.
2	Research  report  effects  of  texting  on  satisfaction  in  romantic  relationships  the  role  of  attachment.  This  study  sought  to  overcome  some  of  the  methodological  limitations  in  previous  studies  and  clarify  the  role  of  attachment  in  the  associations  between  texting  and  romantic  relationship  satisfaction.  Specifically,  a  new,  relative  measure  of  texting  usage  was  created  to  estimate  the  share  of  texting  in  communication  compared  to  other  channels  (e.g.,  face-to-face,  phone,  and  etc.),  in  addition  to  using  the  traditional,  absolute  measure  of  texting  usage  (i.e.,  the  number  of  texts  sent  to  partners).  A  sample  of  395  participants  completed  an  online  survey  regarding  texting  behavior.  Background  variables  were  controlled  for  in  all  analyses,  particularly  the  physical  distance  between  partners,  which  was  often  overlooked  in  previous  research.  The  results  suggested  that  texting  share  showed  positive  links  with  both  attachment  dimensions  and  a  negative  link  with  relationship  satisfaction,  whereas  the  sheer  volume  of  texts  had  little  association  with  attachment  dimensions  or  satisfaction.
2	Full  length  articleadolescents  experience  of  offline  and  online  risks  separate  and  joint  propensities.  Adolescence  is  a  period  of  increased  risk  experience  and  ever  more  often  these  occur  online.  The  current  study  aims  to  investigate  whether  adolescents'  online  and  offline  risk  experiences  are  driven  by  the  same  general  propensity  to  risks.  Data  from  a  representative  study  of  N  =  19,406  (50%  girls)  internet-using  11–16  year  olds  (M  =  13.54,  SD  =  1.68)  youth  in  Europe  were  subjected  to  the  current  analyses.  Three  confirmatory  factor  analyses  were  applied  to  measures  of  offline  and  online  risk  experiences  (five  each).  A  bi-factor  model  of  a  general  risk  factor  and  two  specific  factors  of  online  and  offline  risks  was  shown  to  provide  the  best  theoretical  and  empirical  fit.  All  risk  experiences  loaded  significantly  on  the  general  risk  factor  while  additionally  all  offline  risks  loaded  significantly  on  the  offline  risk  factor.  However,  none  of  the  online  risks  loaded  significantly  on  the  online  risk  factor.  Online  risks  could  not  be  explained  by  factors  that  go  beyond  a  general  propensity  to  experience  risks  suggesting  that  new  technologies  do  not  bring  with  them  a  new  type  of  risk  propensity  driven  by  that  environment.  Interventions  should  target  risk  and  protective  factors  that  can  account  for  adolescents’  experiences  across  risk  types  (online  and  offline).
2	Does  personal  social  media  usage  affect  efficiency  and  well  being.  Social  media  usage  is  proposed  to  be  associated  with  negative  effects.A  two-part  exploratory  model  of  social  media  usage  is  proposed.Social  media  usage  negatively  affects  task  performance.Social  media  usage  negatively  affects  happiness.Future  research  on  negative  effects  should  be  explored.  Personal  social  media  usage  is  pervasive  in  both  personal  and  professional  lives.  Practitioner  articles  and  news  stories  have  commented  on  the  addicting  and  distracting  nature  of  social  media.  Previous  empirical  research  has  established  the  negative  effects  of  distractions  on  primary  tasks.  To  date,  little  research  has  looked  at  the  potentially  distracting  nature  of  social  media  and  the  negative  effects  that  can  arise  from  usage.  This  research  addresses  this  gap  by  investigating  the  effects  of  personal  social  media  usage  on  task  performance.  To  extend  this  research,  I  also  examined  the  effects  that  the  personal  social  media  usage  has  on  individuals'  technostress  and  happiness  levels.  I  tested  these  effects  by  creating  a  classroom  task  environment  and  measuring  subjects'  usage  of  social  media  and  their  task  performance.  From  this,  it  was  found  that  higher  amounts  of  personal  social  media  usage  led  to  lower  performance  on  the  task,  as  well  as  higher  levels  of  technostress  and  lower  happiness.  These  results  are  consistent  across  different  levels  of  attentional  control  and  multitasking  computer  self-efficacy.  These  results  suggest  that  the  personal  usage  of  social  media  during  professional  (vs.  personal  or  play)  times  can  lead  to  negative  consequences  and  is  worthy  of  further  study.
2	If  you  are  quick  enough  i  will  think  about  it  information  speed  and  trust  in  public  health  organizations.  Social  media  continues  to  gain  prominence  as  an  information  resource.  However,  little  is  known  about  how  people  perceive  trust  and  credibility  in  social  media  messages,  particularly  in  terms  of  abstract  dispositions  toward  organizations.  The  current  experiment  examines  the  role  of  speed  of  updates  on  a  twitter  feed  with  perceptions  of  trust.  The  experiment  is  also  used  to  address  the  convergent  validity  of  the  RAND  Public  Health  Disaster  Trust  Scale.  The  results  do  not  provide  evidence  of  a  direct  relationship  between  speed  of  twitter  feed  updates  and  trust,  but  do  support  a  mediation  model  in  which  cognitive  elaboration  mediates  the  relationship.  Further,  the  convergent  validity  of  the  RAND  Public  Health  Disaster  Trust  Scale  is  discussed,  along  with  its  utility  for  future  studies  of  this  type.
2	The  effect  of  music  streaming  services  on  music  piracy  among  college  students.  We  analyze  the  impact  of  music  streaming  services  in  digital  music  piracy.Using  a  sample  of  197  surveys,  we  evaluate  correlations  between  groups.We  develop  a  Logit  regression  model  to  test  five  hypothesis  regarding  music  piracy.We  find  that  music  streamers  are  more  likely  to  illegally  download  music.We  find  that  optimistic  beliefs  about  risk  and  rewards  explain  music  piracy.  Previous  studies  examining  the  intention  and  level  of  music  piracy  among  young  people  have  postulated  that  collective  attitudes,  optimistic  biases  toward  risk,  and  beliefs  about  copyright  laws  are  the  key  factors  involved  in  their  decisions.  We  extend  the  analysis  by  studying  the  impact  of  music  streaming  services.  Music  streaming  is  an  alternative  business  model  in  which,  for  a  small  fee,  the  consumer  has  access  to  a  large  set  of  songs  without  downloading  them  onto  their  devices.  Results  from  a  Logit  model  show  that  college  students  who  are  frequent  users  of  music  streaming  are  also  more  likely  to  download  music  illegally.  A  plausible  explanation  is  that  those  engaged  in  music  streaming  are  also  heavy  users  of  computer  technology,  software  downloading,  and  digital  sharing  -  factors  that  facilitate  the  conditions  for  music  piracy.  Demographics,  collective  attitudes,  and  beliefs  about  risk  and  rewards  continue  to  play  a  key  role  in  explaining  music  piracy,  but  their  relevance  is  slightly  reduced  once  controlled  for  music  streaming  usage.
2	Full  length  articleself  disclosure  on  sns  do  disclosure  intimacy  and  narrativity  influence  interpersonal  closeness  and  social  attraction.  On  social  media,  users  can  easily  share  their  feelings,  thoughts,  and  experiences  with  the  public,  including  people  who  they  have  no  previous  interaction  with.  Such  information,  though  often  embedded  in  a  stream  of  others’  news,  may  influence  recipients’  perception  toward  the  discloser.  We  used  a  special  design  that  enables  a  quasi-experience  of  SNS  browsing,  and  examined  if  browsing  other’s  posts  in  a  news  stream  can  create  a  feeling  of  familiarity  and  (even)  closeness  toward  the  discloser.  In  addition,  disclosure  messages  can  vary  in  the  degree  of  intimacy  (from  superficial  to  intimate)  and  narrativity  (from  a  random  blather  to  a  story-like  narrative).  The  roles  of  disclosure  intimacy  and  narrativity  on  perceived  closeness  and  social  attraction  were  examined  by  a  2  ×  2  experimental  design.  By  conducting  one  lab  study  and  another  online  replication,  we  consistently  found  that  disclosure  frequency,  when  perceived  as  appropriate,  predicted  familiarity  and  closeness.  The  effects  of  disclosure  intimacy  and  narrativity  were  not  stable.  Further  exploratory  analyses  showed  that  the  roles  of  disclosure  intimacy  on  closeness  and  social  attraction  were  constrained  by  the  perceived  appropriateness,  and  the  effects  of  narrativity  on  closeness  and  social  attraction  were  mediated  by  perceived  entertainment  value.
2	Internet  use  and  human  values.  We  use  Schwartz's  model  to  explore  the  impact  of  values  on  individual  internet  use.Study  1  uses  49  developed  and  emerging  nations  based  on  World  Value  Survey  data.Study  2  uses  8  nations  developed  and  emerging  based  on  European  Social  Survey  data.From  Study  1  Schwartz-like  value  types  are  significant  on  Internet  use  in  both  groups.Study  2:  tradition  and  security  are  most  relevant  and  different  impact  in  each  group.  In  this  study,  we  use  Schwartz's  value  framework  to  explore  the  impact  of  values  on  internet  use  at  an  individual  level.  This  approach  differs  from  many  of  the  previous  studies  that  report  on  national  level  analyses.  The  gap  in  the  literature  arising  from  the  absence  of  Schwartz's  value  types  in  information  and  communication  technologies  (ICT)  studies  calls  for  investigation  of  the  impact  of  individual  values  on  ICT  use  at  the  individual  level.  For  study  1,  we  use  the  set  of  developing  and  developed  nations,  based  on  World  Values  Survey  data.  For  study  2,  we  use  eight  nations  grouped  into  two  economic/cultural  clusters  (developed  and  developing)  based  on  the  data  from  the  European  Social  Survey.  Study  1  findings  indicate  that  4  out  of  8,  6  out  of  8  and  7  out  of  8  Schwartz-like  human  value  types  are  significant  for  Internet  use  for  developed,  developing  and  all  nations  respectively,  with  robust  effect  sizes.  The  study  2  findings  indicate  that  for  developed  nations,  Schwartz  value  types  such  as  conformity,  tradition,  security,  and  power  are  relevant  in  at  least  two  or  more  out  of  the  four  nations.  In  the  case  of  the  developing  nations,  achievement,  stimulation,  self-direction,  tradition  and  security  are  relevant  in  at  least  two  or  more  out  of  four  nations.  Tradition  and  security  are  the  two  value  types  that  are  most  relevant  in  both  groups  of  nations.  These  results  indicate  that  value  types  have  different  as  well  as  similar  impacts  in  developed  and  developing  nations.  Similarities  and  differences  also  exist  within  developed  and  developing  nations.  The  results  show  that  value  types  in  general  and  Schwartz's  value  types  in  particular,  are  useful  in  explaining  Internet  use.  The  implications  of  these  findings  are  discussed  and  future  studies  suggested.
2	Investigating  the  relationship  between  thinking  style  and  personal  electronic  device  use  and  its  implications  for  academic  performance.  Concrete  reasoning  is  positively  related  with  PED  use  hours  for  entertainment  and  communication.Using  PEDs  can  be  academically  beneficial  or  harmful  depending  on  the  purpose  of  PED  use.Thinking  style  interacts  with  educational  PED  use  on  determining  academic  performance.  This  article  examines  the  relationship  between  students'  thinking  style  and  the  level  of  their  use  of  personal  electronic  devices  (PEDs).  Also  investigated  are  the  educational  connotations  of  PED  use  and  the  moderating  effect  of  abstract/concrete  reasoning  on  the  relationship  between  PED  use  and  academic  performance.  To  these  ends,  506  Taiwanese  college  students  were  surveyed.  The  results  point  to  the  significance  of  concrete  reasoning  for  the  prevalence  of  non-educational  PED  use,  while  thinking  style  is  not  statistically  useful  in  explaining  educational  use  of  PEDs.  We  also  find  that  thinking  style  interacts  with  educational  PED  use  in  determining  academic  performance.  As  a  whole,  using  PEDs  can  be  academically  beneficial  or  harmful  for  students  depending  on  whether  they  use  PEDs  for  educational  or  non-educational  purposes.
2	Physiological  and  psychophysiological  responses  in  experienced  players  while  playing  different  dance  exer  games.  Display  Omitted  The  perception  of  experienced  players  differs  from  novices.Greater  physical  exertion  was  related  to  higher  engagement  and  game-flow.Problems  with  usability  detracted  from  engagement  and  game-flow.With  increased  skill,  perceived  user  control  may  matter  more.Higher  levels  of  interface  embodiment  did  not  reflect  a  more  positive  experience.  As  exer-game  technology  is  increasingly  integrated  into  workout  and  rehabilitation  programs,  it  is  important  to  understand  how  physical  exertion,  hedonics,  and  game  usability  experiences  affect  the  knowledgeable  player.  In  a  repeated  measures  experiment,  seven  experienced  players  played  five  popular  dance  exer-games  on  separate  days.  Exertion  was  measured  using  indirect  calorimetry.  Hedonic  experience  was  measured  using  questions  about  engagement  and  game-flow.  Platform  and  software  usability  was  determined  by  questionnaire  and  percent  time  spent  in  non-play  activities  (menu  interfaces,  software  loading).  Mean  MET  levels  ranged  from  4.26  to  9.18  during  gameplay,  depending  on  the  game.  Player  engagement  and  game-flow  were  closely  related  to  MET  activity.  Usability  scores  were  closely  related  to  time  spent  in  non-play  activities.  Games  with  increased  MET  levels  and  higher  usability  scores  were  reflected  in  higher  engagement  and  game-flow  scores.  Degree  of  interface  embodiment  did  not  affect  these  outcomes.  Based  on  these  results,  problems  in  usability  are  barriers  to  engagement.  Designing  games  that  provide  a  balance  of  challenge,  immersion,  and  engagement  is  a  difficult  task.  These  results  demonstrate  the  importance  of  examining  players  of  varying  skill  levels  in  order  to  fully  understand  the  interaction  of  physical  exertion,  platform  usability  and  player  engagement.
2	Performance  on  the  traditional  and  the  touch  screen  tablet  versions  of  the  corsi  block  and  the  tower  of  hanoi  tasks.  Psychologists  routinely  administer  cognitive  tasks  to  assess  a  range  of  mental  abilities.  In  recent  years,  researchers  and  practitioners  have  employed  new  (i.e.  digital)  technologies  to  test  cognitive  performance,  with  tablet  computer  based  tasks  often  replacing  traditional  versions.  However,  the  extent  to  which  findings  from  traditional  and  touch  screen  tablet  based  tasks  are  equivalent  remains  unclear.  In  the  present  study,  sixty  participants  (18  men  and  42  women)  completed  both  the  Tower  of  Hanoi  and  Corsi  Block  tasks  in  their  traditional  (wooden)  form  and  using  a  touch  screen  tablet.  Performance  outcome  measures  (span  length,  number  of  moves,  and  time  taken)  were  recorded  alongside  subjective  workload  for  each  task.  Findings  revealed  that  number  of  moves  and  span  length  do  not  significantly  differ  between  the  traditional  and  tablet  based  versions  of  each  task.  However,  the  computerized  Tower  of  Hanoi  task  was  completed  more  quickly  than  the  traditional  version.  Differences  were  noted  for  subjective  workload  with  higher  physical  demand  reported  for  the  traditional  versions  of  each  task.  Participants  also  reported  the  traditional  Tower  of  Hanoi  task  to  be  more  enjoyable  but  more  mentally  demanding.  In  conclusion,  the  touch  screen  versions  of  the  Tower  of  Hanoi  and  Corsi  Block  tasks  appear  largely  equivalent  to  the  traditional  versions.  Traditional  and  tablet  based  Tower  of  Hanoi  and  Corsi  Block  tasks  were  compared.Performance  (i.e.  number  of  moves  and  span  length)  did  not  differ  between  versions.The  computerized  Tower  of  Hanoi  task  was  completed  more  quickly.Differences  were  identified  for  subjective  workload  and  enjoyment.Traditional  and  tablet  versions  of  the  cognitive  tasks  are  largely  equivalent.
2	Emoticons  influence  on  advice  taking.  Abstract  Although  there  is  a  lot  of  research  on  advice  and  its  utilization,  little  research  on  advice  has  examined  the  effect  of  computer  mediated  communication  (CMC)  on  advice  outcomes.  Emoticons,  which  are  graphic  representations  of  facial  expressions,  are  unique  to  CMC  and  commonly  used  as  a  substitute  for  nonverbal  cues  that  are  missing  from  CMC.  We  explored  the  relationship  between  emoticons  in  advice  messages  and  advice  utilization.  Because  emoticons  convey  emotional  information,  the  effects  of  emoticons  on  advice  utilization  may  be  higher  when  the  receiver  is  less  willing  or  able  to  analyze  the  message  more  systematically.  Therefore,  we  examined  two  moderators  that  affect  message  processing:  involvement  and  need  for  cognition.  Two  experiments  tested  our  hypothesis.  In  Study  1,  emoticons  increased  intention  to  utilize  advice,  but  only  under  conditions  of  low  involvement.  Study  2  replicated  the  effects  of  Study  1  but  also  found  that  emoticons  had  a  stronger  effect  when  participants  had  a  low  need  for  cognition.
2	High  tension  lines  negative  social  exchange  and  psychological  well  being  in  the  context  of  instant  messaging.  Abstract  Instant  messengers  (IM)  mediate  several  types  of  negative  social  exchanges  that  can  affect  the  psychological  well-being  of  their  users.  Using  an  instant  messenger  popular  in  Taiwan  (LINE)  as  an  example,  this  study  examined  the  relationship  between  IM  communication  and  well-being.  An  online  survey  was  conducted  and  found  that  negative  IM  social  exchanges  in  which  messages  were  ignored  by  their  recipients,  interfered  with  other  activities,  or  contained  hostility  or  ridicule  were  directly  related  to  a  global  measure  of  negative  social  exchange  that  included  face-to-face  as  well  as  online  interactions.  Insensitive  IM  social  exchanges  were  positively  related  to  loneliness,  depression,  and  fear  of  missing  out  (FoMO),  and  were  negatively  related  to  social  support.
2	Artificial  intelligence  versus  maya  angelou  experimental  evidence  that  people  cannot  differentiate  ai  generated  from  human  written  poetry.  Abstract  The  release  of  openly  available,  robust  natural  language  generation  algorithms  (NLG)  has  spurred  much  public  attention  and  debate.  One  reason  lies  in  the  algorithms'  purported  ability  to  generate  humanlike  text  across  various  domains.  Empirical  evidence  using  incentivized  tasks  to  assess  whether  people  (a)  can  distinguish  and  (b)  prefer  algorithm-generated  versus  human-written  text  is  lacking.  We  conducted  two  experiments  assessing  behavioral  reactions  to  the  state-of-the-art  Natural  Language  Generation  algorithm  GPT-2  (Ntotal = 830).  Using  the  identical  starting  lines  of  human  poems,  GPT-2  produced  samples  of  poems.  From  these  samples,  either  a  random  poem  was  chosen  (Human-out-of-theloop)  or  the  best  one  was  selected  (Human-in-the-loop)  and  in  turn  matched  with  a  human-written  poem.  In  a  new  incentivized  version  of  the  Turing  Test,  participants  failed  to  reliably  detect  the  algorithmicallygenerated  poems  in  the  Human-in-the-loop  treatment,  yet  succeeded  in  the  Human-out-of-the-loop  treatment.  Further,  people  reveal  a  slight  aversion  to  algorithm-generated  poetry,  independent  on  whether  participants  were  informed  about  the  algorithmic  origin  of  the  poem  (Transparency)  or  not  (Opacity).  We  discuss  what  these  results  convey  about  the  performance  of  NLG  algorithms  to  produce  human-like  text  and  propose  methodologies  to  study  such  learning  algorithms  in  human-agent  experimental  settings.
2	Do  social  ties  matter  for  purchase  frequency  the  role  of  buyers  attitude  towards  social  media  marketing.  Abstract  In  social  media  marketing,  it  is  common  practices  to  leverage  social  ties  to  promote  business.  However,  whether  do  social  ties  matter  for  buyers'  purchase  behavior?  Combining  the  transaction  utility  theory  with  the  motivations  of  social  interaction,  we  conducted  an  empirical  study  by  using  the  trading  data  of  a  large  social  media  platform  (i.e.,  WeChat).  The  following  conclusions  are  reached:  the  buyers  with  strong  social  ties  with  sellers  reveal  higher  purchase  frequencies  than  those  with  weak  social  ties.  However,  such  marketing  effects  of  social  ties  can  be  attenuated  by  buyers’  attitudes  towards  social  media  marketing,  for  buying  higher-priced  goods.  Finally,  we  also  provide  suggestions  for  social  media  marketing  practice  and  insights  for  future  research.
2	Leveraging  web  2  0  technologies  to  foster  collective  civic  environmental  initiatives  among  low  income  urban  communities.  Abstract  There  are  a  number  of  challenges  facing  the  low-income  urban  communities  living  in  slums  in  most  of  the  developing  countries  such  as  the  Mathare  slum.  Provision  of  essential  public  basic  services  in  the  Nairobi  slums  is  lower  compared  to  what  is  offered  to  the  rest  of  the  City.  But,  services  such  as  the  garbage  removal  also  requires  participation  of  the  residents  for  successful  service  delivery.  The  objective  of  this  study  was  to  investigate  whether  use  of  Facebook,  as  a  Web  2.0  technology,  can  support  the  residents  of  Mathare  slum  to  actively  participate  in  civic  environmental  initiatives  and  to  foster  community  activeness  towards  civic  environmental  protection.  The  study  employed  a  mixed  methods  approach  to  investigate  the  problem.  This  was  achieved  through  a  preliminary  survey  (700  respondents)  to  collect  data  on  use  of  Web  2.0  among  the  residents  of  Mathare  slum,  an  experiment  with  175  residents,  who  participated  in  a  community  civic  environmental  initiative  using  Facebook  and  a  survey  to  measure  continuance  intentions  on  use  of  Web  2.0  technologies  for  collective  community  Web  2.0-mediated  activities  towards  protecting  the  environment  among  low-income  urban  communities.  The  study  constructed  and  successfully  tested  a  model  of  Web  2.0  use  and  online  environmental  protection  initiatives.  Use  of  Web  2.0  technologies  for  environmental  protection  emerged  as  a  significant  predictor  of  online  social  capital,  community  environmental  activeness  and  continuance  intentions  to  participate  in  environmental  initiatives,  while  perceived  cost  negatively  moderates  the  relationship  between  Web  2.0  use  and  continuance  intentions.  Implications  and  recommendations  for  policy,  practice  and  research  are  provided.
2	Gamers  insights  into  the  phenomenology  of  normal  gaming  and  game  addiction  a  mixed  methods  study.  Abstract  In  response  to  calls  for  further  research  into  the  phenomenology  of  Internet  gaming  disorder  (IGD),  we  used  a  community-engaged  consensus  development  approach  to  evaluate  how  members  of  the  “gamer  culture”  describe  problematic  gaming  and  the  relationship  of  these  descriptions  to  the  proposed  IGD  criteria.  Two  focus  groups  of  gamers  were  recruited  at  a  video  game  convention.  Participants  were  asked  to  submit  suggestions  for  signs  of  game  “addiction”.  Participants  discussed  and  ranked  the  criteria  in  order  of  conceptual  importance.  The  rankings  were  analyzed  quantitatively,  and  then  a  multidisciplinary  team  compared  the  ranked  criteria  to  the  DSM-5  IGD  proposed  criteria.  The  strongest  agreement  between  participants’  rankings  and  IGD  symptomatology  was  found  for  harms/functional  impairment  due  to  gaming,  continued  use  despite  problems,  unsuccessful  attempts  to  control  gaming,  and  loss  of  interest  in  previous  hobbies  and  entertainment.  There  was  less  support  for  other  IGD  criteria.  Participants  also  offered  new  content  domains.  These  findings  suggest  that  collaborative  knowledge-building  approaches  may  help  researchers  and  policymakers  understand  the  characteristics  and  processes  specific  to  problematic  video  game  play  and  improve  content  validity  of  IGD  criteria.  Future  efforts  may  benefit  from  multi-stakeholder  approaches  to  refine  IGD  criteria  and  inform  theory,  measurement  and  intervention.
2	Internet  the  great  radicalizer  exploring  relationships  between  seeking  for  online  extremist  materials  and  cognitive  radicalization  in  young  adults.  Abstract  Anecdotal  evidence  asserts  that  extremist  materials  on  the  internet  play  a  decisive  role  in  radicalization  processes.  However,  due  to  a  structural  absence  of  empirical  data  in  the  current  literature,  it  remains  uncertain  if—and  to  what  extent—online  extremist  materials  radicalize.  Therefore,  the  approach  of  the  current  study  was  two-fold.  First,  we  explored  what  types  of  online  jihadist  media  are  pro-actively  sought  and  consumed  by  young  adults.  Second,  we  investigated  if  and  how  active  exposure  to  online  jihadist  media  is  related  to  cognitive  radicalization,  whilst  taking  into  account  one's  moral  disengagement,  prior  involvement  in  petty  crime,  and  socio-demographics.  Cross-sectional  data  analyses  within  a  sample  of  Belgian  young  adults  (n = 1,872)  show  that  beheading  videos—the  most  violent  and  radical  form  of  any  of  the  jihadist  materials  under  scrutiny—were  most  sought  online  (36%),  but  were,  paradoxically,  the  least  predictive  for  radicalization.  On  the  contrary,  the  rather  static  jihadist  magazines  were  sought  by  a  small  minority  (10–11%)  but  were  the  most  strongly  associated  with  radicalization.  A  stepwise  linear  regression  analysis  and  Structural  Equation  Model  support  our  hypothesis  that  the  process  of  cognitive  radicalization  is  a  complex,  phasic  trajectory  from  actively  seeking  out  extremist  materials  to  sympathies  for  violent  political  behaviors.
2	Phone  use  while  parenting  an  observational  study  to  assess  the  association  of  maternal  sensitivity  and  smartphone  use  in  a  playground  setting.  Abstract  Smartphone  use  has  become  an  “always-on”  activity.  Caring  for  children  and  being  sensitive  to  their  needs  seems  to  conflict  with  this  immersive  and  time-consuming  activity.  Building  on  John  Bowlby's  Attachment  Theory,  we  focused  on  how  mothers'  smartphone  use  is  related  to  maternal  sensitivity.  Using  the  Mini-Maternal  Behavior  Q-Sort  method  and  a  post-observation  questionnaire,  we  collected  data  from  89  mother-child  dyads  on  playgrounds.  Our  results  showed  that  mothers  who  used  their  smartphones  longer  had  lower  sensitivity  ratings  (β = −0.51,  p
2	The  contradiction  between  self  protection  and  self  presentation  on  knowledge  sharing  behavior.  Abstract      This  study  decomposes  information  security  awareness  (ISA)  into  general  information  security  awareness  (GISA)  and  information  security  policy  awareness  (ISPA),  to  explore  how  these  factors  affect  knowledge-sharing  behavior.  It  also  investigates  the  effects  of  threat  appraisal  and  source  credibility  on  knowledge-sharing  behavior  from  self-protection  and  self-presentation  perspectives.  This  study  adopted  an  online  questionnaire  through  mySurvey  for  data  collection.  This  research  collected  and  analyzed  598  valid  responses  by  using  a  structural  equation  modeling  to  validate  the  research  hypotheses.  The  results  indicate  that  GISA  and  ISPA  have  significant  positive  effects  on  threat  appraisal.  Whereas  ISPA  has  a  significant  positive  effect  on  knowledge-sharing  behavior,  GISA  does  not.  With  regard  to  social  ties,  bonding  social  capital  and  bridging  social  capital  have  significant  positive  effects  on  both  source  credibility  and  knowledge-sharing  behavior.  Threat  appraisal  has  a  significant  negative  effect,  and  source  credibility  has  a  significant  positive  effect,  on  knowledge-sharing  behavior.  In  addition,  threat  appraisal  and  source  credibility  exert  partial  mediation  effects  on  ISPA,  bonding  social  capital,  bridging  social  capital,  and  knowledge-sharing  behavior.
2	Understanding  participation  on  video  sharing  communities.  A  community's  development  and  sustainability  rely  heavily  on  user  participation.  Facing  fierce  competition  among  video  websites,  many  companies  have  been  consciously  elevating  user  participation  through  the  practice  of  video  sharing  communities.  However,  few  studies  have  discussed  this  phenomenon.  The  purpose  of  this  paper  is  to  examine  the  effectiveness  of  self-construal  and  community  interactivity  in  promoting  user  participation  on  video  sharing  communities.  Based  on  Triadic  reciprocal  determinism,  a  framework  was  developed  to  explain  the  mechanism  of  user  participation  behaviors  on  video  sharing  communities.  Users  from  a  video  sharing  community  in  China  were  invited  to  accomplish  an  online  survey,  and  184  valid  responses  returned.  Structural  equation  modeling  with  SmartPLS  3.0  was  adopted  to  evaluate  the  research  model.  The  empirical  results  show  that  self-construal  (independent-construal  and  interdependent  construal)  and  community  interactivity  (active  control  and  reciprocal  communication)  are  positively  associated  with  user  participation.  In  turn,  user  participation  significantly  increases  perceived  customer  value.  In  addition,  self-construal  positively  moderates  the  relationship  between  community  interactivity  and  user  participation.  The  present  study  is  among  the  first  attempts  to  empirically  consider  the  effect  of  self-construal  and  community  interactivity  within  a  video  sharing  community  context  in  a  comprehensive  and  systematical  way.  We  developed  a  model  to  understand  user  participation  on  video  sharing  communities  based  on  Triadic  reciprocal  determinism.We  collected  data  through  online  questionnaire  survey  and  validated  our  framework  with  Structural  Equation  Modeling.We  discovered  the  influence  of  self-construal  and  community  interactivity  on  user  participation.We  also  verified  the  interaction  effects  of  self-construal  and  community  interactivity.
2	Internet  use  and  leisure  time  physical  activity  of  adults  a  nationwide  survey.  We  examined  the  relationships  between  physical  activity  and  Internet  use  habits,  using  the  Theory  of  Planned  Behavior  (TPB)  and  the  Health  Action  Process  Approach  (HAPA)  model  as  a  framework.  The  purpose  was  to  examine  how  Internet  access,  digital  use,  and  time  spent  online  correlate  with  the  odds  of  engaging  in  three  types  of  PA:  strenuous,  moderate,  and  muscle-strengthening  Data  relied  on  the  Annual  Social  Survey  conducted  by  Israel's  Central  Bureau  of  Statistics  in  2010,  with  6035  participants  aged  20-65.  Logistic  regressions  revealed  that  the  odds  of  engaging  versus  not  engaging  in  three  types  of  physical  activity  were  higher  among  Internet  users  compared  to  non-users.  The  odds  of  engaging  versus  not  engaging  in  strenuous  physical  activity  were  higher  among  those  who  used  the  Internet  for  studying,  social  media,  and  downloading;  in  strengthening  muscles  were  higher  among  those  who  used  the  Internet  for  seeking  information  and  social  media;  and  in  moderate  physical  activity  were  higher  among  those  who  used  the  Internet  for  studying,  compared  to  non-users.  We  suggest  that  the  measured  digital  uses  represent  intention  and  action  plans  similar  in  their  determinants  to  being  physically  active.  Health  care  decision  makers  should  increase  availability  of  information  on  the  Internet  regarding  a  healthy  lifestyle,  concentrating  on  leisure-time  physical  activity  habits.  Engagement  in  three  types  of  PA  was  higher  among  Internet  users  than  non-users.Digital  use  stems  from  the  intention  to  make  plans  for  participating  in  PA,  as  described  in  the  Theory  of  Planned  Behavior.Participation  in  PA  was  higher  among  those  engaging  in  digital  uses  for  studying,  social  media,  and  seeking  information.
2	Virtual  vs  real  body  in  exergames.  The  study  uses  an  exergame  for  exercise  interventions  through  the  use  of  avatars.Individuals  with  body  image  dissatisfaction  are  benefitted  from  exergame  play.Individuals'  social  physique  anxiety  is  being  reduced  during  exergame  play.Presence  mediates  the  relationship  between  exercise  context  and  exergame  experiences.  Research  indicates  that  people  with  body  image  dissatisfaction  (BID)  are  not  benefited  from  exercising  in  group  contexts.  The  current  study  tested  whether  exercise  video  games  (exergames)  can  provide  unique  opportunities  for  exercise  interventions  through  the  use  of  avatars.  An  experiment  was  conducted  using  a  2  (BID:  High  vs.  Low)×2  (Exercise  context:  Solitary  vs.  Group)  between-subjects  design.  Results  demonstrated  that  individuals  with  high  BID  reported  similar  or  more  favorable  exergame  experiences  compared  to  individuals  with  low  BID.  Further,  individuals'  social  physique  anxiety  was  significantly  reduced  during  exergame  play.  Self-presence  mediated  the  relationship  between  exercise  context  and  exergame  experiences.
2	Feasibility  of  online  divergent  thinking  assessment.  The  project  examined  creative  thinking  in  online  settings.The  goal  was  to  aid  creative  thinking  research  by  increasing  ecological  validity.Online  administration  had  no  detrimental  affect  on  divergent  thinking.Imposition  of  time  limits  effectively  reduced  individual  differences  in  time-on-task,  fluency,  and  originality.Divergent  thinking  was  stable  across  two  different  online  samples.  Two  studies  explored  whether  assessment  of  creative  thinking  is  feasible  using  web-based  methods  and  how  participants  reacted  to  the  imposition  of  time  constraints  in  online  settings.  Sixty-five  participants  (Study  1)  completed  a  verbal  fluency  task  and  a  divergent  thinking  task,  half  of  the  participants  doing  so  over  the  Internet.  Online  administration  did  not  affect  originality,  but  led  to  slightly  fewer  responses  overall.  This  demonstrated  that  online  administration  of  creative  thinking  is  indeed  feasible  and  reliable,  though  steps  must  be  taken  to  ensure  participants  exhaust  all  possible  responses.  To  test  the  effect  of  time  limits  on  responses,  84  participants  (Study  2)  completed  a  verbal  fluency  task  and  three  divergent  thinking  tasks  online,  half  of  the  participants  doing  so  under  time  pressure  (3min).  There  were  significant  interactions  between  time  limits  and  task  content  for  both  time-on-task  and  fluency  variables,  but  the  task  type  was  the  dominant  force  in  the  varying  fluency  and  time-on-task  scores.  Originality  was  not  significantly  affected  by  time  limits,  but  did  vary  across  tasks.  In  all  cases  the  results  illustrate  that  assessment  of  divergent  thinking,  as  a  proxy  of  creative  thinking,  is  feasible  using  online  methods.  Implications  for  future  work  in  this  area  are  discussed.
2	Extracting  multilayered  communities  of  interest  from  semantic  user  profiles  application  to  group  modeling  and  hybrid  recommendations.  A  Community  of  Interest  is  a  specific  type  of  Community  of  Practice.  It  is  formed  by  a  group  of  individuals  who  share  a  common  interest  or  passion.  These  people  exchange  ideas  and  thoughts  about  the  given  passion.  However,  they  are  often  not  aware  of  their  membership  to  the  community,  and  they  may  know  or  care  little  about  each  other  outside  of  this  clique.  This  paper  describes  a  proposal  to  automatically  identify  Communities  of  Interest  from  the  tastes  and  preferences  expressed  by  users  in  personal  ontology-based  profiles.  The  proposed  strategy  clusters  those  semantic  profile  components  shared  by  the  users,  and  according  to  the  clusters  found,  several  layers  of  interest  networks  are  built.  The  social  relations  of  these  networks  might  then  be  used  for  different  purposes.  Specifically,  we  outline  here  how  they  can  be  used  to  model  group  profiles  and  make  semantic  content-based  collaborative  recommendations.
2	I  did  it  for  the  lulz  how  the  dark  personality  predicts  online  disinhibition  and  aggressive  online  behavior  in  adolescence.  Abstract  A  large  proportion  of  youth  believe  that  the  world  of  cyberspace  provides  them  with  a  relatively  safe  and  anonymous  digital  bubble  ripe  for  uninhibited  self-expression.  At  the  same  time,  observers  have  noted  an  increase  of  individuals  behaving  in  an  unrestrained  manner  on  the  Internet,  while  researchers  have  reported  elevated  rates  of  cyber  aggressive  behavior.  What  remains  unclear,  however,  is  whether,  and  how,  disinhibition  might  be  related  to  cyber  aggression.  In  an  aim  to  explore  the  possible  associations,  a  large  sample  (total  N = 709)  of  high  school  (Mage = 15.56  years)  respondents  from  New  Zealand  were  recruited,  and  completed  a  survey  featuring  scales  assessing  personality  and  technology  behaviors,  attitudes,  habits,  and  trends.  The  present  study  was  designed  to  investigate  whether  the  three  dark  personality  traits  of  narcissism,  psychopathy,  and  sadism  would  predict  false  self  perceptions,  and  in  sequence,  online  disinhibition  and  aggressive  online  behavior.  All  three  dark  personality  traits,  as  well  as  false  self,  were  positively  associated  with  online  disinhibition.  Perceptions  of  false  self  were  found  to  be  a  significant  predictor  of  cyber  aggression  when  mediated  by  online  disinhibition.  In  the  case  of  cyber  aggression,  however,  psychopathy,  sadistic  traits,  and  online  disinhibition  were  found  to  be  significant  predictors  of  this  outcome.  The  results  collectively  provide  a  more  nuanced  understanding  of  how  antisocial  personality  traits  are  associated  with  maladaptive  identity  formation  (i.e.,  endorsement  of  false  self  beliefs)  as  well  as  maladaptive  online  behavior.
2	What  happens  online  stays  online  social  media  dependency  online  support  behavior  and  offline  effects  for  lgbt.  Abstract  Social  media  is  increasingly  popular  among  LGBT  (lesbian,  gay,  bisexual  and  transgender)  who  have  a  higher  risk  of  depression  and  dis-identification  in  China.  However,  it  is  unclear  whether  psychological  dependency  on  social  media  and  social  media  use  behavior  have  beneficial  offline  effects.  We  conducted  an  online  survey  of  1391  valid  LGBT  recruited  in  China  and  also  interviewed  five  LGBT  Weibo  users  to  better  explain  some  results.  We  found  LGBT  with  higher  levels  of  depression  and  dis-identification  were  more  likely  to  be  psychologically  dependent  on  social  media.  Higher  levels  of  social  media  dependency  were  predictive  of  social  media  use,  and  both  online  social  support  seeking  and  offering  activities  contributed  individually  as  predictor  of  perceived  online  social  support.  Perceived  online  social  support  was  not  negatively  related  to  offline  depression  and  dis-identification,  which  meant  what  happens  online  may  stay  online.  LGBT  who  has  a  longer  history  of  using  Weibo  would  be  less  dependent  on  social  media,  perceive  less  online  social  support  and  have  better  offline  psychological  wellbeing.  The  mechanism  of  how  social  media  use  affects  offline  life  of  LGBT  needs  to  be  further  examined.  Limitations,  future  directions  and  recommendations  are  discussed.
2	A  study  on  the  relationships  between  different  presentation  modes  of  graphical  icons  and  users  attention.  With  the  vigorous  development  of  applications  (App),  graphical  user  interfaces  (GUI)  have  been  widely  found  in  computers  and  handheld  devices.  This  study  aimed  to  explore  the  relationships  between  different  presentation  modes  of  graphical  icons  and  users'  attention.  An  eye  tracker  was  employed  to  measure  each  participant's  experimental  data;  in  addition,  subjective  evaluation  of  attention  was  conducted.  Thus,  the  optimum  presentation  mode  attracting  the  most  attention  might  be  determined.  The  experiment  was  designed  to  investigate  two  variables:  icon  composition  and  background.  Through  permutation  and  combination,  six  presentation  modes  were  obtained  as  follows:  lineź+źpositive  background  (M1),  planeź+źpositive  background  (M2),  lineź+źnegative  background  (M3),  planeź+źnegative  background  (M4),  lineź+źno  background  (M5),  and  planeź+źno  background  (M6).  Thirty-six  participants  were  requested  to  view  thirty  stimuli,  or  the  contour  drawings  of  graphical  icons,  presented  simultaneously  in  six  modes  mentioned  above.  The  participants'  fixation  duration,  fixation  frequency,  and  subjective  evaluation  of  attention  were  analyzed  through  two-way  ANOVA.  The  analytical  results  showed  that  in  terms  of  the  three  performance  indicators  above,  M4  performed  the  best  among  the  six  presentation  modes.  Moreover,  regarding  icon  composition,  planes  performed  better  than  lines  in  terms  of  the  three  performance  indicators.  As  for  background,  negative  background  performed  the  best  in  terms  of  the  three  performance  indicators,  positive  background  ranked  second,  and  no  background  performed  the  worst.  The  findings  can  serve  as  a  reference  when  icons  are  researched  or  designed  in  the  future.  Icon  composition  and  background  have  a  statistically  significant  effect  on  fixation  duration.The  icon  composed  of  planeź+źnegative  background  performs  the  best,  attracting  the  greatest  amount  of  attention.Planes  perform  better  than  lines  in  terms  of  fixation  duration,  fixation  frequency,  and  subjective  evaluation  of  attention.Negative  background  performs  better  than  positive  background,  and  positive  background  performs  better  than  no  background.
2	A  holistic  system  for  troll  detection  on  twitter.  Abstract  Various  techniques  based  on  artificial  intelligence  have  been  proposed  for  the  automatic  detection  of  online  anti-social  behaviors,  both  in  existing  systems  and  in  the  scientific  literature.  In  this  article,  we  describe  TrollPacifier,  a  holistic  system  for  troll  detection,  which  analyses  many  different  features  of  trolls  and  legitimate  users  on  the  popular  Twitter  platform.  In  this  system,  the  most  known  and  promising  approaches  and  research  lines  are  applied,  along  with  original  new  ideas,  in  a  form  that  fits  such  a  large  public  platform.  In  particular,  we  have  identified  six  groups  of  features,  based  respectively  on  the  analysis  of  writing  style,  sentiment,  behaviors,  social  interactions,  linked  media,  and  publication  time.  As  its  main  scientific  contributions,  this  work  provides:  (i)  an  up-to-date  analysis  of  the  state  of  the  art  for  the  problem  of  troll  detection;  (ii)  the  systematic  collection  and  grouping  of  features,  on  Twitter;  (iii)  the  description  of  a  working  holistic  system  for  troll  detection,  with  a  very  high  accuracy  (95.5%);  and  (iv)  a  comparison  among  the  different  features,  with  a  machine  learning  approach.  Our  results  demonstrate  that  automatic  classification  can  be  useful  in  the  whole  process  of  identification  and  management  of  online  anti-social  behaviors.  However,  a  multi-faceted  approach  is  required,  in  order  to  obtain  an  adequate  accuracy.
2	The  differential  impact  of  mood  on  consumers  decisions  a  case  of  mobile  payment  adoption.  Abstract  Research  on  consumer  technology  adoption  has  predominantly  focused  on  technology  acceptance  models;  the  role  of  consumers'  affective  states  and  individual  characteristics  has  largely  remained  underexplored.  Drawing  on  the  Mood-Behavior  Model  and  the  Affect  Infusion  Model,  this  research  suggests  that  consumers'  mood  is  an  important  factor  that  influences  their  decision  to  adopt  in-store  m-payment  services.  More  importantly,  the  nature  of  this  impact  differs  depending  on  two  individual  characteristics:  consumers'  decision-making  style  (maximizer/satisficer)  and  need  for  gratification.  A  scenario-based  experiment  (n = 322)  provides  empirical  evidence  for  the  significance  of  consumers'  affective  states  in  their  judgements  and  decisions.  When  experiencing  positive  mood,  those  satisficers  who  have  a  higher  need  for  gratification  are  more  likely  to  use  m-payment  services.  In  contrast,  in  a  negative  mood  state,  maximizers  with  a  higher  need  for  gratification  are  more  inclined  to  use  m-payment.  The  findings  contribute  to  the  literature  by  demonstrating  that  mood  is  an  important  determinant  of  technology  adoption  and  that  consumers'  individual  characteristics  define  how  positive  and  negative  mood  can  influence  their  adoption  decisions  in  different  ways.  The  results  also  inform  managers  on  an  interesting  consumer  segmentation  approach  based  on  consumers’  decision-making  style  and  need  for  gratification  when  promoting  in-store  m-payment  services.
2	A  network  based  conceptualization  of  social  commerce  and  social  commerce  value.  Abstract  An  increasing  number  of  businesses  and  consumers  talk  about,  invest,  and  engage  in  social  commerce,  which  is  a  form  of  commerce  mediated  by  social  media.  Despite  the  growing  recognition  and  adoption  of  social  commerce,  its  practice  is  often  driven  by  fast-paced  technological  evolutions,  hype,  and  ad-hoc  business  cases.  In  parallel,  social  commerce  has  been  the  target  of  limited  scientific  inquiries,  and  it  remains  ill  conceived,  which  constitutes  an  obstacle  to  theory  development  and  cumulative  research.  This  paper  addresses  this  issue  by  proposing  a  focus  on  social  commerce  networks  (SCNs).  SCNs  refer  to  the  digital  network  ties  created  and  leveraged  by  consumers  and  business  entities  as  they  connect  and  interact  among  and  between  each  other.  Based  on  this  definition,  the  paper  develops  a  typology  of  key  SCN  structures,  and  it  proposes  a  framework  depicting  the  value  that  consumers  and  businesses  can  gain  from  creating  and  exploiting  them.  We  explain  how  the  proposed  SCN  concept,  typology,  and  framework  can  guide  future  conversations  on  the  topic  of  social  commerce,  and  how  they  can  provide  researchers  and  practitioners  with  a  better  understanding  of  social  commerce  and  its  considerable  value  potential.
2	Enabling  instrumental  interaction  through  electronics  making  effects  on  children  s  storytelling.  The  electronics  Making  phenomenon,  spearheaded  by  the  introduction  of  open  source  microprocessors  and  3D  printers,  has  been  quickly  infiltrating  into  children’s  domains,  but  how  Making  interacts  with  storytelling  has  not  been  addressed.  This  paper  achieves  the  following:  it  proposes  the  argument  of  instrumental  interaction  for  Making-based  storytelling,  it  details  a  custom  Maker  kit  that  integrates  Making  with  storytelling,  and  it  presents  a  study  that  investigates  the  effects  of  electronics  Making-based  storytelling  on  the  semantics  of  children’s  puppet  stories.  Analysis  results  showed  that  the  LED  instrumental  interaction  contributes  significantly  to  the  children’s  story  meanings.
2	The  expressive  space  of  ids  as  art.  Much  of  the  research  and  development  effort  in  the  IDS  community  is  guided  by  a  design  approach,  according  to  which  user  desires,  expectations  and  agency  are  central.  This  approach  may  place  unnecessary  limitations  on  the  design  space.  I  argue  for  the  validity  and  significance  of  an  alternative,  expressive  approach,  which  may  be  more  appealing  to  artists,  and  offer  some  formal  parameters  of  IDS-as-Art.  These  open  up  for  authors  the  ability  to  explore  the  creation  of  vastly  complex  and  dynamic  storyworlds  that  highlight  contingency  and  probability;  and  the  possibility  of  manipulating  agency  as  subject  matter  to  implicate  users,  through  userly  performance,  in  the  storyworld's  meaning.  IDS  can  thus  offer  users  an  opportunity  to  reflect  upon  various  aspects  of  their  performance,  including  un-  and  subintentional  aspects.
2	Faoladh  a  case  study  in  cinematic  vr  storytelling  and  production.  Portraying  traditional  cinematic  narratives  in  virtual  reality  (VR)  is  an  emerging  practice  where  often  the  methods  normally  associated  with  cinematic  storytelling  need  to  be  adapted  to  the  \(360^\circ  \)  format.  In  this  paper  we  investigate  some  proposed  cinematic  practices  for  narrative  storytelling  in  a  cinematic  VR  film  set  in  late  9th  century  Ireland  that  follows  the  perilous  journey  young  Celt  as  he  evades  being  captured  by  Viking  raiders.  From  this  we  will  analyze  the  fidelity  of  those  practices  with  results  collected  from  YouTube  Analytics.
2	Profiling  and  benchmarking  event  and  message  passing  based  asynchronous  realtime  interactive  systems.  This  article  describes  a  set  of  metrics  for  a  message-passing-based  asynchronous  Realtime  Interactive  System  (RIS).  Current  trends  in  concurrent  RISs  are  analyzed,  several  profiling  tools  are  outlined,  and  common  metrics  are  identified.  A  set  of  nine  metrics  is  presented  in  a  unified  and  formalized  way.  The  implementation  of  a  profiler  that  measures  and  calculates  these  metrics  is  illustrated.  The  implementation  of  an  instrumentation  and  a  visualization  tool  are  described.  A  case  study  shows  how  this  approach  proved  beneficial  during  the  optimization  of  latency  of  an  actual  system.
2	Voice  in  virtual  worlds  the  design  use  and  influence  of  voice  chat  in  online  play.  Communication  is  a  critical  aspect  of  any  collaborative  system.  In  online  multiplayer  games  and  virtual  worlds  it  is  especially  complex.  Users  are  present  over  long  periods,  require  both  synchronous  and  asynchronous  communication,  and  may  prefer  to  be  pseudonymous  or  engage  in  identity-play  while  managing  virtual  and  physical  use  contexts.  Initially  the  only  medium  for  player-to-player  communication  in  virtual  worlds  was  text,  a  medium  well  suited  to  identity-play  and  asynchronous  communication,  less  so  to  fast-paced  coordination  and  sociability  among  friends.  During  the  past  decade  vendors  have  introduced  facilities  for  gamers  to  communicate  by  voice.  Yet  little  research  has  been  conducted  to  help  us  understand  the  influence  of  voice  on  the  experience  of  using  virtual  space:  Where,  when,  and  for  whom  voice  is  beneficial,  and  how  it  might  be  configured.  To  address  this  gap  we  examined  a  range  of  online  gaming  environments.  We  analyzed  our  observations  in  the  light  of  theory  from  Human–Computer  Interaction,  Computer-Supported  Cooperative  Work,  and  Computer-Mediated  Communication.  We  conclude  that  voice  radically  transforms  the  experience  of  online  gaming,  making  virtual  spaces  more  intensely  social  but  removing  some  of  the  opportunity  for  identity  play,  multitasking,  and  multigaming  while  introducing  ambiguity  over  what  is  being  transmitted  to  whom.
2	Exploring  body  language  as  narrative  interface.  The  limitations  of  current  technology  in  the  field  of  user  interaction  cause  a  bottleneck  in  the  development  of  interactive  digital  storytelling  systems.  The  use  of  body  language  has  recently  arisen  as  a  simple  but  expressive  approach  to  human-computer  interaction,  motion  capture  devices  being  used  in  interactive  simulations  and  videogames.  This  paper  discusses  the  use  of  these  devices  for  recognition  of  body  movements  as  key  elements  for  designing  interactive  stories  in  virtual  environments,  presenting  our  findings  in  the  early  stages  of  development  of  a  serious  game  that  uses  Microsoft  Kinect  for  allowing  players  to  interact  in  a  more  sensitive  and  amusing  way.
2	Social  viewing  in  cinematic  virtual  reality  challenges  and  opportunities.  Cinematic  Virtual  Reality  (CVR)  has  been  increasing  in  popularity  in  the  last  years.  However,  viewers  can  feel  isolated  when  watching  360°  movies  with  a  Head-Mounted  Display.  Since  watching  movies  is  a  social  experience  for  most  people,  we  investigate  if  the  use  of  Head  Mounted  Displays  is  appropriate  for  enabling  shared  CVR  experiences.  In  this  context,  even  if  viewers  are  watching  the  movie  simultaneously,  they  do  not  automatically  see  the  same  field  of  view,  since  they  can  freely  choose  the  viewing  direction.  Based  on  the  literature  and  experiences  from  past  user  studies,  we  identify  seven  challenges.  To  address  these  challenges,  we  present  and  discuss  design  ideas  for  a  CVR  social  movie  player  and  highlight  directions  for  future  work.
2	Towards  an  interaction  model  for  interactive  narratives.  In  the  discussion  of  interactive  narrative  experiences  and  story-driven  games,  much  of  the  current  work  has  focused  on  analyzing  and  proposing  models  and  frameworks  based  on  narrative  theory  and  ludology.  However,  the  players’  experience  and  interaction  with  such  narrative  structure  and  content  is  a  topic  that  is  currently  understudied  in  the  field.  Specifically,  questions  regarding  how  the  player  interacts  and  perceives  the  impact  of  their  interaction  on  the  story  are  currently  unanswered.  This  paper  presents  a  step  towards  defining  an  interaction  model  that  can  be  used  to  design  and  compare  how  a  user  participates  in  an  interactive  narrative.
2	Hybrid  books  for  interactive  digital  storytelling  connecting  story  entities  and  emotions  to  smart  environments.  Nowadays,  many  people  use  e-books,  having  high  expectations  regarding  their  reading  experience.  In  the  case  of  digital  storytelling,  enhanced  e-books  can  connect  story  entities  and  emotions  to  real-world  elements.  In  this  paper,  we  present  the  novel  concept  of  a  Hybrid  Book,  a  generic  Interactive  Digital  Narrative  (IDN)  artifact  that  requires  seamless  collaboration  between  content  and  smart  devices.  To  that  end,  we  extract  data  from  a  story  and  broadcast  these  data  in  RDF  as  Linked  Data.  Smart  devices  can  then  receive  and  process  these  data  in  order  to  execute  corresponding  actions.  By  following  open  standards,  a  Hybrid  Book  can  also  be  seen  as  an  interoperable  and  sustainable  IDN  artifact.  Furthermore,  according  to  our  user-based  evaluation,  a  Hybrid  Book  makes  it  possible  to  provide  human  sensible  feedback  while  flipping  pages,  enabling  a  more  enjoyable  reading  experience.  Finally,  the  participants  positive  willingness  to  pay  makes  it  possible  to  generate  more  revenue  for  publishers.
2	Heuristics  for  evaluating  it  security  management  tools.  The  usability  of  IT  security  management  (ITSM)  tools  is  hard  to  evaluate  by  regular  methods,  making  heuristic  evaluation  attractive.  In  this  article,  we  explore  how  domain  specific  heuristics  are  created  by  examining  prior  research  in  the  area  of  heuristic  and  guideline  creation.  We  then  describe  our  approach  of  creating  usability  heuristics  for  ITSM  tools,  which  is  based  on  guidelines  for  ITSM  tools  that  are  interpreted  and  abstracted  with  activity  theory.  With  a  between-subjects  study,  we  compared  the  employment  of  the  ITSM  and  Nielsen's  heuristics  for  evaluation  of  a  commercial  identity  management  system.  Participants  who  used  the  ITSM  set  found  more  problems  categorized  as  severe  than  those  who  used  Nielsen's.  We  analyzed  several  aspects  of  our  heuristics  including  the  performance  of  individual  participants  using  the  heuristic,  the  performance  of  individual  heuristics,  the  similarity  of  our  heuristics  to  Nielsen's,  and  the  participants'  opinion  about  the  use  of  heuristics  for  evaluation  of  IT  security...
2	Towards  comparable  evaluation  methods  and  measures  for  timing  behavior  of  virtual  reality  systems.  A  low  latency  is  a  fundamental  timeliness  requirement  to  reduce  the  potential  risks  of  cyber  sickness  and  to  increase  effectiveness,  efficiency,  and  user  experience  of  Virtual  Reality  Systems.  The  effects  of  uniform  latency  degradation  based  on  mean  or  worst-case  values  are  well  researched.  In  contrast,  the  effects  of  latency  jitter,  the  distribution  pattern  of  latency  changes  over  time  has  largely  been  ignored  so  far  although  today's  consumer  VR  systems  are  extremely  vulnerable  in  this  respect.  We  investigate  the  applicability  of  the  Walsh,  generalized  ESD,  and  the  modified  z-score  test  for  the  detection  of  outliers  as  one  central  latency  distribution  aspect.  The  tests  are  applied  to  well  defined  test  cases  mimicking  typical  timing  behavior  expected  from  concurrent  architectures  of  today.  We  introduce  accompanying  graphical  visualization  methods  to  inspect,  analyze  and  communicate  the  latency  behavior  of  VR  systems  beyond  simple  mean  or  worst-case  values.  As  a  result,  we  propose  a  stacked  modified  z-score  test  for  more  detailed  analysis.
2	Subject  and  subjectivity  a  conversational  game  using  possible  worlds.  We  present  Subject  and  Subjectivity,  an  exploratory  conversational  game  where  players  are  tasked  with  matching  their  friends  with  the  ideal  bachelor.  The  system  uses  a  modal  logic  approach  to  modeling  the  narrative,  based  upon  Marie-Laure  Ryan’s  possible  worlds  model  for  narrative  [1].  The  dialogue  occurs  in  real-time  and  consists  of  navigating  each  character’s  multiple  and  often  conflicting  world  views.  The  system  allows  flexible  character  authoring,  demonstrated  by  having  the  demo  be  playable  with  either  hand-authored  or  procedurally  generated  characters.  The  demo  serves  as  an  early  experiment  into  the  use  of  possible  worlds  logic  for  interactive  storytelling  and  dialogue  systems.
2	The  benefits  of  dof  separation  in  mid  air  3d  object  manipulation.  Object  manipulation  is  a  key  feature  in  almost  every  virtual  environment.  However,  it  is  difficult  to  accurately  place  an  object  in  immersive  virtual  environments  using  mid-air  gestures  that  mimic  interactions  in  the  physical  world,  although  being  a  direct  and  natural  approach.  Previous  research  studied  mouse  and  touch  based  interfaces  concluding  that  separation  of  degrees-of-freedom  (DOF)  led  to  improved  results.  In  this  paper,  we  present  the  first  user  evaluation  to  assess  the  impact  of  explicit  6  DOF  separation  in  mid-air  manipulation  tasks.  We  implemented  a  technique  based  on  familiar  virtual  widgets  that  allow  single  DOF  control,  and  compared  it  against  a  direct  approach  and  PRISM,  which  dynamically  adjusts  the  ratio  between  hand  and  object  motions.  Our  results  suggest  that  full  DOF  separation  benefits  precision  in  spatial  manipulations,  at  the  cost  of  additional  time  for  complex  tasks.  From  our  results  we  draw  guidelines  for  3D  object  manipulation  in  mid-air.
2	Virtual  immersive  educational  systems  the  case  of  360  video  and  co  learning  design.  The  focus  of  this  research  is  to  depict  the  design  process  of  a  cost-effective,  robust  but  user-friendly  Virtual  Immersive  Educational  (VIE)  system.  Thus,  assist  researchers,  instructors  and  designers  in  identifying  an  effective  method  to  design  VIE  systems.  In  this  report,  we  describe  our  initial  steps  to  design  such  a  system  in  order  to  educate  engineering  students  on  the  basic  health  and  safety  guidelines  of  safe  interaction  with  a  robotic  arm.  To  do  so,  a  set  of  360°  videos  have  been  designed,  developed  and  tested.
2	Automated  interior  design  using  a  genetic  algorithm.  In  this  paper,  we  present  a  system  that  automatically  populates  indoor  virtual  scenes  with  furniture  objects  and  optimizes  their  positions  and  orientations  with  respect  to  aesthetic,  ergonomic  and  functional  rules  called  interior  design  guidelines.  These  guidelines  are  represented  as  mathematical  expressions  which  form  the  cost  function.  Our  system  optimizes  the  set  of  multiple  interior  designs  by  minimizing  the  cost  function  using  a  genetic  algorithm.  Moreover,  we  extend  the  optimization  to  transdimensional  space  by  enabling  automatic  selection  of  furniture  objects.  Finally,  we  optimize  the  assignment  of  materials  to  the  furniture  objects  to  achieve  a  unified  design  and  harmonious  color  distribution.  We  investigate  the  capability  of  our  system  to  generate  sensible  and  livable  interior  designs  in  a  perceptual  study.
2	Fast  generation  of  realistic  virtual  humans.  In  this  paper  we  present  a  complete  pipeline  to  create  ready-to-animate  virtual  humans  by  fitting  a  template  character  to  a  point  set  obtained  by  scanning  a  real  person  using  multi-view  stereo  reconstruction.  Our  virtual  humans  are  built  upon  a  holistic  character  model  and  feature  a  detailed  skeleton,  fingers,  eyes,  teeth,  and  a  rich  set  of  facial  blendshapes.  Furthermore,  due  to  the  careful  selection  of  techniques  and  technology,  our  reconstructed  humans  are  quite  realistic  in  terms  of  both  geometry  and  texture.  Since  we  represent  our  models  as  single-layer  triangle  meshes  and  animate  them  through  standard  skeleton-based  skinning  and  facial  blendshapes,  our  characters  can  be  used  in  standard  VR  engines  out  of  the  box.  By  optimizing  for  computation  time  and  minimizing  manual  intervention,  our  reconstruction  pipeline  is  capable  of  processing  whole  characters  in  less  than  ten  minutes.
2	Computer  vision  for  3dtv  and  augmented  reality.  Recent  computer  vision  technology  makes  innovative  progress  in  3D  visual  media  industry.  In  this  paper,  I  would  like  to  introduce  our  approaches  for  making  use  of  the  computer  vision  technology  in  order  to  achieve  innovative  application  systems  of  3DTV  and  augmented  reality.  First,  I  would  demonstrate  the  effectiveness  of  multiple  viewpoint  videos  and  depth  videos  in  3DTV  applications,  in  which  3D  shape  reconstructions  and  view  synthesis  as  computer  vision  technologies  are  used.  Augmented  reality  is  a  method  for  presenting  digital  information  over  the  real  world  with  a  see-through  display.  For  such  application  of  AR,  real-time  camera  tracking  is  one  of  significant  technology  which  is  also  based  on  a  state-of-art  in  computer  vision.
2	Analyzing  children  s  contributions  and  experiences  in  co  design  activities  synthesizing  productive  practices.  Today,  it  has  been  broadly  acknowledged  in  the  CCI  community  that  children  are  not  only  active  learners  and  users  of  technology,  but  can  also  actively  participate  in  the  design  process.  However,  it  remains  challenging  to  analyze  children's  experiences  and  creative  contributions  resulting  from  co-design  activities  (e.g.  stories,  paper  prototypes,  enacted  ideas).  Broadly  speaking,  a  distinction  can  be  made  between  researchers  looking  for  inspiration  in  the  form  of  useful  design  ideas,  and  researchers  that  take  a  more  interpretative  stance  by  looking  beyond  the  surface  level  of  children's  ideas  to  better  understand  and  empathize  with  them.  This  knowledge  about  children  is  often  used  to  more  accurately  define  the  problem  space  at  the  early  stages  of  design.  Both  perspectives  to  co-design  can  be  seen  as  the  opposite  ends  of  the  same  continuum,  and  many  researchers  combine  aspects  of  both  depending  on  where  they  are  in  the  design  process  (e.g.  defining  the  design  problem,  prototyping  stage).  This  workshop  will  explore  different  ways  to  analyze  children's  (0  to  18  years)  experiences  and  contributions  in  co-design  activities,  the  perceived  benefits  and  challenges  of  these  approaches,  and  will  serve  as  a  venue  for  synthesizing  productive  practices  that  will  move  the  CCI  community  forward.
2	Cultural  and  gender  differences  in  password  behaviors  evidence  from  china  turkey  and  the  uk.  A  survey  investigated  the  password  behaviors  of  a  sample  of  202  men  and  women  from  three  countries  with  very  different  cultures:  China,  Turkey  and  the  UK.  The  survey  covered  four  areas:  the  context  of  password  use,  password  creation,  password  management  and  attitudes  to  passwords.  A  complex  pattern  of  country  and  gender  differences  emerged,  with  most  country  differences  in  the  context  of  password  use  and  password  creation  behaviors,  and  gender  differences  in  context  of  password  use,  password  creation  and  management  behaviors.  There  was  little  support  for  three  hypotheses  concerning  cultural  differences  in  password  behaviors  derived  from  the  dimensions  of  Power  Distance,  Individualism-Collectivism,  and  Uncertainty  Avoidance.  However,  the  results  suggest  that  both  cultural  background  and  gender  need  to  be  taken  into  account  when  studying  users'  password  behaviors.
2	Challenging  group  dynamics  in  participatory  design  with  children  lessons  from  social  interdependence  theory.  In  this  paper  we  explore  whether  Social  Interdependence  Theory  (SIT)  is  a  useful  theoretical  framework  to  anticipate  on  challenging  intragroup  dynamics  in  co-design  with  children.  According  to  SIT,  there  are  five  principles  that  mediate  the  effectiveness  of  cooperation:  positive  interdependence,  individual  accountability,  promotive  interaction  patterns,  social  skills  and  group  processing.  First,  we  theoretically  ground  six  challenging  group  dynamics  encountered  in  a  previous  study.  Next,  we  introduce  SIT  and  describe  how  we  applied  each  of  the  five  mediating  principles  in  a  new  case  study  in  which  49  children  aged  9  to  10  were  involved  in  a  series  of  co-design  sessions.  Afterwards,  we  present  our  findings  and  reflect  upon  the  SIT  inspired  co-design  procedure.  Finally  we  touch  upon  topics  for  further  research  and  we  make  a  call  for  more  research  on  SIT  in  the  Child  Computer  Interaction  (CCI)  community.
2	Bicultural  examining  teenage  latinas  perspectives  on  technologies  for  emotional  support.  Latina  adolescents  are  one  of  the  fastest  growing  demographic  groups  in  the  USA  and  have  been  found  to  experience  higher  levels  of  emotional  distress  than  their  non-Latino  counterparts.  Reliance  on  emotional  support  from  the  teens'  social  network  is  a  common  coping  mechanism.  While  digital  media  is  increasingly  used  by  Latinas  to  communicate  and  express  themselves,  few,  if  any,  studies  have  explored  the  perspectives  of  Latinas  on  the  role  of  technology  in  helping  the  enactment  of  emotional  support.  In  this  paper,  we  share  the  perspectives  of  thirteen  Latina  adolescents  (15-18  years  old)  that  emerged  through  a  series  of  participation  design  workshops.  Our  findings  suggest  that  bicultural  aspects  of  Latina  identity  can  influence  perspectives  on  the  role  of  technology,  and  provide  insights  into  attitudes  towards  bicultural  conflicts  in  emotional  health.  We  also  share  our  participatory  design  technique  that  incorporated  an  ecological  framework  and  suggestions  for  future  use  of  such  an  approach.
2	Understanding  grassroots  sports  gamification  in  the  wild.  While  gamification  is  an  often  used  tool  in  building  interactive  experiences  for  sports,  little  work  has  addressed  systems  designed  by  users  for  users  and  deeply  embedded  in  the  social  setting  of  physical  exercise.  Consequently,  a  better  understanding  of  sports  gamification  in  the  wild  is  needed  to  build  systems  that  reflect  the  users'  pre-existing  social  context.  This  paper  presents  a  qualitative  study  of  a  gamification  system,  the  Boar  Board,  designed  by  a  sports  coach  to  support  users  participating  in  regular  exercises.  Through  surveys,  interviews  and  observations  over  eight  months,  we  built  an  understanding  of  the  user  adoption  of  the  system  and  how  the  Boar  Board  supported  the  goals  of  the  group.  Based  on  this,  we  endeavour  to  understand  the  social  aspects  of  the  system,  including  trust,  and  posit  a  number  of  design  considerations  for  future  inquiry  into  gamification  systems  for  sports.
2	Involving  end  users  in  game  based  ideation  a  case  study  in  hospital  logistics.  In  this  paper  we  investigate  the  use  of  a  board  game  to  facilitate  ideation  with  end-users  for  the  study  of  applications  of  future  technology.  In  the  described  case  study,  we  focus  on  the  use  of  wearable  technology  for  logistical  work  in  hospitals.  Based  on  prior  ethnographic  research,  we  designed  a  tailor-made  board  game.  This  game  was  played  with  personnel  in  two  hospitals.  The  game-based  ideation  provided  valuable  insights  concerning  preferred  workflow  and  interaction,  and  we  observed  a  high  level  of  engagement  during  the  sessions.  We  present  and  explain  three  recommendations  for  the  use  of  game-based  ideation:  use  collaboration  to  improve  discussion,  abstract  technology,  and  stage  the  game.
2	Co  designing  smart  home  technology  with  people  with  dementia  or  parkinson  s  disease.  Involving  users  is  crucial  to  designing  technology  successfully,  especially  for  vulnerable  users  in  health  and  social  care,  yet  detailed  descriptions  and  critical  reflections  on  the  co-design  process,  techniques  and  methods  are  rare.  This  paper  introduces  the  PERCEPT  (PERrsona-CEntred  Participatory  Technology)  approach  for  the  co-design  process  and  we  analyse  and  discuss  the  lessons  learned  for  each  step  in  this  process.  We  applied  PERCEPT  in  a  project  to  develop  a  smart  home  toolset  that  will  allow  a  person  living  with  early  stage  dementia  or  Parkinson's  to  plan,  monitor  and  self-manage  his  or  her  life  and  well-being  more  effectively.  We  present  a  set  of  personas  which  were  co-created  with  people  and  applied  throughout  the  project  in  the  co-design  process.  The  approach  presented  in  this  paper  will  enable  researchers  and  designers  to  better  engage  with  target  user  groups  in  co-design  and  point  to  considerations  to  be  made  at  each  step  for  vulnerable  users.
2	Frogs  to  think  with  improving  students  computational  thinking  and  understanding  of  evolution  in  a  code  first  learning  environment.  This  paper  presents  Frog  Pond,  an  interactive  code-first  learning  environment  about  biological  evolution.  We  deployed  Frog  Pond  as  part  of  a  six-day  curricular  unit  on  natural  selection  implemented  in  six  7th  grade  science  classes.  Here  we  describe  a  case  study  of  two  students,  Charlie  and  Aaron  who  participated  in  the  unit.  Comparing  pre-  and  post-  interviews  in  which  they  were  asked  to  design  a  program  for  a  hypothetical  simulation  of  evolution,  we  found  that  both  students  shifted  from  an  event-based  programming  approach  to  a  rule-based  approach.  Both  students  also  drew  upon  their  experience  with  Frog  Pond  to  explain  an  evolutionary  phenomenon.  However,  the  level  of  sophistication  of  the  two  students'  explanations  varied  along  with  the  aspects  of  Frog  Pond  they  drew  upon.  These  findings  have  implications  for  design  improvement  to  better  support  students'  understanding  of  evolution.
2	Semantize  visualizing  the  sentiment  of  individual  document.  A  plethora  of  tools  exist  for  extracting  and  visualizing  key  sentiment  information  from  a  corpus  of  text  documents.  Often,  however,  there  is  a  need  for  quickly  assessing  the  sentiment  and  feelings  that  arise  from  an  individual  document.  We  describe  an  interactive  tool  that  visualizes  the  sentiment  of  a  specific  document  such  as  an  online  opinion,  blog,  or  transcript,  by  visually  highlighting  the  sentiment  features  while  leaving  the  document  text  intact.
2	Representing  and  visualizing  folksonomies  as  graphs  a  reference  model.  We  present  a  reference  model  for  the  representation  and  visualization  of  folksonomies  as  graphs.  We  discuss  the  formal  representation  of  folksonomies  and  derive  a  hypergraph  structure  that  is  core  to  any  visualization  approach.  We  split  this  hypergraph  into  subgraphs  that  can  guide  the  development  of  folksonomy  visualizations.  We  use  these  subgraphs  to  classify  existing  graph  visualizations  of  folksonomies  from  the  literature  and  web.  We  found  that  most  works  display  the  interrelated  set  of  tags,  while  the  relations  between  resources  and/or  users  are  largely  neglected  by  current  visualization  approaches.  Based  on  these  and  further  findings,  we  discuss  research  challenges  and  potentials  for  future  graph  visualizations  of  folksonomies.
2	Hci  and  the  educational  technology  revolution  hcied2018  a  workshop  on  video  making  for  teaching  and  learning  human  computer  interaction.  Over  the  years,  the  HCI  Educators  series  has  studied  a  number  of  challenges  for  the  teaching  and  learning  of  Human-Computer  Interaction  at  a  time  of  radical  educational  change.  Though  video  has  historically  played  an  important  part  on  the  teaching  and  development  of  HCI,  only  recently  video-making  and  editing  technologies  have  become  accessible  in  an  unprecedented  way,  allowing  students  to  become  proficient  video  "prosumers"  (producers  and  consumers).  Further,  there  are  numerous  educational  gains  to  be  had  through  these  technologies.  Through  a  very  interactive  workshop  we  explore  how  can  video  be  used  in  practice  to  leverage  skills  and  foster  creativity  whilst  facilitating  knowledge  acquisition.
2	Conveying  real  time  ambivalent  feelings  through  asymmetric  facial  expressions.  Achieving  effective  facial  emotional  expressivity  within  a  real-time  rendering  constraint  requests  to  leverage  on  all  possible  inspiration  sources  and  especially  from  the  observations  of  real  individuals.  One  of  them  is  the  frequent  asymmetry  of  facial  expressions  of  emotions,  which  allows  to  express  complex  emotional  feelings  such  as  suspicion,  smirk,  and  hidden  emotion  due  to  social  conventions.  To  achieve  such  a  higher  degree  of  facial  expression,  we  propose  a  new  model  for  mapping  emotions  onto  a  small  set  of  1D  Facial  Part  Actions  (FPA)s  that  act  on  antagonist  muscle  groups  or  on  individual  head  orientation  degree  of  freedoms.  The  proposed  linear  model  can  automatically  drive  a  large  number  of  autonomous  virtual  humans  or  support  the  interactive  design  of  complex  facial  expressions  over  time.
2	The  collaborative  work  of  heritage  open  challenges  for  cscw.  This  paper  discusses  seminal  contributions  by  and  current  open  challenges  for  CSCW  in  the  study  of  cultural  heritage  practices.  It  provides  an  overview  of  key  issues  relating  to  social  and  cooperative  interactions—particularly  around  the  design  and  use  of  technology—at  heritage  sites  that  have  emerged  in  CSCW,  and  pertaining  the  conduct  of  visitors,  the  design  and  evaluation  of  interactive  installations  for  guidance  and  access,  and  the  creation  of  novel  artistic  performances.  The  paper  then  presents  a  set  of  open  challenges  for  future  CSCW  work,  particularly  regarding  the  very  re-definition  of  heritage  in  light  of  the  social  and  collaborative  practices  that  have  arisen  in  recent  years  within  the  museum  and  heritage  professionals  community,  and  the  emergence  of  new  roles  and  practices  for  organisations,  staff,  visitors  and  related  stakeholders.  The  paper  aims  at  consolidating  the  range  of  contributions  that  CSCW  has  made  to  cultural  heritage  and  at  outlining  key  issues  and  challenges  for  future  research  in  this  domain.
2	It  s  about  business  not  politics  software  development  between  palestinians  and  israelis.  This  paper  focuses  on  the  collaboration  in  an  Israeli-Palestinian  tech  start-up  company.  We  investigate  the  strategies  enacted  by  the  IT  developers  for  managing  the  political  dynamics  and  making  collaboration  possible  under  the  highly  challenging  political  conditions.  We  found  that  one  of  the  key  strategies  was  explicitly  separating  the  work  domain  of  software  development  from  the  domain  of  politics.  We  argue  that  the  IT  developers  manage  to  collaborate  by  displacing  the  political  conflict  through  strategies  of  non-confrontation  instead  of  engaging  in  translating  conflicting  agendas  against  each  other.  By  insisting  on  keeping  politics  outside  of  the  workspace,  the  IT  developers  adopt  a  strategy  of  keeping  the  collaboration  together  by  keeping  politics  and  work  apart.  However,  we  found  that  despite  the  attempts  to  manage  the  sub-group  dynamics,  politics  constantly  invade  the  workspace  and  challenge  the  collaboration.  Significant  resources  are  invested  into  managing  the  regimes  of  differentiated  identity  cards,  permits,  and  checkpoints,  all  of  which  have  consequences  on  the  employees’  freedom  or  restriction  of  mobility.  Thus,  we  argue  that  the  IT  development  domain  is  inseparable  from  and  deeply  dependent  upon  the  political  domain.
2	In2co  a  visualization  framework  for  intuitive  collaboration.  Today,  the  need  for  interaction  and  visualization  techniques  to  fulfill  user  requirements  for  collaborative  work  is  ever  increasing.  Current  approaches  do  not  suffice  since  they  do  not  consider  the  simultaneous  work  of  participating  users,  different  views  of  the  data  being  analyzed,  or  the  exchange  of  information  between  different  data  emphases.  We  introduce  Intuitive  Collaboration  (IN2CO),  a  scalable  visualization  framework  that  supports  decision-making  processes  concerning  multilevels  and  multi-roles.  IN2CO  improves  the  state  of  the  art  by  integrating  ubiquitous  technologies  and  existing  techniques  to  explore  and  manipulate  data  and  dependencies  collaboratively.  A  prototype  has  been  tested  by  mechanical  engineers  with  expertise  in  factory  planning.  Preliminary  results  imply  that  IN2CO  supports  communication  and  decision-making  in  a  team-oriented  manner.
2	Animated  transitions  between  user  interface  views.  User  interface  development  life  cycle  often  involve  several  different  views  of  the  user  interface  over  time  either  at  the  same  level  of  abstraction  or  at  different  levels  of  abstraction.  The  relationship  between  these  different  views  is  often  supported  by  tiling  coordinated  windows  containing  these  related  views  simultaneously,  thus  leaving  the  developer  with  the  responsibility  to  effectively  and  efficiently  link  the  corresponding  elements  of  these  different  views.  This  paper  attempts  to  overcome  the  shortcomings  posed  by  the  coordinated  visualization  of  multiple  views  by  providing  UsiView,  a  user  interface  rendering  engine  in  which  one  single  window  ensures  an  animated  transition  between  these  different  user  interface  views  dynamically  an  internal  view,  an  external  view,  and  a  conceptual  view.  Examples  include  the  following  cases:  an  authoring  environment  ensures  an  animated  transition  between  an  internal  view  (e.g.,  HTML5)  and  its  external  view  (e.  g.,  a  web  page),  an  Integrated  Development  Environment  ensures  an  animated  transition  between  its  conceptual  view  and  its  external  view;  a  model-driven  engineering  environment  ensures  an  animated  transition  between  the  conceptual  view  at  different  levels  of  abstraction,  e.g.,  from  task  to  abstract  user  interface  to  concrete  user  interface  until  final  user  interface.  The  paper  discusses  the  potential  advantages  of  using  animated  transitions  between  user  interface  views  during  the  development  life  cycle.
2	The  future  of  computing  and  food  extended  abstract.  The  excitement  around  computing  technology  in  all  aspects  of  life  requires  that  we  tackle  fundamental  issues  of  healthcare,  leisure,  labor,  education,  and  food  to  create  the  society  we  want.  The  aim  of  this  satellite  event  was  to  bring  together  a  variety  of  different  stakeholders,  ranging  from  local  food  producers,  chefs,  designers,  engineers,  data  scientists,  and  sensory  scientists,  to  discuss  the  interwoven  future  of  computing  technology  and  food.  This  event  was  co-located  with  the  AVI  2018  conference  and  supported  by  the  ACM  Future  of  Computing  Academy  (ACM-FCA).  The  event  followed  a  co-creation  approach  that  encourages  conjoined  creative  and  critical  thinking  that  feeds  into  the  formulation  of  a  manifesto  on  the  future  of  computing  and  food.  We  hope  this  will  inspire  future  discussions  on  the  transformative  role  of  computing  technology  on  food.
2	Capturing  close  interactions  with  objects  using  a  magnetic  motion  capture  system  and  a  rgbd  sensor.  Games  and  interactive  virtual  worlds  increasingly  rely  on  interactions  with  the  environment,  and  require  animations  for  displaying  them.  Manually  synthesizing  such  animations  is  a  daunting  task  due  to  the  difficulty  of  handling  the  close  interactions  between  a  character’s  body  and  the  object.  Capturing  such  movements  using  optical  motion  capture  systems,  which  are  the  most  prevalent  devices  for  motion  capturing,  is  also  not  very  straightforward  due  to  occlusions  happening  between  the  body  markers  and  the  object  or  body  itself.  In  this  paper,  we  describe  a  scheme  to  capture  such  movements  using  a  magnetic  motion  capture  system.  The  advantage  of  using  a  magnetic  motion  capture  system  is  that  it  can  obtain  the  global  data  without  any  concern  for  the  occlusion  problem.  This  allows  us  to  digitally  recreate  captured  close  interactions  without  significant  artist  work  in  creating  the  scene  after  capture.  We  show  examples  of  capturing  movements  including  opening  a  bottle,  drawing  on  a  paper,  taking  on  /  off  pen  caps,  carrying  objects  and  interacting  with  furniture.  The  captured  data  is  currently  published  as  a  publicly  available  database.
2	Acting  the  part  the  role  of  gesture  on  avatar  identity.  Recent  advances  in  scanning  technology  have  enabled  the  widespread  capture  of  3D  character  models  based  on  human  subjects.  However,  in  order  to  generate  a  recognizable  3D  avatar,  the  movement  and  behavior  of  the  human  subject  should  be  captured  and  replicated  as  well.  We  present  a  method  of  generating  a  3D  model  from  a  scan,  as  well  as  a  method  to  incorporate  a  subjects  style  of  gesturing  into  a  3D  character.  We  present  a  study  which  shows  that  3D  characters  that  used  the  gestural  style  as  their  original  human  subjects  were  more  recognizable  as  the  original  subject  than  those  that  don't.
2	Irregular  trend  finder  visualization  tool  for  analyzing  time  series  big  data.  We  created  a  visualization  tool  called  Irregular  Trend  Finder  (ITF)  for  the  VAST  Challenge  2012  Mini  Challenge  1.  ITF  is  an  interactive  tool  designed  for  analyzing  large  amounts  of  data  with  times-tamp  and  hierarchy  structure  in  mind,  so  that  a  user  can  see  the  overview  first,  and  then  obtain  more  detailed  information.  We  discovered  the  answers  for  the  Challenge,  and  confirmed  that  overview  first  and  then  looking  in  detail  is  an  efficient  method  to  seek  particular  information  in  case  of  large  amounts  of  data.
2	Feedback  driven  interactive  exploration  of  large  multidimensional  data  supported  by  visual  classifier.  The  extraction  of  relevant  and  meaningful  information  from  multivariate  or  high-dimensional  data  is  a  challenging  problem.  One  reason  for  this  is  that  the  number  of  possible  representations,  which  might  contain  relevant  information,  grows  exponentially  with  the  amount  of  data  dimensions.  Also,  not  all  views  from  a  possibly  large  view  space,  are  potentially  relevant  to  a  given  analysis  task  or  user.  Focus+Context  or  Semantic  Zoom  Interfaces  can  help  to  some  extent  to  efficiently  search  for  interesting  views  or  data  segments,  yet  they  show  scalability  problems  for  very  large  data  sets.  Accordingly,  users  are  confronted  with  the  problem  of  identifying  interesting  views,  yet  the  manual  exploration  of  the  entire  view  space  becomes  ineffective  or  even  infeasible.  While  certain  quality  metrics  have  been  proposed  recently  to  identify  potentially  interesting  views,  these  often  are  defined  in  a  heuristic  way  and  do  not  take  into  account  the  application  or  user  context.  We  introduce  a  framework  for  a  feedback-driven  view  exploration,  inspired  by  relevance  feedback  approaches  used  in  Information  Retrieval.  Our  basic  idea  is  that  users  iteratively  express  their  notion  of  interestingness  when  presented  with  candidate  views.  From  that  expression,  a  model  representing  the  user's  preferences,  is  trained  and  used  to  recommend  further  interesting  view  candidates.  A  decision  support  system  monitors  the  exploration  process  and  assesses  the  relevance-driven  search  process  for  convergence  and  stability.  We  present  an  instantiation  of  our  framework  for  exploration  of  Scatter  Plot  Spaces  based  on  visual  features.  We  demonstrate  the  effectiveness  of  this  implementation  by  a  case  study  on  two  real-world  datasets.  We  also  discuss  our  framework  in  light  of  design  alternatives  and  point  out  its  usefulness  for  development  of  user-  and  context-dependent  visual  exploration  systems.
2	Save  sensor  anomaly  visualization  engine.  Diagnosing  a  large-scale  sensor  network  is  a  crucial  but  challenging  task.  Particular  challenges  include  the  resource  and  bandwidth  constraints  on  sensor  nodes,  the  spatiotemporally  dynamic  network  behaviors,  and  the  lack  of  accurate  models  to  understand  such  behaviors  in  a  hostile  environment.  In  this  paper,  we  present  the  Sensor  Anomaly  Visualization  Engine  (SAVE),  a  system  that  fully  leverages  the  power  of  both  visualization  and  anomaly  detection  analytics  to  guide  the  user  to  quickly  and  accurately  diagnose  sensor  network  failures  and  faults.  SAVE  combines  customized  visualizations  over  separate  sensor  data  facets  as  multiple  coordinated  views.  Temporal  expansion  model,  correlation  graph  and  dynamic  projection  views  are  proposed  to  effectively  interpret  the  topological,  correlational  and  dimensional  sensor  data  dynamics  and  their  anomalies.  Through  a  case  study  with  real-world  sensor  network  system  and  administrators,  we  demonstrate  that  SAVE  is  able  to  help  better  locate  the  system  problem  and  further  identify  the  root  cause  of  major  sensor  network  failure  scenarios.
2	Emergent  practice  as  a  methodological  lens  for  public  displays  in  the  wild.  In  this  paper,  we  seek  to  complement  the  current  public  display  evaluation  toolbox  with  the  analytical  lens  of  emergent  practices.  This  evaluation  method  facilitates  capturing  the  variety  of  ways  in  which  a  public  display  operates  and  creates  value  in  a  given  context.  From  this  viewpoint,  a  public  display  appears  as  a  material  resource  for  the  urban  practice,  as  well  as  a  site  for  socio-cultural  enactment.  Our  main  argument  is  that  this  type  of  evaluation  can  lead  to  extraction  of  more  sustained  value  from  actual  use  of  public  displays.  We  discuss  the  theoretical  foundations  of  emergent  practices,  their  place  and  use  within  the  public  display  evaluation  toolbox,  and  situate  practices  as  a  valuable  next  step  of  in-the-wild  studies.  Through  an  empirical  dataset,  we  also  discuss  real  world  emergent  practices  observed  as  part  of  the  UBI-display  network  in  Oulu,  Finland.
2	Image  based  x  ray  visualization  techniques  for  spatial  understanding  in  outdoor  augmented  reality.  This  paper  evaluates  different  state-of-the-art  approaches  for  implementing  an  X-ray  view  in  Augmented  Reality  (AR).  Our  focus  is  on  approaches  supporting  a  better  scene  understanding  and  in  particular  a  better  sense  of  depth  order  between  physical  objects  and  digital  objects.  One  of  the  main  goals  of  this  work  is  to  provide  effective  X-ray  visualization  techniques  that  work  in  unprepared  outdoor  environments.  In  order  to  achieve  this  goal,  we  focus  on  methods  that  automatically  extract  depth  cues  from  video  images.  The  extracted  depth  cues  are  combined  in  ghosting  maps  that  are  used  to  assign  each  video  image  pixel  a  transparency  value  to  control  the  overlay  in  the  AR  view.  Within  our  study,  we  analyze  three  different  types  of  ghosting  maps,  1)  alpha-blending  which  uses  a  uniform  alpha  value  within  the  ghosting  map,  2)  edge-based  ghosting  which  is  based  on  edge  extraction  and  3)  image-based  ghosting  which  incorporates  perceptual  grouping,  saliency  information,  edges  and  texture  details.  Our  study  results  demonstrate  that  the  latter  technique  helps  the  user  to  understand  the  subsurface  location  of  virtual  objects  better  than  using  alpha-blending  or  the  edge-based  ghosting.
2	A  case  study  tracking  and  visualizing  the  evolution  of  dark  matter  halos  and  groups  of  satellite  halos  in  cosmology  simulations.  In  this  poster,  we  track  the  evolution  of  cosmic  structures  and  higher  level  host  structures  in  cosmological  simulation  as  they  interact  with  each  other.  The  structures  found  in  these  simulations  are  made  up  of  groups  of  dark  matter  tracer  particles  called  satellite  halos  and  groups  of  satellite  halos  called  host  halos.  We  implement  a  multilevel  tracking  model  to  track  dark  matter  tracer  particles,  satellite  halos  and  host  halos  to  understand  their  behaviour  and  show  how  the  different  structures  are  formed  over  time.  We  also  represent  the  evolution  of  halos  in  the  form  of  merger  trees  for  detailed  analysis  by  cosmologists.
2	Mixed  initiative  visual  analytics  using  task  driven  recommendations.  Visual  data  analysis  is  composed  of  a  collection  of  cognitive  actions  and  tasks  to  decompose,  internalize,  and  recombine  data  to  produce  knowledge  and  insight.  Visual  analytic  tools  provide  interactive  visual  interfaces  to  data  to  support  discovery  and  sensemaking  tasks,  including  forming  hypotheses,  asking  questions,  and  evaluating  and  organizing  evidence.  Myriad  analytic  models  can  be  incorporated  into  visual  analytic  systems  at  the  cost  of  increasing  complexity  in  the  analytic  discourse  between  user  and  system.  Techniques  exist  to  increase  the  usability  of  interacting  with  analytic  models,  such  as  inferring  data  models  from  user  interactions  to  steer  the  underlying  models  of  the  system  via  semantic  interaction,  shielding  users  from  having  to  do  so  explicitly.  Such  approaches  are  often  also  referred  to  as  mixed-initiative  systems.  Sensemaking  researchers  have  called  for  development  of  tools  that  facilitate  analytic  sensemaking  through  a  combination  of  human  and  automated  activities.  However,  design  guidelines  do  not  exist  for  mixed-initiative  visual  analytic  systems  to  support  iterative  sensemaking.  In  this  paper,  we  present  candidate  design  guidelines  and  introduce  the  Active  Data  Environment  (ADE)  prototype,  a  spatial  workspace  supporting  the  analytic  process  via  task  recommendations  invoked  by  inferences  about  user  interactions  within  the  workspace.  ADE  recommends  data  and  relationships  based  on  a  task  model,  enabling  users  to  co-reason  with  the  system  about  their  data  in  a  single,  spatial  workspace.  This  paper  provides  an  illustrative  use  case,  a  technical  description  of  ADE,  and  a  discussion  of  the  strengths  and  limitations  of  the  approach.
2	Towards  a  user  defined  visual  interactive  definition  of  similarity  functions  for  mixed  data.  The  creation  of  similarity  functions  based  on  visual-interactive  user  feedback  is  a  promising  means  to  capture  the  mental  similarity  notion  in  the  heads  of  domain  experts.  In  particular,  concepts  exist  where  users  arrange  multivariate  data  objects  on  a  2D  data  landscape  in  order  to  learn  new  similarity  functions.  While  systems  that  incorporate  numerical  data  attributes  have  been  presented  in  the  past,  the  remaining  overall  goal  may  be  to  develop  systems  also  for  mixed  data  sets.  In  this  work,  we  present  a  feedback  model  for  categorical  data  which  can  be  used  alongside  of  numerical  feedback  models  in  future.
2	The  leap  motion  controller  a  view  on  sign  language.  This  paper  presents  an  early  exploration  of  the  suitability  of  the  Leap  Motion  controller  for  Australian  Sign  Language  (Auslan)  recognition.  Testing  showed  that  the  controller  is  able  to  provide  accurate  tracking  of  hands  and  fingers,  and  to  track  movement.  This  detection  loses  accuracy  when  the  hand  moves  into  a  position  that  obstructs  the  controller's  ability  to  view,  such  as  when  the  hand  rotates  and  is  perpendicular  to  the  controller.  The  detection  also  fails  when  individual  elements  of  the  hands  are  brought  together,  such  as  finger  to  finger.  In  both  of  these  circumstances,  the  controller  is  unable  to  read  or  track  the  hand.  There  is  potential  for  the  use  of  this  technology  for  recognising  Auslan,  however  further  development  of  the  Leap  Motion  API  is  required.
2	Active  ageing  with  avatars  a  virtual  exercise  class  for  older  adults.  In  this  paper  we  describe  the  development  and  testing  of  a  virtual  exercise  class  for  older  people  delivered  in  real-time  using  active  gaming  technology  (Microsoft's  Kinect  for  Xbox  360).  Four  seniors  (women,  aged  64--74)  participated.  Each  person  received  a  gaming  console  and  face-to-face  instruction.  At  a  set  time  each  week  for  6  weeks,  participants  logged  on  to  take  part  in  a  30-minute  exercise  class  led  by  an  instructor.  On-screen,  each  person  was  represented  by  an  avatar  (Avatar  Kinect  software).  Participants  were  able  to  see  the  avatars  on  their  television  screen  and  hear  each  other  via  headsets.  Overall,  we  found  the  virtual  exercise  class  format  to  be  feasible  and  acceptable  to  older  people.  Despite  some  limitations  with  the  hardware  and  software,  participants  enjoyed  the  experience  of  learning  to  use  new  technology.  The  use  of  avatars  added  novelty  and  made  the  program  fun.  The  potential  for  a  virtual  exercise  class,  using  gaming  technology  and  avatars,  to  increase  physical  activity  and  enhance  social  connection  amongst  older  Australians  warrants  further  investigation.
2	Ability  based  optimization  design  and  evaluation  of  touchscreen  keyboards  for  older  adults  with  dyslexia.  This  short  paper  investigates  a  computational  approach  toward  improving  user  interface  designs  for  older  adults  with  cognitive  impairments.  We  explore  keyboard  design  on  touchscreen  devices  where  individual  abilities  are  parametrically  expressed  as  part  of  a  task-specific  cognitive  model,  and  the  model  estimates  how  the  individual  might  adapt  user  interaction  to  the  task.  The  resultant  design  can  potentially  improve  speed  and  reduce  errors  for  older  adults  having  dyslexia  over  the  baseline  QWERTY  layout.  We  believe  that  the  proposed  computational  touchscreen  design  approach  will  be  able  to  develop  interface  designs  more  adapted  to  the  specific  abilities  of  the  aging  populations.
2	Supporting  conversation  and  community  interaction  with  a  table  top  community  garden  application.  Third  places  are  social  places  like  coffee  shop  and  bars,  where  people  come  together  to  catch  up  with  friends  and  meet  new  people.  Our  research  explores  how  Ubiquitous  Computing  experiences  in  public  space,  particularly  interactive  public  displays,  can  be  leveraged  to  encourage  interaction  between  strangers.  We  present  a  multi-display  application  based  on  the  metaphor  of  a  table-top  community  garden.  This  application  is  built  using  our  Really  Easy  Displays  (RED)  framework,  a  set  of  web  based  technologies  that  allow  the  rapid  development  of  applications  that  span  multiple  displays,  sensors  and  actuators.  The  prototype  makes  use  of  a  situated  large  screen,  a  projected  surface  and  an  Arduino  microcontroller  to  support  collaborative  interaction,  by  allowing  groups  of  people  to  collectively  nurture  a  table-top  garden  by  interacting  with  furniture,  touch-enabled  projections  and  mobile  phones.
2	Hybrid  optical  resonator  for  nanostructured  virus  detection  and  sizing.  We  investigate  a  method  for  the  detection  of  influenza  A  virus  in  order  to  reduce  the  risks  associated  with  its  toxicity.  Our  work  is  based  on  the  analysis  of  the  optical  properties  of  a  whispering  gallery  mode  microresonator  interacting  with  a  spherical  nanoparticle  modeling  the  virion.  The  microresonator  shows  a  Q-factor  quite  high,  of  the  order  of  6·104.  Higher  is  the  Q-factor,  higher  is  the  perturbation  that  the  light  propagating  inside  the  resonator  experiences  when  interacting  with  a  scattering  center,  i.e.  a  nanoparticle.  Thus,  from  the  transmission  spectrum  of  the  resonator-virion  system,  we  are  able  to  derive  the  size  of  nanoparticles  having  a  radius  in  the  range  30  –  100  nm,  with  a  small  error  with  respect  to  the  nanoparticle  nominal  radius.
2	Improvement  on  conductivity  image  reconstruction  in  magnetic  induction  tomography.  Magnetic  induction  tomography  (MIT)  images  the  conductivity  and  permeability  properties  of  the  targets  inside  an  object  based  on  electrical  impedance  measurements  from  excitation  and  detection  coils.  MIT  has  potential  and  practical  industrial  and  biomedical  applications.  In  this  research,  we  have  first  classified  the  previous  methods  on  conductivity  image  reconstruction  and  then  have  made  an  effort  to  improve  the  MIT  reconstructed  images.  In  previous  studies,  generally  the  real  part  of  the  induced  voltages  has  been  used  for  the  conductivity  image  reconstruction  in  low-conductivity  and  the  imaginary  part  has  been  used  in  high-conductivity  applications.  In  this  paper,  an  improvement,  which  uses  both  real  and  imaginary  parts  of  the  induced  voltages,  in  conductivity  image  reconstruction  is  presented  for  both  low  and  high  conductivity  applications.  Through  a  2D  numerical  experiment,  the  performance  of  proposed  method  is  investigated.  Results  show  the  efficiency  of  the  proposed  method  compared  to  the  previous  methods.  The  proposed  method  can  be  used  in  a  general-purpose  MIT  system.
2	Eulerian  magnification  of  multi  modal  rgb  d  video  for  heart  rate  estimation.  Eulerian  Video  Magnification  (EVM)  has  been  shown  to  be  highly  effective  for  non-contact,  unobtrusive,  and  non-invasive  patient  heart  rate  (HR)  estimation  systems.  EVM  is  typically  applied  to  RGB  video  to  amplify  minute  changes  in  skin  color  due  to  varying  blood  flow,  thereby  estimating  HR.  Previous  methods  require  knowledge  of  the  expected  HR  to  optimize  the  passband  to  be  amplified  via  EVM.  Furthermore,  most  EVM  methods  operating  on  natural  light  video  often  fail  in  low-light  environments.  This  paper  proposes  a  multi-modal  selective  passband  search  approach,  utilizing  predefined  EVM  passbands,  and  the  use  of  intelligent  data  fusion  of  the  three  different  modalities  provided  by  the  Intel  RealSense  RGB-D  camera.  We  demonstrate  the  effectiveness  of  using  the  color,  depth,  and  near-infrared  streams  to  obtain  a  consensus  HR  estimate  under  various  lighting  conditions  and  subject  poses.  Results  indicate  that  the  fusion  of  HR  estimates  acquired  from  each  modality  is  effective  and  robust  to  environmental  conditions.
2	Performance  comparisons  of  phrase  sets  and  presentation  styles  for  text  entry  evaluations.  We  empirically  compare  five  different  publicly-available  phrase  sets  in  two  large-scale  (N  =  225  and  N  =  150)  crowdsourced  text  entry  experiments.  We  also  investigate  the  impact  of  asking  participants  to  memorize  phrases  before  writing  them  versus  allowing  participants  to  see  the  phrase  during  text  entry.  We  find  that  asking  participants  to  memorize  phrases  increases  entry  rates  at  the  cost  of  slightly  increased  error  rates.  This  holds  for  both  a  familiar  and  for  an  unfamiliar  text  entry  method.  We  find  statistically  significant  differences  between  some  of  the  phrase  sets  in  terms  of  both  entry  and  error  rates.  Based  on  our  data,  we  arrive  at  a  set  of  recommendations  for  choosing  suitable  phrase  sets  for  text  entry  evaluations.
2	Demo  making  plans  scrutable  with  argumentation  and  natural  language  generation.  Autonomous  systems  perform  tasks  without  human  guidance.  Techniques  for  making  autonomous  systems  scrutable  and,  hence,  more  transparent  are  required  in  order  to  support  humans  working  with  such  systems.  The  Scrutable  Autonomous  Systems  (SAsSy)  demo  shows  a  novel  way  of  combining  argumentation  and  natural  language  to  generate  a  human  understandable  explanation  dialogue.  By  interacting  with  SAsSy  users  are  able  to  ask  why  a  certain  plan  was  selected  for  execution,  why  other  alternatives  were  not  selected,  also  allowing  users  to  modify  information  in  the  system.
2	Analysis  of  postural  kinetics  data  using  artificial  neural  networks  in  alzheimer  s  disease.  Inertial  measurement  Units  (IMU)  (accelerometers  and  gyroscopes),  placed  in  strategic  parts  of  the  human  body,  are  a  growing  field  on  kinetic  posture  and  imbalance  study  in  Alzheimer's  Disease  (AD).  On  the  other  hand,  Artificial  Neural  Network  (ANN)  are  a  powerful  statistical  tool  used  on  pattern  recognition  on  big  data  such  as  IMU  kinetic  records.  Still,  on  ANN  research,  issues  like  the  best  number  of  hidden  layers  and  the  best  number  of  neurons  in  each  hidden  layer  remain  open.  In  our  study  we  developed  a  software  tool  of  Multilayer  Perceptrons  ANN  analysis  (Back  Propagation  and  Scale  Gradient  Conjugate  training  algorithms)  that  automatically  tests  different  configurations  for  the  ANNs  on  the  diagnosis  of  Alzheimer's  disease.  Analysis  was  performed  primarily  on  all  159  variables,  biometrics  and  IMU  records  of  21  AD  patients  and  21  healthy  subjects  exposed  to  seven  different  tasks  with  increasing  postural  stress,  and  posteriorly  on  selected  data  derived  from  Mann-Whitney  analysis.  Multilayer  Perceptron  ANN  reached  a  performance  of  95%  on  the  diagnosis  of  AD,  proving  to  be  a  potential  useful  clinical  tool.
2	A  software  based  platform  for  multichannel  electrophysiological  data  acquisition.  Recent  improvements  in  microelectrodes  technology  have  enabled  neuroscientists  to  record  electrophysiological  signals  from  hundreds  of  neurons  and  simultaneously  from  a  large  number  of  channels.  However,  several  environmental  factors  may  introduce  noise  and  artefacts  and  affect  proper  interpretation  of  recordings.  Thus,  the  development  of  appropriate  signal  acquisition  and  processing  platforms  dealing  with  large  data  sets  and  in  real-time  represents  a  current  fundamental  challenge.  In  the  present  work,  we  present  an  easily-expandable  Lab  VIEW  based  software  for  handling  data  in  real-time  during  a  multichannel  neurophysiological  signal  acquisition.  The  software  was  designed  to  exploit  modern  MultiCore  CPUs  for  large  scale  data  processing  and,  by  freely  setting  key  acquisition  parameters,  to  work  with  virtually  any  kind  of  biological  signal.  The  software  allows  for  data  storage  in  MATLAB  format  to  facilitate  off-line  signal  processing.  Examples  of  local  field  potential  signal  acquisitions  from  the  mouse  hippocampus  are  reported  to  illustrate  software  features.
2	Effect  of  electrode  impedance  on  the  transient  response  of  ecg  recording  amplifiers.  The  transient  response  of  ECG  amplifiers  is  investigated  in  the  light  of  the  IEC  60601  performance  standard  for  ECG  recording  equipment.  Simulations  carried  out  using  models  of  several  modern  electrodes  reveal  that  the  undershoot  limit  of  100  μV  and  the  recovery  slope  limit  of  300  μVs-1  in  response  to  a  narrow  3  mV–100  ms  rectangular  pulse  are  violated  when  the  recommended  input  impedance  of  10  MΩ  and  a  high-pass  cut-off  frequency  of  0.05  Hz  are  used  in  the  amplifier.  Signal  distortions  have  been  observed  which  have  clinical  diagnostic  implications.  The  results  suggest  that  an  amplmer  input  impedance  exceeding  1  GΩ  and  a  cut-off  frequency  somewhat  lower  than  0.05  Hz  are  necessary  in  order  to  meet  the  IEC  60601  performance  criteria  and  avoid  distortion  of  the  recorded  ECG  signal.  The  authors  therefore  recommend  that  future  revisions  and  releases  of  the  IEC  60601  standard  should  include  an  electrical  model  for  the  skin-electrode  interface  in  the  transient  response  specification.
2	Towards  understanding  human  mistakes  of  programming  by  example  an  online  user  study.  Programming-by-Example  (PBE)  enables  users  to  create  programs  without  writing  a  line  of  code.  However,  there  is  little  research  on  people's  ability  to  accomplish  complex  tasks  by  providing  examples,  which  is  the  key  to  successful  PBE  solutions.  This  paper  presents  an  online  user  study,  which  reports  observations  on  how  well  people  decompose  complex  tasks,  and  disambiguate  sub-tasks.  Our  findings  suggest  that  disambiguation  and  decomposition  are  difficult  for  inexperienced  users.  We  identify  seven  types  of  mistakes  made,  and  suggest  new  opportunities  for  actionable  feedback  based  on  unsuccessful  examples,  with  design  implications  for  future  PBE  systems.
2	Cartograph  unlocking  spatial  visualization  through  semantic  enhancement.  This  paper  introduces  Cartograph,  a  visualization  system  that  harnesses  the  vast  amount  of  world  knowledge  encoded  within  Wikipedia  to  create  thematic  maps  of  almost  any  data.  Cartograph  extends  previous  systems  that  visualize  non-spatial  data  using  geographic  approaches.  While  these  systems  required  data  with  an  existing  semantic  structure,  Cartograph  unlocks  spatial  visualization  for  a  much  larger  variety  of  datasets  by  enhancing  input  datasets  with  semantic  information  extracted  from  Wikipedia.  Cartograph's  map  embeddings  use  neural  networks  trained  on  Wikipedia  article  content  and  user  navigation  behavior.  Using  these  embeddings,  the  system  can  reveal  connections  between  points  that  are  unrelated  in  the  original  data  sets,  but  are  related  in  meaning  and  therefore  embedded  close  together  on  the  map.  We  describe  the  design  of  the  system  and  key  challenges  we  encountered,  and  we  present  findings  from  an  exploratory  user  study
2	Adaptive  contextualization  combating  bias  during  high  dimensional  visualization  and  data  selection.  Large  and  high-dimensional  real-world  datasets  are  being  gathered  across  a  wide  range  of  application  disciplines  to  enable  data-driven  decision  making.  Interactive  data  visualization  can  play  a  critical  role  in  allowing  domain  experts  to  select  and  analyze  data  from  these  large  collections.  However,  there  is  a  critical  mismatch  between  the  very  large  number  of  dimensions  in  complex  real-world  datasets  and  the  much  smaller  number  of  dimensions  that  can  be  concurrently  visualized  using  modern  techniques.  This  gap  in  dimensionality  can  result  in  high  levels  of  selection  bias  that  go  unnoticed  by  users.  The  bias  can  in  turn  threaten  the  very  validity  of  any  subsequent  insights.  In  this  paper,  we  present  Adaptive  Contextualization  (AC),  a  novel  approach  to  interactive  visual  data  selection  that  is  specifically  designed  to  combat  the  invisible  introduction  of  selection  bias.  Our  approach  (1)  monitors  and  models  a  user's  visual  data  selection  activity,  (2)  computes  metrics  over  that  model  to  quantify  the  amount  of  selection  bias  after  each  step,  (3)  visualizes  the  metric  results,  and  (4)  provides  interactive  tools  that  help  users  assess  and  avoid  bias-related  problems.  We  also  share  results  from  a  user  study  which  demonstrate  the  effectiveness  of  our  technique.
2	Chemical  production  and  molecular  computing  in  addressable  reaction  compartments.  Biological  systems  employ  compartmentalisation  in  order  to  orchestrate  a  multitude  of  biochemical  processes  by  simultaneously  enabling  “data  hiding”  and  modularisation.  In  this  paper,  we  present  recent  research  projects  that  embrace  compartmentalisation  as  an  organisational  programmatic  principle  in  synthetic  biological  and  biomimetic  systems.  In  these  systems,  artificial  vesicles  and  synthetic  minimal  cells  are  envisioned  as  nanoscale  reactors  for  programmable  biochemical  synthesis  and  as  chassis  for  molecular  information  processing.  We  present  P  systems,  brane  calculi,  and  the  recently  developed  chemtainer  calculus  as  formal  frameworks  providing  data  hiding  and  modularisation  and  thus  enabling  the  representation  of  highly  complicated  hierarchically  organised  compartmentalised  reaction  systems.  We  demonstrate  how  compartmentalisation  can  greatly  reduce  the  complexity  required  to  implement  computational  functionality,  and  how  addressable  compartments  permit  the  scaling-up  of  programmable  chemical  synthesis.
2	Social  and  collaborative  web  search  an  evaluation  study.  In  this  paper  we  describe  the  results  of  a  live-user  study  to  demonstrate  the  benefits  of  using  the  social  search  utility  HeyStaks,  a  novel  approach  to  Web  search  that  combines  ideas  from  personalization  and  social  networking  to  provide  a  more  collaborative  search  experience.
2	Exploring  personalized  command  recommendations  based  on  information  found  in  web  documentation.  Prior  work  on  command  recommendations  for  feature-rich  software  has  relied  on  data  supplied  by  a  large  community  of  users  to  generate  personalized  recommendations.  In  this  work,  we  explore  the  feasibility  of  using  an  alternative  data  source:  web  documentation.  Specifically,  our  approach  uses  QF-Graphs,  a  previously  proposed  technique  that  maps  higher-level  tasks  (i.e.,  search  queries)  to  commands  referenced  in  online  documentation.  Our  approach  uses  these  command-to-task  mappings  as  an  automatically  generated  plan  library,  enabling  our  prototype  system  to  make  personalized  recommendations  for  task-relevant  commands.  Through  both  offline  and  online  evaluations,  we  explore  potential  benefits  and  drawbacks  of  this  approach.
2	Accelerometry  based  study  of  body  vibration  dampening  during  whole  body  vibration  training.  The  purpose  of  our  study  was  to  characterize  the  vibration  delivered  by  a  whole-body  vibration  (WBV)  exercise  platform  and  quantify  the  acceleration  transmissibility  throughout  the  body  during  different  WBV  exercises.  Our  accelerometry-based  experimental  setup,  includes  materials  and  methods  for  assessing  vibration  frequencies  and  corresponding  magnitudes  both  at  the  side-alternating  vibration  platform  and  on  multiple  anatomic  landmarks  of  the  subject's  body.  Fourteen  subjects  completed  a  sequence  of  four  different  exercises  on  the  platform  at  different  vibration  frequencies.  Results  have  shown  that  researchers/clinicians  should  verify  the  vibration  characteristics  of  the  platform  before  beginning  its  use.  The  information  provided  by  the  manufacturers  can  have  limited  utility  when  prescribing  an  exercise/rehabilitation  program  on  a  WBV  platform.  Vibration  seems  to  be  dampened  while  it  travels  the  body  distal  to  proximal,  with  maximal  attenuation  seen  at  the  shoulder  level.  Different  exercises  seem  to  influence  the  vibration  transmissibility  to  some  extent.
2	Assessment  of  the  enhancement  potential  of  halloysite  nanotubes  for  echographic  imaging.  Halloysite  Nanotubes  (HNTs)  are  nanomaterials  composed  of  double  layered  aluminosilicate  minerals  with  a  predominantly  hollow  tubular  structure  in  submicron  range.  HNTs  are  characterized  by  a  wide  range  of  applications  in  anticancer  therapy,  sustained  agent  delivery,  being  particularly  interesting  because  of  their  tunable  release  rates  and  fast  adsorption  rates.  However  systematic  investigations  of  their  acoustic  properties  are  still  poorly  documented.  This  paper  shows  a  quantitative  assessment  of  the  effectiveness  of  HNTs  as  scatterers  at  conventional  ultrasonic  frequencies  (5.7  -  7  MHz)  in  low  range  of  concentrations  (1.5-5  mg/mL).  Different  samples  of  HNT  (diameter:  40-50  nm;  length:  0.5  to  2  microns,  empty  lumen  diameter:  15-20  nm)  containing  agarose  gel  were  imaged  through  a  commercially  available  echographic  system  and  acquired  data  were  processed  through  a  dedicated  prototypal  platform  in  order  to  extract  the  average  ultrasonic  signal  amplitude  associated  to  the  considered  sample.  Relationships  have  been  established  among  backscatter,  HNT  concentration  and  the  employed  echographic  frequency.  Our  results  demonstrated  that  improvement  in  image  backscatter  could  be  achieved  incrementing  HNT  concentration,  determining  a  non-linear  signal  enhancement  due  to  the  fact  that  they  are  poly-disperse  in  length.  On  the  other  hand  the  effect  of  different  echographic  frequencies  used  was  almost  constant  at  all  concentrations,  specifically  using  higher  values  of  echographic  frequency  allows  yielding  a  signal  enhanced  of  a  factor  1.75±0.26.
2	Toward  a  context  awareness  framework  for  healthcare  applications.  We  are  witnessing  a  significant  expansion  and  penetration  of  wireless  appliances,  sensors,  smart  phones,  and  PDAs  in  a  wide  range  of  domains  such  as  hospitals,  healthcare,  smart  homes,  and  elderly  home  care.  Healthcare  applications  are  mission-critical,  real-time,  and  are  used  in  a  complex  socio-technical  system  that  requires  collaboration,  communication  and  coordination  between  many  actors  (doctors,  patients,  nurses,  lab  technicians,  administration,  etc.)  and  devices  in  the  system,  depending  on  their  context.  This  paper  introduces  a  framework  for  supporting  context  awareness,  which  is  built  from  available  open  source  components.  The  paper  illustrates  how  the  framework  can  be  used  in  the  healthcare  domain  with  the  help  of  an  emergency  department  application.
2	Caffe  neve  a  podcasting  capable  framework  for  crearing  flexible  and  extensible  3d  applications.  The  popularization  of  podcasting  provides  Internet  users  more  opportunities  to  access  interesting  and  entertaining  multimedia  content.  The  podcasting  content  hosting  sites  became  the  shared  platforms  for  all  Internet  users  to  access  third-party  contributions.  More  and  more  Internet  users  are  encouraged  to  become  active  content  providers  that  share  their  resources  and  multimedia  productions.  Building  a  large  scale  multi-user  3D  application  can  benefit  from  the  same  content  distribution  pattern  as  the  development  cost  can  be  distributed  if  the  application  implementation  is  outsourced.  The  application  is  constructed  by  integrating  3D  content  from  various  third-party  service  providers.  In  addition,  the  implementation  outsourcing  can  inspire  contributors  to  provide  creative  application  content.  Unfortunately,  sharing  and  distributing  3D  resources  are  more  challenging  and  the  podcasting  framework  cannot  resolve  the  problems  effectively.      We  introduce  a  standardized  framework  that  supports  the  seamless  integration  of  distributed  3D  resources  as  coarse-grained  components  of  constructed  Distributed  Virtual  Environments  (DVEs).  Component  Application  Framework  For  Extensible  NEtworked  Virtual  Environments  (Caffe  Neve)  uses  service-oriented  architecture  (SOA)  and  incorporates  streaming  technology  for  dynamic  content  delivery.  Using  Caffe  Neve,  third-party  developers  can  create  distributed  services  to  stream  3D  content.  The  framework  provides  an  application  integrator  that  constructs  a  shared  virtual  environment  and  delivers  the  application  content  to  the  end  users.      In  this  paper  we  describe  the  framework  details.  The  application  integration  infrastructure  of  the  framework  is  designed  based  on  a  distributed  Model-View-Controller  model.  The  framework  uses  an  ontology  to  support  the  semantic  level  content  integration  and  service  coordination.  In  addition,  the  framework  incorporates  streaming  within  the  architecture  for  the  real-time  data  delivery  and  improved  performance.  By  deploying  services  within  the  framework,  3D  resource  owners  can  conveniently  podcast  or  broadcast  their  3D  content  to  the  end  users.  In  the  end,  we  demonstrate  how  to  use  the  framework  to  construct  3D  podcast  and  broadcast  applications.  Regardless  of  the  type  of  3D  applications  and  delay  tolerance,  performance  is  important  and  has  to  be  carefully  studied.  We  evaluate  the  framework  performance  by  examining  the  sample  application  and  present  the  analysis  results.  We  also  present  the  experiment  results  on  the  simulated  framework  application  in  large  scale  by  using  simulation  tools.
2	Enhancing  understanding  of  safety  aspects  in  embedded  systems  through  an  interactive  visual  tool.  In  this  work,  we  present  a  demonstration  of  a  visual  interactive  tool  called  ESSAVis  that  helps  different  engineers  in  collaborating  together  for  understanding  the  failure  mechanisms  in  complex  embedded  systems.  ESSAVis  provides  a  2Dplus3D  visual  user  interface  that  integrates  intuitively  between  different  data  sets  related  with  embedded  systems  failure  mechanisms.  The  tool  accepts  a  CFT  model  describing  a  specific  hazard  in  the  underlying  system,  and  a  CAD  model  describing  the  geometry  of  system  components.  In  this  paper,  we  present  different  interaction  options  of  ESSAVis  that  are  used  for  intuitively  extracting  safety  aspects  of  the  underlying  embedded  system.
2	Adaptive  click  and  cross  adapting  to  both  abilities  and  task  improves  performance  of  users  with  impaired  dexterity.  Computer  users  with  impaired  dexterity  often  have  difficulty  accessing  small,  densely  packed  user  interface  elements.  Past  research  in  software-based  solutions  has  mainly  employed  two  approaches:  modifying  the  interface  and  modifying  the  interaction  with  the  cursor.  Each  approach,  however,  has  limitations.  Modifying  the  user  interface  by  enlarging  interactive  elements  makes  access  efficient  for  simple  interfaces  but  increases  the  cost  of  navigation  for  complex  ones  by  displacing  items  to  screens  that  require  tabs  or  scrolling  to  reach.  Modifying  the  interaction  with  the  cursor  makes  access  possible  to  unmodified  interfaces  but  may  perform  poorly  on  densely  packed  targets  or  require  the  user  to  perform  multiple  steps.  We  developed  a  new  approach  that  combines  the  strengths  of  the  existing  approaches  while  minimizing  their  shortcomings,  introducing  only  minimal  distortion  to  the  original  interface  while  making  access  to  frequently  used  parts  of  the  user  interface  efficient  and  access  to  all  other  parts  possible.  We  instantiated  this  concept  as  Adaptive  Click-and-Cross,  a  novel  interaction  technique.  Our  user  study  demonstrates  that,  for  sufficiently  complex  interfaces,  Adaptive  Click-and-Cross  slightly  improves  the  performance  of  users  with  impaired  dexterity  compared  to  only  modifying  the  interface  or  only  modifying  the  cursor.
2	Toward  crowdsourcing  micro  level  behavior  annotations  the  challenges  of  interface  training  and  generalization.  Research  that  involves  human  behavior  analysis  usually  requires  laborious  and  costly  efforts  for  obtaining  micro-level  behavior  annotations  on  a  large  video  corpus.  With  the  emerging  paradigm  of  crowdsourcing  however,  these  efforts  can  be  considerably  reduced.  We  first  present  OCTAB  (Online  Crowdsourcing  Tool  for  Annotations  of  Behaviors),  a  web-based  annotation  tool  that  allows  precise  and  convenient  behavior  annotations  in  videos,  directly  portable  to  popular  crowdsourcing  platforms.  As  part  of  OCTAB,  we  introduce  a  training  module  with  specialized  visualizations.  The  training  module's  design  was  inspired  by  an  observational  study  of  local  experienced  coders,  and  it  enables  an  iterative  procedure  for  effectively  training  crowd  workers  online.  Finally,  we  present  an  extensive  set  of  experiments  that  evaluates  the  feasibility  of  our  crowdsourcing  approach  for  obtaining  micro-level  behavior  annotations  in  videos,  showing  the  reliability  improvement  in  annotation  accuracy  when  properly  training  online  crowd  workers.  We  also  show  the  generalization  of  our  training  approach  to  a  new  independent  video  corpus.
2	Inferring  meal  eating  activities  in  real  world  settings  from  ambient  sounds  a  feasibility  study.  Dietary  self-monitoring  has  been  shown  to  be  an  effective  method  for  weight-loss,  but  it  remains  an  onerous  task  despite  recent  advances  in  food  journaling  systems.  Semi-automated  food  journaling  can  reduce  the  effort  of  logging,  but  often  requires  that  eating  activities  be  detected  automatically.  In  this  work  we  describe  results  from  a  feasibility  study  conducted  in-the-wild  where  eating  activities  were  inferred  from  ambient  sounds  captured  with  a  wrist-mounted  device;  twenty  participants  wore  the  device  during  one  day  for  an  average  of  5  hours  while  performing  normal  everyday  activities.  Our  system  was  able  to  identify  meal  eating  with  an  F-score  of  79.8%  in  a  person-dependent  evaluation,  and  with  86.6%  accuracy  in  a  person-independent  evaluation.  Our  approach  is  intended  to  be  practical,  leveraging  off-the-shelf  devices  with  audio  sensing  capabilities  in  contrast  to  systems  for  automated  dietary  assessment  based  on  specialized  sensors.
2	On  higher  effective  descriptive  set  theory.  In  the  framework  of  computable  topology,  we  propose  an  approach  how  to  develop  higher  effective  descriptive  set  theory.  We  introduce  a  wide  class  \(\mathbb  {K}\)  of  effective  \(T_0\)-spaces  admitting  Borel  point  recovering.  For  this  class  we  propose  the  notion  of  an  \((\alpha  ,m)\)-retractive  morphism  that  gives  a  great  opportunity  to  extend  classical  results  from  EDST  to  the  class  \(\mathbb  {K}\).  We  illustrate  this  by  several  examples.
2	Energy  expenditure  of  three  public  and  three  home  based  active  video  games  in  children.  The  purpose  of  this  study  was  to  assess  the  energy  expenditure  (EE)  experienced  by  children  when  playing  six  active  video  games,  which  can  be  used  in  a  home  environment  and  in  a  public  setting  (e.g.  game  center),  and  to  evaluate  whether  the  intensity  of  playing  these  games  can  meet  the  threshold  for  moderate-intensity  physical  activity,  which  is  set  at  an  EE  equivalent  to  three  times  resting  metabolic  rate.  Children  are  recommended  to  be  physically  active  at  a  moderate  intensity  for  at  least  one  hour  a  day.
2	Some  nonstandard  equivalences  in  reverse  mathematics.  Reverse  Mathematics  (RM)  is  a  program  in  the  foundations  of  mathematics  founded  by  Friedman  and  developed  extensively  by  Simpson.  The  aim  of  RM  is  finding  the  minimal  axioms  needed  to  prove  a  theorem  of  ordinary  (i.e.  non-set  theoretical)  mathematics.  In  the  majority  of  cases,  one  also  obtains  an  equivalence  between  the  theorem  and  its  minimal  axioms.  This  equivalence  is  established  in  a  weak  logical  system  called  the  base  theory;  four  prominent  axioms  which  boast  lots  of  such  equivalences  are  dubbed  mathematically  natural  by  Simpson.  In  this  paper,  we  show  that  a  number  of  axioms  from  Nonstandard  Analysis  are  equivalent  to  theorems  of  ordinary  mathematics  not  involving  Nonstandard  Analysis.  These  equivalences  are  proved  in  a  base  theory  recently  introduced  by  van  den  Berg  and  the  author.  Our  results  combined  with  Simpson’s  criterion  for  naturalness  suggest  the  controversial  point  that  Nonstandard  Analysis  is  actually  mathematically  natural.
2	A  preliminary  characterization  of  a  whole  body  vibration  platform  prototype  for  medical  and  rehabilitation  application.  Whole-body  vibration  (WBV)  is  receiving  increasing  interest  as  an  exercise  intervention  in  physiology  and  rehabilitation.  Although  there  are  many  commercial  and  professional  WBV  platforms  to  provide  controlled  vibrations,  very  few  have  actually  been  tested  in  terms  of  amplitude  (mm),  frequency  spectra  (Hz)  and  shape  of  the  vibratory  motion  wave.  In  this  regard  a  prototype  of  a  novel  WBV  platform  where  the  vibration  amplitude  can  be  set  to  three  values  in  the  frequency  range  20–60  Hz  is  here  proposed.  The  device  has  been  preliminarily  characterized  by  processing  the  measurement  signal  from  a  piezoelectric  monoaxial  accelerometer  mounted  in  the  center  of  the  WBV  plate.  In  particular  the  accelerometer  signal  has  been  processed  to  evaluate  frequency  spectrum,  waveform  shape  and  displacement.  Moreover  to  evaluate  the  performance  of  the  prototype,  same  tests  were  carried  out  on  an  equivalent  professional  WBV  platform,  so  that  data  coming  from  the  two  platforms  have  been  compared  and  commented:  test  results  have  shown  that  the  vibrations  produced  by  the  two  WBV  plates  are  very  similar,  nevertheless  it  has  been  found  that  the  prototype  works  with  a  maximum  error  of  about  5  %  in  frequency,  that  is  less  than  in  the  other  devices,  furthermore  the  greater  distortion  of  the  signal  is  always  at  the  twice  of  the  operating  frequency  (second  harmonic).  Further  investigations  are  needed  to  complete  the  characterization  of  the  prototype  and  assess  the  vibration  amplitude  is  not  load  dependent.
2	Bitconeview  visualization  of  flows  in  the  bitcoin  transaction  graph.  Bitcoin  is  a  digital  currency  whose  transactions  are  stored  into  a  public  ledger,  called  blockchain,  that  can  be  viewed  as  a  directed  graph  with  more  than  70  million  nodes,  where  each  node  represents  a  transaction  and  each  edge  represents  Bitcoins  flowing  from  one  transaction  to  another  one.  We  describe  a  system  for  the  visual  analysis  of  how  and  when  a  flow  of  Bitcoins  mixes  with  other  flows  in  the  transaction  graph.  Such  a  system  relies  on  high-level  metaphors  for  the  representation  of  the  graph  and  the  size  and  characteristics  of  transactions,  allowing  for  high  level  analysis  of  big  portions  of  it.
2	Assessing  the  effects  of  interactive  blogging  on  student  attitudes  towards  peer  interaction  learning  motivation  and  academic  achievements.  Blogs  have  been  increasingly  used  to  supplement  traditional  classroom  lectures  in  higher  education.  This  paper  explores  the  use  of  blogs,  and  how  student  attitudes  towards  online  peer  interaction  and  peer  learning,  as  well  as  motivation  to  learn  from  peers,  may  differ  when  using  the  blog  comments  feature,  and  when  students  are  encouraged  to  read  and  comment  on  each  other's  work.  We  contrast  two  ways  blogs  affect  learning  engagement:  (1)  solitary  blogs  as  personal  digital  portfolios  for  writers;  or  (2)  blogs  used  interactively  to  facilitate  peer  interaction  by  exposing  blogging  content  and  comments  to  peers.  A  quasi-experiment  was  conducted  across  two  semesters,  involving  154  graduate  and  undergraduate  students.  The  result  suggests  that  interactive  blogs,  compared  with  isolated  blogs,  are  associated  with  positive  attitudes  towards  academic  achievement  in  course  subjects  and  in  online  peer  interaction.  Students  showed  positive  motivation  to  learn  from  peer  work,  regardless  of  whether  blogs  were  interactive  or  solitary.  ©  2012  Wiley  Periodicals,  Inc.
2	Detecting  breakdowns  in  local  coherence  in  the  writing  of  chinese  english  learners.  This  paper  introduces  CTutor,  an  automated  writing  evaluation  (AWE)  tool  for  detecting  breakdowns  in  local  coherence  and  reports  on  a  study  that  applies  it  to  the  writing  of  Chinese  L2  English  learners.  The  program  is  based  on  Centering  theory  (CT),  a  theory  of  local  coherence  and  salience.  The  principles  of  CT  are  first  introduced  and  then  the  design  and  function  of  CTutor  are  described.  The  effectiveness  and  reliability  of  the  program  was  evaluated  in  a  study  that  compared  performance  by  CTutor  and  two  human  raters  on  the  analysis  of  local  incoherence  and  provision  of  revision  on  learner  essays.  Intermediate  Chinese  English  as  a  foreign  language  learners  (n = 52)  were  divided  into  two  groups:  one  receiving  CTutor  feedback  and  the  other  receiving  feedback  from  human  raters.  Learners  in  both  groups  completed  three  essays;  each  of  which  involved  the  submission  of  a  first  draft,  revision  with  feedback  on  local  coherence  quality  and  re-submission.  Our  results  from  the  comparison  between  CTutor  and  human  experts  showed  that  this  software  tool  is  able  to  detect  local  coherence  breakdowns  with  moderate  accuracy  (F1-measure  is  around  0.4).  There  was  also  little  difference  between  participants'  responses  to  CTutor  feedback  and  human  feedback  in  terms  of  revision  behaviour,  with  both  feedback  modes  resulting  in  similar  revision  pattern.  Potential  use  of  the  program  in  instructional  settings  is  discussed.  ©  2012  Wiley  Periodicals,  Inc.
2	Profiling  sympathetic  arousal  in  a  physics  course  how  active  are  students.  Low  arousal  states  (especially  boredom)  have  been  shown  to  be  more  deleterious  to  learning  than  high  arousal  states,  though  the  latter  have  received  much  more  attention  (e.g.,  test  anxiety,  confusion,  and  frustration).  Aiming  at  profiling  arousal  in  the  classroom  (how  active  students  are)  and  examining  how  activation  levels  relate  to  achievement,  we  studied  sympathetic  arousal  during  two  runs  of  an  elective  advanced  physics  course  in  a  real  classroom  setting,  including  the  course  exam.  Participants  were  high  school  students  (N  =  24)  who  were  randomly  selected  from  the  course  population.  Arousal  was  indexed  from  electrodermal  activity,  measured  unobtrusively  via  the  Empatica  E4  wristband.  Low  arousal  was  the  level  with  the  highest  incidence  (60%  of  the  lesson  on  average)  and  longest  persistence,  lasting  on  average  three  times  longer  than  medium  arousal  and  two  times  longer  than  high  arousal  level  occurrences.  During  the  course  exam,  arousal  was  positively  and  highly  correlated  (r  =  .66)  with  achievement  as  measured  by  the  students'  grades.  Implications  for  a  need  to  focus  more  on  addressing  low  arousal  states  in  learning  are  discussed,  together  with  potential  applications  for  biofeedback,  teacher  intervention,  and  instructional  design.
2	The  effect  of  a  graph  oriented  computer  assisted  project  based  learning  environment  on  argumentation  skills.  The  purpose  of  this  quasi-experimental  study  was  to  explore  how  seventh  graders  in  a  suburban  school  in  the  United  States  developed  argumentation  skills  and  science  knowledge  in  a  project-based  learning  environment  that  incorporated  a  graph-oriented,  computer-assisted  application.  A  total  of  54  students  three  classes  comprised  this  treatment  condition  and  were  engaged  in  a  project-based  learning  environment  that  incorporated  a  graph-oriented,  computer-assisted  application,  whereas  a  total  of  57  students  three  classes  comprised  the  control  condition  and  were  engaged  in  a  project-based  learning  environment  without  this  graph-oriented,  computer-assisted  application.  Verbal  collaborative  argumentation  was  recorded  and  the  students'  post  essays  were  collected.  A  random  effects  analysis  of  variance  ANOVA  was  conducted  and  a  significant  difference  in  science  knowledge  about  alternative  energies  between  conditions  was  observed.  A  multivariate  analysis  of  variance  MANOVA  was  conducted  and  there  was  a  significant  difference  in  counterargument  and  rebuttal  skills  between  conditions.  A  qualitative  analysis  was  conducted  to  examine  how  the  graph-oriented,  computer-assisted  application  supported  students'  development  of  argumentation  skills  and  affected  the  quality  of  collaborative  argumentation.  The  difference  in  argumentation  structure  and  quality  of  argumentation  between  conditions  might  explain  a  difference  in  science  knowledge  as  well  counterargument  and  rebuttal  skills  argumentation  between  both  conditions.  This  study  concluded  that  a  project-based  learning  environment  incorporating  a  graph-oriented,  computer-assisted  application  was  effective  in  improving  students'  science  knowledge  and  developing  their  scientific  argumentation  skills.
2	An  empirical  study  on  perceptually  masking  privacy  in  graph  visualizations.  Researchers  such  as  sociologists  create  visualizations  of  multivariate  node-link  diagrams  to  present  findings  about  the  relationships  in  communities.  Unfortunately,  such  visualizations  can  inadvertently  expose  the  ostensibly  private  identities  of  the  persons  that  make  up  the  dataset.  By  purposely  violating  graph  readability  metrics  for  a  small  region  of  the  graph,  we  conjecture  that  local,  exposed  privacy  leaks  may  be  perceptually  masked  from  easy  recognition.  In  particular,  we  consider  three  commonly  known  metrics—edge  crossing,  node  clustering,  and  node-edge  overlapping—as  a  strategy  to  hide  leaks.  We  evaluate  the  effectiveness  of  violating  these  metrics  by  conducting  a  user  study  that  measures  subject  performance  at  visually  searching  for  and  identifying  a  privacy  leak.  Results  show  that  when  more  masking  operations  are  applied,  participants  needed  more  time  to  locate  the  privacy  leak,  though  exhaustive,  brute  force  search  can  eventually  find  it.  We  suggest  future  directions  on  how  perceptual  masking  can  be  a  viable  strategy,  primarily  where  modifying  the  underlying  network  structure  is  unfeasible.
2	Tablet  use  in  schools  a  critical  review  of  the  evidence  for  learning  outcomes.  The  increased  popularity  of  tablets  in  general  has  led  to  uptake  in  education.  We  critically  review  the  literature  reporting  use  of  tablets  by  primary  and  secondary  school  children  across  the  curriculum,  with  a  particular  emphasis  on  learning  outcomes.  The  systematic  review  methodology  was  used,  and  our  literature  search  resulted  in  33  relevant  studies  meeting  the  inclusion  criteria.  A  total  of  23  met  the  minimum  quality  criteria  and  were  examined  in  detail  16  reporting  positive  learning  outcomes,  5  no  difference  and  2  negative  learning  outcomes.  Explanations  underlying  these  observations  were  analysed,  and  factors  contributing  to  successful  uses  of  tablets  are  discussed.  While  we  hypothesize  how  tablets  can  viably  support  children  in  completing  a  variety  of  learning  tasks  across  a  range  of  contexts  and  academic  subjects,  the  fragmented  nature  of  the  current  knowledge  base,  and  the  scarcity  of  rigorous  studies,  makes  it  difficult  to  draw  firm  conclusions.  The  generalizability  of  evidence  is  limited,  and  detailed  explanations  as  to  how,  or  why,  using  tablets  within  certain  activities  can  improve  learning  remain  elusive.  We  recommend  that  future  research  moves  beyond  exploration  towards  systematic  and  in-depth  investigations  building  on  the  existing  findings  documented  here.
2	A  comparison  of  elaborated  and  restricted  feedback  in  logex  a  tool  for  teaching  rewriting  logical  formulae.  This  article  describes  an  experiment  with  LogEx,  an  e‐learning  environment  that  supports  students  in  learning  how  to  prove  the  equivalence  between  two  logical  formulae,  using  standard  equivalences  such  as  DeMorgan.  In  the  experiment,  we  compare  two  groups  of  students.  The  first  group  uses  the  complete  learning  environment,  including  hints,  next  steps,  worked  solutions,  and  informative  timely  feedback.  The  second  group  uses  a  version  of  the  environment  without  hints  or  next  steps,  but  with  worked  solutions,  and  delayed  flag  feedback.  We  use  pretest  and  posttest  to  measure  the  performance  of  both  groups  with  respect  to  error  rate  and  completion  of  the  exercises.  We  analyse  the  loggings  of  the  student  activities  in  the  learning  environment  to  compare  its  use  by  the  different  groups.  Both  groups  score  significantly  better  on  the  posttest  than  on  the  pretest.  We  did  not  find  significant  differences  between  the  groups  in  the  posttest,  although  the  group  using  the  full  learning  environment  performed  slightly  better  than  the  other  group.  In  the  examination,  which  took  place  5  weeks  after  the  experiment,  the  group  of  students  who  used  the  complete  learning  environment  scored  significantly  better  than  a  control  group  of  students  who  did  not  participate  in  the  experiment.
2	Ubiquitous  games  for  learning  ubiqgames  weatherlings  a  worked  example.  This  paper  provides  a  rationale  for  a  class  of  mobile,  casual,  and  educational  games,  which  we  call  UbiqGames.  The  study  is  motivated  by  the  desire  to  understand  how  students  use  educational  games  in  light  of  additional  distractions  on  their  devices,  and  how  game  design  can  make  those  games  appealing,  educationally  useful,  and  practical.  In  particular,  we  explain  the  choices  made  to  build  an  engaging  and  educational  first  example  of  this  line  of  games,  namely  Weatherlings.  Further,  we  report  results  from  a  pilot  study  with  20  students  that  suggest  that  students  are  engaged  by  the  game  and  are  interested  in  learning  more  about  academic  content  topics,  specifically  weather  and  climate,  after  playing  the  game.  Research  should  continue  to  determine  whether  Weatherlings  specifically  does  increase  learning  in  these  areas,  and  more  generally  to  determine  whether  any  learning  gains  and  similar  results  with  regard  to  engagement  can  be  replicated  in  other  content  areas  following  the  general  model  for  game  design.  ©  2012  Wiley  Periodicals,  Inc.
2	Promoting  argumentation  in  primary  science  contexts  an  analysis  of  students  interactions  in  formal  and  informal  learning  environments.  The  paper  reports  on  the  outcomes  of  a  study  that  utilized  a  graphical  tool,  Digalo,  to  stimulate  argumentative  interactions  in  both  school  and  informal  learning  settings.  Digalo  was  developed  in  a  European  study  to  explore  argumentation  in  a  range  of  learning  environments.  The  focus  here  is  on  the  potential  for  using  Digalo  in  promoting  argumentative  interactions  of  students  in  primary  science,  first,  in  a  school-based  context  of  students  investigating  and  learning  about  electricity,  and  second,  in  a  hands-on  science  discovery  centre  where  students  are  interacting  with  different  scientific  phenomena.  Data  sources  included  observations  of  students  using  Digalo  in  the  two  contexts  and  the  resultant  Digalo  maps.  Analysis  of  observations  focused  on  students'  engagement  and  interactions,  and  of  Digalo  maps  in  terms  of  the  process  and  content  of  argumentation.  A  previously  developed  level  system  was  used  to  evaluate  the  process  of  argumentation.  The  study  has  revealed  some  limitations  of  Digalo  as  a  teaching  resource,  but  has  provided  insights  into  ways  in  which  students  build  their  knowledge  with  the  help  of  Digalo  as  they  interact  with  each  other  and  with  scientific  phenomena.  ©  2012  Wiley  Periodicals,  Inc.
2	Early  identification  of  ineffective  cooperative  learning  teams.  Cooperative  learning  has  many  pedagogical  benefits.  However,  if  the  cooperative  learning  teams  become  ineffective,  these  benefits  are  lost.  Accordingly,  this  study  developed  a  computer-aided  assessment  method  for  identifying  ineffective  teams  at  their  early  stage  of  dysfunction  by  using  the  Mahalanobis  distance  metric  to  examine  the  difference  between  the  sequential  test  scores  of  the  unknown  team  and  the  test  scores  of  a  reference  group  of  functioning  teams.  The  effectiveness  of  the  proposed  method  was  verified  by  conducting  field  experiments  over  an  18-week  engineering  course  in  Taiwan.  Forty-eight  students  were  randomly  assigned  to  cooperative  learning  teams.  The  students'  learning  performance  was  evaluated  by  means  of  unit  tests  and  homework  tests.  The  functioning  of  the  cooperative  teams  was  examined  at  seven  different  points  during  the  course  of  the  study.  The  ineffective  teams  were  identified  with  quantified  type  I  errors.  It  was  found  that  some  teams  failed  persistently.  Such  teams  require  some  form  of  external  intervention  to  remedy  the  group  dynamics.  The  results  also  showed  that  teams  can  become  ineffective  at  any  stage  of  the  cooperative  learning  process.  Thus,  continuous  monitoring  is  required  to  ensure  that  appropriate  remedial  actions  are  taken  in  a  timely  manner.
2	Early  reading  intervention  by  means  of  a  multicomponent  reading  game.  This  study  examined  the  effects  of  an  intervention  with  a  multicomponent  reading  game  on  the  development  of  reading  skills  in  60  Dutch  primary  school  children  with  special  educational  needs.  The  game  contains  evidence-based  reading  exercises  and  is  based  on  principles  of  applied  gaming.  Using  a  multiple  baseline  approach,  we  tested  children's  word,  pseudoword  and  text  reading  fluency,  as  well  as  their  reading  motivation,  at  three  test  occasions.  The  results  indicated  that  the  short  intervention  (9  x  15  min)  enhanced  children's  pseudoword  reading  fluency  as  well  as  their  text  reading  fluency.  Interestingly,  the  early  intervention  group  showed  a  sustained  intervention  effect  (i.e.,  also  during  retention),  which  shows  that  using  this  reading  game  has  a  long-term  effect  on  early  text  reading  development.  Intervention  did  not  affect  reading  motivation,  which  suggests  that  the  multicomponent  reading  game  can  be  used  to  facilitate  early  reading  development  without  compromising  reading  motivation.
2	Quality  in  e  learning  a  conceptual  framework  based  on  experiences  from  three  international  benchmarking  projects.  Between  2008  and  2010,  Lund  University  took  part  in  three  international  benchmarking  projects,  E-xcellence+,  the  eLearning  Benchmarking  Exercise  2009,  and  the  First  Dual-Mode  Distance  Learning  Benchmarking  Club.  A  comparison  of  these  models  revealed  a  rather  high  level  of  correspondence.  From  this  finding  and  from  desktop  studies  of  the  current  discourse  regarding  e-learning,  a  conceptual  framework  for  e-learning  has  emerged  based  on  a  range  of  critical  success  factors.  This  model  could  be  used  as  a  foundation  for  future  e-learning  and  as  an  inspiration  to  develop,  implement,  evaluate,  and  internalize  e-learning.  It  shows  that  various  aspects  of  accessibility,  flexibility,  interactiveness,  personalization,  and  productivity  should  be  embedded  in  all  levels  of  management  and  services  within  the  field  of  e-learning  in  higher  education.  To  meet  students'  expectations,  demands,  and  rights,  these  critical  issues  should  be  taken  into  account  from  a  holistic  perspective  with  transparency  and  innovation  in  mind.  Therefore,  successful  e-learning  requires  change  from  an  organizational  as  well  as  a  pedagogical  perspective.  One  conclusion  from  this  study  is  that  a  revolution  is  on  the  way  and  that  learning  will  be  reoriented  along  paradigms  of  collaboration  and  networking.  Globalization,  sustainability,  and  lifelong  learning  will  be  some  of  the  leading  concepts  in  this  process.
2	Stability  analysis  of  car  driving  with  a  joystick  interface.  This  paper  presents  stability  analysis  of  car  driving  with  a  joystick  interface.  A  car  drive  assist  system  with  a  joystick  interface  enables  disabled  persons  to  drive  a  car  with  joystick  operations.  A  steering  wheel  and  gas/brake  pedals  are  actuated  by  electric  motors  which  rotations  are  controlled  by  micro-computers  based  on  2DOF  joystick  commands.  Since  the  drive  assist  system  involves  a  time  delay  and  an  amplifier  with  a  high  gain,  a  car  would  show  unstable  behaviors  when  a  stable  condition  is  not  guaranteed.  To  analyze  the  stable  and  unstable  behaviors  of  car  driving,  we  derive  a  vehicle  dynamic  model  together  with  the  joystick  system  and  human  recognition  model  (driver  model)  for  various  car  velocities.
2	Developing  a  virtual  reality  application  for  the  improvement  of  depth  perception.  In  this  paper  a  development  of  a  virtual  reality  application  is  presented  while  also  mentioning  earlier  works  which  provided  a  basis  for  the  presented  application.  Immersive  virtual  reality  was  chosen  as  it  allows  for  better  learning  and  has  more  research  and  testing  capabilities.  While  making  the  application  a  survey  has  also  been  conducted  with  the  users  to  enhance  the  application.  The  aim  of  the  developed  application  is  to  allow  people  with  weak  stereopsis  to  use  it  at  home  with  the  intention  to  improve  their  stereopsis  and  depth  perception  via  dichoptic  perceptual  learning.
2	Driver  drowsiness  monitoring  by  learning  vehicle  telemetry  data.  For  a  safety  critical  task  like  driving,  it  is  very  important  for  the  driver  to  be  vigilant  at  all  times.  In  this  study,  we  explore  a  driver  drowsiness  monitoring  and  early  warning  system,  which  uses  machine  learning  techniques  based  on  vehicle  telemetry  data.  The  proposed  system  can  ensure  safe  driving  by  real  time  monitoring  of  driving  pattern.  This  proves  to  be  a  very  cost  effective  technique  over  biometric  and  camera  based  techniques  since  it  doesn't  involve  expensive  sensors.  The  detection  of  drowsy  state  of  a  driver  is  modeled  as  a  binary  classification  issue.  We  outline  the  design  methodology  followed  and  challenges  faced  in  developing  a  machine  learning  based  classification  scheme  on  vehicle  telemetry  data,  which  is  essentially  a  time-series  data.  Even  though  our  study  is  focused  on  an  exemplary  application  like  driver  attention  monitoring,  the  results  can  be  equally  applied  for  vehicle  prognostics,  driving  style  analytics  and  various  other  telematics  applications  in  automotive  industry.  We  have  done  our  entire  development  and  validation  on  a  driver-in-loop  simulation  platform.
2	Mathability  and  an  animation  related  to  a  convex  like  property.  In  connection  with  investigations  related  to  mathability  and  to  applications  of  computer  assisted  methods  for  studying  mathematical  problems,  an  animation  of  the  m-convex  hull  of  finite  sets  of  points  on  the  Cartesian  plane  is  presented.
2	Construction  of  neural  nets  in  brain  computer  interface  for  robot  arm  steering.  The  paper  presents  an  attempt  to  construct  a  direct  brain-to-machine  interface  (BCI)  that  could  be  used  to  control  movements  of  a  robotic  arm.  A  series  of  experiments  was  performed  to  collect  data  from  subjects,  train  and  validate  an  effective  neural  network  and  search  for  a  generalized  solution.  The  ERD/ERS  imagery  paradigm  was  chosen  to  extract  valid  data  from  the  EEG  signal.  Since  categorizing  between  two  opposite  states  (like  Left-Right)  proved  to  be  the  most  reliable,  a  control  structure  containing  multiple  neural  networks  was  proposed.  New  research  concerns  the  method  of  development  and  the  use  of  neural  networks  classifiers.  An  automated  procedure  was  used  to  select  the  best  bipolar  classifiers  from  the  set  of  machine-generated  neural  networks.  The  chosen  classifiers  were  used  in  a  hierarchical  structure  responsible  for  signal  interpretation.  Adoption  of  this  method  was  motivated  mainly  by  the  complexity  of  arm  movement.  The  movement  consisted  of  several  phases,  such  as  the  initiation,  continuation,  change  of  direction,  change  of  speed.  It  was  observed  that  simple  bipolar  classifiers  produced  better  output  than  classifiers  designed  to  recognize  complex  decisions.
2	Improving  the  methodology  of  text  entry  experiments.  In  this  paper,  the  methodology  of  text  entry  experiments  is  discussed  and  the  current  practice  is  compared  to  the  text  entry  experiment  standardized  by  ISO  9241-4.  Text  entry  experiments  focus  on  measuring  and  comparing  performance  of  users  entering  text.  Vast  majority  of  research  of  new  text  entry  methods  does  not  follow  the  ISO  9241-4  methodology.  We  express  the  need  for  a  new  standard  which  should  be  discussed  in  the  research  community.  A  text  entry  method  is  usually  examined  in  a  series  of  text-copy  tasks,  a  setup  in  which  the  user  is  requested  to  copy  the  presented  phrase  using  the  method  that  is  being  examined.  The  outcome  of  the  text-copy  task  is  influenced  by  many  factors.  This  paper  examines  the  effect  of  one  of  these  factors  -  the  relative  position  of  the  presented  phrase  and  the  transcribed  text.  We  conducted  a  controlled  experiment  with  62  participants  using  3  different  text  entry  methods.  We  found  that  the  position  has  a  significant  effect  on  the  subjective  evaluation  of  the  text  entry  method.
2	Situation  aware  robots  and  transporters.  Inter-cognitive  communication  plays  the  key  role  in  the  development  of  engineering  applications  where  natural  and  artificial  cognitive  systems  should  work  together  efficiently.  Situation-awareness  is  essential  in  that  cooperation.  The  focus  of  this  study  is  to  report  our  experiences  and  applications  of  situation-aware  robots  and  transporters.  Industrial  robots  are  an  important  part  of  production  in  several  fields  of  current  industry.  Mobile  robots  and  wheeled  transporters  such  as  forklift  trucks  will  in  the  future  be  key  elements  in  material  handling  and  transporting  tasks  in  production  and  in  warehouses.  In  addition,  robots  will  also  play  key  roles  in  wellness  services  in  response  to  the  aging  population.  Situation-awareness  and  especially  location-awareness  is  essential  in  the  development  of  efficient  human-robot  interaction.  In  the  future,  cognitive  communication  processes  between  operators  and  intelligent  transporters  may  benefit  from  many  features  developed  for  intelligent  traffic  systems.  We  also  present  our  experiences  from  the  development  a  of  situation-aware  traffic  system.
2	Emotional  faces  of  children  and  adults  what  changes  in  their  perception.  This  work  investigates  disparities  between  children  and  middle  aged  adults  in  their  ability  to  decode  the  six  primary  facial  expressions  of  emotions  when  portrayed  by  contemporary  children  and  adult  faces.  The  analyses  were  conducted  on  a  sample  of  40  (20  females)  very  closely  aged  children  (mean  age=7.4;  SD=±0.2),  and  40  middle  aged  (mean  age=54.3;  SD=±2.9)  adults  (20  females).  Four  different  experimental  conditions  were  assessed:  a)  20  children  and  20  middle  aged  adults  assessing  child  faces;  b)  20  children  and  20  middle  aged  adults  assessing  adult  faces;  c)  20  children  assessing  adult  faces  and  20  children  assessing  child  faces;  d)  20  middle  aged  adults  assessing  adult  faces  and  20  middle  aged  adults  assessing  child  faces.  The  analyses  do  not  show  significant  differences  between  children  and  adults  for  conditions  a),  and  b).  Children  performances  on  condition  c)  did  not  support  the  peers-prejudice  theory,  since  no  significant  differences  were  found  among  children  on  their  ability  to  decode  either  facial  expressions  of  adults  or  children.  Middle  aged  adults  were  significantly  more  accurate  in  decoding  adult  rather  than  children  faces.  No  significant  gender  differences  were  found  in  the  four  conditions,  even  though  significant  interactions  were  found  between  emotional  categories  and  gender  of  stimuli.  In  particular,  the  gender  of  stimuli  had  a  significant  effect  in  condition  a)  where  emotional  faces  portrayed  by  male  children  are  more  accurately  decoded  than  those  portrayed  by  female  children.  Several  significant  interactions  were  observed  between  emotional  categories,  participants’  age,  and  gender  of  stimuli.  Details  are  discussed  in  the  text.
2	Usability  evaluation  of  biometrics  in  mobile  environments.  Actual  trends  in  HCI  tend  to  move  systems  to  mobile  environments.  Moreover,  biometrics  is  a  technology  that  is  entering  maturity,  getting  involved  in  several  security  architectures  nowadays.  Thus,  migrating  biometrics  to  mobile  scenarios  is  a  trending  topic  in  the  research  community.  Nevertheless,  in  this  kind  of  systems,  usability  has  been  put  aside  in  the  intent  of  produce  better  performance  and  it  could  involve  undesirable  results.  In  this  work  a  behavioural  biometric  modality  (handwritten  signature  recognition)  is  tested  in  mobile  environments,  in  order  to  obtain  a  complete  usability  evaluation.  Users  signed  in  an  iPad  with  different  styluses  in  different  scenarios,  correlating  performance  results  with  several  usability  parameters  (gathered  through  video,  notes  and  forms)  and  obtaining  interesting  outcomes.
2	Feasibility  study  of  optimization  of  a  genetic  algorithm  for  traffic  network  division  for  distributed  road  traffic  simulation.  This  paper  deals  with  optimization  of  a  genetic  algorithm  for  road  traffic  network  division  for  distributed  road  traffic  simulation  which  we  developed.  Two  approaches  for  finding  of  optimal  setting  of  the  genetic  algorithm  are  considered  an  optimizing  genetic  algorithm  and  a  systematic  testing  of  possible  settings.  Since  both  approaches  are  expected  to  be  extremely  computation-consuming  their  distributed  versions  are  proposed  and  a  feasibility  study  is  evaluated.
2	Edbb  biometrics  and  behavior  for  assessing  remote  education.  We  present  a  platform  for  student  monitoring  in  remote  education  consisting  of  a  collection  of  sensors  and  software  that  capture  biometric  and  behavioral  data.  We  define  a  collection  of  tasks  to  acquire  behavioral  data  that  can  be  useful  for  facing  the  existing  challenges  in  automatic  student  monitoring  during  remote  evaluation.  Additionally,  we  release  an  initial  database  including  data  from  20  different  users  completing  these  tasks  with  a  set  of  basic  sensors:  webcam,  microphone,  mouse,  and  keyboard;  and  also  from  more  advanced  sensors:  NIR  camera,  smartwatch,  additional  RGB  cameras,  and  an  EEG  band.  Information  from  the  computer  (e.g.  system  logs,  MAC,  IP,  or  web  browsing  history)  is  also  stored.  During  each  acquisition  session  each  user  completed  three  different  types  of  tasks  generating  data  of  different  nature:  mouse  and  keystroke  dynamics,  face  data,  and  audio  data  among  others.  The  tasks  have  been  designed  with  two  main  goals  in  mind:  i)  analyse  the  capacity  of  such  biometric  and  behavioral  data  for  detecting  anomalies  during  remote  evaluation,  and  ii)  study  the  capability  of  these  data,  i.e.  EEG,  ECG,  or  NIR  video,  for  estimating  other  information  about  the  users  such  as  their  attention  level,  the  presence  of  stress,  or  their  pulse  rate.
2	Eye  gaze  controlled  robotic  arm  for  persons  with  ssmi.  Background:  People  with  severe  speech  and  motor  impairment  (SSMI)  often  uses  a  technique  called  eye  pointing  to  communicate  with  outside  world.  One  of  their  parents,  caretakers  or  teachers  hold  a  printed  board  in  front  of  them  and  by  analyzing  their  eye  gaze  manually,  their  intentions  are  interpreted.  This  technique  is  often  error  prone  and  time  consuming  and  depends  on  a  single  caretaker.    Objective:  We  aimed  to  automate  the  eye  tracking  process  electronically  by  using  commercially  available  tablet,  computer  or  laptop  and  without  requiring  any  dedicated  hardware  for  eye  gaze  tracking.  The  eye  gaze  tracker  is  used  to  develop  a  video  see  through  based  AR  (augmented  reality)  display  that  controls  a  robotic  device  with  eye  gaze  and  deployed  for  a  fabric  printing  task.    Methodology:  We  undertook  a  user  centred  design  process  and  separately  evaluated  the  web  cam  based  gaze  tracker  and  the  video  see  through  based  human  robot  interaction  involving  users  with  SSMI.  We  also  reported  a  user  study  on  manipulating  a  robotic  arm  with  webcam  based  eye  gaze  tracker.    Results:  Using  our  bespoke  eye  gaze  controlled  interface,  able  bodied  users  can  select  one  of  nine  regions  of  screen  at  a  median  of  less  than  2  secs  and  users  with  SSMI  can  do  so  at  a  median  of  4  secs.  Using  the  eye  gaze  controlled  human-robot  AR  display,  users  with  SSMI  could  undertake  representative  pick  and  drop  task  at  an  average  duration  less  than  15  secs  and  reach  a  randomly  designated  target  within  60  secs  using  a  COTS  eye  tracker  and  at  an  average  time  of  2  mins  using  the  webcam  based  eye  gaze  tracker.
2	An  unattended  study  of  users  performing  security  critical  tasks  under  adversarial  noise.  Author(s):  Kaczmarek,  Tyler;  Kobsa,  Alfred;  Sy,  Robert;  Tsudik,  Gene  |  Abstract:  User  errors  while  performing  security-critical  tasks  can  lead  to  undesirable  or  even  disastrous  consequences.  One  major  factor  influencing  mistakes  and  failures  is  complexity  of  such  tasks,  which  has  been  studied  extensively  in  prior  research.  Another  important  issue  which  hardly  received  any  attention  is  the  impact  of  both  accidental  and  intended  distractions  on  users  performing  security-critical  tasks.  In  particular,  it  is  unclear  whether,  and  to  what  extent,  unexpected  sensory  cues  (e.g.,  auditory  or  visual)  can  influence  user  behavior  and/or  trigger  mistakes.  Better  understanding  of  the  effects  of  intended  distractions  will  help  clarify  their  role  in  adversarial  models.  As  part  of  the  research  effort  described  in  this  paper,  we  administered  a  range  of  naturally  occurring  --  yet  unexpected  --  sounds  while  study  participants  attempted  to  perform  a  security-critical  task.  We  found  that,  although  these  auditory  cues  lowered  participants'  failure  rates,  they  had  no  discernible  effect  on  their  task  completion  times.  To  this  end,  we  overview  some  relevant  literature  that  explains  these  somewhat  counter-intuitive  findings.  Conducting  a  thorough  and  meaningful  study  on  user  errors  requires  a  large  number  of  participants,  since  errors  are  typically  infrequent  and  should  not  be  instigated  more  than  once  per  subject.  To  reduce  the  effort  of  running  numerous  subjects,  we  developed  a  novel  experimental  setup  that  was  fully  automated  and  unattended.  We  discuss  our  experience  with  this  setup  and  highlight  the  pros  and  cons  of  generalizing  its  usage.
2	A  comparative  analysis  of  virtual  reality  head  mounted  display  systems.  With  recent  advances  of  Virtual  Reality  (VR)  technology,  the  deployment  of  such  will  dramatically  increase  in  non-entertainment  environments,  such  as  professional  education  and  training,  manufacturing,  service,  or  low  frequency/high  risk  scenarios.  Clinical  education  is  an  area  that  especially  stands  to  benefit  from  VR  technology  due  to  the  complexity,  high  cost,  and  difficult  logistics.  The  effectiveness  of  the  deployment  of  VR  systems,  is  subject  to  factors  that  may  not  be  necessarily  considered  for  devices  targeting  the  entertainment  market.  In  this  work,  we  systematically  compare  a  wide  range  of  VR  Head-Mounted  Displays  (HMDs)  technologies  and  designs  by  defining  a  new  set  of  metrics  that  are  1)  relevant  to  most  generic  VR  solutions  and  2)  are  of  paramount  importance  for  VR-based  education  and  training.  We  evaluated  ten  HMDs  based  on  various  criteria,  including  neck  strain,  heat  development,  and  color  accuracy.  Other  metrics  such  as  text  readability,  comfort,  and  contrast  perception  were  evaluated  in  a  multi-user  study  on  three  selected  HMDs,  namely  Oculus  Rift  S,  HTC  Vive  Pro  and  Samsung  Odyssey+.  Results  indicate  that  the  HTC  Vive  Pro  performs  best  with  regards  to  comfort,  display  quality  and  compatibility  with  glasses.
2	User  driven  intelligent  interface  on  the  basis  of  multimodal  augmented  reality  and  brain  computer  interaction  for  people  with  functional  disabilities.  The  analysis  of  the  current  integration  attempts  of  some  modes  and  use  cases  of  user-machine  interaction  is  presented.  The  new  concept  of  the  user-driven  intelligent  interface  is  proposed  on  the  basis  of  multimodal  augmented  reality  and  brain-computer  interaction  for  various  applications:  in  disabilities  studies,  education,  home  care,  health  care,  etc.  The  several  use  cases  of  multimodal  augmentation  are  presented.  The  perspectives  of  the  better  human  comprehension  by  the  immediate  feedback  through  neurophysical  channels  by  means  of  brain-computer  interaction  are  outlined.  It  is  shown  that  brain-computer  interface  (BCI)  technology  provides  new  strategies  to  overcome  limits  of  the  currently  available  user  interfaces,  especially  for  people  with  functional  disabilities.  The  results  of  the  previous  studies  of  the  low  end  consumer  and  open-source  BCI-devices  allow  us  to  conclude  that  combination  of  machine  learning  (ML),  multimodal  interactions  (visual,  sound,  tactile)  with  BCI  will  profit  from  the  immediate  feedback  from  the  actual  neurophysical  reactions  classified  by  ML  methods.  In  general,  BCI  in  combination  with  other  modes  of  AR  interaction  can  deliver  much  more  information  than  these  types  of  interaction  themselves.  Even  in  the  current  state  the  combined  AR-BCI  interfaces  could  provide  the  highly  adaptable  and  personal  services,  especially  for  people  with  functional  disabilities.
2	Themisto  towards  automated  documentation  generation  in  computational  notebooks.  Computational  notebooks  allow  data  scientists  to  express  their  ideas  through  a  combination  of  code  and  documentation.  However,  data  scientists  often  pay  attention  only  to  the  code,  and  neglect  creating  or  updating  their  documentation  during  quick  iterations,  which  leads  to  challenges  in  sharing  their  notebooks  with  others  and  future  selves.  Inspired  by  human  documentation  practices  from  analyzing  80  highly-voted  Kaggle  notebooks,  we  design  and  implement  Themisto,  an  automated  documentation  generation  system  to  explore  the  Human-AI  Collaboration  opportunity  in  the  code  documentation  scenario.  Themisto  facilitates  the  creation  of  different  types  of  documentation  via  three  approaches:  a  deep-learning-based  approach  to  generate  documentation  for  source  code  (fully  automated),  a  query-based  approach  to  retrieve  the  online  API  documentation  for  source  code  (fully  automated),  and  a  user  prompt  approach  to  motivate  users  to  write  more  documentation  (semi-automated).  We  evaluated  Themisto  in  a  within-subjects  experiment  with  24  data  science  practitioners,  and  found  that  automated  documentation  generation  techniques  reduced  the  time  for  writing  documentation,  reminded  participants  to  document  code  they  would  have  ignored,  and  improved  participants'  satisfaction  with  their  computational  notebook.
2	Assessment  of  empathy  in  an  affective  vr  environment  using  eeg  signals.  With  the  advancements  in  social  robotics  and  virtual  avatars,  it  becomes  increasingly  important  that  these  agents  adapt  their  behavior  to  the  mood,  feelings  and  personality  of  their  users.  One  such  aspect  of  the  user  is  empathy.  Whereas  many  studies  measure  empathy  through  offline  measures  that  are  collected  after  empathic  stimulation  (e.g.  post-hoc  questionnaires),  the  current  study  aimed  to  measure  empathy  online,  using  brain  activity  collected  during  the  experience.  Participants  watched  an  affective  360  video  of  a  child  experiencing  domestic  violence  in  a  virtual  reality  headset  while  their  EEG  signals  were  recorded.  Results  showed  a  significant  attenuation  of  alpha,  theta  and  delta  asymmetry  in  the  frontal  and  central  areas  of  the  brain.  Moreover,  a  significant  relationship  between  participants'  empathy  scores  and  their  frontal  alpha  asymmetry  at  baseline  was  found.  These  results  demonstrate  specific  brain  activity  alterations  when  participants  are  exposed  to  an  affective  virtual  reality  environment,  with  the  level  of  empathy  as  a  personality  trait  being  visible  in  brain  activity  during  a  baseline  measurement.  These  findings  suggest  the  potential  of  EEG  measurements  for  development  of  passive  brain-computer  interfaces  that  assess  the  user's  affective  responses  in  real-time  and  consequently  adapt  the  behavior  of  socially  intelligent  agents  for  a  personalized  interaction.
2	Design  of  two  combined  health  recommender  systems  for  tailoring  messages  in  a  smoking  cessation  app.  In  this  article,  we  describe  the  design  of  two  recommender  systems  (RS)  designed  to  support  the  smoking  cessation  process  through  a  mobile  application.  We  plan  to  use  a  hybrid  RS  (content-based,  utility-based,  and  demographic  filtering)  to  tailor  health  recommendation  messages,  and  a  content-based  RS  to  schedule  a  timely  delivery  of  the  message.  We  also  define  metrics  that  we  will  use  to  assess  their  performance,  helping  people  quit  smoking  when  we  run  the  pilot.
2	Clustrophile  a  tool  for  visual  clustering  analysis.  While  clustering  is  one  of  the  most  popular  methods  for  data  mining,  analysts  lack  adequate  tools  for  quick,  iterative  clustering  analysis,  which  is  essential  for  hypothesis  generation  and  data  reasoning.  We  introduce  Clustrophile,  an  interactive  tool  for  iteratively  computing  discrete  and  continuous  data  clusters,  rapidly  exploring  different  choices  of  clustering  parameters,  and  reasoning  about  clustering  instances  in  relation  to  data  dimensions.  Clustrophile  combines  three  basic  visualizations  --  a  table  of  raw  datasets,  a  scatter  plot  of  planar  projections,  and  a  matrix  diagram  (heatmap)  of  discrete  clusterings  --  through  interaction  and  intermediate  visual  encoding.  Clustrophile  also  contributes  two  spatial  interaction  techniques,  $\textit{forward  projection}$  and  $\textit{backward  projection}$,  and  a  visualization  method,  $\textit{prolines}$,  for  reasoning  about  two-dimensional  projections  obtained  through  dimensionality  reductions.
2	Mixing  realities  for  sketch  retrieval  in  virtual  reality.  Drawing  tools  for  Virtual  Reality  (VR)  enable  users  to  model  3D  designs  from  within  the  virtual  environment  itself.  These  tools  employ  sketching  and  sculpting  techniques  known  from  desktop-based  interfaces  and  apply  them  to  hand-based  controller  interaction.  While  these  techniques  allow  for  mid-air  sketching  of  basic  shapes,  it  remains  difficult  for  users  to  create  detailed  and  comprehensive  3D  models.  In  our  work,  we  focus  on  supporting  the  user  in  designing  the  virtual  environment  around  them  by  enhancing  sketch-based  interfaces  with  a  supporting  system  for  interactive  model  retrieval.  Through  sketching,  an  immersed  user  can  query  a  database  containing  detailed  3D  models  and  replace  them  into  the  virtual  environment.  To  understand  supportive  sketching  within  a  virtual  environment,  we  compare  different  methods  of  sketch  interaction,  i.e.,  3D  mid-air  sketching,  2D  sketching  on  a  virtual  tablet,  2D  sketching  on  a  fixed  virtual  whiteboard,  and  2D  sketching  on  a  real  tablet.  %using  a  2D  physical  tablet,  a  2D  virtual  tablet,  a  2D  virtual  whiteboard,  and  3D  mid-air  sketching.  Our  results  show  that  3D  mid-air  sketching  is  considered  to  be  a  more  intuitive  method  to  search  a  collection  of  models  while  the  addition  of  physical  devices  creates  confusion  due  to  the  complications  of  their  inclusion  within  a  virtual  environment.  While  we  pose  our  work  as  a  retrieval  problem  for  3D  models  of  chairs,  our  results  can  be  extrapolated  to  other  sketching  tasks  for  virtual  environments.
2	Visual  analytics  for  explainable  deep  learning.  Recently,  deep  learning  has  been  advancing  the  state  of  the  art  in  artificial  intelligence  to  a  new  level,  and  humans  rely  on  artificial  intelligence  techniques  more  than  ever.  However,  even  with  such  unprecedented  advancements,  the  lack  of  explanation  regarding  the  decisions  made  by  deep  learning  models  and  absence  of  control  over  their  internal  processes  act  as  major  drawbacks  in  critical  decision-making  processes,  such  as  precision  medicine  and  law  enforcement.  In  response,  efforts  are  being  made  to  make  deep  learning  interpretable  and  controllable  by  humans.  In  this  paper,  we  review  visual  analytics,  information  visualization,  and  machine  learning  perspectives  relevant  to  this  aim,  and  discuss  potential  challenges  and  future  research  directions.
2	Retrieve  then  adapt  example  based  automatic  generation  for  proportion  related  infographics.  Infographic  is  a  data  visualization  technique  which  combines  graphic  and  textual  descriptions  in  an  aesthetic  and  effective  manner.  Creating  infographics  is  a  difficult  and  time-consuming  process  which  often  requires  significant  attempts  and  adjustments  even  for  experienced  designers,  not  to  mention  novice  users  with  limited  design  expertise.  Recently,  a  few  approaches  have  been  proposed  to  automate  the  creation  process  by  applying  predefined  blueprints  to  user  information.  However,  predefined  blueprints  are  often  hard  to  create,  hence  limited  in  volume  and  diversity.  In  contrast,  good  infogrpahics  have  been  created  by  professionals  and  accumulated  on  the  Internet  rapidly.  These  online  examples  often  represent  a  wide  variety  of  design  styles,  and  serve  as  exemplars  or  inspiration  to  people  who  like  to  create  their  own  infographics.  Based  on  these  observations,  we  propose  to  generate  infographics  by  automatically  imitating  examples.  We  present  a  two-stage  approach,  namely  retrieve-then-adapt.  In  the  retrieval  stage,  we  index  online  examples  by  their  visual  elements.  For  a  given  user  information,  we  transform  it  to  a  concrete  query  by  sampling  from  a  learned  distribution  about  visual  elements,  and  then  find  appropriate  examples  in  our  example  library  based  on  the  similarity  between  example  indexes  and  the  query.  For  a  retrieved  example,  we  generate  an  initial  drafts  by  replacing  its  content  with  user  information.  However,  in  many  cases,  user  information  cannot  be  perfectly  fitted  to  retrieved  examples.  Therefore,  we  further  introduce  an  adaption  stage.  Specifically,  we  propose  a  MCMC-like  approach  and  leverage  recursive  neural  networks  to  help  adjust  the  initial  draft  and  improve  its  visual  appearance  iteratively,  until  a  satisfactory  result  is  obtained.  We  implement  our  approach  on  proportion-related  infographics,  and  demonstrate  its  effectiveness  by  sample  results  and  expert  reviews.
2	Thousands  of  positive  reviews  distributed  mentoring  in  online  fan  communities.  Young  people  worldwide  are  participating  in  ever-increasing  numbers  in  online  fan  communities.  Far  from  mere  shallow  repositories  of  pop  culture,  these  sites  are  accumulating  significant  evidence  that  sophisticated  informal  learning  is  taking  place  online  in  novel  and  unexpected  ways.  In  order  to  understand  and  analyze  in  more  detail  how  learning  might  be  occurring,  we  conducted  an  in-depth  nine-month  ethnographic  investigation  of  online  fanfiction  communities,  including  participant  observation  and  fanfiction  author  interviews.  Our  observations  led  to  the  development  of  a  theory  we  term  distributed  mentoring,  which  we  present  in  detail  in  this  paper.  Distributed  mentoring  exemplifies  one  instance  of  how  networked  technology  affords  new  extensions  of  behaviors  that  were  previously  bounded  by  time  and  space.  Distributed  mentoring  holds  potential  for  application  beyond  the  spontaneous  mentoring  observed  in  this  investigation  and  may  help  students  receive  diverse,  thoughtful  feedback  in  formal  learning  environments  as  well.
2	Streamingbandit  developing  adaptive  persuasive  systems.  This  paper  introduces  StreamingBandit,  a  (back-end)  solution  for  developing  adaptive  and  personalized  persuasive  systems.  Creating  successful  persuasive  applications  requires  a  combination  of  design,  social  science,  and  technology.  StreamingBandit  contributes  to  the  required  technology  by  providing  a  platform  that  can  be  used  to  adapt  persuasive  technologies  in  real-time  and  at  large  scales.  We  first  introduce  the  design  philosophy  of  StreamingBandit  using  a  running  example  and  highlight  how  a  large  number  of  adaptive  persuasive  systems  can  be  regarded  as  solutions  to  (contextual)  multi-armed  bandit  problems:  a  type  of  problem  that  StreamingBandit  was  built  to  address.  Subsequently,  we  detail  several  scenarios  of  the  use  of  StreamingBandit  to  create  adaptive  persuasive  systems  and  detail  its  future  developments.
2	A  review  on  dyadic  conversation  visualizations  purposes  data  lens  of  analysis.  Many  professional  services  are  provided  through  text  and  voice  systems,  from  voice  calls  over  the  internet  to  messaging  and  emails.  There  is  a  growing  need  for  both  individuals  and  organizations  to  understand  these  online  conversations  better  and  find  actionable  insights.  One  method  that  allows  the  user  to  explore  insights  is  to  build  intuitive  and  rich  visualizations  that  illustrate  the  content  of  the  conversation.  In  this  paper,  we  present  a  systematic  survey  of  the  various  methods  of  visualizing  a  conversation  and  research  papers  involving  interactive  visualizations  and  human  participants.  Findings  from  the  survey  show  that  there  have  been  attempts  to  visualize  most,  if  not  all,  of  the  types  of  conversation  that  are  taking  place  digitally,  from  speech  to  messages  and  emails.  Through  this  survey,  we  make  two  contributions.  One,  we  summarize  the  current  practices  in  the  domain  of  visualizing  dyadic  conversations.  Two,  we  provide  suggestions  for  future  dialogue  visualization  research.
2	Interactive  task  and  concept  learning  from  natural  language  instructions  and  gui  demonstrations.  Natural  language  programming  is  a  promising  approach  to  enable  end  users  to  instruct  new  tasks  for  intelligent  agents.  However,  our  formative  study  found  that  end  users  would  often  use  unclear,  ambiguous  or  vague  concepts  when  naturally  instructing  tasks  in  natural  language,  especially  when  specifying  conditionals.  Existing  systems  have  limited  support  for  letting  the  user  teach  agents  new  concepts  or  explaining  unclear  concepts.  In  this  paper,  we  describe  a  new  multi-modal  domain-independent  approach  that  combines  natural  language  programming  and  programming-by-demonstration  to  allow  users  to  first  naturally  describe  tasks  and  associated  conditions  at  a  high  level,  and  then  collaborate  with  the  agent  to  recursively  resolve  any  ambiguities  or  vagueness  through  conversations  and  demonstrations.  Users  can  also  define  new  procedures  and  concepts  by  demonstrating  and  referring  to  contents  within  GUIs  of  existing  mobile  apps.  We  demonstrate  this  approach  in  PUMICE,  an  end-user  programmable  agent  that  implements  this  approach.  A  lab  study  with  10  users  showed  its  usability.
2	What  makes  a  dark  pattern  dark  design  attributes  normative  considerations  and  measurement  methods.  There  is  a  rapidly  growing  literature  on  dark  patterns,  user  interface  designs  --  typically  related  to  shopping  or  privacy  --  that  researchers  deem  problematic.  Recent  work  has  been  predominantly  descriptive,  documenting  and  categorizing  objectionable  user  interfaces.  These  contributions  have  been  invaluable  in  highlighting  specific  designs  for  researchers  and  policymakers.  But  the  current  literature  lacks  a  conceptual  foundation:  What  makes  a  user  interface  a  dark  pattern?  Why  are  certain  designs  problematic  for  users  or  society?    We  review  recent  work  on  dark  patterns  and  demonstrate  that  the  literature  does  not  reflect  a  singular  concern  or  consistent  definition,  but  rather,  a  set  of  thematically  related  considerations.  Drawing  from  scholarship  in  psychology,  economics,  ethics,  philosophy,  and  law,  we  articulate  a  set  of  normative  perspectives  for  analyzing  dark  patterns  and  their  effects  on  individuals  and  society.  We  then  show  how  future  research  on  dark  patterns  can  go  beyond  subjective  criticism  of  user  interface  designs  and  apply  empirical  methods  grounded  in  normative  perspectives.
2	Understanding  college  students  phone  call  behaviors  towards  a  sustainable  mobile  health  and  wellbeing  solution.  During  the  transition  from  high  school  to  on-campus  college  life,  a  student  leaves  home  and  starts  facing  enormous  life  changes,  including  meeting  new  people,  more  responsibilities,  being  away  from  family,  and  academic  challenges.  These  recent  changes  lead  to  an  elevation  of  stress  and  anxiety,  affecting  a  student's  health  and  wellbeing.  With  the  help  of  smartphones  and  their  rich  collection  of  sensors,  we  can  continuously  monitor  various  factors  that  affect  students'  behavioral  patterns,  such  as  communication  behaviors  associated  with  their  health,  wellbeing,  and  academic  success.  In  this  work,  we  try  to  assess  college  students'  communication  patterns  (in  terms  of  phone  call  duration  and  frequency)  that  vary  across  various  geographical  contexts  (e.g.,  dormitories,  classes,  dining)  during  different  times  (e.g.,  epochs  of  a  day,  days  of  a  week)  using  visualization  techniques.  Findings  from  this  work  will  help  foster  the  design  and  delivery  of  smartphone-based  health  interventions;  thereby,  help  the  students  adapt  to  the  changes  in  life.
2	Dark  intentions  or  persuasion  ux  designers  activation  of  stakeholder  and  user  values.  Formalized  frameworks  that  reference  ethics  and  values  have  received  increasing  attention  in  the  HCI  community.  These  methods  emphasize  the  importance  of  values  in  relation  to  design  but  provide  little  guidance  to  reveal  the  values  that  are  present  or  have  impact  on  designers'  decision  making.  In  this  work-inprogress,  we  identify  the  values  considered  by  student  UX  designers  when  conducting  an  authentic  design  task,  allowing  for  interrogation  of  the  possible  intentions  that  underlie  their  decision  making.  Our  exploratory  analysis  revealed  that  participants  had  sensitivity  towards  user  values,  but  often  contradicted  these  values  through  dark,  often  tacit,  intentions  to  persuade  users,  thereby  achieving  stakeholder  goals.  We  provide  provocations  for  future  research  on  the  role  of  ethics  and  values  in  practice  and  design  education.
2	Poseidon  passive  acoustic  ocean  sensor  for  entertainment  and  interactive  data  gathering  in  opportunistic  nautical  activities.  Recent  years  demonstrate  an  increased  interest  in  Passive  Acoustic  Monitoring  (PAM)  applications  when  studying  cetaceans.  However,  they  remain  expensive  underwater  systems  and  targeted  for  industrial  and  military  purposes.  While  the  usage  of  smartphones  as  acoustic  sensors  has  been  observed  in  terrestrial  environments,  ocean  and  nautical  PAM  applications  remain  greatly  unexplored.  This  paper  presents  the  design,  deployment  and  testing  of  a  POSEIDON  system,  used  for  real-time  augmentation  of  whale-watching  experiences.  We  collect  and  use  cetaceans'  vocal  call  acoustic  samples  (clicks,  moans  and  whistles)  and  apply  machine  learning  for  offline  model  training  and  prediction.  When  discriminating  the  calls,  we  find  that  Extra  Trees  and  Gradient  Boosting  outperform  other  classifiers  (>0.95  confidence  threshold).  Collected  samples  are  at  disposal  to  citizen  scientists  and  marine  biologists.  Future  studies  involve  real-time  on-boat  user  testing.
2	Design  maintenance  and  the  menstruating  body.  In  this  submission,  I  discuss  my  research  on  the  maintenance  of  public  restrooms  and,  more  specifically,  the  distribution  and  stratification  of  menstrual  resources  throughout  the  city  of  Seattle.  Through  interviews,  technology  development,  and  field  engagements,  I  show  how  digital  artifacts-such  as  those  connected  to  the  Internet  of  Things-structure  experiences  of  hygiene  access  and  help  expose  the  socioeconomic  logics  undergirding  infrastructures  of  public  life.  I  use  design  interventions  to  further  examine  the  role  of  technology  to  cultivate  and  maintain  collective  responsibility  and  forms  of  participatory  infrastructure.
2	Crafting  code  at  the  demo  scene.  This  paper  introduces  the  idea  of  craftsmanship  as  a  way  of  understanding  the  shaping  and  re-shaping  of  code  as  a  material  crafting  practice.  We  build  our  analysis  on  a  qualitative  study  of  a  coder  engaged  in  creative  and  expressive  programming  on  an  old  hardware  platform.  The  contribution  of  the  paper  is  a  set  of  conceptual  categories:  craft  engagement,  craftsmanship  rhythm  and  craftsmanship  expressivity,  that  conceptualizes  coding  as  crafting.
2	Patterns  of  persuasion  for  sustainability.  Research  into  the  values  motivating  unsustainable  behavior  has  generated  unique  insight  into  how  NGOs  and  environmental  campaigns  contribute  toward  successfully  fostering  significant  and  long-term  behavior  change,  yet  thus  far  this  research  has  not  been  applied  to  the  domain  of  sustainable  HCI.  We  explore  the  implications  of  this  research  as  it  relates  to  the  potential  limitations  of  current  approaches  to  persuasive  technology,  and  what  it  means  for  designing  higher  impact  interventions.  As  a  means  of  communicating  these  implications  to  be  readily  understandable  and  implementable,  we  develop  a  set  of  antipatterns  to  describe  persuasive  technology  approaches  that  values  research  suggests  are  unlikely  to  yield  significant  sustainability  wins,  and  a  complementary  set  of  patterns  to  describe  new  guidelines  for  what  may  become  persuasive  technology  best  practice.
2	Tunetracker  tensions  in  the  surveillance  of  traditional  music.  We  describe  the  design  and  deployment  of  the  first  system  ever  to  dynamically  track  and  publish  records  of  folk  music  playing.  TuneTracker  is  a  software  system  that  has  been,  at  time  of  writing,  deployed  at  a  pub  in  Dublin,  Ireland  for  five  months.  It  captures,  stores,  and  posts  the  names  of  tunes  played  in  Irish  traditional  music  sessions  on  a  public  website.  This  paper  makes  two  contributions:  (1)  drawing  from  a  two  year  ethnographic  study  of  trad  musicians,  it  details  the  design  and  development  of  a  system  to  track  and  publish  traditional  musicians'  practices  while  respecting  the  ethos  of  tradition,  and  (2)  it  presents  a  discussion  of  professional  musicians'  reactions  to  having  their  music  practices  surveilled.  This  latter  fieldwork  revealed  divergent  viewpoints  on  the  effect  that  TuneTracker  would  have  on  local  sessions  and  the  process  of  tradition.
2	Creating  conditions  for  patients  values  to  emerge  in  clinical  conversations  perspectives  of  health  care  team  members.  Eliciting,  understanding,  and  honoring  patients'  values--the  things  most  important  to  them  in  daily  life--is  a  cornerstone  of  patient-centered  care.  However,  this  rarely  occurs  explicitly  as  a  routine  part  of  clinical  practice.  This  is  particularly  problematic  for  individuals  with  multiple  chronic  conditions  (MCC)  because  they  face  difficult  choices  about  how  to  balance  competing  demands  for  self-care  in  accordance  with  their  values.  In  this  study,  we  sought  to  inform  the  design  of  interventions  to  support  conversations  about  patient  values  between  patients  with  MCC  and  their  health  care  providers.  We  conducted  a  field  study  that  included  observations  of  21  clinic  visits  for  patients  who  have  MCC,  and  interviews  with  16  care  team  members  involved  in  those  visits.  This  paper  contributes  a  practice-based  account  of  ways  in  which  providers  engage  with  patient  values,  and  discusses  how  future  work  in  interactive  systems  design  might  extend  and  enrich  these  engagements.
2	See  the  world  through  the  eyes  of  a  child  learning  from  children  s  cognitive  maps  for  the  design  of  child  targeted  locative  systems.  Inspired  by  HCI  research  advocating  for  the  inclusion  of  children  in  the  design  process,  this  pictorial  provides  a  qualitative  case  study  on  children's  perceptions  of  urban  landscapes.  Our  goal  is  to  create  digital  maps  in  the  context  of  locative  systems  and  wayfinding  for  children.  For  this  purpose,  we  engaged  70  students  from  the  city  of  Funchal  (Portugal)  in  the  drawing  of  a  cognitive  map  of  their  journey  from  home  to  school  in  the  Fall  of  2017.  These  children  (9-12  years  old)  also  replied  to  a  brief  survey,  and  31  out  of  70  responded  to  a  face-to-face  interview  in  the  Spring  of  2018.  This  pictorial  offers  an  analysis  of  the  drawings  as  well  as  providing  highlights  of  the  children's  own  account  of  their  maps.  Our  work  generates  a  set  of  10  themes  related  to  landmarks  and  design  ideas  for  the  creation  of  digital  maps  for  children.
2	Citizen  dialogue  kit  public  polling  and  data  visualization  displays  for  bottom  up  citizen  participation.  In  this  demo  we  introduce  our  ongoing  research  on  how  to  leverage  the  situated  visualization  of  open  and  citizen  science  data  within  public  space  to  inform  and  engage  citizens.  We  developed  an  open-source  toolkit,  coined  "Citizen  Dialogue  Kit"  that  is  able  to  convey  data  visualizations  on  a  set  of  interactive,  wirelessly  networked  displays  that  can  be  freely  positioned  in  urban  space.  The  toolkit  consists  of  a  participative  methodology  to  guide  stakeholders  with  the  choice  of  data  and  the  design  of  its  visualization,  a  set  of  off-the-shelf  hardware  components,  and  custom-made  open  source  software  that  controls  the  whole  system.  We  summarize  the  design  of  the  toolkit  and  its  initial  deployment  and  conclude  by  discussing  implications  for  urban  visualization  and  future  work.
2	Designing  for  the  end  of  life  of  iot  objects.  The  Internet  of  Things  (IoT)  and  ubiquitous  computing  are  leading  to  an  increase  in  objects  with  a  short  lifespan  -  either  through  breakage,  "bricking"  by  the  manufacturer,  or  discontinued  use  by  the  owner.  This  leads  to  a  surplus  of  material  and  e-waste  that  cannot  or  is  not  readily  recycled,  upcycled  or  otherwise  reused,  aggravating  material  scarcity.  In  part,  this  is  due  to  the  use  of  unrecyclable  materials  and  custom-built  hardware.  However,  it  is  also  due  to  the  limited  value  people  place  on  these  objects  (e.g.,  sentimental  and  environmental).  This  one-day  workshop  will  explore  how  the  configuration  of  values  designed  into  IoT  objects  influences  the  end-user  practices  of  disposal,  recycling  and  upcycling.  Through  this  lens,  we  will  collectively  consider  potential  design  strategies  that  can  be  instilled  during  the  process  of  design,  to  support  the  continuity  of  the  material  life  of  IoT  objects  after  their  "death".
2	Designing  documentary  informatics.  In  this  paper  we  describe  a  Research  through  Design  inquiry  about  a  speculative  wedding  documentation  service,  in  the  mode  of  the  Quantified  Self.  We  reflect  on  our  design  research,  which  included  design  ethnography,  interviews,  enactments  of  parts  of  the  service,  and  the  production  of  a  concept  brochure.  In  so  doing,  we  explore  the  design  of  personal  tracking  as  a  documentary  activity,  one  intended  for  longer-term  self-expression  and  remembering  --  rather  than  simply  to  monitor,  regulate  and  motivate  a  data-driven  life.  Developing  the  Lived  Informatics  discourse,  we  use  our  design-led  inquiry  to  propose  "Documentary  Informatics"  as  an  alternative  and  longer-term  design  perspective  on  self-tracking  tools.
2	Magic  land  the  design  and  evaluation  of  an  interactive  tabletop  supporting  therapeutic  play  with  children.  We  consider  the  role  and  design  of  digital  technologies  in  play  therapy  settings  with  young  children.  Through  an  aggregation  of  the  researcher  and  practitioner  literature,  and  results  of  discussions  with  therapists  and  counselors,  we  propose  a  set  of  design  requirements  for  digital  technologies  that  support  non-directive  play  within  a  play  therapy  context.  We  explore  how  to  design  for  these  complex  requirements  through  the  development  and  evaluation  of  Magic  Land,  a  set  of  four  play  therapy  applications  for  an  interactive  tabletop.  Based  on  our  experiences  we  recommend  that  designers  create  digital  interactive  toys,  which  create  opportunities  for  play  that  would  not  normally  be  possible  within  the  traditional  play  therapy  environment.
2	Measuring  the  learnability  of  interactive  systems  using  a  petri  net  based  approach.  A  learnable  system  allows  a  user  to  know  how  to  perform  correctly  any  task  of  the  system  after  having  executed  it  a  few  times  in  the  past.  In  this  paper,  we  propose  an  approach  to  measure  the  learnability  of  interactive  systems  during  their  daily  use.  We  rely  on  recording  in  a  user  log  the  user  actions  that  take  place  during  a  run  of  the  system  and  on  replaying  them  over  the  system  interaction  models,  which  describe  the  expected  ways  of  executing  system  tasks.  Our  approach  identifies  deviations  between  the  interaction  models  and  the  user  log  and  assesses  their  weight  through  a  fitness  value.  By  measuring  the  rate  of  the  fitness  value  for  subsequent  executions  of  the  system  we  are  able  not  only  to  understand  if  the  system  is  learnable  with  respect  to  its  tasks,  but  also  to  quantify  its  degree  of  learnability  over  time  and  to  identify  potential  learning  issues.
2	Instime  a  case  study  on  the  co  design  of  interactive  installations  on  deep  time.  New  technologies  and  practices  are  constantly  transforming  our  interaction  with  computational  systems.  These  transformations  bring  challenges  to  the  Human-Computer  Interaction  (HCI)  field,  along  with  a  constant  need  to  better  understand  and  describe  how  the  design  of  interactive  systems  is  changing.  Currently,  movements  and  theories  such  as  speculative  design  and  embodied  cognition  present  unconventional  perspectives  and  bring  debate  into  the  field.  We  investigate  what  kind  of  artifacts  and  attitude  towards  the  design  of  interactive  systems  emerged  in  the  InsTime  project,  in  which  9  interactive  installations  were  co-designed  addressing  the  concept  of  deep  time.  Our  discussion  draws  on  a  juxtaposition  of  backgrounds  in  philosophy  of  science,  speculative  design,  and  embodied  cognition  to  analyze  and  characterize  the  empirical  data  from  the  InsTime  project.  As  contributions,  besides  presenting  the  9  interactive  installations  from  InsTime  and  their  co-design  process,  our  discussion  leads  to  a  characterization  of  an  attitude  towards  design  that  we  named  socioenactive  design.
2	The  bluenetwork  concept.  Most  of  energy  efficiency  and  carbon  reduction  initiatives  and  concepts  attempt  to  regulate  and  optimize  machines  behavior,  and  therefore,  human  behavior  itself  is  left  neglected.  Although  most  of  energy  and  resource  consumption  is  the  result  of  machines  functioning  and  behavior  (including  domesticated  animals  such  as  cows),  these  behaviors  themselves  are  actually  in  answer  to  humans  demands  and  needs,  and  therefore,  can  be  considered  the  indirect  results  of  humans  behavior.  Resolving  the  source  of  problems,  i.e.,  the  unhealthy  human  behavior,  not  only  reduces  these  footprints  including  energy  and  water  consumption,  and  GHG  emissions,  it  also  helps  increasing  the  quality  of  life  in  society.  Here,  we  propose  an  approach  which  focuses  on  adjusting  humans  behavior  toward  eliminating  unnecessary  demand  on  the  machines  that  consequently  lowers  the  consumption.  This  goal  is  achieved  by  creating  a  social  environment  in  which  directed  and  selective  interactions  help  humans  to  adjust  to  healthier  behavior.  The  solution  consists  of  human-friendly  interfaces  and  also  artificial  intelligence  software  in  order  to  learn  and  emulate  human  interactions.
2	Discoveryspace  suggesting  actions  in  complex  software.  Complex  software  offers  power  for  experts,  yet  overwhelms  new  users.  Novices  often  do  not  know  how  to  execute  tasks,  what  they  want  to  achieve,  or  even  what  is  possible.  To  address  this,  we  introduce  the  DiscoverySpace  interface  for  executable  action  suggestions.  DiscoverySpace  is  a  prototype  extension  panel  for  Adobe  Photoshop  that  suggests  task-level  action  macros  to  apply  to  photographs  based  on  visual  features.  DiscoverySpace  harvests  these  one-click  actions  from  the  online  Photoshop  user  community.  A  between-subjects  study  indicated  that  action  suggestions  may  help  novices  maintain  confidence,  accomplish  tasks,  and  discover  features.  This  work  demonstrates  how  interfaces  can  leverage  user-generated  content  to  help  novices  navigate  complex  software.
2	Hot  swap  probing  embodied  game  interfaces  with  reconfigurable  controllers.  HOT  SWAP  is  a  game  platform  based  on  unique  physical  controllers  that  are  designed  to  activate  the  physical  and  social  space  around  players.  The  controllers  require  players  to  keep  track  of  and  utilize  multiple  physical  input  components  that  must  be  exchanged  as  a  crucial  gameplay  component.  In  this  paper,  we  reflect  on  the  design  process  of  two  iterations  of  HOT  SWAP  games  and  our  initial  observations  on  how  they  facilitate  embodied  interaction.  The  games  presented  in  this  paper  are  designed  to  probe  how  game  interfaces  can  be  more  social,  tangible,  and  customizable  for  embodied  experiences.
2	Pinsight  a  novel  way  of  creating  and  sharing  digital  content  through  things  in  the  wild.  Existing  platforms  for  sharing  locative  digital  content  rely  on  the  use  of  mobile  phones  for  accessing  the  content.  This  can  be  a  major  deterrent  to  wider  public  access  and  also  hinders  immediacy  and  'in  the  moment'  discoverability.  Building  on  previous  work  in  situated  public  installations,  we  developed  Pinsight,  a  novel  platform  for  enabling  end-users,  such  as  local  communities,  to  create  and  share  digital  content  in-situ  with  public  audiences  through  physical  interactive  devices.  Pinsight  is  based  on  a  set  of  design  principles  that  focus  on  supporting  both  the  expressiveness  of  content  creators  and  the  appeal  to  public  audiences.  This  paper  describes  the  design  of  the  platform  and  how  it  supports  sharing  knowledge  in  ways  different  to  conventional  media.  Through  preliminary  evaluations  and  two  in-the-wild  studies,  we  explore  how  such  a  situated  technology  can  be  used  by  different  user  groups  (content  designers,  history  communities,  local  residents)  for  sharing  content  with  public  audiences  (visitors,  pedestrians,  residents)  in  different  contexts.
2	Exploring  hygge  as  a  desirable  design  vision  for  the  sustainable  smart  home.  In  this  paper,  we  present  an  exploratory  study  of  hygge  as  a  low-energy  design  vision  for  the  smart  home.  Hygge  is  a  Danish  concept  that  embodies  aesthetic  experiences  related  to  conviviality,  often  shaped  by  orchestrating  atmospheres  through  low-level  lighting.  To  explore  this  vision,  we  probe  two  Australian  households  that  already  live  with  smart  home  lighting  technology.  We  report  on  household  reflections  of  embedding  hygge  into  everyday  life.  We  conclude  by  outlining  future  directions  for  exploring  desirable  and  sus¬tainable  smart  home  visions.
2	Econundrum  visualizing  the  climate  impact  of  dietary  choice  through  a  shared  data  sculpture.  While  there  is  a  strong  relationship  between  climate  change  and  human  food  consumption,  it  is  challenging  to  understand  the  implications  and  impact  from  an  individual  perspective.  The  lack  of  a  shared  frame  of  reference,  that  allows  people  to  compare  their  impact  to  others,  limits  awareness  on  this  complex  topic.  To  support  group  reflections  and  social  comparison  of  the  impact  of  people's  food  consumption  on  climate  change,  we  designed  Econundrum,  a  shared  physical  data  sculpture  that  visualizes  carbon  emissions  resulting  from  dietary  choices  of  a  small  community.  Our  three-week  field  study  demonstrates  how  Econundrum  helped  people  (i)  understand  the  climate  impact  of  various  food  types,  (ii)  reflect  on  the  environmental  impact  of  their  food  choices;  and  (iii)  discuss  the  relation  between  climate  impact  and  food  consumption  with  others.  Our  study  shows  how  a  shared  physical  data  sculpture  mediates  a  complex  topic  to  a  community  by  facilitating  the  social  dynamics  in  context.
2	Betacube  enhancing  training  for  climbing  by  a  self  calibrating  camera  projection  unit.  In  rock  climbing,  discussing  climbing  techniques  with  others  to  master  a  specific  route  and  getting  practical  advice  from  more  experienced  climbers  is  an  inherent  part  of  the  culture  and  tradition  of  the  sport.  Spatial  information,  such  as  the  position  of  holds,  as  well  as  learning  complex  body  postures  plays  a  major  role  in  this  process.  A  typical  problem  that  occurs  during  advising  is  an  alignment  effect  when  trying  to  picture  orientation-specific  knowledge,  e.g.  explaining  how  to  perform  a  certain  self-climbed  move  to  others.  We  propose  betaCube,  a  self-calibrating  camera-projection  unit  that  features  3D  tracking  and  distortion-free  projection.  The  system  enables  a  life-sized  video  replay  and  climbing  route  creation  using  augmented  reality.  We  contribute  an  interface  for  automatic  setup  of  mobile  distortion-free  projection,  blob  detection  for  climbing  holds,  as  well  as  an  automatic  method  for  extracting  planar  trackables  from  artificial  climbing  walls.
2	High  volume  hypothesis  testing  for  large  scale  web  log  analysis.  Time-stamped  event  sequence  data  is  being  generated  across  many  domains:  shopping  transactions,  web  traffic  logs,  medical  histories,  etc.  Oftentimes,  analysts  are  interested  in  comparing  the  similarities  and  differences  between  two  or  more  groups  of  event  sequences  to  better  understand  processes  that  lead  to  different  outcomes  (e.g.,  a  customer  did  or  did  not  make  a  purchase).  CoCo  is  a  visual  analytics  tool  for  Cohort  Comparison  that  combines  automated  high-volume  hypothesis  testing  (HVHT)  with  and  interactive  visualization  and  user  interface  for  improved  exploratory  data  analysis.  This  paper  covers  the  first  case  study  of  CoCo  for  large-scale  web  log  analysis  and  the  challenges  that  arise  when  scaling  a  visual  analytics  tool  to  large  datasets.  The  direct  contributions  of  this  paper  are:  (1)  solutions  to  7  challenges  of  scaling  a  visual  analytics  tool  to  larger  datasets,  and  (2)  a  case  study  with  three  real-world  analysts  with  these  solutions  implemented.
2	Helping  in  the  legal  use  of  open  images.  Media  creation  applications  cater  poorly  to  one  very  common  usage:  Situations  in  which  the  users  need  media  that  they  do  not  own  and  for  which  they  are  unwilling  to  pay.  Finding  and  using  externally  produced  media  is  currently  a  cumbersome  process.  Often,  users  locate  the  content  using  a  search  engine,  copy  it  into  their  work,  cross  their  fingers,  and  hope  they  do  not  infringe  on  any  copyrights.  While  the  authors  have  shared  hundreds  of  millions  of  images  with  permissive  licenses,  the  license  terms  are  too  complicated  for  other  users  to  follow.  In  our  studies,  we  found  that  even  the  well-intentioned  users  still  fail  to  respect  copyrights  in  simple  image  reuse  situations.  We  therefore  introduce  an  Open  Media  Retrieval  (OMR)  model  to  remedy  this  problem  and  supplement  it  with  prototypes  that  access  various  legal  image  sources  directly  within  the  creative  work  flow  and  provide  automatic  credits  to  the  original  authors.
2	Direct  answers  for  search  queries  in  the  long  tail.  Web  search  engines  now  offer  more  than  ranked  results.  Queries  on  topics  like  weather,  definitions,  and  movies  may  return  inline  results  called  answers  that  can  resolve  a  searcher's  information  need  without  any  additional  interaction.  Despite  the  usefulness  of  answers,  they  are  limited  to  popular  needs  because  each  answer  type  is  manually  authored.  To  extend  the  reach  of  answers  to  thousands  of  new  information  needs,  we  introduce  Tail  Answers:  a  large  collection  of  direct  answers  that  are  unpopular  individually,  but  together  address  a  large  proportion  of  search  traffic.  These  answers  cover  long-tail  needs  such  as  the  average  body  temperature  for  a  dog,  substitutes  for  molasses,  and  the  keyboard  shortcut  for  a  right-click.  We  introduce  a  combination  of  search  log  mining  and  paid  crowdsourcing  techniques  to  create  Tail  Answers.  A  user  study  with  361  participants  suggests  that  Tail  Answers  significantly  improved  users'  subjective  ratings  of  search  quality  and  their  ability  to  solve  needs  without  clicking  through  to  a  result.  Our  findings  suggest  that  search  engines  can  be  extended  to  directly  respond  to  a  large  new  class  of  queries.
2	Mining  whining  in  support  forums  with  frictionary.  Millions  of  people  request  help  with  software  in  support  forums,  creating  a  massive  repository  of  user  experiences  ripe  for  mining.  We  present  Frictionary,  a  tool  for  automatically  extracting,  aggregating,  and  organizing  problem  described  in  support  forums,  enabling  timely  problem  frequency  and  prevalence  metrics.  We  applied  it  to  89,760  Firefox  support  requests  from  4  sources  gathered  over  10  months.  Interviews  with  the  Firefox  principal  designer  and  support  lead  suggest  that  Frictionary  could  be  a  useful  tool  for  prioritizing  engineering  efforts,  but  that  the  extraction  would  need  to  be  more  precise  to  be  useful.
2	Does  proprioception  guide  back  of  device  pointing  as  well  as  vision.  We  present  research  that  investigates  the  amount  of  guidance  required  by  users  for  precise  back-of-device  interaction.  We  explore  how  pointing  effectiveness  is  influenced  by  the  presence  or  absence  of  visual  guidance  feedback.  Participants  were  asked  to  select  targets  displayed  on  an  iPad  device,  by  touching  and  releasing  them  from  underneath  the  device.  Another  iPad  was  used  to  detect  finger  positions  from  the  rear.  Results  showed  that  participants  were  able  to  select  targets  as  accurately  without  visual  feedback  of  finger  position  as  they  were  with  it.  Additionally,  no  significant  increase  in  workload  was  identified  when  visual  feedback  was  removed.  Our  results  show  that  users  do  not  require  complex  techniques  to  visualize  finger  position  on  the  rear  of  device.  Visual  feedback  does  not  affect  any  performance  parameters,  such  as  effectiveness,  perceived  performance,  and  the  number  of  trials  needed  to  select  a  target.  We  also  outline  the  implications  of  our  findings  and  our  future  work  to  fully  investigate  the  effect  of  visual  guidance  feedback.
2	The  whats  and  hows  of  programmers  foraging  diets.  One  of  the  least  studied  areas  of  Information  Foraging  Theory  is  diet:  the  information  foragers  choose  to  seek.  For  example,  do  foragers  choose  solely  based  on  cost,  or  do  they  stubbornly  pursue  certain  diets  regardless  of  cost?  Do  their  debugging  strategies  vary  with  their  diets?  To  investigate  "what"  and  "how"  questions  like  these  for  the  domain  of  software  debugging,  we  qualitatively  analyzed  9  professional  developers'  foraging  goals,  goal  patterns,  and  strategies.  Participants  spent  50%  of  their  time  foraging.  Of  their  foraging,  58%  fell  into  distinct  dietary  patterns  -  mostly  in  patterns  not  previously  discussed  in  the  literature.  In  general,  programmers'  foraging  strategies  leaned  more  heavily  toward  enrichment  than  we  expected,  but  different  strategies  aligned  with  different  goal  types.  These  and  our  other  findings  help  fill  the  gap  as  to  what  programmers'  dietary  goals  are  and  how  their  strategies  relate  to  those  goals.
2	How  revealing  are  eye  movements  for  understanding  web  engagement  in  young  children.  This  paper  presents  a  critical  review  of  eye  tracking  as  a  research  approach  and  evaluates  its  potential  for  usability  testing  in  pre-school  children.  We  argue  that  eye-tracking  data  is  useful  for  assessing  web  engagement  in  this  age-group,  but  only  if  triangulated  against  other  usability  methods.  Recommendations  for  potential  usability  methods  to  use  in  tandem  with  eye-tracking  are  presented  as  part  of  a  work  in  progress  within  a  joint  partner  project  between  the  University  of  Salford  (UK)  and  the  British  Broadcasting  Corporation  (BBC)  exploring  best-fit  methodologies  for  understanding  web  engagement  in  young  children.
2	Mistable  reach  through  personal  screens  for  tabletops.  We  present  MisTable,  a  tabletop  system  that  combines  a  conventional  horizontal  interactive  surface  with  personal  screens  between  the  user  and  the  tabletop  surface.  These  personal  screens,  built  using  fog,  are  both  see-through  and  reach-through.  Being  see-through  provides  direct  line  of  sight  of  the  personal  screen  and  the  elements  behind  it  on  the  tabletop.  Being  reach-through  allows  the  user  to  switch  from  interacting  with  the  personal  screen  to  reaching  through  it  to  interact  with  the  tabletop  or  the  space  above  it.  The  personal  screen  allows  a  range  of  customisations  and  novel  interactions  such  as  presenting  2D  personal  contents  on  the  screen,  3D  contents  above  the  tabletop  or  augmenting  and  relighting  tangible  objects  differently  for  each  user.  Besides,  having  a  personal  screen  for  each  user  allows  us  to  customize  the  view  of  each  of  them  according  to  their  identity  or  preferences.  Finally,  the  personal  screens  preserve  all  well-established  tabletop  interaction  techniques  like  touch  and  tangible  interactions.  We  explore  the  challenges  in  building  such  a  reach-through  system  through  a  proof-of-concept  implementation  and  discuss  the  possibilities  afforded  by  the  system.
2	Visual  composition  of  graphical  elements  on  non  rectangular  displays.  Graphical  user  interfaces  are  composed  of  varying  elements  (text,  images,  etc.)  whose  visual  arrangement  has  been  relatively  well  established  in  the  context  of  rectangular  interfaces.  The  advent  of  non-rectangular  displays  questions  this  knowledge.  In  this  paper  we  study  how  traditional  content  layouts  can  be  adapted  to  fit  different  non-rectangular  displays.  We  performed  a  first  qualitative  study  where  graphic  designers  fitted  text  and  images  into  different  non-rectangular  displays.  From  the  analysis  of  their  output  we  generalize  and  adapt  ten  composition  principles  that  have  been  proposed  in  the  literature  for  rectangular  displays.  We  evaluate  the  revised  principles  through  a  paired  comparison  questionnaire  where  57  participants  compared  pairs  of  layouts.  Using  the  Bradley-Terry-Luce  model  to  analyze  our  data  we  show  that  some  results  contradict  current  conventions  on  visual  design  for  rectangular  displays.  We  then  extracted  the  most  interesting  cases  and  conducted  a  follow  up  study  with  additional  shapes  to  investigate  how  the  principles  generalize.  From  these  results  we  propose  a  set  of  guidelines  for  designing  visual  content  for  non-rectangular  displays.
2	Co  3deator  a  team  first  collaborative  3d  design  ideation  tool.  We  present  Co-3Deator,  a  sketch-based  collaborative  3D  modeling  system  based  on  the  notion  of  "team-first"  ideation  tools,  where  the  needs  and  processes  of  the  entire  design  team  come  before  that  of  an  individual  designer.  Co-3Deator  includes  two  specific  team-first  features:  a  concept  component  hierarchy  which  provides  a  design  representation  suitable  for  multi-level  sharing  and  reusing  of  design  information,  and  a  collaborative  design  explorer  for  storing,  viewing,  and  accessing  hierarchical  design  data  during  collaborative  design  activities.  We  conduct  two  controlled  user  studies,  one  with  individual  designers  to  elicit  the  form  and  functionality  of  the  collaborative  design  explorer,  and  the  other  with  design  teams  to  evaluate  the  utility  of  the  concept  component  hierarchy  and  design  explorer  towards  collaborative  design  ideation.  Our  results  support  our  rationale  for  both  of  the  proposed  team-first  collaboration  mechanisms  and  suggest  further  ways  to  streamline  collaborative  design.
2	Allergybot  a  chatbot  technology  intervention  for  young  adults  with  food  allergies  dining  out.  Dining  out  is  one  of  the  biggest  challenges  people  with  food  allergies  face.  For  young  adults,  especially,  the  fear  of  having  an  allergic  reaction  when  dining  out  impairs  social  aspects  of  their  life.  The  exhausting  process  of  searching  online  and  communicating  with  restaurants  also  increases  their  anxiety.      To  improve  the  quality  of  life  for  young  adults  with  food  allergies,  we  present  AllergyBot,  an  intelligent  and  humane  Chatbot  that  provides  restaurants'  allergy  accommodation  information  based  on  users'  allergens.  We  use  established  instant  messaging  platforms  to  create  a  form  of  conversation  that  young  adults  are  familiar  with.  AllergyBot  aims  to  reduce  the  users'  inquiry  overload,  improve  their  overall  dining  out  experiences,  and  support  their  social  life.
2	Development  of  a  checklist  for  the  prevention  of  intradialytic  hypotension  in  hemodialysis  care  design  considerations  based  on  activity  theory.  Hemodialysis  is  life-saving  therapy  for  end-stage  renal  disease;  yet,  20%  of  hemodialysis  sessions  are  complicated  by  intradialytic  hypotension  ("IDH").  There  is  a  need  for  approaches  to  preventing  IDH  that  account  for  their  implementation  contexts.  Using  Activity  Theory,  we  outline  the  design  of  a  digital  diagnostic  checklist  to  identify  patients  at  risk  of  IDH.  Checklists  were  chosen  a  priori  as  an  outcome  due  to  prior  evidence  of  effectiveness.  Drawing  on  individual  interviews  with  20  clinicians  and  three  focus  groups  with  17  patients,  we  describe  four  activity  systems  within  hemodialysis  care.  We  then  outline  a  novel  design  process  that  includes  co-design  activities  with  clinicians,  and  four  rapid-cycle  iterations  that  progressively  incorporated  activity  system  elements  into  checklist  design.  We  contribute  a  new  type  of  checklist  design  to  HCI:  one  that  supports  diagnostic  thinking  rather  than  consistent  task  completion.  We  further  broaden  checklist  design  by  including  a  formal  role  for  patients  in  checklist  completion.
2	Content  is  king  leadership  lags  effects  of  prior  experience  on  newcomer  retention  and  productivity  in  online  production  groups.  Organizers  of  online  groups  often  struggle  to  recruit  members  who  can  most  effectively  carry  out  the  group's  activities  and  remain  part  of  the  group  over  time.  In  a  study  of  a  sample  of  30,000  new  editors  belonging  to  1,054  English  WikiProjects,  we  empirically  examine  the  effects  of  generalized  prior  work-productivity  experience  (measured  by  overall  prior  article  edits),  prior  leadership  experience  (measured  by  overall  prior  project  edits),  and  localized  prior  work-productivity  experience  (measured  by  pre-joining  article  edits  on  a  project)  on  early  retention  and  productivity.  We  find  that  (1)generalized  prior  work-productivity  experience  is  positively  associated  with  retention,  but  negatively  associated  with  productivity  (2)  prior  leadership  experience  is  negatively  associated  with  both  retention  and  productivity,  and  (3)  localized  prior  work-productivity  experience  is  positively  associated  with  both  retention  and  productivity  within  that  focal  project.  We  then  discuss  implications  to  inform  the  designs  of  early  interventions  aimed  at  group  success.
2	Supporting  communication  between  grandparents  and  grandchildren  through  tangible  storytelling  systems.  Grandparents  and  grandchildren  that  live  apart  often  rely  on  communication  technologies,  such  as  messengers,  video  conferencing,  and  phone  calls  for  maintaining  relationships.  While  some  of  these  systems  are  challenging  for  grandparents,  others  are  less  engaging  for  children.  To  facilitate  communication,  we  developed  StoryBox,  a  tangible  device  that  allows  sharing  photos,  tangible  artifacts,  and  audio  recordings  of  everyday  life.  We  conducted  a  preliminary  study  with  two  families  to  identify  design  issues,  and  further  refine  the  prototype.  Subsequently,  we  conducted  a  field  study  with  four  families  for  up  to  four  weeks  to  better  understand  real-world  use  and  examine  inter-generational  connectedness.  We  found  that  StoryBox  was  accessible,  simple,  and  helped  bridge  the  technological  gap  between  grandparents  and  grandchildren.  Children  communicated  asynchronously  in  a  playful  and  idiosyncratic  manner,  and  grandparents  shared  past  family  memories.  We  provide  insights  on  how  to  ease  communication  between  different  generations,  engage  them  in  sharing  activities,  and  strengthen  family  relationships.
2	Supporting  rhythm  activities  of  deaf  children  using  music  sensory  substitution  systems.  Rhythm  is  the  first  musical  concept  deaf  people  learn  in  music  classes.  However,  hearing  loss  limits  the  amount  of  information  that  allows  a  deaf  person  to  evaluate  his  or  her  performance  and  stay  in  sync  with  other  musicians.  In  this  paper,  we  investigated  how  a  visual  and  vibrotactile  music-sensory-substitution  device,  MuSS-Bits++,  affects  rhythm  discrimination,  reproduction,  and  expressivity  of  deaf  people.  We  conducted  a  controlled  study  with  11  deaf  children  and  found  that  most  participants  felt  more  confident  wearing  the  device  in  vibration  mode  even  when  it  did  not  objectively  improve  their  accuracy.  Furthermore,  we  studied  how  MuSS-Bits++  can  be  used  in  music  classes  at  deaf  schools  and  what  challenges  and  opportunities  arise  in  such  a  setting.  Based  on  these  studies,  we  discuss  insights  and  future  directions  that  support  the  design  and  development  of  music-sensory-substitution  systems  for  music  making.
2	Exploring  the  design  space  for  parent  child  reading.  Given  the  significant  potential  of  shared  book  reading  to  promote  children's  learning,  the  design  of  e-books  has  focused  on  maximising  this  learning  experience.  However,  recent  studies  have  begun  to  show  that  shared  reading  is  a  broader  opportunity  for  the  family  to  spend  quality  time  together.  Our  study  aims  to  explore  this  perspective  further,  focusing  on  the  types  of  parent-child  interactions  during  shared  reading  and  the  ways  in  which  shared  reading  may  foster  intimacy  when  parents  and  children  read  digital  books.  We  used  cultural  probes  and  contextual  interviews  to  capture  the  shared  reading  experiences  of  7  parents  and  6  children  in  their  homes.  We  discuss  the  different  nuances  of  the  shared  reading  practices  identified.  We  use  these  findings  to  suggest  new  design  opportunities  that  support  the  complex  practices  of  shared  reading  with  technologies  at  home.
2	It  should  be  a  game  for  fun  not  exercise  tensions  in  designing  health  related  features  for  pokemon  go.  Leveraging  existing  popular  games  such  as  Pokemon  GO  to  promote  health  can  engage  people  in  healthy  activities  without  sacrificing  gaming  appeal.  However,  little  is  known  about  what  potential  tensions  arise  from  incorporating  new  health-related  features  to  already  existing  and  popular  games  and  how  to  resolve  those  tensions  from  players'  perspectives.  In  this  paper,  we  identify  design  tensions  surrounding  the  appeals  of  Pokemon  GO,  perspectives  on  different  health  needs,  and  mobile  health  technologies.  By  conducting  surveys  and  design  workshops  with  20  avid  Pokemon  GO  players,  we  demonstrate  four  design  tensions:  (1)  diverse  goals  and  rewards  vs.  data  accuracy,  (2)  strong  bonds  between  players  and  characters  vs.  gaming  obsession,  (3)  collaborative  play  vs.  social  anxiety,  and  (4)  connection  of  in-real-life  experiences  with  the  game  vs.  different  individual  contexts.  We  provide  design  implications  to  resolve  these  tensions  in  Pokemon  GO  and  discuss  how  to  extend  our  findings  to  the  broader  context  of  health  promotion  in  location-based  games.
2	Pinchtype  text  entry  for  virtual  and  augmented  reality  using  comfortable  thumb  to  fingertip  pinches.  Text  entry  is  an  integral  component  to  many  use  cases  in  virtual  and  augmented  reality.  We  present  PinchType:  A  new  method  of  virtual  text  entry  that  combines  users'  existing  knowledge  of  the  QWERTY  keyboard  layout  with  simple  thumb  and  finger  interactions.  Users  pinch  with  the  thumb  and  fingertip  to  select  from  the  same  group  of  letters  the  finger  would  press  on  a  QWERTY  keyboard;  a  language  model  disambiguates.  In  a  preliminary  study  with  14  participants,  we  investigated  PinchType's  speed  and  accuracy  on  initial  use,  as  well  as  its  physical  comfort  relative  to  a  mid-air  keyboard.  After  entering  40  phrases,  most  people  reported  that  PinchType  was  more  comfortable  than  the  mid-air  keyboard.  Most  participants  reached  a  mean  speed  of  12.54  WPM,  or  20.07  WPM  without  the  time  spent  correcting  errors.  This  compares  favorably  to  other  thumb-to-finger  virtual  text  entry  methods.
2	Relationship  between  visual  complexity  and  aesthetics  of  webpages.  Substantial  HCI  research  investigated  the  relationship  between  webpage  complexity  and  aesthetics,  but  without  a  definitive  conclusion.  Some  research  showed  an  inverse  linear  correlation,  some  other  showed  an  inverted  u-shaped  curve,  while  the  rest  showed  no  relationship  at  all.  Such  a  lack  of  clarity  complicates  hypothesis  formulation  and  result  interpretation  for  future  research,  and  lowers  the  reliability  and  generalizability  of  potential  advice  for  Web  design  practice.  We  re-collected  complexity  and  aesthetics  ratings  for  five  datasets  previously  used  in  webpage  aesthetics  and  complexity  research.  The  results  were  mixed,  but  suggested  an  inverse  linear  relationship  with  a  weaker  u-shaped  sub-component.  A  subsequent  visual  inspection  of  revealed  several  confounding  factors  that  may  have  led  to  the  mixed  results,  including  some  webpages  looking  broken  or  archaic.  The  second  data  collection  showed  that  accounting  for  these  factors  generally  eliminates  the  u-shaped  tendency  of  the  complexity-aesthetics  relationship,  at  least,  for  a  relatively  homogeneous  sample  of  English-speaking  participants.
2	Muscle  propelled  force  feedback  bringing  force  feedback  to  mobile  devices.  Force  feedback  devices  resist  miniaturization,  because  they  require  physical  motors  and  mechanics.  We  propose  mobile  force  feedback  by  eliminating  motors  and  instead  actuating  the  user's  muscles  using  electrical  stimulation.  Without  the  motors,  we  obtain  substantially  smaller  and  more  energy-efficient  devices.  We  present  a  prototype  that  fits  on  the  back  of  a  mobile  phone.  It  actuates  users'  forearm  muscles  via  four  electrodes,  which  causes  users'  muscles  to  contract  involuntarily,  so  that  they  tilt  the  device  sideways.  As  users  resist  this  motion  using  their  other  arm,  they  perceive  force  feedback.  We  demonstrate  the  interaction  at  the  example  of  an  interactive  videogame  in  which  users  steer  an  airplane  through  winds  rendered  using  force  feedback.  In  a  first  user  study,  we  found  our  device  to  cause  users  to  produce  up  to  18.7N  of  force,  when  used  to  actuate  their  palm  flexors.  In  a  second  study,  participants  played  the  video  game  de-scribed  above;  all  ten  participants  reported  to  prefer  the  experience  of  muscle-propelled  force  feedback  to  vibrotactile  feedback.
2	Pass  them  around  collaborative  use  of  mobile  phones  for  photo  sharing.  In  this  paper  we  explore  shared  collocated  interactions  with  mobile  phones.  We  introduce  a  phone-based  application  that  allows  a  small  group  of  collocated  people  to  share  photos  using  the  metaphor  of  passing  paper  photos  around.  The  prototype  encourages  people  to  share  their  devices  and  use  them  interchangeably  while  discussing  photos  face-to-face.  The  prototype  supports  ad-hoc  photo  sharing  in  different  contexts  by  taking  into  account  the  spatial  arrangement  of  users  around  a  table,  measured  with  sensors  embedded  in  their  mobile  phones.  Our  evaluations  show  that  people  are  willing  to  share  and  connect  their  mobile  phones  to  engage  in  collaborative  interactions.  Participants  were  able  to  easily  share  their  collections  of  photos  using  our  proposed  interaction  techniques.
2	Predicting  postpartum  changes  in  emotion  and  behavior  via  social  media.  We  consider  social  media  as  a  promising  tool  for  public  health,  focusing  on  the  use  of  Twitter  posts  to  build  predictive  models  about  the  forthcoming  influence  of  childbirth  on  the  behavior  and  mood  of  new  mothers.  Using  Twitter  posts,  we  quantify  postpartum  changes  in  376  mothers  along  dimensions  of  social  engagement,  emotion,  social  network,  and  linguistic  style.  We  then  construct  statistical  models  from  a  training  set  of  observations  of  these  measures  before  and  after  the  reported  childbirth,  to  forecast  significant  postpartum  changes  in  mothers.  The  predictive  models  can  classify  mothers  who  will  change  significantly  following  childbirth  with  an  accuracy  of  71%,  using  observations  about  their  prenatal  behavior,  and  as  accurately  as  80-83%  when  additionally  leveraging  the  initial  2-3  weeks  of  postnatal  data.  The  study  is  motivated  by  the  opportunity  to  use  social  media  to  identify  mothers  at  risk  of  postpartum  depression,  an  underreported  health  concern  among  large  populations,  and  to  inform  the  design  of  low-cost,  privacy-sensitive  early-warning  systems  and  intervention  programs  aimed  at  promoting  wellness  postpartum.
2	Sustainably  unpersuaded  how  persuasion  narrows  our  vision  of  sustainability.  In  this  paper  we  provide  a  critical  analysis  of  persuasive  sustainability  research  from  2009-2011.  Drawing  on  critical  sociological  theory  of  modernism,  we  argue  that  persuasion  is  based  on  a  limited  framing  of  sustainability,  human  behavior,  and  their  interrelationship.  This  makes  supporting  sustainability  easier,  but  leads  to  characteristic  patterns  of  breakdown.  We  then  detail  problems  that  emerge  from  this  narrowing  of  vision,  such  as  how  the  framing  of  sustainability  as  the  optimization  of  a  simple  metrics  places  technologies  incorrectly  as  objective  arbiters  over  complex  issues  of  sustainability.  We  conclude  by  suggesting  alternative  approaches  to  move  beyond  these  problems.
2	Rehandle  towards  integrating  physical  rehabilitation  in  everyday  life.  In  this  paper  we  present  ReHandle,  an  emerging  design  space  currently  inhabited  and  shaped  by  three  different  design  sketches.  We  describe  how  the  three  sketches  point  to  three  possible  dimensions  for  exploring  the  role  of  digital  technology  in  facilitating  self-monitoring;  aimed  at  promoting  an  integration  of  the  rehab  activities  with  the  everyday  activities  of  senior  citizens.  We  expect  that  our  articulation  of  the  emerging  ReHandle  design  space  will  be  informative  and  inspirational  for  the  interaction  design  and  HCI  community  exploring  the  role  of  digital  technology  for  successful  rehabilitation  of  senior  citizens.
2	Shape  your  body  control  a  virtual  silhouette  using  body  motion.  In  this  paper  we  propose  to  use  our  body  as  a  puppetry  controller,  giving  life  to  a  virtual  silhouette  through  acting.  A  framework  was  deployed  based  on  Microsoft  Kinect  using  OpenNI  and  Unity  to  animate  in  real-time  a  silhouette.  This  was  used  to  perform  a  set  of  experiments  related  to  the  user's  interaction  with  human  and  non-human  like  puppets.  We  believe  that  a  performance-driven  silhouette  can  be  just  as  expressive  as  a  traditional  shadow  puppet  with  a  high  degree  of  freedom,  making  use  of  our  entire  body  as  an  input.  We  describe  our  solution  that  allows  real-time  interactive  control  of  virtual  shadow  puppets  for  performance  animation  based  on  body  motion.  We  show  through  our  experiment,  performed  by  non-expert  artists,  that  using  our  body  to  control  puppets  is  like  mixing  the  performance  of  an  actor  with  the  manipulation  of  a  puppeteer.
2	Pass  the  ball  enforced  turn  taking  in  activity  tracking.  We  have  developed  a  mobile  application  called  Pass  The  Ball  that  enables  users  to  track,  reflect  on,  and  discuss  physical  activity  with  others.  We  followed  an  iterative  design  process,  trialling  a  first  version  of  the  app  with  20  people  and  a  second  version  with  31.  The  trials  were  conducted  in  the  wild,  on  users'  own  devices.  The  second  version  of  the  app  enforced  a  turn-taking  system  that  meant  only  one  member  of  a  group  of  users  could  track  their  activity  at  any  one  time.  This  constrained  tracking  at  the  individual  level,  but  more  successfully  led  users  to  communicate  and  interact  with  each  other.  We  discuss  the  second  trial  with  reference  to  two  concepts:  social-relatedness  and  individual-competence.  We  discuss  six  key  lessons  from  the  trial,  and  identify  two  high-level  design  implications:  attend  to  "practices"  of  tracking;  and  look  within  and  beyond  "collaboration"  and  "competition"  in  the  design  of  activity  trackers.
2	Designing  for  a  billion  users  a  case  study  of  facebook.  Facebook  is  the  world's  largest  social  network,  connecting  over  800  million  users  worldwide.  The  type  of  phenomenal  growth  experienced  by  Facebook  in  a  short  time  is  rare  for  any  technology  company.  As  the  Facebook  user  base  approaches  the  1  billion  mark,  a  number  of  exciting  opportunities  await  the  world  of  social  networking  and  the  future  of  the  web.  We  present  a  case  study  of  what  it  is  like  to  design  for  a  billion  users  at  Facebook  from  the  perspective  of  designers,  engineers,  managers,  user  experience  researchers,  and  other  stakeholders  at  the  company.  Our  case  study  illustrates  various  complexities  and  tradeoffs  in  design  through  a  Human-Computer  Interaction  (HCI)  lens  and  highlights  implications  for  tackling  the  challenges  through  research  and  practice.
2	The  effects  of  tactile  feedback  and  movement  alteration  on  interaction  and  awareness  with  digital  embodiments.  Collaborative  tabletop  systems  can  employ  direct  touch,  where  people's  real  arms  and  hands  manipulate  objects,  or  indirect  input,  where  people  are  represented  on  the  table  with  digital  embodiments.  The  input  type  and  the  resulting  embodiment  dramatically  influence  tabletop  interaction:  in  particular,  the  touch  avoidance  that  naturally  governs  people's  touching  and  crossing  behavior  with  physical  arms  is  lost  with  digital  embodiments.  One  result  of  this  loss  is  that  people  are  less  aware  of  each  others'  arms,  and  less  able  to  coordinate  actions  and  protect  personal  territories.  To  determine  whether  there  are  strategies  that  can  influence  group  interaction  on  shared  digital  tabletops,  we  studied  augmented  digital  arm  embodiments  that  provide  tactile  feedback  or  movement  alterations  when  people  touched  or  crossed  arms.  The  study  showed  that  both  augmentation  types  changed  people's  behavior  (people  crossed  less  than  half  as  often)  and  also  changed  their  perception  (people  felt  more  aware  of  the  other  person's  arm,  and  felt  more  awkward  when  touching).  This  work  shows  how  groupware  designers  can  influence  people's  interaction,  awareness,  and  coordination  abilities  when  physical  constraints  are  absent.
2	Revisiting  social  practices  surrounding  music.  Music  shapes  our  social  lives.  While  previous  research  has  provided  a  foundational  understanding  of  the  social  affordances  surrounding  people's  interactions  with  music,  there  is  a  need  to  update  this  understanding  in  light  of  recent  key  developments  in  our  digital  technological  landscape.  This  paper  describes  a  qualitative  study  of  people's  social  activities  and  practices  around  music  in  households.  It  extends  previous  research  by  revealing  the  impact  key  technologies  have  on  how,  where,  when,  and  with  who  people's  interactions  surrounding  music  occur.  It  also  reveals  people's  creative  attempts  to  design  their  musical  experiences  with  others  through  reconfiguring  and  connecting  to  various  digital  technologies  and  digital  platforms  in  order  to  pursue  more  opportunities  for  communicating,  sharing,  bonding,  and  celebrating  lives  with  others.
2	Splitboard  a  simple  split  soft  keyboard  for  wristwatch  sized  touch  screens.  Text  entry  on  a  smartwatch  is  a  challenging  problem  due  to  the  device's  limited  screen  area.  In  this  paper,  we  introduce  the  SplitBoard,  which  is  a  soft  keyboard  designed  for  a  smartwatch.  As  the  user  flicks  left  or  right  on  the  keyboard,  it  switches  between  the  left  and  right  halves  of  a  QWERTY  keyboard.  We  report  the  results  of  two  user  experiments  where  the  SplitBoard  was  compared  to  an  ordinary  QWERTY  keyboard,  the  ZoomBoard,  SlideBoard,  and  Qwerty-like  keypad.  We  measured  the  initial  performance  with  new  users  for  each  method.  The  SplitBoard  outperformed  all  other  techniques  in  the  experiments.  The  SplitBoard  is  expected  to  be  a  viable  option  for  smartwatch  text  entry  because  of  its  light  processing  requirements,  good  performance,  and  immediate  learnability.
2	Evaluating  item  item  similarity  algorithms  for  movies.  Recommender  systems  such  as  those  used  in  e-commerce  or  Video-On-Demand  systems  generally  show  users  a  list  of  "similar  items."  Many  algorithms  exist  to  calculate  item-item  similarity  and  we  wished  to  evaluate  how  users  perceive  these  numerically  expressed  similarity.  In  our  experiment,  we  performed  a  user  study  with  four  similarity  algorithms  to  evaluate  perceived  correctness  in  item-item  similarity  as  it  relates  to  movies.  We  implemented  three  algorithms:  collaborative  filtering  with  Pearson,  collaborative  filtering  with  cosine,  and  content-filtering  with  TF-IDF.  A  pre-generated  similarity  list  from  TheMovieDB.org  (TMDb)  was  used  as  the  baseline.  Our  experiment  showed  that  TMDb  has  the  highest  perceived  similarity,  followed  by  cosine  and  TF-IDF,  while  Pearson  was  practically  unusable  for  users.  A  by-product  of  our  experiment  was  a  set  of  similar  movie  pairs,  which  we  intend  to  use  for  offline  evaluation.
2	Tablecross  exuding  a  shared  space  into  personal  spaces  to  encourage  its  voluntary  maintenance.  A  shared  space  should  be  cooperatively  maintained  by  all  users.  However,  due  to  social  loafing,  often  nobody  maintains  it  and  its  condition  worsens.  We  propose  exudation  of  a  shared  space.  Part  of  a  shared  space  is  exuded  into  personal  workspaces  so  that  office  workers  are  forced  to  subjectively  experience  the  atmosphere  of  the  shared  space,  even  while  they  remain  at  their  personal  workspaces.  This  paper  illustrates  the  first  prototype  named  "TableCross,"  which  reflects  the  degree  of  disorder  of  a  table  in  a  shared  space  to  the  desktop  of  each  worker's  PC.  We  also  report  some  results  of  our  pilot  user  study.
2	Experiences  of  delivering  a  public  health  data  service.  The  turn  to  in-the-wild  within  HCI  has  given  rise  to  an  increasing  concern  around  designing  technologies  which  are  available  at  large  scale.  Uniquely,  at  the  intersection  of  public  health  and  HCI,  our  work  has  supported  the  deployment  of  a  mobile  application,  FeedFinder,  over  the  last  three  years.  We  delineate  the  ground-work  that  was  required  to  sustain  this  mobile  application  over  the  long-term.  Focussing  in  particular  on  efforts  made  to  engage  institutions  in  taking  ownership  over  FeedFinder  and  the  data  it  provides,  we  reflect  on  the  tensions  that  arose  between  users  and  civic  institutions,  particularly  around  "what  matters".  We  provide  a  reflection  on  key  requirements  when  designing  a  health  data  service  and  provide  three  lessons  learnt  which  can  guide  researchers  toward  their  own  successful  and  productive  long-term  research  deployments.
2	Swivrchair  a  motorized  swivel  chair  to  nudge  users  orientation  for  360  degree  storytelling  in  virtual  reality.  We  present  SwiVRChair,  a  motorized  swivel  chair  to  nudge  users'  orientation  in  360  degree  storytelling  scenarios.  Since  rotating  a  scene  in  virtual  reality  (VR)  leads  to  simulator  sickness,  storytellers  currently  have  no  way  of  controlling  users'  attention.  SwiVRChair  allows  creators  of  360  degree  VR  movie  content  to  be  able  to  rotate  or  block  users'  movement  to  either  show  certain  content  or  prevent  users  from  seeing  something.  To  enable  this  functionality,  we  modified  a  regular  swivel  chair  using  a  24V  DC  motor  and  an  electromagnetic  clutch.  We  developed  two  demo  scenarios  using  both  mechanisms  (rotate  and  block)  for  the  Samsung  GearVR  and  conducted  a  user  study  (n=16)  evaluating  the  presence,  enjoyment  and  simulator  sickness  for  participants  using  SwiVRChair  compared  to  self  control  (Foot  Control).  Users  rated  the  experience  using  SwiVRChair  to  be  significantly  more  immersive  and  enjoyable  whilst  having  a  decrease  in  simulator  sickness.
2	Making  3d  printed  objects  interactive  using  wireless  accelerometers.  We  present  an  approach  that  allows  designers  and  others  to  quickly  and  easily  make  3D  printed  objects  interactive,  without  the  need  for  hardware  or  software  expertise  and  with  little  modification  to  an  object's  physical  design.  With  our  approach,  a  designer  simply  attaches  or  embeds  small  three-axis  wireless  accelerometer  modules  into  the  moving  parts  of  a  3D  printed  object.  A  simple  graphical  user  interface  is  then  used  to  configure  the  system  to  interpret  the  movements  of  these  accelerometers  as  if  they  were  common  physical  controls  such  as  buttons  or  dials.  The  designer  can  then  associate  events  generated  by  these  controls  with  a  range  of  interactive  behavior,  including  web  browser  and  media  player  control.
2	Openhtml  designing  a  transitional  web  editor  for  novices.  We  describe  the  initial  design  rationale  and  early  findings  from  studies  of  a  web  editor  for  beginners  called  openHTML.  We  explain  our  strategy  of  transitional  design  that  views  web  editors  as  a  part  of  a  complex  socio-technical  system  that  spans  multiple  tools,  practices,  and  actors.  Our  goal  is  to  create  a  toolkit  that  can  engage  beginners  in  meaningful  activities  now  and  prepare  them  for  more  sophisticated  activities  in  the  future.
2	Pixeltone  a  multimodal  interface  for  image  editing.  Photo  editing  can  be  a  challenging  task,  and  it  becomes  even  more  difficult  on  the  small,  portable  screens  of  mobile  devices  that  are  now  frequently  used  to  capture  and  edit  images.  To  address  this  problem  we  present  PixelTone,  a  multimodal  photo  editing  interface  that  combines  speech  and  direct  manipulation.  We  observe  existing  image  editing  practices  and  derive  a  set  of  principles  that  guide  our  design.  In  particular,  we  use  natural  language  for  expressing  desired  changes  to  an  image,  and  sketching  to  localize  these  changes  to  specific  regions.  To  support  the  language  commonly  used  in  photo-editing  we  develop  a  customized  natural  language  interpreter  that  maps  user  phrases  to  specific  image  processing  operations.  Finally,  we  perform  a  user  study  that  evaluates  and  demonstrates  the  effectiveness  of  our  interface.
2	Universal  design  ballot  interfaces  on  voting  performance  and  satisfaction  of  voters  with  and  without  vision  loss.  Voting  is  a  glocalized  event  across  countries,  states  and  municipalities  in  which  individuals  of  all  abilities  want  to  participate.  To  enable  people  with  disabilities  to  participate  accessible  voting  is  typically  implemented  by  adding  assistive  technologies  to  electronic  voting  machines  to  accommodate  people  with  disabilities.  To  overcome  the  complexities  and  inequities  in  this  practice,  two  interfaces,  EZ  Ballot,  which  uses  a  linear  yes/no  input  system  for  all  selections,  and  QUICK  Ballot,  which  provides  random  access  voting  through  direct  selection,  were  designed  to  provide  one  system  for  all  voters.  This  paper  reports  efficacy  testing  of  both  interfaces.  The  study  demonstrated  that  voters  with  a  range  of  visual  abilities  were  able  to  use  both  ballots  independently.  While  non-sighted  voters  made  fewer  errors  on  the  linear  ballot  (EZ  Ballot),  partially-sighted  and  sighted  voters  completed  the  random  access  ballot  (QUICK  Ballot)  in  less  time.  In  addition,  a  higher  percentage  of  non-sighted  participants  preferred  the  linear  ballot,  and  a  higher  percentage  of  sighted  participants  preferred  the  random  ballot.
2	Smart  flashlight  map  navigation  using  a  bike  mounted  projector.  While  mobile  phones  affect  our  behavior  and  tend  to  separate  us  from  our  physical  environment,  this  very  environment  could  instead  become  a  responsive  part  of  the  information  domain.  For  navigation  using  a  map  while  cycling  in  an  urban  environment,  we  studied  two  alternative  solutions:  smartphone  display  and  projection  on  the  road.  This  paper  firstly  demonstrates  by  proof-of-concept  a  GPS-based  map  navigation  using  a  bike-mounted  projector.  Secondly,  it  implements  a  prototype  using  both  a  projector  and  a  smartphone  mounted  on  a  bike,  comparing  them  for  use  in  a  navigation  system  for  nighttime  cycling.  Thirdly,  it  examines  how  visuo-spatial  factors  influence  navigation.  We  believe  that  our  findings  will  be  useful  for  designing  navigation  systems  for  bikes  and  even  for  cars,  helping  cyclists  and  drivers  be  more  attentive  to  their  environment  while  navigating,  and  to  provide  useful  information  while  moving.
2	Crowdsourcing  suggestions  to  programming  problems  for  dynamic  web  development  languages.  Developers  increasingly  consult  online  examples  and  message  boards  to  find  solutions  to  common  programming  tasks.  On  the  web,  finding  solutions  to  debugging  problems  is  harder  than  searching  for  working  code.  Prior  research  introduced  a  social  recommender  system,  HelpMeOut,  that  crowdsources  debugging  suggestions  by  presenting  fixes  to  errors  that  peers  have  applied  in  the  past.  However,  HelpMeOut  only  worked  for  statically  typed,  compiled  programming  languages  like  Java.  We  investigate  how  suggestions  can  be  provided  for  dynamic,  interpreted  web  development  languages.  Our  primary  insight  is  to  instrument  test-driven  development  to  collect  examples  of  bug  fixes.  We  present  Crowd::Debug,  a  tool  for  Ruby  programmers  that  realizes  these  benefits.
2	Influencers  in  multiplayer  online  shooters  evidence  of  social  contagion  in  playtime  and  social  play.  In  a  wide  range  of  social  networks,  people's  behavior  is  influenced  by  social  contagion:  we  do  what  our  network  does.  Networks  often  feature  particularly  influential  individuals,  commonly  called  "influencers."  Existing  work  suggests  that  in-game  social  networks  in  online  games  are  similar  to  real-life  social  networks  in  many  respects.  However,  we  do  not  know  whether  there  are  in-game  equivalents  to  influencers.  We  therefore  applied  standard  social  network  features  used  to  identify  influencers  to  the  online  multiplayer  shooter  Tom  Clancy's  The  Division.  Results  show  that  network  feature-defined  influencers  had  indeed  an  outsized  impact  on  playtime  and  social  play  of  players  joining  their  in-game  network.
2	Thermorph  democratizing  4d  printing  of  self  folding  materials  and  interfaces.  We  develop  a  novel  method  printing  complex  self-folding  geometries.  We  demonstrated  that  with  a  desktop  fused  deposition  modeling  (FDM)  3D  printer,  off-the-shelf  printing  filaments  and  a  design  editor,  we  can  print  flat  thermoplastic  composites  and  trigger  them  to  self-fold  into  3D  with  arbitrary  bending  angles.  This  is  a  suitable  technique,  called  Thermorph,  to  prototype  hollow  and  foldable  3D  shapes  without  losing  key  features.  We  describe  a  new  curved  folding  origami  design  algorithm,  compiling  given  arbitrary  3D  models  to  2D  unfolded  models  in  G-Code  for  FDM  printers.  To  demonstrate  the  Thermorph  platform,  we  designed  and  printed  complex  self-folding  geometries  (up  to  70  faces),  including  15  self-curved  geometric  primitives  and  4  self-curved  applications,  such  as  chairs,  the  simplified  Stanford  Bunny  and  flowers.  Compared  to  the  standard  3D  printing,  our  method  saves  up  to  60%  -  87%  of  the  printing  time  for  all  shapes  chosen.
2	Hapticserpent  a  wearable  haptic  feedback  robot  for  vr.  Haptic  feedback  is  an  important  part  of  virtual  reality  (VR),  where  it  can  increase  the  immersion  and  enjoyment.  Within  VR,  research  literature  and  products  are  mainly  limited  to  vibrotactile  feedback  for  the  torso.  We  believe  that  additional  types  of  haptic  feedback  around  other  areas  of  the  body  could  potentially  yield  interesting  VR  experiences.  Thus,  we  present  HapticSerpent,  which  is  a  waist-worn  robot  capable  of  various  haptic  feedback  on  the  torso,  neck,  face,  arms  and  hands.  We  present  our  implementation  specifications,  followed  by  an  initial  evaluation  to  measure  the  distinguishability  of  taps  applied  to  the  torso.  Also,  we  surveyed  the  acceptability  of  receiving  feedback  in  different  locations  on  the  body.  Participants  noted  an  overall  higher  accuracy  on  the  upper  and  sides  of  the  torso  and  they  generally  disfavored  haptic  feedback  in  sensitive  areas  due  to  potential  harm.  Lastly,  we  discuss  various  research  opportunities  and  challenges  and  present  our  future  direction.
2	Supernumerary  arms  for  gestural  communication.  Cyborgs  are  human-machine  hybrids  with  organic  and  mechatronic  body  parts.  Like  humans,  cyborgs  may  use  their  additional  body  parts  for  physical  tasks  and  communication.  In  this  study,  we  investigate  how  additional  arms  can  be  used  to  communicate.  While  using  additional  arms  to  perform  physical  tasks  has  been  researched,  using  them  to  communicate  is  an  area  that  is  largely  unexplored.  Our  study  is  divided  into  three  stages:  a  pilot  study,  implementation,  and  a  user  study.  In  this  paper,  we  discuss  our  efforts  as  related  to  the  first  two  stages  of  our  study.  The  pilot  study  was  used  to  determine  user  expectations  for  the  arms.  Participants  found  the  arms  effective  for  describing  an  area  from  a  fixed  location.  Users  also  preferred  additional  arms  that  can  be  controlled  and  are  physically  similar  to  their  existing  arms.  Our  prototype  consists  of  a  virtual  mirror  that  augments  the  user's  body  with  additional  arms.  We  discuss  future  directions  for  improving  our  implementation  and  outline  a  plan  for  the  user  study.
2	Rewire  interface  design  assistance  from  examples.  Interface  designers  often  use  screenshot  images  of  example  designs  as  building  blocks  for  new  designs.  Since  images  are  unstructured  and  hard  to  edit,  designers  typically  reconstruct  screenshots  with  vector  graphics  tools  in  order  to  reuse  or  edit  parts  of  the  design.  Unfortunately,  this  reconstruction  process  is  tedious  and  slow.  We  present  Rewire,  an  interactive  system  that  helps  designers  leverage  example  screenshots.  Rewire  automatically  infers  a  vector  representation  of  screenshots  where  each  UI  component  is  a  separate  object  with  editable  shape  and  style  properties.  Based  on  this  representation,  the  system  provides  three  design  assistance  modes  that  help  designers  reuse  or  redraw  components  of  the  example  design.  The  results  from  our  quantitative  and  user  evaluations  demonstrate  that  Rewire  can  generate  accurate  vector  representations  of  interface  screenshots  found  in  the  wild  and  that  design  assistance  enables  users  to  reconstruct  and  edit  example  designs  more  efficiently  compared  to  a  baseline  design  tool.
2	Share  and  share  alike  social  information  and  interaction  style  in  coordination  of  shared  use.  Interfaces  are  commonly  designed  from  the  perspective  of  individual  users,  even  though  most  of  the  systems  we  use  in  everyday  life  are  in  fact  shared.  We  argue  that  more  attention  is  needed  for  system  sharing,  especially  because  interfaces  are  known  to  influence  coordination  of  shared  use.  In  this  work,  we  aim  to  deepen  the  understanding  of  this  relation.  To  do  so,  we  design  three  interfaces  for  a  shared  lighting  system  that  vary  in  the  type  of  social  information  they  allow  people  to  share  with  others  and  in  their  overall  interaction  style.  We  systematically  compare  longitudinal  and  real-life  use  of  the  interfaces,  evaluating  (1)  people's  appraisal  of  three  types  of  social  information  and  (2)  the  influence  of  an  interaction  style  on  coordination  of  shared  use.  The  results  disclose  relations  between  the  interface  and  the  amount  of  verbal  communication,  consideration,  and  accountability.  With  this  work,  we  urge  the  need  for  interaction  designers  to  consider  shared  use.
2	Gamification  of  a  to  do  list  with  emotional  reinforcement.  Gamification  can  change  how  and  why  people  interact  with  software.  A  common  approach  is  to  use  quantitative  feedback  to  give  users  a  feeling  of  progress  or  achievement.  There  are,  however,  other  ways  to  provide  users  with  motivation  or  meaning  during  normal  computer  interactions,  such  as  using  emotional  reinforcement.  This  could  provide  a  powerful  new  tool  to  allow  the  positive  effects  of  gamification  to  reach  wider  contexts.  This  paper  investigates  the  design  and  evaluation  of  a  mobile  to-do  list  application,  'Tamu  To-Do',  which  utilises  gamified  emotional  reinforcement,  as  seen  in  Figure  1.  A  week-long  field  study  (N=9)  recorded  user  activity  and  impressions  with  the  application.  The  results  supported  emotional  reinforcement's  potential  as  a  gamification  strategy  to  improve  user  motivation  and  engagement.
2	Memory  through  design  supporting  cultural  identity  for  immigrants  through  a  paper  based  home  drafting  tool.  Current  research  in  HCI  with  immigrants  predominantly  focuses  on  their  practical  needs  and  little  attention  is  given  to  their  cultural  identities.  As  such,  we  aim  to  understand  how  newcomers  reflect  their  cultural  values  within  domestic  settings.  We  explore  this  by  provoking  memories  immigrants  associate  with  physical  spaces  inside  their  homes.  Hence,  we  built  "Our  Home  Sketcher":  a  paper-based  home  drafting  tool  that  allows  novice  users  to  design  their  homes  by  sketching  and  implicitly  expressing  their  space,  light,  and  privacy  preferences.  The  collected  drawings  are  then  fed  into  a  computer  algorithm  that  produces  3D  models  of  the  sketched  houses.  This  process  of  design  acts  as  an  artifact-driven  storytelling  for  heritage  sharing  and  rapport  building  within  migrant  communities.  We  engage  13  Middle  Eastern  newcomers  in  Canada  with  the  tool  and  use  Halbwachs'  [44]  theory  of  collective  memory  to  frame  how  home  sketching  provokes  former  experiences.  Our  findings  show  a  strong  longing  for  reclaiming  the  past,  narrating  space-related  oral  history,  and  designing  beyond  current  limitations.
2	Red  alert  a  cognitive  countermeasure  to  mitigate  attentional  tunneling.  Attentional  tunneling,  that  is  the  inability  to  detect  unexpected  changes  in  the  environment,  has  been  shown  to  have  critical  consequences  in  air  traffic  control.  The  motivation  of  this  study  was  to  assess  the  design  of  a  cognitive  countermeasure  dedicated  to  mitigate  such  failure  of  attention.  The  Red  Alert  cognitive  countermeasure  relies  on  a  brief  orange-red  flash  (300  ms)  that  masks  the  entire  screen  with  a  15%  opacity.  Twenty-two  air  traffic  controllers  faced  two  demanding  scenarios,  with  or  without  the  cognitive  countermeasure.  The  volunteers  were  not  told  about  the  Red  Alert  so  as  to  assess  the  intuitiveness  of  the  design  without  prior  knowledge.  Behavioral  results  indicated  that  the  cognitive  countermeasure  reduced  reaction  time  and  improved  the  detection  of  the  notification  when  compared  to  the  classical  operational  design.  Further  analyses  showed  this  effect  was  even  stronger  for  half  of  our  participants  (91.7%  detection  rate)  who  intuitively  understood  the  purpose  of  this  design.
2	Human  centered  approaches  to  fair  and  responsible  ai.  As  AI  changes  the  way  decisions  are  made  in  organizations  and  governments,  it  is  ever  more  important  to  ensure  that  these  systems  work  according  to  values  that  diverse  users  and  groups  find  important.  Researchers  have  proposed  numerous  algorithmic  techniques  to  formalize  statistical  fairness  notions,  but  emerging  work  suggests  that  AI  systems  must  account  for  the  real-world  contexts  in  which  they  will  be  embedded  in  order  to  actually  work  fairly.  These  findings  call  for  an  expanded  research  focus  beyond  statistical  fairness  to  that  which  includes  fundamental  understandings  of  human  use  and  the  social  impact  of  AI  systems,  a  theme  central  to  the  HCI  community.  The  HCI  community  can  contribute  novel  understandings,  methods,  and  techniques  for  incorporating  human  values  and  cultural  norms  into  AI  systems;  address  human  biases  in  developing  and  using  AI;  and  empower  individual  users  and  society  to  audit  and  control  AI  systems.  Our  goal  is  to  bring  together  academic  and  industry  researchers  in  the  fields  of  HCI,  ML  and  AI,  and  the  social  sciences  to  devise  a  cross-disciplinary  research  agenda  for  fair  and  responsible  AI  systems.  This  workshop  will  build  on  previous  algorithmic  fairness  workshops  at  AI  and  ML  conferences,  map  research  and  design  opportunities  for  future  innovations,  and  disseminate  them  in  each  community.
2	A  view  on  the  viewer  gaze  adaptive  captions  for  videos.  Subtitles  play  a  crucial  role  in  cross-lingual  distribution  of  multimedia  content  and  help  communicate  information  where  auditory  content  is  not  feasible  (loud  environments,  hearing  impairments,  unknown  languages).  Established  methods  utilize  text  at  the  bottom  of  the  screen,  which  may  distract  from  the  video.  Alternative  techniques  place  captions  closer  to  related  content  (e.g.,  faces)  but  are  not  applicable  to  arbitrary  videos  such  as  documentations.  Hence,  we  propose  to  leverage  live  gaze  as  indirect  input  method  to  adapt  captions  to  individual  viewing  behavior.  We  implemented  two  gaze-adaptive  methods  and  compared  them  in  a  user  study  (n=54)  to  traditional  captions  and  audio-only  videos.  The  results  show  that  viewers  with  less  experience  with  captions  prefer  our  gaze-adaptive  methods  as  they  assist  them  in  reading.  Furthermore,  gaze  distributions  resulting  from  our  methods  are  closer  to  natural  viewing  behavior  compared  to  the  traditional  approach.  Based  on  these  results,  we  provide  design  implications  for  gaze-adaptive  captions.
2	Vibrotactile  funneling  illusion  and  localization  performance  on  the  head.  The  vibrotactile  funneling  illusion  is  the  sensation  of  a  single  (non-existing)  stimulus  somewhere  in-between  the  actual  stimulus  locations.  Its  occurrence  depends  upon  body  location,  distance  between  the  actuators,  signal  synchronization,  and  intensity.  Related  work  has  shown  that  the  funneling  illusion  may  occur  on  the  forehead.  We  were  able  to  reproduce  these  findings  and  explored  five  further  regions  to  get  a  more  complete  picture  of  the  occurrence  of  the  funneling  illusion  on  the  head.  The  results  of  our  study  (24  participants)  show  that  the  actuator  distance,  for  which  the  funneling  illusion  occurs,  strongly  depends  upon  the  head  region.  Moreover,  we  evaluated  the  centralizing  bias  (smaller  perceived  than  actual  actuator  distances)  for  different  head  regions,  which  also  showed  widely  varying  characteristics.  We  computed  a  detailed  heat  map  of  vibrotactile  localization  accuracies  on  the  head.  The  results  inform  the  design  of  future  tactile  head-mounted  displays  that  aim  to  support  the  funneling  illusion.
2	Who  uses  bots  a  statistical  analysis  of  bot  usage  in  moderation  teams.  Adopting  new  technology  is  challenging  for  volunteer  moderation  teams  of  online  communities.  Challenges  are  aggravated  when  communities  increase  in  size.  In  a  prior  qualitative  study,  Kiene  et  al.  found  evidence  that  moderator  teams  adapted  to  challenges  by  relying  on  their  experience  in  other  technological  platforms  to  guide  the  creation  and  adoption  of  innovative  custom  moderation  "bots."  In  this  study,  we  test  three  hypotheses  on  the  social  correlates  of  user  innovated  bot  usage  drawn  from  a  previous  qualitative  study.  We  find  strong  evidence  of  the  proposed  relationship  between  community  size  and  the  use  of  user  innovated  bots.  Although  previous  work  suggests  that  smaller  teams  of  moderators  will  be  more  likely  to  use  these  bots  and  that  users  with  experience  moderating  in  the  previous  platform  will  be  more  likely  to  do  so,  we  find  little  evidence  in  support  of  either  proposition.
2	Swiss  cheese  extended  an  object  recognition  method  for  ubiquitous  interfaces  based  on  capacitive  proximity  sensing.  Swiss-Cheese  Extended  proposes  a  novel  real-time  method  for  recognizing  objects  with  capacitive  proximity  sensors.  Applying  this  technique  to  ubiquitous  user  interfaces,  it  is  possible  to  detect  the  3D-position  of  multiple  human  hands  in  different  configurations  above  a  surface  that  is  equipped  with  a  small  number  of  sensors.  The  retrieved  object  configurations  can  significantly  improve  a  user's  interaction  experience  or  an  application's  execution  context,  for  example  by  detecting  multi-hand  zoom  and  rotation  gestures  or  recognizing  a  grasping  hand.  We  emphasize  the  broad  applicability  of  the  proposed  method  with  a  study  of  a  multi-hand  gesture  recognition  device.
2	Bezel  tap  gestures  quick  activation  of  commands  from  sleep  mode  on  tablets.  We  present  Bezel-Tap  Gestures,  a  novel  family  of  interaction  techniques  for  immediate  interaction  on  handheld  tablets  regardless  of  whether  the  device  is  alive  or  in  sleep  mode.  The  technique  rests  on  the  close  succession  of  two  input  events:  first  a  bezel  tap,  whose  detection  by  accelerometers  will  awake  an  idle  tablet  almost  instantly,  then  a  screen  contact.  Field  studies  confirmed  that  the  probability  of  this  input  sequence  occurring  by  chance  is  very  low,  excluding  the  accidental  activation  concern.  One  experiment  examined  the  optimal  size  of  the  vocabulary  of  commands  for  all  four  regions  of  the  bezel  (top,  bottom,  left,  right).  Another  experiment  evaluated  two  variants  of  the  technique  which  both  allow  two-level  selection  in  a  hierarchy  of  commands,  the  initial  bezel  tap  being  followed  by  either  two  screen  taps  or  a  screen  slide.  The  data  suggests  that  Bezel-Tap  Gestures  may  serve  to  design  large  vocabularies  of  micro-interactions  with  a  sleeping  tablet.
2	Child  computer  interaction  sig  new  challenges  and  opportunities.  This  SIG  will  provide  child-computer  interaction  researchers  and  practitioners  an  opportunity  to  discuss  four  topics  that  represent  new  challenges  and  opportunities  for  the  community.  The  four  areas  are:  interactive  technologies  for  children  under  the  age  of  five,  technology  for  inclusion,  privacy  and  information  security  in  the  age  of  the  quantified  self,  and  the  maker  movement.
2	Evaluation  of  appearance  based  methods  and  implications  for  gaze  based  applications.  Appearance-based  gaze  estimation  methods  that  only  require  an  off-the-shelf  camera  have  significantly  improved  but  they  are  still  not  yet  widely  used  in  the  human-computer  interaction  (HCI)  community.  This  is  partly  because  it  remains  unclear  how  they  perform  compared  to  model-based  approaches  as  well  as  dominant,  special-purpose  eye  tracking  equipment.  To  address  this  limitation,  we  evaluate  the  performance  of  state-of-the-art  appearance-based  gaze  estimation  for  interaction  scenarios  with  and  without  personal  calibration,  indoors  and  outdoors,  for  different  sensing  distances,  as  well  as  for  users  with  and  without  glasses.  We  discuss  the  obtained  findings  and  their  implications  for  the  most  important  gaze-based  applications,  namely  explicit  eye  input,  attentive  user  interfaces,  gaze-based  user  modelling,  and  passive  eye  monitoring.  To  democratise  the  use  of  appearance-based  gaze  estimation  and  interaction  in  HCI,  we  finally  present  OpenGaze  (www.opengaze.org),  the  first  software  toolkit  for  appearance-based  gaze  estimation  and  interaction.
2	Trust  me  i  m  partially  right  incremental  visualization  lets  analysts  explore  large  datasets  faster.  Queries  over  large  scale  (petabyte)  data  bases  often  mean  waiting  overnight  for  a  result  to  come  back.  Scale  costs  time.  Such  time  also  means  that  potential  avenues  of  exploration  are  ignored  because  the  costs  are  perceived  to  be  too  high  to  run  or  even  propose  them.  With  sampleAction  we  have  explored  whether  interaction  techniques  to  present  query  results  running  over  only  incremental  samples  can  be  presented  as  sufficiently  trustworthy  for  analysts  both  to  make  closer  to  real  time  decisions  about  their  queries  and  to  be  more  exploratory  in  their  questions  of  the  data.  Our  work  with  three  teams  of  analysts  suggests  that  we  can  indeed  accelerate  and  open  up  the  query  process  with  such  incremental  visualizations.
2	Steering  through  sequential  linear  path  segments.  The  steering  law  models  human  motor  performance  and  has  been  verified  to  hold  for  a  single  linear  and/or  circular  path.  Some  extensions  investigated  steering  around  corners.  Yet,  little  is  known  about  human  performance  in  navigating  joined  linear  paths,  i.e.,  successions  of  path  segments  with  different  widths.  Such  operations  appear  in  graphical  user  interface  tasks,  including  lasso  operations  in  illustration  software.  In  this  work,  we  conducted  several  experiments  involving  joined  paths.  The  results  show  that  users  significantly  changed  their  behavior,  and  that  this  strategy  change  can  be  predicted  beforehand.  A  simple  model  summing  the  two  indexes  of  difficulty  (IDs)  for  each  path  predicts  movement  time  well,  but  more  sophisticated  models  were  also  evaluated.  The  best  model  in  terms  of  both  of  R2  and  AIC  values  includes  the  ID  of  the  crossing  operation  to  enter  the  second  path.
2	Breaking  news  on  twitter.  After  the  news  of  Osama  Bin  Laden's  death  leaked  through  Twitter,  many  people  wondered  if  Twitter  would  fundamentally  change  the  way  we  produce,  spread,  and  consume  news.  In  this  paper  we  provide  an  in-depth  analysis  of  how  the  news  broke  and  spread  on  Twitter.  We  confirm  the  claim  that  Twitter  broke  the  news  first,  and  find  evidence  that  Twitter  had  convinced  a  large  number  of  its  audience  before  mainstream  media  confirmed  the  news.  We  also  discover  that  attention  on  Twitter  was  highly  concentrated  on  a  small  number  of  "opinion  leaders"  and  identify  three  groups  of  opinion  leaders  who  played  key  roles  in  spreading  the  news:  individuals  affiliated  with  media  played  a  large  part  in  breaking  the  news,  mass  media  brought  the  news  to  a  wider  audience  and  provided  eager  Twitter  users  with  content  on  external  sites,  and  celebrities  helped  to  spread  the  news  and  stimulate  conversation.  Our  findings  suggest  Twitter  has  great  potential  as  a  news  medium.
2	Effects  of  balancing  for  physical  abilities  on  player  performance  experience  and  self  esteem  in  exergames.  Game  balancing  can  help  players  with  different  skill  levels  play  multiplayer  games  together;  however,  little  is  known  about  how  the  balancing  approach  affects  performance,  experience,  and  self-esteem'especially  when  differences  in  player  strength  result  from  given  abilities,  rather  than  learned  skill.  We  explore  three  balancing  approaches  in  a  dance  game  and  show  that  the  explicit  approach  commonly  used  in  commercial  games  reduces  self-esteem  and  feelings  of  relatedness  in  dyads,  whereas  hidden  balancing  improves  self-esteem  and  reduces  score  differential  without  affecting  game  outcome.  We  apply  our  results  in  a  second  study  with  dyads  where  one  player  had  a  mobility  disability  and  used  a  wheelchair.  By  making  motion-based  games  accessible  for  people  with  different  physical  abilities,  and  by  enabling  people  with  mobility  disabilities  to  compete  on  a  par  with  able-bodied  peers,  we  show  how  to  provide  empowering  experiences  through  enjoyable  games  that  have  the  potential  to  increase  physical  activity  and  self-esteem.
2	Mousehints  easing  task  switching  in  parallel  browsing.  We  present  a  technique  to  help  users  regain  context  either  after  an  interruption  or  when  multitasking  while  performing  web  tasks.  Using  mouse  movements  as  an  indicator  of  attention,  a  browser  plugin  records  in  background  the  user's  interactions  (including  clicks,  dwell  times,  and  DOM  elements).  On  leaving  the  page,  this  information  is  stored  to  be  rendered  as  an  overlay  when  the  user  returns  to  such  page.  The  results  of  a  short  study  showed  that  participants  resumed  tasks  three  times  faster  with  MouseHints  and  completed  their  tasks  in  about  half  the  time.  Related  applications  and  further  research  are  also  envisioned.
2	Tactilevr  integrating  physical  toys  into  learn  and  play  virtual  reality  experiences.  We  present  TactileVR,  an  immersive  presence  and  tactile  feedback  into  virtual  reality.  Our  system  allows  users  free  to  move  around  and  interact  with  physical  objects  and  toys,  which  co-exist  in  the  virtual  world.  By  integrating  tracking  information  from  the  space  as  well  as  head,  hands  and  feet  of  the  user,  we  represent  this  information  as  virtual  proxies  in  the  3D  environment.  Each  object  has  a  unique  appearance  and  behavior  e.g.  in  an  electric  circuits  lab  toy  blocks  serve  as  switches,  batteries  and  light  bulbs.  By  tracking  and  integrating  toys  and  other  everyday  objects  into  VR,  we  are  able  to  create  educational  and  recreational  experiences  for  children,  an  environment  in  which  they  can  play  and  learn  more  autonomously.
2	Challenges  and  opportunities  for  technology  in  foreign  language  classrooms.  We  present  the  results  of  a  two-month  ethnographic  study  of  three  introductory  Russian  classrooms.  Through  observation  and  interviews,  we  identify  several  distinct  roles  played  by  physical  artifacts  in  the  classrooms,  such  as  providing  a  reference  to  necessary  foreign-language  material  and  serving  as  props  in  creative  role-play.  The  range  of  roles  taken  on  by  artifacts  and  the  attitudes  students  have  toward  them  provide  a  basis  for  our  discussion  about  how  technology  might  be  more  effectively  introduced  into  the  socially  negotiated  environment  of  the  introductory  foreign-language  classroom.  We  identify  the  need  to  balance  between  collaborative  and  personal  technology  in  a  stressful,  but  social,  context.  Our  findings  inform  a  range  of  roles  that  technology  can  undertake  in  replacing  or  augmenting  existing  classroom  artifacts.
2	Fly  studying  recall  macrostructure  understanding  and  user  experience  of  canvas  presentations.  Most  presentation  software  uses  the  slide  deck  metaphor  to  create  visual  presentation  support.  Recently,  canvas  presentation  tools  such  as  Fly  or  Prezi  have  begun  to  use  a  zoomable  free-form  canvas  to  arrange  information  instead.  While  their  effect  on  authoring  presentations  has  been  evaluated  previously,  we  studied  how  they  impact  the  audience.  In  a  quantitative  study,  we  compared  audience  retention  and  macrostructure  understanding  of  slide  deck  vs.  canvas  presentations.  We  found  both  approaches  to  be  equally  capable  of  communicating  information  to  the  audience.  Canvas  presentations,  however,  were  rated  by  participants  to  better  aid  them  in  staying  oriented  during  a  talk.  This  makes  canvas  presentation  tools  a  promising  slideware  alternative.
2	Why  is  this  happening  to  me  how  player  attribution  can  broaden  our  understanding  of  player  experience.  Games  user  research  (GUR)  measures  the  performance  and  preference  of  digital  game  players,  and  interprets  these  measurements  in  the  context  of  theories  that  explain  human  behavior.  There  are  many  validated  approaches  for  measuring  player  experience  that  are  grounded  in  psychological  theories  on  motivation  and  emotion.  Attribution  theory  explains  how  people  assign  causes  to  events  and  how  these  attributions  affect  peoples'  emotional  reactions  and  motivations.  In  this  paper  we  argue  that  attribution  theory  can  provide  additional  value  to  the  existing  suite  of  GUR  tools;  however,  there  are  currently  no  validated  tools  to  assess  player  attribution  in  the  context  of  games.  This  paper  describes  the  conceptualization  of  player  attribution  based  on  literature,  presents  the  development  and  validation  of  a  scale  to  assess  player  attribution  in  games,  and  discusses  the  implications  of  adding  player  attribution  to  the  toolbox  of  methods  for  the  design  and  evaluation  of  digital  games.
2	Selph  progressive  learning  and  support  of  manual  photo  color  enhancement.  Color  enhancement  is  a  very  important  aspect  of  photo  editing.  Even  when  photographers  have  tens  of  or  hundreds  of  photographs,  they  must  enhance  each  photo  one  by  one  by  manually  tweaking  sliders  in  software  such  as  brightness  and  contrast,  because  automatic  color  enhancement  is  not  always  satisfactory  for  them.  To  support  this  repetitive  manual  task,  we  present  self-reinforcing  color  enhancement,  where  the  system  implicitly  and  progressively  learns  the  user's  preferences  by  training  on  their  photo  editing  history.  The  more  photos  the  user  enhances,  the  more  effectively  the  system  supports  the  user.  We  present  a  working  prototype  system  called  SelPh,  and  then  describe  the  algorithms  used  to  perform  the  self-reinforcement.  We  conduct  a  user  study  to  investigate  how  photographers  would  use  a  self-reinforcing  system  to  enhance  a  collection  of  photos.  The  results  indicate  that  the  participants  were  satisfied  with  the  proposed  system  and  strongly  agreed  that  the  self-reinforcing  approach  is  preferable  to  the  traditional  workflow.
2	Collapse  informatics  augmenting  the  sustainability  ict4d  discourse  in  hci.  Research  in  many  fields  argues  that  contemporary  global  industrial  civilization  will  not  persist  indefinitely  in  its  current  form,  and  may,  like  many  past  human  societies,  eventually  collapse.  Arguments  in  environmental  studies,  anthropology,  and  other  fields  indicate  that  this  transformation  could  begin  within  the  next  half-century.  While  imminent  collapse  is  far  from  certain,  it  is  prudent  to  consider  now  how  to  develop  sociotechnical  systems  for  use  in  these  scenarios.  We  introduce  the  notion  of  collapse  informatics---the  study,  design,  and  development  of  sociotechnical  systems  in  the  abundant  present  for  use  in  a  future  of  scarcity---as  a  complement  to  ICT4D  and  mitigation-oriented  sustainable  HCI.  We  draw  on  a  variety  of  literatures  to  offer  a  set  of  relevant  concepts  and  articulate  the  relationships  among  them  to  orient  and  evaluate  collapse  informatics  work.  Observing  that  collapse  informatics  poses  a  unique  class  of  cross-cultural  design  problems,  we  sketch  the  design  space  of  collapse  informatics  and  provide  a  variety  of  example  projects.  We  explore  points  of  connection  and  distinction  between  collapse  informatics  and  sustainable  HCI,  ICT4D,  and  crisis  informatics.  Finally,  we  discuss  next  steps  and  comment  on  the  potential  value  of  collapse  informatics  work  even  in  the  event  that  collapse  never  occurs.
2	One  dimensional  handwriting  inputting  letters  and  words  on  smart  glasses.  We  present  1D  Handwriting,  a  unistroke  gesture  technique  enabling  text  entry  on  a  one-dimensional  interface.  The  challenge  is  to  map  two-dimensional  handwriting  to  a  reduced  one-dimensional  space,  while  achieving  a  balance  between  memorability  and  performance  efficiency.  After  an  iterative  design,  we  finally  derive  a  set  of  ambiguous  two-length  unistroke  gestures,  each  mapping  to  1-4  letters.  To  input  words,  we  design  a  Bayesian  algorithm  that  takes  into  account  the  probability  of  gestures  and  the  language  model.  To  input  letters,  we  design  a  pause  gesture  allowing  users  to  switch  into  letter  selection  mode  seamlessly.  Users  studies  show  that  1D  Handwriting  significantly  outperforms  a  selection-based  technique  (a  variation  of  1Line  Keyboard)  for  both  letter  input  (4.67  WPM  vs.  4.20  WPM)  and  word  input  (9.72  WPM  vs.  8.10  WPM).  With  extensive  training,  text  entry  rate  can  reach  19.6  WPM.  Users'  subjective  feedback  indicates  1D  Handwriting  is  easy  to  learn  and  efficient  to  use.  Moreover,  it  has  several  potential  applications  for  other  one-dimensional  constrained  interfaces.
2	Tapboard  2  simple  and  effective  touchpad  like  interaction  on  a  multi  touch  surface  keyboard.  We  introduce  TapBoard  2,  a  touchpad-based  keyboard  that  solves  the  problem  of  typing  and  pointing  disambiguation.  The  pointing  interaction  design  of  TapBoard  2  is  nearly  identical  to  natural  touchpad  interaction,  and  its  shared  workspace  naturally  invites  bimanual  pointing  interaction.  To  implement  TapBoard  2,  we  developed  a  novel  gesture  representation  scheme  for  a  systematic  design  and  gesture  recognizer.  A  user  evaluation  showed  that  TapBoard  2  successfully  supports  collocated  pointing  and  typing  interaction.  It  was  able  to  disambiguate  typing  and  pointing  actions  with  an  accuracy  of  greater  than  95%.  In  addition,  the  typing  and  pointing  performance  of  TapBoard  2  were  comparable  to  that  of  a  separate  keyboard  and  mouse.  In  particular,  the  bimanual  pointing  operations  of  TapBoard  2  are  highly  efficient  and  strongly  favored  by  participants.
2	Snap  decisions  how  users  content  and  aesthetics  interact  to  shape  photo  sharing  behaviors.  Participants  in  social  media  systems  must  balance  many  considerations  when  choosing  what  to  share  and  with  whom.  Sharing  with  others  invites  certain  risks,  as  well  as  potential  benefits;  achieving  the  right  balance  is  even  more  critical  when  sharing  photos,  which  can  be  particularly  engaging,  but  potentially  compromising.  In  this  paper,  we  examine  photo-sharing  decisions  as  an  interaction  between  high-level  user  preferences  and  specific  features  of  the  images  being  shared.  Our  analysis  combines  insights  from  a  96-user  survey  with  metadata  from  10.4M  photos  to  develop  a  model  integrating  these  perspectives  to  predict  permissions  settings  for  uploaded  photos.  We  discuss  implications,  including  how  such  a  model  can  be  applied  to  provide  online  sharing  experiences  that  are  more  safe,  more  scalable,  and  more  satisfying.
2	A  fantastica  fabrica  de  chocolate  levando  o  sabor  de  ihc  para  meninas  do  ensino  fundamental.  In  this  paper  we  describe  a  Computer  Science  Unplugged  activity  (The  Chocolate  Factory)  carried  out  in  a  Brazilian  school.  The  activity  was  about  HCI  design  and  was  done  with  nine  teenagers.  The  main  results  of  the  pilot  experiment  are  discussed  in  this  paper.
2	Not  another  z  piece  adaptive  difficulty  in  tetris.  Difficulty  in  TETRIS  is  adjusted  by  adapting  the  speed  with  which  blocks  fall.  In  this  contribution,  we  describe  results  of  an  exploratory  study  in  which  we  investigated  relationships  between  players'  performance  and  their  subjective  assessment  of  difficulty  and  fun.  We  tested  five  different  algorithms  that,  instead  of  adjusting  game  speed,  adjust  difficulty  by  choosing  blocks  based  on  the  current  game  state.  With  our  results,  we  establish  pile  height  and  bumpiness  as  parameters  that  indicate  the  performance  of  a  player  during  a  live  game,  discuss  the  inherent  difficulty  of  different  block  choosing  algorithms  and  show  how  the  relationship  between  fun  and  perceived  difficulty  varies  for  distinct  player  groups.  With  regard  to  adapting  difficulty,  we  argue  that  one  can  still  teach  an  old  dog  such  a  TETRIS  a  lot  of  new  tricks.
2	Peripheral  agent  implementation  of  peripheral  cognition  technology.  Information  notification  on  a  display  for  e-mail  arrival,  micro-blog  updates,  and  application  updates  is  becoming  increasingly  important.  We  propose  a  novel  information  notification  method,  the  peripheral  agent  (PA)  as  an  implementation  of  peripheral  cognition  technology  (PCT)  that  uses  the  human  cognitive  properties  that  a  human  does  not  recognize  subtle  changes  in  a  peripheral  area  of  cognition  when  he/she  concentrates  on  a  task  and  that  he/she  automatically  recognizes  the  changes  when  not  concentrating  on  the  task.  By  only  setting  a  PA  in  the  peripheral  area,  a  user  automatically  and  easily  accepts  the  notification  only  when  his/her  concentration  breaks.  We  conducted  two  experiments  to  investigate  a  VFN  area  and  evaluate  the  effectiveness  of  PAs.
2	Sweatatoms  understanding  physical  activity  through  material  artifacts.  In  this  video,  we  present  a  novel  approach  of  representing  physical  activity  in  the  form  of  material  artifacts.  By  designing  such  material  representations,  our  aim  is  to  understand  what  these  artifacts  might  offer  in  terms  of  reflecting  upon  physical  activity.  For  example,  what  types  of  affect  do  material  artifacts,  representing  ones'  physical  activity  over  time  create  for  the  user?  In  order  to  advance  this  understanding,  we  have  designed  a  system  called  SweatAtoms  that  transforms  the  physical  activity  data  based  on  heart  rate  into  3D  printed  material  artifacts  and  provides  5  different  material  representations  of  their  physical  activity.  This  video  offers  few  reflections  on  designing  material  representations  for  physical  activity.  We  hope  that  our  work  will  inspire  designers  to  consider  new  possibilities  afforded  by  digital  fabrication  to  support  user's  experience  with  physical  activity  by  utilizing  interactive  technologies  at  our  disposal.
2	Sig  nime  music  technology  and  human  computer  interaction.  This  SIG  intends  to  investigate  the  ongoing  dialogue  between  music  technology  and  the  field  of  human-computer  interaction.  Our  specific  aims  are  to  consider  major  findings  of  musical  interface  research  over  recent  years  and  discuss  how  these  might  best  be  conveyed  to  CHI  researchers  interested  but  not  yet  active  in  this  area,  as  well  as  to  consider  how  to  stimulate  future  collaborations  between  music  technology  and  CHI  research  communities.  \
2	Movemeant  anonymously  building  community  through  shared  location  histories.  Awareness  of  and  connections  to  a  local  community  are  important  for  building  social  capital,  sharing  resources,  and  providing  physical  support,  but  have  been  elusive  to  create  in  dense  urban  environments.  We  describe  the  design  and  implementation  of  MoveMeant,  a  system  aimed  to  increase  local  community  awareness  through  shared  location  traces.  MoveMeant  securely  uses  anonymized  location  data  generated  automatically  by  mobile  devices  to  display  aggregate,  community-level  location  data.  We  report  findings  from  interviews  with  residents  in  the  Bronx,  New  York  City  who  participated  in  a  deployment  of  MoveMeant  over  a  6-week  period.  Our  findings  show  that  people  use  the  anonymous  information  to  make  judgments  about  the  people  and  places  in  their  community,  while  opting  to  reveal  their  identity  for  third  places  where  there  is  an  opportunity  to  connect  socially.
2	Crowdcamp  rapidly  iterating  ideas  related  to  collective  intelligence  crowdsourcing.  The  field  of  collective  intelligence  -  encompassing  aspects  of  crowdsourcing,  human  computation,  and  social  computing  -  is  having  tremendous  impact  on  our  lives,  and  the  fields  are  rapidly  growing.  We  propose  a  hands-on  event  that  takes  the  main  benefits  of  a  workshop  -  provocative  discussion  and  community  building  -  and  allows  time  to  focus  on  developing  ideas  into  actual  outputs:  experiment  designs,  in-depth  thoughts  on  wicked  problems,  paper  or  coded  prototypes.  We  will  bring  together  researchers  to  discuss  future  visions  and  make  tangible  headway  on  those  visions,  as  well  as  seeding  collaboration.  The  outputs  from  brainstorming,  discussion,  and  building  will  persist  after  the  workshop  for  attendees  and  the  community  to  view,  and  will  be  written  up.
2	Visually  impaired  users  on  an  online  social  network.  In  this  paper  we  present  the  first  large-scale  empirical  study  of  how  visually  impaired  people  use  online  social  networks,  specifically  Facebook.  We  identify  a  sample  of  50K  visually  impaired  users,  and  study  the  activities  they  perform,  the  content  they  produce,  and  the  friendship  networks  they  build  on  Facebook.  We  find  that  visually  impaired  users  participate  on  Facebook  (e.g.  status  updates,  comments,  likes)  as  much  as  the  general  population,  and  receive  more  feedback  (i.e.,  comments  and  likes)  on  average  on  their  content.  By  analyzing  the  content  produced  by  visually  impaired  users,  we  find  that  they  share  their  experience  and  issues  related  to  vision  impairment.  We  also  identify  distinctive  patterns  in  their  language  and  technology  use.  We  also  show  that,  compared  to  other  users,  visually  impaired  users  have  smaller  social  networks,  but  such  differences  have  decreased  over  time.  Our  findings  have  implications  for  improving  the  utility  and  usability  of  online  social  networks  for  visually  impaired  users.
2	Appropriated  or  inauthentic  care  in  gig  economy  platforms  a  psycho  linguistic  analysis  of  uber  and  lyft.  In  this  late  breaking  work,  we  present  preliminary  results  from  a  portion  of  an  auto-ethnography  in  which  an  HCI  scholar  drove  for  both  Uber  and  Lyft  over  the  course  of  4  months,  recording  his  thoughts  about  the  driving  experience  as  well  as  his  experiences  with-and  emails  from-both  platforms.  The  first  phase  of  results  we  present  here  are  based  on  several  text  analyses  of  the  collected  emails,  as  well  as  a  preliminary  examination  of  field  notes  in  relation  to  these  emails.  We  found  that  while  Uber  and  Lyft  participate  in  the  gig  economy  in  almost  identical  ways,  the  difference  in  tone  apparent  through  each  platform's  messaging  could  lead  to  conflicting  experiences  for  drivers.  We  identify  implications  for  the  potential  future  analyses  of  our  autoethnographic  data  in  relation  to  this  psycholinguistic  analysis.
2	Understanding  users  capability  to  transfer  information  between  mixed  and  virtual  reality  position  estimation  across  modalities  and  perspectives.  Mixed  Reality  systems  combine  physical  and  digital  worlds,  with  great  potential  for  the  future  of  HCI.  It  is  possible  to  design  systems  that  support  flexible  degrees  of  virtuality  by  combining  complementary  technologies.  In  order  for  such  systems  to  succeed,  users  must  be  able  to  create  unified  mental  models  out  of  heterogeneous  representations.  In  this  paper,  we  present  two  studies  focusing  on  the  users'  accuracy  on  heterogeneous  systems  using  Spatial  Augmented  Reality  (SAR)  and  immersive  Virtual  Reality  (VR)  displays,  and  combining  viewpoints  (egocentric  and  exocentric).  The  results  show  robust  estimation  capabilities  across  conditions  and  viewpoints.
2	Double  sided  printed  tactile  display  with  electro  stimuli  and  electrostatic  forces  and  its  assessment.  Humans  can  perceive  tactile  sensation  through  multimodal  stimuli.  To  demonstrate  realistic  pseudo  tactile  sensation  for  the  users,  a  tactile  display  is  needed  that  can  provide  multiple  tactile  stimuli.  In  this  paper,  we  have  explicated  a  novel  printed  tactile  display  that  can  provide  both  the  electrical  stimulus  and  the  electrostatic  force.  The  circuit  patterns  for  each  stimulus  were  fabricated  by  employing  the  technique  of  double-sided  conductive  ink  printing.  Requirements  for  the  fabrication  process  were  analyzed  and  the  durability  of  the  tactile  display  was  evaluated.  Users'  perceptions  of  a  single  tactile  stimulus  and  multiple  tactile  stimuli  were  also  investigated.  The  obtained  experimental  results  indicate  that  the  proposed  tactile  display  is  capable  of  exhibiting  realistic  tactile  sensation  and  can  be  incorporated  by  various  applications  such  as  tactile  sensation  printing  of  pictorial  illustrations  and  paintings.  Furthermore,  the  proposed  hybrid  tactile  display  can  contribute  to  accelerated  prototyping  and  development  of  new  tactile  devices.
2	Tangible  awareness  how  tangibles  on  tabletops  influence  awareness  of  each  other  s  actions.  Tangibles  on  multitouch  tabletops  increase  speed,  accuracy,  and  eyes-free  operability  for  individual  users,  and  verbal  and  behavioral  social  interaction  among  multiple  users  around  smaller  tables  with  a  shared  focus  of  attention.  Modern  multitouch  tables,  however,  provide  sizes  and  resolutions  that  let  groups  work  alongside  each  other  in  separate  workspaces.  But  how  aware  do  these  users  remain  of  each  other's  actions,  and  what  impact  can  tangibles  have  on  their  awareness?  In  our  study,  groups  of  2--4  users  around  the  table  played  an  individual  game  grabbing  their  attention  as  primary  task,  while  they  also  had  to  occasionally  become  aware  of  other  players'actions  and  react  as  secondary  task.  We  found  that  players  were  significantly  more  aware  of  other  players'actions  using  tangibles  than  those  using  pure  multitouch  interaction,  indicated  by  faster  reaction  times.  This  effect  was  especially  strong  with  more  players.  We  close  with  qualitative  user  feedback  and  design  recommendations.  We  found  that  players  were  significantly  more  aware  of  other  players'actions  using  tangibles  than  those  using  pure  multitouch  interaction,  indicated  by  faster  reaction  times.  This  effect  was  especially  strong  with  more  players.  We  close  with  qualitative  user  feedback  and  design  recommendations.
2	Evaluating  hci  research  beyond  usability.  Evaluating  research  artefacts  is  an  important  step  to  showcase  the  validity  of  a  chosen  approach.  The  CHI  community  has  developed  and  agreed  upon  a  large  variety  of  evaluation  methods  for  HCI  research;  however,  sometimes  those  methods  are  not  applicable  or  not  sufficient.  This  is  especially  the  case  when  the  contribution  lies  within  the  context  of  the  application  area,  such  as  for  research  in  sustainable  HCI,  HCI  for  development,  or  design  fiction  and  futures  studies.  In  this  SIG,  we  invite  the  CHI  community  to  share  their  insights  from  projects  that  encountered  problems  in  evaluating  research  and  aim  to  discuss  solutions  for  this  difficult  topic.  We  invite  researchers  from  all  areas  of  HCI  research  who  are  interested  to  engage  in  a  debate  of  issues  in  the  process  of  validating  research  artefacts.
2	Developing  a  community  of  practice  to  support  global  hci  education.  ACM  SIGCHI  has  been  supporting  research  in  HCI  education  for  many  years,  most  actively  from  2011-2014.  At  CHI2014,  a  workshop  on  developing  a  new  HCI  living  curriculum  was  held,  building  on  three  years  of  research  and  collaboration.  We  believe  the  time  is  right  to  develop  and  implement  the  suggested  HCI  living  curriculum.  We  propose  a  hands-on  workshop  to  develop  a  concrete  active  community  of  practice  of  HCI  scholars  and  educators,  sharing  and  collaborating  to  develop  course  outlines,  curricula,  and  teaching  materials.  The  workshop  will  define  the  conceptual  framework  and  user  experience  of  the  HCI  living  curriculum,  develop  its  information  architecture  and  infrastructure,  and  evaluate  how  existing  platforms  do  and  do  not  fulfill  the  proposed  needs.  Post-workshop  initiatives  will  aim  to  move  towards  implementing  the  first  iteration  of  the  living  curriculum.
2	Analogy  mining  for  specific  design  needs.  Finding  analogical  inspirations  in  distant  domains  is  a  powerful  way  of  solving  problems.  However,  as  the  number  of  inspirations  that  could  be  matched  and  the  dimensions  on  which  that  matching  could  occur  grow,  it  becomes  challenging  for  designers  to  find  inspirations  relevant  to  their  needs.  Furthermore,  designers  are  often  interested  in  exploring  specific  aspects  of  a  product--  for  example,  one  designer  might  be  interested  in  improving  the  brewing  capability  of  an  outdoor  coffee  maker,  while  another  might  wish  to  optimize  for  portability.  In  this  paper  we  introduce  a  novel  system  for  targeting  analogical  search  for  specific  needs.  Specifically,  we  contribute  an  analogical  search  engine  for  expressing  and  abstracting  specific  design  needs  that  returns  more  distant  yet  relevant  inspirations  than  alternate  approaches.
2	Personalizing  persuasive  strategies  in  gameful  systems  to  gamification  user  types.  Persuasive  gameful  systems  are  effective  tools  for  motivating  behaviour  change.  Research  has  shown  that  tailoring  these  systems  to  individuals  can  increase  their  efficacy;  however,  there  is  little  knowledge  on  how  to  personalize  them.  We  conducted  a  large-scale  study  of  543  participants  to  investigate  how  different  gamification  user  types  responded  to  ten  persuasive  strategies  depicted  in  storyboards  representing  persuasive  gameful  health  systems.  Our  results  reveal  that  people's  gamification  user  types  play  significant  roles  in  the  perceived  persuasiveness  of  different  strategies.  People  scoring  high  in  the  'player'  user  type  tend  to  be  motivated  by  competition,  comparison,  cooperation,  and  reward  while  'disruptors'  are  likely  to  be  demotivated  by  punishment,  goal-setting,  simulation,  and  self-monitoring.  'Socialisers'  could  be  motivated  using  any  of  the  strategies;  they  are  the  most  responsive  to  persuasion  overall.  Finally,  we  contribute  to  CHI  research  and  practice  by  offering  design  guidelines  for  tailoring  persuasive  gameful  systems  to  each  gamification  user  type.
2	Human  all  too  human  noaa  weather  radio  and  the  emotional  impact  of  synthetic  voices.  The  integration  of  text-to-speech  into  an  open  technology  stack  for  low-power  FM  community  radio  stations  is  an  opportunity  to  automate  laborious  processes  and  increase  accessibility  to  information  in  remote  communities.  However,  there  are  open  questions  as  to  the  perceived  contrast  of  synthetic  voices  with  the  local  and  intimate  format  of  community  radio.  This  paper  presents  an  exploratory  focus  group  on  the  topic,  followed  by  a  thematic  analysis  of  public  comments  on  YouTube  videos  of  the  synthetic  voices  used  for  broadcasting  by  National  Oceanic  and  Atmospheric  Administration  (NOAA)  Weather  Radio.  We  find  that  despite  observed  reservations  about  the  suitability  of  TTS  for  radio,  there  is  significant  evidence  of  anthropomorphism,  nostalgia  and  emotional  connection  in  relation  to  these  voices.  Additionally,  introduction  of  a  more  "human  sounding"  synthetic  voice  elicited  significant  negative  feedback.  We  identify  pronunciation,  speed,  suitability  to  content  and  acknowledgment  of  limitations  as  more  relevant  factors  in  listeners'  stated  sense  of  connection.
2	An  object  centric  paradigm  for  robot  programming  by  demonstration.  In  robot  programming  by  demonstration,  we  hypothesize  that  in  a  class  of  procedural  tasks  where  the  end  goal  primarily  consists  of  the  states  of  objects  that  are  external  to  a  task  performer,  one  can  significantly  reduce  complexity  of  a  robot  learner  by  not  processing  a  human  demonstrator’s  motions  at  all.  In  this  class  of  tasks,  object  behaviors  are  far  more  critical  than  human  behaviors.  Based  on  this  virtual  demonstrator  hypothesis,  this  paper  presents  a  paradigm  where  a  human  demonstrates  an  object  manipulation  task  in  a  simulated  world  without  any  of  the  human  demonstrator’s  body  parts  being  sensed  by  a  robot  learner.  Based  on  the  object  movements  alone,  the  robot  learns  to  perform  the  same  task  in  the  physical  world.  These  results  provide  strong  support  for  the  virtual  demonstrator  hypothesis.
2	Paper  generators  harvesting  energy  from  touching  rubbing  and  sliding.  We  present  a  new  energy  harvesting  technology  that  generates  electrical  energy  from  a  user's  interaction  with  paper-like  materials.  The  energy  harvesters  are  flexible,  light,  and  inexpensive,  and  they  utilize  a  user's  gestures  such  as  tapping,  touching,  rubbing  and  sliding  to  generate  electrical  energy.      The  harvested  energy  is  then  used  to  actuate  LEDs,  e-paper  displays  and  various  other  devices  to  create  novel  interactive  applications,  such  as  enhancing  books  and  other  printed  media  with  interactivity.
2	Haptic  retargeting  dynamic  repurposing  of  passive  haptics  for  enhanced  virtual  reality  experiences.  Manipulating  a  virtual  object  with  appropriate  passive  haptic  cues  provides  a  satisfying  sense  of  presence  in  virtual  reality.  However,  scaling  such  experiences  to  support  multiple  virtual  objects  is  a  challenge  as  each  one  needs  to  be  accompanied  with  a  precisely-located  haptic  proxy  object.  We  propose  a  solution  that  overcomes  this  limitation  by  hacking  human  perception.  We  have  created  a  framework  for  repurposing  passive  haptics,  called  haptic  retargeting,  that  leverages  the  dominance  of  vision  when  our  senses  conflict.  With  haptic  retargeting,  a  single  physical  prop  can  provide  passive  haptics  for  multiple  virtual  objects.  We  introduce  three  approaches  for  dynamically  aligning  physical  and  virtual  objects:  world  manipulation,  body  manipulation  and  a  hybrid  technique  which  combines  both  world  and  body  manipulation.  Our  study  results  indicate  that  all  our  haptic  retargeting  techniques  improve  the  sense  of  presence  when  compared  to  typical  wand-based  3D  control  of  virtual  objects.  Furthermore,  our  hybrid  haptic  retargeting  achieved  the  highest  satisfaction  and  presence  scores  while  limiting  the  visible  side-effects  during  interaction.
2	Gotyourback  an  internet  of  toilets  for  the  trans  community.  Transgender  individuals  frequently  report  negative  experiences  when  attempting  to  access  gender  segregated  areas,  such  as  toilets.  This  includes  physical  and  verbal  harassment  and  being  denied  access  to  facilities.  The  existing  applications  used  to  locate  gender-neutral  toilets  are  limited  by  the  few  facilities  available.  Our  research  shows  that  many  transgender  individuals  feel  more  comfortable  using  gendered  toilets  in  the  presence  of  people  who  support  gender  diversity.  In  this  paper  we  present  GotYourBack,  a  mobile  application,  which  utilizes  this  support  network  and  works  with  an  Internet  of  Toilets  ecosystem  to  improve  safe  access  to  gendered  toilets.  GotYourBack  works  by  utilizing  motion  sensors  and  Bluetooth  beacons  to  provide  real-time  data  on  toilet  capacity  and  presence  of  supporters.
2	Copyme  a  portable  real  time  feedback  expression  recognition  game  for  children.  Assistive  tools  are  commonly  used  to  aid  children  experiencing  emotional  developmental  problems  associated  with  psychological  disorders,  such  as  autism  spectrum  disorders  (ASDs).  In  many  cases  early  intervention  is  crucial  to  ease  the  struggle  to  identify  key  facial  expressions  and  the  emotions  they  are  used  to  convey.  Combining  automatic  facial  expression  recognition  technology  with  real-time  feedback  on  player  performance,  CopyMe  is  an  iPad  game  that  aims  to  provide  a  means  for  children  to  learn  expressions  to  demonstrate  emotions.  In  this  paper,  we  discuss  findings  from  a  pilot  study  conducted  at  a  childcare  centre  to  evaluate  the  feasibility  of  CopyMe's  use  as  a  serious  game  for  children  to  learn  emotions  through  observation  and  mimicry.  Based  on  observational  and  interview  data,  we  found  that  the  children,  especially  the  ones  affected  by  ASDs,  were  able  to  perform  well  in  the  game  and  generally  expressed  enjoyment  during  play.  The  design  of  CopyMe  as  well  as  our  current  findings  will  be  most  interesting  for  CHI  attendees  working  in  the  domain  of  affective  interfaces  and  serious  games,  especially  those  that  target  children.
2	Photographing  information  needs  the  role  of  photos  in  experience  sampling  method  style  research.  The  Experience  Sampling  Method  (ESM)  enables  researchers  to  capture  information  about  participants'  experiences  in  the  moment.  Adding  an  end-of-day  retrospective  survey  also  allows  participants  to  elaborate  on  those  experiences.  Although  the  use  of  photos  in  retrospective  interviews  and  surveys  for  memory  elicitation  is  well  known,  little  research  has  investigated  the  use  of  photos  in  ESM  studies.  As  smartphone  adoption  increases  facilitating  ESM  studies  and  making  photo  sharing  easier,  researchers  need  to  continuously  evaluate  the  method  and  investigate  the  role  of  photos  in  such  studies.  We  conducted  a  large-scale  ESM  and  retrospective  survey  study  via  Android  smartphones  with  more  than  1,000  US  participants,  and  analyzed  participants'  photo  submissions,  including  how  photo  use  correlated  with  participants'  data  quality  and  what,  if  any,  value  photos  added  for  researchers.  Our  study  sheds  light  on  the  role  of  photos  in  ESM  and  retrospective  studies  that  researchers  can  reference  when  constructing  future  study  designs.
2	Mixed  initiative  creative  interfaces.  Enabled  by  artificial  intelligence  techniques,  we  are  witnessing  the  rise  of  a  new  paradigm  of  computational  creativity  support:  mixed-initiative  creative  interfaces  put  human  and  computer  in  a  tight  interactive  loop  where  each  suggests,  produces,  evaluates,  modifies,  and  selects  creative  outputs  in  response  to  the  other.  This  paradigm  could  broaden  and  amplify  creative  capacity  for  all,  but  has  so  far  remained  mostly  confined  to  artificial  intelligence  for  game  content  generation,  and  faces  many  unsolved  interaction  design  challenges.  This  workshop  therefore  convenes  CHI  and  game  researchers  to  advance  mixed-initiative  approaches  to  creativity  support.
2	Who  s  the  doctor  physicians  perception  of  internet  informed  patients  in  india.  Internet  health  information  seeking  can  potentially  alter  physician-patient  interactions,  which  in  turn  can  influence  healthcare  delivery.  Investigating  physicians'  perceptions  about  internet-informed  patients  is  important  for  understanding  this  phenomenon  in  countries  like  India,  where  this  is  a  relatively  recent  trend.  We  conducted  a  qualitative  study  to  this  effect,  conceptualizing  internet  health  information  access  as  a  disintermediation  process,  and  examining  this  phenomenon  through  the  dimensions  of  meanings  ascribed,  power  dynamics  and  social  norms.  We  found  that  physicians'  perceptions  about  internet  informed  patients  and  their  interactions  with  these  patients  were  largely  adversarial.  However,  some  physicians  viewed  the  phenomenon  as  inevitable.  They  developed  methods  that  leveraged  patients'  internet  access  for  the  purpose  of  increasing  patient  awareness  and  self-efficacy.  We  conceptualize  this  new  role  of  physicians  as  apomediation,  and  present  recommendations  for  design  and  implementation  of  health  information  platforms  in  countries  such  as  India,  where  power  dynamics  form  a  salient  part  of  physician-patient  interactions.
2	Gaussmarbles  spherical  magnetic  tangibles  for  interacting  with  portable  physical  constraints.  This  work  develops  a  system  of  spherical  magnetic  tangibles,  GaussMarbles,  that  exploits  the  unique  affordances  of  spherical  tangibles  for  interacting  with  portable  physical  constraints.  The  proposed  design  of  each  magnetic  sphere  includes  a  magnetic  polyhedron  in  the  center.  The  magnetic  polyhedron  provides  bi-polar  magnetic  fields,  which  are  expanded  in  equal  dihedral  angles  as  robust  features  for  tracking,  allowing  an  analog  Hall-sensor  grid  to  resolve  the  near-surface  3D  position  accurately  in  real-time.  Possible  interactions  between  the  magnetic  spheres  and  portable  physical  constraints  in  various  levels  of  embodiment  were  explored  using  several  example  applications.
2	Games  against  health  a  player  centered  design  philosophy.  This  paper  announces  the  "Games  Against  Health"  (GAH)  research  agenda,  a  criticism  of,  and  response  to,  the  cultural  imperialism  of  the  "Games  for  Health"  paradigm.  Committed  to  player-centric  design  ethics,  GAH  seeks  to  dismantle  the  "games  for  health"  myth  as  neo-liberal  elitist  diktat.  We  acknowledge  the  values,  tastes  and  pleasures  of  billions  of  game  players  worldwide.  We  argue  that  game  designers  should  engage  more  efficiently  in  the  disimprovement  of  player  health  and  wellbeing  in  order  to  cater  to  those  players'  existing  preferences.  We  hope  the  paper  can  serve  as  a  convenient  reference  for  those  designing  psychotic,  sociopathic  or  antisocial  games.
2	Means  and  ends  in  human  computer  interaction  sustainability  through  disintermediation.  There  has  been  an  increased  interest  in  broader  contexts  from  ecology  and  economics  within  the  HCI  community  in  recent  years.  These  developments  suggest  that  the  HCI  community  should  engage  with  and  respond  to  concerns  that  are  external  to  computing  yet  profoundly  impact  human  society.  In  this  paper  we  observe  that  taking  these  broader  contexts  into  account  yields  a  fundamentally  different  way  to  think  about  sustainable  interaction  design,  one  in  which  the  designer's  focus  must  be  on  a)  ecological  limits,  b)  creating  designs  and  artifacts  that  do  not  further  a  cornucopian  paradigm,  and  c)  fundamental  human  needs.      It  can  be  hard  to  be  responsive  to  these  contexts  in  practical  HCI  work.  To  address  this,  we  propose  that  the  design  rubric  of  disintermediation  can  serve  as  a  unifying  approach  for  work  that  aims  to  meet  the  ecological  and  economic  challenges  outlined  in  the  literature.  After  discussing  the  potential  use  and  impact  of  disintermedation,  we  perform  an  analysis  using  this  design  rubric  to  several  key  application  areas.
2	Fighting  gulliver  an  experiment  with  cross  platform  players  fighting  a  body  controlled  giant.  Pretend  play  is  a  play  where  children  assign  roles  and  then  act  them  out.  A  child  may  even  imagine  himself  as  a  giant  among  his  toys  in  his  play.  To  rebuild  such  pretend  play,  immersive  gaming  experience  is  required  throughout  the  gameplay.  We  introduce  Fighting  Gulliver,  featuring  an  innovative  game  design  that  integrates  cutting-edge  technologies  including  virtual  reality  (VR),  body  motion  input  methods,  mobile  devices,  and  wearable  devices.  The  player  with  VR  headsets  and  body  motion  capturing  devices  plays  as  the  giant,  while  others  without  the  two  devices  join  forces  to  defeat  the  giant.  Players  playing  character  roles  other  than  the  giant  are  not  limited  on  a  desktop  device.  Entering  the  game  through  different  devices  results  in  different  character  roles,  skills,  and  user  interfaces.  As  a  result,  Fighting  Gulliver  is  a  cross-platform  multiplayer  online  role-playing  game  (MORPG)  with  novel  gaming  experience  and  unique  game  settings  rarely  found  in  previous  works.  Game  balance  design  and  a  user  study  are  also  discussed  in  this  work.
2	Neurotics  can  t  focus  an  in  situ  study  of  online  multitasking  in  the  workplace.  In  HCI  research,  attention  has  focused  on  understanding  external  influences  on  workplace  multitasking.  We  explore  instead  how  multitasking  might  be  influenced  by  individual  factors:  personality,  stress,  and  sleep.  Forty  information  workers'  online  activity  was  tracked  over  two  work  weeks.  The  median  duration  of  online  screen  focus  was  40  seconds.  The  personality  trait  of  Neuroticism  was  associated  with  shorter  online  focus  duration  and  Impulsivity-Urgency  was  associated  with  longer  online  focus  duration.  Stress  and  sleep  duration  showed  trends  to  be  inversely  associated  with  online  focus.  Shorter  focus  duration  was  associated  with  lower  assessed  productivity  at  day's  end.  Factor  analysis  revealed  a  factor  of  lack  of  control  which  significantly  predicts  multitasking.  Our  results  suggest  that  there  could  be  a  trait  for  distractibility  where  some  individuals  are  susceptible  to  online  attention  shifting  in  the  workplace.  Our  results  have  implications  for  information  systems  (e.g.  educational  systems,  game  design)  where  attention  focus  is  key.
2	When  the  tissue  box  says  bless  you  using  speech  to  build  socially  interactive  objects.  From  the  Internet  of  things  to  ubiquitous  computing,  smart  objects  are  everywhere  and  have  become  an  increasingly  significant  part  of  the  information  supply  chain.  However,  these  objects  remain  invisible  to  end-users  mostly  because  they  do  not  interact  with  them.  Our  project  is  devoted  to  brainstorming  different  design  possibilities  for  building  interfaces  for  these  smart  objects.  This  paper  explores  one  such  possibility-outfitting  the  object  with  a  speech  interface.  Study  participants  (N  =  63)  witnessed  the  experimenter  sneezing,  followed  by  a  "Bless  You"  from  either  a  nearby  tissue  box,  a  robot  in  the  room,  or  a  person  in  the  room.  Surprisingly,  users  found  the  speaking  tissue  box  to  be  as  social  and  agentic  as  a  humanoid  robot  and  a  human.  We  also  found  significant  moderating  effects  of  users'  preference  for  consistency,  parasocial  tendency  and  power  usage.  Participants  who  scored  high  on  these  traits  were  more  likely  to  regard  the  study  object  as  intelligent  and  likeable.  Users  also  tended  to  show  the  same  non-verbal  reactions  to  the  tissue  box  as  they  would  to  a  human  or  a  robot.
2	Caracterizacao  das  adaptacoes  em  metodos  de  avaliacao  para  aplicacoes  colaborativas.  Although  the  development  and  use  of  collaborative  systems  has  increased  lately,  evaluate  them  is  not  a  simple  task.  Most  of  the  HCI  evaluation  methods  that  are  consolidated  are  aimed  at  single-user  systems  and  require  adaptation  in  order  to  be  applied  to  collaborative  systems.  The  goal  of  this  paper  is  to  present  an  initial  characterization  of  the  adaptations  that  have  been  proposed  in  order  to  allow  single-user  evaluation  methods  to  be  applied  to  different  collaborative  systems  and  contexts.
2	The  effects  of  intended  use  on  target  acquisition.  Fitts's  Law  has  been  used  extensively  in  HCI  to  describe  2D  targeting;  however,  the  controlled  tasks  generally  used  neglect  aspects  of  real-world  pointing,  including  how  the  intended  use  of  a  target  affects  its  acquisition.  We  studied  aiming  to  a  target  in  four  tasks  requiring  varying  precision  after  acquisition.  Our  results  present  the  first  evidence  that  the  intended  use  of  a  target  affects  its  acquisition  in  terms  of  movement  time  and  motion  kinematics  for  computer  aiming.  Important  for  researchers  who  model  2D  targeting,  our  results  also  have  particular  impact  for  HCI  research  that  uses  motion  kinematics.
2	A  coord  input  coordinating  auxiliary  input  streams  for  augmenting  contextual  pen  based  interactions.  The  human  hand  can  naturally  coordinate  multiple  finger  joints,  and  simultaneously  tilt,  press  and  roll  a  pen  to  write  or  draw.  For  this  reason,  digital  pens  are  now  embedded  with  auxiliary  input  sensors  to  capture  these  actions.  Prior  research  on  auxiliary  input  channels  has  mainly  investigated  them  in  isolation  of  one  another.  In  this  work,  we  explore  the  coordinated  use  of  two  auxiliary  channels,  a  class  of  interaction  techniques  we  refer  to  as  a-coord  input.  Through  two  separate  experiments,  we  explore  the  design  space  of  a-coord  input.  In  the  first  study  we  identify  if  users  can  successfully  coordinate  two  auxiliary  channels.  We  found  a  strong  degree  of  coordination  between  channels.  In  a  second  experiment,  we  evaluate  the  effectiveness  of  a-coord  input  in  a  task  with  multiple  steps,  such  as  multi-parameter  selection  and  manipulation.  We  find  that  a-coord  input  facilitates  coordination  even  with  a  complex,  aforethought  sequential  task.  Overall  our  results  indicate  that  users  can  control  at  least  two  auxiliary  input  channels  in  conjunction  which  can  facilitate  a  number  of  common  tasks  can  on  the  pen.
2	Still  at  the  office  designing  for  physical  movement  inclusion  during  office  work.  In  this  paper  we  describe,  analyse  and  reflect  on  experiences  and  knowledge  generated  from  designing  for  physical  movement  integration  during  office  work.  Work  in  traditional  modern  office  settings  provides  few  physically  demanding  tasks.  Evidence  from  research  indicates  that  sedentary  life  styles  are  increasing  our  risk  for  developing  a  host  of  diseases  and  other  medical  complications.    Together  with  students  and  through  user-centered  design,  concepts  for  inviting  the  body  "back  to  work"  were  developed.  The  concepts  inspired  the  design  of  three  physical  movement  probes  that  were  explored  by  office  workers.  The  participants  were  encouraging  to  the  attempt  to  transform  the  sedentary  nature  of  office  work  into  more  physically  sustainable  work.  They  described  their  work  environments  as  filled  with  stuff  for  enhancing  physical  activity  but  these  were  seldom  used.  Integrating  physical  movements  in  the  design  of  future  office  work  tools  may  have  considerable  positive  effects  on  public  health.
2	Do  that  there  an  interaction  technique  for  addressing  in  air  gesture  systems.  When  users  want  to  interact  with  an  in-air  gesture  system,  they  must  first  address  it.  This  involves  finding  where  to  gesture  so  that  their  actions  can  be  sensed,  and  how  to  direct  their  input  towards  that  system  so  that  they  do  not  also  affect  others  or  cause  unwanted  effects.  This  is  an  important  problem  which  lacks  a  practical  solution.  We  present  an  interaction  technique  which  uses  multimodal  feedback  to  help  users  address  in-air  gesture  systems.  The  feedback  tells  them  how  ("do  that")  and  where  ("there")  to  gesture,  using  light,  audio  and  tactile  displays.  By  doing  that  there,  users  can  direct  their  input  to  the  system  they  wish  to  interact  with,  in  a  place  where  their  gestures  can  be  sensed.  We  discuss  the  design  of  our  technique  and  three  experiments  investigating  its  use,  finding  that  users  can  "do  that"  well  (93.2%-99.9%)  while  accurately  (51mm-80mm)  and  quickly  (3.7s)  finding  "there".
2	Tickers  and  talker  an  accessible  labeling  toolkit  for  3d  printed  models.  Three-dimensional  models  are  important  learning  resources  for  blind  people.  With  advances  in  3D  printing,  3D  models  are  becoming  more  available.  However,  unlike  visual  or  tactile  graphics,  there  is  no  standard  accessible  way  to  label  components  in  3D  models.  We  present  a  labeling  toolkit  that  enables  users  to  add  and  access  audio  labels  to  3D  printed  models.  The  toolkit  includes  Tickers,  small  3D  printed  percussion  instruments  added  to  3D  models,  and  Talker,  a  signal  processing  application  that  detects  and  classifies  Ticker  sounds.  To  use  the  toolkit,  a  model  designer  adds  Tickers  to  a  model  using  3D  modeling  software.  A  user  then  prints  the  model  with  Tickers  and  records  audio  labels  for  each  Ticker.  Finally,  users  can  strum  the  Tickers  and  Talker  will  play  the  corresponding  labels.  We  evaluated  Tickers  and  Talker  with  three  models  in  a  study  with  nine  blind  participants.  Our  toolkit  achieved  an  accuracy  of  93%  across  all  participants  and  models.  We  discuss  design  implications  and  future  work  for  accessible  3D  printed  models.
2	Undesigning  technology  considering  the  negation  of  design  by  design.  Motivated  by  substantive  concerns  with  the  limitations  and  negative  effects  of  technology,  this  paper  inquires  into  the  negation  of  technology  as  an  explicit  and  intentional  aspect  of  design  research  within  HCI.  Building  on  theory  from  areas  including  philosophy  and  design  theory,  this  paper  articulates  a  theoretical  framework  for  conceptualizing  the  intentional  negation  of  technology  (i.e.,  the  undesign  of  technology),  ranging  from  the  inhibition  of  particular  uses  of  technology  to  the  total  erasure  or  foreclosure  of  technology.  The  framework  is  then  expanded  upon  to  articulate  additional  areas  of  undesigning,  including  self-inhibition,  exclusion,  removal,  replacement,  restoration,  and  safeguarding.  In  conclusion  a  scheme  is  offered  for  addressing  questions  concerning  the  disciplinary  scope  of  undesign  in  the  context  of  HCI,  along  with  suggestions  for  ways  that  undesigning  may  be  more  strongly  incorporated  within  HCI  research.
2	Engagement  with  online  mental  health  interventions  an  exploratory  clinical  study  of  a  treatment  for  depression.  Online  mental  health  interventions  can  benefit  people  experiencing  a  range  of  psychological  difficulties,  but  attrition  is  a  major  problem  in  real-world  deployments.  We  discuss  strategies  to  reduce  attrition,  and  present  SilverCloud,  a  platform  designed  to  provide  more  engaging  online  experiences.  The  paper  presents  the  results  of  a  practice-based  clinical  study  in  which  45  clients  and  6  therapists  used  an  online  Cognitive  Behavioural  Therapy  programme  for  depression.  Pre  and  post-treatment  assessments,  using  the  Beck  Depression  Inventory,  indicate  a  statistically  significant  improvement  in  depressive  symptoms,  with  a  large  effect  size,  for  the  moderate-to-severe  clinical  sub-sample  receiving  standalone  online  treatment  (n=18).  This  group  was  the  primary  target  for  the  intervention.  A  high  level  of  engagement  was  also  observed  compared  to  a  prior  online  intervention  used  within  the  same  service.  We  discuss  strategies  for  design  in  this  area  and  consider  how  the  quantitative  and  qualitative  results  contribute  towards  our  understanding  of  engagement.
2	My  blood  sugar  is  higher  on  the  weekends  finding  a  role  for  context  and  context  awareness  in  the  design  of  health  self  management  technology.  Tools  for  self-care  of  chronic  conditions  often  do  not  fit  the  contexts  in  which  self-care  happens  because  the  influence  of  context  on  self-care  practices  is  unclear.  We  conducted  a  diary  study  with  15  adolescents  with  Type  1  Diabetes  and  their  caregivers  to  understand  how  context  affects  self-care.  We  observed  different  contextual  settings,  which  we  call  contextual  frames,  in  which  diabetes  self-management  varied  depending  on  certain  factors  -  physical  activity,  food,  emotional  state,  insulin,  people,  and  attitudes.  The  relative  prevalence  of  these  factors  across  contextual  frames  impacts  self-care  necessitating  different  types  of  support.  We  show  that  contextual  frames,  as  phenomenological  abstractions  of  context,  can  help  designers  of  context-aware  systems  systematically  explore  and  model  the  relation  of  context  with  behavior  and  with  technology  supporting  behavior.  Lastly,  considering  contextual  frames  as  sensitizing  concepts,  we  provide  design  direction  for  using  context  in  technology  design.
2	Frame  analysis  of  voice  interaction  gameplay.  Voice  control  is  an  increasingly  common  feature  of  digital  games,  but  the  experience  of  playing  with  voice  control  is  often  hampered  by  feelings  of  embarrassment  and  dissonance.  Past  research  has  recognised  these  tensions,  but  has  not  offered  a  general  model  of  how  they  arise  and  how  players  respond  to  them.  In  this  study,  we  use  Erving  Goffman's  frame  analysis,  as  adapted  to  the  study  of  games  by  Conway  and  Trevillian,  to  understand  the  social  experience  of  playing  games  by  voice.  Based  on  24  interviews  with  participants  who  played  voice-controlled  games  in  a  social  setting,  we  put  forward  a  frame  analytic  model  of  gameplay  as  a  social  event,  along  with  seven  themes  that  describe  how  voice  interaction  enhances  or  disrupts  the  player  experience.  Our  results  demonstrate  the  utility  of  frame  analysis  for  understanding  social  dissonance  in  voice  interaction  gameplay,  and  point  to  practical  considerations  for  designers  to  improve  engagement  with  voice-controlled  games.
2	Virtual  performance  augmentation  in  an  immersive  jump  run  exergame.  Human  performance  augmentation  through  technology  has  been  a  recurring  theme  in  science  and  culture,  aiming  to  increase  human  capabilities  and  accessibility.  We  investigate  a  related  concept:  virtual  performance  augmentation  (VPA),  using  VR  to  give  users  the  illusion  of  greater  capabilities  than  they  actually  have.  We  propose  a  method  for  VPA  of  running  and  jumping,  based  on  in  place  movements,  and  studied  its  effects  in  a  VR  exergame.  We  found  that  in  place  running  and  jumping  in  VR  can  be  used  to  create  a  somewhat  natural  experience  and  can  elicit  medium  to  high  physical  exertion  in  an  immersive  and  intrinsically  motivating  manner.  We  also  found  that  virtually  augmenting  running  and  jumping  can  increase  intrinsic  motivation,  perceived  competence  and  flow,  and  may  also  increase  motivation  for  physical  activity  in  general.  We  discuss  implications  of  VPA  for  safety  and  accessibility,  with  initial  evidence  suggesting  that  VPA  may  help  users  with  physical  impairments  enjoy  the  benefits  of  exergaming.
2	Unintended  consonances  methods  to  understand  robot  motor  sound  perception.  Recent  research  suggests  that  a  robot's  motors  make  sounds  that  can  influence  users'  perception  of  the  robot's  characteristics.  To  more  deeply  understand  users'  associations  with  specific  sonic  characteristics,  we  adapted  methods  from  sensory  science  including  Check  All  That  Apply  (CATA)  questions  and  Polarized  Sensory  Positioning  (PSP)  to  tease  out  small  differences  in  motor  sounds  in  an  online  survey.  These  methods  are  straightforward  for  untrained  people  to  do  in  an  online  setting,  mathematically  rigorous,  and  can  explore  a  variety  of  subtle  auditory  and  perceptual  stimuli.  We  describe  how  to  use  these  methods,  interpret  the  results  with  several  intuitive  visual  representations,  and  show  that  the  results  align  with  a  previous  study  of  the  same  dataset.  We  close  by  discussing  benefits  and  limitations  of  applying  these  methods  to  study  subtle  phenomena  in  the  HCI  community.
2	Mixed  reality  remote  collaboration  combining  360  video  and  3d  reconstruction.  Remote  Collaboration  using  Virtual  Reality  (VR)  and  Augmented  Reality  (AR)  has  recently  become  a  popular  way  for  people  from  different  places  to  work  together.  Local  workers  can  collaborate  with  remote  helpers  by  sharing  360-degree  live  video  or  3D  virtual  reconstruction  of  their  surroundings.  However,  each  of  these  techniques  has  benefits  and  drawbacks.  In  this  paper  we  explore  mixing  360  video  and  3D  reconstruction  together  for  remote  collaboration,  by  preserving  benefits  of  both  systems  while  reducing  drawbacks  of  each.  We  developed  a  hybrid  prototype  and  conducted  user  study  to  compare  benefits  and  problems  of  using  360  or  3D  alone  to  clarify  the  needs  for  mixing  the  two,  and  also  to  evaluate  the  prototype  system.  We  found  participants  performed  significantly  better  on  collaborative  search  tasks  in  360  and  felt  higher  social  presence,  yet  3D  also  showed  potential  to  complement.  Participant  feedback  collected  after  trying  our  hybrid  system  provided  directions  for  improvement.
2	A  survey  on  the  awareness  of  brazilian  web  development  community  about  cognitive  accessibility.  Although  the  awareness  about  cognitive  disabilities  has  been  increasing  in  the  last  few  years,  it  is  still  less  than  necessary  compared  to  other  disabilities.  The  need  for  an  investigation  about  this  issue  is  part  of  the  agenda  of  the  Challenge  2  (Accessibility  and  Digital  Inclusion)  from  GranDIHC-Br.  This  paper  describes  the  results  of  an  online  exploratory  survey  conducted  with  105  web  development  professionals  to  understand  their  knowledge  and  barriers  regarding  accessibility  for  people  with  cognitive  disabilities.  The  results  evidenced  three  biases  that  potentially  prevent  those  professionals  from  approaching  cognitive  disabilities:  strong  organizational  barriers;  difficulty  to  understand  issues  related  to  cognitive  disabilities;  a  knowledge  gap  about  web  accessibility.  Thus,  we  confirmed  that  web  development  professionals  still  lack  awareness  about  cognitive  web  accessibility,  and  we  suggest  that  applied  research  studies  focus  on  how  to  fill  this  knowledge  gap  before  providing  tools,  artifacts  or  frameworks.
2	Recall  crowdsourcing  on  basic  phones  to  financially  sustain  voice  forums.  Although  voice  forums  are  widely  used  to  enable  marginalized  communities  to  produce,  consume,  and  share  information,  their  financial  sustainability  is  a  key  concern  among  HCI4D  researchers  and  practitioners.  We  present  ReCall,  a  crowdsourcing  marketplace  accessible  via  phone  calls  where  low-income  rural  residents  vocally  transcribe  audio  files  to  gain  free  airtime  to  participate  in  voice  forums  as  well  as  to  earn  money.  We  conducted  a  series  of  experimental  and  usability  evaluations  with  28  low-income  people  in  rural  India  to  examine  the  effect  of  phone  types,  channel  types,  and  review  modes  on  speech  transcription  performance.  We  then  deployed  ReCall  for  two  weeks  to  24  low-income  rural  residents  who  placed  5,879  phone  calls,  completed  29,000  micro  tasks  to  yield  transcriptions  with  85%  accuracy,  and  earned  INR  20,500.  Our  mixed-methods  analysis  indicates  that  each  minute  of  crowd  work  on  ReCall  gives  users  eight  minutes  of  free  airtime  on  another  voice  forum,  and  thus  illustrates  a  way  to  address  the  financial  sustainability  of  voice  forums.
2	Active  edge  designing  squeeze  gestures  for  the  google  pixel  2.  Active  Edge  is  a  feature  of  Google  Pixel  2  smartphone  devices  that  creates  a  force-sensitive  interaction  surface  along  their  sides,  allowing  users  to  perform  gestures  by  holding  and  squeezing  their  device.  Supported  by  strain  gauge  elements  adhered  to  the  inner  sidewalls  of  the  device  chassis,  these  gestures  can  be  more  natural  and  ergonomic  than  on-screen  (touch)  counterparts.  Developing  these  interactions  is  an  integration  of  several  components:  (1)  an  insight  and  understanding  of  the  user  experiences  that  benefit  from  squeeze  gestures;  (2)  hardware  with  the  sensitivity  and  reliability  to  sense  a  user's  squeeze  in  any  operating  environment;  (3)  a  gesture  design  that  discriminates  intentional  squeezes  from  innocuous  handling;  and  (4)  an  interaction  design  to  promote  a  discoverable  and  satisfying  user  experience.  This  paper  describes  the  design  and  evaluation  of  Active  Edge  in  these  areas  as  part  of  the  product's  development  and  engineering.
2	I  can  do  everything  but  see  how  people  with  vision  impairments  negotiate  their  abilities  in  social  contexts.  This  research  takes  an  orientation  to  visual  impairment  (VI)  that  does  not  regard  it  as  fixed  or  determined  alone  in  or  through  the  body.  Instead,  we  consider  (dis)ability  as  produced  through  interactions  with  the  environment  and  configured  by  the  people  and  technology  within  it.  Specifically,  we  explore  how  abilities  become  negotiated  through  video  ethnography  with  six  VI  athletes  and  spectators  during  the  Rio  2016  Paralympics.  We  use  generated  in-depth  examples  to  identify  how  technology  can  be  a  meaningful  part  of  ability  negotiations,  emphasizing  how  these  embed  into  the  social  interactions  and  lives  of  people  with  VI.  In  contrast  to  treating  technology  as  a  solution  to  a  'sensory  deficit',  we  understand  it  to  support  the  triangulation  process  of  sense-making  through  provision  of  appropriate  additional  information.  Further,  we  suggest  that  technology  should  not  try  and  replace  human  assistance,  but  instead  enable  people  with  VI  to  better  identify  and  interact  with  other  people  in-situ.
2	Designing  an  emergency  response  community  for  opioid  overdoses  in  philadelphia.  Fatal  overdoses  are  a  common  symptom  of  the  opioid  epidemic  which  has  been  devastating  communities  throughout  the  USA  for  decades.  Philadelphia  has  been  particularly  impacted,  with  a  drug  overdose  death  rate  of  46.8  per  100,000  individuals,  far  surpassing  other  large  cities'  rates.  Despite  city  and  community  efforts,  this  rate  continues  to  increase,  indicating  the  need  for  new,  more  effective  approaches  aimed  at  mitigating  and  combating  this  issue.  Through  a  human-centered  design  process,  we  investigated  motivators  and  barriers  to  participation  in  a  smartphone-based  system  that  mobilizes  community  members  to  administer  emergency  care  for  individuals  experiencing  an  overdose.  We  discuss  evidence  of  the  system's  feasibility,  and  how  it  would  benefit  from  integration  with  existing  community-based  efforts.
2	Mathland  playful  mathematical  learning  in  mixed  reality.  Mathematical  experiences  are  intrinsic  to  our  everyday  lives,  yet  mathematics  education  is  mostly  confined  to  textbooks.  Seymour  Papert  used  the  term  'Mathland'  to  propose  a  world  where  one  would  learn  mathematics  as  naturally  as  one  learns  French  while  growing  up  in  France.  We  demonstrate  a  Mixed  Reality  application  that  augments  the  physical  world  with  interactive  mathematical  concepts  to  enable  constructionist  mathematical  learning  in  the  real  world.  Using  Mathland,  people  can  collaboratively  explore,  experience  and  experiment  with  mathematical  phenomena  in  playful,  applied  and  exploratory  ways.  We  implemented  Mathland  using  the  Microsoft  Hololens  and  two  custom  controllers  to  afford  complete  immersion  through  tangible  interactions,  embodiment  and  situated  learning.
2	Towards  personality  driven  persuasive  health  games  and  gamified  systems.  Persuasive  games  and  gamified  systems  are  effective  tools  for  motivating  behavior  change  using  various  persuasive  strategies.  Research  has  shown  that  tailoring  these  systems  can  increase  their  efficacy.  However,  there  is  little  knowledge  on  how  game-based  persuasive  systems  can  be  tailored  to  individuals  of  various  personality  traits.  To  advance  research  in  this  area,  we  conducted  a  large-scale  study  of  660  participants  to  investigate  how  different  personalities  respond  to  various  persuasive  strategies  that  are  used  in  persuasive  health  games  and  gamified  systems.  Our  results  reveal  that  people's  personality  traits  play  a  significant  role  in  the  perceived  persuasiveness  of  different  strategies.  Conscientious  people  tend  to  be  motivated  by  goal  setting,  simulation,  self-monitoring  and  feedback;  people  who  are  more  open  to  experience  are  more  likely  to  be  demotivated  by  rewards,  competition,  comparison,  and  cooperation.  We  contribute  to  the  CHI  community  by  offering  design  guidelines  for  tailoring  persuasive  games  and  gamified  designs  to  a  particular  group  of  personalities.
2	On  the  road  with  an  autonomous  passenger  shuttle  integration  in  public  spaces.  The  integration  of  autonomous  vehicles  (AVs)  onto  public  roads  presents  both  technical  and  social  challenges.  Public  understanding  and  acceptance  of  AVs  requires  engagement  with  people  who  live  in,  work  at  or  visit  cities  where  they  are  deployed  on  public  road  networks.  We  investigate  the  impact  of  one  of  the  first  placements  of  AV  passenger  transport  on  public  roadways:  the  Sion  "SmartShuttle".  This  late-breaking  research  presents  preliminary  results  from  interviews  with  local  shopkeepers,  residents,  pedestrians  and  drivers  to  understand  their  attitudes  and  opinions  of  the  shuttle.  We  also  discuss  video-based  fieldwork  that  demonstrates  how  drivers  negotiate  next  moves  with  one  another  through  their  windscreens  using  embodied  signals  such  as  gestures,  lip-reading,  and  head  nods  to  coordinate  and  manage  a  traffic  situation.  Finally,  we  consider  the  implications  for  how  fully  autonomous  vehicles  might  be  designed  to  take  into  account  the  subtle  negotiations  that  road  users  engage  in  to  coordinate  with  one  another.
2	Improving  the  usability  and  ux  of  the  swiss  internet  voting  interface.  Up  to  20%  of  residential  votes  and  up  to  70%  of  absentee  votes  in  Switzerland  are  cast  online.  The  Swiss  system  aims  to  provide  individual  verifiability  by  different  verification  codes.  The  voters  have  to  carry  out  verification  on  their  own,  making  the  usability  and  UX  of  the  interface  of  great  importance.  To  improve  the  usability,  we  first  performed  an  evaluation  with  12  human-computer  interaction  experts  to  uncover  usability  weaknesses  of  the  Swiss  Internet  voting  interface.  Based  on  the  experts'  findings,  related  work,  and  an  exploratory  user  study  with  36  participants,  we  propose  a  redesign  that  we  evaluated  in  a  user  study  with  49  participants.  Our  study  confirmed  that  the  redesign  indeed  improves  the  detection  of  incorrect  votes  by  33%  and  increases  the  trust  and  understanding  of  the  voters.  Our  studies  furthermore  contribute  important  lessons  for  designing  verifiable  e-voting  systems  in  general.
2	Snapstream  snapshot  based  interaction  in  live  streaming  for  visual  art.  Live  streaming  visual  art  such  as  drawing  or  using  design  software  is  gaining  popularity.  An  important  aspect  of  live  streams  is  the  direct  and  real-time  communication  between  streamers  and  viewers.  However,  currently  available  text-based  interaction  limits  the  expressiveness  of  viewers  as  well  as  streamers,  especially  when  they  refer  to  specific  moments  or  objects  in  the  stream.  To  investigate  the  feasibility  of  using  snapshots  of  streamed  content  as  a  way  to  enhance  streamer-viewer  interaction,  we  introduce  Snapstream,  a  system  that  allows  users  to  take  snapshots  of  the  live  stream,  annotate  them,  and  share  the  annotated  snapshots  in  the  chat.  Streamers  can  also  verbally  reference  a  specific  snapshot  during  streaming  to  respond  to  viewers'  questions  or  comments.  Results  from  live  deployments  show  that  participants  communicate  more  expressively  and  clearly  with  increased  engagement  using  Snapstream.  Participants  used  snapshots  to  reference  part  of  the  artwork,  give  suggestions  on  it,  make  fun  images  or  memes,  and  log  intermediate  milestones.  Our  findings  suggest  that  visual  interaction  enables  richer  experiences  in  live  streaming.
2	A  composite  cognitive  workload  assessment  system  in  pilots  under  various  task  demands  using  ensemble  learning.  The  preservation  of  attentional  resources  under  mental  stress  holds  particular  importance  for  the  execution  of  effective  performance.  Specifically,  the  failure  to  conserve  attentional  resources  could  result  in  an  overload  of  attentional  capacity,  the  failure  to  execute  critical  brain  processes,  and  suboptimal  decision-making  for  effective  motor  performance.  Therefore,  assessment  of  attentional  resources  is  particularly  important  for  individuals  such  as  pilots  who  must  retain  adequate  attentional  reserve  to  respond  to  unexpected  events  when  executing  their  primary  task.  This  study  aims  to  devise  an  expert  model  to  assess  an  operator’s  dynamic  cognitive  workload  in  a  flight  simulator  under  various  levels  of  challenge.  The  results  indicate  that  the  operator’s  cognitive  workload  can  be  effectively  predicted  with  combined  classifiers  of  neurophysiological  biomarkers,  subjective  assessments  of  perceived  cognitive  workload,  and  task  performance.  This  work  provides  conceptual  feasibility  to  develop  a  real-time  cognitive  state  monitoring  tool  that  facilitates  adaptive  human-computer  interaction  in  operational  environments.
2	Affordance  allowing  objects  to  communicate  dynamic  use.  We  propose  extending  the  affordance  of  objects  by  allowing  them  to  communicate  dynamic  use,  such  as  (1)  motion  (e.g.,  spray  can  shakes  when  touched),  (2)  multi-step  processes  (e.g.,  spray  can  sprays  only  after  shaking),  and  (3)  behaviors  that  change  over  time  (e.g.,  empty  spray  can  does  not  allow  spraying  anymore).  Rather  than  enhancing  objects  directly,  however,  we  implement  this  concept  by  enhancing  the  user.  We  call  this  affordance++.  By  stimulating  the  user's  arms  using  electrical  muscle  stimulation,  our  prototype  allows  objects  not  only  to  make  the  user  actuate  them,  but  also  perform  required  movements  while  merely  approaching  the  object,  such  as  not  to  touch  objects  that  do  not  "want"  to  be  touched.  In  our  user  study,  affordance++  helped  participants  to  successfully  operate  devices  of  poor  natural  affordance,  such  as  a  multi-functional  slicer  tool  or  a  magnetic  nail  sweeper,  and  to  stay  away  from  cups  filled  with  hot  liquids.
2	Find  an  expert  designing  expert  selection  interfaces  for  formal  help  giving.  A  critical  aspect  of  formal  help-giving  tasks  in  the  enterprise  is  finding  the  right  expert.  We  built  and  evaluated  a  tool,  Find  an  Expert,  to  examine  what  the  most  important  expert  selection  criteria  are  for  help-seekers  and  how  to  represent  them  in  expert  selection  interfaces  for  formal  help-giving  tasks.  We  observed  users'  expert  selection  decisions  and  found  that  the  diversity  of  topic  expertise  and  the  amount  of  related  experience  were  significantly  important  in  helping  users  decide  which  expert  to  contact.  Through  self-reported  data  from  users,  we  found  that  in  addition  to  expertise  and  experience,  expert  accessibility  indicators,  like  online  availability  and  language  proficiency,  were  considered  important  criteria  for  selecting  experts.  Finally,  publicly-displayed  crowdsourced  ratings  of  experts,  while  deemed  useful  indicators  of  expert  quality  by  help-seekers,  raised  concerns  for  experts.  We  conclude  with  suggestions  regarding  the  design  of  expert  selection  interfaces  for  formal  help-giving  tasks.
2	Museegk  a  brain  computer  musical  interface.  We  present  a  novel  integration  of  a  brain-computer  interface  (BCI)  with  a  music  step  sequencer  composition  program.  Previous  BCIs  that  utilize  EEG  data  to  form  music  provide  users  little  control  over  the  final  composition  or  do  not  provide  enough  feedback.  Our  interface  allows  a  user  to  create  and  modify  a  melody  in  real  time  and  provides  continuous  aural  and  visual  feedback  to  the  user,  thus  affording  them  a  controllable  means  to  achieve  creative  expression.
2	The  reality  of  fantasy  uncovering  information  seeking  behaviors  and  needs  in  online  fantasy  sports.  Online  fantasy  sports  are  rapidly  growing  in  popularity.  Fantasy  sports  players  consume  massive  amounts  of  sports  and  player  statistics  in  order  to  manage  their  teams,  such  as  to  determine  who  they  want  on  their  fantasy  sports  team  and  what  changes  they  want  to  make  during  the  season.  With  more  people  actively  engaging  in  this  activity  and  increasing  investment  in  this  industry,  this  case  study  performs  the  first  detailed  investigation  into  information-seeking  behaviors  and  information  needs  of  online  fantasy  sports  players.  Two  online  fantasy  sports  were  studied:  fantasy  football  and  NASCAR.  Common  themes  from  one-on-one  interviews  with  active  fantasy  sports  players  are  discussed  and  areas  for  future  research  identified.  Implications  for  system  design  include  more  targeted  data  provision  throughout  the  sports  seasons,  better  aggregation  of  online  sports  statistics  and  data,  development  of  mobile  applications,  and  innovation  in  fantasy  sports  gaming.
2	Social  consequences  of  grindr  use  extending  the  internet  enhanced  self  disclosure  hypothesis.  Grindr,  a  location-based  real-time  dating  application,  provides  sexual-minority  men  (SMM)  a  space  through  which  they  can  identify,  access,  and  communicate  with  one  another.  Although  previous  research  has  examined  user  motivations  and  public  self-disclosure  patterns  on  Grindr,  we  investigate  the  effects  intimate  self-disclosure  and  sexting  via  the  application's  private  messaging  on  internalized  homophobia  and  loneliness.  Using  the  Internet-enhanced  self-disclosure  hypothesis  (ISDH)  as  a  framework,  we  conducted  an  online  survey  of  274  Grindr  users.  Serial  mediation  analysis  showed  support  for  the  ISDH,  suggesting  that  Grindr  use  was  negatively  associated  with  loneliness.  Intimate  self-disclosure  and  internalized  homophobia  mediated  the  relationship  between  Grindr  use  and  loneliness,  but  sexting  had  no  relationship  with  internalized  homophobia  or  loneliness.  We  discuss  implications  for  the  ISDH,  Grindr,  self-disclosure,  and  sexting.
2	Engaging  older  people  using  participatory  design.  The  use  of  digital  technologies  is  increasingly  proposed  in  health  and  social  care  to  address  the  aging  population  phenomenon  but,  in  practice,  the  designers  of  these  technologies  are  ill  equipped  to  design  for  older  people.  We  suggest  participatory  design  as  an  approach  to  improving  the  quality  of  design  for  older  people  but,  based  on  previous  work  and  our  own  experiences,  identify  four  central  issues  that  participatory  design  approaches  need  to  address.  We  describe  an  approach  to  early  engagement  in  design  with  older  people  that  address  each  of  these  issues  and  some  of  our  experiences  applying  the  approach  in  a  variety  of  different  design  projects.  We  conclude  by  discussing  some  of  the  issues  that  have  been  highlighted  when  attempting  apply  this  approach  in  different  design  contexts  and  the  issues  that  have  been  raised  when  working  with  partners  who  are  less  committed  to  the  idea  of  engaging  with  older  adults  in  participatory  design.
2	Customization  bias  in  decision  support  systems.  Many  Decision  Support  Systems  (DSS)  afford  customization  of  inputs  or  algorithms  before  generating  recommendations  to  a  decision  maker.  This  paper  describes  an  experiment  in  which  users  make  decisions  assisted  by  recommendations  of  a  DSS  in  a  fantasy  baseball  game.  This  experiment  shows  that  the  act  of  customizing  a  DSS  can  lead  to  biased  decision  making.  I  show  that  users  who  believe  they  have  customized  a  DSS's  recommendation  algorithm  are  more  likely  to  follow  the  recommendations  regardless  of  their  accuracy.  I  also  show  that  this  customization  bias  is  the  result  of  using  a  DSS  to  seek  confirmatory  information  in  a  recommendation.
2	Rethinking  the  mobile  food  journal  exploring  opportunities  for  lightweight  photo  based  capture.  Food  choices  are  among  the  most  frequent  and  important  health  decisions  in  everyday  life,  but  remain  notoriously  difficult  to  capture.  This  work  examines  opportunities  for  lightweight  photo-based  capture  in  mobile  food  journals.  We  first  report  on  a  survey  of  257  people,  examining  how  they  define  healthy  eating,  their  experiences  and  challenges  with  existing  food  journaling  methods,  and  their  ability  to  interpret  nutritional  information  that  can  be  captured  in  a  food  journal.  We  then  report  on  interviews  and  a  field  study  with  27  participants  using  a  lightweight,  photo-based  food  journal  for  between  4  to  8  weeks.  We  discuss  mismatches  between  motivations  and  current  designs,  challenges  of  current  approaches  to  food  journaling,  and  opportunities  for  photos  as  an  alternative  to  the  pervasive  but  often  inappropriate  emphasis  on  quantitative  tracking  in  mobile  food  journals.
2	Re  imagining  persuasion  designing  for  self  transcendence.  The  last  few  years  have  seen  a  flurry  of  persuasive  technologies  aiming  to  encourage  pro-environmental  behaviors.  In  this  study,  I  critique  the  dominant  means  of  persuasion  by  operationalizing  and  applying  the  lessons  of  a  robust  body  of  psychology  research  on  values,  specifically  exploring  the  kinds  of  values  accommodated  by  and  appealed  to  with  these  technologies.  Results  indicate  that  these  designs  overwhelming  appeal  to  Self-Enhancement  values,  the  same  strategic  approach  associated  with  historically  unsuccessful  environmental  and  social  campaigns.  This  insight  is  used  as  a  springboard  for  discussion  about  a  radically  different,  and  thus  far  untried  strategy  for  addressing  the  challenge  of  sustainability  within  persuasive  technology  research  and  sustainable  HCI  more  generally.
2	A  systematic  review  of  quantitative  studies  on  the  enjoyment  of  digital  entertainment  games.  Enjoyment  has  been  identified  as  a  central  component  of  the  player  experience  (PX),  but  various,  overlapping  concepts  within  PX  make  it  difficult  to  develop  valid  measures  and  a  common  understanding  of  game  enjoyment.  We  conducted  a  systematic  review  of  87  quantitative  studies,  analyzing  different  operationalizations  and  measures  of  game  enjoyment,  its  determinants,  and  how  these  were  related  to  other  components  of  PX,  such  as  flow,  presence  and  immersion.  Results  suggest  that  game  enjoyment  describes  the  positive  cognitive  and  affective  appraisal  of  the  game  experience,  and  may  in  part  be  associated  with  the  support  of  player  needs  and  values.  Further,  we  outline  that  enjoyment  is  distinct  from  flow  in  that  it  may  occur  independently  of  challenge  and  cognitive  involvement,  and  argue  that  enjoyment  may  be  understood  as  the  valence  of  the  player  experience.  We  conclude  with  a  discussion  of  methodological  challenges  and  point  out  opportunities  for  future  research  on  game  enjoyment.
2	Sprweb  preserving  subjective  responses  to  website  colour  schemes  through  automatic  recolouring.  Colours  are  an  important  part  of  user  experiences  on  the  Web.  Colour  schemes  influence  not  only  the  aesthetics,  but  also  our  first  impressions  and  long-term  engagement  with  websites  (e.g.,  Figure  1  shows  a  'warm'  website  colour  scheme).  However,  five  percent  of  people  perceive  a  subset  of  all  colours  because  they  have  colour  vision  deficiency  (CVD),  resulting  in  an  unequal  and  presumably  less-rich  user  experience  on  the  Web  (Figure  2).  Traditionally,  people  with  CVD  have  been  supported  by  recolouring  tools  that  improve  colour  differentiability,  but  do  not  consider  the  subjective  properties  of  colour  schemes  while  recolouring  (Figure  3  shows  Figure  1  after  standard  recolouring;  it  is  now  'cool'  instead  of  'warm').  To  address  this,  we  developed  SPRWeb,  a  tool  that  recolours  websites  to  preserve  subjective  responses  and  improve  colour  differentiability  -  thus  enabling  users  with  CVD  to  have  similar  online  experiences  (Figure  4  shows  Figure  1  recoloured  using  SPRWeb;  it  is  once  again  'warm').  To  develop  SPRWeb,  we  extended  existing  models  of  non-CVD  subjective  responses  to  people  with  CVD,  then  used  this  extended  model  to  steer  the  recolouring  process.  In  a  lab  study,  we  found  that  SPRWeb  did  significantly  better  than  a  standard  recolouring  tool  at  preserving  the  temperature  and  naturalness  of  websites,  while  achieving  similar  weight  and  differentiability  preservation.  We  also  found  that  recolouring  did  not  preserve  activity,  and  hypothesize  that  visual  complexity  influences  activity  more  than  colour.  SPRWeb  is  the  first  tool  to  automatically  preserve  the  subjective  and  perceptual  properties  of  website  colour  schemes  thereby  equalizing  the  colour-based  web  experience  for  people  with  CVD.
2	Immerseboard  immersive  telepresence  experience  using  a  digital  whiteboard.  ImmerseBoard  is  a  system  for  remote  collaboration  through  a  digital  whiteboard  that  gives  participants  a  3D  immersive  experience,  enabled  only  by  an  RGBD  camera  (Microsoft  Kinect)  mounted  on  the  side  of  a  large  touch  display.  Using  3D  processing  of  the  depth  images,  life-sized  rendering,  and  novel  visualizations,  ImmerseBoard  emulates  writing  side-by-side  on  a  physical  whiteboard,  or  alternatively  on  a  mirror.  User  studies  involving  three  tasks  show  that  compared  to  standard  video  conferencing  with  a  digital  whiteboard,  ImmerseBoard  provides  participants  with  a  quantitatively  better  ability  to  estimate  their  remote  partners'  eye  gaze  direction,  gesture  direction,  intention,  and  level  of  agreement.  Moreover,  these  quantitative  capabilities  translate  qualitatively  into  a  heightened  sense  of  being  together  and  a  more  enjoyable  experience.  ImmerseBoard's  form  factor  is  suitable  for  practical  and  easy  installation  in  homes  and  offices.
2	Optimizing  touchscreen  keyboards  for  gesture  typing.  Despite  its  growing  popularity,  gesture  typing  suffers  from  a  major  problem  not  present  in  touch  typing:  gesture  ambiguity  on  the  Qwerty  keyboard.  By  applying  rigorous  mathematical  optimization  methods,  this  paper  systematically  investigates  the  optimization  space  related  to  the  accuracy,  speed,  and  Qwerty  similarity  of  a  gesture  typing  keyboard.  Our  investigation  shows  that  optimizing  the  layout  for  gesture  clarity  (a  metric  measuring  how  unique  word  gestures  are  on  a  keyboard)  drastically  improves  the  accuracy  of  gesture  typing.  Moreover,  if  we  also  accommodate  gesture  speed,  or  both  gesture  speed  and  Qwerty  similarity,  we  can  still  reduce  error  rates  by  52%  and  37%  over  Qwerty,  respectively.  In  addition  to  investigating  the  optimization  space,  this  work  contributes  a  set  of  optimized  layouts  such  as  GK-D  and  GK-T  that  can  immediately  benefit  mobile  device  users.
2	Beyond  energy  monitors  interaction  energy  and  emerging  energy  systems.  Motivated  by  a  recent  surge  of  research  related  to  energy  and  sustainability,  this  paper  presents  a  review  of  energy-related  work  within  HCI  as  well  as  from  literature  outside  of  HCI.  Our  review  of  energy-related  HCI  research  identifies  a  central  cluster  of  work  focused  on  electricity  consumption  feedback  (ECF).  Our  review  of  literature  outside  of  HCI  highlights  a  number  of  emerging  energy  systems  trends  of  strong  relevance  to  HCI  and  interaction  design,  including  smart  grid,  demand  response,  and  distributed  generation  technologies.  We  conclude  by  outlining  a  range  of  opportunities  for  HCI  to  engage  with  the  experiential,  behavioral,  social,  and  cultural  aspects  of  these  emerging  systems,  including  highlighting  new  areas  for  ECF  research  that  move  beyond  our  field's  current  focus  on  energy  feedback  displays  to  increase  awareness  and  motivate  individual  conservation  behavior.
2	Cyrafour  how  two  human  avatars  communicate  with  each  other.  Human  avatars  or  physical  surrogates  are  becoming  increasingly  present  in  leisure,  artistic  and  business  activities  that  seek  to  augment  the  sensory  richness  available  to  telepresent  participants.  While  a  number  of  studies  have  focused  on  how  human  avatars  relate  to  other  humans,  little  attention  has  been  paid  to  the  particularities  of  human  avatar  to  human  avatar  interaction.  This  paper  examines  characteristic  features  of  such  interaction  through  Cyrafour,  a  playful  embodied  identity  game  in  which  two  human  avatars  clone  various  conversations  generated  elsewhere.  Such  cloning,  or  speech  shadowing,  seems  to  allow  for  an  empathic  embodiment  of  the  meaning  transmitted  and  appears  to  create  a  frame  for  further  discussion  on  the  topics  raised.  This  project  contributes  to  the  study  of  telepresence  with  new  insights  applicable  to  the  design  and  research  of  human  computer  and  human  robot  interfaces.
2	Infrastructure  as  creative  action  online  buying  selling  and  delivery  in  phnom  penh.  This  paper  describes  a  complex  global  sales  and  logistics  network  based  in  Phnom  Penh,  Cambodia,  which  utilizes  Internet  tools  (particularly  Facebook)  as  well  as  a  suite  of  offline  tools  such  as  feature  phones,  paper  receipts,  and  motorcycles  to  facilitate  the  buying  and  selling  of  clothes  and  other  commodities.  Against  the  gap  or  import  models  that  sometimes  limit  HCI  understandings  of  computational  change  in  non-Western  environments,  we  argue  that  the  consumers,  business  owners,  delivery  drivers,  and  call  center  staff  play  active  and  formative  roles  in  producing  this  infrastructure,  integrating  new  tools  into  older  cultural  practices  and  determining  how  they  work  within  the  limits  and  conventions  of  the  environment.  We  argue  that  resourceful  and  imaginative  activities  such  as  these  constitute  a  form  of  creative  infrastructural  action  and  are  central  to  the  ways  that  new  tools  circulate  in  the  world,  though  they  often  go  unrecognized  by  HCI  as  innovation.
2	Playing  with  leadership  and  expertise  military  tropes  and  teamwork  in  an  arg.  Ad-hoc  virtual  teams  often  lack  tools  to  formalize  leadership  and  structure  collaboration,  yet  they  are  often  successful.  How  does  this  happen?  We  argue  that  the  emergence  of  leadership  and  the  development  of  expertise  occurs  in  the  process  of  taking  action  and  in  direct  response  to  a  lack  of  structure.  Using  a  twinned  set  of  eight  modality  sliders,  we  examine  the  interactions  of  fourteen  players  in  an  alternate  reality  game.  We  find  that  players  adopted  military  language  and  culture  to  structure  and  arrange  their  play.  We  determine  that  it  is  critical  to  account  for  the  context  of  play  across  these  modalities  in  order  to  design  appropriately  for  effective  in-game  virtual  organizing.
2	Remote  handshaking  touch  enhances  video  mediated  social  telepresence.  Since  past  studies  on  haptic  and  visual  communication  have  tended  to  be  isolated  from  each  other,  it  has  remained  unclear  whether  a  touch  channel  can  still  enrich  mediated  communication  where  video  and  audio  channels  are  already  available.  To  clarify  this,  we  analyzed  remote  handshaking  in  which  a  robot  hand  that  was  attached  just  under  a  videoconferencing  terminal's  display  moved  according  to  the  opening  and  closing  motion  of  a  conversation  partner's  hand.  Combining  touch  and  video  channels  raises  a  question  as  to  whether  the  partner's  action  of  touching  a  haptic  device  should  be  visible  to  the  user.  If  it  can  be  invisible,  the  action  may  be  unnecessary,  and  a  unilaterally  controlled  device  may  be  enough  to  establish  an  effective  touch  channel.  Our  analysis  revealed  that  the  feeling  of  being  close  to  the  partner  can  be  enhanced  by  mutual  touch  in  which  the  partner's  action  needs  to  occur  but  should  be  invisible.
2	Toward  principles  for  the  design  of  navigation  affordances  in  code  editors  an  empirical  investigation.  Design  principles  are  a  key  tool  for  creators  of  interactive  systems;  however,  a  cohesive  set  of  principles  has  yet  to  emerge  for  the  design  of  code  editors.  In  this  paper,  we  conducted  a  between-subjects  empirical  study  comparing  the  navigation  behaviors  of  32  professional  LabVIEW  programmers  using  two  different  code-editor  interfaces:  the  ubiquitous  tabbed  editor  and  the  experimental  Patchworks  editor.  Our  analysis  focused  on  how  the  programmers  arranged  and  navigated  among  open  information  patches  (i.e.,  code  modules  and  program  output).  Key  findings  of  our  study  included  that  Patchworks  users  made  significantly  fewer  click  actions  per  navigation,  juxtaposed  patches  side  by  side  significantly  more,  and  exhibited  significantly  fewer  navigation  mistakes  than  tabbed-editor  users.  Based  on  these  findings  and  more,  we  propose  five  general  principles  for  the  design  of  effective  navigation  affordances  in  code  editors.
2	Designing  usable  web  forms  empirical  evaluation  of  web  form  improvement  guidelines.  This  study  reports  a  controlled  eye  tracking  experiment  (N  =  65)  that  shows  the  combined  effectiveness  of  20  guidelines  to  improve  interactive  online  forms  when  applied  to  forms  found  on  real  company  websites.  Results  indicate  that  improved  web  forms  lead  to  faster  completion  times,  fewer  form  submission  trials,  and  fewer  eye  movements.  Data  from  subjective  questionnaires  and  interviews  further  show  increased  user  satisfaction.  Overall,  our  findings  highlight  the  importance  for  web  designers  to  improve  their  web  forms  using  UX  guidelines.
2	Smartrsvp  facilitating  attentive  speed  reading  on  small  screen  wearable  devices.  Smart  watches  can  enrich  everyday  interactions  by  providing  both  glanceable  information  and  instant  access  to  frequent  tasks.  However,  reading  text  messages  on  a  1.5-inch  screen  is  inherently  challenging,  especially  when  a  user's  attention  is  divided.  We  present  SmartRSVP,  an  attentive  speed-reading  system  to  facilitate  text  reading  on  small  screen  wearable  devices.  SmartRSVP  leverages  camera-based  visual  attention  tracking  as  a  play/pause  control  channel,  and  uses  implicit  physiological  signal  sensing  to  adjust  speed  in  real-time  to  make  text  reading  via  Rapid  Serial  Visual  Presentation  (RSVP)  more  enjoyable  and  practical  on  smart  watches.  Through  two  pilot  studies,  we  received  positive  feedback  on  the  visual  attention  controlling  feature  and  confirmed  the  feasibility  of  the  speed  adjusting  feature  of  SmartRSVP.  This  paper  reports  the  preliminary  results  of  the  studies.
2	Embodied  reading  a  multisensory  experience.  Reading  fiction  is  a  silent  activity,  where  readers  come  to  know  imaginary  worlds  and  characters  from  the  book's  pages.  However,  we  perceive  the  natural  world  with  more  than  our  eyes,  and  literature  should  be  no  different.  Thus,  an  embodied  reading  experience  is  proposed,  adding  sound  effects  and  haptic  feedback  to  allow  readers  to  listen  and  feel  the  narrative  text.  This  paper  presents  a  preliminary  prototype  for  multisensory  narratives  and  an  experimental  methodology  to  measure  embodiment  in  literature.  Results  for  the  subjective  assessment  of  immersion  and  user  experience  from  15  participants  in  three  modalities:  haptic,  sound,  both  combined  are  discussed.
2	I  see  you  there  developing  identity  preserving  embodied  interaction  for  museum  exhibits.  Museums  are  increasingly  embracing  technologies  that  provide  highly-individualized  and  highly-interactive  experiences  to  visitors.  With  embodied  interaction  experiences,  increased  localization  accuracy  supports  greater  nuance  in  interaction  design,  but  there  is  usually  a  tradeoff  between  fast,  accurate  tracking  and  the  ability  to  preserve  the  identity  of  users.  Customization  of  experience  relies  on  the  ability  to  detect  the  identity  of  visitors,  however.  We  present  a  method  that  combines  fine-grained  indoor  tracking  with  robust  preservation  of  the  unique  identities  of  multiple  users.  Our  model  merges  input  from  an  RFID  reader  with  input  from  a  commercial  camera-based  tracking  system.  We  developed  a  probabilistic  Bayesian  model  to  infer  at  run-time  the  correct  identification  of  the  subjects  in  the  camera's  field  of  view.  This  method,  tested  in  a  lab  and  at  a  local  museum,  requires  minimal  modification  to  the  exhibition  space,  while  addressing  several  identity-preservation  problems  for  which  many  indoor  tracking  systems  do  not  have  robust  solutions.
2	Technology  to  support  emergent  literacy  skills  in  young  children  with  visual  impairments.  Developing  emergent  literacy  skills  and  attitudes  within  children  with  visual  impairments  is  critical  to  cultivating  their  lifelong  ability  to  construct  concepts  about  the  function  of  symbols  and  develop  tactile  acuity.  We  offer  novel  HCI  research  on  the  factors  that  impact  parents'  abilities  to  create  tactile  pictures  to  meet  their  child's  unique  needs.  The  findings  presented  here  are  our  first  steps  towards  developing  technologies  and  interfaces  that  support  teachers  and  parents  in  easily  and  efficiently  creating  unique  and  replicable  tactile  graphics  for  children  with  visual  impairments.
2	Taking  data  exposure  into  account  how  does  it  affect  the  choice  of  sign  in  accounts.  Online  services  collect  personal  data  from  their  users,  sometimes  with  no  clear  need.  We  studied  how  users  sign-in  to  web  sites  using  federated  IDs,  and  found  that  most  survey  respondents  were  not  aware  of  the  data  they  expose.  However,  when  presented  with  the  tradeoffs  behind  each  sign-in  option,  respondents  reported  a  willingness  to  change  how  they  sign-in  to  reduce  their  data  exposure  or,  in  fewer  cases,  to  increase  it  to  receive  more  benefits  from  the  service.  Our  findings  suggest  that  data  exposure  is  a  concern  for  users,  and  that  there  is  a  need  for  finding  clearer  ways  for  communicating  it  for  each  sign-in  option.
2	Thumb  pen  interaction  on  tablets.  Modern  tablets  support  simultaneous  pen  and  touch  input,  but  it  remains  unclear  how  to  best  leverage  this  capability  for  bimanual  input  when  the  nonpreferred  hand  holds  the  tablet.  We  explore  Thumb  +  Pen  interactions  that  support  simultaneous  pen  and  touch  interaction,  with  both  hands,  in  such  situations.  Our  approach  engages  the  thumb  of  the  device-holding  hand,  such  that  the  thumb  interacts  with  the  touch  screen  in  an  indirect  manner,  thereby  complementing  the  direct  input  provided  by  the  preferred  hand.  For  instance,  the  thumb  can  determine  how  pen  actions  (articulated  with  the  opposite  hand)  are  interpreted.  Alternatively,  the  pen  can  point  at  an  object,  while  the  thumb  manipulates  one  or  more  of  its  parameters  through  indirect  touch.  Our  techniques  integrate  concepts  in  a  novel  way  that  derive  from  marking  menus,  spring-loaded  modes,  indirect  input,  and  multi-touch  conventions.  Our  overall  approach  takes  the  form  of  a  set  of  probes,  each  representing  a  meaningfully  distinct  class  of  application.  They  serve  as  an  initial  exploration  of  the  design  space  at  a  level  which  will  help  determine  the  feasibility  of  supporting  bimanual  interaction  in  such  contexts,  and  the  viability  of  the  Thumb  +  Pen  techniques  in  so  doing.
2	What  happens  after  disclosing  stigmatized  experiences  on  identified  social  media  individual  dyadic  and  social  network  outcomes.  Disclosing  stigmatized  experiences  or  identity  facets  on  identified  social  media  (e.g.,  Facebook)  can  be  risky,  inhibited,  yet  beneficial  for  the  discloser.  I  investigate  such  disclosures'  outcomes  when  they  do  happen  on  identified  social  media  as  perceived  by  the  individuals  who  perform  them.  I  draw  on  interviews  with  women  who  have  experienced  pregnancy  loss  and  are  social  media  users  in  the  U.S.  I  document  outcomes  at  the  social/network,  individual,  and  dyad  levels.  I  highlight  the  powerful  role  of  connecting  with  others  with  a  similar  experience  within  networks  of  known  ties,  how  disclosures  lead  to  relationship  changes,  how  disclosers  take  on  new  social  roles  as  mentors  and  support  sources,  and  how  helpful  connections  following  disclosures  originate  from  various  kinds  of  ties  via  diverse  communication  channels.  I  emphasize  reciprocal  disclosures  as  an  outcome  contributing  to  further  outcomes  (e.g.,  destigmatizing  pregnancy  loss).  I  provide  design  implications  related  to  facilitating  being  a  support  source  and  mentor,  helpful  reciprocal  disclosures,  and  finding  similar  others  within  networks  of  known  ties.
2	Refugees  hci  sig  situating  hci  within  humanitarian  research.  Currently,  the  United  Nations  High  Commissioner  for  refugees  estimates  that  there  are  around  65.8  million  forcibly  displaced  people  worldwide  [16].  As  digital  technologies  have  become  more  available,  humanitarian  researchers  and  organizations  have  begun  to  explore  how  technologies  may  be  used  to  address  refugee  needs  under  the  umbrella  of  Digital  Humanitarianism.  Interest  in  refugee  and  humanitarian  contexts  has  also  been  expressed  within  the  HCI  community  through  the  organization  of  workshops  at  conferences.  While  previous  engagements  within  the  HCI  community  have  focused  on  our  experiences  of  working  within  refugee  contexts  as  well  as  developing  a  common  research  agenda,  we  have  yet  to  explore  how  HCI  research  fits  within  wider  humanitarian  research  and  in  relation  to  digital  humanitarianism.  This  SIG  invites  HCI  researchers  to  engage  in  discussions  on  situating  HCI  research  within  humanitarian  research  and  response.
2	Using  time  and  space  efficiently  in  driverless  cars  findings  of  a  co  design  study.  The  alternative  use  of  travel  time  is  a  widely  discussed  benefits  of  driverless  cars.  We  therefore  conducted  14  co-design  sessions  to  examine  how  people  manage  their  time,  to  determine  how  they  perceive  the  value  of  time  in  driverless  cars  and  derive  design  implications.  Our  findings  suggest  that  driverless  mobility  will  affect  people's  use  of  travel  time  and  their  time  management  in  general.  The  participants  repeatedly  stated  the  desire  of  completing  tasks  while  traveling  to  save  time  for  activities  that  are  normally  neglected  in  everyday  life.  Using  travel  time  efficiently  requires  using  car  space  efficiently.  We  found  out  that  the  design  concept  of  tiny  houses  could  serve  as  common  design  pattern  to  deal  with  the  limited  space  within  cars  and  support  diverse  needs.
2	Mediate  a  spatial  tangible  interface  for  mixed  reality.  Recent  Virtual  Reality  (VR)  systems  render  highly  immersive  visual  experiences,  yet  currently  lack  tactile  feedback  for  feeling  virtual  objects  with  our  hands  and  bodies.  Shape  Displays  offer  solid  tangible  interaction  but  have  not  been  integrated  with  VR  or  have  been  restricted  to  desktop-scale  workspaces.  This  work  represents  a  fusion  of  mobile  robotics,  haptic  props,  and  shape-display  technology  and  commercial  Virtual  Reality  to  overcome  these  limitations.  We  present  Mediate,  a  semi-autonomous  mobile  shape-display  that  locally  renders  3D  physical  geometry  co-located  with  room-sized  virtual  environments  as  a  conceptual  step  towards  large-scale  tangible  interaction  in  Virtual  Reality.  We  compare  this  "dynamic  just-in-time  mockup"  concept  to  other  haptic  paradigms  and  discuss  future  applications  and  interaction  scenarios.
2	Evaluating  user  satisfaction  with  typography  designs  via  mining  touch  interaction  data  in  mobile  reading.  Previous  work  has  demonstrated  that  typography  design  has  a  great  influence  on  users'  reading  experience.  However,  current  typography  design  guidelines  are  mainly  for  general  purpose,  while  the  individual  needs  are  nearly  ignored.  To  achieve  personalized  typography  designs,  an  important  and  necessary  step  is  accurately  evaluating  user  satisfaction  with  the  typography  designs.  Current  evaluation  approaches,  e.g.,  asking  for  users'  opinions  directly,  however,  interrupt  the  reading  and  affect  users'  judgments.  In  this  paper,  we  propose  a  novel  method  to  address  this  challenge  by  mining  users'  implicit  feedbacks,  e.g.,  touch  interaction  data.  We  conduct  two  mobile  reading  studies  in  Chinese  to  collect  the  touch  interaction  data  from  91  participants.  We  propose  various  features  based  on  our  three  hypotheses  to  capture  meaningful  patterns  in  the  touch  behaviors.  The  experiment  results  show  the  effectiveness  of  our  evaluation  models  with  higher  accuracy  on  comparing  with  the  baseline  under  three  text  difficulty  levels,  respectively.
2	Parallel  web  browsing  in  tangible  augmented  reality  environments.  Parallel  browsing  is  the  behavior  of  concurrently  visiting  multiple  web  pages.  Modern  web  browsers  use  multiple  windows  and  tabs  to  support  different  parallel  browsing  users'  tasks.  Yet,  multiple  windows  and  tabs  have  significant  flaws  that  hinder  users'  performance.  Our  approach  attempts  to  support  parallel  browsing  with  an  Augmented  Reality  web  browsing  environment  that  mainly  relies  on  Tangible  Interaction.  To  verify  our  approach,  we  developed  a  prototype  and  carried  out  a  short  user-study.  In  the  short  user-study,  70%  of  participants  achieved  a  21.37%  decrease  in  the  time  required  to  complete  a  comparison  task  with  our  prototype.  In  addition,  we  found  further  advantages  in  flexibility  of  movement,  learning  time,  and  reduced  memory  load.  Finally,  we  provided  a  short  description  about  our  future  direction.
2	Chasing  luck  data  driven  prediction  faith  hunch  and  cultural  norms  in  rural  betting  practices.  HCI  research  predominantly  uses  scientific  rationality  to  explain  users’  behaviors,  decisions,  and  interactions  with  statistical  models  based  and  data-driven  systems.  However,  such  interactions  are  often  more  diverse  in  real  life,  and  may  straddle  beyond  scientific  and  economic  rationality.  Building  on  a  ten-month  ethnography  at  seven  Bangladeshi  villages,  we  explore  the  social  and  cultural  factors  that  influence  the  online  betting  practices  among  the  villagers.  We  describe  how  bets  harmonize  with  users’  faith,  hunch,  and  cultural  practices,  along  with  statistical  recommendations.  Drawing  on  a  rich  body  of  social  science  work  on  gambling,  we  contribute  to  the  HCI  scholarship  in  rationality,  justification,  and  postcolonial  computing.  Finally,  we  present  such  betting  as  an  under-appreciated  site  for  HCI  that  contradicts  with  the  ideological  hegemony  of  statistical  rationality,  and  recommend  a  smooth  integration  of  AI  system  with  the  “other”  rationalities  of  the  Global  South.
2	Braincode  electroencephalography  based  comprehension  detection  during  reading  and  listening.  The  pervasive  availability  of  media  in  foreign  languages  is  a  rich  resource  for  language  learning.  However,  learners  are  forced  to  interrupt  media  consumption  whenever  comprehension  problems  occur.  We  present  BrainCoDe,  a  method  to  implicitly  detect  vocabulary  gaps  through  the  evaluation  of  event-related  potentials  (ERPs).  In  a  user  study  (N=16),  we  evaluate  BrainCoDe  by  investigating  differences  in  ERP  amplitudes  during  listening  and  reading  of  known  words  compared  to  unknown  words.  We  found  significant  deviations  in  N400  amplitudes  during  reading  and  in  N100  amplitudes  during  listening  when  encountering  unknown  words.  To  evaluate  the  feasibility  of  ERPs  for  real-time  applications,  we  trained  a  classifier  that  detects  vocabulary  gaps  with  an  accuracy  of  87.13%  for  reading  and  82.64%  for  listening,  identifying  eight  out  of  ten  words  correctly  as  known  or  unknown.  We  show  the  potential  of  BrainCoDe  to  support  media  learning  through  instant  translations  or  by  generating  personalized  learning  content.
2	Designing  voice  interfaces  back  to  the  curriculum  basics.  Voice  user  interfaces  (VUIs)  are  rapidly  increasing  in  popularity  in  the  consumer  space.  This  leads  to  a  concurrent  explosion  of  available  applications  for  such  devices,  with  many  industries  rushing  to  offer  voice  interactions  for  their  products.  This  pressure  is  then  transferred  to  interface  designers;  however,  a  large  majority  of  designers  have  been  only  trained  to  handle  the  usability  challenges  specific  to  Graphical  User  Interfaces  (GUIs).  Since  VUIs  differ  significantly  in  design  and  usability  from  GUIs,  we  investigate  in  this  paper  the  extent  to  which  current  educational  resources  prepare  designers  to  handle  the  specific  challenges  of  VUI  design.  For  this,  we  conducted  a  preliminary  scoping  scan  and  syllabi  meta  review  of  HCI  curricula  at  more  than  twenty  top  international  HCI  departments,  revealing  that  the  current  offering  of  VUI  design  training  within  HCI  education  is  rather  limited.  Based  on  this,  we  advocate  for  the  updating  of  HCI  curricula  to  incorporate  VUI  design,  and  for  the  development  of  VUI-specific  pedagogical  artifacts  to  be  included  in  new  curricula.
2	Recog  supporting  blind  people  in  recognizing  personal  objects.  We  present  ReCog,  a  mobile  app  that  enables  blind  users  to  recognize  objects  by  training  a  deep  network  with  their  own  photos  of  such  objects.  This  functionality  is  useful  to  differentiate  personal  objects,  which  cannot  be  recognized  with  pre-trained  recognizers  and  may  lack  distinguishing  tactile  features.  To  ensure  that  the  objects  are  well-framed  in  the  captured  photos,  ReCog  integrates  a  camera-aiming  guidance  that  tracks  target  objects  and  instructs  the  user  through  verbal  and  sonification  feedback  to  appropriately  frame  them.  We  report  a  two-session  study  with  10  blind  participants  using  ReCog  for  object  training  and  recognition,  with  and  without  guidance.  We  show  that  ReCog  enables  blind  users  to  train  and  recognize  their  personal  objects,  and  that  camera-aiming  guidance  helps  novice  users  to  increase  their  confidence,  achieve  better  accuracy,  and  learn  strategies  to  capture  better  photos.
2	Design  study  lite  methodology  expediting  design  studies  and  enabling  the  synergy  of  visualization  pedagogy  and  social  good.  Design  studies  are  frequently  used  to  conduct  problem-driven  visualization  research  by  working  with  real-world  domain  experts.  In  visualization  pedagogy,  design  studies  are  often  introduced  but  rarely  practiced  due  to  their  large  time  requirements.  This  limits  students  to  a  classroom  curriculum,  often  involving  projects  that  may  not  have  implications  beyond  the  classroom.  Thus  we  present  the  Design  Study  "Lite"  Methodology,  a  novel  framework  for  implementing  design  studies  with  novice  students  in  14  weeks.  We  utilized  the  Design  Study  "Lite"  Methodology  in  conjunction  with  Service-Learning  to  teach  five  Data  Visualization  courses  and  demonstrate  that  it  benefits  not  only  the  students  but  also  the  community  through  service  to  non-profit  partners.  In  this  paper,  we  provide  a  detailed  breakdown  of  the  methodology  and  how  Service-Learning  can  be  incorporated  with  it.  We  also  include  an  extensive  reflection  on  the  methodology  and  provide  recommendations  for  future  applications  of  the  framework  for  teaching  visualization  courses  and  research.
2	Checklist  design  reconsidered  understanding  checklist  compliance  and  timing  of  interactions.  We  examine  the  association  between  user  interactions  with  a  checklist  and  task  performance  in  a  time-critical  medical  setting.  By  comparing  98  logs  from  a  digital  checklist  for  trauma  resuscitation  with  activity  logs  generated  by  video  review,  we  identified  three  non-compliant  checklist  use  behaviors:  failure  to  check  items  for  completed  tasks,  falsely  checking  items  when  tasks  were  not  performed,  and  inaccurately  checking  items  for  incomplete  tasks.  Using  video  review,  we  found  that  user  perceptions  of  task  completion  were  often  misaligned  with  clinical  practices  that  guided  activity  coding,  thereby  contributing  to  non-compliant  check-offs.  Our  analysis  of  associations  between  different  contexts  and  the  timing  of  check-offs  showed  longer  delays  when  (1)  checklist  users  were  absent  during  patient  arrival,  (2)  patients  had  penetrating  injuries,  and  (3)  resuscitations  were  assigned  to  the  highest  acuity.  We  discuss  opportunities  for  reconsidering  checklist  designs  to  reduce  non-compliant  checklist  use.
2	Pedagogical  agents  for  fostering  question  asking  skills  in  children.  Question  asking  is  an  important  tool  for  constructing  academic  knowledge,  and  a  self-reinforcing  driver  of  curiosity.  However,  research  has  found  that  question  asking  is  infrequent  in  the  classroom  and  children's  questions  are  often  superficial,  lacking  deep  reasoning.  In  this  work,  we  developed  a  pedagogical  agent  that  encourages  children  to  ask  divergent-thinking  questions,  a  more  complex  form  of  questions  that  is  associated  with  curiosity.  We  conducted  a  study  with  95  fifth  grade  students,  who  interacted  with  an  agent  that  encourages  either  convergent-thinking  or  divergent-thinking  questions.  Results  showed  that  both  interventions  increased  the  number  of  divergent-thinking  questions  and  the  fluency  of  question  asking,  while  they  did  not  significantly  alter  children's  perception  of  curiosity  despite  their  high  intrinsic  motivation  scores.  In  addition,  children's  curiosity  trait  has  a  mediating  effect  on  question  asking  under  the  divergent-thinking  agent,  suggesting  that  question-asking  interventions  must  be  personalized  to  each  student  based  on  their  tendency  to  be  curious.
2	Curiosity  notebook  a  platform  for  learning  by  teaching  conversational  agents.  Learning  by  teaching  is  an  established  pedagogical  technique;  however,  the  exact  process  through  which  learning  happens  remains  difficult  to  assess,  in  part  due  to  the  variability  in  the  tutor-tutee  pairing  and  interaction.  Prior  research  proposed  the  use  of  teachable  agents  acting  as  students,  in  order  to  facilitate  more  controlled  studies  of  the  learning  by  teaching  phenomenon.  In  this  work,  we  introduce  a  learning  by  teaching  platform,  Curiosity  Notebook,  which  allows  students  to  work  individually  or  in  groups  to  teach  a  conversational  agent  a  classification  task  in  a  variety  of  subject  topics.  We  conducted  a  4-week  exploratory  study  with  12  fourth  and  fifth  grade  elementary  school  children,  who  taught  a  conversational  robot  how  to  classify  animals,  rocks/minerals  and  paintings.  This  paper  outlines  the  architecture  of  our  system,  describes  the  lessons  learned  from  the  study,  and  contributes  design  considerations  on  how  to  design  conversational  agents  and  applications  for  learning  by  teaching  scenarios.
2	Optimizing  team  performance  when  resilience  falters  an  integrated  training  approach.  The  U.S.  Army  strives  to  provide  effective  training  for  its  soldiers.  Part  of  this  training  is  designed  to  build  resilience,  enabling  soldiers  and  leaders  to  optimize  personal  readiness  and  performance  in  environments  of  uncertainty  and  persistent  danger.  Training  complex  tasks  under  high  levels  of  stress  is  one  way  to  support  the  development  of  resilience;  another  way  is  to  train  individuals  in  the  use  of  specific  resilience-based  skills.  Soldiers  can  use  these  skills  not  only  to  benefit  their  functioning  but  also  to  benefit  the  functioning  of  their  teammates.  The  current  paper  reports  on  an  innovative  team-based  approach  to  resilience  training.  Both  the  training  content  and  training  method  provide  novel  approaches  to  addressing  resilience  in  the  context  of  high-stress  scenarios.  In  terms  of  content,  the  training  includes  specific  performance  enhancement  techniques  that  individuals  can  use  to  focus  attention  and  optimize  energy,  and  a  method  for  intervening  at  the  point-of-injury  if  a  teammate  experiences  an  acute  stress  reaction.  In  terms  of  method,  the  training  includes  classroom,  virtual  simulation,  and  live  training.  The  resultant  integrated  training  approach  is  Team  Overmatch.  This  training  milieu  allows  for  the  development,  implementation,  and  evaluation  of  training  modules  fully  embedded  into  tactical  training.  This  paper  discusses  how  innovative  resilience  strategies  are  integrated  in  a  larger  curriculum,  including  situational  awareness,  teamwork,  and  medical  care,  and  how  the  training  is  being  assessed  in  terms  of  knowledge  and  implementation.
2	Visual  analysis  of  habitat  suitability  index  model  for  predicting  the  locations  of  fishing  grounds.  In  this  study,  we  propose  a  novel  integrated  visualization  system  that  enables  interactive  visual  development  of  a  Habitat  Suitability  Index  (HSI)  model  for  predicting  the  locations  of  fishing  grounds.  Our  system  enables  the  interactive  selection  of  variables  that  are  highly  correlated  to  fish  catches  recorded  by  fishermen  on  their  vessels.  We  illustrate  the  use  of  this  system  using  a  real-world  simulation  of  the  Pacific  Ocean.  Vortex  structures,  such  as  those  used  for  fishing  grounds  exploration,  are  crucial  for  exploring  fishing  ground  because  they  can  drive  nutrients  from  the  ocean  floor  to  the  sea  surface.  The  ability  to  locate  fishing  grounds  therefore  depends  on  accurate  ocean  forecasting  systems  for  calculating  ocean  model  variables  such  as  temperature,  salinity,  velocity  etc.  Currently,  these  forecasts  are  based  on  multiple  spatio-temporal  simulations  that  produce  multidimensional  and  multivariate  results.  Critical  points  in  the  ocean  model  are  good  indicators  of  the  likelihood  of  finding  microscale  vortices,  and  our  visualization  approach  enables  the  interactive  exploration  and  analysis  of  these  critical  points.  The  proposed  system  makes  it  possible  to  analyze  the  vortex  structure  in  the  spatial  domain,  and  explore  fishing  grounds  on  the  sea  surface  in  detail.
2	Visual  analysis  of  dynamic  protein  cavities  and  binding  sites.  We  present  a  visual  analysis  application  for  protein  simulations  that  allows  researchers  to  investigate  dynamic  data  interactively.  Special  focus  lies  on  the  analysis  of  cavities  and  binding  sites,  which  play  a  critical  role  in  protein  function.  Cavities  are  extracted  and  classified.  The  surface  area  of  all  cavities  and  the  diameter  of  pockets  and  channels  is  computed  to  provide  evidence  for  their  accessibility.  These  values  are  also  plotted  in  2D  graphs  for  a  quantitative  time-dependent  analysis.  For  dynamic  simulation  data  sets,  the  cavities  are  tracked  to  show  their  stability  over  time.  The  user  is  provided  with  a  range  of  application-related  parameters  to  interactively  adjust  the  analysis  algorithms.  A  sequence  diagram  shows  the  structure  of  the  protein  and  additional  annotations  like  binding  sites.  Furthermore,  all  views  support  brushing  and  linking  for  consistent  selection  and  filtering.  All  algorithmic  steps  are  implemented  to  run  interactively  on  a  commodity  workstation.  As  a  result,  the  user  can  immediately  see  the  effect  of  a  parameter  change.  This  enables  the  real-time  analysis  of  a  running  simulation  for  in-situ  visualization.
2	Comparative  visualization  of  multiple  time  surfaces  by  planar  surface  reformation.  Comparing  time  surfaces  at  different  integration  time  points,  or  from  different  seeding  areas,  can  provide  valuable  insight  into  transport  phenomena  of  fluid  flows.  Such  a  comparative  study  is  challenging  due  to  the  often  convoluted  shapes  of  these  surfaces.  We  propose  a  new  approach  for  comparative  flow  visualization  based  on  time  surfaces,  which  exploits  the  idea  of  embedding  the  surfaces  in  a  carefully  designed,  reformed  2D  visualization  space.  Such  an  embedding  enables  new  opportunities  for  comparative  flow  visualization.  We  present  three  different  strategies  for  comparative  flow  visualization  that  take  advantage  of  the  reformation.  By  reforming  the  time  surfaces,  we  not  only  mitigate  occlusion  issues,  but  we  can  devote  also  the  third  dimension  of  the  visualization  space  to  the  comparative  aspects  of  the  visualization.  Our  approach  is  effective  in  a  variety  of  flow  study  cases.  The  direct  comparison  of  individual  time  surfaces  reveals  small  scale  differences  and  fine  details  about  the  fluid's  motion.  The  concurrent  study  of  multiple  surface  families  enables  the  identification  and  the  comparison  of  the  most  prominent  motion  patterns.  This  work  was  developed  in  close  collaboration  with  an  expert  in  fluid  dynamics,  who  assessed  the  potential  usefulness  of  this  approach  in  his  field.
2	K  core  based  multi  level  graph  visualization  for  scale  free  networks.  We  present  a  new  multi-level  graph  drawing  algorithm  based  on  the  k-core  coarsening,  a  well-known  cohesive  subgroup  analysis  method  in  social  network  analysis.  The  k-core  of  a  graph  is  also  known  as  the  degeneracy  in  graph  theory,  and  can  be  computed  in  linear  time.  Our  k-core  based  multi-level  algorithm  also  includes  a  new  concentric  circle  placement  and  a  variation  of  force-directed  layout  to  display  the  structure  of  graphs  effectively.  Experiments  with  real-world  networks  suggest  that  our  algorithm  performs  well  for  visualization  of  large  and  complex  scale-free  networks,  with  a  power-law  degree  distribution,  a  short  diameter  and  a  high  clustering  coefficient.  Comparison  with  other  multi-level  algorithms  shows  that  our  method  is  fast  and  effective,  in  particular  performs  better  than  Walshaw  [26]  and  FM3  [15].
2	Multidimensional  projection  with  radial  basis  function  and  control  points  selection.  Multidimensional  projection  techniques  provide  an  appealing  approach  for  multivariate  data  analysis,  for  their  ability  to  translate  high-dimensional  data  into  a  low-dimensional  representation  that  preserves  neighborhood  information.  In  recent  years,  pushed  by  the  ever  increasing  data  complexity  in  many  areas,  numerous  advances  in  such  techniques  have  been  observed,  primarily  in  terms  of  computational  efficiency  and  support  for  interactive  applications.  Both  these  achievements  were  made  possible  due  to  the  introduction  of  the  concept  of  control  points,  which  are  used  in  many  different  multidimensional  projection  techniques.  However,  little  attention  has  been  drawn  towards  the  process  of  control  points  selection.  In  this  work  we  propose  a  novel  multidimensional  projection  technique  based  on  radial  basis  functions  (RBF).  Our  method  uses  RBF  to  create  a  function  that  maps  the  data  into  a  low-dimensional  space  by  interpolating  the  previously  calculated  position  of  control  points.  We  also  present  a  built-in  method  for  the  control  points  selection  based  on  "forward-selection"  and  "Orthogonal  Least  Squares"  techniques.  We  demonstrate  that  the  proposed  selection  process  allows  our  technique  to  work  with  only  a  few  control  points  while  retaining  the  projection  quality  and  avoiding  redundant  control  points.
2	Optimal  sankey  diagrams  via  integer  programming.  We  present  the  first  practical  Integer  Linear  Programming  model  for  Sankey  Diagram  layout.  We  show  that  this  approach  is  viable  in  terms  of  running  time  for  reasonably  complex  diagrams  and  also  that  the  quality  of  the  layout  is  measurably  and  visibly  better  than  heuristic  approaches  in  terms  of  crossing  reduction.  Finally,  we  demonstrate  that  the  model  is  easily  extensible  through  the  addition  of  constraints,  such  as  arbitrary  grouping  of  nodes.
2	Can  we  apply  learning  analytics  tools  in  challenge  based  learning  contexts.  The  information  and  Communication  Technologies  changes  how  we  interact  with  others  and  with  the  information.  It  can  be  really  accessed  at  anytime  and  anywhere.  Future  professionals  should  be  ready  for  this  reality  which  requires  changes  in  traditional  teaching  and  learning  methods.  Challenge  Based  Learning  is  an  example  of  them.  This  method  poses  challenges  to  students  that  they  should  solve  employing  the  technology  they  use  during  their  daily  life.  The  evaluation  of  challenges  solutions  should  take  into  account  students’  final  outcomes  but  also  the  interactions  that  take  place  between  them.  This  could  be  very  hard  given  the  wide  choice  of  tools  that  students  can  apply.  Learning  analytics  tools  could  be  a  solution.
2	H  treasure  hunt  a  location  and  object  based  serious  game  for  cultural  heritage  learning  at  a  historic  site.  Serious  game  is  commonly  used  to  support  cultural  heritage  such  as  historical  teaching  and  learning,  and  enhancing  historic  site  visits.  Nowadays  most  of  in  situ  serious  games  have  been  supported  by  GPS  but  it  is  not  suitable  for  a  small-scale  historic  site.  In  this  paper,  we  propose  a  location  and  object-based  serious  game  application  H-Treasure  Hunt.  H-Treasure  hunt  integrates  location-based  service  with  object-based  sensors  to  find  more  exact  location  of  artifacts  at  a  historic  site.  In  the  game,  the  players  wear  Head  Mounted  Display  (HMD)  and  explore  a  historic  site  interacting  with  artifacts  to  complete  missions.  In  this  way,  H-Treasure  Hunt  will  act  as  a  tour  guide  helping  users  learn  about  the  historic  site  and  artifacts.  The  use  of  this  application  is  to  support  cultural  heritage  teaching  and  learning  as  well  as  enhancing  historical  site  visits.
2	Coupled  3d  reconstruction  of  sparse  facial  hair  and  skin.  Although  facial  hair  plays  an  important  role  in  individual  expression,  facial-hair  reconstruction  is  not  addressed  by  current  face-capture  systems.  Our  research  addresses  this  limitation  with  an  algorithm  that  treats  hair  and  skin  surface  capture  together  in  a  coupled  fashion  so  that  a  high-quality  representation  of  hair  fibers  as  well  as  the  underlying  skin  surface  can  be  reconstructed.  We  propose  a  passive,  camera-based  system  that  is  robust  against  arbitrary  motion  since  all  data  is  acquired  within  the  time  period  of  a  single  exposure.  Our  reconstruction  algorithm  detects  and  traces  hairs  in  the  captured  images  and  reconstructs  them  in  3D  using  a  multiview  stereo  approach.  Our  coupled  skin-reconstruction  algorithm  uses  information  about  the  detected  hairs  to  deliver  a  skin  surface  that  lies  underneath  all  hairs  irrespective  of  occlusions.  In  dense  regions  like  eyebrows,  we  employ  a  hair-synthesis  method  to  create  hair  fibers  that  plausibly  match  the  image  data.  We  demonstrate  our  scanning  system  on  a  number  of  individuals  and  show  that  it  can  successfully  reconstruct  a  variety  of  facial-hair  styles  together  with  the  underlying  skin  surface.
2	Video  enhanced  gigapixel  panoramas.  We  present  a  method  for  embedding  video  clips  within  gigapixel  scale  imagery.  The  combination  of  high-resolution  imagery  and  video  enables  users  to  pan  and  zoom  across  the  gigapixel  panorama  to  explore  complex  scenes  with  motion.  The  sparsity  of  the  video  content  within  the  gigapixel  context  introduces  several  challenges  which  we  overcome  by  optimizing  the  traversal  of  the  scene  coupled  with  appropriate  playback  of  the  embedded  video.  We  also  discuss  aligning  the  video  clips  both  geometrically  and  photometrically  to  reduce  visible  seams  between  the  dynamic  and  static  content.  Embedding  video  in  large  scale  panoramas  fills  a  gap  between  static  gigapixel  images  and  video  footage  and  thus  presents  a  new  interactive  medium.
2	Rakugacky  making  sounds  with  drawing.  Images  and  sounds  are  closely  related,  and  sounds  could  emphasis  or  change  impressions  of  images.  Thus  several  methods  to  synthesize  sounds  from  images  are  developed  [Hermann  et  al.  1999].  In  this  paper,  we  propose  a  novel  interactive  media  system  "RAKUGACKY"  that  can  make  sound  with  drawing.  In  this  system,  a  user  draws  a  picture  on  a  screen,  and  the  system  synthesizes  sounds  for  the  picture  automatically.  The  system  analyzes  the  picture  during  drawing,  and  synthesizes  or  changes  sounds  interactively  according  to  the  analyzing  result.
2	Ev  pen  an  electrovibration  haptic  feedback  pen  for  touchscreens.  This  paper  presents  an  Electrovibration  Pen  (EV-Pen)  which  incorporates  electrovibration  into  pen  interactions.  The  EV-Pen  not  only  exploits  the  advantages  of  conventional  pen-based  interactions  but  also  provides  a  wide  range  of  haptic  feedback  modes  without  any  form  of  mechanical  actuator.  The  EV-Pen  simulates  real  pen-on-paper  feeling,  enhancing  the  user  experience  in  drawing  and  handwriting,  and  providing  feedback  while  interacting  with  virtual  textures  and  GUI  elements  on  touchscreens.  Furthermore,  we  have  discussed  implications  of  the  EV-Pen  for  future  explorations.
2	Juke  cylinder  a  device  to  metamorphose  hands  to  a  musical  instrument.  If  you  knock  an  object,  it  sounds.  If  you  play  music,  loudspeakers  sound.  The  sound  you  usually  listen  to  is  generated  by  actions  or  objects  supposed  to  generate  sound.  However,  the  development  of  parametric  loudspeakers  made  it  possible  for  people  to  feel  that  the  sound  comes  from  actions  or  objects  which  are  not  supposed  to  generate  sound,  because  the  parametric  loudspeaker  can  localize  the  sound  image  on  the  reflected  surface[1][2].  The  parametric  speakers  work  in  an  entirely  different  way  from  conventional  loudspeakers.  They  generate  ultrasound,  and  it  travels  out  from  a  parametric  loudspeaker  in  a  narrowly  focused  column  like  a  flashlight  beam.  When  it  hits  something,  it  turns  back  into  ordinary  sound  you  can  hear.  There  is  a  computer  interface  using  this  characteristic  of  the  parametric  speaker[3].
2	Mechvr  interactive  vr  motion  simulation  of  mech  biped  robot.  MechVR  is  an  interactive  VR/Motion  simulator  that  offers  an  experience  of  driving  a  3D  simulated  robot  in  a  custom  training  world.  From  the  user's  perspective,  they  "enter"  the  cockpit  of  a  giant  motorized  biped,  and  experience  an  immersive,  interactive  virtual  reality  training  ride  as  they  drive  the  simulated  robotic  machine  themselves.  The  idea  is  not  unlike  a  flight  simulator,  but  for  an  imaginary,  giant  walking,  running,  and  flying  robot  machine.  Under  the  hood  is  a  custom  software  base  that  drives  state-of-the-art  motion  simulation  equipment  based  on  a  custom  user  interface.  Our  research  questions  include  the  control  and  physical  fidelity  of  the  virtual  robot,  and  the  transfer  of  the  experience  into  an  interactive  real-world  simulation  (hardware)  as  well  as  the  design  of  the  user  experience  for  this  and  similar  applications,  e.g.  prototyping  amusement  park  rides,  and/or  novel  arcade-style  motion  experiences.
2	Contact  preserving  shape  transfer  for  rigging  free  motion  retargeting.  Retargeting  a  motion  from  a  source  to  a  target  character  is  an  important  problem  in  computer  animation,  as  it  allows  to  reuse  existing  rigged  databases  or  transfer  motion  capture  to  virtual  characters.  Surface  based  pose  transfer  is  a  promising  approach  to  avoid  the  trial-and-error  process  when  controlling  the  joint  angles.  The  main  contribution  of  this  paper  is  to  investigate  whether  shape  transfer  instead  of  pose  transfer  would  better  preserve  the  original  contextual  meaning  of  the  source  pose.  To  this  end,  we  propose  an  optimization-based  method  to  deform  the  source  shape+pose  using  three  main  energy  functions:  similarity  to  the  target  shape,  body  part  volume  preservation,  and  collision  management  (preserve  existing  contacts  and  prevent  penetrations).  The  results  show  that  our  method  is  able  to  retarget  complex  poses,  including  several  contacts,  to  very  different  morphologies.  In  particular,  we  introduce  new  contacts  that  are  linked  to  the  change  in  morphology,  and  which  would  be  difficult  to  obtain  with  previous  works  based  on  pose  transfer  that  aim  at  distance  preservation  between  body  parts.  These  preliminary  results  are  encouraging  and  open  several  perspectives,  such  as  decreasing  computation  time,  and  better  understanding  how  to  model  pose  and  shape  constraints.
2	The  emergence  and  growth  of  evolutionary  art  1980  1993.  One  of  the  most  interesting--if  frustrating--aspects  of  charting  the  history  of  computer  art  is  trying  to  understand  the  intersections  of  specific  technologies  and  artistic  experimentation.  It  is  rarely  as  clear-cut  as  a  simple  linear  influence  of  one  to  the  other,  partly  because  artists  are  able  to  envision  all  kinds  of  possibilities  that  technology  might  enable  them  to  realize  in  some  kind  of  form,  but  as  they  do  so,  the  technology  is  itself  shaped,  especially  in  terms  of  how  it  is  perceived  by  others.  Do  artists  find  a  way  to  give  technologies  an  aesthetic  outlet,  or  do  some  technologies  possess--or  facilitate--a  characteristic  aesthetic  that  finds  its  expression  through  specific  artists?  Certainly,  in  the  history  of  computer  art  it  would  seem  that  particular  aesthetics,  technologies,  and  artists  are  closely  intertwined  in  certain  periods.  This  intertwining  of  art,  technology,  and  ideas  stolen  from  the  natural  world  has  never  been  so  arguably  merged  as  the  period  in  the  history  of  computer  art  from  1980  to  1993.  We  take  as  the  defining  start  of  this  period  the  initial  work  of  Mandelbrot  on  fractals  that  became  known  as  the  Mandelbrot  set  and  led  to  his  famous  illustrated  art-science  book  The  Fractal  Geometry  of  Nature.  In  1993,  this  first  highly  creative  period  in  evolutionary  computer  art  came  to  an  end  with  major  publications  by  pioneers  Karl  Sims,  Stephen  Todd,  and  William  Latham.
2	Polka  dot  the  garden  of  water  spirits.  Physical  and  tangible  representations  of  information  have  provided  users  with  intuitive  interactions  in  which  users  can  control  information  through  tangible  controls  using  their  hands.  In  these  techniques,  flexible  materials  have  often  been  utilized.  For  example,  clay  has  been  used  for  an  intuitive  modeling  tool  which  senses  the  shape  of  the  clay  and  updates  its  3D  model  data  [Piper  et  al.  2002].  This  example  enables  users  to  create  3D  models  without  knowledge  of  computational  methods  for  constructing  3D  models.  Though  these  tangible  representations  of  information  accept  user  input,  they  cannot  provide  bi-directional  physical  interactions  since  their  physical  properties  are  not  controlled  by  computers.  Therefore,  it  is  difficult  to  represent  dynamic  changes  of  information  using  physical  properties  such  as  motion,  size,  and  color.  These  tangible  user  interfaces  employ  static  materials.
2	Coded  lens  using  coded  aperture  for  low  cost  and  versatile  imaging.  We  propose  Coded  Lens,  a  novel  system  for  lensless  photography.  The  system  does  not  require  highly  calibrated  optics,  but  instead,  utilizes  a  coded  aperture  for  guiding  lights.  Compressed  sensing  (CS)  is  used  to  reconstruct  scene  from  the  raw  image  obtained  through  the  coded  aperture.  Experimenting  with  synthetic  and  real  scenes,  we  show  the  applicability  of  the  technique  and  also  demonstrate  additional  functionality  such  as  changing  focus  programmatically.  We  believe  this  will  lead  to  a  more  compact,  cheaper  and  even  versatile  imaging  systems.
2	Real  time  dynamic  fracture  with  volumetric  approximate  convex  decompositions.  We  propose  a  new  fast,  robust  and  controllable  method  to  simulate  the  dynamic  destruction  of  large  and  complex  objects  in  real  time.  The  common  method  for  fracture  simulation  in  computer  games  is  to  pre-fracture  models  and  replace  objects  by  their  pre-computed  parts  at  run-time.  This  popular  method  is  computationally  cheap  but  has  the  disadvantages  that  the  fracture  pattern  does  not  align  with  the  impact  location  and  that  the  number  of  hierarchical  fracture  levels  is  fixed.  Our  method  allows  dynamic  fracturing  of  large  objects  into  an  unlimited  number  of  pieces  fast  enough  to  be  used  in  computer  games.  We  represent  visual  meshes  by  volumetric  approximate  convex  decompositions  (VACD)  and  apply  user-defined  fracture  patterns  dependent  on  the  impact  location.  The  method  supports  partial  fracturing  meaning  that  fracture  patterns  can  be  applied  locally  at  multiple  locations  of  an  object.  We  propose  new  methods  for  computing  a  VACD,  for  approximate  convex  hull  construction  and  for  detecting  islands  in  the  convex  decomposition  after  partial  destruction  in  order  to  determine  support  structures.
2	Moana  performing  water.  For  Disney's  Moana,  water  was  a  dominant  part  of  island  life,  in  fact  it  had  a  life  of  itfis  own.  Presenting  itself  as  a  character,  water  was  ever  present,  in  a  multitude  of  shapes  and  scales.  An  end-to-end  water  pipeline  was  developed  for  this  film  [Garcia  et  al.  2016],  including  the  creation  of  proprietary  fluid  APIC  solver  [Jiang  et  al.  2015]  named  Splash.  This  gave  us  physically  accurate  simulations.  The  challenge  with  performing  water  was  to  provide  art-directed  simulations,  defying  physics,  yet  remaining  in  a  grounded  sense  of  possibility.  Incorporating  natural  swells  and  flows  to  support  the  building  of  designed  shapes  limited  anthropomorphic  features,  and  played  to  our  goal  of  communicating  that  this  character  is  the  ocean  as  a  whole.
2	Blue  noise  point  sampling  using  kernel  density  model.  Stochastic  point  distributions  with  blue-noise  spectrum  are  used  extensively  in  computer  graphics  for  various  applications  such  as  avoiding  aliasing  artifacts  in  ray  tracing,  halftoning,  stippling,  etc.  In  this  paper  we  present  a  new  approach  for  generating  point  sets  with  high-quality  blue  noise  properties  that  formulates  the  problem  using  a  statistical  mechanics  interacting  particle  model.  Points  distributions  are  generated  by  sampling  this  model.  This  new  formulation  of  the  problem  unifies  randomness  with  the  requirement  for  equidistant  point  spacing,  responsible  for  the  enhanced  blue  noise  spectral  properties.  We  derive  a  highly  efficient  multi-scale  sampling  scheme  for  drawing  random  point  distributions  from  this  model.  The  new  scheme  avoids  the  critical  slowing  down  phenomena  that  plagues  this  type  of  models.  This  derivation  is  accompanied  by  a  model-specific  analysis.      Altogether,  our  approach  generates  high-quality  point  distributions,  supports  spatially-varying  spatial  point  density,  and  runs  in  time  that  is  linear  in  the  number  of  points  generated.
2	Grab  carry  release  manipulating  physical  objects  in  a  real  scene  through  a  smart  phone.  Individuals  often  discuss  how  to  configure  their  real  world  space,  such  as  switching  the  location  of  an  object,  designing  furniture  layouts  or  room  decorations,  shopping  for  suitable  house  appliances,  or  even  commanding  a  robot  to  perform  certain  tasks.  However,  verbal  communication  is  often  insufficient  to  convey  the  imagined  results  for  such  spatial  arrangements.  Users  must  occasionally  spend  considerable  efforts  in  either  moving  the  real  objects  physically  or  using  photos  or  videos  to  composite  the  mockup  to  simulate  the  layout.  Nevertheless,  those  methods  are  too  tedious  for  real-time  communication.
2	Enchanted  scissors  a  scissor  interface  for  support  in  cutting  and  interactive  fabrication.  We  present  an  approach  to  support  basic  and  complex  cutting  processes  through  an  interactive  fabrication  experience  [Willis  et  al.  2011].  Our  system,  enchanted  scissors,  is  a  digitally  controlled  pair  of  scissors  (Figure  1).  It  restricts  areas  that  can  be  cut  while  requiring  the  user's  exertion  of  force  and  decision  to  execute  each  cut.  Therefore,  unlike  a  completely  digitalized  cutting  device,  the  user  can  freely  apply  improvisations  within  the  permitted  areas  in  real-time.  A  pair  of  scissors  is  a  common  tool  seen  and  used  in  everyday  life;  the  user  can  instantly  recognize  its  operation  method.  It  has  varieties  of  usage  from  opening  a  letter  to  creating  a  complicated  paper  craft.  While  using  scissors,  it  is  common  to  cut  unintended  parts  or  difficult  to  control  the  blades  for  cutting  intricate  details.  enchanted  scissors  prevents  these  errors  in  advance  by  using  two  switchable  programs  to  restrict  the  areas  that  can  be  cut.  Both  programs  provide  real-time  feedback  to  the  user  during  the  cutting  process  as  regular  scissors  would.  This  allows  a  comfortable  connection  of  the  user's  physical  input  and  the  output  implemented  by  the  device.
2	Printone  interactive  resonance  simulation  for  free  form  print  wind  instrument  design.  This  paper  presents  an  interactive  design  interface  for  three-dimensional  free-form  musical  wind  instruments.  The  sound  of  a  wind  instrument  is  governed  by  the  acoustic  resonance  as  a  result  of  complicated  interactions  of  sound  waves  and  internal  geometries  of  the  instrument.  Thus,  creating  an  original  free-form  wind  instrument  by  manual  methods  is  a  challenging  problem.  Our  interface  provides  interactive  sound  simulation  feedback  as  the  user  edits,  allowing  exploration  of  original  wind  instrument  designs.  Sound  simulation  of  a  3D  wind  musical  instrument  is  known  to  be  computationally  expensive.  To  overcome  this  problem,  we  first  model  the  wind  instruments  as  a  passive  resonator,  where  we  ignore  coupled  oscillation  excitation  from  the  mouthpiece.  Then  we  present  a  novel  efficient  method  to  estimate  the  resonance  frequency  based  on  the  boundary  element  method  by  formulating  the  resonance  problem  as  a  minimum  eigenvalue  problem.  Furthermore,  we  can  efficiently  compute  an  approximate  resonance  frequency  using  a  new  technique  based  on  a  generalized  eigenvalue  problem.  The  designs  can  be  fabricated  using  a  3D  printer,  thus  we  call  the  results  "print-wind  instruments"  in  association  with  woodwind  instruments.  We  demonstrate  our  approach  with  examples  of  unconventional  shapes  performing  familiar  songs.
2	Towards  a  general  architecture  for  a  co  learning  of  brain  computer  interfaces.  In  this  article  we  propose  a  software  architecture  for  asynchronous  BCIs  based  on  co-learning,  where  both  the  system  and  the  user  jointly  learn  by  providing  feedback  to  one  another.  We  propose  the  use  of  recent  filtering  techniques  such  as  Riemann  Geometry  and  ICA  followed  by  multiple  classifications,  by  both  incremental  supervised  classifiers  and  minimally  supervised  classifiers.  The  classifier  outputs  are  then  combined  adaptively  according  to  the  feedback  using  recursive  neural  networks.
2	Using  brain  connectivity  measure  of  eeg  synchrostates  for  discriminating  typical  and  autism  spectrum  disorder.  In  this  paper  we  utilized  the  concept  of  stable  phase  synchronization  topography  -  synchrostates  -  over  the  scalp  derived  from  EEG  recording  for  formulating  brain  connectivity  network  in  Autism  Spectrum  Disorder  (ASD)  and  typically-growing  children.  A  synchronization  index  is  adapted  for  forming  the  edges  of  the  connectivity  graph  capturing  the  stability  of  each  of  the  synchrostates.  Such  network  is  formed  for  11  ASD  and  12  control  group  children.  Comparative  analyses  of  these  networks  using  graph  theoretic  measures  show  that  children  with  autism  have  a  different  modularity  of  such  networks  from  typical  children.  This  result  could  pave  the  way  to  a  new  modality  for  possible  identification  of  ASD  from  non-invasively  recorded  EEG  data.
2	Charge  steering  in  a  novel  dbs  electrode  may  accommodate  surgical  targeting  errors.  Deep  brain  stimulation  (DBS)  alleviates  the  symptoms  of  some  neurological  disorders  by  stimulating  specific  neural  targets.  If  electrodes  are  placed  off  target,  DBS  amplitude  must  be  decreased  to  avoid  side  effects  induced  by  stimulating  other  neural  regions.  We  present  a  novel  electrode  geometry  capable  of  radial  charge  steering.  Computational  modeling  results  show  that  radial  charge  steering  will  allow  this  new  electrode  to  constrain  stimulation  within  target  boundaries,  despite  surgical  placement  error,  thus  reducing  side-effects  from  over  stimulation.
2	Modular  flux  transfer  efficient  rendering  of  high  resolution  volumes  with  repeated  structures.  The  highest  fidelity  images  to  date  of  complex  materials  like  cloth  use  extremely  high-resolution  volumetric  models.  However,  rendering  such  complex  volumetric  media  is  expensive,  with  brute-force  path  tracing  often  the  only  viable  solution.  Fortunately,  common  volumetric  materials  (fabrics,  finished  wood,  synthesized  solid  textures)  are  structured,  with  repeated  patterns  approximated  by  tiling  a  small  number  of  exemplar  blocks.  In  this  paper,  we  introduce  a  precomputation-based  rendering  approach  for  such  volumetric  media  with  repeated  structures  based  on  a  modular  transfer  formulation.  We  model  each  exemplar  block  as  a  voxel  grid  and  precompute  voxel-to-voxel,  patch-to-patch,  and  patch-to-voxel  flux  transfer  matrices.  At  render  time,  when  blocks  are  tiled  to  produce  a  high-resolution  volume,  we  accurately  compute  low-order  scattering,  with  modular  flux  transfer  used  to  approximate  higher-order  scattering.  We  achieve  speedups  of  up  to  12×  over  path  tracing  on  extremely  complex  volumes,  with  minimal  loss  of  quality.  In  addition,  we  demonstrate  that  our  approach  outperforms  photon  mapping  on  these  materials.
2	Pyramid  of  arclength  descriptor  for  generating  collage  of  shapes.  This  paper  tackles  a  challenging  2D  collage  generation  problem,  focusing  on  shapes:  we  aim  to  fill  a  given  region  by  packing  irregular  and  reasonably-sized  shapes  with  minimized  gaps  and  overlaps.  To  achieve  this  nontrivial  problem,  we  first  have  to  analyze  the  boundary  of  individual  shapes  and  then  couple  the  shapes  with  partially-matched  boundary  to  reduce  gaps  and  overlaps  in  the  collages.  Second,  the  search  space  in  identifying  a  good  coupling  of  shapes  is  highly  enormous,  since  arranging  a  shape  in  a  collage  involves  a  position,  an  orientation,  and  a  scale  factor.  Yet,  this  matching  step  needs  to  be  performed  for  every  single  shape  when  we  pack  it  into  a  collage.  Existing  shape  descriptors  are  simply  infeasible  for  computation  in  a  reasonable  amount  of  time.  To  overcome  this,  we  present  a  brand  new,  scale-  and  rotation-invariant  2D  shape  descriptor,  namely  pyramid  of  arclength  descriptor  (PAD).  Its  formulation  is  locally  supported,  scalable,  and  yet  simple  to  construct  and  compute.  These  properties  make  PAD  efficient  for  performing  the  partial-shape  matching.  Hence,  we  can  prune  away  most  search  space  with  simple  calculation,  and  efficiently  identify  candidate  shapes.  We  evaluate  our  method  using  a  large  variety  of  shapes  with  different  types  and  contours.  Convincing  collage  results  in  terms  of  visual  quality  and  time  performance  are  obtained.
2	Expression  flow  for  3d  aware  face  component  transfer.  We  address  the  problem  of  correcting  an  undesirable  expression  on  a  face  photo  by  transferring  local  facial  components,  such  as  a  smiling  mouth,  from  another  face  photo  of  the  same  person  which  has  the  desired  expression.  Direct  copying  and  blending  using  existing  compositing  tools  results  in  semantically  unnatural  composites,  since  expression  is  a  global  effect  and  the  local  component  in  one  expression  is  often  incompatible  with  the  shape  and  other  components  of  the  face  in  another  expression.  To  solve  this  problem  we  present  Expression  Flow,  a  2D  flow  field  which  can  warp  the  target  face  globally  in  a  natural  way,  so  that  the  warped  face  is  compatible  with  the  new  facial  component  to  be  copied  over.  To  do  this,  starting  with  the  two  input  face  photos,  we  jointly  construct  a  pair  of  3D  face  shapes  with  the  same  identity  but  different  expressions.  The  expression  flow  is  computed  by  projecting  the  difference  between  the  two  3D  shapes  back  to  2D.  It  describes  how  to  warp  the  target  face  photo  to  match  the  expression  of  the  reference  photo.  User  studies  suggest  that  our  system  is  able  to  generate  face  composites  with  much  higher  fidelity  than  existing  methods.
2	Gum  gum  shooting  inducing  a  sense  of  arm  elongation  via  forearm  skin  stretch  and  the  change  in  the  center  of  gravity.  Many  people  sometimes  imagine  if  they  can  wield  superhuman  abilities  like  that  appear  in  games  and  animation.  Among  these  abilities,  we  focused  particularly  on  representing  the  experience  of  arm  stretching  beyond  the  limits  of  the  human  body.  We  proposed  a  method  for  inducing  a  sense  of  arm  stretching  by  designing  the  device  attached  to  forearm  and  giving  the  user  a  visual  cue  by  changing  the  body  structure  of  the  user's  avatar  in  the  virtual  environment.  Our  device  shifts  the  mass  from  the  elbow  to  the  wrist  while  stretching  the  skin  of  the  forearm  according  to  the  animation  in  the  virtual  environment.  The  sensation  of  the  elongation  of  the  arm  skin  as  well  as  the  change  in  the  weight  of  arm  is  thought  to  be  the  feeling  when  the  arms  are  stretched  out.  As  a  result,  we  introduce  these  two  mechanisms  into  our  device,  which  allows  the  user  to  feel  the  sense  of  arm  stretching.
2	Feature  extraction  from  the  hermitian  manifold  for  brain  computer  interfaces.  Riemannian  geometry-based  methods  have  shown  to  be  effective  in  many  sorts  of  Brain-Computer  Interface  (BCI)  applications,  but  are  only  capable  of  measuring  the  power  of  the  measured  signal.  This  paper  proposes  a  set  of  novel  features  derived  via  the  Hilbert  transform  and  applies  them  to  the  generalized  Riemannian  manifold,  the  Hermitian  manifold,  to  see  whether  the  classification  accuracy  benefits  from  this  treatment.  To  validate  these  features,  we  benchmark  them  with  the  Mother  of  All  BCI  Benchmarks  framework,  a  recently  introduced  tool  to  make  BCI  methods  research  more  reproducible.  The  results  indicate  that  in  some  settings  the  analytic  covariance  matrix  can  improve  BCI  performance.
2	To  what  extent  can  retinal  prostheses  restore  vision.  After  a  general  consideration  of  the  various  approaches  to  electrical  stimulation  of  the  retina,  a  thorough  in  vitro  investigation  of  retinal  responses  to  voltage-controlled  stimuli  is  discussed  within  the  context  of  the  Alpha  IMS  subretinal  implant  (Retina  Implant  AG,  Reutlingen,  Germany).  This  is  supplemented  by  a  clinical  trial  interim  report  describing  results  obtained  in  29  patients  blind  from  retinitis  pigmentosa  who  have  received  the  Alpha  IMS  implant.  It  is  concluded  that  the  surgical  procedure  is  safe  and  blind  patients  can  benefit  in  visual  tasks  of  daily  life  with  this  device  that  has  meanwhile  received  approval  for  commercial  use  in  Europe.
2	The  effect  of  focused  ultrasonic  stimulation  on  the  activity  of  hippocampal  neurons  in  multi  channel  electrode.  Many  experiments  investigated  the  neuro  modulatory  effects  of  ultrasonic  stimulation,  yet  there  are  few  researches  that  investigate  which  parameters  are  needed  to  change  activity  of  excitable  cells.  In  this  study,  changes  in  neural  network  activities  were  recorded  during  ultrasonic  stimulation  with  various  parameters  using  a  multi-electrode  array  (MEA).  This  study  concludes,  ultrasonic  stimulation  with  intensity  of  16.14  mW/cm2  induces  change  to  the  neuronal  network  activity.  However,  the  timing  of  neuronal  activity  and  stimulus  was  not  synchronized.  The  mechanical  stimuli  may  not  have  a  direct  effect  on  neuronal  activities  but  somehow  help  neuronal  cells  to  be  in  a  more  excitable  state,  although  further  study  is  necessary.
2	Steady  state  visual  evoked  potential  based  classification  system  for  detecting  migraine  seizures.  The  recurrent  migraine  attacks  between  interictal  phenomenon  is  triggered  by  the  migraineurs'  brain  lacking  for  habituation,  due  to  the  stimulations  from  the  outside  world  that  increase  the  excitability  of  brain  activity,  which  have  been  considered  as  the  possible  reasons  for  migraine  seizure.  The  variation  of  habituation  level  within  the  migraine  cycle  is  proposed  to  be  a  critical  symptom  to  describe  the  physiological  states  of  migraine  headache.  This  study  proposed  Steady-State  Visual  Evoked  Potentials  (SSVEP)  examination  to  utilize  habituation  for  classifying  the  different  physiologic  states  of  migraine  cycle,  and  implement  a  classification  system  to  determine  different  migraine  states.  The  developed  system  may  be  extended  to  detect  migraine  seizure,  and  provide  an  opportunity  to  a  clinically  individual-based  headache  monitoring  program,  aiming  for  early  migraine  detection.
2	Analysis  of  phase  coding  ssvep  based  on  canonical  correlation  analysis  cca.  Steady-state  visual  evoked  potential  (SSVEP)  has  been  widely  applied  in  brain  computer  interface  (BCI)  systems.  The  amplitude  and  phase  features  of  SSVEP  were  commonly  extracted  by  Fourier  analysis  method  from  single-channel  EEG  data.  In  the  multichannel  case,  canonical  correlation  analysis  (CCA)  has  been  utilized  for  the  analysis  of  frequency  coding  SSVEP.  This  paper  presents  the  analysis  of  phase  coding  SSVEP  using  CCA.  The  phase  coding  scheme  consists  of  six  targets  flickering  at  10Hz,  with  a  60°  phase  difference  between  any  two  sequential  targets.  For  each  target,  20  trials  of  8s  EEG  signal  were  acquired.  Using  CCA,  we  achieve  channel  selection  and  extraction  of  phase  features;  a  classification  accuracy  of  above  80%  is  obtained,  with  the  length  of  the  time  window  up  to  4s.  The  results  demonstrate  that  phase  coding  SSVEP  analysis  based  on  CCA  is  feasible.
2	Quicktumornet  fast  automatic  multi  class  segmentation  of  brain  tumors.  Non-invasive  techniques  such  as  magnetic  resonance  imaging  (MRI)  are  widely  employed  in  brain  tumor  diagnostics.  However,  manual  segmentation  of  brain  tumors  from  3D  MRI  volumes  is  a  time-consuming  task  that  requires  trained  expert  radiologists.  Due  to  the  subjectivity  of  manual  segmentation,  there  is  low  inter-rater  reliability  which  can  result  in  diagnostic  discrepancies.  As  the  success  of  many  brain  tumor  treatments  depends  on  early  intervention,  early  detection  is  paramount.  In  this  context,  a  fully  automated  segmentation  method  for  brain  tumor  segmentation  is  necessary  as  an  efficient  and  reliable  method  for  brain  tumor  detection  and  quantification.  In  this  study,  we  propose  an  end-to-end  approach  for  brain  tumor  segmentation,  capitalizing  on  a  modified  version  of  QuickNAT,  a  brain  tissue  type  segmentation  deep  convolutional  neural  network  (CNN).  Our  method  was  evaluated  on  a  data  set  of  233  patient's  T1  weighted  images  containing  three  tumor  type  classes  annotated  (meningioma,  glioma,  and  pituitary).  Our  model,  QuickTumorNet,  demonstrated  fast,  reliable,  and  accurate  brain  tumor  segmentation  that  can  be  utilized  to  assist  clinicians  in  diagnosis  and  treatment.
2	Occlusion  culling  in  alan  wake.  The  combination  of  large  outdoor  environments  and  dynamic,  shadow  casting  light  sources  posed  a  rendering  performance  challenge  Remedy  had  to  tackle  during  the  production  of  Alan  Wake.
2	Seeing  around  corners  with  a  mobile  phone  synthetic  aperture  audio  imaging.  Seeing  around  corners,  in  the  dark,  and  through  smoke  is  difficult  without  specialized  sensors[Velten  et  al.  2012],  and  so  far  impossible  with  a  mobile  phone.  We  use  an  active  audio  system  to  sense  objects  around  occluders.  Current  techniques  perform  passive  localization  of  sound  sources  with  a  microphone  array,  however,  we  demonstrate  that  with  one  microphone  and  one  speaker  pair,  such  as  the  ones  found  in  mobile  phones,  it  is  possible  to  sense  the  specular  reflection  of  silent  objects  such  as  mannequins  around  occluding  objects.  We  demonstrate  this  technique  by  sensing  a  mannequin  occluded  by  a  wall.
2	Perceptually  based  downscaling  of  images.  We  propose  a  perceptually  based  method  for  downscaling  images  that  provides  a  better  apparent  depiction  of  the  input  image.  We  formulate  image  downscaling  as  an  optimization  problem  where  the  difference  between  the  input  and  output  images  is  measured  using  a  widely  adopted  perceptual  image  quality  metric.  The  downscaled  images  retain  perceptually  important  features  and  details,  resulting  in  an  accurate  and  spatio-temporally  consistent  representation  of  the  high  resolution  input.  We  derive  the  solution  of  the  optimization  problem  in  closed-form,  which  leads  to  a  simple,  efficient  and  parallelizable  implementation  with  sums  and  convolutions.  The  algorithm  has  running  times  similar  to  linear  filtering  and  is  orders  of  magnitude  faster  than  the  state-of-the-art  for  image  downscaling.  We  validate  the  effectiveness  of  the  technique  with  extensive  tests  on  many  images,  video,  and  by  performing  a  user  study,  which  indicates  a  clear  preference  for  the  results  of  the  new  algorithm.
2	Haptic  interaction  with  virtual  geometry  on  robotic  touch  surface.  Touch  screens  are  rapidly  penetrating  our  daily  life.  Even  general  users  are  enjoying  various  applications  such  as  drawing  sketches  and  playing  games  with  their  mobile  phones.  In  particular,  the  improvement  of  graphics  capabilities  with  large  display  screens  now  allows  easy  access  at  any  location  to  3D  environments  including  3D  games,  animation,  and  augmented  reality  applications  that  were  formerly  limited  to  desktop  environments.  Touch  screen  devices  have  distinct  features  compared  with  previous  mobile  phones:  the  screen  is  widened  and  the  device  is  mainly  operated  with  touch  input.  As  a  natural  consequence,  users  want  to  be  able  to  feel  the  touched  objects  in  many  cases.  From  the  viewpoint  of  haptics  on  surfaces,  a  variety  of  haptic  rendering/interaction  techniques  have  been  proposed  to  make  interactions  on  surface  richer  and  more  natural.  One  approach  to  lend  applications  tactility  is  to  provide  frictional  feedback  on  touch  surfaces  [Bau  and  Poupyrev  2012;  Kim  et  al.  2013;  Winfield  et  al.  2007].  These  technologies  plausibly  reproduce  finetuned  textures,  such  as  craters  on  the  surface  of  the  Moon  and  the  lines  on  the  palm  of  a  hand.  However,  the  technologies  are  not  suitable  to  reproduce  the  object  surface  with  contours  that  extend  beyond  the  fingertip,  such  as  the  large-scaled  geometry  of  the  Moon  and  the  palm.  To  handle  such  issues,  a  recent  work  proposed  a  lateral  haptic  display  that  can  provide  two-dimensional  force  feedback  slightly  above  the  screen  [Saga  and  Deguchi  2012].  They  exploited  the  phenomenon  that  people  tend  to  have  the  perception  of  touching  3D  objects  if  they  receive  force  feedback  in/against  the  direction  of  movement  on  a  2D  surface.  A  similar  technique  was  applied  to  image-based  haptic  interaction  in  another  recent  work  [Kim  and  Kwon  2013].  This  type  of  feeling  can  be  reproduced  by  considering  one  important  feature  of  haptics:  collocation,  which  is  often  ignored  in  the  context  of  surface  interaction  in  that  interaction  space  is  limited  to  a  2D  surface.  This  is  a  substantially  different  setup  from  that  used  for  conventional  haptic  rendering,  which  generally  collocates  haptic  information  in  the  real  interaction  space.  A  recent  study  dealt  with  this  issue  by  proposing  a  1-dimensional  translational  robot  system  [Sinclair  et  al.  2013].  Their  robotic  touch  display  allows  users  to  explore  virtual  3D  content  such  as  volumetric  medical  images  by  moving  the  display  surface  along  the  z-axis  perpendicular  to  the  display  surface.  This  trial  has  significant  meaning  as  the  first  work  to  take  collocation  into  consideration  in  the  context  of  surface  haptics.  Another  recent  study  addressed  this  issue  by  proposing  a  haptic  stylus  with  a  variable  tip  length.  The  proposed  system  is  designed  to  give  users  the  illusion  that  some  part  of  the  stylus  is  immersed  in  virtual  space,  enabling  the  direct  touch  of  virtual  objects  displayed  on  the  flat  surface  [Withana  et  al.  2010].
2	An  anatomically  constrained  local  deformation  model  for  monocular  face  capture.  We  present  a  new  anatomically-constrained  local  face  model  and  fitting  approach  for  tracking  3D  faces  from  2D  motion  data  in  very  high  quality.  In  contrast  to  traditional  global  face  models,  often  built  from  a  large  set  of  blendshapes,  we  propose  a  local  deformation  model  composed  of  many  small  subspaces  spatially  distributed  over  the  face.  Our  local  model  offers  far  more  flexibility  and  expressiveness  than  global  blendshape  models,  even  with  a  much  smaller  model  size.  This  flexibility  would  typically  come  at  the  cost  of  reduced  robustness,  in  particular  during  the  under-constrained  task  of  monocular  reconstruction.  However,  a  key  contribution  of  this  work  is  that  we  consider  the  face  anatomy  and  introduce  subspace  skin  thickness  constraints  into  our  model,  which  constrain  the  face  to  only  valid  expressions  and  helps  counteract  depth  ambiguities  in  monocular  tracking.  Given  our  new  model,  we  present  a  novel  fitting  optimization  that  allows  3D  facial  performance  reconstruction  from  a  single  view  at  extremely  high  quality,  far  beyond  previous  fitting  approaches.  Our  model  is  flexible,  and  can  be  applied  also  when  only  sparse  motion  data  is  available,  for  example  with  marker-based  motion  capture  or  even  face  posing  from  artistic  sketches.  Furthermore,  by  incorporating  anatomical  constraints  we  can  automatically  estimate  the  rigid  motion  of  the  skull,  obtaining  a  rigid  stabilization  of  the  performance  for  free.  We  demonstrate  our  model  and  single-view  fitting  method  on  a  number  of  examples,  including,  for  the  first  time,  extreme  local  skin  deformation  caused  by  external  forces  such  as  wind,  captured  from  a  single  high-speed  camera.
2	A  compressive  light  field  projection  system.  For  about  a  century,  researchers  and  experimentalists  have  strived  to  bring  glasses-free  3D  experiences  to  the  big  screen.  Much  progress  has  been  made  and  light  field  projection  systems  are  now  commercially  available.  Unfortunately,  available  display  systems  usually  employ  dozens  of  devices  making  such  setups  costly,  energy  inefficient,  and  bulky.  We  present  a  compressive  approach  to  light  field  synthesis  with  projection  devices.  For  this  purpose,  we  propose  a  novel,  passive  screen  design  that  is  inspired  by  angle-expanding  Keplerian  telescopes.  Combined  with  high-speed  light  field  projection  and  nonnegative  light  field  factorization,  we  demonstrate  that  compressive  light  field  projection  is  possible  with  a  single  device.  We  build  a  prototype  light  field  projector  and  angle-expanding  screen  from  scratch,  evaluate  the  system  in  simulation,  present  a  variety  of  results,  and  demonstrate  that  the  projector  can  alternatively  achieve  super-resolved  and  high  dynamic  range  2D  image  display  when  used  with  a  conventional  screen.
2	Multi  scale  modeling  and  rendering  of  granular  materials.  We  address  the  problem  of  modeling  and  rendering  granular  materials---such  as  large  structures  made  of  sand,  snow,  or  sugar---where  an  aggregate  object  is  composed  of  many  randomly  oriented,  but  discernible  grains.  These  materials  pose  a  particular  challenge  as  the  complex  scattering  properties  of  individual  grains,  and  their  packing  arrangement,  can  have  a  dramatic  effect  on  the  large-scale  appearance  of  the  aggregate  object.  We  propose  a  multi-scale  modeling  and  rendering  framework  that  adapts  to  the  structure  of  scattered  light  at  different  scales.  We  rely  on  path  tracing  the  individual  grains  only  at  the  finest  scale,  and---by  decoupling  individual  grains  from  their  arrangement---we  develop  a  modular  approach  for  simulating  longer-scale  light  transport.  We  model  light  interactions  within  and  across  grains  as  separate  processes  and  leverage  this  decomposition  to  derive  parameters  for  classical  radiative  transport,  including  standard  volumetric  path  tracing  and  a  diffusion  method  that  can  quickly  summarize  the  large  scale  transport  due  to  many  grain  interactions.  We  require  only  a  one-time  precomputation  per  exemplar  grain,  which  we  can  then  reuse  for  arbitrary  aggregate  shapes  and  a  continuum  of  different  packing  rates  and  scales  of  grains.  We  demonstrate  our  method  on  scenes  containing  mixtures  of  tens  of  millions  of  individual,  complex,  specular  grains  that  would  be  otherwise  infeasible  to  render  with  standard  techniques.
2	Pinoky  a  ring  that  animates  your  plush  toys.  PINOKY  is  a  wireless  ring-like  device  that  can  be  externally  attached  to  any  plush  toy  as  an  accessory  that  animates  the  toy,  such  as  by  moving  its  limbs.  A  user  is  thus  able  to  instantly  convert  any  plush  toy  into  a  soft  robot.  The  user  can  control  the  toy  remotely  or  input  the  movement  desired  by  moving  the  plush  toy  and  having  the  data  recorded  and  played  back.  Unlike  other  methods  for  animating  plush  toys,  PINOKY  is  non-intrusive,  so  alterations  to  the  toy  are  not  required.
2	Singularity  constrained  octahedral  fields  for  hexahedral  meshing.  Despite  high  practical  demand,  algorithmic  hexahedral  meshing  with  guarantees  on  robustness  and  quality  remains  unsolved.  A  promising  direction  follows  the  idea  of  integer-grid  maps,  which  pull  back  the  Cartesian  hexahedral  grid  formed  by  integer  isoplanes  from  a  parametric  domain  to  a  surface-conforming  hexahedral  mesh  of  the  input  object.  Since  directly  optimizing  for  a  high-quality  integer-grid  map  is  mathematically  challenging,  the  construction  is  usually  split  into  two  steps:  (1)  generation  of  a  surface-aligned  octahedral  field  and  (2)  generation  of  an  integer-grid  map  that  best  aligns  to  the  octahedral  field.  The  main  robustness  issue  stems  from  the  fact  that  smooth  octahedral  fields  frequently  exhibit  singularity  graphs  that  are  not  appropriate  for  hexahedral  meshing  and  induce  heavily  degenerate  integer-grid  maps.  The  first  contribution  of  this  work  is  an  enumeration  of  all  local  configurations  that  exist  in  hex  meshes  with  bounded  edge  valence,  and  a  generalization  of  the  Hopf-Poincare  formula  to  octahedral  fields,  leading  to  necessary  local  and  global  conditions  for  the  hex-meshability  of  an  octahedral  field  in  terms  of  its  singularity  graph.  The  second  contribution  is  a  novel  algorithm  to  generate  octahedral  fields  with  prescribed  hex-meshable  singularity  graphs,  which  requires  the  solution  of  a  large  nonlinear  mixed-integer  algebraic  system.  This  algorithm  is  an  important  step  toward  robust  automatic  hexahedral  meshing  since  it  enables  the  generation  of  a  hex-meshable  octahedral  field.
2	Look  over  here  attention  directing  composition  of  manga  elements.  Picture  subjects  and  text  balloons  are  basic  elements  in  comics,  working  together  to  propel  the  story  forward.  Japanese  comics  artists  often  leverage  a  carefully  designed  composition  of  subjects  and  balloons  (generally  referred  to  as  panel  elements)  to  provide  a  continuous  and  fluid  reading  experience.  However,  such  a  composition  is  hard  to  produce  for  people  without  the  required  experience  and  knowledge.  In  this  paper,  we  propose  an  approach  for  novices  to  synthesize  a  composition  of  panel  elements  that  can  effectively  guide  the  reader's  attention  to  convey  the  story.  Our  primary  contribution  is  a  probabilistic  graphical  model  that  describes  the  relationships  among  the  artist's  guiding  path,  the  panel  elements,  and  the  viewer  attention,  which  can  be  effectively  learned  from  a  small  set  of  existing  manga  pages.  We  show  that  the  proposed  approach  can  measurably  improve  the  readability,  visual  appeal,  and  communication  of  the  story  of  the  resulting  pages,  as  compared  to  an  existing  method.  We  also  demonstrate  that  the  proposed  approach  enables  novice  users  to  create  higher-quality  compositions  with  less  time,  compared  with  commercially  available  programs.
2	Audeosynth  music  driven  video  montage.  We  introduce  music-driven  video  montage,  a  media  format  that  offers  a  pleasant  way  to  browse  or  summarize  video  clips  collected  from  various  occasions,  including  gatherings  and  adventures.  In  music-driven  video  montage,  the  music  drives  the  composition  of  the  video  content.  According  to  musical  movement  and  beats,  video  clips  are  organized  to  form  a  montage  that  visually  reflects  the  experiential  properties  of  the  music.  Nonetheless,  it  takes  enormous  manual  work  and  artistic  expertise  to  create  it.  In  this  paper,  we  develop  a  framework  for  automatically  generating  music-driven  video  montages.  The  input  is  a  set  of  video  clips  and  a  piece  of  background  music.  By  analyzing  the  music  and  video  content,  our  system  extracts  carefully  designed  temporal  features  from  the  input,  and  casts  the  synthesis  problem  as  an  optimization  and  solves  the  parameters  through  Markov  Chain  Monte  Carlo  sampling.  The  output  is  a  video  montage  whose  visual  activities  are  cut  and  synchronized  with  the  rhythm  of  the  music,  rendering  a  symphony  of  audio-visual  resonance.
2	An  algebraic  model  for  parameterized  shape  editing.  We  present  an  approach  to  high-level  shape  editing  that  adapts  the  structure  of  the  shape  while  maintaining  its  global  characteristics.  Our  main  contribution  is  a  new  algebraic  model  of  shape  structure  that  characterizes  shapes  in  terms  of  linked  translational  patterns.  The  space  of  shapes  that  conform  to  this  characterization  is  parameterized  by  a  small  set  of  numerical  parameters  bounded  by  a  set  of  linear  constraints.  This  convex  space  permits  a  direct  exploration  of  variations  of  the  input  shape.  We  use  this  representation  to  develop  a  robust  interactive  system  that  allows  shapes  to  be  intuitively  manipulated  through  sparse  constraints.
2	Gaze  correction  for  home  video  conferencing.  Effective  communication  using  current  video  conferencing  systems  is  severely  hindered  by  the  lack  of  eye  contact  caused  by  the  disparity  between  the  locations  of  the  subject  and  the  camera.  While  this  problem  has  been  partially  solved  for  high-end  expensive  video  conferencing  systems,  it  has  not  been  convincingly  solved  for  consumer-level  setups.  We  present  a  gaze  correction  approach  based  on  a  single  Kinect  sensor  that  preserves  both  the  integrity  and  expressiveness  of  the  face  as  well  as  the  fidelity  of  the  scene  as  a  whole,  producing  nearly  artifact-free  imagery.  Our  method  is  suitable  for  mainstream  home  video  conferencing:  it  uses  inexpensive  consumer  hardware,  achieves  real-time  performance  and  requires  just  a  simple  and  short  setup.  Our  approach  is  based  on  the  observation  that  for  our  application  it  is  sufficient  to  synthesize  only  the  corrected  face.  Thus  we  render  a  gaze-corrected  3D  model  of  the  scene  and,  with  the  aid  of  a  face  tracker,  transfer  the  gaze-corrected  facial  portion  in  a  seamless  manner  onto  the  original  image.
2	Field  aligned  online  surface  reconstruction.  Today's  3D  scanning  pipelines  can  be  classified  into  two  overarching  categories:  offline,  high  accuracy  methods  that  rely  on  global  optimization  to  reconstruct  complex  scenes  with  hundreds  of  millions  of  samples,  and  online  methods  that  produce  real-time  but  low-quality  output,  usually  from  structure-from-motion  or  depth  sensors.  The  method  proposed  in  this  paper  is  the  first  to  combine  the  benefits  of  both  approaches,  supporting  online  reconstruction  of  scenes  with  hundreds  of  millions  of  samples  from  high-resolution  sensing  modalities  such  as  structured  light  or  laser  scanners.  The  key  property  of  our  algorithm  is  that  it  sidesteps  the  signed-distance  computation  of  classical  reconstruction  techniques  in  favor  of  direct  filtering,  parametrization,  and  mesh  and  texture  extraction.  All  of  these  steps  can  be  realized  using  only  weak  notions  of  spatial  neighborhoods,  which  allows  for  an  implementation  that  scales  approximately  linearly  with  the  size  of  each  dataset  that  is  integrated  into  a  partial  reconstruction.  Combined,  these  algorithmic  differences  enable  a  drastically  more  efficient  output-driven  interactive  scanning  and  reconstruction  workflow,  where  the  user  is  able  to  see  the  final  quality  field-aligned  textured  mesh  during  the  entirety  of  the  scanning  procedure.  Holes  or  parts  with  registration  problems  are  displayed  in  real-time  to  the  user  and  can  be  easily  resolved  by  adding  further  localized  scans,  or  by  adjusting  the  input  point  cloud  using  our  interactive  editing  tools  with  immediate  visual  feedback  on  the  output  mesh.  We  demonstrate  the  effectiveness  of  our  algorithm  in  conjunction  with  a  state-of-the-art  structured  light  scanner  and  optical  tracking  system  and  test  it  on  a  large  variety  of  challenging  models.
2	Kinect  based  facial  animation.  In  this  demo  we  present  our  system  for  performance-based  character  animation  that  enables  any  user  to  control  the  facial  expressions  of  a  digital  avatar  in  realtime.  Compared  to  existing  technologies,  our  system  is  easy  to  deploy  and  does  not  require  any  face  markers,  intrusive  lighting,  or  complex  scanning  hardware.  Instead,  the  user  is  recorded  in  a  natural  environment  using  the  non-intrusive,  commercially  available  Microsoft  Kinect  3D  sensor.  Since  high  noise  levels  in  the  acquired  data  prevent  conventional  tracking  methods  to  work  well,  we  developed  a  method  to  combine  a  database  of  existing  animations  with  facial  tracking  to  generate  compelling  animations.  Realistic  facial  tracking  facilitates  a  range  of  new  applications,  e.g.  in  digital  gameplay,  telepresence  or  social  interactions.
2	Gradient  domain  path  tracing.  We  introduce  gradient-domain  rendering  for  Monte  Carlo  image  synthesis.  While  previous  gradient-domain  Metropolis  Light  Transport  sought  to  distribute  more  samples  in  areas  of  high  gradients,  we  show,  in  contrast,  that  estimating  image  gradients  is  also  possible  using  standard  (non-Metropolis)  Monte  Carlo  algorithms,  and  furthermore,  that  even  without  changing  the  sample  distribution,  this  often  leads  to  significant  error  reduction.  This  broadens  the  applicability  of  gradient  rendering  considerably.  To  gain  insight  into  the  conditions  under  which  gradient-domain  sampling  is  beneficial,  we  present  a  frequency  analysis  that  compares  Monte  Carlo  sampling  of  gradients  followed  by  Poisson  reconstruction  to  traditional  Monte  Carlo  sampling.  Finally,  we  describe  Gradient-Domain  Path  Tracing  (G-PT),  a  relatively  simple  modification  of  the  standard  path  tracing  algorithm  that  can  yield  far  superior  results.
2	Reduced  order  shape  optimization  using  offset  surfaces.  Given  the  2-manifold  surface  of  a  3d  object,  we  propose  a  novel  method  for  the  computation  of  an  offset  surface  with  varying  thickness  such  that  the  solid  volume  between  the  surface  and  its  offset  satisfies  a  set  of  prescribed  constraints  and  at  the  same  time  minimizes  a  given  objective  functional.  Since  the  constraints  as  well  as  the  objective  functional  can  easily  be  adjusted  to  specific  application  requirements,  our  method  provides  a  flexible  and  powerful  tool  for  shape  optimization.  We  use  manifold  harmonics  to  derive  a  reduced-order  formulation  of  the  optimization  problem,  which  guarantees  a  smooth  offset  surface  and  speeds  up  the  computation  independently  from  the  input  mesh  resolution  without  affecting  the  quality  of  the  result.  The  constrained  optimization  problem  can  be  solved  in  a  numerically  robust  manner  with  commodity  solvers.  Furthermore,  the  method  allows  simultaneously  optimizing  an  inner  and  an  outer  offset  in  order  to  increase  the  degrees  of  freedom.  We  demonstrate  our  method  in  a  number  of  examples  where  we  control  the  physical  mass  properties  of  rigid  objects  for  the  purpose  of  3d  printing.
2	Eulerian  on  lagrangian  cloth  simulation.  We  resolve  the  longstanding  problem  of  simulating  the  contact-mediated  interaction  of  cloth  and  sharp  geometric  features  by  introducing  an  Eulerian-on-Lagrangian  (EOL)  approach  to  cloth  simulation.  Unlike  traditional  Lagrangian  approaches  to  cloth  simulation,  our  EOL  approach  permits  bending  exactly  at  and  sliding  over  sharp  edges,  avoiding  parasitic  locking  caused  by  over-constraining  contact  constraints.  Wherever  the  cloth  is  in  contact  with  sharp  features,  we  insert  EOL  vertices  into  the  cloth,  while  the  rest  of  the  cloth  is  simulated  in  the  standard  Lagrangian  fashion.  Our  algorithm  manifests  as  new  equations  of  motion  for  EOL  vertices,  a  contact-conforming  remesher,  and  a  set  of  simple  constraint  assignment  rules,  all  of  which  can  be  incorporated  into  existing  state-of-the-art  cloth  simulators  to  enable  smooth,  inequality-constrained  contact  between  cloth  and  objects  in  the  world.
2	Visual  rhythm  and  beat.  We  present  a  visual  analogue  for  musical  rhythm  derived  from  an  analysis  of  motion  in  video,  and  show  that  alignment  of  visual  rhythm  with  its  musical  counterpart  results  in  the  appearance  of  dance.  Central  to  our  work  is  the  concept  of  visual  beats  ---  patterns  of  motion  that  can  be  shifted  in  time  to  control  visual  rhythm.  By  warping  visual  beats  into  alignment  with  musical  beats,  we  can  create  or  manipulate  the  appearance  of  dance  in  video.  Using  this  approach  we  demonstrate  a  variety  of  retargeting  applications  that  control  musical  synchronization  of  audio  and  video:  we  can  change  what  song  performers  are  dancing  to,  warp  irregular  motion  into  alignment  with  music  so  that  it  appears  to  be  dancing,  or  search  collections  of  video  for  moments  of  accidentally  dance-like  motion  that  can  be  used  to  synthesize  musical  performances.
2	Autocomplete  3d  sculpting.  Digital  sculpting  is  a  popular  means  to  create  3D  models  but  remains  a  challenging  task.  We  propose  a  3D  sculpting  system  that  assists  users,  especially  novices,  in  freely  creating  models  with  reduced  input  labor  and  enhanced  output  quality.  With  an  interactive  sculpting  interface,  our  system  silently  records  and  analyzes  users'  workflows  including  brush  strokes  and  camera  movements,  and  predicts  what  they  might  do  in  the  future.  Users  can  accept,  partially  accept,  or  ignore  the  suggestions  and  thus  retain  full  control  and  individual  style.  They  can  also  explicitly  select  and  clone  past  workflows  over  output  model  regions.  Our  key  idea  is  to  consider  how  a  model  is  authored  via  dynamic  workflows  in  addition  to  what  is  shaped  in  static  geometry.  This  allows  our  method  for  more  accurate  analysis  of  user  intentions  and  more  general  synthesis  of  shape  structures  than  prior  workflow  or  geometry  methods,  such  as  large  overlapping  deformations.  We  evaluate  our  method  via  user  feedbacks  and  authored  models.
2	Integration  of  spatial  sound  in  immersive  virtual  environments  an  experimental  study  on  effects  of  spatial  sound  on  presence.  Sound  is  an  important  part  of  an  immersive  virtual  environment,  contributing  to  immersion,  presence,  and  user  performance.  The  experimental  study  presented  analyzed  the  effect  of  spatial-sound  vs.  no-sound  display  on  presence  experienced  in  a  3D  virtual  scene.  Results  indicate  a  medium  to  strong  effect  of  spatial  sound,  leading  to  higher  levels  of  presence  experienced.
2	A  real  time  welding  training  system  base  on  virtual  reality.  Onew360  is  a  training  simulator  for  simulating  gas  metal  arc  welding  (GMAW)  welding.  This  system  is  comprised  of  standard  welding  hardware  components  (helmet,  gun,  work-piece),  a  PC,  a  head-mounted  display,  a  tracking  system  for  both  the  torch  and  the  user's  head,  and  external  audio  speakers.  The  track  model  of  welding  simulator  using  single-camera  vision  measurement  technology  to  calculate  the  position  of  the  welding  gun  and  helmet,  and  the  simulation  model  using  simple  model  method  to  simulate  the  weld  geometry  based  on  the  orientation  and  speed  of  the  welding  torch.  So  that  the  system  produce  a  realistic,  interactive,  and  immersive  welding  experience.
2	Comparing  three  interaction  methods  for  manipulating  thin  deformable  virtual  objects.  We  present  results  of  a  user  study  in  which  we  compared  three  interaction  methods  for  manipulating  deformable  objects  in  immersive  virtual  environments.  The  task  was  to  control  a  virtual  robot  hand  removing  a  thin  foil  cover  from  a  satellite  in  an  on-orbit  servicing  training  simulator.  The  lack  of  haptic  feedback  placed  a  high  challenge  on  the  user  when  trying  to  apply  the  right  force  for  grasping  the  foil  without  losing  grip  or  damaging  it.  We  compared  the  intuitiveness  and  effectiveness  of  using  a  tracked  joystick,  finger  distance  measurement,  and  a  novel  prototype  enabling  direct  force  input  through  pinching.
2	Real  time  vr  simulation  of  laparoscopic  cholecystectomy  based  on  parallel  position  based  dynamics  in  gpu.  In  recent  years,  virtual  reality  (VR)  based  training  has  greatly  changed  surgeons  learning  mode.  It  can  simulate  the  surgery  from  the  visual,  auditory,  and  tactile  aspects.  VR  medical  simulator  can  greatly  reduce  the  risk  of  the  real  patient  and  the  cost  of  hospitals.  Laparoscopic  cholecystectomy  is  one  of  the  typical  representatives  in  minimal  invasive  surgery  (MIS).  Due  to  the  large  incidence  of  cholecystectomy,  the  application  of  its  VR-based  simulation  is  vital  and  necessary  for  the  residents’  surgical  training.  In  this  paper,  we  present  a  VR  simulation  framework  based  on  position-based  dynamics  (PBD)  for  cholecystectomy.  To  further  accelerate  the  deformation  of  organs,  PBD  constraints  are  solved  in  parallel  by  a  graph  coloring  algorithm.  We  introduce  a  bio-thermal  conduction  model  to  improve  the  realism  of  the  fat  tissue  electrocautery.  Finally,  we  design  a  hybrid  multi-model  connection  method  to  handle  the  interaction  and  simulation  of  the  liver-gallbladder  separation.  This  simulation  system  has  been  applied  to  laparoscopic  cholecystectomy  training  in  several  hospitals.  From  the  experimental  results,  users  can  operate  in  real-time  with  high  stability  and  fidelity.  The  simulator  is  also  evaluated  by  a  number  of  digestive  surgeons  through  preliminary  studies.  They  believed  that  the  system  can  offer  great  help  to  the  improvement  of  surgical  skills.
2	Perceptual  characteristic  of  multi  spectral  vibrations  beyond  the  human  perceivable  frequency  range.  In  this  paper,  we  show  experimental  results  indicating  that  tactile  mechanoreceptors  have  non-linear  sensitivity  to  an  applied  vibration.  We  focus  on  the  perceptual  characteristic  of  high  frequency  amplitude-modulated  (AM)  vibration.  It  is  known  that  humans  can  feel  AM  vibration  even  when  its  carrier  frequency  is  higher  than  the  perceivable  range.  Non-linearity  of  perception  should  be  taken  into  consideration  for  explaining  this  characteristic.  We  observed  a  deformation  of  the  skin  when  the  AM  vibration  is  applied.  We  found  that  we  can  detect  an  envelope  of  the  applied  AM  vibration,  even  when  the  skin  surface  deforms  as  a  linear  elastic  body;  whereas  harmonic  vibratory  pairs  cannot  be  perceived.  This  result  indicates  that  only  a  squared  value  of  a  displacement,  which  is  proportional  to  strain  energy  density,  cannot  explain  the  non-linearity  of  mechanoreceptors.  These  perceptual  characteristics  should  be  taken  into  consideration  for  designing  a  tactile  display  with  multi-spectral  vibrations.  Otherwise,  the  simultaneous  multi-spectral  vibrations  can  affect  each  other  and  may  change  perceptions  because  of  the  non-linearity  of  the  perceptual  process.
2	The  p300  potential  for  fixations  onto  target  object  when  exploring  natural  scenes  during  a  visual  task  after  denoising  overlapped  efrp.  Electroencephalography  (EEG)  studies  have  largely  reported  the  P300  Event  Related  Potential  (ERP)  elicited  by  target  stimulus  processing  compared  to  non-target  stimulus.  These  studies  used  constrain  experimental  paradigms  during  which  participants  did  not  move  their  eyes.  However,  during  more  ecological  paradigms  where  participants  can  move  their  eyes,  the  P300  potential  might  be  more  difficult  to  identify  due  to  the  overlapped  potentials  elicited  by  consecutive  ocular  fixations.  In  this  study,  we  use  the  xDawn  algorithm  for  denoising  the  overlapped  Eye-Fixation  Related  Potentials  (EFRP),  and  we  observe  the  P300  potential  elicited  on  the  first,  but  also  on  the  consecutive  subsequent  fixations  landed  on  the  target  stimuli.
2	Development  of  an  extensible  ssvep  bci  software  platform  and  application  to  wheelchair  control.  Visual-evoked  potential  (VEP)-based  BCIs  have  been  shown  to  provide  the  highest  information  transfer  rates  and  reliability  among  BCI  approaches.  However,  to  date,  no  flexible  software  platform  exists  that  allows  investigators  and  end-users  to  easily  evaluate  and  optimize  VEP  stimulus  parameters  such  as  size,  position,  flashing  rate,  color,  etc.,  with  seamless  integration  to  an  application  environment.  This  paper  provides  an  overview  of  the  development  of  such  a  customizable  VEP-BCI  software  platform  called  Visual  Evoked  Stimulation  and  SELection  Software  (VESSELS),  and  its  implementation  for  control  of  a  motorized  wheelchair.
2	Identification  of  brain  networks  with  high  time  space  resolution  using  dense  eeg.  A  challenging  issue  in  cognition  is  how  to  precisely  identify  brain  networks  at  very  short  temporal  scales.  So  far,  very  few  studies  have  addressed  this  problem  as  it  requires  high  temporal  and  spatial  resolution  simultaneously.  The  recent  past  years  have  seen  a  noticeable  increase  of  interest  for  electroencephalography  (EEG)  to  analyze  functional  connectivity  through  brain  sources  reconstructed  from  scalp  signals.  Here,  we  performed  a  novel  study  based  on  EEG  source  connectivity  to  identify  large  scale  networks  with  high  temporal  and  spatial  resolution.  We  show  clear  evidence  of  the  ability  of  EEG  source  connectivity  to  identify  brain  networks  with  high  time/space  resolution  during  the  visual  processing  period  of  picture  naming  task.  Our  qualitative  and  quantitative  observations  show  that  the  identified  brain  networks  are  in  accordance  with  fMRI-based  results  reported  in  the  literature  regarding  involved  brain  areas.
2	Bold  correlates  of  alpha  and  beta  eeg  rhythm  during  a  motor  task.  In  this  study,  simultaneously  acquired  EEG  and  fMRI  data  from  a  motor  experiment  are  analyzed.  The  motor  task  consists  in  moving  the  right  hand  and  is  performed  by  a  group  of  healthy  volunteers.  The  objective  is  to  find  the  most  adequate  way  to  model  the  movement-related  blood  oxygen  level-dependent  (BOLD)  response  present  in  the  fMRI  data.  The  analysis  of  the  fMRI  data  is  performed  using  Statistical  Parametric  Mapping  (SPM)  and  estimating  two  different  models.  In  the  first  one  (motor  event  model),  the  BOLD  response  is  modeled  following  the  time  instants  of  the  motor  events.  The  second  one  (brain  wave  model)  incorporates  the  dynamics  of  the  5  canonical  EEG  rhythms  (a,  p,  y,  8,  6)  to  describe  the  BOLD  response.  From  the  results,  it  can  be  concluded  that  the  motor  event  model  better  describes  the  BOLD  response  related  to  the  movement  itself,  but  that  the  brain  wave  model  is  better  suited  to  characterize  the  BOLD  response  of  complementary  brain  processes.
2	Stable  electromyographic  sequence  prediction  during  movement  transitions  using  temporal  convolutional  networks.  Transient  muscle  movements  influence  the  temporal  structure  of  myoelectric  signal  patterns,  often  leading  to  unstable  prediction  behavior  from  movement-pattern  classification  methods.  We  show  that  temporal  convolutional  network  sequential  models  leverage  the  myoelectric  signal’s  history  to  discover  contextual  temporal  features  that  aid  in  correctly  predicting  movement  intentions,  especially  during  interclass  transitions.  We  demonstrate  myoelectric  classification  using  temporal  convolutional  networks  to  effect  3  simultaneous  hand  and  wrist  degrees-of-freedom  in  an  experiment  involving  nine  human-subjects.  Temporal  convolutional  networks  yield  significant  (p<0.001)  performance  improvements  over  other  state-of-the-art  methods  in  terms  of  both  classification  accuracy  and  stability.
2	Auditory  imagery  classification  with  a  non  invasive  brain  computer  interface.  Brain  Computer  Interfaces  (BCIs)  controlled  by  motor  imagery  have  been  explored  during  many  years  mainly  because  of  its  ease  of  imagination,  and  localization  of  cortex  activity.  Auditory  imagery,  nonetheless,  has  been  a  rarely  explored  approach,  but  a  one  that  might  open  new  possibilities  in  musical  interfaces,  communication  and  speech  synthesis.  It  can  also  bring  a  complementary  way  to  motor  imagery  for  improving  accuracy  in  BCIs.Using  a  non-invasive  BCI,  open  source  software,  and  auditory  imagery  of  white  noise,  we  tested  several  classification  algorithms  to  determine  the  accuracy  and  usefulness  of  using  non-motor  imagery  EEG  signals.We  tested  15  healthy  adults  with  a  6  electrode  EEG  setup  and  the  open  source  platform  OpenVibe.  The  signals  were  analyzed  with  MultiLayer  Perceptron,  Linear  Discriminant  Analysis,  and  Support  Vector  Machines  classifiers.Our  results  show  that  using  a  Support  Vector  Machine  classifier  and  our  specific  experimental  setup,  we  could  achieve  up  to  93%  accuracy  in  white  noise  imagery  in  our  subjects  (Linear  Discriminant  Analysis  and  MultiLayer  Perceptron  setups  yielded  accuracy  of  73-76%).  Thus,  auditory  imagery  can  be  a  promising  complementary  approach  to  traditional  motor-imagery  BCIs.
2	The  application  of  tactile  audible  and  ultrasonic  forces  to  human  fingertips  using  broadband  electroadhesion.  We  report  an  approach  to  controlling  friction  forces  on  sliding  human  fingertips  in  order  to  produce  simultaneous  vibrations  across  an  exceedingly  broad  range  of  tactile,  audible,  and  ultrasonic  frequencies.  Vibrations  in  the  skin  can  be  felt  directly  by  the  fingertip,  and  vibrations  in  the  air  can  be  heard  emanating  from  the  proximity  of  the  finger.  We  introduce  and  detail  an  experimental  apparatus  capable  of  recording  friction  forces  up  to  a  frequency  of  6  kHz,  and  describe  a  custom  designed  electroadhesive  amplifier  and  system  with  a  flat  current  to  force  magnitude  response  throughout  this  entire  measurement  range.  Recordings  with  a  MEMS  microphone  confirm  the  existence  of  ultrasonic  forces  applied  to  the  finger  and  further  reveal  the  ultra  wideband  capability  of  broadband  electroadhesion.  Implications  for  the  design  of  surface  haptic  and  general  audio-haptic  displays  are  discussed.
2	A  method  to  quantitatively  evaluate  changes  in  tremor  during  deep  brain  stimulation  surgery.  Deep  Brain  Stimulation  (DBS)  surgery  is  used  increasingly  as  a  symptomatic  treatment  for  patients  with  movement  related  neuro-degenerative  disorders.  However,  the  method  of  intraoperative  symptom  evaluation  is  subjective.  This  paper  proposes  a  method  to  quantitatively  evaluate  tremor  by  measuring  the  acceleration  of  the  patient's  wrist  during  the  surgery.  The  results  of  applying  the  method  to  2  patients  suggest  that  the  acceleration  measurements  are  very  sensitive  to  the  change  in  the  tremor  and  that  they  can  be  used  to  identify  clinically  effective  stimulation  amplitudes.  By  collecting  acceleration  data  from  DBS  surgeries  for  many  patients,  we  hope  to  add  more  knowledge  to  the  mechanisms  of  deep  brain  stimulation.
2	Mixed  reality  game  prototypes  for  upper  body  exercise  and  rehabilitation.  This  research  demonstration  consists  of  an  integrated  hardware  and  software  platform  developed  for  rapid  prototyping  of  virtual  reality-based  games  for  upper  body  exercise  and  rehabilitation.  The  exercise  protocol  has  been  adopted  from  an  evidence-based  shoulder  exercise  program  for  individuals  with  spinal  cord  injury.  The  hardware  consists  of  a  custom  metal  rig  that  holds  a  standard  wheelchair,  six  Gametraks  attached  to  elastic  exercise  bands,  a  Microsoft  Kinect,  a  laptop  and  a  large  screen.  A  total  of  21  prototypes  were  built  using  drivers  for  Kinect,  MaxMSP  and  Unity  Pro  3  in  order  to  evaluate  game  ideas  based  on  deconstruction  of  the  exercise  protocol.  Future  directions  include  validation  of  our  heuristic  design  and  evaluation  model  and  the  development  of  an  exercise  suite  of  point-of-care  VR  games.
2	Haptic  force  guided  sound  synthesis  in  multisensory  virtual  reality  vr  simulation  for  rigid  fluid  interaction.  This  paper  tackles  a  challenging  problem  for  interactive  rigid-fluid  interaction  sound  synthesis.  One  core  issue  of  the  rigid-fluid  interaction  in  multisensory  VR  system  is  how  to  balance  the  algorithm  efficiency,  result  authenticity  and  result  synchronization.  Since  the  sampling  rate  of  audio  is  far  greater  than  visual  and  haptic  modalities,  sound  synthesis  for  a  multisensory  VR  system  is  more  difficult  than  visual  simulation  and  haptic  rendering,  which  still  remains  an  open  challenge  until  now.  Therefore,  this  paper  focuses  on  developing  an  efficient  sound  synthesis  method  tailored  for  a  multisensory  system.  To  improve  the  result  authenticity  while  ensuring  real  time  performance  and  result  synchronization,  we  propose  a  novel  haptic  force  guided  granular  sound  synthesis  method  tailored  for  sounding  in  multisensory  VR  systems.  To  the  best  of  our  knowledge,  this  is  the  first  step  that  exploits  haptic  force  feedback  from  the  tactile  channel  for  guiding  sound  synthesis  in  a  multisensory  VR  system.  Specifically,  we  propose  a  modified  spectral  granular  sound  synthesis  method,  which  can  ensure  real  time  simulation  and  improve  the  result  authenticity  as  well.  Then,  to  balance  the  algorithm  efficiency  and  result  synchronization,  we  design  a  multi-force  (MF)  granulation  algorithm  which  avoids  repeated  analysis  of  fluid  particle  motion  and  thereby  improves  the  synchronization  performance.  Various  results  show  that  the  proposed  sound  synthesis  method  effectively  overcomes  the  limitations  of  existing  methods  in  terms  of  audio  modality,  which  has  great  potential  to  provide  powerful  technological  support  for  building  a  more  immersive  multisensory  VR  system.
2	Towards  a  framework  on  accessible  and  social  vr  in  education.  In  this  extended  abstract,  we  argue  that  for  virtual  reality  to  be  a  successful  tool  in  social  learning  spaces  (e.g.  classrooms  or  museums)  we  must  also  look  outside  the  virtual  reality  literature  to  provide  greater  focus  on  accessible  and  social  collaborative  content.  We  explore  work  within  Computer  Supported  Collaborative  Learning  (CSCL)  and  social  VR  domains  to  move  towards  developing  a  design  framework  for  socio-educational  VR.  We  also  briefly  describe  our  work-in-progress  application  framework,  Circles,  including  these  features  in  WebVR.
2	Augmenting  virtual  reality  with  near  real  world  objects.  Pure  virtual  reality  headsets  lack  support  for  user  awareness  of  real  world  objects.  We  show  how  to  augment  a  virtual  environment  with  real  world  objects  by  incorporating  color  and  stereo  information  from  the  front-mounted  stereo  camera  system  of  the  HTC  Vive  Pro.  With  an  adjustable  amount  of  virtual  and  real  elements  by  specifying  the  depth  range  of  interest  for  real  objects  these  can  be  either  always  augmented  on  top  of  the  virtual  reality  or  embedded  in  the  virtual  world  with  correct  occlusion.  Possible  use  cases  are  training  with  appliances  that  require  you  to  see  both  the  appliance  as  well  as  your  hands,  collaborating  in  virtual  reality  with  the  real  users  instead  of  avatars,  as  well  as  static  and  moving  obstacle  detection.  Objects  of  interest  are  cut  out  depending  on  depth  values  of  the  stereo  matching  system's  depth  image.  The  depth  image  matches  only  one  of  both  camera  sensors  so  a  second  depth  image  is  derived  to  match  the  other  camera  sensor  as  well.  We  overlay  the  rendered  virtual  reality  images  for  both  eyes  with  the  corresponding  cut-out  camera  images.  Furthermore  we  address  the  problem  of  missing  depth  data  especially  for  very  near  objects  with  an  integral  image  based  depth-dependent  foreground  mask  expansion  algorithm.  Since  a  comfortable  mixed  reality  experience  requires  high  framerates  of  optimally  90  frames  per  second  we  utilize  the  GPU  for  optimized  implementations  of  the  algorithms  and  computations  involved.
2	Disguising  rotational  gain  for  redirected  walking  in  virtual  reality  effect  of  visual  density.  In  virtual  reality  environments  that  allow  users  to  walk  freely,  the  area  of  the  virtual  environment  (VE)  is  constrained  to  the  size  of  the  tracking  area.  By  using  redirection  techniques,  this  problem  can  be  partially  circumvented;  one  of  the  techniques  involves  rotating  the  user  more  or  less  in  the  virtual  world  than  in  the  physical  world;  this  technique  is  referred  to  as  rotational  gain.  Experiments  conducted  by  Frank  Steinicke  et  al.  [7]  sought  to  uncover  users'  ability  to  detect  the  usage  of  rotational  gain.  This  paper  seeks  to  further  investigate  this  area,  examining  the  effect  of  visual  density  in  the  VE.  Testing  visual  density  using  0,  4  and  16  objects  did  not  seem  to  reveal  the  point  at  which  the  test  participants  become  noticeably  affected.
2	Mixed  reality  for  cultural  heritage.  In  this  paper  we  present  two  different  approaches  with  Mixed  (Virtual  and  Augmented)  Reality  to  give  pupils  an  understanding  of  ancient  Greek  culture  and  history  in  a  motivating  way.  We  identify  the  possibilities  of  AR  in  a  museum  for  ancient  statues  to  represent  virtual  information  and  to  guide  the  visitor.  Moreover,  we  discuss  typical  issues  of  mobile  and  stationary  Virtual  Reality  (VR)  systems  in  the  context  of  public  usage  within  schools  and  museums.  Additionally,  a  new  VR-streaming  approach  called  SaMaXVR  is  presented  that  combines  the  benefits  of  mobile  and  stationary  consumer  ready  VR  systems  in  terms  of  usability,  maintenance,  safety  and  graphics-quality  by  mitigating  many  of  their  individual  disadvantages  when  used  in  daily  business.  As  this  streaming  solution  consists  of  cost-effective  hardware,  this  approach  can  be  seen  as  a  valuable  and  affordable  alternative  for  schools,  museums  and  students.  To  demonstrate  its  potential,  an  application  was  developed  that  visualizes  complex  3D  scans  of  cultural  heritage  in  their  former  original  environment  to  give  users  the  possibility  to  travel  back  in  time  (and  place)  to  see  how  the  artifacts  might  have  looked  like  in  the  past.
2	Real  time  3d  magnetic  field  visualization  based  on  augmented  reality.  In  physics  teaching,  electromagnetism  is  one  of  the  most  difficult  concepts  for  students  to  understand.  This  paper  proposes  a  real  time  visualization  method  for  3-D  magnetic  field  based  on  the  augmented  reality  technology,  which  can  not  only  visualize  magnetic  flux  lines  in  real  time,  but  also  simulates  the  approximate  sparse  distribution  of  magnetic  flux  lines  in  space.  An  application  utilizing  this  method  is  also  presented.  It  permits  leaners  to  freely  and  interactively  move  the  magnets  in  3-D  space  and  to  observe  the  magnetic  flux  lines  in  real  time.  As  a  result,  the  proposed  method  visualizes  the  invisible  factors  in  3-D  magnetic  field,  with  which  students  will  have  real-life  reference  when  studying  electromagnetic.
2	Cybersickness  and  anxiety  in  virtual  environments.  The  question  whether  feelings  of  anxiety  are  confounded  with  cybersickness  in  studies  on  virtual  reality  exposure  therapy  (VRET)  was  raised  since  the  questionnaires  used  to  measure  them  contain  overlapping  items.  In  the  experiment,  88  participants  were  asked  to  talk  in  front  of  a  virtual  audience.  Previous  research  has  shown  that  this  task  may  induce  feelings  of  anxiety  (Slater,  Pertaub,  &  Steed,  1999).  A  significant  correlation  between  levels  of  experienced  anxiety  and  the  nausea  subscale  of  the  Simulator  Sickness  Questionnaire  was  found  for  people  who  reported  no  cybersickness  in  a  virtual  neutral  world.  Therefore  it  must  be  concluded  that  when  cybersickness  is  measured  in  VRET  experiments,  the  results  may  partly  be  explained  by  feel-ings  of  anxiety  rather  than  cybersickness  per  se.
2	Visual  olfactory  immersive  environment  for  product  evaluation.  Today  smells  are  used  for  communicating  information  about  products  as  household  cleaners  and  food.  However,  smells  can  be  also  applied  to  any  kind  of  products.  Several  researches  have  focused  on  integrating  smells  in  virtual  environments.  The  research  questions  addressed  in  this  work  concern  whether  Virtual  Prototypes,  including  the  sense  of  smell,  can  be  used  for  evaluating  products  as  effectively  as  studies  performed  in  real  environments,  and  whether  smells  can  increase  the  users'  sense  of  presence  in  the  virtual  environment.  For  this  purpose,  an  experimental  framework  including  a  wearable  olfactory  display  has  been  developed,  and  experimental  tests  have  been  performed.
2	Comparative  study  of  input  devices  for  a  vr  mine  simulation.  It  has  been  shown  that  virtual  reality  (VR)  can  be  used  to  train  mine  workers  for  safety  in  critical  situations  [4].  The  National  Institute  for  Occupational  Safety  and  Health  (NIOSH)  has  a  virtual  reality  (VR)  laboratory  on  its  Pittsburgh  campus.  Currently,  input  devices  for  the  system  are  an  Xbox  360  game  pad  and  an  air  mouse.  Due  to  the  high  cost  and  added  complexity  of  most  3D  tracking  systems,  we  wanted  to  first  test  to  see  if  the  mine  safety  application  could  benefit  from  an  upgrade  to  a  6-DOF  tracking  system.  Thus,  we  conducted  a  pilot  study  at  Duke  University's  six-sided  CAVE-type  system,  and  collected  performance  and  questionnaire  data  for  three  tasks  (selection,  navigation,  and  maneuvering)  and  three  devices  (gamepad,  air  mouse,  6-DOF  wand).  Results  indicate  that  the  wand  allows  users  to  complete  tasks  faster  and  is  preferred  by  users.  However,  in  certain  situations  its  use  led  to  more  errors.
2	Super  kave  an  immersive  visualization  tool  for  neutrino  physics.  Located  under  Japan's  Mount  Ikenoyama,  the  Super-Kamiokande  (or  “Super-K”)  neutrino  detector  is  used  to  study  neutrino  particle  physics.  The  Super-K  detector  consists  of  a  cylindrical  stainless  steel  tank  (41.4m  tall  and  39.3m  in  diameter)  holding  50,000  tons  of  water  and  13,031  photomultiplier  tubes  (PMTs).  To  view  the  data  captured  by  these  sensors,  many  physicists  use  2D  visualization  tools  which  present  the  data  color-coded  on  a  deconstructed  representation  of  the  cylinder.  Unfortunately,  this  deconstructed  visualization  makes  it  difficult  for  physicists  to  fully  visualize  patterns  of  neutrino  interactions.  To  address  this,  we  have  developed  a  novel  virtual  reality  (VR)  application  called  “Super-KAVE”,  which  uses  a  CAVE  to  immerse  users  in  a  lifesize  representation  of  the  Super-K  detector.  Super-KAVE  displays  the  collocation  of  photon  sensors  and  their  color-coded  data,  provides  a  new  visualization  technique  for  neutrino-interaction  patterns,  and  supports  transitioning  between  data  events.  In  this  paper,  we  describe  the  Super-K  detector  and  its  data,  discuss  the  design  and  implementation  of  our  Super-KAVE  application,  and  report  on  its  expected  uses.
2	Evaluation  of  environment  independent  techniques  for  3d  position  marking  in  augmented  reality.  Specifying  3D  positions  in  the  real  world  is  an  essential  step  to  create  augmented  reality  content.  However,  this  task  can  be  challenging  when  information  about  the  depth  or  geometry  of  the  target  location  is  not  available.  In  this  paper,  we  evaluate  alternative  techniques  for  3D  pointing  at  a  distance  without  knowledge  about  the  environment.  We  present  the  results  of  two  studies  evaluating  the  accuracy  of  techniques  based  on  geometry  and  human  depth  perception.  We  find  that  geometric  methods  provide  higher  accuracy  but  may  suffer  from  low  precision  due  to  pointing  errors.  We  propose  a  solution  that  increases  precision  by  combining  multiple  samples  to  obtain  a  better  estimate  of  the  target  position.
2	Use  of  virtual  reality  to  teach  teamwork  and  patient  safety  in  surgical  education.  The  use  360VR  Videos  may  increase  the  Engagement  and  Attentiveness  of  Students  compared  to  Traditional  2D  videos  used  in  Medical  Education  (1,2).  We  therefore  Developed  a  Stereoscopic  360VR  Video  to  Demonstrate  how  to  Use  the  WHO's  Surgical  Safety  Checklist  in  the  Operating  Room  (see  Figure  1).  With  use  of  VR  technology  we  aimed  to  give  the  Medical  Students  a  Realistic  Experience  of  the  Operating  Room  where  they  can  observe  the  Teamwork  performed  to  Ensure  Patient  Safety  during  Surgery.  The  video  is  recorded  with  a  Vuze  3D  360  Spherical  VR  Camera  and  edited  in  Final  Cut  Pro  with  use  of  Dashwoods  360VR  Toolbox  Workflow  Plugins.
2	Evaluation  of  a  vibrotactile  feedback  device  for  spatial  guidance.  In  the  present  study,  a  vibrotactile  feedback  device  for  spatial  guidance  was  evaluated  in  a  tracking  task  paradigm.  Participants  (N  =  18)  had  to  translate  and  rotate  virtual  objects  according  to  the  vibrotactile  vs.  verbal  cues  without  visual  information.  Both  types  of  spatial  guidance  were  evaluated  using  objective  performance  data  (i.e.  speed,  accuracy)  as  well  as  subjective  judgments.  Results  indicate  that  distinguishing  spatial  cues  during  the  translational  task  was  more  difficult  when  being  guided  by  vibrotactile  feedback  compared  to  verbal  feedback.  Nevertheless,  individuals  with  vibrotactile  guidance  showed  better  performance  at  rotational  tasks.  Implications  for  the  further  design  process  and  other  areas  of  application  are  discussed.
2	Applying  latency  to  half  of  a  self  avatar  s  body  to  change  real  walking  patterns.  Latency  (i.e.,  time  delay)  in  a  Virtual  Environment  is  known  to  disrupt  user  performance,  presence  and  induce  simulator  sickness.  However,  can  we  utilize  the  effects  caused  by  experiencing  latency  to  benefit  virtual  rehabilitation  technologies?  We  investigate  this  question  by  conducting  an  experiment  that  is  aimed  at  altering  gait  by  introducing  latency  applied  to  one  side  of  a  self-avatar  with  a  front-facing  mirror.  This  work  was  motivated  by  previous  findings  where  participants  altered  their  gait  with  increasing  latency,  even  when  participants  failed  to  notice  considerably  high  latencies  as  150ms  or  225ms.  In  this  paper,  we  present  the  results  of  a  study  that  applies  this  novel  technique  to  average  healthy  persons  (i.e.,  to  demonstrate  the  feasibility  of  the  approach  before  applying  it  to  persons  with  disabilities).  The  results  indicate  a  tendency  to  create  asymmetric  gait  in  persons  with  symmetric  gait  when  latency  is  applied  to  one  side  of  their  self-avatar.  Thus,  the  study  shows  the  potential  of  applying  one-sided  latency  in  a  self-avatar,  which  could  be  used  to  develop  asymmetric  gait  rehabilitation  techniques.
2	Omnimr  omnidirectional  mixed  reality  with  spatially  varying  environment  reflections  from  moving  360  video  cameras.  We  propose  a  new  approach  for  creating  omnidirectional  mixed  reality  (OmniMR)  from  moving-camera  360°  video.  To  insert  virtual  computer-generated  elements  into  a  moving-camera  360°  video,  we  reconstruct  camera  motion  and  sparse  scene  content  via  structure  from  motion  on  stitched  equirectangular  video  (the  default  output  format  of  current  360°  cameras).  Then,  to  plausibly  reproduce  realworld  lighting  conditions  for  these  inserted  elements,  we  employ  inverse  tone  mapping  to  recover  high  dynamic  range  environment  maps  which  vary  spatially  along  the  camera  path.  We  implement  our  approach  into  the  Unity  rendering  engine  for  real-time  object  rendering  with  dynamic  lighting  and  user  interaction.  This  expands  the  use  and  flexibility  of  360°  video  for  mixed  reality.
2	Effects  of  hand  representations  for  typing  in  virtual  reality.  Alphanumeric  text  entry  is  a  challenge  for  Virtual  Reality  (VR)  applications.  VR  enables  new  capabilities,  impossible  in  the  real  world,  such  as  an  unobstructed  view  of  the  keyboard,  without  occlusion  by  the  user's  physical  hands.  Several  hand  representations  have  been  proposed  for  typing  in  VR  on  standard  physical  keyboards.  However,  to  date,  these  hand  representations  have  not  been  compared  regarding  their  performance  and  effects  on  presence  for  VR  text  entry.  Our  work  addresses  this  gap  by  comparing  existing  hand  representations  with  minimalistic  fingertip  visualization.  We  study  the  effects  of  four  hand  representations  (no  hand  representation,  inverse  kinematic  model,  fingertip  visualization  using  spheres  and  video  inlay)  on  typing  in  VR  using  a  standard  physical  keyboard  with  24  participants.  We  found  that  the  fingertip  visualization  and  video  inlay  both  resulted  in  statistically  significant  lower  text  entry  error  rates  compared  to  no  hand  or  inverse  kinematic  model  representations.  We  found  no  statistical  differences  in  text  entry  speed.
2	Object  size  perception  in  immersive  virtual  reality  avatar  realism  affects  the  way  we  perceive.  How  does  the  representation  of  an  embodied  avatar  influence  the  way  in  which  a  human  perceives  the  scale  of  a  virtual  environment?  It  has  been  shown  that  the  scale  of  the  external  environment  is  perceived  relative  to  the  size  of  one's  body.  However,  the  influence  of  avatar  realism  on  the  perceived  scale  has  not  been  investigated,  despite  the  fact  that  it  is  common  to  embody  avatars  of  various  representations,  from  iconic  to  realistic.  This  study  examined  how  avatar  realism  would  affect  perceived  graspable  object  sizes  as  the  size  of  the  avatar  hand  changes.  In  the  experiment,  we  manipulated  the  realism  (high,  medium,  and  low)  and  size  (veridical  and  enlarged)  of  the  avatar  hand,  and  measured  the  perceived  size  of  a  cube.  The  results  showed  that  the  size  of  the  cube  was  perceived  to  be  smaller  when  the  avatar  hand  was  enlarged  for  all  degrees  of  realism  of  the  hand.  However,  the  enlargement  of  the  avatar  hand  had  a  greater  influence  on  the  perceived  cube  size  for  the  highly  realistic  avatar  than  for  the  medium-level  and  low-level  realism  conditions.  This  study  shed  new  light  on  the  importance  of  the  avatar  representation  in  a  three-dimensional  user  interface  field,  in  how  it  can  affect  the  manner  in  which  we  perceive  the  scale  of  a  virtual  environment.
2	A  hand  exoskeleton  device  for  robot  assisted  sensory  motor  training  after  stroke.  Robot  assisted  rehabilitation  devices  are  becoming  popular  for  stroke  rehabilitation,  but  they  are  mainly  highlighted  in  training  and  recovery  of  motor  function.  This  paper  describes  the  development  of  a  hand  exoskeleton  rehabilitation  device  which  focuses  not  only  on  rehabilitation  of  motor  function,  but  also  sensory  training  and  stimulation.  The  device  was  designed  to  enable  stroke  patients  to  train  hand  movements  with  less  aid  of  a  therapist,  and  allows  objective  assessment  of  hand  function.  The  mechanical  design  allows  to  control  the  movement  of  five  fingers  individually.  Force  feedback  is  provided  via  a  leverage  mechanism,  which  is  driven  by  DC  motors.  Additionally,  tactile  feedback  is  presented  through  vibration  motors,  attached  to  each  fingertip.  We  present  three  different  operating  modes,  active  assisted,  passive  assisted  and  active  non-assisted  haptic  interaction  mode.  The  performance  of  the  device  shows  that  it  is  possible  to  provide  feedback  forces  up  to  about  14  N  for  each  finger,  a  maximum  joint  angle  of  62°,  88°  for  MCP  and  PIP  joint  respectively  and  a  settling  time  of  0.37  s  in  the  passive  assisted  mode.  Based  on  the  characterization  of  the  proposed  device,  it  has  a  potential  to  be  utilized  in  hand  rehabilitation  in  terms  of  regaining  both  sensory  and  motor  function.
2	Feeling  multiple  edges  the  tactile  perception  of  short  ultrasonic  square  reductions  of  the  finger  surface  friction.  This  study  investigates  human  perception  of  tactile  feedback  using  ultrasonic  lubrication,  in  situation  where  feedback  is  provided  using  short  frictional  cues  of  varying  duration  and  sharpness.  In  a  first  experiment,  we  asked  participants  to  discriminate  the  transition  time  and  duration  of  short  square  ultrasonic  reductions  of  friction.  They  proved  very  sensitive  to  discriminate  millisecond  differences  in  these  two  parameters  with  the  average  psychophysical  thresholds  being  2.4  ms  for  discriminating  duration  and  2.06  ms  for  transition  time.  A  second  experiment  focused  on  participant's  perception  of  square  friction  reductions  of  variable  transition  time  and  duration  and  we  found  that  for  durations  of  the  stimulation  larger  than  90  ms,  participants  often  perceived  3  or  4  edges  when  only  two  stimulations  were  presented  while  they  consistently  felt  2  edges  for  signals  shorter  than  50  ms.  These  results  confirm  the  sensitivity  of  touch  to  transient  frictional  cues  on  smooth  surfaces  and  raises  the  question  of  how  such  cues  are  processed  by  the  neural  mechanisms  mediating  the  perception  of  friction.  Moreover,  the  knowledge  of  how  potentially  ambiguous  frictional  cues  are  resolved  is  central  to  the  implementation  of  tactile  patterns  on  friction-based  displays  with  haptic  feedback  as  well  as  to  the  definition  of  unambiguous  core  frictional  blocks.
2	A  semi  formal  framework  for  describing  interaction  design  spaces.  Interactive  system  design  is  typically  more  successful  if  it  is  an  iterative  process  involving  collaboration  between  multi-disciplinary  teams  with  different  viewpoints.  While  some  sub-teams  may  focus  on  the  creative  aspects  of  the  user  interface  design  and  other  sub-groups  on  the  implementation  of  required  functionality,  all  must  ensure  that  they  are  working  towards  the  same  goal.  They  must  also  satisfy  the  requirements  and  needs  of  all  stakeholders.  Although  many  suggestions  have  been  made  as  to  how  such  design  might  be  supported  in  a  more  formal  way  (such  as  by  using  a  model-driven  process),  less  focus  has  been  given  to  managing  the  co-ordination  of  design  sub-teams  following  a  creative  process.  In  this  paper  we  propose  a  semi-formal  framework  to  describe  and  to  compare  design  spaces,  and  the  external  design  representations  within  those  spaces.  The  framework  is  based  on  ideas  from  interaction  design  and  on  formal  refinement  approaches.  It  suggests  a  distinction  of  design  options  into  alternatives  and  variants  to  describe  and  guide  processes  of  idea  generation  and  convergence  within,  and  between,  different  design  sub-spaces  and  sub-groups.  We  provide  a  small  example  to  illustrate  our  approach  and  to  show  how  it  can  be  implemented  by  using  standard  formal  approaches  alongside  less  formal  design  notations  and  human-computer  interaction  processes.
2	Emotion  recognition  in  the  wild  challenge  2013.  Emotion  recognition  is  a  very  active  field  of  research.  The  Emotion  Recognition  In  The  Wild  Challenge  and  Workshop  (EmotiW)  2013  Grand  Challenge  consists  of  an  audio-video  based  emotion  classification  challenges,  which  mimics  real-world  conditions.  Traditionally,  emotion  recognition  has  been  performed  on  laboratory  controlled  data.  While  undoubtedly  worthwhile  at  the  time,  such  laboratory  controlled  data  poorly  represents  the  environment  and  conditions  faced  in  real-world  situations.  The  goal  of  this  Grand  Challenge  is  to  define  a  common  platform  for  evaluation  of  emotion  recognition  methods  in  real-world  conditions.  The  database  in  the  2013  challenge  is  the  Acted  Facial  Expression  in  the  Wild  (AFEW),  which  has  been  collected  from  movies  showing  close-to-real-world  conditions.
2	Dozing  off  or  thinking  hard  classifying  multi  dimensional  attentional  states  in  the  classroom  from  video.  In  this  paper,  we  extract  features  of  head  pose,  eye  gaze,  and  facial  expressions  from  video  to  estimate  individual  learners'  attentional  states  in  a  classroom  setting.  We  concentrate  on  the  analysis  of  different  definitions  for  a  student's  attention  and  show  that  available  generic  video  processing  components  and  a  single  video  camera  are  sufficient  to  estimate  the  attentional  state.
2	Hand  foot  or  voice  alternative  input  modalities  for  touchless  interaction  in  the  medical  domain.  During  medical  interventions,  direct  interaction  with  medical  image  data  is  a  cumbersome  task  for  physicians  due  to  the  sterile  environment.  Even  though  touchless  input  via  hand,  foot  or  voice  is  possible,  these  modalities  are  not  available  for  these  tasks  all  the  time.  Therefore,  we  investigated  touchless  input  methods  as  alternatives  to  each  other  with  focus  on  two  common  interaction  tasks  in  sterile  settings:  activation  of  a  system  to  avoid  unintentional  input  and  manipulation  of  continuous  values.  We  created  a  system  where  activation  could  be  achieved  via  voice,  hand  or  foot  gestures  and  continuous  manipulation  via  hand  and  foot  gestures.  We  conducted  a  comparative  user  study  and  found  that  foot  interaction  performed  best  in  terms  of  task  completion  times  and  scored  highest  in  the  subjectively  assessed  measures  usability  and  usefulness.  Usability  and  usefulness  scores  for  hand  and  voice  were  only  slightly  worse  and  all  participants  were  able  to  perform  all  tasks  in  a  sufficient  short  amount  of  time.  This  work  contributes  by  proposing  methods  to  interact  with  computers  in  sterile,  dynamic  environments  and  by  providing  evaluation  results  for  direct  comparison  of  alternative  modalities  for  common  interaction  tasks.
2	A  multi  modal  approach  for  driver  gaze  prediction  to  remove  identity  bias.  Driver  gaze  prediction  is  an  important  task  in  Advanced  Driver  Assistance  System  (ADAS).  Although  the  Convolutional  Neural  Network  (CNN)  can  greatly  improve  the  recognition  ability,  there  are  still  several  unsolved  problems  due  to  the  challenge  of  illumination,  pose  and  camera  placement.  To  solve  these  difficulties,  we  propose  an  effective  multi-model  fusion  method  for  driver  gaze  estimation.  Rich  appearance  representations,  i.e.  holistic  and  eyes  regions,  and  geometric  representations,  i.e.  landmarks  and  Delaunay  angles,  are  separately  learned  to  predict  the  gaze,  followed  by  a  score-level  fusion  system.  Moreover,  pseudo-3D  appearance  supervision  and  identity-adaptive  geometric  normalization  are  proposed  to  further  enhance  the  prediction  accuracy.  Finally,  the  proposed  method  achieves  state-of-the-art  accuracy  of  82.5288%  on  the  test  data,  which  ranks  1st  at  the  EmotiW2020  driver  gaze  prediction  sub-challenge.
2	An  interactive  control  strategy  is  more  robust  to  non  optimal  classification  boundaries.  We  consider  a  new  paradigm  for  EEG-based  brain  computer  interface  (BCI)  cursor  control  involving  signaling  satisfaction  or  dissatisfaction  with  the  current  motion  direction  instead  of  the  usual  direct  control  of  signaling  rightward  or  leftward  desired  motion.  We  start  by  assuming  that  the  same  underlying  EEG  signals  are  used  to  either  signal  directly  the  intent  for  right  and  left  motion  or  to  signal  satisfaction  and  dissatisfaction  with  the  current  motion.  We  model  the  paradigm  as  an  absorbing  Markov  chain  and  show  that  while  both  the  standard  system  and  the  new  interactive  system  have  equal  information  transfer  rate  (ITR)  when  the  Bayes  optimal  classification  boundary  (between  the  underlying  EEG  feature  distributions  used  for  the  two  classes)  is  exactly  known  and  non-changing,  the  interactive  system  is  much  more  robust  to  using  a  suboptimal  classification  boundary.  Due  to  non-stationarity  of  EEG  recordings,  in  real  systems  the  classification  boundary  will  often  be  suboptimal  for  the  current  EEG  signals.  We  note  that  a  variable  step  size  gives  a  higher  ITR  for  both  systems  (but  the  same  robustness  improvement  of  the  interactive  system  remains).  Finally,  we  present  a  way  to  probabilistically  combine  classifiers  of  natural  signals  of  satisfaction  and  dissatisfaction  with  classifiers  using  standard  left/right  controls.
2	Don  t  queue  up  user  attitudes  towards  mobile  interactions  with  public  terminals.  Public  terminals  for  service  provision  provide  high  convenience  to  users  due  to  their  constant  availability.  Yet,  the  interaction  with  them  lacks  security  and  privacy  as  it  takes  place  in  a  public  setting.  Additionally,  users  have  to  wait  in  line  until  they  can  interact  with  the  terminal.  In  comparison  to  that,  personal  mobile  devices  allow  for  private  service  execution.  Since  many  services,  like  with-drawing  money  from  an  ATM,  require  physical  presence  at  the  terminal,  hybrid  approaches  have  been  developed.  These  move  parts  of  the  interaction  to  a  mobile  device.  In  this  work  we  present  the  results  of  a  four  week  long  real  world  user  study,  in  which  we  investigated  whether  hybrid  approaches  would  actually  be  used.  The  results  show  that  users  accept  the  hybrid  service  as  they  understood  that  they  could  use  down  downtimes  (like  bus  rides)  to  prepare  the  interaction  with  the  public  terminal.  Our  findings  give  novel  insights  about  security  relevant  aspects  such  as  where  and  when  users  interact  with  the  mobile  service  before  accessing  the  public  terminal.  So  the  preparation  of  the  transaction  on  the  mobile  phone  was  often  conducted  much  further  away  from  the  terminal  than  expected  (81.0%  with  a  distance  greater  than  400m)  and  earlier  than  expected  (82.1%  at  least  5  minutes  in  advance).
2	Perceived  physicality  in  audio  enhanced  force  input.  This  paper  investigates  how  the  perceived  physicality  of  the  action  of  applying  force  with  a  finger  on  a  rigid  surface  (such  as  on  a  force-sensing  touch  screen)  can  be  enhanced  using  real-time  synthesized  audio  feedback.  A  selection  of  rich  and  evocative  audio  designs  was  used.  Additionally,  audio-tactile  cross-modal  integration  was  encouraged,  by  observing  that  the  main  rules  of  multisensory  integration  were  supported.  The  study  conducted  showed  that  richness  of  perceived  physicality  increased  considerably,  mostly  in  its  auditory  expression  (what  pressing  sounded  like).  In  addition,  in  many  instances  it  was  observed  that  the  haptic  expression  of  physicality  also  increased  (what  pressing  felt  like),  including  some  perception  of  compliance.  This  last  result  was  particularly  interesting  as  it  showed  that  audio-tactile  cross-modal  integration  might  be  present.
2	To  react  or  not  to  react  end  to  end  visual  pose  forecasting  for  personalized  avatar  during  dyadic  conversations.  Non  verbal  behaviours  such  as  gestures,  facial  expressions,  body  posture,  and  para-linguistic  cues  have  been  shown  to  complement  or  clarify  verbal  messages.  Hence  to  improve  telepresence,  in  form  of  an  avatar,  it  is  important  to  model  these  behaviours,  especially  in  dyadic  interactions.  Creating  such  personalized  avatars  not  only  requires  to  model  intrapersonal  dynamics  between  a  avatar’s  speech  and  their  body  pose,  but  it  also  needs  to  model  interpersonal  dynamics  with  the  interlocutor  present  in  the  conversation.  In  this  paper,  we  introduce  a  neural  architecture  named  Dyadic  Residual-Attention  Model  (DRAM),  which  integrates  intrapersonal  (monadic)  and  interpersonal  (dyadic)  dynamics  using  selective  attention  to  generate  sequences  of  body  pose  conditioned  on  audio  and  body  pose  of  the  interlocutor  and  audio  of  the  human  operating  the  avatar.  We  evaluate  our  proposed  model  on  dyadic  conversational  data  consisting  of  pose  and  audio  of  both  participants,  confirming  the  importance  of  adaptive  attention  between  monadic  and  dyadic  dynamics  when  predicting  avatar  pose.  We  also  conduct  a  user  study  to  analyze  judgments  of  human  observers.  Our  results  confirm  that  the  generated  body  pose  is  more  natural,  models  intrapersonal  dynamics  and  interpersonal  dynamics  better  than  non-adaptive  monadic/dyadic  models.
2	Semi  automated  level  design  via  auto  playtesting  for  handheld  casual  game  creation.  We  provide  a  proof  of  principle  that  novel  and  engaging  mobile  casual  games  with  new  aesthetics,  game  mechanics  and  player  interactions  can  be  designed  and  tested  directly  on  the  device  for  which  they  are  intended.  We  describe  the  Gamika  iOS  application  which  includes  generative  art  assets;  a  design  interface  enabling  the  making  of  physics-based  casual  games  containing  multiple  levels  with  aspects  ranging  from  Frogger-like  to  Asteroids-like  and  beyond;  a  configurable  automated  playtester  which  can  give  feedback  on  the  playability  of  levels;  and  an  automated  fine-tuning  engine  which  searches  for  level  parameterisations  that  enable  the  game  to  pass  a  battery  of  tests,  as  evaluated  by  the  auto-playtester.  Each  aspect  of  the  implementation  represents  a  baseline  with  much  room  for  improvement,  and  we  present  some  experimental  results  and  describe  how  these  will  guide  the  future  directions  for  Gamika.
2	Interpreting  behaviors  of  mobile  game  players  from  in  game  data  and  context  logs.  Human  behaviors  can  be  interpreted  based  on  its  routine  activities.  Since,  mobile  phones  are  very  common  now-a-days,  the  activities  performed  on  a  mobile  phone  can  be  an  effective  tool  to  judge  the  behavior  of  an  individual.  In  this  paper,  we  interpret  the  behavior  of  different  individuals  playing  a  mobile  video  game.  For  this  purpose,  we  developed  a  “Shoot  them  up”  android  based  video  game  and  maintained  in-game  data  and  context  logs  for  each  player  in  a  database.  Then,  we  attempted  to  analyze  the  player's  position  and  the  skill  level  based  on  the  data  recorded  during  the  game  play.  We  believe  that  this  work  will  be  a  good  initial  study  to  understand  mobile  game  players.
2	The  geographical  interest  of  historical  documents  to  interpret  the  scientific  evolution  of  the  glacier  existing  in  the  veleta  cirque  sierra  nevada  spain  during  the  little  ice  age.  Historical  documents  have  shown  their  potential  to  infer  the  origin  and  evolution  of  the  glacier  existing  in  the  Veleta  cirque,  in  the  massif  of  Sierra  Nevada  (Spain).  This  information  encompasses  written  sources  spanning  from  the  17th  to  the  mid-20th  centuries,  and  provides  valuable  knowledge  about  the  Little  Ice  Age.  These  new  data  complement  the  already  existing  geomorphological  knowledge  about  the  natural  system  and  landscape  evolution  in  Sierra  Nevada,  particularly  with  regards  to  glacial  geomorphic  events  in  the  summit  areas.  From  a  transdisciplinary  methodological  approach,  the  results  show  that  the  Veleta  glacier  was  a  singular  geomorphic  event  that  owed  its  existence  to  the  particular  environmental  conditions  of  the  high  lands  of  Sierra  Nevada,  besides  the  favourable  morpho-topographical  setting,  altitude,  aspect  as  well  as  microclimate  conditions  prevailing  in  this  area.
2	Multimodal  detection  of  depression  in  clinical  interviews.  Current  methods  for  depression  assessment  depend  almost  entirely  on  clinical  interview  or  self-report  ratings.  Such  measures  lack  systematic  and  efficient  ways  of  incorporating  behavioral  observations  that  are  strong  indicators  of  psychological  disorder.  We  compared  a  clinical  interview  of  depression  severity  with  automatic  measurement  in  48  participants  undergoing  treatment  for  depression.  Interviews  were  obtained  at  7-week  intervals  on  up  to  four  occasions.  Following  standard  cut-offs,  participants  at  each  session  were  classified  as  remitted,  intermediate,  or  depressed.  Logistic  regression  classifiers  using  leave-one-out  validation  were  compared  for  facial  movement  dynamics,  head  movement  dynamics,  and  vocal  prosody  individually  and  in  combination.  Accuracy  (remitted  versus  depressed)  for  facial  movement  dynamics  was  higher  than  that  for  head  movement  dynamics;  and  each  was  substantially  higher  than  that  for  vocal  prosody.  Accuracy  for  all  three  modalities  together  reached  88.93  %,  exceeding  that  for  any  single  modality  or  pair  of  modalities.  These  findings  suggest  that  automatic  detection  of  depression  from  behavioral  indicators  is  feasible  and  that  multimodal  measures  afford  most  powerful  detection.
2	Quantification  of  cinematography  semiotics  for  video  based  facial  emotion  recognition  in  the  emotiw  2015  grand  challenge.  The  Emotion  Recognition  in  the  Wild  challenge  poses  significant  problems  to  state  of  the  art  auditory  and  visual  affect  quantification  systems.  To  overcome  the  challenges,  we  investigate  supplementary  meta  features  based  on  film  semiotics.  Movie  scenes  are  often  presented  and  arranged  in  such  a  way  as  to  amplify  the  emotion  interpreted  by  the  viewing  audience.  This  technique  is  referred  to  as  mise  en  scene  in  the  film  industry  and  involves  strict  and  intentional  control  of  color  palette,  light  source  color,  and  arrangement  of  actors  and  objects  in  the  scene.  To  this  end,  two  algorithms  for  extracting  mise  en  scene  information  are  proposed.  Rule  of  thirds  based  motion  history  histograms  detect  motion  along  rule  of  thirds  guidelines.  Rule  of  thirds  color  layout  descriptors  compactly  describe  a  scene  at  rule  of  thirds  intersections.  A  comprehensive  system  is  proposed  that  measures  expression,  emotion,  vocalics,  syntax,  semantics,  and  film-based  meta  information.  The  proposed  mise  en  scene  features  have  a  higher  classification  rate  and  ROC  area  than  LBP-TOP  features  on  the  validation  set  of  the  EmotiW  2015  challenge.  The  complete  system  improves  classification  performance  over  the  baseline  algorithm  by  3.17%  on  the  testing  set.
2	How  to  shape  the  humor  of  a  robot  social  behavior  adaptation  based  on  reinforcement  learning.  A  shared  sense  of  humor  can  result  in  positive  feelings  associated  with  amusement,  laughter,  and  moments  of  bonding.  If  robotic  companions  could  acquire  their  human  counterparts'  sense  of  humor  in  an  unobtrusive  manner,  they  could  improve  their  skills  of  engagement.  In  order  to  explore  this  assumption,  we  have  developed  a  dynamic  user  modeling  approach  based  on  Reinforcement  Learning,  which  allows  a  robot  to  analyze  a  person's  reaction  while  it  tells  jokes  and  continuously  adapts  its  sense  of  humor.  We  evaluated  our  approach  in  a  test  scenario  with  a  Reeti  robot  acting  as  an  entertainer  and  telling  different  types  of  jokes.  The  exemplary  adaptation  process  is  accomplished  only  by  using  the  audience's  vocal  laughs  and  visual  smiles,  but  no  other  form  of  explicit  feedback.  We  report  on  results  of  a  user  study  with  24  participants,  comparing  our  approach  to  a  baseline  condition  (with  a  non-learning  version  of  the  robot)  and  conclude  by  providing  limitations  and  implications  of  our  approach  in  detail.
2	Benefits  of  subliminal  feedback  loops  in  human  computer  interaction.  A  lot  of  efforts  have  been  directed  to  enriching  human-computer  interaction  to  make  the  user  experiencemore  pleasing  or  efficient.  In  this  paper,  we  briefly  present  work  in  the  fields  of  subliminal  perception  and  affective  computing,  before  we  outline  a  new  approach  to  add  analog  communication  channels  to  the  human-computer  interaction  experience.  In  this  approach,  in  addition  to  symbolic  predefined  mappings  of  input  to  output,  a  subliminal  feedback  loop  is  used  that  provides  feedback  in  evolutionary  subliminal  steps.  In  two  studies  involving  concentration-intensive  games,  we  investigated  the  impact  of  this  approach.  In  a  first  study  evolutionary  feedback  loops  adjusted  the  user  interface  of  a  memory  game  whereas  in  the  second  study  the  lighting  of  the  test  room  was  adjusted  dynamically.  The  results  show  that  in  settings  with  an  evolutionary  feedback  loop  test  participants  were  able  to  reach  significantly  higher  scores  compared  to  the  static  counterparts.  Finally,  we  discuss  the  impact  that  such  subliminally  working  applications  might  have  on  the  user's  acceptance.
2	Lightbeam  interacting  with  augmented  real  world  objects  in  pico  projections.  Pico  projectors  have  lately  been  investigated  as  mobile  display  and  interaction  devices.  We  propose  to  use  them  as  'light  beams':  Everyday  objects  sojourning  in  a  beam  are  turned  into  dedicated  projection  surfaces  and  tangible  interaction  devices.  This  way,  our  daily  surroundings  get  populated  with  interactive  objects,  each  one  temporarily  chartered  with  a  dedicated  sub-issue  of  pervasive  interaction.  While  interaction  with  objects  has  been  studied  in  larger,  immersive  projection  spaces,  the  affordances  of  pico  projections  are  fundamentally  different:  they  have  a  very  small,  strictly  limited  field  of  projection,  and  they  are  mobile.  This  paper  contributes  the  results  of  an  exploratory  field  study  on  how  people  interact  with  everyday  objects  in  pico  projections  in  nomadic  settings.  Based  upon  these  results,  we  present  novel  interaction  techniques  that  leverage  the  limited  field  of  projection  and  trade-off  between  digitally  augmented  and  traditional  uses  of  everyday  objects.
2	Is  this  you  identifying  a  mobile  user  using  only  diagnostic  features.  Mobile  smart  phones  capture  a  great  amount  of  information  about  a  user  across  a  variety  of  different  data  domains.  This  information  can  be  sensitive  and  allow  for  identifying  a  user  profile,  thus  causing  potential  threats  to  a  user's  privacy.  Our  work  shows  that  diagnostic  information  that  is  not  considered  sensitive,  could  be  used  to  identify  a  user  after  just  three  consecutive  days  of  monitoring.  We  have  used  the  Device  Analyzer  dataset  to  determine  what  features  of  a  mobile  device  are  important  in  identifying  a  user.      Many  mobile  games  and  applications  collect  diagnostic  data  as  a  means  of  identifying  or  resolving  issues.  Diagnostic  data  is  commonly  accepted  as  less  sensitive  information.  Our  experimental  results  demonstrate  that  using  only  diagnostic  features  like  hardware  statistics  and  system  settings,  a  user's  device  can  be  identified  at  an  accuracy  of  94%  with  a  Naive  Bayes  classifier.
2	Designing  a  framework  to  support  the  development  of  smart  cross  device  applications.  We  live  surrounded  by  computing  devices,  but  applications  are  mostly  confined  to  run  on  a  single  device.  It  should  be  possible  to  make  better  use  of  the  multiple  devices  around  us  by  coming  up  with  ways  of  integrating  and  combining  them,  while  leveraging  the  specific  strengths  of  some  devices  and  minimizing  the  individual  weaknesses  of  others.  We  want  to  explore  the  possibility  of  building  applications  that  have  their  user  interface  seamlessly  distributed  across  co-located  devices.  We  created  the  YanuX  Framework  to  provide  the  guidelines  and  tools  needed  by  developers  to  build  those  applications.  This  paper  presents  the  framework  and  its  architecture,  which  is  outlined  along  with  the  description  of  its  components.  We  end  by  presenting  an  early  evaluation  of  YanuX  and  by  discussing  the  status  of  our  work  along  with  directions  for  further  research.
2	Improving  generalization  ability  in  a  puzzle  game  using  reinforcement  learning.  Nowadays  machine  learning  has  attracted  much  attention.  In  order  to  apply  it  to  various  problems  without  relearning,  its  generalization  ability  is  needed.  Geometry  Friends  is  a  puzzle  game  where  a  player  has  to  collect  all  targets  in  a  two-dimensional  world,  and  it  is  used  in  some  artificial  intel­ligence  competitions.  Although  sufficient  generalization  ability  is  needed  to  apply  the  machine  learning  to  this  game,  such  machine  learning  methods  are  not  proposed  yet.  In  this  paper,  we  propose  a  method  based  on  reinforcement  learning  in  which  the  generalization  ability  is  improved  for  Geometry  Friends.
2	Personalised  track  design  in  car  racing  games.  Real-time  adaptation  of  computer  games'  content  to  the  users'  skills  and  abilities  can  enhance  the  player's  engagement  and  immersion.  Understanding  of  the  user's  potential  while  playing  is  of  high  importance  in  order  to  allow  the  successful  procedural  generation  of  user-tailored  content.  We  investigate  how  player  models  can  be  created  in  car  racing  games.  Our  user  model  uses  a  combination  of  data  from  unobtrusive  sensors,  while  the  user  is  playing  a  car  racing  simulator.  It  extracts  features  through  machine  learning  techniques,  which  are  then  used  to  comprehend  the  user's  gameplay,  by  utilising  the  educational  theoretical  frameworks  of  the  Concept  of  Flow  and  Zone  of  Proximal  Development.  The  end  result  is  to  provide  at  a  next  stage  a  new  track  that  fits  to  the  user  needs,  which  aids  both  the  training  of  the  driver  and  their  engagement  in  the  game.  In  order  to  validate  that  the  system  is  designing  personalised  tracks,  we  associated  the  average  performance  from  41  users  that  played  the  game,  with  the  difficulty  factor  of  the  generated  track.  In  addition,  the  variation  in  paths  of  the  implemented  tracks  between  users  provides  a  good  indicator  for  the  suitability  of  the  system.
2	Changes  in  the  frequency  and  severity  of  hydrological  droughts  over  ethiopia  from  1960  to  2013.  Here  we  present  an  analysis  of  drought  occurrence  and  variability  in  Ethiopia,  based  on  the  monthly  precipitation  data  from  the  Climate  Research  Unit  (CRU-v3.22)  over  the  period  from  1960  to  2013.  The  drought  events  were  characterized  by  means  of  the  Standardized  Precipitation  Index  (SPI)  applied  to  precipitation  data  at  a  temporal  scale  of  12  months.  At  the  national  scale,  the  results  reveal  a  statistically  significant  decrease  in  the  severity  of  droughts  over  the  54-year  period,  a  pattern  that  is  mostly  attributed  to  a  statistically  significant  decrease  in  the  frequency  of  high  intensity  drought  episodes  (i.e.,  extreme  and  very  extreme  droughts),  compared  to  moderate  droughts.  To  assess  the  general  patterns  of  drought  evolution,  a  principal  component  analysis  (PCA)  was  applied  to  the  SPI  series.  PCA  results  indicate  a  high  spatial  heterogeneity  in  the  SPI  variations  over  the  investigated  period,  with  ten  different  spatially  well-defined  regions  identified.  These  PCA  components  accounted  for  72.9%  of  the  total  variance  of  drought  in  the  region.  These  regions  also  showed  considerable  differences  in  the  temporal  variability  of  drought,  as  most  of  the  regions  exhibited  an  increase  in  wetness  conditions  in  recent  decades.  In  contrast,  the  regions  that  receive  less  than  400  mm  of  annual  precipitation  showed  a  declining   trend,  with  the  largest  changes  occurring  over  Afar  region.  Generally,  the  highly  elevated  regions  over  the  central  Ethiopian  Highlands  showed  the  weakest  changes,  compared  to  the  lowlands.  This  study  confirms  the  local  character  of  drought  evolution  over  Ethiopia,  providing  evidence  for  policy  makers  to  adopt  appropriate  local  policies  to  cope  with  the  risks  of  drought.  Over  Ethiopia,  the  detailed  spatial  assessment  of  drought  evolution  is  required  for  a  better  understanding  of  the  possible  impacts  of  recurrent  drought  on  agriculture,  food  production,  soil  degradation,  human  settlements  and  migrations,  as  well  as  energy  production  and  water  resources  management  across  Ethiopia.
2	Glacial  chronology  of  the  sierra  nevada  california  from  the  last  glacial  maximum  to  the  holocene.  During  the  Last  Glacial  Maximum  the  Sierra  Nevada  in  California,  USA,  supported  a  mountain  glacier/ice  cap  complex  that  covered  over  20,000  km  2  .  The  history  of  this  ice  cover  can  be  reconstructed  using    14  C  and  cosmogenic-nuclide  surface-exposure  dating.  These  show  that  the  glaciers  reached  their  maximum  extent  for  the  last  glacial  cycle  between  21  and  18  ka,  i.e.,  during  the  global  Last  Glacial  Maximum.  This  is  termed  the  Tioga  3  advance.  A  slow  retreat  began  at  18  ka  and  accelerated  rapidly  at  about  17  ka.  After  retreating  an  unknown  distance,  the  glaciers  began  to  readvance  at  about  16.7  ka,  reaching  the  Tioga  4  limit  at  16.2  ka.  They  then  rapidly  retreated  to  the  crest  of  the  range,  probably  within  500  to  1000  years.  There  is  no  indication  of  subsequent  glacial  expansion  until  the  Recess  Peak  advance  between  14.0  and  12.5  ka.  Unfortunately,  chronological  control  is  not  adequate  to  determine  whether  this  advance  was  during  the  early  Younger  Dryas  or  slightly  preceded  it.  The  equilibrium-line-altitude  reduction  during  the  Tioga  3  was  about  1200  m,  that  during  the  Tioga  4  about  800  m,  and  during  the  Recess  Peak  100  to  200  m.  The  Tioga  4  advance  coincided  with  the  expansion  of  nearby  pluvial  Lake  Lahontan  to  its  maximum  size.  The  Sierra  Nevada  advances  correlate  well  with  the  glacial  chronology  of  the  Alps  during  the  same  period,  and  also  with  the  episodes  of  melting  and  advance  of  the  European  and  Laurentide  Ice  Sheets.  Times  of  glacial  advance  in  the  Sierra  Nevada  may  be  connected  to  the  melting  history  of  the  ice  sheets,  and  to  Heinrich  events,  by  expansion  and  contraction  of  sea  ice  in  the  southern  North  Atlantic.
2	Long  term  drought  variability  and  trends  in  barcelona  1787  2014.  Long-term  drought  variability  and  trends  were  assessed  in  Barcelona  at  annual  and  seasonal  scale  for  the  period  1787-2014  and  sub-periods  1851-2014,  1901-2014  and  1951-2014  to  identify  changes  in  drought  patterns  across  time.  High  quality  and  adjusted  monthly  temperature  and  precipitation  series  were  required  for  this  purpose.  The  Standardized  Precipitation  Index  (SPI),  based  on  precipitation,  and  the  Standardized  Precipitation  Evapotranspiration  Index  (SPEI),  based  on  the  difference  between  precipitation  and  reference  evapotranspiration  (ET0),  were  calculated  to  describe  temporal  drought  fluctuations.  Therefore,  major  droughts  and  wet  events  were  identified  and  an  accurate  analysis  of  drought  severity,  magnitude  and  duration  were  also  carried  out.    Both  drought  indices  provided  similar  results  related  to  drought  variability  and  trends  in  Barcelona  across  time,  although  the  SPEI  showed  larger  drought  severity  than  SPI  especially  during  the  second  half  of  the  20th  century.  Trends  analysis  revealed  a  significant  drying  trend  at  annual  scale  according  to  the  SPEI  since  mid-19th  century  while  the  SPI  did  not  show  changes  in  drought  patterns.  At  seasonal  scale,  both  the  SPI  and  SPEI  found  a  clear  drying  trend  only  for  summer  (JJA)  during  the  current  period  (1951-2014),  although  the  SPEI  was  indicating  the  trend  towards  drier  conditions  for  the  whole  period  (1787-2014).  Drought  severity  in  SPEI  series  increased  13%  during  the  second  half  of  the  20th  century  compared  with  the  whole  period  under  study  while  drought  magnitude  and  duration  did  not  present  significant  changes  in  both  the  SPI  and  SPEI  series.  The  increasing  atmospheric  evaporative  demand  associated  to  the  large  temperature  rising  experienced  in  Barcelona  during  the  last  decades  could  have  played  a  substantial  role  in  explaining  the  increase  of  drought  severity  and  trends  found  in  the  SPEI  series.
2	Ut  2  human  like  behavior  via  neuroevolution  of  combat  behavior  and  replay  of  human  traces.  The  UT⁁2  bot,  which  had  a  humanness  rating  of  27.2727%  in  BotPrize  2010,  is  based  on  two  core  ideas:  (1)  multiobjective  neuroevolution  is  used  to  learn  skilled  combat  behavior,  but  filters  on  the  available  combat  actions  ensure  that  the  behavior  is  still  human-like  despite  being  evolved  for  performance,  and  (2)  a  database  of  traces  of  human  play  is  used  to  help  the  bot  get  unstuck  when  its  navigation  capabilities  fail.  Several  changes  have  recently  been  made  to  UT⁁2:  Extra  input  features  have  been  provided  to  the  bot  to  help  it  evolve  better  combat  behavior,  the  role  of  human  traces  in  the  navigation  of  the  bot  has  been  expanded,  and  an  extra  control  module  has  been  added  which  encourages  the  bot  to  observe  other  players  the  way  a  human  would,  rather  than  simply  battle  them.  These  changes  should  make  UT⁁2  act  more  human-like  in  this  year's  BotPrize  competition.
2	The  nash  and  the  bandit  approaches  for  adversarial  portfolios.  In  this  paper  we  study  the  use  of  a  portfolio  of  policies  for  adversarial  problems.  We  use  two  different  portfolios  of  policies  and  apply  it  to  the  game  of  Go.  The  first  portfolio  is  composed  of  different  version  of  the  GnuGo  agent.  The  second  portfolio  is  composed  of  fixed  random  seeds.  First  we  demonstrate  that  learning  an  offline  combination  of  these  policies  using  the  notion  of  Nash  Equilibrium  generates  a  stronger  opponent.  Second,  we  show  that  we  can  learn  online  such  distributions  through  a  bandit  approach.  The  advantages  of  our  approach  are  (i)  diversity  (the  Nash-Portfolio  is  more  variable  than  its  components)  (ii)  adaptivity  (the  Bandit-Portfolio  adapts  to  the  opponent)  (iii)  simplicity  (no  computational  overhead)  (iv)  increased  performance.  Due  to  the  importance  of  games  on  mobile  devices,  designing  artificial  intelligences  for  small  computational  power  is  crucial;  our  approach  is  particularly  suited  for  mobile  device  since  it  create  a  stronger  opponent  simply  by  biaising  the  distribution  over  the  policies  and  moreover  it  generalizes  quite  well.
2	Mental  rotation  of  directional  tactile  stimuli.  Several  researchers  have  developed  haptic  devices  capable  of  rendering  directional  stimuli.  When  these  devices  are  integrated  into  mobile  or  handheld  devices,  it  becomes  possible  for  a  user  to  hold  the  haptic  device  in  any  orientation  and  thereby  receive  directional  stimuli  that  may  be  out  of  alignment  with  rest  of  the  world.  In  such  cases,  it  becomes  necessary  for  the  user  to  perform  a  mental  transformation  of  the  directional  stimuli,  so  that  the  stimuli  may  be  understood  in  a  fixed  or  global  reference  frame.  This  paper  addresses  two  questions:  1.  can  users  perform  such  transformations  and  successfully  interpret  stimuli,  and  2.  what  cognitive  processes  are  involved  in  these  transformations?  In  our  experiments,  users  performed  timed  identification  of  directional  tactile  stimuli  with  their  hand  in  a  variety  of  orientations  around  a  single  axis.  The  results  show  that:  1.  users  can  successfully  identify  directional  stimuli  both  quickly  and  accurately,  even  when  the  stimuli  are  rendered  in  a  rotated  reference  frame,  and  2.  these  tasks  involve  the  mental  rotation  of  a  spatial  mental  representation  of  the  stimulus,  and  also  show  evidence  of  embodiment  effects.  Furthermore,  small  angles  of  rotation  (up  to  ∼40°)  incur  very  little  cognitive  cost,  suggesting  that  tactile  direction  stimuli  delivered  through  a  handheld  device  would  be  robust  to  variations  in  user  hand  orientation.
2	The  influence  of  base  rates  on  correlations  an  evaluation  of  proposed  alternative  effect  sizes  with  real  world  data.  Correlations  are  the  simplest  and  most  commonly  understood  effect  size  statistic  in  psychology.  The  purpose  of  the  current  paper  was  to  use  a  large  sample  of  real-world  data  (109  correlations  with  60,415  participants)  to  illustrate  the  base  rate  dependence  of  correlations  when  applied  to  dichotomous  or  ordinal  data.  Specifically,  we  examined  the  influence  of  the  base  rate  on  different  effect  size  metrics.  Correlations  decreased  when  the  dichotomous  variable  did  not  have  a  50  %  base  rate.  The  higher  the  deviation  from  a  50  %  base  rate,  the  smaller  the  observed  Pearson’s  point-biserial  and  Kendall’s  tau  correlation  coefficients.  In  contrast,  the  relationship  between  base  rate  deviations  and  the  more  commonly  proposed  alternatives  (i.e.,  polychoric  correlation  coefficients,  AUCs,  Pearson/Thorndike  adjusted  correlations,  and  Cohen’s  d)  were  less  remarkable,  with  AUCs  being  most  robust  to  attenuation  due  to  base  rates.  In  other  words,  the  base  rate  makes  a  marked  difference  in  the  magnitude  of  the  correlation.  As  such,  when  using  dichotomous  data,  the  correlation  may  be  more  sensitive  to  base  rates  than  is  optimal  for  the  researcher’s  goals.  Given  the  magnitude  of  the  association  between  the  base  rate  and  point-biserial  correlations  (r  =  −.81)  and  Kendall’s  tau  (r  =  −.80),  we  recommend  that  AUCs,  Pearson/Thorndike  adjusted  correlations,  Cohen’s  d,  or  polychoric  correlations  should  be  considered  as  alternate  effect  size  statistics  in  many  contexts.
2	Concreteness  norms  for  1  659  french  words  relationships  with  other  psycholinguistic  variables  and  word  recognition  times.  Words  that  correspond  to  a  potential  sensory  experience—concrete  words—have  long  been  found  to  possess  a  processing  advantage  over  abstract  words  in  various  lexical  tasks.  We  collected  norms  of  concreteness  for  a  set  of  1,659  French  words,  together  with  other  psycholinguistic  norms  that  were  not  available  for  these  words—context  availability,  emotional  valence,  and  arousal—but  which  are  important  if  we  are  to  achieve  a  better  understanding  of  the  meaning  of  concreteness  effects.  We  then  investigated  the  relationships  of  concreteness  with  these  newly  collected  variables,  together  with  other  psycholinguistic  variables  that  were  already  available  for  this  set  of  words  (e.g.,  imageability,  age  of  acquisition,  and  sensory  experience  ratings).  Finally,  thanks  to  the  variety  of  psychological  norms  available  for  this  set  of  words,  we  decided  to  test  further  the  embodied  account  of  concreteness  effects  in  visual-word  recognition,  championed  by  Kousta,  Vigliocco,  Vinson,  Andrews,  and  Del  Campo  (Journal  of  Experimental  Psychology:  General,  140,  14–34,  2011).  Similarly,  we  investigated  the  influences  of  concreteness  in  three  word  recognition  tasks—lexical  decision,  progressive  demasking,  and  word  naming—using  a  multiple  regression  approach,  based  on  the  reaction  times  available  in  Chronolex  (Ferrand,  Brysbaert,  Keuleers,  New,  Bonin,  Meot,  Pallier,  Frontiers  in  Psychology,  2;  306,  2011).  The  norms  can  be  downloaded  as  supplementary  material  provided  with  this  article.
2	Psiturk  an  open  source  framework  for  conducting  replicable  behavioral  experiments  online.  Online  data  collection  has  begun  to  revolutionize  the  behavioral  sciences.  However,  conducting  carefully  controlled  behavioral  experiments  online  introduces  a  number  of  new  of  technical  and  scientific  challenges.  The  project  described  in  this  paper,  psiTurk,  is  an  open-source  platform  which  helps  researchers  develop  experiment  designs  which  can  be  conducted  over  the  Internet.  The  tool  primarily  interfaces  with  Amazon’s  Mechanical  Turk,  a  popular  crowd-sourcing  labor  market.  This  paper  describes  the  basic  architecture  of  the  system  and  introduces  new  users  to  the  overall  goals.  psiTurk  aims  to  reduce  the  technical  hurdles  for  researchers  developing  online  experiments  while  improving  the  transparency  and  collaborative  nature  of  the  behavioral  sciences.
2	Validating  mouse  tracking  how  design  factors  influence  action  dynamics  in  intertemporal  decision  making.  Mouse-tracking  is  an  increasingly  popular  process-tracing  method.  It  builds  on  the  assumption  that  the  continuity  of  cognitive  processing  leaks  into  the  continuity  of  mouse  movements.  Because  this  assumption  is  the  prerequisite  for  meaningful  reverse  inference,  it  is  an  important  question  whether  the  assumed  interaction  between  continuous  processing  and  movement  might  be  influenced  by  the  methodological  setup  of  the  measurement.  Here  we  studied  the  impacts  of  three  commonly  occurring  methodological  variations  on  the  quality  of  mouse-tracking  measures,  and  hence,  on  the  reported  cognitive  effects.  We  used  a  mouse-tracking  version  of  a  classical  intertemporal  choice  task  that  had  previously  been  used  to  examine  the  dynamics  of  temporal  discounting  and  the  date-delay  effect  (Dshemuchadse,  Scherbaum,  &  Goschke,  2013).  The  data  from  this  previous  study  also  served  as  a  benchmark  condition  in  our  experimental  design.  Between  studies,  we  varied  the  starting  procedure.  Within  the  new  study,  we  varied  the  response  procedure  and  the  stimulus  position.  The  starting  procedure  had  the  strongest  influence  on  common  mouse-tracking  measures,  and  therefore  on  the  cognitive  effects.  The  effects  of  the  response  procedure  and  the  stimulus  position  were  weaker  and  less  pronounced.  The  results  suggest  that  the  methodological  setup  crucially  influences  the  interaction  between  continuous  processing  and  mouse  movement.  We  conclude  that  the  methodological  setup  is  of  high  importance  for  the  validity  of  mouse-tracking  as  a  process-tracing  method.  Finally,  we  discuss  the  need  for  standardized  mouse-tracking  setups,  for  which  we  provide  recommendations,  and  present  two  promising  lines  of  research  toward  obtaining  an  evidence-based  gold  standard  of  mouse-tracking.
2	Testing  the  construct  validity  of  competing  measurement  approaches  to  probed  mind  wandering  reports.  Psychology  faces  a  measurement  crisis,  and  mind-wandering  research  is  not  immune.  The  present  study  explored  the  construct  validity  of  probed  mind-wandering  reports  (i.e.,  reports  of  task-unrelated  thought  [TUT])  with  a  combined  experimental  and  individual-differences  approach.  We  examined  laboratory  data  from  over  1000  undergraduates  at  two  U.S.  institutions,  who  responded  to  one  of  four  different  thought-probe  types  across  two  cognitive  tasks.  We  asked  a  fundamental  measurement  question:  Do  different  probe  types  yield  different  results,  either  in  terms  of  average  reports  (average  TUT  rates,  TUT-report  confidence  ratings),  or  in  terms  of  TUT-report  associations,  such  as  TUT  rate  or  confidence  stability  across  tasks,  or  between  TUT  reports  and  other  consciousness-related  constructs  (retrospective  mind-wandering  ratings,  executive-control  performance,  and  broad  questionnaire  trait  assessments  of  distractibility-restlessness  and  positive-constructive  daydreaming)?  Our  primary  analyses  compared  probes  that  asked  subjects  to  report  on  different  dimensions  of  experience:  TUT-content  probes  asked  about  what  they'd  been  mind-wandering  about,  TUT-intentionality  probes  asked  about  why  they  were  mind-wandering,  and  TUT-depth  probes  asked  about  the  extent  (on  a  rating  scale)  of  their  mind-wandering.  Our  secondary  analyses  compared  thought-content  probes  that  did  versus  didn't  offer  an  option  to  report  performance-evaluative  thoughts.  Our  findings  provide  some  "good  news"-that  some  mind-wandering  findings  are  robust  across  probing  methods-and  some  "bad  news"-that  some  findings  are  not  robust  across  methods  and  that  some  commonly  used  probing  methods  may  not  tell  us  what  we  think  they  do.  Our  results  lead  us  to  provisionally  recommend  content-report  probes  rather  than  intentionality-  or  depth-report  probes  for  most  mind-wandering  research.
2	Lexique  infra  grapheme  phoneme  phoneme  grapheme  regularity  consistency  and  other  sublexical  statistics  for  137  717  polysyllabic  french  words.  Psycholinguistic  research  has  shown  that  both  the  regularity  and  consistency  of  the  grapheme-phoneme  and  phoneme-grapheme  correspondences  impact  word  processing.  Lexique-Infra  is  a  new  database  providing  infra-lexical  statistics  for  137,717  French  words.  The  frequencies  of  the  grapheme-phoneme  and  phoneme-grapheme  correspondences  as  well  as  other  indicators  (consistency,  regularity,  letter  frequencies,  bigrams,  trigrams,  phonemes,  biphones,  and  syllables,  etc.)  are  proposed  and  have  been  computed  from  the  corpus  of  subtitles  in  Lexique  3.83.  The  aim  of  this  new  database  is  to  propose  numerous  infra-lexical  variables  based  on  adult  frequencies  for  a  large  number  of  words.
2	Series  elasticity  for  free  free  space  motion  for  free.  Series  elastic  actuators  are  used  to  significant  advantage  in  many  robot  designs  but  have  not  found  their  way  into  the  design  of  haptic  devices.  We  use  a  pneumatic  circuit  to  realize  both  a  flexible  power  transmission  as  well  as  the  elastic  element  in  a  series  elastic  actuator.  The  pneumatic  circuit  effectively  hides  the  impedance  of  a  high  friction,  high  mass  ball-screw  actuator,  while  a  low  friction,  low  mass  pneumatic  cylinder  is  used  at  the  end-effector.  We  offer  a  comparative  study  of  an  impedance-control  device,  admittance-control  device,  and  a  device  incorporating  a  series  elastic  actuator,  investigating  both  the  open-loop  and  the  closed-loop  impedance  displayed  to  the  user.  While  all  hardware  and  control  designs  offer  an  ability  to  shape  the  impedance  within  their  operational  bandwidths,  the  series  elastic  design  has  the  particular  advantage  of  low  impedance  (a  very  compliant  spring)  outside  of  that  bandwidth.  Thus,  a  haptic  device  featuring  series  elastic  actuation  is  capable  of  providing  both  the  low  impedances  required  in  free-space  and  the  high  impedance  required  for  rendering  stiff  virtual  walls.
2	D67  sensory  substitution  using  3  degree  of  freedom  tangential  and  normal  skin  deformation  feedback.  During  manual  interactions,  we  experience  both  kinesthetic  forces  and  tactile  sensations.  Friction  and  normal  force  between  the  fingerpads  and  the  tool/interaction  surfaces  cause  shear  and  normal  deformation  of  the  skin.  Capitalizing  on  this  observation,  we  designed  a  3-degree-of-freedom  (DoF)  tactile  device  that  is  grasped  by  a  user  and  can  render  both  tangential  skin  stretch  and  normal  deformation  on  the  skin  of  the  user's  fingerpads.  Tactile  feedback  from  the  device  is  delivered  in  a  manner  consistent  with  natural  tactile  cues  from  manual  interaction.  An  experiment  assessed  the  accuracy  with  which  users  can  locate  the  center  of  a  contoured  hole  on  a  virtual  surface.  The  task  was  completed  under  four  conditions:  the  cases  of  skin  deformation  and  force  feedback,  with  both  3-  and  1-DoF  feedback  in  each  case.  With  3-DoF  feedback,  users  located  the  hole  faster  and  more  accurately  than  with  1-DoF  feedback,  for  both  force  and  skin  deformation  feedback.  These  results  indicated  that  users  were  able  to  interpret  the  additional  DoF  cues  provided  by  our  3-DoF  tactile  device  to  improve  task  performance.
2	Multilevel  models  for  multiple  baseline  data  modeling  across  participant  variation  in  autocorrelation  and  residual  variance.  Multilevel  models  (MLM)  have  been  used  as  a  method  for  analyzing  multiple-baseline  single-case  data.  However,  some  concerns  can  be  raised  because  the  models  that  have  been  used  assume  that  the  Level-1  error  covariance  matrix  is  the  same  for  all  participants.  The  purpose  of  this  study  was  to  extend  the  application  of  MLM  of  single-case  data  in  order  to  accommodate  across-participant  variation  in  the  Level-1  residual  variance  and  autocorrelation.  This  more  general  model  was  then  used  in  the  analysis  of  single-case  data  sets  to  illustrate  the  method,  to  estimate  the  degree  to  which  the  autocorrelation  and  residual  variances  differed  across  participants,  and  to  examine  whether  inferences  about  treatment  effects  were  sensitive  to  whether  or  not  the  Level-1  error  covariance  matrix  was  allowed  to  vary  across  participants.  The  results  from  the  analyses  of  five  published  studies  showed  that  when  the  Level-1  error  covariance  matrix  was  allowed  to  vary  across  participants,  some  relatively  large  differences  in  autocorrelation  estimates  and  error  variance  estimates  emerged.  The  changes  in  modeling  the  variance  structure  did  not  change  the  conclusions  about  which  fixed  effects  were  statistically  significant  in  most  of  the  studies,  but  there  was  one  exception.  The  fit  indices  did  not  consistently  support  selecting  either  the  more  complex  covariance  structure,  which  allowed  the  covariance  parameters  to  vary  across  participants,  or  the  simpler  covariance  structure.  Given  the  uncertainty  in  model  specification  that  may  arise  when  modeling  single-case  data,  researchers  should  consider  conducting  sensitivity  analyses  to  examine  the  degree  to  which  their  conclusions  are  sensitive  to  modeling  choices.
2	Rated  age  of  acquisition  norms  for  over  3  200  german  words.  Words  that  have  been  learned  early  in  life  are  responded  to  faster  than  words  that  have  been  acquired  later.  Subjective  ratings  of  acquisition  ages  have  been  successfully  employed  to  study  the  effect  of  age  of  acquisition  (AoA).  Although  a  large  number  of  norms  exist  in  many  languages,  fewer  are  available  for  German.  Therefore,  subjective  AoA  ratings  for  3,259  German  words  were  collected  online,  including  2,363  nouns  and  473  verbs.  These  words  were  presented  in  lists  of  140  words,  and  participants  rated  the  age  in  years  at  which  they  had  first  learned  each  word.  A  split-half  correlation  testified  to  a  high  internal  reliability.  There  were  also  high  correlations  with  rated  AoA  values  for  subsets  of  the  items  that  had  been  collected  in  previous  studies,  in  both  German  and  English.  Age  and  gender  were  found  to  influence  the  ratings  very  weakly,  in  that  older  and  male  participants  tended  to  give  slightly  higher  age  ratings.  Education,  multilingualism,  and  frequent  usage  of  languages  other  than  German  did  not  exert  an  influence  on  the  rating  values.  These  new  ratings  will  extend  the  currently  existing  norms  available  for  language  and  reading  research  across  languages  and  will  provide  researchers  with  a  wider  choice  of  word  stimuli.  The  ratings  are  available  expressed  in  two  measurements:  age  in  years,  and  AoA  rated  on  a  7-point  Likert  scale.
2	Default  gunel  and  dickey  bayes  factors  for  contingency  tables.  The  analysis  of  R×C  contingency  tables  usually  features  a  test  for  independence  between  row  and  column  counts.  Throughout  the  social  sciences,  the  adequacy  of  the  independence  hypothesis  is  generally  evaluated  by  the  outcome  of  a  classical  p-value  null-hypothesis  significance  test.  Unfortunately,  however,  the  classical  p-value  comes  with  a  number  of  well-documented  drawbacks.  Here  we  outline  an  alternative,  Bayes  factor  method  to  quantify  the  evidence  for  and  against  the  hypothesis  of  independence  in  R×C  contingency  tables.  First  we  describe  different  sampling  models  for  contingency  tables  and  provide  the  corresponding  default  Bayes  factors  as  originally  developed  by  Gunel  and  Dickey  (Biometrika,  61(3):545–557  (1974)).  We  then  illustrate  the  properties  and  advantages  of  a  Bayes  factor  analysis  of  contingency  tables  through  simulations  and  practical  examples.  Computer  code  is  available  online  and  has  been  incorporated  in  the  “BayesFactor”  R  package  and  the  JASP  program  (jasp-stats.org).
2	A  computerized  multidimensional  measurement  of  mental  workload  via  handwriting  analysis.  The  goal  of  this  study  was  to  test  the  effect  of  mental  workload  on  handwriting  behavior  and  to  identify  characteristics  of  low  versus  high  mental  workload  in  handwriting.  We  hypothesized  differences  between  handwriting  under  three  different  load  conditions  and  tried  to  establish  a  profile  that  integrated  these  indicators.  Fifty-six  participants  wrote  three  numerical  progressions  of  varying  difficulty  on  a  digitizer  attached  to  a  computer  so  that  we  could  evaluate  their  handwriting  behavior.  Differences  were  found  in  temporal,  spatial,  and  angular  velocity  handwriting  measures,  but  no  significant  differences  were  found  for  pressure  measures.  Using  data  reduction,  we  identified  three  clusters  of  handwriting,  two  of  which  differentiated  well  according  to  the  three  mental  workload  conditions.  We  concluded  that  handwriting  behavior  is  affected  by  mental  workload  and  that  each  measure  provides  distinct  information,  so  that  they  present  a  comprehensive  indicator  of  mental  workload.
2	Randomized  single  case  ab  phase  designs  prospects  and  pitfalls.  Single-case  experimental  designs  (SCEDs)  are  increasingly  used  in  fields  such  as  clinical  psychology  and  educational  psychology  for  the  evaluation  of  treatments  and  interventions  in  individual  participants.  The  AB  phase  design,  also  known  as  the  interrupted  time  series  design,  is  one  of  the  most  basic  SCEDs  used  in  practice.  Randomization  can  be  included  in  this  design  by  randomly  determining  the  start  point  of  the  intervention.  In  this  article,  we  first  introduce  this  randomized  AB  phase  design  and  review  its  advantages  and  disadvantages.  Second,  we  present  some  data-analytical  possibilities  and  pitfalls  related  to  this  design  and  show  how  the  use  of  randomization  tests  can  mitigate  or  remedy  some  of  these  pitfalls.  Third,  we  demonstrate  that  the  Type  I  error  of  randomization  tests  in  randomized  AB  phase  designs  is  under  control  in  the  presence  of  unexpected  linear  trends  in  the  data.  Fourth,  we  report  the  results  of  a  simulation  study  investigating  the  effect  of  unexpected  linear  trends  on  the  power  of  the  randomization  test  in  randomized  AB  phase  designs.  The  implications  of  these  results  for  the  analysis  of  randomized  AB  phase  designs  are  discussed.  We  conclude  that  randomized  AB  phase  designs  are  experimentally  valid,  but  that  the  power  of  these  designs  is  sufficient  only  for  large  treatment  effects  and  large  sample  sizes.  For  small  treatment  effects  and  small  sample  sizes,  researchers  should  turn  to  more  complex  phase  designs,  such  as  randomized  ABAB  phase  designs  or  randomized  multiple-baseline  designs.
2	Multiple  moderator  meta  analysis  using  the  r  package  meta  cart.  In  meta-analysis,  heterogeneity  often  exists  between  studies.  Knowledge  about  study  features  (i.e.,  moderators)  that  can  explain  the  heterogeneity  in  effect  sizes  can  be  useful  for  researchers  to  assess  the  effectiveness  of  existing  interventions  and  design  new  potentially  effective  interventions.  When  there  are  multiple  moderators,  they  may  amplify  or  attenuate  each  other's  effect  on  treatment  effectiveness.  However,  in  most  meta-analysis  studies,  interaction  effects  are  neglected  due  to  the  lack  of  appropriate  methods.  The  method  meta-CART  was  recently  proposed  to  identify  interactions  between  multiple  moderators.  The  analysis  result  is  a  tree  model  in  which  the  studies  are  partitioned  into  more  homogeneous  subgroups  by  combinations  of  moderators.  This  paper  describes  the  R-package  metacart,  which  provides  user-friendly  functions  to  conduct  meta-CART  analyses  in  R.  This  package  can  fit  both  fixed-  and  random-effects  meta-CART,  and  can  handle  dichotomous,  categorical,  ordinal  and  continuous  moderators.  In  addition,  a  new  look  ahead  procedure  is  presented.  The  application  of  the  package  is  illustrated  step-by-step  using  diverse  examples.
2	Examining  the  effects  of  probe  frequency  response  options  and  framing  within  the  thought  probe  method.  A  recent  surge  of  interest  in  the  empirical  measurement  of  mind-wandering  has  led  to  an  increase  in  the  use  of  thought-probing  to  measure  attentional  states,  which  has  led  to  large  variation  in  methodologies  across  studies  (Weinstein  in  Behavior  Research  Methods,  50,  642-661,  2018).  Three  sources  of  variation  in  methodology  include  the  frequency  of  thought  probes  during  a  task,  the  number  of  response  options  provided  for  each  probe,  and  the  way  in  which  various  attentional  states  are  framed  during  the  task  instructions.  Method  variation  can  potentially  affect  behavioral  performance  on  the  tasks  in  which  thought  probes  are  embedded,  the  experience  of  various  attentional  states  within  those  tasks,  and/or  response  biases  to  the  thought  probes.  Therefore,  such  variation  can  be  problematic,  both  pragmatically  and  theoretically.  Across  three  experiments,  we  examined  how  manipulating  probe  frequency,  response  options,  and  framing  affected  behavioral  performance  and  responses  to  thought  probes.  Probe  frequency  and  framing  did  not  affect  behavioral  performance  or  probe  responses.  But,  in  light  of  the  present  results,  we  argue  that  thought  probes  need  at  least  three  responses,  corresponding  to  on-task,  off-task,  and  task-related  interference.  When  researchers  are  specifically  investigating  mind-wandering,  the  probe  responses  should  also  distinguish  between  mind-wandering,  external  distraction,  and  mind-blanking.
2	Defining  eye  fixation  sequences  across  individuals  and  tasks  the  binocular  individual  threshold  bit  algorithm.  We  propose  a  new  fully  automated  velocity-based  algorithm  to  identify  fixations  from  eye-movement  records  of  both  eyes,  with  individual-specific  thresholds.  The  algorithm  is  based  on  robust  minimum  determinant  covariance  estimators  (MDC)  and  control  chart  procedures,  and  is  conceptually  simple  and  computationally  attractive.  To  determine  fixations,  it  uses  velocity  thresholds  based  on  the  natural  within-fixation  variability  of  both  eyes.  It  improves  over  existing  approaches  by  automatically  identifying  fixation  thresholds  that  are  specific  to  (a)  both  eyes,  (b)  x-  and  y-  directions,  (c)  tasks,  and  (d)  individuals.  We  applied  the  proposed  Binocular-Individual  Threshold  (BIT)  algorithm  to  two  large  datasets  collected  on  eye-trackers  with  different  sampling  frequencies,  and  compute  descriptive  statistics  of  fixations  for  larger  samples  of  individuals  across  a  variety  of  tasks,  including  reading,  scene  viewing,  and  search  on  supermarket  shelves.  Our  analysis  shows  that  there  are  considerable  differences  in  the  characteristics  of  fixations  not  only  between  these  tasks,  but  also  between  individuals.
2	Qualitative  tests  of  remote  eyetracker  recovery  and  performance  during  head  rotation.  What  are  the  decision  criteria  for  choosing  an  eyetracker?  Often  the  choice  is  based  on  specifications  by  the  manufacturer  of  the  validity  (accuracy)  and  reliability  (precision)  of  measurements  that  can  be  achieved  using  a  particular  eyetracker.  These  specifications  are  mostly  achieved  under  optimal  conditions—for  example,  by  using  an  artificial  eye  or  trained  participants  fixed  in  a  chinrest.  Research,  however,  does  not  always  take  place  in  optimal  conditions:  For  instance,  when  investigating  eye  movements  in  infants,  school  children,  and  patient  groups  with  disorders  such  as  attention-deficit  hyperactivity  disorder,  it  is  practically  impossible  to  restrict  movement.  We  modeled  movements  often  seen  in  infant  research  in  two  behaviors:  (1)  looking  away  from  and  back  to  the  screen,  to  investigate  eyetracker  recovery,  and  (2)  head  orientations,  to  investigate  eyetracker  performance  with  nonoptimal  orientations  of  the  eyes.  We  investigated  how  eight  eyetracking  setups  by  three  manufacturers  (SMI,  Tobii,  and  LC  Technologies)  coped  with  these  modeled  behaviors  in  adults.  We  report  that  the  tested  SMI  eyetrackers  dropped  in  sampling  frequency  when  the  eyes  were  not  visible  to  the  eyetracker,  whereas  the  other  systems  did  not,  and  discuss  the  potential  consequences  thereof.  Furthermore,  we  report  that  the  tested  eyetrackers  varied  in  their  rates  of  data  loss  and  systematic  offsets  during  shifted  head  orientations.  We  conclude  that  (prospective)  eye-movement  researchers  who  cannot  restrict  movement  or  nonoptimal  head  orientations  in  their  participants  might  benefit  from  testing  their  eyetracker  in  nonoptimal  conditions.  Additionally,  researchers  should  be  aware  of  the  data  loss  and  inaccuracies  that  might  result  from  nonoptimal  head  orientations.
2	Rendering  mass  using  model  matching  framework.  This  paper  presents  a  model  matching  based  controller  design  technique  to  render  mass  using  a  haptic  interface.  The  main  hindrance  in  stably  rendering  mass  through  the  open  loop  impedance  control  is  the  noise  added  to  the  acceleration  signal  due  to  twice  differentiation  of  the  position  signal.  We  show  that  using  model  matching  framework  we  can  stably  and  accurately  render  desired  mass.  In  addition,  we  present  that  effects  of  inherent  dynamics  such  as  damping  present  in  the  system  can  be  minimized  using  the  model  matching  framework.  Further,  we  perform  experiments  on  a  single  degree  of  freedom  haptic  device  to  validate  our  claim.
2	Nailing  down  multi  touch  anchored  above  the  surface  interaction  for  3d  modeling  and  navigation.  We  present  anchored  multi-touch,  a  technique  for  extending  multi-touch  interfaces  by  using  gestures  based  on  both  multi-touch  surface  input  and  3D  movement  of  the  hand(s)  above  the  surface.  These  interactions  have  nearly  the  same  potential  for  rich,  expressive  input  as  do  freehand  3D  interactions  while  also  having  an  advantage  that  the  passive  haptic  feedback  provided  by  the  surface  makes  them  easier  to  control.  In  addition,  anchored  multi-touch  is  particularly  well  suited  for  working  with  3D  content  on  stereoscopic  displays.  This  paper  contributes  two  example  applications:  (1)  an  interface  for  navigating  3D  datasets,  and  (2)  a  surface  bending  interface  for  freeform  3D  modeling.  Two  methods  for  sensing  the  gestures  are  introduced,  one  employing  a  depth  camera.
2	Efficient  trajectory  extraction  and  parameter  learning  for  data  driven  crowd  simulation.  We  present  a  trajectory  extraction  and  behavior-learning  algorithm  for  data-driven  crowd  simulation.  Our  formulation  is  based  on  incrementally  learning  pedestrian  motion  models  and  behaviors  from  crowd  videos.  We  combine  this  learned  crowd-simulation  model  with  an  online  tracker  based  on  particle  filtering  to  compute  accurate,  smooth  pedestrian  trajectories.  We  refine  this  motion  model  using  an  optimization  technique  to  estimate  the  agents'  simulation  parameters.  We  highlight  the  benefits  of  our  approach  for  improved  data-driven  crowd  simulation,  including  crowd  replication  from  videos  and  merging  the  behavior  of  pedestrians  from  multiple  videos.  We  highlight  our  algorithm's  performance  in  various  test  scenarios  containing  tens  of  human-like  agents.
2	Interactive  steering  of  mesh  animations.  Creating  geometrically  detailed  mesh  animations  is  an  involved  and  resource-intensive  process  in  digital  content  creation.  In  this  work  we  present  a  method  to  rapidly  combine  available  sparse  motion  capture  data  with  existing  mesh  sequences  to  produce  a  large  variety  of  new  animations.  The  key  idea  is  to  model  shape  changes  correlated  to  the  pose  of  the  animated  object  via  a  part-based  statistical  shape  model.  We  observe  that  compact  linear  models  suffice  for  a  segmentation  into  nearly  rigid  parts.  The  same  segmentation  further  guides  the  parameterization  of  the  pose  which  is  learned  in  conjunction  with  the  marker  movement.  Besides  the  inherent  high  geometric  detail,  further  benefits  of  the  presented  method  arise  from  its  robustness  against  errors  in  segmentation  and  pose  parameterization.  Due  to  efficiency  of  both  learning  and  synthesis  phase,  our  model  allows  to  interactively  steer  virtual  avatars  based  on  few  markers  extracted  from  video  data  or  input  devices  like  the  Kinect  sensor.
2	6d  frictional  contact  for  rigid  bodies.  We  present  a  new  approach  to  modeling  contact  between  rigid  objects  that  augments  an  individual  Coulomb  friction  point-contact  model  with  rolling  and  spinning  friction  constraints.  Starting  from  the  intersection  volume,  we  compute  a  contact  normal  from  the  volume  gradient.  We  compute  a  contact  position  from  the  first  moment  of  the  intersection  volume,  and  approximate  the  extent  of  the  contact  patch  from  the  second  moment  of  the  intersection  volume.  By  incorporating  knowledge  of  the  contact  patch  into  a  point  contact  Coulomb  friction  formulation,  we  produce  a  6D  constraint  that  provides  appropriate  limits  on  torques  to  accommodate  displacement  of  the  center  of  pressure  within  the  contact  patch,  while  also  providing  a  rotational  torque  due  to  dry  friction  to  resist  spinning.  A  collection  of  examples  demonstrate  the  power  and  benefits  of  this  simple  formulation.
2	Annotation  based  video  enrichment  for  blind  people  a  pilot  study  on  the  use  of  earcons  and  speech  synthesis.  Our  approach  to  address  the  question  of  online  video  accessibility  for  people  with  sensory  disabilities  is  based  on  video  annotations  that  are  rendered  as  video  enrichments  during  the  playing  of  the  video.  We  present  an  exploratory  work  that  focuses  on  video  accessibility  for  blind  people  with  audio  enrichments  composed  of  speech  synthesis  and  earcons  (i.e.  nonverbal  audio  messages).  Our  main  results  are  that  earcons  can  be  used  together  with  speech  synthesis  to  enhance  understanding  of  videos;  that  earcons  should  be  accompanied  with  explanations;  and  that  a  potential  side  effect  of  earcons  is  related  to  video  rhythm  perception.
2	Design  recommendations  for  tv  user  interfaces  for  older  adults  findings  from  the  ecaalyx  project.  While  guidelines  for  designing  websites  and  iTV  applications  for  older  adults  exist,  no  previous  work  has  suggested  how  to  best  design  TV  user  interfaces  (UIs)  that  are  accessible  to  older  adults.  Building  upon  pertinent  guidelines  from  related  areas,  this  paper  presents  thirteen  recommendations  for  designing  UIs  for  TV  applications  for  older  adults.  These  recommendations  are  the  result  of  iterative  design,  testing,  and  development  of  a  TV-based  health  system  for  older  adults  that  aims  to  provide  a  holistic  solution  to  improve  quality  of  life  for  older  adults  with  chronic  conditions  by  fostering  their  autonomy  and  reducing  hospitalization  costs.  The  authors'  work  and  experience  shows  that  widely  known  UI  design  guidelines  unsurprisingly  apply  to  the  design  of  TV-based  applications  for  older  adults,  but  acquire  a  crucial  importance  in  this  context.
2	Value  beyond  function  analyzing  the  perception  of  wheelchair  innovations  in  kenya.  Innovations  in  the  field  of  assistive  technology  are  usually  evaluated  based  on  practical  considerations  related  to  their  ability  to  perform  certain  functions.  However,  social  and  emotional  aspects  play  a  huge  role  in  how  people  with  disabilities  interact  with  assistive  products  and  services.  Over  a  five  months  period,  we  tested  an  innovative  wheelchair  service  provision  model  that  leverages  3D  printing  and  Computer  Aided  Design  to  provide  bespoke  wheelchairs  in  Kenya.  The  study  involved  eight  expert  wheelchair  users  and  five  healthcare  professionals  who  routinely  provide  wheelchair  services  in  their  community.  Results  from  the  study  show  that  both  users  and  providers  attributed  great  value  to  both  the  novel  service  delivery  model  and  the  wheelchairs  produced  as  part  of  the  study.  The  reasons  for  their  appreciation  went  far  beyond  the  practical  considerations  and  were  rooted  in  the  fact  that  the  service  delivery  model  and  the  wheelchairs  promoted  core  values  of  agency,  empowerment  and  self-expression.
2	Designing  embodied  musical  interaction  for  children  with  autism.  This  paper  describes  the  design,  implementation,  and  pilot  evaluation  of  an  interface  to  support  embodied  musical  interaction  for  children  with  Autism  Spectrum  Conditions  (ASC),  in  the  context  of  music  therapy  sessions.  Previous  research  suggests  music  and  movement  therapies  are  powerful  tools  for  supporting  children  with  autism  in  their  development  of  communication,  expression,  and  motor  skills.  OSMoSIS  (Observation  of  Social  Motor  Synchrony  with  an  Interactive  System)  is  an  interactive  musical  system  which  tracks  body  movements  and  transforms  them  into  sounds  using  the  Microsoft  Kinect  motion  capture  system.  It  is  designed  so  that,  regardless  of  motor  abilities,  children  can  generate  sounds  by  moving  in  the  environment  either  freely  or  guided  by  a  facilitator.  OSMoSIS  was  inspired  by  the  author's  experiences  as  a  music  therapist  and  supports  observation  of  Social  Motor  Synchrony  to  allow  facilitators  and  researchers  to  record  and  investigate  this  aspect  of  the  therapy  sessions.  It  converts  movements  into  sounds  using  Microsoft  Kinect  body  tracking,  in  the  context  of  an  interactive  game.  From  our  preliminary  testing  with  11  children  with  autism  (aged  5  –  11  years  old),  we  observed  that  our  design  actively  connects  children,  who  displayed  a  notable  increase  in  engagement  and  interaction  when  the  system  was  used.
2	Tongue  able  interfaces  evaluating  techniques  for  a  camera  based  tongue  gesture  input  system.  Tongue-computer  interaction  techniques  create  a  new  pathway  between  the  human  and  the  computer,  with  particular  utility  for  people  with  upper  limb  impairment.  This  study  investigated  the  usability  problems  of  camera-based  tongue  computer  interface  reflected  through  the  user  behavior  and  participants'  feedback;  specifically  the  exploration  of  referential  techniques  to  make  users  aware  of  their  tongue  position  and  help  adjust  their  gesture.  Pros  and  cons  of  the  referential  strategies  are  discussed  to  foster  future  assistive  tongue-computer  interface  design.
2	Bypassing  lists  accelerating  screen  reader  fact  finding  with  guided  tours.  Navigating  back  and  forth  from  a  list  of  links  (index)  to  its  target  pages  is  common  on  the  web,  but  tethers  screen-reader  users  to  unnecessary  cognitive  and  mechanical  steps.  This  problem  worsens  when  indexes  lack  information  scent:  cues  that  enable  users  to  select  a  link  with  confidence  during  fact-finding.  This  paper  investigates  how  blind  users  who  navigate  the  web  with  screen-readers  can  bypass  a  scentless  index  with  guided  tours:  a  much  simpler  browsing  pattern  that  linearly  concatenates  items  of  a  collection.  In  a  controlled  study  (N=11)  at  the  Indiana  School  for  the  Blind  and  Visually  Impaired  (ISBVI),  guided  tours  lowered  user's  cognitive  effort  and  significantly  decreased  time-on-task  and  number  of  pages  visited  when  compared  to  an  index  with  poor  information  scent.  Our  findings  suggest  that  designers  can  supplement  indexes  with  guided  tours  to  benefit  screen-reader  users  in  a  variety  of  web  navigation  contexts.
2	Deaf  and  hard  of  hearing  individuals  perceptions  of  communication  with  hearing  colleagues  in  small  groups.  This  survey-based  study  investigated  deaf  and  hard  of  hearing  (DHH)  individuals'  perceived  need  for  technologies  that  may  facilitate  communication  when  meeting  in  small  groups  with  hearing  colleagues.  Participants  were  108  DHH  postsecondary  students  who  participated  in  co-op  (internship)  and  capstone  experiences  at  workplaces  with  hearing  employees  within  the  past  two  years.  Participants'  responses  to  a  survey  indicated  that  they  were  generally  not  satisfied  with  their  current  strategies  and  technologies  for  communicating  with  hearing  persons  in  small  groups.
2	Good  fonts  for  dyslexia.  Around  10%  of  the  people  have  dyslexia,  a  neurological  disability  that  impairs  a  person's  ability  to  read  and  write.  There  is  evidence  that  the  presentation  of  the  text  has  a  significant  effect  on  a  text's  accessibility  for  people  with  dyslexia.  However,  to  the  best  of  our  knowledge,  there  are  no  experiments  that  objectively  measure  the  impact  of  the  font  type  on  reading  performance.  In  this  paper,  we  present  the  first  experiment  that  uses  eye-tracking  to  measure  the  effect  of  font  type  on  reading  speed.  Using  a  within-subject  design,  48  subjects  with  dyslexia  read  12  texts  with  12  different  fonts.  Sans  serif,  monospaced  and  roman  font  styles  significantly  improved  the  reading  performance  over  serif,  proportional  and  italic  fonts.  On  the  basis  of  our  results,  we  present  a  set  of  more  accessible  fonts  for  people  with  dyslexia.
2	Introducing  web  accessibility  to  localization  students  implications  for  a  universal  web.  The  importance  of  web  accessibility  has  spread  throughout  close  technical  disciplines,  leading  to  new  forms  of  collaboration  between  that  area  of  study  and  other  related  fields,  such  as  internationalization  and  web  localization.  Recent  investigations  have  illustrated  that  web  accessibility  experts  support  the  involvement  of  localization  professionals  in  the  achievement  of  a  more  accessible  web  for  all,  especially  in  the  case  of  the  multilingual  web.  However,  most  training  institutions  do  not  teach  yet  the  basic  technical  competence  on  the  matter.  Within  such  research  framework,  over  the  last  two  years,  a  series  of  seminars  on  web  accessibility  have  been  taught  both  for  undergraduate  and  graduate  translation  students  at  two  European  universities.  The  relevance  of  acquiring  web  accessibility  knowledge  and  know  how  was  generally  welcomed  by  all  participants,  who  showed  a  high  level  of  interest  and  motivation.  Data  gathered  up  to  date  have  helped  to  develop  a  better  informed  theoretical  framework  about  the  participation  of  localizers  in  the  web  development  cycle  and  their  contribution  to  a  universal  web.
2	Tangibles  programming  audio  stories  fun.  Block-based  programming  languages  enable  novice  programmers,  including  children,  to  learn  the  basics  of  programming.  However,  most  block-based  programming  languages  are  not  accessible  to  blind  and  visually  impaired  users  because  they  rely  upon  visual  drag-and-drop  interaction,  and  because  they  typically  create  visual  output.  To  improve  access  to  block-based  programming  languages,  we  introduce  Story  Blocks,  a  programming  toolkit  that  uses  tangible  blocks  to  represent  story  components,  and  which  produces  output  in  the  form  of  accessible  audio  stories  and  games.  Story  Blocks  provides  an  introductory  programming  environment  that  can  be  enjoyed  by  people  of  all  abilities.
2	Adaptive  augmented  reality  plasticity  of  augmentations.  An  augmented  reality  system  is  used  to  complete  the  real  world  with  virtual  objects  (computer  generated)  so  they  seem  to  coexist  in  the  same  space  as  the  real  world.  The  concept  of  plasticity  [4][5]  was  first  introduced  for  Human  Computer  Interaction  (HCI).  It  denotes  the  ability  of  an  HCI  interface  to  fit  the  context  of  use  defined  by  the  user,  the  environment  and  the  platform.  We  believe  that  plasticity  is  a  very  important  notion  in  the  domain  of  augmented  reality.  Therefore,  we  rely  on  it  in  order  to  introduce  the  concept  of  adaptive  augmented  reality.  This  concept  is  based  on  the  triplet  (user,  environment  and  platform)  constituting  the  context  of  use.  Adaptive  augmented  reality  can  foster  functional  ability,  ease  of  use  and  portability  of  new  augmented  reality  applications.  Thus,  we  describe  in  this  paper  three  applications  showing  the  adaptation  of  augmentation  based  on  three  variables:  the  scene  illumination,  the  distance  to  the  target  and  the  ambient  noise.
2	Ergonomic  approach  for  pillow  concept  design.  Abstract      Sleep  quality  is  an  essential  factor  to  human  beings  for  health.  The  current  paper  conducted  four  studies  to  provide  a  suitable  pillow  for  promoting  sleep  quality.    Study  1  investigated  the  natural  positions  of  40  subjects  during  sleep  to  derive  key-points  for  a  pillow  design.  The  results  suggested  that  the  supine  and  lateral  positions  were  alternatively  24  times  a  night,  and  the  current  pillows  were  too  high  for  the  supine  position  and  too  low  for  lateral  positions.    Study  2  measured  body  dimensions  related  to  pillow  design  of  40  subjects  to  determine  pillow  sizes.  The  results  suggested  that  the  pillow  height  were  quite  different  in  supine  position  and  lateral  position  and  needed  to  take  into  consideration  for  a  pillow  design.    Study  3  created  a  pillow  design  based  on  the  results  of  above  studies.  The  pillow  was  a  U-form  in  the  front  of  view  in  which  the  pillow  height  in  the  middle  area  was  lower  for  the  supine  position,  and  both  sides  were  higher  for  the  lateral  positions.    Study  4  assessed  sleep  quality  of  6  subjects  by  using  the  proposed  pillows  and  the  current  pillows.  The  results  showed  that  the  newly  designed  pillow  led  to  significantly  higher  sleep  quality,  and  the  new  design  received  an  innovation  patent.
2	Work  activity  in  food  service  the  significance  of  customer  relations  tipping  practices  and  gender  for  preventing  musculoskeletal  disorders.  Abstract      Some  evidence  shows  that  food  servers  are  exposed  to  an  elevated  risk  of  musculoskeletal  disorders  and  injuries,  and  that  their  work  activity  varies  by  gender.  Interviews  of  servers  and  observations  of  food  service  in  Quebec,  Canada,  were  carried  out  in  three  restaurants  and  a  questionnaire  was  administered  to  64  workers  from  44  other  restaurants.  The  relationship  with  the  customer  has  specific  effects  on  work  activity  and  transforms  the  physical,  emotional  and  cognitive  work.  Strategies  intended  to  speed  service  or  otherwise  related  to  the  customer  relationship  can  involve  health  risks.  Women  reported  more  direct  food  service  (p
2	Associations  between  shift  schedule  characteristics  with  sleep  need  for  recovery  health  and  performance  measures  for  regular  semi  continuous  3  shift  systems.  In  this  cross-sectional  study  associations  were  examined  between  eight  shift  schedule  characteristics  with  shift-specific  sleep  complaints  and  need  for  recovery  and  generic  health  and  performance  measures.  It  was  hypothesized  that  shift  schedule  characteristics  meeting  ergonomic  recommendations  are  associated  with  better  sleep,  need  for  recovery,  health  and  performance.  Questionnaire  data  were  collected  from  491  shift  workers  of  18  companies  with  9  regular  (semi)-continuous  shift  schedules.  The  shift  schedule  characteristics  were  analyzed  separately  and  combined  using  multilevel  linear  regression  models.  The  hypothesis  was  largely  not  confirmed.  Relatively  few  associations  were  found,  of  which  the  majority  was  in  the  direction  as  expected.  In  particular  early  starts  of  morning  shifts  and  many  consecutive  shifts  seem  to  be  avoided.  The  healthy  worker  effect,  limited  variation  between  included  schedules  and  the  cross-sectional  design  might  explain  the  paucity  of  significant  results.
2	An  investigation  of  the  performance  of  novel  chorded  keyboards  in  combination  with  pointing  input  devices.  Rapid  advances  in  computing  power  have  driven  the  development  of  smaller  and  lighter  technology  products,  with  novel  input  devices  constantly  being  produced  in  response  to  new  user  behaviors  and  usage  contexts.  The  aim  of  this  research  was  to  investigate  the  feasibility  of  operating  chorded  keyboard  control  modules  in  concert  with  pointing  devices  such  as  styluses  and  mice.  We  compared  combinations  of  two  novel  chorded  keyboards  with  different  pointing  devices  in  hopes  of  finding  a  better  combination  for  future  electronic  products.  Twelve  participants  were  recruited  for  simulation  testing,  and  paired  sample  t  testing  was  conducted  to  determine  whether  input  and  error  rates  for  the  novel  keyboards  were  improved  significantly  over  those  of  traditional  input  methods.  The  most  efficient  input  device  combination  tested  was  the  combination  of  a  novel  cross-shaped  key  keyboard  and  a  stylus,  suggesting  the  high  potential  for  use  of  this  combination  with  future  mobile  IT  products.
2	Applying  systems  ergonomics  methods  in  sport  a  systematic  review.  Abstract  Introduction  As  sports  systems  become  increasingly  more  complex,  competitive,  and  technology-centric,  there  is  a  greater  need  for  systems  ergonomics  methods  to  consider  the  performance,  health,  and  safety  of  athletes  in  context  with  the  wider  settings  in  which  they  operate.  Therefore,  the  purpose  of  this  systematic  review  was  to  identify  and  critically  evaluate  studies  which  have  applied  a  systems  ergonomics  research  approach  in  the  context  of  sports  performance  and  injury  management.  Material  and  methods  Five  databases  (PubMed,  Scopus,  ScienceDirect,  Web  of  Science,  and  SPORTDiscus)  were  searched  for  the  dates  01  January  1990  to  01  August  2017,  inclusive,  for  original  peer-reviewed  journal  articles  and  conference  papers.  Reported  analyses  were  underpinned  by  a  recognised  systems  ergonomics  method,  and  study  aims  were  related  to  the  optimisation  of  sports  performance  (e.g.  communication,  playing  style,  technique,  tactics,  or  equipment),  and/or  the  management  of  sports  injury  (i.e.  identification,  prevention,  or  treatment).  Results  A  total  of  seven  articles  were  identified.  Two  articles  were  focussed  on  understanding  and  optimising  sports  performance,  whereas  five  examined  sports  injury  management.  The  methods  used  were  the  Event  Analysis  of  Systemic  Teamwork,  Cognitive  Work  Analysis  (the  Work  Domain  Analysis  Abstraction  Hierarchy),  Rasmussen's  Risk  Management  Framework,  and  the  Systems  Theoretic  Accident  Model  and  Processes  method.  The  individual  sport  application  was  distance  running,  whereas  the  team  sports  contexts  examined  were  cycling,  football,  Australian  Football  League,  and  rugby  union.  Conclusions  The  included  systems  ergonomics  applications  were  highly  flexible,  covering  both  amateur  and  elite  sports  contexts.  The  studies  were  rated  as  valuable,  providing  descriptions  of  injury  controls  and  causation,  the  factors  influencing  injury  management,  the  allocation  of  responsibilities  for  injury  prevention,  as  well  as  the  factors  and  their  interactions  underpinning  sports  performance.  Implications  and  future  directions  for  research  are  described.
2	Fatigue  related  risk  perception  among  emergency  physicians  working  extended  shifts.  There  is  a  growing  body  of  studies  indicating  that  extended  shift  duration  has  an  adverse  effect  on  fatigue,  consequently  leading  to  reduced  work  performance  and  higher  risk  of  accident.  Following  modern  fatigue  risk  management  systems  (FRMS),  acceptable  performance  could  be  maintained  by  the  mobilization  of  appropriate  mitigation  strategies.  However,  the  effective  deployment  of  such  strategies  assume  that  workers  are  able  to  assess  their  own  level  of  fatigue-related  impairments.  In  this  study,  we  sought  to  determine  whether  emergency  physicians'  subjective  feelings  of  sleepiness  could  provide  accurate  knowledge  of  actual  fatigue-related  impairments  while  working  extended  shifts.  We  conducted  a  prospective  observational  study  with  a  within-subjects  repeated  measures  component.  We  collected  sleep  logs,  sleepiness  ratings  and  reaction  times  on  a  Psychomotor  Vigilance  Task  (PVT)  at  different  time  points  during  shifts.  Our  results  show  that  the  PVT  is  sensitive  to  sleep  loss  and  fatigue,  with  a  10%  increase  in  mean  reaction  time  across  the  shift.  Subjective  sleepiness,  however,  showed  no  significant  association  with  time  since  awakening  and  was  not  a  significant  predictor  of  PVT  performance.  Our  results  are  consistent  with  experimental  studies  showing  that  individuals  tend  to  underestimate  fatigue-related  impairments  when  sleep  deprived  or  functioning  under  adverse  circadian  phase.  The  discrepancy  between  subjective  sleepiness  and  actual  fatigue-related  impairments  may  give  workers  the  illusion  of  being  in  control  and  hinder  the  deployment  of  mitigation  strategies.  Further  research  is  needed  to  determine  the  relative  weight  of  circadian  phase  shifting  and  cumulative  sleep  deprivation  in  the  decline  of  self-knowledge  in  extended  shifts.
2	A  simplified  thermoregulation  model  of  the  human  body  in  warm  conditions.  Thermoregulation  models  of  the  human  body  have  been  widely  used  in  thermal  comfort  studies.  The  existing  models  are  complicated  and  not  fully  verified  for  application  in  China.  This  paper  presents  a  simplified  thermoregulation  model  which  has  been  statistically  validated  by  the  predicted  and  measured  mean  skin  temperature  in  warm  environments,  including  21  typical  conditions  with  400  Chinese  subjects.  This  model  comprises  three  parts:  i)  the  physical  model;  ii)  the  controlled  system;  and  iii)  the  controlling  system,  and  considers  three  key  questions  formerly  ignored  by  the  existing  models  including:  a)  the  evaporation  efficiency  of  regulatory  sweat;  b)  the  proportional  relation  of  total  skin  blood  flow  and  total  heat  loss  by  regulatory  sweating  against  body  surface  area;  and  c)  discrepancies  in  the  mean  skin  temperatures  by  gender.  The  developed  model  has  been  validated  to  be  within  the  95%  confidence  interval  of  the  population  mean  skin  temperature  in  three  cases.
2	Identifying  and  characterising  the  physical  demands  for  an  australian  specialist  policing  unit.  Many  police  organisations  incorporate  specialist  policing  roles  where  incumbents  are  tasked  with  providing  operational  response  capabilities  above  and  beyond  the  general  duties  policing  role.  The  current  research  utilised  subjective  job  task  analysis  methods  to  identify  and  characterise  the  physically  demanding,  frequently  occurring,  and  operationally  important  tasks,  as  well  as  the  dominant  fitness  component  for  each  task,  inherent  to  specialist  policing  roles  in  an  Australian  policing  organisation.  This  was  achieved  through  engagement  with  subject  matter  experts  and  online  survey  responses  from  specialist  police  incumbents.  In  total,  11  criterion  tasks  were  identified,  which  covered  a  range  of  physical  capacities  including  muscular  strength,  muscular  endurance,  and  aerobic  power.  The  most  physically  demanding  tasks  included  those  with  an  arrest  component,  requiring  high  muscular  strength  and  power  capacities.  Having  identified  the  criterion  tasks,  three  operational  scenarios  were  constructed,  which  incorporated  each  of  the  11  tasks  in  different  operational  contexts.  The  criterion  tasks  and  composite  scenarios  will  allow  practitioners  within  specialised  police  units  to  develop  evidence-based  strategies,  including  physical  selection  procedures  and  physical  training  programs,  specific  to  the  demands  of  their  work.
2	Shoulder  pain  among  male  industrial  workers  validation  of  a  conceptual  model  in  two  independent  french  working  populations.  Abstract  This  study  aims  to  validate  a  conceptual  model  for  shoulder  pain  risk  factors  in  two  independent  samples  of  male  industrial  workers:  the  Cosali  cohort  (n = 334)  and  one  pharmaceutical  company  (n = 487).  Direct  and  indirect  relationships  between  work  organization  factors  (automatic  speed  of  a  machine  or  movement  of  a  product  and  work  pace  dependent  on  customers’  demand),  psychosocial  factors  (Job  strain  model),  biomechanical  factors  (working  with  abducted  arms,  working  with  arms  at  or  above  shoulder  level,  and  perceived  physical  exertion),  perceived  stress,  and  shoulder  pain  were  explored  using  structural  equation  models.  Shoulder  pain  was  positively  associated  with  biomechanical  exposure  in  both  samples,  and  with  perceived  stress  only  in  the  pharmaceutical  preparation  manufacturer,  while  factors  related  to  work  organization  and  psychosocial  factors  had  indirect  impacts  on  the  risk  of  chronic  shoulder  pain  in  both  samples.  The  results  provide  a  deeper  understanding  of  the  complex  relationships  between  workplace  risk  factors  and  shoulder  pain.
2	Visualizing  disaster  attitudes  resulting  from  terrorist  activities.  The  purpose  of  this  study  was  to  analyze  people's  attitudes  to  disasters  by  investigating  how  people  feel,  behave  and  think  during  disasters.  We  focused  on  disasters  induced  by  humans,  such  as  terrorist  attacks.  Two  types  of  textual  information  were  collected  -  from  Internet  blogs  and  from  research  papers.  The  analysis  enabled  forecasting  of  attitudes  for  the  design  of  proactive  disaster  advisory  scheme.  Text  was  analyzed  using  a  text  mining  tool,  Leximancer.  The  outcome  of  this  analysis  revealed  core  themes  and  concepts  in  the  text  concerning  people's  attitudes.  The  themes  and  concepts  were  sorted  into  three  broad  categories:  Affect,  Behaviour,  and  Cognition  (ABC),  and  the  data  was  visualized  in  semantic  maps.  The  maps  reveal  several  knowledge  pathways  of  ABC  for  developing  attitudinal  ontologies,  which  describe  the  relations  between  affect,  behaviour  and  cognition,  and  the  sequence  in  which  they  develop.  Clearly,  terrorist  attacks  induced  trauma  and  people  became  highly  vulnerable.  Language:  en
2	Effects  of  eva  gloves  on  grip  strength  and  fatigue  under  low  temperature  and  low  pressure.  Abstract      Objective    To  study  the  effects  of  wearing  extravehicular  activity  (EVA)  gloves  on  grip  strength  and  fatigue  in  low  temperature,  low  pressure  and  mixing  of  two  factors  (low  temperature  and  low  pressure).        Methods    The  maximum  grip  strength  and  fatigue  tests  were  performed  with  10  healthy  male  subjects  wearing  gloves  in  a  variety  of  simulated  environments.  The  data  was  analysed  using  the  normalization  method.        Results    The  results  showed  that  wearing  gloves  significantly  affected  the  maximum  grip  strength  and  fatigue.  Pressure  (29.6,  39.2 kPa)  had  more  influence  on  the  maximum  grip  compared  with  control  group  while  low  temperatures  (−50, −90, −110 °C)  had  no  influence  on  grip  but  affected  fatigue  dramatically.  The  results  also  showed  that  the  maximum  grip  strength  and  fatigue  were  influenced  significantly  in  a  compound  environment.        Conclusions    Space  environment  remarkably  reduced  strength  and  endurance  of  the  astronauts.  However,  the  effects  brought  by  the  compound  environment  cannot  be  understood  as  the  superimposition  of  low  temperature  and  pressure  effects.
2	Body  size  and  ability  to  pass  through  a  restricted  space  observations  from  3d  scanning  of  210  male  uk  offshore  workers.  Offshore  workers  are  subjected  to  a  unique  physical  and  cultural  environment  which  has  the  ability  to  affect  their  size  and  shape.  Because  they  are  heavier  than  the  UK  adult  population  we  hypothesized  they  would  have  larger  torso  dimensions  which  would  adversely  affect  their  ability  to  pass  one  another  in  a  restricted  space.  A  sample  of  210  male  offshore  workers  was  selected  across  the  full  weight  range,  and  measured  using  3D  body  scanning  for  shape.  Bideltoid  breadth  and  maximum  chest  depth  were  extracted  from  the  scans  and  compared  with  reference  population  data.  In  addition  a  size  algorithm  previously  calculated  on  44  individuals  was  applied  to  adjust  for  wearing  a  survival  suit  and  re-breather  device.  Mean  bideltoid  breadth  and  chest  depth  was  51.4  cm  and  27.9  cm  in  the  offshore  workers,  compared  with  49.7  cm  and  25.4  cm  respectively  in  the  UK  population  as  a  whole.  Considering  the  probability  of  two  randomly  selected  people  passing  within  a  restricted  space  of  100  cm  and  80  cm,  offshore  workers  are  28%  and  34%  less  likely  to  pass  face  to  face  and  face  to  side  respectively,  as  compared  with  UK  adults,  an  effect  which  is  exacerbated  when  wearing  personal  protective  equipment.
2	Measured  and  perceived  environmental  comfort  field  monitoring  in  an  italian  school.  Abstract      Microclimatic  conditions  were  recorded  in  an  Italian  school  and  Fanger's  indexes  PMV  and  PPD  were  calculated  under  different  conditions.  Students'  sensations  were  investigated  four  times  by  means  of  two  surveys,  one  related  to  actual  microclimatic  conditions  and  one  on  overall  satisfaction,  interaction  occupant-building  and  reactions  to  discomfort.  Pupils'  classroom  position  was  considered  to  look  for  possible  influence  on  thermal  comfort:  a  difference  emerged  from  PMV  and  the  survey,  but  the  results  obtained  from  the  two  approaches  differ  for  both  the  entity  of  discomfort  and  its  distribution  within  each  classroom.  Innovative  multivariate  nonparametric  statistical  techniques  were  applied  to  compare  and  rank  the  classrooms  in  accordance  with  students'  subjective  perceptions;  a  global  ranking  has  been  also  calculated,  considering  thermal  and  visual  comfort  and  air  quality.  Comparing  pupil-sensation-based  ranking  with  environmental  parameters  no  clear  correspondence  was  found,  except  for  mid-season,  where  PMV,  CO2  concentration  and  desk  illuminance  were  similar  in  all  the  classrooms.
2	Effects  of  child  restraint  system  features  on  installation  errors.  This  study  examined  how  child  restraint  system  (CRS)  features  contribute  to  CRS  installation  errors.  Sixteen  convertible  CRS,  selected  to  include  a  wide  range  of  features,  were  used  in  volunteer  testing  with  32  subjects.  Subjects  were  recruited  based  on  their  education  level  (high  or  low)  and  experience  with  installing  CRS  (none  or  experienced).  Each  subject  was  asked  to  perform  four  child  restraint  installations  in  the  right-rear  passenger  seat  of  a  2006  Pontiac  G6  sedan  using  a  crash  dummy  as  a  child  surrogate.  Each  subject  installed  two  CRS  forward-facing  (FF),  one  with  LATCH  and  one  with  the  vehicle  seatbelt,  and  two  CRS  rear-facing  (RF),  one  with  LATCH  and  one  with  the  seatbelt.  After  each  installation,  the  experimenter  evaluated  42  factors  for  each  installation,  such  as  choice  of  belt  routing  path,  tightness  of  installation,  and  harness  snugness.  Analyses  used  linear  mixed  models  to  identify  CRS  installation  outcomes  associated  with  CRS  features.  LATCH  connector  type,  LATCH  strap  adjustor  type,  and  the  presence  of  belt  lockoffs  were  associated  with  the  tightness  of  the  CRS  installation.  The  type  of  harness  shoulder  height  adjuster  was  associated  with  the  rate  of  achieving  a  snug  harness.  Correct  tether  use  was  associated  with  the  tether  storage  method.  In  general,  subject  assessments  of  the  ease-of-use  of  CRS  features  were  not  highly  correlated  with  the  quality  of  their  installation,  suggesting  a  need  for  feedback  with  incorrect  installations.  The  data  from  this  study  provide  quantitative  assessments  of  some  CRS  features  that  were  associated  with  reductions  in  CRS  installation  errors.  These  results  provide  child  restraint  designers  with  design  guidelines  for  developing  easier-to-use  products.  Research  on  providing  effective  feedback  during  the  child  restraint  installation  process  is  recommended.
2	Evaluating  effectiveness  of  information  visualizations  using  cognitive  fit  theory  a  neuroergonomics  approach.  Information  visualizations  may  be  evaluated  from  the  perspective  of  how  they  match  tasks  that  must  be  performed  with  them,  a  cognitive  fit  perspective.  However,  there  is  a  gap  between  the  high-level  references  made  to  cognitive  fit  and  the  low-level  ability  to  identify  and  measure  it  during  human  interaction  with  visualizations.  We  bridge  this  gap  by  using  an  electroencephalography  metric  derived  from  frontal  midline  theta  power  and  parietal  alpha  power,  known  as  the  task  load  index,  to  determine  if  cognitive  effort  measured  at  the  level  of  cortical  activity  is  less  when  cognitive  fit  is  present  compared  to  when  cognitive  fit  is  not.  We  found  that  when  there  is  cognitive  fit  between  the  type  of  problem  to  be  solved  and  the  information  displayed  by  a  system,  the  task  load  index  is  lower  compared  to  when  cognitive  fit  is  not  present.  We  support  this  finding  with  subjective  (NASA  task  load  index)  and  performance  (response  time  and  accuracy)  measures.  Our  approach,  using  electroencephalography,  provides  supplemental  information  to  self-report  and  performance  measures.  Findings  from  this  study  are  important  because  they  (1)  provide  more  validity  to  the  cognitive  fit  theory  using  a  neurophysiological  measure,  and  (2)  use  the  electroencephalography  task  load  index  metric  as  a  means  to  assess  cognitive  workload  and  effort  in  general.
2	Interventions  and  measurements  of  highly  reliable  resilient  organization  implementations  a  literature  review.  Since  the  inception  of  High  Reliability  Theory  in  1981,  researchers  and  practitioners  have  theorized  about  the  questions,  "How  do  you  know  if  you're  an  HRO,  and  how  do  you  validate  it?"  Evidence  now  exists  that  organizations  seeking  high  reliability  and  resilience  have  moved  away  from  the  theoretical  phase,  and  into  the  application  phase  where  organizations  adopt  the  HRO  hallmarks  plus  culture  as  operational  targets  and  create  interventions  to  effect  change.  The  evidence  of  high  reliability  operations  in  organizations  is  key  for  validating  that  HRO  is  implementable  and  is  also  beneficial.  After  collecting  over  1400  artifacts,  we  found  34  scholarly  efforts  published  which  purposefully  targeted  implementation  measures  toward  achieving  an  HRO  state  and  measured  the  outcomes.  From  that  evidence,  we  concluded  that  three  specific  interventions  have  been  used  which  were  useful  and  generalizable  to  guide  practitioners  in  moving  toward  an  HRO  state:  process  redesign,  training,  and  organization  redesign.  We  suggest  that  this  evidence  may  assert  that  organizations  which  are  not  functioning  as  an  HRO  can  be  redesigned  to  do  so.
2	Seat  and  seatbelt  accommodation  in  fire  apparatus  anthropometric  aspects.  This  study  developed  anthropometric  information  on  U.S.  firefighters  to  guide  fire-apparatus  seat  and  seatbelt  designs  and  future  standards  development.  A  stratified  sample  of  863  male  and  88  female  firefighters  across  the  U.S.  participated  in  the  study.  The  study  results  suggested  498  mm  in  width,  404  mm  in  depth,  and  365-476  mm  in  height  for  seat  pans;  429-522  mm  in  width  and  542  mm  in  height  for  seat  back;  871  mm  in  height  for  head  support;  a  seat  space  of  733  mm  at  shoulder  and  678  mm  at  hip;  and  a  knee/leg  clearance  of  909  mm  in  fire  truck  cab.  Also,  1520  mm  of  lap  belt  web  effective  length  and  2828  mm  of  lap-and-shoulder  belt  web  effective  length  were  suggested.  These  data  for  fire-truck  seats  and  seatbelts  provide  a  foundation  for  fire  apparatus  manufacturers  and  standards  committees  to  improve  firefighter  seat  designs  and  seatbelt  usage  compliance.  Language:  en
2	The  development  of  a  mobile  user  interface  ability  evaluation  system  for  the  elderly.  This  research  aimed  to  develop  a  comprehensive  evaluation  of  the  mobile  user  interface  abilities  of  the  elderly  so  that  technology  can  be  designed  to  meet  individualized  needs.  A  total  of  135  older  adults  were  evaluated  with  the  developed  system,  the  Elderly  Mobile  User  Interface  Ability  Evaluation  System  (EMUIAES).  The  prediction  of  age  and  the  use  of  technology  on  elderly  mobile  interface  usage  were  investigated  based  on  the  findings  of  the  evaluation.  The  relationship  between  performance  on  Fitts'  task  and  elderly  mobile  user  interface  ability  (EMUIA)  was  also  examined.  The  findings  showed  a  strong  effect  of  age  on  the  elderly's  use  of  mobile  user  interfaces.  Previous  experience  with  personal  and  tablet  computers  also  contributed  to  the  use  of  mobile  user  interfaces.  In  addition,  this  research  demonstrated  the  application  of  Fitts'  law  to  describe  the  elderly  mobile  user  interface  behaviors,  particularly  for  tasks  involving  fast  tapping  and  pointing.  The  EMUIAES  can  provide  future  researchers  and  designers  a  comprehensive  tool  to  describe  the  elderly's  diverse  behaviors  and  changes  in  their  ability  to  use  mobile  interfaces.  Individualized  interface  designs  for  elderly  users  can  be  developed  based  on  these  findings  to  improve  the  elderly  users'  experiences  of  using  technology.
2	Development  and  evaluation  of  one  hand  drivable  manual  wheelchair  device  for  hemiplegic  patients.  This  study  was  conducted  for  one-hand  users  including  hemiplegic  clients  currently  using  standard  manual  wheelchairs,  so  as  to  analyze  their  specific  problems  and  recommend  solutions  regarding  usage.  Thirty  hemiplegic  clients  who  were  admitted  to  rehabilitation  and  convalescent  hospitals  participated  as  subjects.  The  research  tools  were  standard  manual  wheelchairs  commonly  used  by  people  with  impaired  gait  and  a  “one-hand  drivable  manual  wheelchair,”  which  was  developed  for  this  study.  The  Wheelchair  Skills  Test  (WST)  was  adopted  for  the  objective  assessment  tool,  while  drivability,  convenience,  difference,  and  acceptability  were  developed  for  the  subjective  evaluation  tools.  The  assessment  procedures  comprise  two  phases  of  pre-assessment  and  post-assessment.  In  the  pre-assessment  phase,  the  WST  and  subjective  evaluation  (drivability,  convenience)  were  conducted  using  the  existing  standard  manual  wheelchair  and  with/without  use  of  a  foot  to  control  the  wheelchair.  In  the  post-assessment  phase,  the  WST  and  subjective  evaluation  (drivability,  convenience,  difference,  acceptability)  were  also  carried  out  using  the  developed  one-hand  drivable  manual  wheelchair.  The  results  showed  that  the  highest  pass  rate  recorded  for  the  WST  items  was  3.3%  when  the  participants  drove  standard  manual  wheelchairs  without  the  use  of  either  foot  and  96.7%  when  using  the  manual  wheelchairs  equipped  with  developed  device.  As  compared  to  the  existing  wheelchair,  statistical  results  showed  significant  effects  on  the  WST,  drivability,  convenience,  difference  and  acceptability  when  the  participants  drove  wheelchairs  equipped  with  the  developed  device.  These  findings  imply  that  the  one-hand  drivable  wheelchair  equipped  with  the  developed  device  can  be  an  active  and  effective  solution  for  hemiplegic  clients  using  existing  manual  wheelchairs  to  increase  their  mobility  and  occupational  performance.
2	Patient  acuity  as  a  determinant  of  paramedics  frequency  of  being  exposed  to  physically  demanding  work  activities.  Abstract      Background    The  purpose  of  this  investigation  was  to  examine  if  paramedics'  frequency  of  being  exposed  to  highly  physically  demanding  activities,  or  their  perception  of  physical,  clinical,  and  emotional  demands  were  altered  by  patients'  acuity  level,  operationalized  using  the  Canadian  Triage  and  Acuity  Scale  (CTAS).        Methods    Physical  demands  descriptions  (PDD)  were  compiled  from  thirteen  services  across  Canada.  The  observation  sessions  took  place  during  a  minimum  of  two  full-shift  (12-h)  ride-outs  at  each  service.  Data  were  obtained  from  53  ride-outs,  which  included  a  total  of  190  calls.        Results    Higher  urgency  calls  (CTAS  level  I  or  II)  required  significantly  more  stretcher  handling,  equipment  handling,  and  intravenous  (IV)  work,  also  prompting  higher  ratings  of  perceived  clinical,  physical,  and  emotional  demand.  Independent  of  CTAS  level,  stretcher  loading  with  patient  (15.0%),  horizontal  patient  transfer  (13.7%),  and  pushing/pulling  the  stretcher  with  patient  (13.1%)  were  identified  as  the  most  physically  demanding  tasks.        Conclusions    Patient  acuity  is  an  important  determinant  affecting  the  frequency  for  which  paramedics  are  exposed  to  work  tasks  with  inherent  ergonomic  hazards  (e.g.,  handling  a  stretcher  with  a  patient).  Patient  acuity  also  affects  paramedics'  perceived  clinical,  physical,  and  emotional  demands  of  a  call.
2	Classroom  furniture  and  anthropometric  characteristics  of  iranian  high  school  students  proposed  dimensions  based  on  anthropometric  data.  Abstract      The  study  evaluated  the  potential  mismatch  between  classroom  furniture  dimensions  and  anthropometric  characteristics  of  978  Iranian  high  school  students  (498  girls,  480  boys),  aged  15–18  years.  Nine  anthropometric  measurements  (stature,  sitting  height,  sitting  shoulder  height,  popliteal  height,  hip  breadth,  elbow–seat  height,  buttock–popliteal  length,  buttock–knee  length  and  thigh  clearance)  and  five  dimensions  from  the  existing  classroom  furniture  were  measured  and  then  compared  together  (using  match  criterion  equations)  to  identify  any  potential  mismatch  between  them.  The  results  indicated  a  considerable  mismatch  between  body  dimensions  of  the  students  and  the  existing  classroom  furniture,  with  seat  height  (60.9%),  seat  width  (54.7%)  and  desktop  height  (51.7%)  being  the  furniture  dimensions  with  a  higher  level  of  mismatch.  The  levels  of  mismatch  varied  between  the  high-school  grade  levels  and  between  genders,  indicating  their  special  requirements  and  possible  problems.  The  proposed  dimensions  of  the  classroom  furniture  more  appropriate  for  the  students  were  given.  This  additional  information  on  students’  anthropometry  can  be  used  by  local  furniture  industries  as  a  starting  point  for  designing  more  appropriate  furniture  for  school  children,  or  used  by  schools  to  aid  in  furniture  selection.
2	Impact  of  two  postural  assist  exoskeletons  on  biomechanical  loading  of  the  lumbar  spine.  Abstract  This  study  evaluated  loading  on  the  low  back  while  wearing  two  commercially  available  postural  assist  exoskeletons.  Ten  male  subjects  lifted  a  box  from  multiple  lift  origins  (combinations  of  vertical  height  and  asymmetry)  to  a  common  destination  using  a  squatting  lifting  technique  with  and  without  the  use  of  either  exoskeleton.  Dependent  measures  included  subject  kinematics,  moment  arms  between  the  torso  or  weight  being  lifted  and  the  lumbar  spine,  and  spinal  loads  as  predicted  by  an  electromyography-driven  spine  model.  One  of  the  exoskeletons  tested  (StrongArm  Technologies™  FLx)  reduced  peak  torso  flexion  at  the  shin  lift  origin,  but  differences  in  moment  arms  or  spinal  loads  attributable  to  either  of  the  interventions  were  not  observed.  Thus,  industrial  exoskeletons  designed  to  control  posture  may  not  be  beneficial  in  reducing  biomechanical  loads  on  the  lumbar  spine.  Interventions  altering  the  external  manual  materials  handling  environment  (lift  origin,  load  weight)  may  be  more  appropriate  when  implementation  is  fesible.
2	A  survey  of  right  angle  power  tool  use  in  canadian  automotive  assembly  plants.  Right-angle  power-tools  (RAPT)  employed  in  automotive  manufacturing  promote  greater  productivity  and  quality  fastenings,  as  well  as,  improve  process  efficiency.  Due  to  RAPT  technological  advances  automotive  manufactures  desire  to  understand  their  ergonomics  consequences  within  a  laboratory  environment,  however,  laboratory-based  representation  must  accurately  represent  the  real  world.  A  survey  within  automotive  assembly  plants  was  conducted  to  capture  RAPT  operation  data.  After  examining  80  total  RAPT  operations,  we  logged  the  3D  locations  of  the  fastener  location  (with  respect  to  the  operator),  direction  and  the  hand  placement  location  used  by  operators.  Four  common  locations  with  respect  to  the  midpoint  between  the  ankle  (in  cm;  X  =  sagittal  plane,  Y  =  transverse  plane,  Z  =  coronal  plane):  1)  2,  113,  62;  2)  42,  104,  45;  3)  -26,  151,  36;  4)  -37,  92,  52.  These  locations  can  be  used  to  simulate  RAPT  operations  within  a  laboratory.  The  survey  provided  insight  into  current  workstation  layout  when  operating  RAPTs  and,  knowledge  for  laboratory-based  RAPT  examinations  so  that  simulated  tasks  best  represent  their  operation  in  automotive  manufacturing.
2	Characterizing  exposure  to  physical  risk  factors  among  reforestation  hand  planters  in  the  southeastern  united  states.  Abstract  Low  back  and  neck/shoulder  pain  are  commonly  reported  among  reforestation  hand  planters.  While  some  studies  have  documented  the  intensive  cardiovascular  demands  of  hand  planting,  limited  information  is  available  regarding  exposures  to  physical  risk  factors  associated  with  the  development  of  musculoskeletal  disorders  (MSDs)  among  hand  planters.  This  study  used  surface  electromyography  (EMG)  and  inertial  measurement  units  (IMUs)  to  characterize  the  muscle  activation  patterns,  upper  arm  and  trunk  postures,  movement  velocities,  and  physical  activity  (PA)  of  fourteen  Southeastern  reforestation  hand  planters  over  one  work  shift.  Results  indicated  that  hand  planters  are  exposed  to  physical  risk  factors  such  as  extreme  trunk  postures  (32.5%  of  time  spent  in  ≥45°  trunk  flexion)  and  high  effort  muscle  exertions  (e.g.,  mean  root-mean-square  right  upper  trapezius  amplitude  of  54.1%  reference  voluntary  exertion)  that  may  place  them  at  increased  risk  for  developing  MSDs.  The  findings  indicate  a  need  for  continued  field-based  research  among  hand  planters  to  identify  and/or  develop  maximally  effective  interventions.
2	Persuasive  technology  based  on  bodily  comfort  experiences  the  effect  of  color  temperature  of  room  lighting  on  user  motivation  to  change  room  temperature.  In  this  paper  we  propose  a  new  perspective  on  persuasive  technology:  Comfort-Experience-Based  Persuasive  Technology.  We  argue  that  comfort  experiences  have  a  dominant  influence  on  people’s  (energy  consumption)  behavior.  In  the  current  research,  we  argue  that  room  lighting  can  influence  heating-related  comfort  experiences  (by  emitting  a  ‘warm’  versus  ‘cold’  lighting  color  temperature).  Two  studies  were  conducted  to  investigate  the  effect  of  lighting  color  temperature  on  participants’  perceptions  of  room  lighting  temperature  and  their  estimations  of  room  temperature,  their  experiences  of  the  comfort  related  to  room  lighting  temperature  and  related  to  room  temperature,  and  also  their  motivation  to  change  room  temperature  settings  and  participants’  temperature-setting  behavior.  Results  indicated  that  lighting  color  temperature  can  influence  a  user’s  perception  of  the  temperature  in  the  room,  and  can  also  motivate  the  user  to  change  room  temperature.  This  research  revealed  that  using  persuasive  strategies  that  targets  user  comfort  experiences  could  help  users  decrease  their  energy  consumption.
2	Participatory  design  of  a  persuasive  mobile  application  for  helping  entrepreneurs  to  recover  from  work.  Involving  end-users  in  a  participatory  design  process  may  help  researchers  and  developers  to  gain  better  understanding  of  the  end  users’  views  about  the  target  system.  In  this  study,  we  utilized  participatory  design  approach  with  focus  group  meetings  and  participatory  design  workshops  to  figure  out  requirements  for  persuasive  features  of  a  mobile  application  for  entrepreneurs  to  recover  from  work  related  strain  and  stress.  In  many  cases,  end-user  participation  in  the  design  process  may  lead  into  building  more  efficient  persuasive  technology  solutions  and  at  least  avoidance  of  many  of  the  design  pitfalls,  but  setting  up  meetings  and  organizing  workshops  can  be  time-consuming.
2	Training  nurses  and  educating  the  public  using  a  virtual  operating  room  with  oculus  rift.  The  purpose  of  this  work  is  to  contribute  to  the  design  and  development  of  a  virtual  university  hospital  as  a  place  for  educational  activities.  The  findings  presented  in  this  paper  are  based  on  two  exploratory  studies  using  a  virtual  operating  room  with  both  medical  and  non-medical  participants.  The  room  was  designed  to  recreate  a  real  one  at  St.  Olav's  university  hospital  in  Trondheim,  Norway.  The  first  participant  category  was  represented  by  surgical  and  anesthesia  postgraduate  nursing  students  who  conducted  role-play  of  realistic  and  relevant  scenarios  in  the  virtual  operating  room.  The  non-medical  participants  went  on  a  virtual  guided  tour  around  in  the  same  operating  room.  Both  participant  groups  provided  suggestions  for  further  development  of  the  virtual  hospital.  We  have  also  investigated  the  use  of  the  Oculus  Rift,  a  head  mounted  display,  as  a  way  of  enhancing  the  immersion  at  the  virtual  operating  room.  The  paper  highlights  and  discusses  the  most  important  findings,  with  suggestions  of  future  work.
2	More  than  sex  the  role  of  femininity  and  masculinity  in  the  design  of  personalized  persuasive  games.  The  goal  of  persuasive  games  is  to  change  behavior  and  attitudes  in  a  desirable  manner,  e.g.,  to  promote  physical  activity.  Research  has  shown  that  personalized  persuasive  approaches  are  more  successful  than  one-size-fits-all  approaches.  As  a  means  for  personalization,  sex  has  been  investigated  with  results  showing  that  women  are  overall  more  persuadable  than  men.  We  argue  that  considering  only  a  dichotomous  sex-type  categorization  may  not  be  able  to  fully  capture  the  differences  in  the  persuasiveness  of  persuasion  strategies.  To  that  end  we  apply  a  dimensional  approach  of  capturing  gender  identity  ---  femininity  and  masculinity.  We  investigate  the  relationship  between  masculinity,  femininity,  sex  and  the  persuasiveness  of  ten  persuasion  strategies  in  an  online  study  ni?ź=i?ź592.  Results  show  that  femininity  is  significantly  associated  with  seven  of  the  ten  strategies,  while  sex  does  only  show  differences  for  two  strategies,  suggesting  gender  identity  could  be  a  reliable  variable  for  personalizing  persuasive  games.
2	Persuasive  practices  learning  from  home  security  advisory  services.  Research  on  persuasive  technologies  PT  focuses,  primarily,  on  the  design  and  development  of  IT  for  inducing  change  of  individual's  behavior  and  attitude  through  computer-human  and  computer-mediated  influence.  The  issue  of  practices  in  co-located  human-human  persuasive  encounters  remained  unattended  in  the  PT  community.  This  study  uses  the  notion  of  persuasive  practices  to  understand  the  course  of  events  in  face-to-face  home  security  advisory  sessions  ---  it  specifies  and  illustrates  such  practices  and  discusses  their  impact  on  the  persuasiveness  of  the  encounter.  Furthermore,  it  presents  potential  of  IT  to  support  such  persuasive  practices  thus  opening  new  research  possibilities  of  PT  research.
2	Learning  and  teaching  experiences  with  a  persuasive  social  robot  in  primary  school  findings  and  implications  from  a  4  month  field  study.  In  the  field  of  child-robot  interaction  (CRI),  long-term  field  studies  with  users  in  authentic  contexts  are  still  rare.  This  paper  reports  the  findings  from  a  4-month  field  study  of  robot-assisted  language  learning  (RALL).  We  focus  on  the  learning  experiences  of  primary  school  pupils  with  a  social,  persuasive  robot,  and  the  experiences  of  the  teachers  of  using  the  robot  as  a  teaching  tool.  Our  qualitative  research  approach  includes  interviews,  observations,  questionnaires  and  a  diary  as  data  collection  methods,  and  affinity  diagram  as  a  data  analysis  method.  The  research  involves  three  target  groups:  the  pupils  of  a  3rd  grade  class  (9–10  years  old,  n  =  20),  language  teachers  (n  =  3)  and  the  parents  (n  =  18).  We  report  findings  on  user  experience  (UX),  the  robot’s  tasks  and  role  in  the  school,  and  the  experience  of  the  multimodal  interaction  with  the  robot.  Based  on  the  findings,  we  discuss  several  aspects  concerning  the  design  of  persuasive  robotics  on  robot-assisted  learning  and  CRI,  for  example  the  benefits  of  robot-specific  ways  of  rewarding,  the  value  of  the  physical  embodiment  and  the  opportunities  of  the  social  role  adopted  by  the  learning  robot.
2	Digital  cartographic  heritage  in  service  to  the  society  landscape  analysis  for  informed  decision  making.  The  paper  describes  the  results  of  the  project  “Atl@nte  dei  Catasti  Storici  e  delle  carte  Topografiche  della  Lombardia”  (Atlas  of  historical  cadastre  and  topographic  maps  of  Lombardy)  and  in  particular  the  potentials  of  an  open  source  platform  for  cartographic  heritage,  developed  as  one  of  the  main  deliverables.  The  value  of  this  research  is  of  twofold  nature.  On  one  hand  side,  it  regards  cartography  as  heritage  addressed  not  only  to  experts  but  to  society  and  general  public,  as  argued  by  International  Working  Group  on  “Digital  Technologies  in  Cartographic  Heritage”:  the  project  queries  methodologies  for  systematic  information  structure,  geo-referencing  and  representation  of  historic  maps  given  their  artistic-historic  as  well  as  technical-cartographic  nature.  On  the  other  side,  the  research  investigates  how  innovative  geo-portal  tools  could  be  used  to  promote  a  progressive  use  of  historic  maps  during  territorial  planning  processes.  The  analysis  of  landscape  transformations  rely  increasingly  upon  integrated  services  based  on  geospatial  information,  creating  a  progressive  demand  of  data  sets  and  service  and  aiming  at  an  extensive  availability  of  these  information  to  different  final  users.  To  this  purpose,  it  is  necessary  to  consider  different  domains  of  technological  progress  but  also  an  appropriate  territorial  dimension,  applying  a  multi-scale  approach  in  an  SDI  framework.  Geo-referenced  maps  of  Atl@nte  at  territorial-regional  and  cadastral-local  scale  could  be  further  integrated  with  data  such  as  satellite,  multi-spectral  airborne  and  in-situ  UAV  images,  to  address  actions  on  built  environment  and  to  provide  new  scenarios  for  retrieving  geospatial  knowledge  of  territory  and  its  multiple  layers  of  history.
2	Online  peer  groups  as  a  persuasive  tool  to  combat  digital  addiction.  Digital  Addiction  DA  denotes  a  problematic  usage  of  digital  devices  characterised  by  properties  such  as  being  compulsive,  impulsive,  excessive  and  hasty.  DA  is  associated  with  negative  behaviours  such  as  anxiety  and  depression.  "Digital  Detox"  programs  have  started  to  appear  and  are  mainly  based  on  a  relatively  expensive  and  heavyweight  in-patient  care  utilising  traditional  solutions  such  as  motivational  interviews  and  cognitive  behavioural  therapies.  For  moderate  addiction,  persuasive  technology  could  have  potential,  as  a  brief  intervention,  to  assist  users  to  regulate  their  usage.  This  paper  explores  the  design  of  online  peer  groups  as  a  persuasive  technique  that  puts  together  people  who  share  a  common  interest  in  combating  their  DA  or  in  helping  others  to  do  so.  We  conducted  empirical  research  to  explore  design  aspects  of  this  mechanism.  The  results  raise  a  range  of  questions  and  challenges  to  address  when  developing  such  a  technique  for  the  behaviour  change  needed  against  DA.
2	Device  free  and  device  bound  activity  recognition  using  radio  signal  strength.  Background:  We  investigate  direct  use  of  802.15.4  radio  signal  strength  indication  (RSSI)  for  human  activity  recognition  when  1)  a  user  carries  a  wireless  node  (device-bound)  and  when  2)  a  user  moves  in  the  wireless  sensor  net  (WSN)  without  a  WSN  node  (device-free).  We  investigate  recognition  feasibility  in  respect  to  network  topology,  subject  and  room  geometry  (door  open,  half,  closed).      Methods:  In  a  2  person  office  room  8  wireless  nodes  are  installed  in  a  3D  topology.  Two  subjects  are  outfitted  with  a  sensor  node  on  the  hip.  Acceleration  and  RSSI  are  recorded  while  subject  performs  6  different  activities  or  room  is  empty.  We  apply  machine  learning  for  analysis  and  compare  our  results  to  acceleration  data.      Results:  10-fold  cross-validation  with  all  nodes  gives  accuracies  of  0.896  (device-bound),  0.894  (device-free)  and  0.88  (accelerometer).  Topology  investigation  reveals  that  similar  accuracies  may  be  reached  with  only  5  (device-bound)  or  4  (device-free)  selected  nodes.  Applying  trained  data  from  one  subject  to  the  other  and  vice-versa  shows  higher  recognition  difference  on  RSSI  than  on  acceleration.  Changing  of  door  state  has  smaller  effect  on  both  systems  than  subject  change;  with  least  impact  when  door  is  closed.      Conclusion:  802.15.4  RSSI  suited  for  activity  recognition.  3D  topology  is  helpful  in  respect  to  type  of  activities.  Discrimination  of  subjects  seems  possible.  Practical  systems  must  adapt  no  only  to  long-term  environmental  dispersion  but  consider  typical  geometric  changes.  Adaptable,  robust  recognition  models  must  be  developed.
2	Hearthere  networked  sensory  prosthetics  through  auditory  augmented  reality.  In  this  paper  we  present  a  vision  for  scalable  indoor  and  outdoor  auditory  augmented  reality  (AAR),  as  well  as  HearThere,  a  wearable  device  and  infrastructure  demonstrating  the  feasibility  of  that  vision.  HearThere  preserves  the  spatial  alignment  between  virtual  audio  sources  and  the  user's  environment,  using  head  tracking  and  bone  conduction  headphones  to  achieve  seamless  mixing  of  real  and  virtual  sounds.  To  scale  between  indoor,  urban,  and  natural  environments,  our  system  supports  multi-scale  location  tracking,  using  fine-grained  (20-cm)  Ultra-WideBand  (UWB)  radio  tracking  when  in  range  of  our  infrastructure  anchors  and  mobile  GPS  otherwise.  In  our  tests,  users  were  able  to  navigate  through  an  AAR  scene  and  pinpoint  audio  source  locations  down  to  1m.  We  found  that  bone  conduction  is  a  viable  technology  for  producing  realistic  spatial  sound,  and  show  that  users'  audio  localization  ability  is  considerably  better  in  UWB  coverage  zones  than  with  GPS  alone.  HearThere  is  a  major  step  towards  realizing  our  vision  of  networked  sensory  prosthetics,  in  which  sensor  networks  serve  as  collective  sensory  extensions  into  the  world  around  us.  In  our  vision,  AAR  would  be  used  to  mix  spatialized  data  sonification  with  distributed,  livestreaming  microphones.  In  this  concept,  HearThere  promises  a  more  expansive  perceptual  world,  or  umwelt,  where  sensor  data  becomes  immediately  attributable  to  extrinsic  phenomena,  externalized  in  the  wearer's  perception.  We  are  motivated  by  two  goals:  first,  to  remedy  a  fractured  state  of  attention  caused  by  existing  mobile  and  wearable  technologies;  and  second,  to  bring  the  distant  or  often  invisible  processes  underpinning  a  complex  natural  environment  more  directly  into  human  consciousness.
2	Modelling  human  behaviour  using  partial  order  planning  based  on  atomic  action  templates.  A  problem  in  assisting  users  in  intelligent  environments  is  the  detection  of  their  long  term  intentions  based  on  the  current  state.  One  approach  to  solving  this  problem  is  the  employment  of  human  behaviour  models,  such  as  CTML  and  PDDL.  At  present,  most  of  these  models  are  based  on  concrete  domains  or  scenarios  which  makes  them  difficult  or  impossible  to  adapt  for  other  use  cases.  To  overcome  this  drawback,  this  paper  introduces  an  approach  that  uses  partial  order  planning  for  generating  a  user  behaviour  model.  Furthermore,  it  proposes  a  generalization  of  human  activities  by  introducing  atomic  action  templates  valid  for  activities  from  various  domains.  To  illustrate  the  approach,  a  scenario  from  the  elderly  care  domain  is  modelled.  Overall,  the  paper  provides  an  effective  general  approach  for  modelling  human  behaviour  which  can  be  embedded  in  the  intention  inference  workflow.
2	Touchsense  classifying  and  measuring  the  force  of  finger  touches  with  an  electromyography  armband.  We  present  TouchSense,  a  system  to  classify  and  to  compute  the  force  of  finger  touches  using  an  inexpensive,  off-the-shelf  electromyography  (EMG)  armband.  From  EMG  input  only,  we  classify  the  finger  touches  and  estimate  the  force  applied  when  pressing  an  object  or  surface  with  the  thumb,  forefinger,  or  middle  finger.  We  propose  a  novel  neural  network  architecture  for  finger  classification  using  EMG  data.  Our  system  runs  in  real  time  and  only  utilizes  the  Thalmic  Labs  Myo  EMG  armband  and  an  Android  smartphone,  thereby  being  wearable  and  mobile.  We  showcase  one  application  for  our  system,  which  controls  the  brightness  of  a  lamp.
2	Assessing  real  world  imagery  in  virtual  environments  for  people  with  cognitive  disabilities.  People  with  cognitive  disabilities  are  often  socially  excluded.  We  propose  a  system  based  on  Virtual  and  Augmented  Reality  that  has  the  potential  to  act  as  an  educational  and  support  tool  in  everyday  tasks  for  people  with  cognitive  disabilities.  Our  solution  consists  of  two  components:  the  first  that  enables  users  to  train  for  several  essential  quotidian  activities  and  the  second  that  is  meant  to  offer  real  time  guidance  feedback  for  immediate  support.  In  order  to  illustrate  the  functionality  of  our  proposed  system,  we  chose  to  train  and  support  navigation  skills.  Thus,  we  conducted  a  preliminary  study  on  people  with  Down  Syndrome  (DS)  based  on  a  navigation  task.  Our  experiment  was  aimed  at  evaluating  the  visual  and  spatial  perception  of  people  with  DS  when  interacting  with  different  elements  of  our  system.  We  provide  a  preliminary  evaluation  that  illustrates  how  people  with  DS  perceive  different  landmarks  and  types  of  visual  feedback,  in  static  images  and  videos.  Although  we  focused  our  study  on  people  with  DS,  people  with  different  cognitive  disabilities  could  also  benefit  from  the  features  of  our  solution.  This  analysis  is  mandatory  in  the  design  of  a  virtual  intelligent  system  with  several  functionalities  that  aims  at  helping  disabled  people  in  developing  basic  knowledge  in  every  day  tasks.
2	A  usability  score  for  mobile  phone  applications  based  on  heuristics.  Mobile  phones  are  becoming  the  most  widespread  personal  consumer  device.  Yet,  offering  mobile  access  anywhere,  anytime  for  anybody  poses  new  challenges  to  usability.  So  far  there  is  little  research  on  how  to  customize  usability  heuristics  to  the  specific  characteristics  of  mobile  phone  applications.  Therefore,  this  article  presents  a  set  of  tailored  usability  heuristics  based  on  a  systematic  literature  review.  In  order  to  facilitate  the  usage  of  these  heuristics,  the  authors  design  and  validate  a  measurement  instrument  checklist  and  scale.  The  checklist  has  been  validated  through  an  empirical  study  in  which  the  results  of  247  heuristic  evaluations  have  been  statistically  analyzed  using  Item  Response  Theory.  Based  on  the  results,  the  measurement  items  have  been  calibrated  and  a  standardized  measurement  scale  has  been  constructed.  The  results  can  be  used  to  measure  usability  of  mobile  phone  applications  from  early  on  in  the  design  process,  and,  thus,  facilitate  evaluations  in  a  cost-effective  way.
2	A  field  study  of  older  adults  with  cognitive  impairment  using  tablets  for  communication  at  home  closing  technology  adoption  gaps  using  intouch.  This  work  was  supported  by  AGE-WELL  NCE  Inc.,  a  member  of  the  Networks  of  Centres  of  Excellence  program.
2	Development  of  embedded  captcha  elements  for  bot  prevention  in  fischer  random  chess.  Cheating  in  chess  can  take  many  forms  and  has  existed  almost  as  long  as  the  game  itself.  The  advent  of  computers  has  introduced  a  new  form  of  cheating  into  the  game.  Thanks  to  the  computational  power  of  modern-day  computers,  a  player  can  use  a  program  to  calculate  thousands  of  moves  for  him  or  her,  and  determine  the  best  possible  scenario  for  each  move  and  countermove.  These  programs  are  often  referred  to  as  "bots,"  and  can  even  play  the  game  without  any  user  interaction.  In  this  paper,  we  describe  a  methodology  aimed  at  preventing  bots  from  participating  in  online  chess  games.  The  proposed  approach  is  based  on  the  integration  of  a  CAPTCHA  protocol  into  a  game  scenario,  and  the  subsequent  inability  of  bots  to  accurately  track  the  game  states.  This  is  achieved  by  rotating  the  images  of  the  individual  chess  pieces  and  adjusting  their  resolution  in  an  attempt  to  render  them  unreadable  by  a  bot.  Feedback  from  users  during  testing  shows  that  there  is  minimal  impact  on  their  ability  to  play  the  game.  Players  rated  the  difficulty  of  reading  the  pieces  on  a  scale  of  one  to  ten,  with  an  average  rank  of  6.5.  However,  the  average  number  of  moves  to  adjust  to  the  distorted  pieces  was  only  3.75.  This  tells  us  that,  although  it  is  difficult  to  read  the  pieces  at  first,  it  is  easy  to  adjust  quickly  to  the  new  image.
2	Hypernet  games  leveraging  sdn  networks  to  improve  multiplayer  online  games.  Many  of  today's  real-time  multi-player  online  games  are  highly  dependent  on  the  communication  network  that  connects  players  with  the  game  and  each  other.  Factors  such  as  network  latency  and  network  throughput  can  significantly  affect  a  user's  gaming  experience.  For  example,  many  online  games  use  a  (logically)  centralized  game  server  that  acts  as  a  rendezvous  point  for  players  to  exchange  real-time  gaming  events.  The  number  and  location  of  these  servers  in  the  network  plays  a  critical  role  in  the  interactive  response  time  of  the  game.  Currently,  gaming  companies  go  to  great  lengths  to  find  the  best  locations  for  the  placement  of  their  game  servers.  However,  finding  the  best  locations  is  challenging  because  the  number  of  players  and  their  locations  is  not  known  ahead  of  time.  Consequently,  game  companies  typically  build  a  generic  (static)  game  network  that  hopefully  will  provide  acceptable  performance  to  most  users.  With  the  growing  popularity  of  cloud  computing  and  software  defined  networking  (SDN),  it  is  now  possible  to  build  a  custom  game  network  on-the-fly;  a  game  network  tailored  for  a  particular  game  and  its  current  participants  (players).  In  this  paper,  we  introduce  the  concept  of  a  HyperNet  Game  that  is  capable  of  dynamically  creating  and  deploying  a  software  defined  network  tailored  to  the  needs  of  the  game  and  the  current  set  of  participants  in  the  game.  We  present  an  example  multi-player  hypernet  game  that  we  have  developed  that  is  able  to  dynamically  deploy  optimally  placed  game  servers  based  on  the  current  set  of  players.  We  present  performance  results  that  show  greatly  improved  user  response  times  compared  to  the  currently  deployed  (static)  game  server  network  used  today.
2	Fifty  years  on  what  exactly  is  a  videogame  an  essentialistic  definitional  approach.  During  the  last  five  decades,  videogames  evolved  into  a  major  component  of  popular  culture  as  well  as  a  multi-billion-dollar  industry.  The  medium  diversified  tremendously,  currently  encompassing  simple  implementations  of  numeric  games  on  the  screen  of  a  cell  phone  as  well  as  vast,  persistent  online  worlds  on  last  generation  consoles  and  PCs.  In  spite  of  its  cultural  and  economic  relevance,  few  attempts  have  been  made  to  define  what  a  videogame  exactly  is.  In  this  article,  I  endeavour  to  propose  an  essentialist  definition  of  videogame,  that  is,  one  structured  in  terms  of  necessary  and  sufficient  conditions.  I  begin  with  a  critical  consideration  of  the  main  theoretical  approaches  to  the  medium  and  the  few  published  definitions.  From  this  analysis,  four  essential  properties  are  defined  and  separately  characterized.  Finally,  a  definition  is  proposed,  capable  of  encompassing  the  medium  in  both  its  specificity  and  variability.
2	Foggy  scene  rendering  based  on  transmission  map  estimation.  Realistic  rendering  of  foggy  scene  is  important  in  game  development  and  virtual  reality.  Traditional  methods  have  many  parameters  to  control  or  require  a  long  time  to  compute,  and  they  are  usually  limited  to  depicting  a  homogeneous  fog  without  considering  the  foggy  scene  with  heterogeneous  fog.  In  this  paper,  a  new  rendering  method  based  on  transmission  map  estimation  is  proposed.  We  first  generate  perlin  noise  image  as  the  density  distribution  texture  of  heterogeneous  fog.  Then  we  estimate  the  transmission  map  using  the  Markov  random  field  (MRF)  model  and  the  bilateral  filter.  Finally,  virtual  foggy  scene  is  realistically  rendered  with  the  generated  perlin  noise  image  and  the  transmission  map  according  to  the  atmospheric  scattering  model.  Experimental  results  show  that  the  rendered  results  of  our  approach  are  quite  satisfactory.
2	Albireo  an  interactive  tool  for  visually  summarizing  computational  notebook  structure.  Computational  notebooks  have  become  a  major  medium  for  data  exploration  and  insight  communication  in  data  science.  Although  expressive,  dynamic,  and  flexible,  in  practice  they  are  loose  collections  of  scripts,  charts,  and  tables  that  rarely  tell  a  story  or  clearly  represent  the  analysis  process.  This  leads  to  a  number  of  usability  issues,  particularly  in  the  comprehension  and  exploration  of  notebooks.  In  this  work,  we  design,  implement,  and  evaluate  Albireo,  a  visualization  approach  to  summarize  the  structure  of  notebooks,  with  the  goal  of  supporting  more  effective  exploration  and  communication  by  displaying  the  dependencies  and  relationships  between  the  cells  of  a  notebook  using  a  dynamic  graph  structure.  We  evaluate  the  system  via  a  case  study  and  expert  interviews,  with  our  results  indicating  that  such  a  visualization  is  useful  for  an  analyst's  self-reflection  during  exploratory  programming,  and  also  effective  for  communication  of  narratives  and  collaboration  between  analysts.
2	A  visual  analytics  framework  for  analyzing  parallel  and  distributed  computing  applications.  To  optimize  the  performance  and  efficiency  of  HPC  applications,  programmers  and  analysts  often  need  to  collect  various  performance  metrics  for  each  computer  at  different  time  points  as  well  as  the  communication  data  between  the  computers.  This  results  in  a  complex  dataset  that  consists  of  multivariate  time-series  and  communication  network  data,  which  makes  debugging  and  performance  tuning  of  HPC  applications  challenging.  Automated  analytical  methods  based  on  statistical  analysis  and  unsupervised  learning  are  often  insufficient  to  support  such  tasks  without  the  background  knowledge  from  the  application  programmers.  To  better  explore  and  analyze  a  wide  spectrum  of  HPC  datasets,  effective  visual  data  analytics  techniques  are  needed.  In  this  paper,  we  present  a  visual  analytics  framework  for  analyzing  HPC  datasets  produced  by  parallel  discrete-event  simulations  (PDES).  Our  framework  leverages  automated  time-series  analysis  methods  and  effective  visualizations  to  analyze  both  multivariate  time-series  and  communication  network  data.  Through  several  case  studies  for  analyzing  the  performance  of  PDES,  we  show  that  our  visual  analytics  techniques  and  system  can  be  effective  in  reasoning  multiple  performance  metrics,  temporal  behaviors  of  the  simulation,  and  the  communication  patterns.
2	Protecting  computer  network  with  encryption  technique  a  study.  In  today’s  world  the  networking  plays  a  very  important  role  in  our  life.  Most  of  the  activities  occur  through  the  Internet.  For  the  safe  and  secured  exchange  of  information,  we  need  to  have  security.  The  encryption  has  very  wide  applications  for  securing  data.  Latest  authentication  deals  with  biometric  application  such  as  fingerprint  and  retina  scan.  The  different  types  of  encryptions  and  their  strength  and  standards  are  discussed.
2	Eye  gaze  tracking  using  an  rgbd  camera  a  comparison  with  a  rgb  solution.  Most  commercial  eye  gaze  tracking  systems  are  based  on  the  use  of  infrared  lights.  However,  such  systems  may  not  work  outdoor  or  may  have  a  very  limited  head  box  for  them  to  work.  This  paper  proposes  a  non-infrared  based  approach  to  track  one's  eye  gaze  with  an  RGBD  camera  (in  our  case,  Kinect).  The  proposed  method  adopts  a  personalized  3D  face  model  constructed  off-line.  To  detect  the  eye  gaze,  our  system  tracks  the  iris  center  and  a  set  of  2D  facial  landmarks  whose  3D  locations  are  provided  by  the  RGBD  camera.  A  simple  onetime  calibration  procedure  is  used  to  obtain  the  parameters  of  the  personalized  eye  gaze  model.  We  compare  the  performance  of  the  proposed  method  against  the  2D  approach  using  only  RGB  input  on  the  same  images,  and  find  that  the  use  of  depth  information  directly  from  Kinect  achieves  more  accurate  tracking.  As  expected,  the  results  from  the  proposed  method  are  not  as  accurate  as  the  ones  from  infrared-based  approaches.  However,  this  method  has  the  potential  for  practical  use  with  upcoming  better  and  cheaper  depth  cameras.
2	Using  text  mining  to  infer  the  purpose  of  permission  use  in  mobile  apps.  Understanding  the  purpose  of  why  sensitive  data  is  used  could  help  improve  privacy  as  well  as  enable  new  kinds  of  access  control.  In  this  paper,  we  introduce  a  new  technique  for  inferring  the  purpose  of  sensitive  data  usage  in  the  context  of  Android  smartphone  apps.  We  extract  multiple  kinds  of  features  from  decompiled  code,  focusing  on  app-specific  features  and  text-based  features.  These  features  are  then  used  to  train  a  machine  learning  classifier.  We  have  evaluated  our  approach  in  the  context  of  two  sensitive  permissions,  namely  ACCESS_FINE_LOCATION  and  READ_CONTACT_LIST,  and  achieved  an  accuracy  of  about  85%  and  94%  respectively  in  inferring  purposes.  We  have  also  found  that  text-based  features  alone  are  highly  effective  in  inferring  purposes.
2	Cobweb  a  robust  map  update  system  using  gps  trajectories.  The  accuracy  and  completeness  of  a  digital  map  plays  a  critical  role  in  determining  the  quality  of  most  location-based  services.  Unfortunately,  road  networks  change  frequently.  Consequently,  we  study  the  issue  of  automatic  map  update  in  this  paper.  We  propose  a  system  called  COBWEB  which  takes  all  the  unmatched  trajectories  as  input  and  generates  the  missing  road  segments  with  both  the  geometry  properties  and  topology  features  well  preserved.  We  conduct  a  comprehensive  experimental  study  via  real  trajectory  data  generated  by  roughly  15,000  taxis  in  Singapore  within  a  5-month  period.  Compared  with  existing  work,  COBWEB  demonstrates  a  better  and  more  stable  performance  and  a  stronger  resilience  to  various  sampling  rates  and  data  sizes.
2	Haptic  reassurance  in  the  pitch  black  for  an  immersive  theatre  experience.  An  immersive  theatre  experience  was  designed  to  raise  awareness  and  question  perceptions  of  'blindness',  through  enabling  both  sighted  and  blind  members  to  experience  a  similar  reality.  A  multimodal  experience  was  created,  comprising  ambient  sounds  and  narratives  --  heard  through  headphones  --  and  an  assortment  of  themed  tactile  objects,  intended  to  be  felt.  In  addition,  audience  members  were  each  provided  with  a  novel  haptic  device  that  was  designed  to  enhance  their  discovery  of  a  pitch-black  space.  An  in  the  wild  study  of  the  cultural  experience  showed  how  blind  and  sighted  audience  members  had  different  'felt'  experiences,  but  that  neither  was  a  lesser  one.  Furthermore,  the  haptic  device  was  found  to  encourage  enactive  exploration  and  provide  reassurance  of  the  environment  for  both  sighted  and  blind  people,  rather  than  acting  simply  as  a  navigation  guide.  We  discuss  the  potential  of  using  haptic  feedback  to  create  cultural  experiences  for  both  blind  and  sighted  people;  rethinking  current  utilitarian  framing  of  it  as  assistive  technology.
2	Automatic  calibration  of  high  density  electric  muscle  stimulation.  Electric  muscle  stimulation  (EMS)  can  enable  mobile  force  feedback,  support  pedestrian  navigation,  or  confer  object  affordances.  To  date,  however,  EMS  is  limited  by  two  interlinked  problems.  (1)  EMS  is  low  resolution  --  achieving  only  coarse  movements  and  constraining  opportunities  for  exploration.  (2)  EMS  requires  time  consuming,  expert  calibration  --  confining  these  interaction  techniques  to  the  lab.  EMS  arrays  have  been  shown  to  increase  stimulation  resolution,  but  as  calibration  complexity  increases  exponentially  as  more  electrodes  are  used,  we  require  heuristics  or  automated  procedures  for  successful  calibration.  We  explore  the  feasibility  of  using  electromyography  (EMG)  to  auto-calibrate  high  density  EMS  arrays.  We  determine  regions  of  muscle  activity  during  human-performed  gestures,  to  inform  stimulation  patterns  for  EMS-performed  gestures.  We  report  on  a  study  which  shows  that  auto-calibration  of  a  60-electrode  array  is  feasible:  achieving  52%  accuracy  across  six  gestures,  with  82%  accuracy  across  our  best  three  gestures.  By  highlighting  the  electrode-array  calibration  problem,  and  presenting  a  first  exploration  of  a  potential  solution,  this  work  lays  the  foundations  for  high  resolution,  wearable  and,  perhaps  one  day,  ubiquitous  EMS  beyond  the  lab.
2	Youth  centered  design  and  usage  results  of  the  in  touch  mobile  self  management  program  for  overweight  obesity.  Overweight/obesity  among  youth  is  a  grave  concern  in  the  USA  due  to  its  potential  impact  on  illness  such  as  hypertension,  high  cholesterol,  type  2  diabetes  and  asthma.  This  paper  reports  on  the  design  and  usage  of  iN  Touch,  a  mobile  self-management  application  for  tracking  observations  of  daily  living  (ODLs)  in  a  health  coaching  program  for  low-income,  urban,  minority  youth  13---24  years  with  overweight/obesity.  We  applied  a  youth-centered,  participatory  design  approach  to  design  and  implementation  of  the  technology  and  intervention  with  a  representative  10-member  youth  advisory  board.  The  recommendations  were  implemented  prior  to  launching  the  technology  in  an  intervention  phase.  The  application  with  food,  exercise,  mood  and  socializing  trackers  along  with  pictures  and  notes  was  delivered  on  an  iPod  Touch  to  24  participants.  Mixed  methods  were  applied  to  evaluate  technology  acceptance  including  system-generated  data,  questionnaires  and  exit  interviews.  There  was  good  engagement  among  participants  who  recorded  2,117  ODLs  over  6  months.  The  mean  rating  for  usefulness  was  3.50/5,  SD  =  1.18  and  for  ease  of  use,  3.83/5,  SD  =  1.27.  Qualitative  analysis  of  exit  interviews  found  that  design  recommendations  were  fulfilled  and  the  resulting  technology  was  compelling.  Future  papers  will  report  on  the  health  impacts  of  iN  Touch.
2	Cosar  hybrid  reasoning  for  context  aware  activity  recognition.  Human  activity  recognition  is  a  challenging  problem  for  context-aware  systems  and  applications.  Research  in  this  field  has  mainly  adopted  techniques  based  on  supervised  learning  algorithms,  but  these  systems  suffer  from  scalability  issues  with  respect  to  the  number  of  considered  activities  and  contextual  data.  In  this  paper,  we  propose  a  solution  based  on  the  use  of  ontologies  and  ontological  reasoning  combined  with  statistical  inferencing.  Structured  symbolic  knowledge  about  the  environment  surrounding  the  user  allows  the  recognition  system  to  infer  which  activities  among  the  candidates  identified  by  statistical  methods  are  more  likely  to  be  the  actual  activity  that  the  user  is  performing.  Ontological  reasoning  is  also  integrated  with  statistical  methods  to  recognize  complex  activities  that  cannot  be  derived  by  statistical  methods  alone.  The  effectiveness  of  the  proposed  technique  is  supported  by  experiments  with  a  complete  implementation  of  the  system  using  commercially  available  sensors  and  an  Android-based  handheld  device  as  the  host  for  the  main  activity  recognition  module.
2	Beyond  sensors  reading  patients  through  caregivers  and  context.  Mobile  technology  for  remotely  sensing  key  health  indicators  about  patients  receiving  long-term  or  outpatient  care  continues  to  become  more  affordable  and  more  easily  embedded,  but  there  remain  certain  patient  variables,  especially  mental  health  and  adaptive  functioning  characteristics,  that  are  difficult  to  automatically  detect  or  problematic  to  self-report.  To  address  this  problem,  we  are  working  on  technology  that  integrates  input  from  caregivers  (as  well  as  patients)  with  enhanced  context  reporting.  We  describe  how  leveraging  both  methods  in  an  application  designed  for  use  by  PTSD/mTBI  patients  and  their  caregivers  can  potentially  lead  to  more  informed  clinical  care  teams,  better  family  engagement  of  the  care  process,  and  potentially  better  treatment  outcomes.
2	Generating  recommendations  for  consensus  negotiation  in  group  personalization  services.  There  are  increasingly  many  personalization  services  in  ubiquitous  computing  environments  that  involve  a  group  of  users  rather  than  individuals.  Ubiquitous  commerce  is  one  example  of  these  environments.  Ubiquitous  commerce  research  is  highly  related  to  recommender  systems  that  have  the  ability  to  provide  even  the  most  tentative  shoppers  with  compelling  and  timely  item  suggestions.  When  the  recommendations  are  made  for  a  group  of  users,  new  challenges  and  issues  arise  to  provide  compelling  item  suggestions.  One  of  the  challenges  a  group  recommender  system  must  cope  with  is  the  potentially  conflicting  preferences  of  multiple  users  when  selecting  items  for  recommendation.  In  this  paper,  we  focus  on  how  individual  user  models  can  be  aggregated  to  reach  a  consensus  on  recommendations.  We  describe  and  evaluate  nine  different  consensus  strategies  and  analyze  them  to  highlight  the  benefits  of  group  recommendation  using  live-user  preference  data.  Moreover,  we  show  that  the  performance  is  significantly  different  among  strategies.
2	Tuis  vs  guis  comparing  the  learning  potential  with  preschoolers.  In  an  effort  to  better  understand  the  learning  potential  of  a  tangible  interface,  we  conducted  a  comparison  study  between  a  tangible  and  a  traditional  graphical  user  interface  for  teaching  preschoolers  (In  Portugal,  children  enter  preschool  at  the  age  of  three  and  they  attend  it  till  entering  school,  normally  at  the  age  of  six)  about  good  oral  hygiene.  The  study  was  carried  with  two  groups  of  children  aged  4  to  5  years.  Questionnaires  to  parents,  children's  drawings,  and  interviews  were  used  for  data  collection  and  analysis  and  revealed  important  indicators  about  children's  change  of  attitude,  involvement,  and  preferences  for  the  interfaces.  The  questionnaires  showed  a  remarkable  change  of  attitude  toward  tooth  brushing  in  the  children  that  interacted  with  the  tangible  interface;  particularly  children's  motivation  increased  significantly.  Children's  drawings  were  used  to  assess  their  degree  of  involvement  with  the  interfaces.  The  drawings  from  the  children  that  interacted  with  the  tangible  interface  were  very  complete  and  detailed  suggesting  that  the  children  felt  actively  involved  with  the  experience.  The  results  suggest  that  the  tangible  interface  was  capable  of  promoting  a  stronger  and  long-lasting  involvement  having  a  greater  potential  to  engage  children,  therefore  potentially  promoting  learning.  Evaluation  through  drawing  seems  to  be  a  promising  method  to  work  with  preliterate  children;  however,  it  is  advisable  to  use  it  together  with  other  methods.
2	I  did  not  smoke  100  cigarettes  today  avoiding  false  positives  in  real  world  activity  recognition.  Activity  recognition  (AR)  systems  are  typically  built  and  evaluated  on  a  predefined  set  of  activities.  AR  systems  work  best  if  the  test  data  contains  and  only  contains  these  predefined  activities.  In  real  world  applications,  AR  systems  trained  in  this  manner  generate  serious  false  positives,  for  example  if  "smoking"  is  one  of  the  activities  in  the  training  data  but  "lifting  weights"  is  not.  Due  to  the  similarity  of  two  activities,  an  AR  system  may  report  a  user  smoking  100  times  a  day  but  he  actually  did  a  bicep  workout  100  times.  In  this  work,  we  propose  a  new  approach  to  train  an  AR  system  leveraging  the  large  quantity  of  unlabeled  data  which  reflects  activities  users  perform  in  real  life.  The  proposed  mPUL  (Multi-class  Positive  and  Unlabeled  Learning)  approach  significantly  reduces  the  false  positives.  We  argue  that  mPUL  is  a  much  more  effective  training  method  for  real-world  AR  applications.
2	Understanding  individuals  phone  call  behavior  for  calendar  events.  The  goal  of  this  position  paper  is  to  highlight  the  issues  of  modeling  individuals'  phone  call  response  behavior  for  their  various  scheduled  events  in  calendar  and  to  describe  the  key  aspects  that  constitute  the  foundation  of  our  behavioral  model  to  overcome  such  issues.
2	Tangibles  for  learning  a  representational  analysis  of  physical  manipulation.  Manipulatives--physical  learning  materials  such  as  cubes  or  tiles--are  prevalent  in  educational  settings  across  cultures  and  have  generated  substantial  research  into  how  actions  with  physical  objects  may  support  children's  learning.  The  ability  to  integrate  digital  technology  into  physical  objects--so-called  `digital  manipulatives'--has  generated  excitement  over  the  potential  to  create  new  educational  materials.  However,  without  a  clear  understanding  of  how  actions  with  physical  materials  lead  to  learning,  it  is  difficult  to  evaluate  or  inform  designs  in  this  area.  This  paper  is  intended  to  contribute  to  the  development  of  effective  tangible  technologies  for  children's  learning  by  summarising  key  debates  about  the  representational  advantages  of  manipulatives  under  two  key  headings:  offloading  cognition--where  manipulatives  may  help  children  by  freeing  up  valuable  cognitive  resources  during  problem  solving,  and  conceptual  metaphors--where  perceptual  information  or  actions  with  objects  have  a  structural  correspondence  with  more  symbolic  concepts.  The  review  also  indicates  possible  limitations  of  physical  objects--most  importantly  that  their  symbolic  significance  is  only  granted  by  the  context  in  which  they  are  used.  These  arguments  are  then  discussed  in  light  of  tangible  designs  drawing  upon  the  authors'  current  research  into  tangibles  and  young  children's  understanding  of  number.
2	Rfid  and  nfc  in  hospital  environments  reaching  a  sustainable  approach.  The  increase  of  safety  and  the  improvement  of  care  received  by  the  patient  during  their  healthcare  process  are  one  of  the  main  challenges  facing  health  professionals.  Obtaining  patient  traceability  and  minimising  the  occurrence  of  adverse  events  during  the  perscription-validation-dispensing-administration  process  of  medication  to  patients,  encourages  making  measures  of  improvement  to  ensure  the  quality  of  the  processes  that  take  place  in  the  clinical  practice  of  a  hospital.  It  is  therefore  essential  to  study  current  leading  technologies  such  as  RFID  and  NFC  in  a  sustainable  way  to  determine  the  feasibility  of  its  application  in  the  healthcare  environment.
2	Optimized  iot  based  decision  making  for  autonomous  vehicles  in  intersections.  Applications  that  use  communication  networks  and  distributed  systems  to  control  traffic  have  high  latency,  especially  in  critical  situations.  The  performance  of  these  applications  largely  depends  on  the  computational  delay  of  algorithms  that  run  on  local  or  central  processors.  Therefore,  providing  an  optimized  solution  to  minimize  this  delay  to  a  tolerable  range  is  highly  needed.  This  article  studies  a  method  in  which  autonomous  vehicles  around  an  intersection  try  to  control  the  intersection  traffic  efficiently  by  communicating  and  interacting  with  each  other  and  road-side  smart  devices.  This  problem  can  be  addressed  in  the  form  of  a  network  utility  maximization  problem.  To  achieve  a  solution  that  is  close  to  an  optimal  solution,  a  gradient  descent  algorithm  with  a  fixed  step  size  can  be  utilized.  It  is  necessary  to  find  a  balance  between  latency  and  accuracy,  which  leads  to  finding  a  velocity  close  to  the  optimal  velocity.  The  number  of  loop  repetitions  in  the  scheduling  algorithm,  determines  the  latency  in  preparation  for  making  the  proper  schedule  for  autonomous  vehicles.  In  this  work,  we  propose  an  approach  to  provide  an  optimized  schedule  for  autonomous  vehicles  in  intersections  considering  pedestrian  traffic.  Autonomous  vehicles  are  able  to  communicate  with  each  other  and  road  side  unites.  However,  surveillance  cameras  are  required  to  observe  pedestrians  passing  the  intersection.  Hence,  we  utilize  cameras,  smart  sensors,  processors,  and  communication  equipment  embedded  in  autonomous  vehicles  and  road  side  unites,  to  collect  the  required  data,  process  it,  and  distribute  the  calculated  optimal  decision  to  autonomous  vehicles.  To  simulate  the  traffic  behaviors  resulting  from  applying  the  proposed  solution,  Simulation  of  Urban  Mobility  software  is  used.
2	Things  of  the  internet  toi  physicalization  of  notification  data.  When  it  comes  to  attention  and  notification  management,  most  of  the  previous  attempts  to  visualise  notifications  and  smartphone  usage  have  focused  on  digital  representations  on  screens  that  are  not  fully  embedded  in  the  users'  environment.  Today,  the  constant  development  in  hardware  and  embedded  systems  including  mini  displays,  LEDs,  actuators  as  well  as  digital  fabrication,  have  begun  to  provide  new  opportunities  for  representing  data  physically  in  surrounding  environments.  In  this  paper,  we  introduce  a  new  way  of  visualising  notification  data  using  physical  representations  that  are  deeply  integrated  within  the  physical  space  and  everyday  objects.  Based  on  our  preliminary  design  and  prototypes,  we  identify  a  variety  of  design  challenges  for  embedded  data  representation,  and  suggest  opportunities  for  future  research.
2	Flair  towards  a  therapeutic  serious  game  for  social  anxiety  disorder.  In  this  paper  we  present  Flair,  an  interactive  fiction  game  that  is  intended  to  serve  as  a  psycho-educational  material  for  the  therapeutic  treatment  of  Social  Anxiety  Disorder  (SAD).  Along  with  the  game  design  approach,  explanation  of  the  inclusion  of  cognitive-behavioral  therapy  (CBT)  techniques  into  Flair's  story,  and  the  psychological  benefits  of  doing  so,  is  extensively  discussed.  The  initial  results  are  encouraging  as  patients  (15  users)  and  a  therapist  respond  positive  on  the  current  game  design.
2	Quantum  computation  from  church  turing  thesis  to  qubits.  In  the  paper  we  interpret  the  Church  Turing  thesis  of  Computation  Algorithm  and  how  it  differs  from  the  strong  Church  Turing  thesis  in  the  field  of  computation.  Church  Turing  thesis  can  be  classified  as  Strong  Church  Turing  thesis  and  Normal  Church  Turing  thesis.  Church  Turing  hypothesis  says  “that  a  computing  problem  can  be  solved  in  any  computer  if  and  only  if  it  can  be  solved  on  a  very  simple  ‘machine’,  named  TURING  MACHINE”.  We  introspect  the  difference  in  the  model  of  computers  that  are  developed  on  these  two  competitive  algorithms.  We  will  also  be  studying  the  physical  realization  of  bits  in  a  quantum  computer  with  special  mention  to  the  commercial  quantum  computer  systems  offered  at  the  moment.  Our  objective  is  to  find  out  the  relation  of  a  quantum  bit  and  the  Strong  Church  Turing  Thesis  converging  the  gap  between  theoretical  model  and  physical  structure.
2	Activity  recognition  in  a  home  setting  using  off  the  shelf  smart  watch  technology.  Being  able  to  detect  in  real-time  the  activity  performed  by  a  user  in  a  home  setting  provides  highly  valuable  context.  It  can  allow  more  effective  use  of  novel  technologies  in  a  large  variety  of  applications,  from  comfort  and  safety  to  energy  efficiency,  remote  health  monitoring  and  assisted  living.  In  a  home  setting,  activity  recognition  has  been  traditionally  studied  based  on  either  a  large  sensor  network  infrastructure  already  set  up  in  a  home,  or  a  network  of  wearable  sensors  attached  to  various  parts  of  the  user's  body.  We  argue  that  both  approaches  suffer  considerably  in  terms  of  practicality  and  propose  instead  the  use  of  commercial-shelf  smart  watches,  already  owned  by  the  users.  We  test  the  feasibility  of  this  approach  with  two  different  smart  watches  of  very  different  capabilities,  on  a  variety  of  activities  performed  daily  in  a  domestic  environment,  from  brushing  teeth  to  preparing  food.  Our  experimental  results  are  encouraging,  as  using  standard  Support  Vector  Machine  based  classification,  the  accuracy  rates  range  between  88%  and  100%,  depending  on  the  type  of  smart  watch  and  the  window  size  chosen  for  data  segmentation.
2	Design  of  a  mac  protocol  for  e  emergency  wsns.  The  wide  adoption  of  WSNs  in  healthcare  is  still  conditioned  by  quality  of  service  (QoS)  issues,  namely  at  the  MAC  level.  Medium  access  protocols  currently  available  for  WSNs  are  incapable  of  providing  the  required  QoS  to  healthcare  applications  in  scenarios  of  emergency  or  intensive  medical  care.  To  fill  this  lacuna,  this  paper  introduces  a  MAC  protocol  presenting  novel  concepts  to  assure  the  QoS  of  e-emergency  WSNs.  Preliminary  validation  tests  showed  that  the  proposed  protocol  presents  a  good  performance  regarding  data  transmission  robustness  without  sacrificing  the  power  consumption  efficiency.
2	Privacy  behaviors  of  lifeloggers  using  wearable  cameras.  A  number  of  wearable  'lifelogging'  camera  devices  have  been  released  recently,  allowing  consumers  to  capture  images  and  other  sensor  data  continuously  from  a  first-person  perspective.  Unlike  traditional  cameras  that  are  used  deliberately  and  sporadically,  lifelogging  devices  are  always  'on'  and  automatically  capturing  images.  Such  features  may  challenge  users'  (and  bystanders')  expectations  about  privacy  and  control  of  image  gathering  and  dissemination.  While  lifelogging  cameras  are  growing  in  popularity,  little  is  known  about  privacy  perceptions  of  these  devices  or  what  kinds  of  privacy  challenges  they  are  likely  to  create.      To  explore  how  people  manage  privacy  in  the  context  of  lifelogging  cameras,  as  well  as  which  kinds  of  first-person  images  people  consider  'sensitive,'  we  conducted  an  in  situ  user  study  (N  =  36)  in  which  participants  wore  a  lifelogging  device  for  a  week,  answered  questionnaires  about  the  collected  images,  and  participated  in  an  exit  interview.  Our  findings  indicate  that:  1)  some  people  may  prefer  to  manage  privacy  through  in  situ  physical  control  of  image  collection  in  order  to  avoid  later  burdensome  review  of  all  collected  images;  2)  a  combination  of  factors  including  time,  location,  and  the  objects  and  people  appearing  in  the  photo  determines  its  'sensitivity;'  and  3)  people  are  concerned  about  the  privacy  of  bystanders,  despite  reporting  almost  no  opposition  or  concerns  expressed  by  bystanders  over  the  course  of  the  study.
2	Understanding  user  behavior  at  scale  in  a  mobile  video  chat  application.  Online  video  chat  services  such  as  Chatroulette  and  Omegle  randomly  match  users  in  video  chat  sessions  and  have  become  increasingly  popular,  with  tens  of  thousands  of  users  online  at  anytime  during  a  day.  Our  interest  is  in  examining  user  behavior  in  the  growing  domain  of  mobile  video,  and  in  particular  how  users  behave  in  such  video  chat  services  as  they  are  extended  onto  mobile  clients.  To  date,  over  four  thousand  people  have  downloaded  and  used  our  Android-based  mobile  client,  which  was  developed  to  be  compatible  with  an  existing  video  chat  service.  The  paper  provides  a  first-ever  detailed  large  scale  study  of  mobile  user  behavior  in  a  random  video  chat  service  over  a  three  week  period.  This  study  identifies  major  characteristics  such  as  mobile  user  session  durations,  time  of  use,  demographic  distribution  and  the  large  number  of  brief  sessions  that  users  click  through  to  find  good  matches.  Through  content  analysis  of  video  and  audio,  as  well  as  analysis  of  texting  and  clicking  behavior,  we  discover  key  correlations  among  these  characteristics,  e.g.,  normal  mobile  users  are  highly  correlated  with  using  the  front  camera  and  with  the  presence  of  a  face,  whereas  misbehaving  mobile  users  have  a  high  negative  correlation  with  the  presence  of  a  face.
2	Computer  vision  based  license  plate  detection  for  automated  vehicle  parking  management  system.  With  proliferation  of  vehicles  across  the  world,  it  is  getting  increasingly  strenuous  to  manage  parking  in  several  spaces  viz  business  parks,  residential  complexes,  shopping  malls  etc.  An  optimum  utilization  of  available  parking  spaces  and  minimizing  the  time  and  effort  involved  in  vehicle  parking,  an  integrated  and  automated  vehicle  parking  management  system  (VPMS)  is  necessitated.  License  plate  contains  relevant  information  about  vehicle  and  its  detection  &  recognition  in  real  time  can  be  utilized  to  develop  an  automated  VPMS.  In  this  paper,  a  solution  is  proposed  for  live  detection  and  recognition  of  a  moving  vehicle's  license  plate  number  using  Computer  Vision  techniques.  Three  different  models  had  been  studied  viz  HAAR  cascade  and  CNN1,  OpenCV2  and  YOLOv3  with  OpenCV3  to  find  the  best  performing  model.  Among  these,  YOLOv3  with  OpenCV  outperforms  other  models  due  to  its  ability  to  detect  the  rectangular  bounding  boxes  with  great  accuracy.  The  automation  of  license  plate  detection  is  a  two-step  process  which  includes  detection  of  custom  object  i.e  License  plate  using  YOLOv3  and  recording/processing  the  number  plate  details  using  Open  CV  algorithms.  The  trained  model  is  validated  and  demonstrated  100%  accuracy  in  detection  of  license  plate  bounding  boxes  along  with  95%  accuracy  in  text  recognition.  This  module  can  be  implemented  and  integrated  with  other  add-on  systems  for  effective  usage  in  various  sectors.
2	Enabling  the  new  economic  actor  data  protection  the  digital  economy  and  the  databox.  This  paper  offers  a  sociological  perspective  on  data  protection  regulation  and  its  relevance  to  design.  From  this  perspective,  proposed  regulation  in  Europe  and  the  USA  seeks  to  create  a  new  economic  actor--the  consumer  as  personal  data  trader--through  new  legal  frameworks  that  shift  the  locus  of  agency  and  control  in  data  processing  towards  the  individual  consumer  or  "data  subject".  The  sociological  perspective  on  proposed  data  regulation  recognises  the  reflexive  relationship  between  law  and  the  social  order,  and  the  commensurate  needs  to  balance  the  demand  for  compliance  with  the  design  of  computational  tools  that  enable  this  new  economic  actor.  We  present  the  Databox  model  as  a  means  of  providing  data  protection  and  allowing  the  individual  to  exploit  personal  data  to  become  an  active  player  in  the  emerging  data  economy.
2	Protecting  the  sink  location  privacy  in  wireless  sensor  networks.  Wireless  sensor  networks  (WSNs)  are  widely  deployed  to  collect  data  in  military  and  civilian  applications  today.  Due  to  the  open  nature  of  a  WSN,  it  is  relatively  easy  for  an  adversary  to  eavesdrop  and  trace  packets  in  order  to  capture  the  receiver.  Therefore,  location  privacy,  particularly  the  location  privacy  of  the  sink  node,  requires  ultimate  protection  because  of  its  critical  position  in  WSNs.  In  this  paper,  we  propose  a  sink  location  privacy  protection  scheme  by  injecting  fake  packets,  but  every  real  packet  is  still  routed  along  its  shortest  path.  The  fake  packets  are  routed  to  some  random  destinations  and  some  fake  sinks  in  order  to  provide  the  path  diversity.  It  is  difficult  for  an  attacker  to  distinguish  the  real  packets  from  the  fake  packets.  Thus,  the  chance  of  finding  the  real  sink  by  packet-tracing  attack  is  reduced.  Privacy  analysis  shows  that  the  sink  location  privacy  can  be  protected  better  with  higher  successful  probability.  We  examine  the  packet  travel  delay,  safe  time,  and  energy  consumption  by  both  mathematical  analysis  and  simulations.
2	Lunchtime  a  slow  casual  game  for  long  term  dietary  behavior  change.  Eating  out  has  recently  become  part  of  our  lifestyle.  However,  when  eating  out  in  restaurants,  many  people  find  it  difficult  to  make  meal  choices  consistent  with  their  health  goals.  Bad  eating  choices  and  habits  are  in  part  responsible  for  the  alarming  increase  in  the  prevalence  of  chronic  diseases  such  as  obesity,  diabetes,  and  high  blood  pressure,  which  burden  the  health  care  system.  Therefore,  there  is  a  need  for  an  intervention  that  educates  the  public  on  how  to  make  healthy  choices  while  eating  away  from  home.  In  this  paper,  we  propose  a  goal-based  slow-casual  game  approach  that  addresses  this  need.  This  approach  acknowledges  different  groups  of  users  with  varying  health  goals  and  adopts  slow  technology  to  promote  learning  and  reflection.  We  model  two  recognized  determinants  of  well-being  into  dietary  interventions  and  provide  feedback  accordingly.  To  demonstrate  the  suitability  of  our  approach  for  long-term  sustained  learning,  reflection,  and  attitude  and/or  behavior  change,  we  develop  and  evaluate  LunchTime--a  goal-based  slow-casual  game  that  educates  players  on  how  to  make  healthier  meal  choices.  The  result  from  the  evaluation  shows  that  LunchTime  facilitates  learning  and  reflection  and  promotes  positive  dietary  attitude  change.
2	Supporting  parents  for  in  home  capture  of  problem  behaviors  of  children  with  developmental  disabilities.  Ubiquitous  computing  has  shown  promise  in  applications  for  health  care  in  the  home.  In  this  paper,  we  focus  on  a  study  of  how  a  particular  ubicomp  capability,  selective  archiving,  can  be  used  to  support  behavioral  health  research  and  practice.  Selective  archiving  technology,  which  allows  the  capture  of  a  window  of  data  prior  to  and  after  an  event,  can  enable  parents  of  children  with  autism  and  related  disabilities  to  record  video  clips  of  events  leading  up  to  and  following  an  instance  of  problem  behavior.  Behavior  analysts  later  view  these  video  clips  to  perform  a  functional  assessment.  In  contrast  to  the  current  practice  of  direct  observation,  a  powerful  method  to  gather  data  about  child  problem  behaviors  but  costly  in  terms  of  human  resources  and  liable  to  alter  behavior  in  the  subjects,  selective  archiving  is  cost  effective  and  has  the  potential  to  provide  rich  data  with  minimal  instructions  to  the  natural  environment.  To  assess  the  effectiveness  of  parent  data  collection  through  selective  archiving  in  the  home,  we  developed  a  research  tool,  CRAFT  (Continuous  Recording  And  Flagging  Technology)  and  conducted  a  study  by  installing  CRAFT  in  eight  households  of  children  with  developmental  disabilities  and  severe  behavior  concerns.  The  results  of  this  study  show  the  promise  and  remaining  challenges  for  this  technology.  We  have  also  shown  that  careful  attention  to  the  design  of  a  ubicomp  system  for  use  by  other  domain  specialists  or  non-technical  users  is  key  to  moving  ubicomp  research  forward.
2	Inferring  human  mobility  patterns  from  taxicab  location  traces.  Taxicabs  equipped  with  real-time  location  sensing  devices  are  increasingly  becoming  popular.  Such  location  traces  are  a  rich  source  of  information  and  can  be  used  for  congestion  pricing,  taxicab  placement,  and  improved  city  planning.  An  important  problem  to  enable  these  application  is  to  identify  human  mobility  patterns  from  the  taxicab  traces,  which  translates  to  being  able  to  identify  pickup  and  dropoff  points  for  a  particular  trip.  In  this  paper,  we  show  that  while  past  approaches  are  effective  in  detecting  hotspots  using  location  traces,  they  are  largely  ineffective  in  identifying  trips  (pairs  of  pickup  and  dropoff  points).  We  propose  the  use  of  a  graph  theory  concept  -  stretch  factor  in  a  novel  manner  to  identify  trip(s)  made  by  a  taxicab  and  show  that  a  Hidden  Markov  Model  based  algorithm  can  identify  trips  (using  real  datasets  from  taxicab  deployments  in  Shanghai  and  partially  simulated  datasets  from  Stockholm)  with  precision  and  recall  of  90-94%,  a  significant  improvement  over  past  approaches  that  result  in  a  precision  and  recall  of  about  50-60%.
2	A  new  scalable  multicast  routing  algorithm  for  interactive  real  time  applications.  Quality  of  service  (QoS)  provisioning  generally  assumes  more  than  one  QoS  measure  that  implies  that  QoS  routing  can  be  categorized  as  an  instance  of  routing  subject  to  multiple  constraints:  delay  jitter,  bandwidth,  cost,  etc.  We  study  the  problem  of  constructing  multicast  trees  to  meet  the  QoS  requirements  of  real-time  interactive  applications  where  it  is  necessary  to  provide  bounded  delays  and  bounded  delay  variation  among  the  source  and  all  destinations  while  keeping  overall  cost  of  the  multicast  tree  low.  The  main  contribution  of  our  work  is  a  new  strategy  for  constructing  multiconstrained  multicast  trees.  We  first  derive  mathematically  a  new  delay-variation  estimation  scheme  and  prove  its  efficiency.  Thereafter,  we  propose  a  simple  and  competitive  (in  terms  of  running  time)  heuristic  algorithm,  for  delay  and  delay  variation  constrained  routing  problem  based  on  the  proposed  delay-variation  estimation  scheme  and  using  the  Extended  Prim-Dijkstra  tradeoffs'  algorithm.  Our  contribution  also  extends  previous  works  in  providing  some  properties  and  analyses  of  delay  bounded  paths  satisfying  delay  variation  constraints.  Extensive  simulation  results  show  that  our  algorithm  outperforms  DVDMR  in  terms  of  multicast  delay  variation  with  the  same  time  complexity  as  DVDMR.
2	Users  sleeping  time  analysis  based  on  micro  blogging  data.  The  emergence  of  new  social  network  services,  often  labeled  as  Web  2.0,  has  permitted  an  amazingly  increase  of  user  generated  content.  In  particular,  Sina  Weibo,  a  popular  Chinese  micro-blogging  service  is  designed  as  platforms  allowing  users  to  generate  contents  that  open  to  the  public.  From  analyzing  activates  of  submitting  posts  to  Sina  Weibo,  some  features  of  users  can  be  estimated.  This  paper  aims  to  contribute  to  this  growing  body  of  literature  by  studying  how  users'  frequent  activities  reflect  their  sleeping  time  and  living  time  zones.  By  mining  a  large  set  of  users'  activates  data  from  Sina  Weibo,  we  demonstrate  its  possible  role  to  detect  the  sleeping  time  of  users  and  find  a  new  method  for  judging  users'  time  zone.
2	What  can  people  nearby  applications  teach  us  about  meeting  new  people.  'People-nearby'  applications  for  meeting  new  people  online  are  some  of  the  most  popular  examples  of  systems  that  lead  people  from  an  online  interaction  to  an  offline  interaction.  This  paper  provides  a  critical  review  of  available  applications,  and  identifies  three  key  properties  that  are  essential  for  the  applications:  physical  location,  identity  management,  and  trust.  The  paper  suggests  open  research  questions  that  can  explain  the  success  of  these  applications  and  guide  the  design  of  new  technologies  that  encourage  offline  interactions.
2	Semantic  trajectories  based  social  relationships  discovery  using  wifi  monitors.  Smartphones  are  configured  to  automatically  send  WiFi  probe  message  transmissions  (latter  called  WiFi  probes)  to  surrounding  environments  to  search  for  available  networks.  Prior  studies  have  provided  evidence  that  it  is  possible  to  uncover  social  relationships  of  mobile  users  by  studying  time  and  location  information  contained  in  these  WiFi  probes.  However,  their  approaches  miss  information  about  transfer  patterns  between  different  locations.  In  this  paper,  we  argue  that  places  where  mobile  users  have  been  to  should  not  be  considered  in  isolation.  We  propose  that  semantic  trajectory  should  be  used  to  model  mobile  users  and  semantic  trajectory  patterns  can  well  characterize  users'  transfer  patterns  between  different  locations.  Then,  we  propose  a  novel  semantic  trajectory  similarity  measurement  to  estimate  similarity  among  mobile  users.  We  deploy  WiFi  detectors  in  a  university  to  collect  WiFi  probes  and  extract  mobile  users'  semantic  trajectories  from  the  dataset.  Through  experimental  evaluation,  we  demonstrate  that  the  proposed  semantic  trajectory  similarity  measurement  is  effective.  Furthermore,  we  experimentally  show  that  the  proposed  trajectory  similarity  measurement  can  be  used  to  exploit  underlying  social  networks  existing  in  the  university,  as  well  as  infer  specific  type  of  social  relationships  between  a  pair  of  mobile  users  by  further  studying  their  matching  trajectory  points.
2	Quality  matters  usage  based  app  popularity  prediction.  In  recent  years,  mobile  application  (app)  economy  has  grown  to  a  huge  market  but  it  is  only  the  top  apps  that  are  able  to  turn  this  boom  into  significant  revenues.  In  this  paper,  we  study  how  the  quality  of  an  app,  as  reflected  in  how  people  start  to  use  it,  is  linked  to  the  popularity  of  the  app.  We  show  that  features  extracted  from  the  Device  Analyzer  dataset,  describing  the  aggregate  usage  of  the  app,  can  be  used  to  predict  its  popularity.  We  also  look  at  the  connection  between  app  popularity  and  the  past  popularity  of  other  apps  from  the  same  publisher  and  find  a  surprisingly  small  correlation  between  the  two.
2	Interweaving  place  and  story  in  a  location  based  audio  drama.  This  paper  presents  a  qualitative  study  of  an  interactive  audio  drama  facilitated  by  a  location-based  application.  The  investigation  focuses  on  an  accessible  experience,  a  play  in  which  the  audience  members  simply  trigger  new  scenes  of  the  audio  drama  as  they  walk  to  predefined  city  areas.  The  findings  draw  attention  to  the  role  of  the  mobile  technology  in  facilitating  this  particular  artistic  experience.  Furthermore,  they  illustrate  the  various  levels  at  which  creative  imagination  and  open  interpretation  emerge  as  audience  members  seek  to  make  sense  of  the  interrelations  between  the  locative  media  experienced  and  elements  of  the  places  inhabited  during  the  audio  narrative.  In  concluding  the  article,  designing  for  loose  coupling  between  mobile  media  and  physical  places  is  suggested  as  a  strategy  to  enable  people's  engagement  in  meaningful  experience  through  the  use  of  various  location-based  services.
2	Eyes  wide  open  eyelid  location  and  eye  aperture  estimation  for  pervasive  eye  tracking  in  real  world  scenarios.  Eyelid  identification  and  aperture  estimation  provide  key  data  that  can  be  used  to  infer  valuable  information  about  a  subject's  mental  state  (e.g.,  vigilance,  fatigue,  and  drowsiness)  as  well  as  validate  or  reduce  the  search  space  of  other  eye  features.  In  this  paper,  we  consider  these  tasks  from  the  perspective  of  pervasive  eye  tracking,  taking  into  account  the  multiple  challenges  and  constraints  that  arise  from  this  scenario.  A  novel  method  for  eyelid  identification  and  aperture  estimation  is  proposed  and  evaluated  against  challenging  data  from  an  eye-tracking  experiment  conducted  in  driving  scenarios  in  the  wild.  The  proposed  method  outperforms  an  state-of-the-art  approach  by  up  to  40  percentage  points  and  runs  in  real-time  on  state-of-the-art  eye  tracking  systems.  The  method  implementation  and  the  realistic  dataset  are  provided  openly  at  www.ti.uni-tuebingen.de/perception.
2	Internet  of  things  and  body  area  network  an  integrated  future.  Modern  technological  advancement  in  sensors  technology,  miniaturization  of  devices  and  wireless  networking  facilitated  the  design  and  proliferation  of  wireless  sensor  networks  by  making  it  capable  to  monitor  independently  and  controlling  the  ambience.  One  of  the  most  important  applications  of  sensor  networks  is  for  human  health  monitoring  using  minuscule  wireless  sensors,  placed  strategically  on  the  human  body,  constitute  a  wireless  network  over  the  human  body,  termed  as  wireless  body  area  network  (WBAN)  capable  of  administering  various  crucial  implications  and  provides  feedback  on  real-time  basis  to  the  user  and  supervising  medical  personnel.  The  Internet  of  Things  (IoT)  can  be  measured  as  the  futuristic  appraisal  of  the  internet  that  realizes  machine-to-machine  learning  and  communication.  As  a  result,  IoT  offers  connectivity  for  everyone  and  everything.  These  two  networks  in  integration  provides  connectivity  between  everything  and  anything.  This  paper  is  a  study  on  integrated  IoT  and  WBAN.
2	Walk  sketch  create  floor  plans  with  an  rgb  d  camera.  Creating  floor  plans  for  large  areas  via  manual  surveying  is  labor-intensive  and  error-prone.  In  this  paper,  we  present  a  system,  Walk&Sketch,  that  creates  floor  plans  of  an  indoor  environment  by  a  person  walking  through  the  environment  at  a  normal  strolling  pace  and  taking  videos  using  a  consumer  RGB-D  camera.  The  method  computes  floor  maps  represented  by  polylines  from  a  3D  point  cloud  based  on  precise  frame-to-frame  alignment.  It  aligns  a  reference  frame  with  the  floor  and  computes  the  frame-to-frame  offsets  from  the  continuous  RGB-D  input.  Line  segments  at  a  certain  height  are  extracted  from  the  3D  point  cloud,  and  are  merged  to  form  a  polyline  map,  which  can  be  further  modified  and  annotated  by  users.  The  explored  area  is  visualized  as  a  sequence  of  polygons,  providing  users  with  the  information  on  coverage.  Experiments  have  done  in  various  areas  of  an  office  building  and  have  shown  encouraging  results.
2	Robots  in  the  smart  home  a  project  towards  interoperability.  In  this  paper,  it  is  addressed  the  problem  of  integrating  service  robots  in  the  smart  home.  To  this  end,  several  middleware  initiatives  are  analysed,  paying  special  attention  to  Universal  Plug  and  Play  (UPnP).  In  addition,  the  iRobot  service  robot  Roomba  is  deeply  studied  and  an  application  to  integrate  it  in  the  smart  home  via  UPnP  is  presented.
2	Mdht  a  multi  level  indexed  dht  algorithm  to  extra  large  scale  data  retrieval  on  hdfs  hadoop  architecture.  Corresponding  to  the  storing  and  fast  searching  needs  of  an  extra-large  scale  of  energy  monitoring  and  statistics  data,  we  propose  a  multi-level-indexed  distributed  hash  table  (mDHT)  algorithm  and  complete  a  MapReduce  implementation  of  the  algorithm  on  the  open-standard  HDFS/Hbase  platform.  Such  an  approach  uses  a  columnar  storage  structure  for  energy  consumption  data  storage  and  creates  a  hashed  index  table  to  provide  a  quick  search  and  retrieval  method  for  extra-large-scale  data  processing  systems.  Such  a  hashed  indexing  scheme  is  implemented  on  a  3-node  Hadoop  cluster,  and  the  simulation  experiments  at  a  scale  up  to  48  million  data  records  indicate  that,  when  the  data  volume  reaches  the  scale  of  12  million  to  48  millions,  the  proposed  mDHT  algorithm  presents  an  outstanding  performance  in  data  writing  operation,  compared  to  that  of  traditional  SQL  Server  implementation.  Even  compared  to  the  single-indexed  DHT  (sDHT)  application,  the  mDHT  solution  outperforms  by  reducing  the  data  retrieval  time  by  24.5---48.6  %.  The  multi-level-indexed  DHT  algorithm  presented  in  this  paper  contributes  a  key  technique  to  developing  a  fast  search  engine  to  the  extra-large  scale  of  data  on  the  cloud  storage  architecture.
2	Foldlings  a  tool  for  interactive  pop  up  card  design.  Crafting  a  3D  paper  pop-up  is  a  creative  and  playful  experience,  which  can  help  develop  spatial  reasoning  skills.  However,  designing  the  cuts  and  folds  is  often  a  frustrating  trial-and-error  process  due  to  the  tight  set  of  geometric  constraints.  We  introduce  Foldlings:  an  iPad  application  that  assists  in  this  exploratory  process.  Our  tool-based  approach  allows  users  of  all  skill  levels  to  create  complex  cards  with  ease,  by  separating  folding  geometries  into  logical  units.  We  present  a  novel  user  interface  for  creating  popup  cards  from  user  input,  leveraging  the  practical  two-dimensional  format  of  paper.  Our  approach  uses  a  modular  set  of  simple  drawing  tools,  which  can  be  combined  to  create  complex  designs.  We  guarantee  the  validity  of  the  folded  card  at  all  stages  of  design,  and  our  interface  provides  an  intuitive  set  of  visual  aids  to  help  users  understand  the  relationship  between  2D  patterns  and  3D  geometry.  We  describe  the  algorithms  and  data  structures  used  in  interpreting  2D  user  input  and  visualizing  the  3D  geometry  of  the  folded  card.  We  discuss  results  of  user  studies,  which  guided  early  stages  of  UI  design  and  demonstrate  the  efficacy  of  the  final  prototype.
2	Vector  field  k  means  clustering  trajectories  by  fitting  multiple  vector  fields.  Scientists  study  trajectory  data  to  understand  trends  in  movement  patterns,  such  as  human  mobility  for  traffic  analysis  and  urban  planning.  In  this  paper,  we  introduce  a  novel  trajectory  clustering  technique  whose  central  idea  is  to  use  vector  fields  to  induce  a  notion  of  similarity  between  trajectories,  letting  the  vector  fields  themselves  define  and  represent  each  cluster.  We  present  an  efficient  algorithm  to  find  a  locally  optimal  clustering  of  trajectories  into  vector  fields,  and  demonstrate  how  vector-field  k-means  can  find  patterns  missed  by  previous  methods.  We  present  experimental  evidence  of  its  effectiveness  and  efficiency  using  several  datasets,  including  historical  hurricane  data,  GPS  tracks  of  people  and  vehicles,  and  anonymous  cellular  radio  handoffs  from  a  large  service  provider.
2	Spherical  visibility  sampling.  Many  3D  scenes  (e.g.  generated  from  CAD  data)  are  composed  of  a  multitude  of  objects  that  are  nested  in  each  other.  A  showroom,  for  instance,  may  contain  multiple  cars  and  every  car  has  a  gearbox  with  many  gearwheels  located  inside.  Because  the  objects  occlude  each  other,  only  few  are  visible  from  outside.  We  present  a  new  technique,  Spherical  Visibility  Sampling  (SVS),  for  real-time  3D  rendering  of  such  --  possibly  highly  complex  --  scenes.  SVS  exploits  the  occlusion  and  annotates  hierarchically  structured  objects  with  directional  visibility  information  in  a  preprocessing  step.  For  different  directions,  the  directional  visibility  encodes  which  objects  of  a  scene's  region  are  visible  from  the  outside  of  the  regions'  enclosing  bounding  sphere.  Since  there  is  no  need  to  store  a  separate  view  space  subdivision  as  in  most  techniques  based  on  preprocessed  visibility,  a  small  memory  footprint  is  achieved.  Using  the  directional  visibility  information  for  an  interactive  walkthrough,  the  potentially  visible  objects  can  be  retrieved  very  efficiently  without  the  need  for  further  visibility  tests.  Our  evaluation  shows  that  using  SVS  allows  to  preprocess  complex  3D  scenes  fast  and  to  visualize  them  in  real  time  (e.g.  a  Power  Plant  model  and  five  animated  Boeing  777  models  with  billions  of  triangles).  Because  SVS  does  not  require  hardware  support  for  occlusion  culling  during  rendering,  it  is  even  applicable  for  rendering  large  scenes  on  mobile  devices.
2	Survey  of  models  for  acquiring  the  optical  properties  of  translucent  materials.  The  outset  of  realistic  rendering  is  a  desire  to  reproduce  the  appearance  of  the  real  world.  Rendering  techniques  therefore  operate  at  a  scale  corresponding  to  the  size  of  objects  that  we  observe  w  ...
2	Reducing  anisotropic  bsdf  measurement  to  common  practice.  We  address  the  problem  of  measuring  and  representing  reflection  and  transmission  for  anisotropic  materials  without  relying  on  mathematical  models  or  a  large  sample  database.  By  eliminating  assumptions  of  material  behavior,  we  arrive  at  a  general  method  that  works  for  any  surface  class,  from  metals  to  fabrics,  fritted  glazing,  and  prismatic  films.  To  make  data  gathering  practical,  we  introduce  a  robust  analysis  method  that  interpolates  a  sparse  set  of  incident  angle  measurements  to  obtain  a  continuous  function  over  the  full  4-D  domain.  We  then  convert  this  interpolant  to  a  standard  representation  tailored  for  efficient  rendering  and  supported  by  a  common  library  that  facilitates  data  sharing.  We  conclude  with  some  remaining  challenges  to  making  anisotropic  BSDF  measurements  truly  practical  for  rendering.
2	Non  rigid  3d  shape  retrieval.  Non-rigid  3D  shape  retrieval  has  become  a  research  hotpot  in  communities  of  computer  graphics,  computer  vision,  pattern  recognition,  etc.  In  this  paper,  we  present  the  results  of  the  SHREC'15  Track:  Non-rigid  3D  Shape  Retrieval.  The  aim  of  this  track  is  to  provide  a  fair  and  effective  platform  to  evaluate  and  compare  the  performance  of  current  non-rigid  3D  shape  retrieval  methods  developed  by  different  research  groups  around  the  world.  The  database  utilized  in  this  track  consists  of  1200  3D  watertight  triangle  meshes  which  are  equally  classified  into  50  categories.  All  models  in  the  same  category  are  generated  from  an  original  3D  mesh  by  implementing  various  pose  transformations.  The  retrieval  performance  of  a  method  is  evaluated  using  6  commonly-used  measures  (i.e.,  PR-plot,  NN,  FT,  ST,  E-measure  and  DCG.).  Totally,  there  are  37  submissions  and  11  groups  taking  part  in  this  track.  Evaluation  results  and  comparison  analyses  described  in  this  paper  not  only  show  the  bright  future  in  researches  of  non-rigid  3D  shape  retrieval  but  also  point  out  several  promising  research  directions  in  this  topic.
2	Efficient  ray  tracing  through  aspheric  lenses  and  imperfect  bokeh  synthesis.  We  present  an  efficient  ray-tracing  technique  to  render  bokeh  effects  produced  by  parametric  aspheric  lenses.  Contrary  to  conventional  spherical  lenses,  aspheric  lenses  do  generally  not  permit  a  simple  closed-form  solution  of  ray-surface  intersections.  We  propose  a  numerical  root-finding  approach,  which  uses  tight  proxy  surfaces  to  ensure  a  good  initialization  and  convergence  behavior.  Additionally,  we  simulate  mechanical  imperfections  resulting  from  the  lens  fabrication  via  a  texture-based  approach.  Fractional  Fourier  transform  and  spectral  dispersion  add  additional  realism  to  the  synthesized  bokeh  effect.  Our  approach  is  well-suited  for  execution  on  graphics  processing  units  (GPUs)  and  we  demonstrate  complex  defocus-blur  and  lens-flare  effects.
2	Interactive  volume  rendering  with  volumetric  illumination.  Interactive  volume  rendering  in  its  standard  formulation  has  become  an  increasingly  important  tool  in  many  application  domains.  In  recent  years  several  advanced  volumetric  illumination  techniques  t  ...
2	Guided  volume  editing  based  on  histogram  dissimilarity.  Segmentation  of  volumetric  data  is  an  important  part  of  many  analysis  pipelines,  but  frequently  requires  manual  inspection  and  correction.  While  plenty  of  volume  editing  techniques  exist,  it  remains  cumbersome  and  errorprone  for  the  user  to  find  and  select  appropriate  regions  for  editing.  We  propose  an  approach  to  improve  volume  editing  by  detecting  potential  segmentation  defects  while  considering  the  underlying  structure  of  the  object  of  interest.  Our  method  is  based  on  a  novel  histogram  dissimilarity  measure  between  individual  regions,  derived  from  structural  information  extracted  from  the  initial  segmentation.  Based  on  this  information,  our  interactive  system  guides  the  user  towards  potential  defects,  provides  integrated  tools  for  their  inspection,  and  automatically  generates  suggestions  for  their  resolution.  We  demonstrate  that  our  approach  can  reduce  interaction  effort  and  supports  the  user  in  a  comprehensive  investigation  for  high-quality  segmentations.
2	Mergeia  a  service  for  dynamic  merging  of  interfering  adaptations  in  ubiquitous  system.  The  composition  of  adaptations  with  system’s  application  does  not  always  yield  to  the  desired  behavior.  Each  adaptation  occurs  correctly  when  it  is  separated  but  it  may  interact  with  other  adaptations  when  they  are  combined.  These  interactions  can  affect  the  final  behavior  after  adaptation;  we  call  this  an  interference.  This  paper  presents  an  on-going  work,  which  aims  to  build  a  generic  approach  for  the  dynamic  resolution  of  adaptation  interferences  in  ubiquitous  applications.  We  represent  application  and  adaptation  details  by  graphs;  then  we  apply  graph  transformation  rules  on  these  graphs  to  resolve  interferences.  This  allows  us  to  express  our  approach  independently  of  any  implementation  details  of  applications  and  adaptations.
2	Elckerlyc  goes  mobile  enabling  technology  for  ecas  in  mobile  applications.  The  fast  growth  of  computational  resources  and  speech  technology  available  on  mobile  devices  makes  it  pos-  sible  for  users  of  these  devices  to  interact  with  service  sys-  tems  through  natural  dialogue.  These  systems  are  sometimes  perceived  as  social  agents  and  presented  by  means  of  an  animated  embodied  conversational  agent  (ECA).  To  take  the  full  advantage  of  the  power  of  ECAs  in  service  systems,  it  is  important  to  support  real-time,  online  and  responsive  interaction  with  the  system  through  the  ECA.  The  design  of  responsive  animated  conversational  agents  is  a  daunting  task.  Elckerlyc  is  a  model-based  platform  for  the  specification  and  animation  of  synchronised  multimodal  responsive  animated  agents.  This  paper  presents  a  new  light-weight  PictureEngine  that  allows  this  platform  to  embed  an  ECA  in  the  user  interface  of  mobile  applications.  The  ECA  can  be  specified  by  using  the  behavior  markup  language  (BML).  An  application  and  user  evaluations  of  Elckerlyc  and  the  PictureEngine  in  a  mobile  embedded  digital  coach  is  presented.
2	Towards  a  software  infrastructure  for  district  energy  management.  Nowadays  ICT  is  becoming  a  key  factor  to  enhance  the  energy  optimization  in  our  cities.  At  district  level,  real-time  information  can  be  accessed  to  monitor  and  control  the  energy  distribution  network.  Moreover,  the  fine  grain  monitoring  and  control  done  at  building  level  can  provide  additional  information  to  develop  more  efficient  control  policies  for  energy  distribution  in  the  district.  In  this  paper  we  present  a  distributed  software  infrastructure  for  district  energy  management,  which  aims  to  provide  a  digital  archive  of  the  city  in  which  energetic  information  is  available.  Such  information  is  considered  as  the  input  for  a  decision  system,  which  aims  to  increase  the  energy  efficiency  by  promoting  local  balancing  and  shaving  peak  loads.  As  case  study,  we  integrated  in  our  proposed  cloud  the  heating  distribution  network  in  Turin  and  we  present  exploitable  options  based  on  real-world  environmental  data  to  increase  the  energy  efficiency  and  minimize  the  peak  request.
2	Security  paradigm  in  ubiquitous  computing.  The  vast  development  of  Information  and  Communication  Technologies  and  the  innovations  applied  in  the  field  of  governance  and  management  push  the  researchers  to  change  their  perspectives  in  finding  new  security  paradigms.  The  major  effort  regards  the  capability  to  identify  some  appropriate  tools  that  have  the  characteristic  of  better  fit  with  the  ``object''  to  protect  in  the  real  world.  One  of  main  aspect  that  can  ensure  the  success  in  this  operation  is  the  correct  integration  and  harmonization  of  the  human  factor  with  all  remaining  factors  of  a  security  system.  This  paper  discusses  why  the  CIA  (Confidentiality,  Integrity  and  Availability)  paradigm  is  no  more  valid  and  able  to  perform  its  effect  in  a  post-modern  world,  and  why  Cloud  and  Pervasive  Computing  requires  a  new  approach  in  which  the  user  become  the  main  actor  of  the  entire  security  system.
2	Biographbot  a  conversational  assistant  for  bioinformatics  graph  databases.  Chatbots  technology  allows  to  easily  add  a  conversational  interface  to  a  large  set  of  applications.  In  this  paper,  we  show  a  conversational  agent  based  on  ALICE  framework  aimed  at  playing  the  role  of  interface  between  human  users  and  a  bioinformatics  graph  database.  The  system  has  been  embedded  into  the  web  frontend  of  BioGraphDB,  a  publicly  available  resource  that  uses  Gremlin  as  the  primary  query  language.  To  be  successfully  exploited,  domain  experts,  such  as  biologists  and  bioinformaticians,  should  also  have  familiarity  with  that  query  language.  The  use  of  a  chatbot  allows  translating  queries  expressed  in  natural  language  to  queries  expressed  in  Gremlin,  simplifying  the  interaction  with  BioGraphDB.
2	Data  aggregation  techniques  in  heart  vessel  modelling  and  recognition  of  pathological  changes.  One  of  the  more  important  fields  in  which  data  aggregation  methods  could  be  applied  is  the  analysis  of  medical  images  showing  the  complicated  morphology  of  examined  structures  or  organs.  Such  techniques  support  a  comprehensive  description  of  the  analysed  image  and  formulating  the  appropriate  diagnosis  by  identifying  the  most  important  parameters  of  given  structures,  and  then  analysing  their  meaning  by  aggregating  various  information.  In  this  paper  is  presented  a  proposal  for  the  semantic  analysis  of  the  heart's  coronary  vessels  for  diagnostic  and  therapeutic  purposes  on  images  originating  from  diagnostic  examinations  with  128-slice  spiral  computed  tomography.  Such  techniques  can  be  of  great  significance  and  find  a  number  of  uses  in  image  recognition.
2	Synchronization  of  peers  in  peer  to  peer  publish  subscribe  systems.  In  distributed  systems,  a  group  of  multiple  processes  are  cooperating  with  one  another  by  exchanging  messages  in  networks.  In  this  paper,  we  consider  a  peer-to-peer  (P2P)  model  of  a  topic-based  publish/subscribe  (P2PPS)  system  composed  of  peer  processes  (peers).  Each  peer  pi  can  both  subscribe  a  subscription  pi.  S  and  publish  an  event  message  e  with  a  publication  e.P,  which  are  specified  in  terms  topics,  i.e.  Keywords.  An  event  message  e  is  notified  to  a  target  peer  pi  whose  subscription  pi.S  includes  a  common  keyword  with  the  publication  e.P.  Each  event  message  e  carries  a  vector  e.V  =  (V1,  ,  Vm)  of  keywords  k1,  ,  km.  An  event  message  e1  causally  precedes  an  event  message  e2  with  respect  to  a  subscription  Siiffe1  causally  precedes  e2  with  respect  to  the  send-receive  relation  and  e1.Vh  <  e2.Vh  for  every  keyword  kh  in  the  publications  e1.P  and  e2.P  and  the  subscription  pi.S.  If  a  pair  of  event  messages  e1  and  e2  are  ordered  so  that  e1  precedes  e2  even  if  e1  does  not  causally  precede  e2,  the  event  messages  e1  and  e2  are  unnecessarily  ordered.  In  this  paper,  every  pair  of  messages  are  causally  delivered  to  every  common  target  peer  by  using  the  linear  clock  and  keyword  vector  to  reduce  pairs  of  messages  unnecessarily  ordered.  We  evaluate  the  keyword  vectors  in  terms  of  number  of  messages  unnecessarily  ordered.
2	Qualitative  evaluation  of  full  body  movements  with  gesture  description  language.  The  motivation  for  writing  this  paper  is  to  present  the  method  for  qualitative  evaluation  of  full  body  movements  with  Gesture  Description  Language.  By  qualitative  evaluation  we  understand  the  possibility  of  evaluating  the  similarity  between  the  template  of  movement  defined  in  GDL  script  (GDLs)  and  physical  activity  performed  by  observed  user.  The  establishing  of  this  methodology  allows  us  to  perform  real  time  evaluation  of  movements  accuracy  defined  as  similarity  to  the  statistically  computed  templates.  The  presented  methodology  can  be  used  not  only  to  perform  advanced  numerical  analysis  of  movements  but  also  to  support  multimedia  visual  feedback  to  various  movements  activities  in  computer  applications.  The  initial  evaluation  presented  in  this  paper  proved  that  proposed  method  of  qualitative  analysis  of  movements  has  a  potential  to  be  used  in  practice.
2	Monitoring  patients  with  mental  disorders.  Mental  disorders  impose  significant  socio-economic  and  geo-political  challenges  which,  if  not  addressed,  have  the  potential  to  overwhelm  the  ability  of  healthcare  systems  globally  to  accommodate  the  growing  demands  both  in  human  and  resource  management  terms.  From  a  treatment  perspective  there  is  a  need  to  implement  multi-modal  systems  where  patients  can  be  diagnosed,  treated,  and  monitored.  Such  systems  must  incorporate  triage  and  treatment  capabilities  in  both  hospital  settings  with  monitoring  in  the  community.  This  paper  considers  the  practical  challenges  in  realising  the  goal  of  achieving  the  effective  monitoring  of  patients  with  mental  disorders  in  Smart-Psychiatric  Intensive  Care  Units  and  in  the  community.  Illustrative  scenarios  are  presented.  We  conclude  that  effective  patient  monitoring  will  provide  benefits  for  all  stakeholders  in  the  management  of  mental  disorders.
2	Developing  useful  mobile  applications  in  cross  media  platforms.  As  the  app  market  expands,  new  challenges  arise  for  developers.  Creating  useful  applications  is  important  in  achieving  extensive  and  cohesive  use.  This  paper  presents  a  mobile  application  called  "The  Time  Machine",  offering  location-dependent  information  about  the  environment  in  a  town.  The  app  is  evaluated  through  expert  evaluation,  as  well  as  user  evaluation  conducted  as  a  field  observation  with  15  participating  users,  measuring  the  level  of  usability  in  the  application.  Results  are  used  to  discuss  guidelines  on  how  to  develop  useful  mobile  applications.  Knowledge  about  user  behavior,  a  clear  application  purpose,  an  understandable  interface,  and  cross-platform  functionality  are  all  important  aspects  for  developers  to  consider.
2	Infrastructure  setup  and  deployment  in  sensing  cloud  environments.  Sensing  and  actuation  as  a  service  (SAaaS)  is  a  novel  paradigm,  modeled  after  Cloud  and  service  computing,  envisioning  an  Infrastructure-oriented  (IaaS-like)  provisioning  model  for  sensors  and  actuators.  In  past  works  we  defined  a  framework  architecture  for  dealing  with  the  requirements  arising  from  the  SAaaS  criteria,  considering  the  involvement  of  both  mobiles  and  sensor  networks.  This  provided  a  static  view  of  the  solution  proposed,  lacking  details  on  how  to  effectively  apply  the  paradigm  exploiting  its  benefits.  This  paper  addresses  the  topic  from  a  different  perspective,  providing  a  dynamic  view  of  the  scenarios  and  therefore  tackling  SAaaS  infrastructure  setup  and  application  service  deployment.  We  identified  a  few  outstanding  use  cases,  which  are  described  in  operative  terms  to  show  the  applicability  and  usefulness  of  the  proposed  solution.
2	Supporting  first  responders  localization  during  crisis  management.  In  recent  years,  homeland  security  has  represented  one  of  the  most  relevant  application  contexts,  which  has  collected  the  interest  of  both  public  authorities  and  research  community.  In  case  of  a  crisis  event,  it  is  the  responsibility  of  public  and  government  authorities  to  manage  the  response  operations.  One  of  the  constant  challenges  encountered  when  managing  emergency  is  the  lack  of  real-time  location-aware  information  by  the  first  responders  that  act  on  the  crisis  site.  This  paper  aims  at  drawing  an  overall  picture  of  the  achieved  research  progressed  for  supporting  localization  of  first  responders  during  crisis  events  occurring  in  urban  areas  and  involving  critical  infrastructures.  In  particular,  this  paper  presents  a  hybrid  positioning  approach,  which  combines  the  strength  of  signals  received  from  landmark  nodes  placed  by  first  responders  on  the  crisis  site  with  information  achieved  from  motion  sensors.  Finally,  we  investigate  the  key  research  topics  in  the  considered  area  and  the  potential  impact  on  the  future  developments  scenario.
2	Malicious  script  blocking  detection  technology  using  a  local  proxy.  The  key  feature  of  HTML5,  the  next-generation  web  standard  announced  in  W3  in  October  2015,  might  be  the  enhancement  of  JavaScript  functions.  While  the  previous  generation  HTML  required  non-standard  plug-ins  such  as  Silverlight  or  Active  X  for  media  play  and  web  socket  communication,  HTML5  provides  the  functions  via  new  APIs  including  JavaScript  audio  and  video,  which  are  powerful  features  that  can  replace  non-standard  technologies  like  Active-X.  Like  this,  web  browser  developers  are  rushing  to  make  their  browsers  HTML5  compatible  and  a  number  of  projects  for  converting  into  the  HTML5  environment  are  ongoing  all  over  the  world.  But  along  with  this  trend,  there  are  increasing  threats  of  new  types  of  cyberattacks  using  Java  script,  which  is  the  core  function  of  HTML5.  Unlike  a  traditional  attack  using  malicious  code,  existing  security  technologies  have  limits  in  detecting  new  types  of  attacks  as  connecting  to  web  pages  with  malscript  causes  malicious  behavior  without  any  infection  to  the  user  PC.  So  this  paper  suggests  methods  to  detect  and  block  malscript  and  obfuscated  malicious  script  by  collecting  and  analyzing  HTTP  traffic  via  local  proxy.
2	A  collaborative  safety  flight  control  system  for  multiple  drones.  There  are  a  rapid  growth  of  the  Drone's  market,  and  so  it  is  expects  us  various  applications  such  as  for  a  military  use,  a  surveillance  system,  a  delivery  services,  and  a  pesticide  spraying  for  harvests.  However,  with  the  wide  spread  of  drones,  the  accidents  or  crimes  by  them  are  rapidly  increasing.  Therefore,  in  this  paper,  the  collaborative  safety  flight  control  system  for  multiple  drones  is  proposed.  The  proposed  methods  mainly  consist  of  the  pattern  recognition  from  the  camera  images,  collaborative  drone  controls  by  multiple  drones,  and  emergent  controls  in  uncontrollable  situations.  Then,  the  prototype  of  the  proposed  methods  is  introduced,  the  experimental  reports  are  discussed  for  the  future  works.
2	Friend  recommendation  for  location  based  mobile  social  networks.  Along  with  the  rapid  growth  of  Internet,  many  social  websites  are  founded,  and  gradually  begin  to  influence  the  people's  life.  Such  as  Facebook,  the  social  network  site  provides  the  personalized  recommendation  system  with  friends-of-friends  method  to  recommend  new  friends  to  users.  The  intuition  is  derived  from  the  idea  that  it  is  more  probable  a  person  will  know  a  friend  of  their  friends  rather  than  a  random  person.  However,  this  approach  does  not  consider  any  insights  into  human  cognitive  components  such  as  social  interactions.  Thus,  we  propose  a  brand-new  friend  recommendation  approach.  The  main  concept  is  to  recommend  friends  who  have  the  similar  interests  or  another  thing  with  self  to  users.  Besides  utilizing  the  information  on  social  networks,  such  as  interests,  the  concept  of  real-life  location  and  dwell  time  is  further  added  in  our  approach.  In  this  paper,  we  develop  two  comparison  methods  to  provide  quality  friend  recommendation.  First  method  combines  the  existing  landmark  and  user's  dwell  time  at  certain  landmark  to  make  the  Voronoi  diagram,  and  analyzes  location  similarity  between  users.  Second  methods  is  to  analyze  the  interest  lists  from  each  social  network  accounts  by  using  pattern  matching  and  finding  longest  common  subsequence.  Through  this  two  comparison  methods,  we  assess  the  acceptable  degree  between  two,  and  successfully  implement  the  friend  recommendation  system.
2	An  efficient  identity  based  strong  designated  verifier  signature  without  delegatability.  We  propose  a  non-delegatable  identity  based  strong  designated  verifier  signature  scheme.  It  combines  an  identity  based  Schnorr  signature  and  an  identification  method.  An  OR  proof  technique  glues  the  two  parts.  It  is  the  second  scheme  secure  in  a  strict  model  proposed  by  Huang  et  al.  And  it  saves  about  half  the  communication  and  computation  costs  comparing  to  the  first  scheme.
2	The  analysis  of  higher  mathematics  teaching  strategy  based  on  the  innovative  teaching  mode.  During  the  epidemic,  China’s  Higher  Education  Vocational  Colleges  carried  out  large-scale,  full-time  online  mathematics  curriculum  teaching.  Taking  it  as  an  opportunity,  this  paper  selects  the  higher  mathematics  course  of  a  vocational  college  in  Western  China  during  the  epidemic  period  as  the  starting  point  to  build  a  blended  teaching  mode  of  higher  mathematics  combining  online  and  offline  teaching.  It  analyzes  the  teaching  effect  of  pure  offline  teaching  of  mathematics  course  in  the  early  stage  by  means  of  questionnaire  survey,  and  evaluates  the  teaching  quality  of  pure  online  teaching  of  mathematics  course  in  the  near  future  based  on  the  real-time  data  of  the  rain  classroom  teaching  platform.  The  purpose  of  this  paper  is  to  accelerate  the  integration  of  educational  information  technology  and  higher  mathematics  teaching,  and  promote  the  innovation  of  classroom  teaching.  We  should  stress  the  teaching  idea  of  “students  as  the  main  part,  teachers  as  the  leading  role”,  rely  on  the  teaching  reform,  build  a  strong  teaching  staff,  cultivate  the  comprehensive  quality  of  students,  and  provide  higher  quality  technical  talents  for  the  society.
