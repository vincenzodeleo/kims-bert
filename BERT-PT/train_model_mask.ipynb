{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527d5d61",
   "metadata": {},
   "source": [
    "**FINE TUNING BERT MODELS FOR MASKED LANGUAGE MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc0c8e",
   "metadata": {},
   "source": [
    "**Code reference**\n",
    "* masked language modeling script: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "* custom token masking example: https://www.analyticsvidhya.com/blog/2022/09/fine-tuning-bert-with-masked-language-modeling/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bf1da",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27aa808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import set_seed\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from src.models import save_model_and_tokenizer\n",
    "\n",
    "import warnings\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb04d1",
   "metadata": {},
   "source": [
    "**Define libraries parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6f3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action=\"ignore\")\n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc664f",
   "metadata": {},
   "source": [
    "**Check gpu info if available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bee935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 20 16:49:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:65:00.0 Off |                  N/A |\n",
      "| 38%   61C    P2    57W / 250W |    991MiB / 11175MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e3550",
   "metadata": {},
   "source": [
    "**Clean gpu memory cache if gpu is available and set torch device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efe0a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cuda.is_available():\n",
    "    gc.collect()\n",
    "    cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62fc7f",
   "metadata": {},
   "source": [
    "**Define useful parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318afbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_FILEPATH = \"data/processed/airbnb_london_20220910.parquet\"\n",
    "INPUT_FILEPATH = \"data/processed/cso_v3.3.parquet\"\n",
    "INPUT_FILENAME = INPUT_FILEPATH.split(\"/\")[-1].split(\".\")[0] if \"cso_v3.3.parquet\" not in INPUT_FILEPATH else \"cso_v3.3\"\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"bert-base-uncased\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "METRIC = evaluate.load(path=\"accuracy\")\n",
    "\n",
    "MODEL_OUTPUT_DIR = f\"models/{PRETRAINED_MODEL_NAME_OR_PATH}-{INPUT_FILENAME}-mask/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffeab4",
   "metadata": {},
   "source": [
    "**Load input file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77bac6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22090</th>\n",
       "      <td>USB phone is related to Computer peripherals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>Irish Terrier is related to Breed standard (do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11743</th>\n",
       "      <td>Kentucky head city Frankfort, Kentucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>Badminton is related to Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>City Pulse is called City Pulse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "22090      USB phone is related to Computer peripherals \n",
       "11050  Irish Terrier is related to Breed standard (do...\n",
       "11743            Kentucky head city Frankfort, Kentucky \n",
       "1483                     Badminton is related to Canada \n",
       "4707                    City Pulse is called City Pulse "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(path=INPUT_FILEPATH)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad058e3",
   "metadata": {},
   "source": [
    "**Split dataframe into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f65abe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 24075, validation set size: 1268\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.05, random_state=42)\n",
    "print(f\"training set size: {df_train.shape[0]}, validation set size: {df_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5487a",
   "metadata": {},
   "source": [
    "**Convert pandas dataframe into huggingface dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9c8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df=df_train)\n",
    "valid_dataset = Dataset.from_pandas(df=df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd57531",
   "metadata": {},
   "source": [
    "**Load model configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed086c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_kwargs = {\n",
    "    \"revision\": \"main\",\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    **config_kwargs\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152389d0",
   "metadata": {},
   "source": [
    "**Load tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620d3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": True,\n",
    "    \"revision\": \"main\"\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    **tokenizer_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a30a0",
   "metadata": {},
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9239e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    config=config,\n",
    "    revision=\"main\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc742a1",
   "metadata": {},
   "source": [
    "**Define function to tokenize texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853cb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example: dict, text_key: str = \"text\") -> dict:\n",
    "    \"\"\"Function to tokenize text examples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    example : dict\n",
    "        Dict with the texts to tokenize.\n",
    "    text_key : str, default='text'\n",
    "        Key name with text values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Input texts tokenized.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        text=example[text_key],\n",
    "        return_special_tokens_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b234695",
   "metadata": {},
   "source": [
    "**Example of tokenization on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccfe665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 3046, 2000, 19204, 4697, 2023, 999, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]], 'special_tokens_mask': [[1, 0, 0, 0, 0, 0, 0, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {\"text\": [\"Try to tokenize this!\"]}\n",
    "encoded_text = tokenize_function(example)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd611b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'try', 'to', 'token', '##ize', 'this', '!', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids=encoded_text[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a27f69",
   "metadata": {},
   "source": [
    "**Apply tokenization to the whole dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f0be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7ff4749493a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0139874a90443baa39a69dd9341546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c161c88104d54b16aaa683d5326f7a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(function=tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_valid_dataset = valid_dataset.map(function=tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a49c06",
   "metadata": {},
   "source": [
    "**Define function to group input texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d191af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples: dict, max_seq_length: int = 128) -> dict:\n",
    "    \"\"\"Group input texts into chuncks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        Dictionary with text examples to group.\n",
    "    max_seq_length : int, default=512\n",
    "        Max sequence length for grouping.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Input texts grouped.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: np.hstack(list(examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    if total_length >= max_seq_length:\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc6395",
   "metadata": {},
   "source": [
    "**Apply group operation on dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0492e42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236c7b590b4645d39be99419156158c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce917b4108f401885ba785b9e545a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_train_dataset = tokenized_train_dataset.map(function=group_texts, batched=True, batch_size=128)\n",
    "lm_test_dataset = tokenized_valid_dataset.map(function=group_texts, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8745ff7",
   "metadata": {},
   "source": [
    "**Define functions to compute metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219368d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(path=\"accuracy\")\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Preprocessing model logits for classification metrics.\"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute classification metrics.\"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    labels = labels.reshape(-1)\n",
    "    preds = preds.reshape(-1)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d456b43",
   "metadata": {},
   "source": [
    "**Define training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42437af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=5e-05,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    save_strategy=\"epoch\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831f105",
   "metadata": {},
   "source": [
    "**Define data collator for masked language modeling**  \n",
    "reference: https://github.com/huggingface/transformers/blob/c836f77266be9ace47bff472f63caf71c0d11333/src/transformers/data/data_collator.py#L609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a94ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab4d9a",
   "metadata": {},
   "source": [
    "**Define trainer object for model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1435eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 189\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 03:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.884815</td>\n",
       "      <td>0.585987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.445691</td>\n",
       "      <td>0.625954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.152229</td>\n",
       "      <td>0.651316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.977283</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.203666</td>\n",
       "      <td>0.595588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.190649</td>\n",
       "      <td>0.656442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.145309</td>\n",
       "      <td>0.628205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.075120</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.415602</td>\n",
       "      <td>0.646667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.063743</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.071491</td>\n",
       "      <td>0.686567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.268507</td>\n",
       "      <td>0.637584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.125523</td>\n",
       "      <td>0.692857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.144661</td>\n",
       "      <td>0.694611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.315238</td>\n",
       "      <td>0.696774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.476586</td>\n",
       "      <td>0.664706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.448348</td>\n",
       "      <td>0.643357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.240963</td>\n",
       "      <td>0.703448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.901518</td>\n",
       "      <td>0.729560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.296944</td>\n",
       "      <td>0.698718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-12\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-12/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-12/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-24\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-24/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-24/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-36\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-36/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-36/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-48\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-48/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-48/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-60\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-60/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-60/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-72\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-72/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-72/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-84\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-84/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-84/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-96\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-96/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-96/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-108\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-108/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-108/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-120\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-120/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-120/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-132\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-132/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-132/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-144\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-144/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-144/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-156\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-156/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-156/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-168\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-168/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-168/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-180\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-180/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-180/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-192\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-192/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-192/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-204\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-204/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-204/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-216\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-216/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-216/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-228\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-228/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-228/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-240\n",
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-240/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-240/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/bert-base-uncased-airbnb_london_20220910-mask/checkpoint-228 (score: 1.9015178680419922).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=2.0653133392333984, metrics={'train_runtime': 237.6427, 'train_samples_per_second': 15.906, 'train_steps_per_second': 1.01, 'total_flos': 248728548096000.0, 'train_loss': 2.0653133392333984, 'epoch': 20.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_train_dataset,\n",
    "    eval_dataset=lm_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c26a8a",
   "metadata": {},
   "source": [
    "**Compute metrics on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "231d246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__, special_tokens_mask. If __index_level_0__, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eval_loss</td>\n",
       "      <td>1.751728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.737226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eval_runtime</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eval_samples_per_second</td>\n",
       "      <td>116.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eval_steps_per_second</td>\n",
       "      <td>23.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>epoch</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    metric       value\n",
       "0                eval_loss    1.751728\n",
       "1            eval_accuracy    0.737226\n",
       "2             eval_runtime    0.086000\n",
       "3  eval_samples_per_second  116.267000\n",
       "4    eval_steps_per_second   23.253000\n",
       "5                    epoch   20.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "\n",
    "df_test_report = pd.DataFrame(data=[metrics]) \\\n",
    "                   .transpose() \\\n",
    "                   .reset_index() \\\n",
    "                   .rename(columns={\"index\": \"metric\", 0: \"value\"})\n",
    "df_test_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a0241",
   "metadata": {},
   "source": [
    "**Serialize fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c534c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/bert-base-uncased-airbnb_london_20220910-mask/config.json\n",
      "Model weights saved in models/bert-base-uncased-airbnb_london_20220910-mask/pytorch_model.bin\n",
      "tokenizer config file saved in models/bert-base-uncased-airbnb_london_20220910-mask/tokenizer_config.json\n",
      "Special tokens file saved in models/bert-base-uncased-airbnb_london_20220910-mask/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "save_model_and_tokenizer(model=model, tokenizer=tokenizer, save_directory=MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f64c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
