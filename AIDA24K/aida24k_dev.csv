,source_id,text,AI,SE,HCI,authors,title
0,0,"A gated self attention memory network for answer selection. Answer selection is an important research problem, with applications in many areas. Previous deep learning based approaches for the task mainly adopt the Compare-Aggregate architecture that performs word-level comparison followed by aggregation. In this work, we take a departure from the popular Compare-Aggregate architecture, and instead, propose a new gated self-attention memory network for the task. Combined with a simple transfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-of-the-art results on two standard answer selection datasets: TrecQA and WikiQA.",1,0,0,online learning+genetic selection+deep learning,
1,1,"Deep learning for multigrade brain tumor classification in smart healthcare systems a prospective survey. Brain tumor is one of the most dangerous cancers in people of all ages, and its grade recognition is a challenging problem for radiologists in health monitoring and automated diagnosis. Recently, numerous methods based on deep learning have been presented in the literature for brain tumor classification (BTC) in order to assist radiologists for a better diagnostic analysis. In this overview, we present an in-depth review of the surveys published so far and recent deep learning-based methods for BTC. Our survey covers the main steps of deep learning-based BTC methods, including preprocessing, features extraction, and classification, along with their achievements and limitations. We also investigate the state-of-the-art convolutional neural network models for BTC by performing extensive experiments using transfer learning with and without data augmentation. Furthermore, this overview describes available benchmark data sets used for the evaluation of BTC. Finally, this survey does not only look into the past literature on the topic but also steps on it to delve into the future of this area and enumerates some research directions that should be followed in the future, especially for personalized and smart healthcare.",1,0,0,neural networks+convolutional neural networks+data augmentation,
2,2,"Deep metric learning with spherical embedding. Deep metric learning has attracted much attention in recent years, due to seamlessly combining the distance metric learning and deep neural network. Many endeavors are devoted to design different pair-based angular loss functions, which decouple the magnitude and direction information for embedding vectors and ensure the training and testing measure consistency. However, these traditional angular losses cannot guarantee that all the sample embeddings are on the surface of the same hypersphere during the training stage, which would result in unstable gradient in batch optimization and may influence the quick convergence of the embedding learning. In this paper, we first investigate the effect of the embedding norm for deep metric learning with angular distance, and then propose a spherical embedding constraint (SEC) to regularize the distribution of the norms. SEC adaptively adjusts the embeddings to fall on the same hypersphere and performs more balanced direction update. Extensive experiments on deep metric learning, face recognition, and contrastive self-supervised learning show that the SEC-based angular space learning strategy significantly improves the performance of the state-of-the-art.",1,0,0,optimization+neural networks+face recognition,
3,3,"Semantic parsing via paraphrasing. A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets.",1,0,0,knowledge base+question answering+parsing algorithm,
4,4,"Mpc based regional path tracking controller design for autonomous ground vehicles. Path tracking issues of autonomous ground vehicles (AGVs) have attracted more attention in recent years with the intelligent and electrified development of vehicles. In order to make AGVs path tracking problem more flexible, regional path tracking problem is discussed in this manuscript based on model predictive control (MPC) method, where the front wheel steering angle is regarded as the control variable. The feasible region for AGVs running is determined first according to the detected road boundaries. In the following, AGVs running in this region is considered using kinematic model. Then, in order to make the actual trajectory of AGVs keep in the region and satisfy the safety requirements, MPC method is employed to design path tracking controller considering the vehicle dynamics, the actuator and state constraints. In order to verify the effectiveness of the proposed algorithm, simulations under various test conditions are carried out using a high fidelity vehicle simulator veDYNA, where the Hongqi vehicle HQ430 parameters are matched. The results obtained from the simulation illustrate that the proposed algorithm obtains good performance in dealing with the regional path tracking problem.",1,0,0,tracking problem+tracking controller+feasible regions,
5,5,"Cross lingual knowledge graph alignment via graph matching neural network. Previous cross-lingual knowledge graph (KG) alignment studies rely on entity embeddings derived only from monolingual KG structural information, which may fail at matching entities that have different facts in two KGs. In this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in KG. From this view, the KB-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. Experiments show that our model outperforms previous state-of-the-art methods by a large margin.",1,0,0,directed graphs+graph matching+neural networks,
6,6,"Type supervised hidden markov models for part of speech tagging with incomplete tag dictionaries. Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data.",1,0,0,hidden markov models+test samples+pos taggers,
7,7,"Reducing noise in gan training with variance reduced extragradient. We study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with a novel stochastic variance-reduced extragradient (SVRE) optimization algorithm, which for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method on MNIST while being computationally cheaper, and that SVRE yields more stable GAN training on standard datasets.",1,0,0,stochastic+gan+generative adversarial networks,
8,8,"Ensemble methods for biclustering tasks. Several biclustering algorithms have been proposed in different fields of microarray data analysis. We present a new approach that improves their performance in using the ensemble methods. An ensemble biclustering is considered and formalized by a problem of binary triclustering. We propose a simple and efficient algorithm to solve it. To illustrate the interest of our ensemble approach, numerical experiments are performed on both artificial and real datasets with two biclustering algorithms commonly used in bioinformatics.",1,0,0,ensemble learning+real data sets+numerical experiments,
9,9,"A fast restarting particle swarm optimizer. Particle swarm optimization (PSO) is a swarm intelligence technique that optimizes a problem by iterative exploration and exploitation in the search space. However, PSO cannot achieve the preservation of population diversity on solving multimodal optimization problems, and once the swarm falls into local convergence, it cannot jump out of the local trap. In order to solve this problem, this paper presents a fast restarting particle swarm optimization (FRPSO), which uses a novel restarting strategy based on a discrete finite-time particle swarm optimization (DFPSO). Taking advantage of frequently speeding up the swarm to converge along with a greater exploitation capability and then jumping out of the trap, this algorithm can preserve population diversity and provide a superior solution. The experiment performs on twenty-five benchmark functions which consists of single-model, multimodal and hybrid composition problems, the experimental result demonstrates that the performance of the proposed FRPSO algorithm is better than the other three representatives of the advanced PSO algorithm on most of these functions.",1,0,0,particle swarm optimization (pso)+local convergence+particle swarm optimizers,
10,10,"Subspace gaussian mixture models for dialogues classification. The main objective of this paper is to identify themes from dialogues of telephone conversations in a real-life customer care service. In order to capture significant semantic content in spite of high expression variability, features are extracted in a large number of hidden spaces constructed with a Latent Dirichlet Allocation (LDA) approach. Multiple views of a spoke document can then be represented with several hidden topic models. Nonetheless, the model diversity due to the multi-model approach introduces a new type of variability. An approach is proposed based on features extracted in a common homogenous subspace with the purpose of reducing the multi-span representation variability. A Gaussian Mixture Model subspace model, inspired by previous work on speaker identification, is proposed for theme identification. This representation, novel for theme classification, is compared with the direct application of multiple topic-model representations. Experiments are reported using a corpus collected in the call center of the Paris Transportation Service. Results show the effectiveness of the proposed representation paradigm with a theme identification accuracy of 78.8%, showing a significant improvement with respect to previous results on the same corpus.",1,0,0,gaussian mixtures+gaussians+speaker identification,
11,11,"Primal dual mesh convolutional neural networks. Recent works in geometric deep learning have introduced neural networks that allow performing inference tasks on three-dimensional geometric data by defining convolution, and sometimes pooling, operations on triangle meshes. These methods, however, either consider the input mesh as a graph, and do not exploit specific geometric properties of meshes for feature aggregation and downsampling, or are specialized for meshes, but rely on a rigid definition of convolution that does not properly capture the local topology of the mesh. We propose a method that combines the advantages of both types of approaches, while addressing their limitations: we extend a primal-dual framework drawn from the graph-neural-network literature to triangle meshes, and define convolutions on two types of graphs constructed from an input mesh. Our method takes features for both edges and faces of a 3D mesh as input and dynamically aggregates them using an attention mechanism. At the same time, we introduce a pooling operation with a precise geometric interpretation, that allows handling variations in the mesh connectivity by clustering mesh faces in a task-driven fashion. We provide theoretical insights of our approach using tools from the mesh-simplification literature. In addition, we validate experimentally our method in the tasks of shape classification and shape segmentation, where we obtain comparable or superior performance to the state of the art.",1,0,0,inference+neural networks+convolutional neural networks,
12,12,"Language independent compound splitting with morphological operations. Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training, they pose a challenge for translation systems. Previous decompounding methods have often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. Furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach.",1,0,0,translation systems+machine translations+parallel corpora,
13,13,"Moving object detection in real time visual surveillance using background subtraction technique. In current scenario, the need of surveillance applications, technological improvements, and the numbers of active real-time surveillance systems are increasing continuously throughout the world. In this research work, we have presented a background subtraction based scheme for moving object detection in video frames. The proposed scheme has a strong potential for applications in consumer electronics and real time surveillance systems. This work is presented in three different sections, first is to model the background using initial few frames. Second is to extract moving object with a threshold and update the background. The threshold value is used to avoid unusual false detection. Third is enhancement where, we have used morphological operators in order to improve the detection quality. The employed Pixel Intensity Based Background Subtraction (PIBBS) scheme demonstrates how a system can be improved by means of a mean value controlling scheme and the incorporation of feedback based background updation scheme, showing an outranking performance in comparison with considered state-of-the-art models.",1,0,0,object detection+moving-object detection+background subtraction techniques,
14,14,"Generalized adaptation for few shot learning. Many Few-Shot Learning research works have two stages: pre-training base model and adapting to novel model. In this paper, we propose to use closed-form base learner, which constrains the adapting stage with pre-trained base model to get better generalized novel model. Following theoretical analysis proves its rationality as well as indication of how to train a well-generalized base model. We then conduct experiments on four benchmarks and achieve state-of-the-art performance in all cases. Notably, we achieve the accuracy of 87.75% on 5-shot miniImageNet which approximately outperforms existing methods by 10%.",1,0,0,parameter adaptation+base learners+multilayer perceptrons,
15,15,"Multi task deep neural networks for natural language understanding. In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",1,0,0,neural networks+language model+regularization,
16,16,"Estimating mutual information for discrete continuous mixtures. Estimation of mutual information from observed samples is a basic primitive in machine learning, useful in several learning tasks including correlation mining, information bottleneck, Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a quantity well-defined for general probability spaces, estimators have been developed only in the special case of discrete or continuous pairs of random variables. Most of these estimators operate using the 3H -principle, i.e., by calculating the three (differential) entropies of X, Y and the pair (X,Y). However, in general mixture spaces, such individual entropies are not well defined, even though mutual information is. In this paper, we develop a novel estimator for estimating mutual information in discrete-continuous mixtures. We prove the consistency of this estimator theoretically as well as demonstrate its excellent empirical performance. This problem is relevant in a wide-array of applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.",1,0,0,mutual informations+random variables+graphical model,
17,17,"Active learning for coreference resolution using discrete annotation. We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.",1,0,0,named entity recognition+dependency parsing+relation extraction,
18,18,"Probabilistic fasttext for multi sense word embeddings. We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the “strength” across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.",1,0,0,word embedding+word similarity+word sense,
19,19,"Design and response performance of capacitance meter for stretchable strain sensor. Flexible and stretchable electronics are expected to contribute to electric conductors, sensors and actuators for various applications, such as robotic systems and power-assisted systems. In this study, a stretchable strain sensor is proposed as a length sensor for a pneumatic artificial muscle. To measure the length of the artificial muscle, the stretchable strain sensor is required to have quick response characteristics. A capacitance measurement circuit based on a band-pass filter is analyzed, and is tuned by the parameters of its electric components. The circuit measures the capacitance of the strain sensor with accuracy equivalent to a commercially available LCR meter. We confirm that the combination of the capacitance meter and the strain sensor measures the length of the artificial muscle with a high sampling rate of 10 kHz.",1,0,0,strain sensors+pneumatic artificial muscle+robotic systems,
20,20,"Anytime optimal algorithms in stochastic multi armed bandits. We introduce an anytime algorithm for stochastic multi-armed bandit with optimal distribution free and distribution dependent bounds (for a specific family of parameters). The performances of this algorithm (as well as another one motivated by the conjectured optimal bound) are evaluated empirically. A similar analysis is provided with full information, to serve as a benchmark.",1,0,0,deterministic+markov decision processes+stochastic,
21,21,"Reinforced active learning for image segmentation. Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.",1,0,0,reinforcement learning+image segmentation+object segmentation,
22,22,"Talakat bullet hell generation through constrained map elites. We describe a search-based approach to generating new levels for bullet hell games, which are action games characterized by and requiring avoidance of a very large amount of projectiles. Levels are represented using a domain-specific description language, and search in the space defined by this language is performed by a novel variant of the Map-Elites algorithm which incorporates a feasible-infeasible approach to constraint satisfaction. Simulation-based evaluation is used to gauge the fitness of levels, using an agent based on best-first search. The performance of the agent can be tuned according to the two dimensions of strategy and dexterity, making it possible to search for level configurations that require a specific combination of both. As far as we know, this paper describes the first generator for this game genre, and includes several algorithmic innovations.",1,0,0,software agents+agent based+intelligent agents,
23,23,"A modified contrastive loss method for face recognition. Abstract Contrastive Loss is frequently used as loss function in CNN for face recognition, but it can result in the overfitting and low sampling efficiency for the positive samples. In this paper, a Modified Contrastive Loss (MCL) is proposed to overcome the shortcomings of contrastive loss. MCL and ResNet are combined with a Joint Bayesian technique to develop a ResNet-Modified Contrastive Loss-Joint Bayesian (ResNet-MCL-JB) model. First, ResNet is used as the basic network structure, and several ResNets are trained to use the MCL. Then, the ResNet with the Joint Bayesian for metric learning is integrated. The state-of-the-art performance of ResNet-MCL-JB attests to its effect. For further improvement, a Progressive Soft Filter Pruning method (PSFP) is applied in the neural network. PSFP can effectively diminish the size of the network while maintaining high accuracy. This method gradually prunes the filters on each layer by the weight of each filter. We combine the MCL and PSFP together with ResNet, and thus, we achieve considerable much improvement in both accuracy and computational cost.Keywords: Modified contrastive loss; Face recognition; Joint bayesian",1,0,0,neural networks+convolutional neural networks+face recognition,
24,24,Evaluating polarity for verbal phraseological units. Fixation in linguistic expressions is an inherent property of natural language that plays a central role in their description. Verbal phraseological units are phrases made up of two or more words characterized for presenting certain degree of fixation or idiomaticity (at least one of these words is a verb that plays the role of the predicate).,1,0,0,linguistics+natural language understanding+part of speech,
25,25,"Automated switching system for skin pixel segmentation in varied lighting. In Computer Vision, colour-based spatial techniquesoften assume a static skin colour model. However, skin colour perceived by a camera can change when lighting changes. In common real environment multiple light sources impinge on the skin. Moreover, detection techniques may vary when the image under study is taken under different lighting condition than the one that was earlier under consideration. Therefore, for robust skin pixel detection, a dynamic skin colour model that can cope with the changes must be employed. This paper shows that skin pixel detection in a digital colour image can be significantly improved by employing automated colour space switching methods. In the root of the switching technique which is employed in this study, lies the statistical mean of value of the skin pixels in the image which in turn has been derived from the Value, measures as a third component of the HSV. The study is based on experimentations on a set of images where capture time conditions varying from highly illuminated to almost dark.",1,0,0,computer vision+reference image+skin color models,
26,26,"The indian buffet hawkes process to model evolving latent influences. Temporal events in the real world often exhibit reinforcing dynamics, where earlier events trigger follow-up activity in the near future. Such dynamics have been represented using models of reinforcement and reciprocation, a canonical example being the Hawkes pro- cess (HP). However, previous HP models do not capture the rich dynamics of real-world activity—which can be driven by multiple la- tent triggering factors shared by past and future events, with the latent features themselves ex- hibiting temporal dependency structures. For instance, rather than view a new document just as a response to other documents in the recent past, it is important to account for the factor- structure underlying all previous documents. This structure itself is not fixed, with the influ- ence of earlier documents decaying with time. To this end, we propose a novel Bayesian non- parametric stochastic point process model, the Indian Buffet Hawkes Processes (IBHP), to learn multiple latent triggering factors under- lying streaming document/message data. The IBP facilitates the inclusion of multiple trig- gering factors in the HP, and the HP allows for modeling latent factor evolution in the IBP. We develop an efficient and scalable learning algo- rithm for the IBHP based on Sequential Monte Carlo and demonstrate the effectiveness of the model, both quantitatively and qualitatively, in experiments on synthetic and real data.",1,0,0,stochastic+latent factor+latent variable,
27,27,"Stylerig rigging stylegan for 3d control over portrait images. StyleGAN generates photorealistic portrait images of faces with eyes, teeth, hair and context (neck, shoulders, background), but lacks a rig-like control over semantic face parameters that are interpretable in 3D, such as face pose, expressions, and scene illumination. Three-dimensional morphable face models (3DMMs) on the other hand offer control over the semantic parameters, but lack photorealism when rendered and only model the face interior, not other parts of a portrait image (hair, mouth interior, background). We present the first method to provide a face rig-like control over a pretrained and fixed StyleGAN via a 3DMM. A new rigging network, RigNet is trained between the 3DMM's semantic parameters and StyleGAN's input. The network is trained in a self-supervised manner, without the need for manual annotations. At test time, our method generates portrait images with the photorealism of StyleGAN and provides explicit control over the 3D semantic parameters of the face.",1,0,0,face models+color images+reference image,
28,28,"Foundations for a probabilistic event calculus. We present PEC, an Event Calculus (EC) style action language for reasoning about probabilistic causal and narrative information. It has an action language style syntax similar to that of the EC variant \(\mathcal {M}\)odular-\(\mathcal {E}\). Its semantics is given in terms of possible worlds which constitute possible evolutions of the domain, and builds on that of Epistemic Functional EC (EFEC). We also describe an ASP implementation of PEC and show the sense in which this is sound and complete.",1,0,0,reasoning+semantics+reasoning about actions,
29,29,"Towards multi level provenance reconstruction of information diffusion on social media. In order to assess the trustworthiness of information on social media, a consumer needs to understand where this information comes from, and which processes were involved in its creation. The entities, agents and activities involved in the creation of a piece of information are referred to as its provenance, which was standardized by W3C PROV. However, current social media APIs cannot always capture the full lineage of every message, leaving the consumer with incomplete or missing provenance, which is crucial for judging the trust it carries. Therefore in this paper, we propose an approach to reconstruct the provenance of messages on social media on multiple levels. To obtain a fine-grained level of provenance, we use an approach from prior work to reconstruct information cascades with high certainty, and map them to PROV using the PROV-SAID extension for social media. To obtain a coarse-grained level of provenance, we adapt our similarity-based, fuzzy provenance reconstruction approach -- previously applied on news. We illustrate the power of the combination by providing the reconstructed provenance of a limited social media dataset gathered during the 2012 Olympics, for which we were able to reconstruct a significant amount of previously unidentified connections.",1,0,0,social media+micro-blog+information diffusion,
30,30,"Analysis of memory capacity for deep echo state networks. In this paper, the echo state network (ESN) memory capacity, which represents the amount of input data an ESN can store, is analyzed for a new type of deep ESNs. In particular, two deep ESN architectures are studied. First, a parallel deep ESN is proposed in which multiple reservoirs are connected in parallel allowing them to average outputs of multiple ESNs, thus decreasing the prediction error. Then, a series architecture ESN is proposed in which ESN reservoirs are placed in cascade that the output of each ESN is the input of the next ESN in the series. This series ESN architecture can capture more features between the input sequence and the output sequence thus improving the overall prediction accuracy. Fundamental analysis shows that the memory capacity of parallel ESNs is equivalent to that of a traditional shallow ESN, while the memory capacity of series ESNs is smaller than that of a traditional shallow ESN. In terms of normalized root mean square error, simulation results show that the parallel deep ESN achieves 38.5% reduction compared to the traditional shallow ESN while the series deep ESN achieves 16.8% reduction.",1,0,0,mean square error+network architecture+echo state networks,
31,31,"The robotchallenge a research inspired practical lecture. This paper presents a new university course combining theoretical lectures with a robot competition. The main intention is to offer a practical course including hands-on experiences being close to current research topics in the field of mobile robotics. For this purpose, a commercial mobile robot is equipped with state of the art sensors, allowing autonomous execution of manipulation tasks. The course consists of three thematical sections. At the beginning of each, lectures provide the theoretical background. On this basis, the participants are addressed to develop algorithms and to solve specific tasks, delivered in homework packages, self-reliantly or in a team. Finally, the developed software components need to be merged to solve a predefined scenario, e.g. autonomous part handling. At the end of a thematic section, students demonstrate their developed solutions within a challenge and explain their approaches in a presentation. Starting with teleoperation and object recognition, the RobotChallenge ends up with navigation in unknown terrain. Besides others, the participants acquire soft-skills, such as project and team management. Being carried out for the first time in winter term 2011/2012, the RobotChallenge successfully promotes profound understanding of mobile robotics that is applied during practical experiences. It turns out, that aspiring to win competions lead to a high motivation of the students w.r.t. development of appropriate solutions.",1,0,0,autonomous mobile robot+robot applications+object recognition,
32,32,"Parallelizing stochastic gradient descent for least squares regression mini batching averaging and model misspecification. This work characterizes the benefits of averaging schemes widely used in conjunction with stochastic gradient descent (SGD). In particular, this work provides a sharp analysis of: (1) mini-batching, a method of averaging many samples of a stochastic gradient to both reduce the variance of the stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD to decrease the variance in SGD's final iterate. This work presents non-asymptotic excess risk bounds for these schemes for the stochastic approximation problem of least squares regression. 
Furthermore, this work establishes a precise problem-dependent extent to which mini-batch SGD yields provable near-linear parallelization speedups over SGD with batch size one. This allows for understanding learning rate versus batch size tradeoffs for the final iterate of an SGD method. These results are then utilized in providing a highly parallelizable SGD method that obtains the minimax risk with nearly the same number of serial updates as batch gradient descent, improving significantly over existing SGD methods. A non-asymptotic analysis of communication efficient parallelization schemes such as model-averaging/parameter mixing methods is then provided. 
Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure minimax risk for the agnostic case must be a function of the noise properties. 
This paper builds on the operator view of analyzing SGD methods, introduced by Defossez and Bach (2015), followed by developing a novel analysis in bounding these operators to characterize the excess risk. These techniques are of broader interest in analyzing computational aspects of stochastic approximation.",1,0,0,stochastic+stochastic gradient descent+stochastic approximation,
33,33,"Modeling prey predator dynamics via particle swarm optimization and cellular automata. Through the years several methods have been used to model organisms movement within an ecosystem modelled with cellular automata, from simple algorithms that change cells state according to some pre-defined heuristic, to diffusion algorithms based on the one dimensional Navier - Stokes equation or lattice gases. In this work we show a novel idea since the predator dynamics evolve through Particle Swarm Optimization.",1,0,0,cellular automata+optimization+particle swarm optimization (pso),
34,34,"Distributed pareto optimization for large scale noisy subset selection. Subset selection, aiming to select the best subset from a ground set with respect to some objective function, is a fundamental problem with applications in many areas, such as combinatorial optimization, machine learning, data mining, computer vision, information retrieval, etc. Along with the development of data collection and storage, the size of the ground set grows larger. Furthermore, in many subset selection applications, the objective function evaluation is subject to noise. We thus study the large-scale noisy subset selection problem in this paper. The recently proposed DPOSS algorithm based on multiobjective evolutionary optimization is a powerful distributed solver for large-scale subset selection. Its performance, however, has been only validated in the noise-free environment. In this paper, we first prove its approximation guarantee under two common noise models, i.e., multiplicative noise and additive noise, disclosing that the presence of noise degrades the performance of DPOSS largely. Next, we propose a new distributed multiobjective evolutionary algorithm called DPONSS for large-scale noisy subset selection. We prove that the approximation guarantee of DPONSS under noise is significantly better than that of DPOSS. We also conduct experiments on the application of sparse regression, where the objective evaluation is often estimated using a sample data, bringing noise. The results on various real-world data sets, whose size can reach millions, clearly show the excellent performance of DPONSS.",1,0,0,evolutionary optimizations+multi-objective evolutionary algorithms+multiplicative noise,
35,35,"Roses a continuous query processor for large scale rss filtering and aggregation. We present RoSeS, a running system for large-scale content-based RSS feed filtering and aggregation. The implementation of RoSeS is based on standard database concepts like declarative query languages, views and multi-query optimization. Users create personalized feeds by defining and composing content-based filtering and aggregation queries on collections of RSS feeds. These queries are translated into continuous multi-query execution plans which are optimized using a new cost-based multi-query optimization strategy.",1,0,0,query processing+continuous queries+optimization,
36,36,"Kitten a tool for normalizing html and extracting its textual content. The web is composed of a gigantic amount of documents that can be very useful for information extraction systems. Most of them are written in HTML and have to be rendered by an HTML engine in order to display the data they contain on a screen. HTML files thus mix both informational and rendering content. Our goal is to design a tool for informational content extraction. A linear extraction with only a basic filtering of rendering content would not be enough as objects such as lists and tables are linearly coded but need to be read in a non-linear way to be well interpreted. Besides these HTML pages are often incorrectly coded from an HTML point of view and use a segmentation of blocks based on blank space that cannot be transposed in a text file without confusing syntactic parsers. For this purpose, we propose the Kitten tool that first normalizes HTML files into unicode XHTML files, then extracts the informational content into a text file with a special processing for sentences, lists and tables.",1,0,0,information contents+information extraction+information extraction systems,
37,37,"Phoneme level text to audio synchronization on speech signals with background music. We address the task of synchronizing a given phoneme transcription with the corresponding speech signal, when the latter is linearly mixed with background music. To that end, we propose a new method based on Non-negative Matrix Factorization in the time-frequency domain, which models the speech as a source-filter factorization that includes a synchronization parameter matrix. Phoneme models, which consist of collections of basic spectral envelopes, are learned from a training set of isolated speech. The model is subjected to an iterative Maximum Likelihood optimization that concurrently estimates pitch, synchronization parameters and the contribution of the music part. Results show the feasibility of the system for application in text-informed audio processing and automatic subtitle synchronization.",1,0,0,matrix factorizations+non-negative matrix factorization+training sample,
38,38,"Near optimal geometric feature selection for visual speech recognition. To improve the accuracy of visual speech recognition systems, selection of visual features is of fundamental importance. Prominent features, which are of maximum relevance for speech classification, need to be selected from a large set of extracted visual attributes. Existing methods apply feature reduction and selection techniques on image pixels constituting region-of-interest (ROI) to reduce data dimensionality. We propose application of feature selection methods on geometrical features to select the most dominant physical features. Two techniques, Minimum Redundancy Maximum Relevance (mRMR) and Correlation-based Feature Selection (CFS), have been applied on the extracted visual features. Experimental results show that recognition accuracy is not compromised when a few selected features from the complete visual feature set are used for classification, thereby reducing processing time and storage overheads considerably. Results are compared with performance of principal components obtained by application of Principal Component Analysis (PCA) on our dataset. Our set of selected features outperforms the PCA transformed data. Results show that the center and corner segments of the mouth are major contributors to visual speech recognition. Teeth pixels are shown to be a prominent visual cue. It is also seen that lip width contributes more towards visual speech recognition accuracy as compared to lip height.",1,0,0,visual feature+feature space+speech recognition systems,
39,39,"Gait planning of omnidirectional walk on inclined ground for biped robots. When a biped robot moves about in a physical environment, it may encounter inclined ground. Biped walking on inclined ground still remains challenging for biped robots. Previous studies have discussed biped walking on inclined ground along specific directions. However, omnidirectional walk on inclined ground has rarely been investigated. In this paper, we propose a gait pattern generation method for omnidirectional biped walking on inclined ground. First, a model that describes the motion of biped walking on inclined ground uniformly with two angle parameters is proposed. A mathematical relationship between motions in the sagittal and coronal planes of the biped robot are presented. Then, based on nonorthogonal motion decoupling, a method that generates gait patterns for omnidirectional walking with a double support phase for biped robots is proposed. The trajectories of each foot are designated by the walking speed, step length, and walking direction. The motion trajectory of the center of mass (CoM) of the robot is planned using a linear inverted pendulum model in the sagittal and coronal planes. The motion of CoM in the sagittal and coronal planes is constrained in parallel to the gradient vector of the inclined ground and the horizontal plane, respectively. Finally, the effectiveness of the proposed gait planning method for biped walking on is validated by simulations and experiments with an actual biped robot.",1,0,0,biped walking+quadruped robots+gait planning,
40,40,"Image generation and editing with variational info generative adversarialnetworks. Recently there has been an enormous interest in generative models for images in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular models. While VAEs tend to produce excellent reconstructions but blurry samples, GANs generate sharp but slightly distorted images. In this paper we propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold: (i) To generated new images conditioned on visual descriptions, and (ii) modify the image, by fixing the latent representation of image and varying the visual description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA and a modified version of MNIST datasets and demonstrate the ability of our model to generate new images as well as to modify a given image by changing attributes.",1,0,0,variational methods+gan+generative adversarial networks,
41,41,"Toward a reactive agent based parking assistance system. Nowadays, automotive equipments tend to offer more and more drinving assistance. Among these, car park assist systems are now widespread on both commercial and research levels. This paper proposes a car park assistance based on the use of the multi-agent paradigm. This approach based on the transformation of vehicle perception into a virtual environment for agents and on the evaluation of agency properties. This evaluation is then turned into a decision for vehicle guidance. The proposal is evaluated in simulation by comparing it with industrial solutions.",1,0,0,agent based+intelligent agents+multi-agent,
42,42,"Comparison of multistart global optimization algorithms on the bbob noiseless testbed. Multi Level Single Linkage is a multistart, stochastic global optimization method which relies on random sampling and local search. In this paper, we benchmarked three variants of the MLSL algorithm by using two gradient based and a derivative-free local search method on the noiseless function testbed. The three methods were also compared with a commercial multistart solver, called OQNLP (OptQuest/NLP).   Our experiment showed that, the results may be influenced essentially by the applied local search procedure. Depending of the type of the problem the gradient based local search methods are faster in the initial stage of the optimization, while the derivative-free method show a superior performance in the final phase for moderate dimensions. Considering the percentage of the solved problems, OQNLP is similar or even better (for multi-modal and weakly structured functions) in 5-D than the MLSL method equipped with the gradient type local search methods, while on 20-D the latter algorithms are usually more faster.",1,0,0,natural language processing+stochastic+binary symmetric channel,
43,43,"On model based adhesion control of a vortex climbing robot. In this article, the adhesion modeling and control case of a Vortex Climbing Robot (VCR) is investigated against a surface of variable orientations. The critical adhesion force exerted from the implemented Vortex Actuator (VA) and the VCR’s achievable payload are analyzed under 3-DOF rotations of the test surface, while extracted from both geometrical analysis and dynamically-simulated numerical results. A model-based control scheme is later proposed, with the goal of achieving adhesion while the VCR remains immobilized, limiting the power consumption and compensating for disturbances (e.g. moving cables) leading to Center-of-Mass (CoM) changes. Finally, the model-based control scheme is experimentally evaluated, with the VCR prototype on a rotating and moving flat surface. The presented results support the use of the proposed methodology in climbing robots targeting inspection and maintenance of stationary surfaces (flat, curved etc.), as well as future robotic solutions operating on moving structures (e.g. ships, cranes, folding bridges).",1,0,0,robots+videocassette recorders+numerical results,
44,44,"Distributed time varying formation control for linear swarm systems with switching topologies using an adaptive output feedback approach. Fully distributed time-varying formation (TVF) control problems are addressed in this paper using an adaptive output-feedback approach for general linear swarm systems with fixed and switching topologies. In contrast to the earlier results on formation control problems, the general linear swarm system can achieve the predefined TVF using only local output information and independent of the Laplacian matrix associated with the communication topologies. The implementation cost of calculating the global information and requiring full state information is avoided when achieving the desired TVF. First, a node-based TVF control protocol is constructed via dynamic output feedback for the case with fixed communication topology, where adaptive-based coupling weights are introduced to eliminate the dependence on the global information about the topology. Then an algorithm is presented to determine the gain matrices in the node-based adaptive TVF control protocol, and a feasible formation constraint is provided. The stability of the algorithm is proved based on the Lyapunov theory. Furthermore, under the case of switching communication topologies, an edge-based TVF control protocol is constructed with dynamic output feedback and an adaptive law for adjusting the coupling weights among agents. A sufficient condition is derived using the common Lyapunov theory for general linear swarm systems to achieve the predefined TVF satisfying the feasible formation constraint. Two numerical examples are presented to illustrate the theoretical results.",1,0,0,lyapunov stability theory+lyapunov theories+adaptive laws,
45,45,"Cross lingual training for automatic question generation. Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences.",1,0,0,question answering+natural language generation+language model,
46,46,"An exponential type anti noise varying gain network for solving disturbed time varying inversion systems. To solve the disturbed time-varying inversion problem, an exponential-type anti-noise varying-gain network (EAVGN) is proposed and analyzed. To do so, a vector-based error function is first defined. By using the varying-gain neural dynamic design method, an EAVGN model is then formulated. Furthermore, the differentiation error and the model-implementation error are considered into the model, and the perturbed EAVGN model is obtained. For better illustrations, comparisons between the EAVGN and the conventional fixed-parameter recurrent neural network (FP-RNN) are conducted to illustrate the advantages of the proposed EAVGN. Mathematical proof demonstrates that the proposed EAVGN has much better anti-noise properties than FP-RNN. On one hand, the residual error of EAVGN can be reduced to zero in any case, but that of FP-RNN is large and cannot be convergent, in particular when the bound of Frobenius norm of the exact solution is large or the noise is large. On the other hand, the bound of the residual error of EAVGN is always smaller than that of FP-RNN. Simulation results verify that when different types of noises exist, the proposed EAVGN owns better anti-noise property compared with the state-of-the-art methods. In addition, a practical application is presented to illustrate the implementation process and the practical benefits of the EAVGN.",1,0,0,exact solution+neural networks+recurrent neural networks,
47,47,"Feature selection via binary simultaneous perturbation stochastic approximation. We propose a new wrapper feature selection (FS) method.This method is based on binary simultaneous perturbation stochastic approximation (BSPSA).BSPSA is a pseudo-gradient descent stochastic algorithm that starts with a random solution vector.BSPSA successively updates the solution vector by simultaneous perturbations to individual components.BSPSA is computationally feasible for big datasets with tens of thousands of features. Feature selection (FS) has become an indispensable task in dealing with today's highly complex pattern recognition problems with massive number of features. In this study, we propose a new wrapper approach for FS based on binary simultaneous perturbation stochastic approximation (BSPSA). This pseudo-gradient descent stochastic algorithm starts with an initial feature vector and moves toward the optimal feature vector via successive iterations. In each iteration, the current feature vector's individual components are perturbed simultaneously by random offsets from a qualified probability distribution. We present computational experiments on datasets with numbers of features ranging from a few dozens to thousands using three widely-used classifiers as wrappers: nearest neighbor, decision tree, and linear support vector machine. We compare our methodology against the full set of features as well as a binary genetic algorithm and sequential FS methods using cross-validated classification error rate and AUC as the performance criteria. Our results indicate that features selected by BSPSA compare favorably to alternative methods in general and BSPSA can yield superior feature sets for datasets with tens of thousands of features by examining an extremely small fraction of the solution space. We are not aware of any other wrapper FS methods that are computationally feasible with good convergence properties for such large datasets.",1,0,0,stochastic+wrapper approach+stochastic approximation,
48,48,"Location aware friend recommendation in event based social networks a bayesian latent factor approach. In this paper we study the friend recommendation problem in event-based social networks (EBSNs). Effective friend recommendation is of benefit to EBSNs, since it can promote user interaction and accelerate information diffusion for promoted events. Different from usual friend recommendations, the aim of making friends in EBSNs is to better participate offline events and enhance user experience. Meanwhile friend recommendation in EBSNs encounters three types of data, i.e. geographical information, implicate user rating, and user behavior. These differences imply that existing friend recommendation approaches are not adequate any more for EBSNs. Under this background, in this paper we propose a Bayesian latent factor model, which can jointly formulate above three types of data, for friend recommendation with better event promotion and user experience. Results on real-world datasets show the efficacy of our approach.",1,0,0,bayesian methods+latent factor+latent variable,
49,49,"Optimal local community detection in social networks based on density drop of subgraphs. The determination of community structures within social networks is a significant problem in the area of data mining. A proper community is usually defined as a subgraph with a higher internal density and a lower crossing density with others subgraphs. Hierarchical clustering algorithms produce a set of nested clusters, sometimes called dense subgraphs, organized as a hierarchical system and the output is always referred as a dendrogram. However, determining which of clusters in the dendrogram will be selected to form communities in the final output is a difficult problem. Most implementations of data mining algorithms require expert guidance in the implementation of the algorithm in order to establish the appropriate selection of such communities, and ultimately the output may not be optimized as with fixed height tree-cutting algorithms. In this paper, a novel algorithm for community selection is proposed. The intuition of our approach is based on drops of densities between each pair of parent and child nodes on the dendrogram - the higher the drop in density, the higher probability the child should form an independent community. Based on the Max-Flow Min-Cut theorem, we propose a novel algorithm which can output an optimal set of local communities automatically. In addition, a faster algorithm running in linear time is also presented for the case that the dendrogram is a tree. Finally, we validate this approach through a variety of data sets ranging from synthetic graphs to real world benchmark data sets.",1,0,0,clustering methods+hierarchical clustering algorithms+community detection,
50,50,"View specific subspace learning and re ranking for semi supervised person re identification. Abstract Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other’s ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.",1,0,0,covariance matrix+subspace learning+locality preserving projections,
51,51,Continuous crf with multi scale quantization feature functions application to structure extraction in old newspaper. We introduce quantization feature functions to represent continuous or large range discrete data into the symbolic CRF data representation. We show that doing this convertion in a simple way allows the CRF to automaticaly select discriminative features to achieve best performance. This system is evaluated on a segmentation task of degraded newspapers archives. The results obtained show the ability of the CRF model to deal with numerical features similarly as for symbolic representation thanks to the use of quantization feature functions. The segmentation task is achieved by the definition of a horizontal CRF model dedicated to pixel labelling.,1,0,0,feature function+conditional random field+quantizers,
52,52,"Scalable bayesian non negative tensor factorization for massive count data. We present a Bayesian non-negative tensor factorization model for count-valued tensor data, and develop scalable inference algorithms both batch and online for dealing with massive tensors. Our generative model can handle overdispersed counts as well as infer the rank of the decomposition. Moreover, leveraging a reparameterization of the Poisson distribution as a multinomial facilitates conjugacy in the model and enables simple and efficient Gibbs sampling and variational Bayes VB inference updates, with a computational cost that only depends on the number of nonzeros in the tensor. The model also provides a nice interpretability for the factors; in our model, each factor corresponds to a ""topic"". We develop a set of online inference algorithms that allow further scaling up the model to massive tensors, for which batch inference methods may be infeasible. We apply our framework on diverse real-world applications, such as multiway topic modeling on a scientific publications database, analyzing a political science data set, and analyzing a massive household transactions data set.",1,0,0,matrix factorizations+gibbs sampling+variational approximation,
53,53,"Undersampled k means approach for handling imbalanced distributed data. \(K\)-means is a partitional clustering technique that is well known and widely used for its low computational cost. However, the performance of \(K\)-means algorithm tends to be affected by skewed data distributions, i.e., imbalanced data. They often produce clusters of relatively uniform sizes, even if input data have varied cluster size, which is called the “uniform effect”. In this paper, we analyze the causes of this effect and illustrate that it probably occurs more in the \(K\)-means clustering process. As the minority class decreases in size, the “uniform effect” becomes evident. To prevent the effect of the “uniform effect”, we revisit the well-known \(K\)-means algorithm and provide a general method to properly cluster imbalance distributed data. The proposed algorithm consists of a novel undersampling technique implemented by intelligently removing noisy and weak instances from majority class. We conduct experiments using twelve UCI datasets from various application domains using five algorithms for comparison on eight evaluation metrics. Experimental results show the effectiveness of the proposed clustering algorithm in clustering balanced and imbalanced data.",1,0,0,adaptive algorithms+clustering methods+data clustering,
54,54,"Research on detection and evaluation technology of cybersecurity in intelligent and connected vehicle. With the continuous development of vehicle intellectualization and network interconnection, the problem of cybersecurity in intelligent and connected vehicle (ICV) has become increasingly prominent. However, at present, the lack of software testing tools in vehicle industry and the lack of cybersecurity detection and evaluation are serious problems, which need to be solved urgently. This paper focuses on this issue, we carry out the research of ICV detection and evaluation platform, and explore the technology of ICV cybersecurity detection. In addition, we build a database of vehicle cybersecurity vulnerabilities to provide data support, and use source code analysis, firmware reverse, penetration test, simulation and other detection technologies to carry out cybersecurity evaluation for vehicle terminals, data communications, cloud platform and APP. Full detection and risk assessment is designed to build an ICV security detection and assessment platform.",1,0,0,communication+vehicles+database systems,
55,55,"Learning longer term dependencies in rnns with auxiliary losses. Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.",1,0,0,backpropagation algorithm+image classification+regularization,
56,56,"Icdar 2013 crohme third international competition on recognition of online handwritten mathematical expressions. We report on the third international Competition on Handwritten Mathematical Expression Recognition (CROHME), in which eight teams from academia and industry took part. For the third CROHME, the training dataset was expanded to over 8000 expressions, and new tools were developed for evaluating performance at the level of strokes as well as expressions and symbols. As an informal measure of progress, the performance of the participating systems on the CROHME 2012 data set is also reported. Data and tools used for the competition will be made publicly available.",1,0,0,hand-written characters+character recognition+off-line handwritten,
57,57,"Finding efficient circuits for ensemble computation. Given a Boolean function as input, a fundamental problem is to find a Boolean circuit with the least number of elementary gates (AND, OR, NOT) that computes the function. The problem generalises naturally to the setting of multiple Boolean functions: find the smallest Boolean circuit that computes all the functions simultaneously. We study an NP-complete variant of this problem titled Ensemble Computation and, especially, its relationship to the Boolean satisfiability (SAT) problem from both the theoretical and practical perspectives, under the two monotone circuit classes: OR-circuits and SUM-circuits. Our main result relates the existence of nontrivial algorithms for CNF-SAT with the problem of rewriting in subquadratic time a given OR-circuit to a SUM-circuit. Furthermore, by developing a SAT encoding for the ensemble computation problem and by employing state-of-the-art SAT solvers, we search for concrete instances that would witness a substantial separation between the size of optimal OR-circuits and optimal SUM-circuits. Our encoding allows for exhaustively checking all small witness candidates. Searching over larger witness candidates presents an interesting challenge for current SAT solver technology.",1,0,0,sat solvers+boolean satisfiability+boolean functions,
58,58,"Language aware weak supervision for salient object detection. Abstract Natural Language Processing has achieved remarkable performance in multitudinous computer tasks, but the potential capability of textual information has not been completely explored in visual saliency detection. In this paper, we learn to detect salient object from natural language by addressing the two essential issues: finding a semantic content matching the corresponding linguistic concept and recovering fine details without any pixel-level annotations. We first propose the Feature Matching Network (FMN) to explore the internal relation between the linguistic concept and visual image in the semantic space. The FMN simultaneously establishes the textual-visual pairwise affinities and generates a language-aware coarse saliency map. to refine the coarse map, the Recurrent Fine-tune Network (RFN) is proposed to enhance its predicted performance progressively by self-supervision. Our approach only leverages the caption to provide important cues of salient object, but generates a fine-detailed foreground map at a detecting speed of 72 FPS without any post-processing. Extensive experiments demonstrate that our method takes full advantage of textual information of natural language in saliency detection, and performs favorably against state-of-the-art approaches on the most existing datasets.",1,0,0,natural language understanding+object detection+matching networks,
59,59,"A hybrid algorithm based on moea d and local search for multiobjective optimization. A hybrid algorithm is proposed for multiobjective optimization in this paper. The proposed algorithm consists of multiobjective evolutionary algorithm based on decomposition (MOEA/D) and recurrent neural network, where MOEA/D is for global search while recurrent neural network is for local search. The performance of the proposed algorithm is compared with other three multi-objective algorithms in terms of hypervolume and inverted generational distance. The performance investigation shows that the proposed algorithm generally outperforms the compared algorithms.",1,0,0,evolutionary algorithms+multi-objective evolutionary algorithms+hypervolume,
60,60,"Auxiliary deep generative models. Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.",1,0,0,neural networks+variational methods+variational approximation,
61,61,"Ucsd adobe at mediqa 2021 transfer learning and answer sentence selection for medical summarization. In this paper, we describe our approach to question summarization and multi-answer summarization in the context of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We propose two kinds of transfer learning for the abstractive summarization of medical questions. First, we train on HealthCareMagic, a large question summarization dataset collected from an online healthcare service platform. Second, we leverage the ability of the BART encoder-decoder architecture to model both generation and classification tasks to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores.",1,0,0,automatic summarization+text summarization+sentence extraction,
62,62,"Pu learning for matrix completion. In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M, and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only ""likes"" or ""friendships"" are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a ""shifted matrix completion"" method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a ""biased matrix completion"" method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds -- if M e Rn×n the error is bounded as O( 1/(1-ρ)n), where 1 -- ρ denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.",1,0,0,observation model+semi-supervised clustering+link prediction,
63,63,"Eve a novel open source web based agent platform. The existing approaches to build multi-agent systems fail at addressing the challenges posed by the current technology, where ubiquitous interconnected electronic devices are no more passive machines operated by humans but rather active computational components cooperating with humans. In this paper, we present a novel open-source web-based agent platform called 'Eve' that features some specific characteristics (e.g., platform and language independence, openness) that make it particularly suitable to be deployed in real-life applications.",1,0,0,multiagent system+agent systems+multi-agent,
64,64,"Burst time prediction in cascades. Studying the bursty nature of cascades in social media is practically important in many applications such as product sales prediction, disaster relief, and stock market prediction. Although the cascade volume prediction has been extensively studied, how to predict when a burst will come remains an open problem. It is challenging to predict the time of the burst due to the ""quick rise and fall"" pattern and the diverse time spans of the cascades. To this end, this paper proposes a classification based approach for burst time prediction by utilizing and modeling rich knowledge in information diffusion. Particularly, we first propose a time window based approach to predict in which time window the burst will appear. This paves the way to transform the time prediction task to a classification problem. To address the challenge that the original time series data of the cascade popularity only are not sufficient for predicting cascades with diverse magnitudes and time spans, we explore rich information diffusion related knowledge and model them in a scale-independent manner. Extensive experiments on a Sina Weibo reposting dataset demonstrate the superior performance of the proposed approach in accurately predicting the burst time of posts.",1,0,0,information diffusion+stock market prediction+stock market,
65,65,"Synthetic data generation for grammatical error correction with tagged corruption models. Synthetic data generation is widely known to boost the accuracy of neural grammatical error correction (GEC) systems, but existing methods often lack diversity or are too simplistic to generate the broad range of grammatical errors made by human writers. In this work, we use error type tags from automatic annotation tools such as ERRANT to guide synthetic data generation. We compare several models that can produce an ungrammatical sentence given a clean sentence and an error type tag. We use these models to build a new, large synthetic pre-training data set with error tag frequency distributions matching a given development set. Our synthetic data set yields large and consistent gains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We also show that our approach is particularly effective in adapting a GEC system, trained on mixed native and non-native English, to a native English test set, even surpassing real training data consisting of high-quality sentence pairs.",1,0,0,test samples+syntactic structure+real data sets,
66,66,"Wind forecasting and wind power generation looking for the best model based on artificial intelligence. Wind forecasting is extremely important to assist in planning and programming studies for the operation of wind power generation. Several studies have shown that the Brazilian wind potential can contribute significantly to the electricity supply, especially in the Northeast, where winds present an important feature of being complementary in relation to the flows of the San Francisco River. However, using wind power to generate electricity has some drawbacks, such as uncertainties in generation and some difficulty in planning and operation of the power system. This work proposes and develops models to forecast hourly average wind speeds and wind power generation based on Artificial Neural Networks, Fuzzy Logic and Wavelets. The models were adjusted for forecasting with variable steps up to twenty-four hours ahead. The gain of some of the developed models in relation to the reference models was of approximately 80% for forecasts in a period of one hour ahead. The results showed that a wavelet analysis combined with artificial intelligence tools provides more reliable forecasts than those obtained with the reference models, especially for forecasts in a period of 1 to 6 hours ahead.",1,0,0,wavelet+forecasting models+neural networks,
67,67,"Shaping energetically efficient brachiation motion for a 24 dof gorilla robot. We consider a 24-degrees-of-freedom monkey robot that is supposed to perform brachiation locomotion, i.e. swinging from one row of a horizontal ladder to the next one using the arms. The robot hand is constructed as a planar hook so that the contact point about which the robot swings is a passive hinge. We identify the 10 most relevant degrees of freedom for this underactuated mechanical system and formulate a tractable search: (a) introduce a family of coordination patterns to be enforced on the dynamics with respect to a path coordinate; (b) formulate geometric equality constraints that are necessary for periodic locomotion; (c) generate trajectories from integrable reduced dynamics associated with the passive hinge; (d) evaluate the energetic cost of transport. Moreover, we observe that a linear approximation of the reduced dynamics can be used for trajectory generation which allows us to incorporate the gradient of the cost function into the search algorithm.",1,0,0,robot hand+mobile robots+robot arms,
68,68,"Tighter theory for local sgd on identical and heterogeneous data. We provide a new analysis of local SGD, removing unnecessary assumptions and elaborating on the difference between two data regimes: identical and heterogeneous. In both cases, we improve the existing theory and provide values of the optimal stepsize and optimal number of local iterations. Our bounds are based on a new notion of variance that is specific to local SGD methods with different data. The tightness of our results is guaranteed by recovering known statements when we plug $H=1$, where $H$ is the number of local steps. The empirical evidence further validates the severe impact of data heterogeneity on the performance of local SGD.",1,0,0,integrated data+step size+stochastic gradient descent,
69,69,"Replay spoofing detection system for automatic speaker verification using multi task learning of noise classes. In this paper, we propose a replay attack spoofing detection system for automatic speaker verification using multi-task learning of noise classes. We define the noise that is caused by the replay attack as replay noise. We explore the effectiveness of training a deep neural network simultaneously for replay attack spoofing detection and replay noise classification. The multi-task learning includes classifying the noise of playback devices, recording environments, and recording devices as well as the spoofing detection. Each of the three types of the noise classes also includes a genuine class. The experiment results on the version 1.0 of ASVspoof2017 datasets demonstrate that the performance of our proposed system is improved by 30% relatively on the evaluation set.",1,0,0,detection algorithm+neural networks+speaker verification,
70,70,"Heterogeneous domain adaptation through progressive alignment. In real-world transfer learning tasks, especially in cross-modal applications, the source domain and the target domain often have different features and distributions, which are well known as the heterogeneous domain adaptation (HDA) problem. Yet, existing HDA methods focus on either alleviating the feature discrepancy or mitigating the distribution divergence due to the challenges of HDA. In fact, optimizing one of them can reinforce the other. In this paper, we propose a novel HDA method that can optimize both feature discrepancy and distribution divergence in a unified objective function. Specifically, we present progressive alignment , which first learns a new transferable feature space by dictionary-sharing coding, and then aligns the distribution gaps on the new space. Different from previous HDA methods that are limited to specific scenarios, our approach can handle diverse features with arbitrary dimensions. Extensive experiments on various transfer learning tasks, such as image classification, text categorization, and text-to-image recognition, verify the superiority of our method against several state-of-the-art approaches.",1,0,0,supervised classification+text classification+image classification,
71,71,"Controlled query evaluation for datalog and owl 2 profile ontologies. We study confidentiality enforcement in ontologies under the Controlled Query Evaluation framework, where a policy specifies the sensitive information and a censor ensures that query answers that may compromise the policy are not returned. We focus on censors that ensure confidentiality while maximising information access, and consider both Datalog and the OWL 2 profiles as ontology languages.",1,0,0,query processing+ontology language+datalog,
72,72,"Mixing habits and planning for multi step target reaching using arbitrated predictive actor critic. Internal models are important when agents make decisions based on predictions of future states and their utilities. However, using internal models for planning can be time consuming. Therefore, it can be useful to use a habitual system for repetitive tasks that can be executed faster and with reduced algorithmic resources. Current evidence suggests that the brain uses both control systems, planning and habitual systems for behavioural control, which then requires an arbitration between these two systems. In our previous work [1], we proposed an Arbitrated Predictive Actor-Critic (APAC), which is a neural architecture demonstrating cooperative mechanisms of planning and habitual control systems for one step mapping. The present study tests the ability of such a model to control a simulated twojoints robotic arm during multiple reaching tasks with movement limitations that require multiple steps to solve the task. Our results show that APAC can learn the multi-step learning under various conditions. Interestingly, the APAC tends to shift from planning to habits by taking actions predicted by a habitual controller over the training time.",1,0,0,control systems+robotic arms+actor critic,
73,73,"Multi plane program induction with 3d box priors. We consider two important aspects in understanding and editing images: modeling regular, program-like texture or patterns in 2D planes, and 3D posing of these planes in the scene. Unlike prior work on image-based program synthesis, which assumes the image contains a single visible 2D plane, we present Box Program Induction (BPI), which infers a program-like scene representation that simultaneously models repeated structure on multiple 2D planes, the 3D position and orientation of the planes, and camera parameters, all from a single image. Our model assumes a box prior, i.e., that the image captures either an inner view or an outer view of a box in 3D. It uses neural networks to infer visual cues such as vanishing points, wireframe lines to guide a search-based algorithm to find the program that best explains the image. Such a holistic, structured scene representation enables 3D-aware interactive image editing operations such as inpainting missing pixels, changing camera parameters, and extrapolate the image contents.",1,0,0,single images+document images+neural networks,
74,74,"Cloth interactive transformer for virtual try on. 2D image-based virtual try-on has attracted increased attention from the multimedia and computer vision communities. However, most of the existing image-based virtual try-on methods directly put both person and the in-shop clothing representations together, without considering the mutual correlation between them. What is more, the long-range information, which is crucial for generating globally consistent results, is also hard to be established via the regular convolution operation. To alleviate these two problems, in this paper we propose a novel two-stage Cloth Interactive Transformer (CIT) for virtual try-on. In the first stage, we design a CIT matching block, aiming to perform a learnable thin-plate spline transformation that can capture more reasonable long-range relation. As a result, the warped in-shop clothing looks more natural. In the second stage, we propose a novel CIT reasoning block for establishing the global mutual interactive dependence. Based on this mutual dependence, the significant region within the input data can be highlighted, and consequently, the try-on results can become more realistic. Extensive experiments on a public fashion dataset demonstrate that our CIT can achieve the new state-of-the-art virtual try-on performance both qualitatively and quantitatively. The source code and trained models are available at this https URL.",1,0,0,matching algorithm+matching methods+computer vision,
75,75,"Unsupervised sar image segmentation using high order conditional random fields model based on product of experts. We propose a HOCRF-POE model for unsupervised segmentation of SAR images.HOCRF-POE model captures the high-order label dependencies.HOCRF-POE model provides satisfying label consistency cost.HOCRF-POE model integrates SAR data under unsupervised Bayesian framework.HOCRF-POE model is robust against speckle and preserves image structure well. Conditional random fields (CRF) model is suitable for image segmentation because this model directly defines the posterior distribution as a Gibbs field and allows one to capture the dependencies of the observed data. However, this model has a limited ability to capture the high-order label dependencies because of only the pairwise potential being constructed. Moreover, for synthetic aperture radar (SAR) image segmentation, SAR scattering statistics that are essential to SAR image processing are not considered in CRF model. Then for unsupervised SAR image multiclass segmentation, we propose a high-order CRF model based on product-of-experts (POE) in this paper, named as HOCRF-POE model. HOCRF-POE model decomposes the high-order label dependencies into the low-order ones and constructs the non-parametric high-order potential based on POE, thus effectively capturing high-order label dependencies. In addition, to capture SAR data information in a more completed manner in the unsupervised SAR image segmentation, HOCRF-POE model integrates the textural features and SAR scattering statistics under unsupervised Bayesian framework. The effectiveness of HOCRF-POE model is demonstrated by the application to the unsupervised segmentation of the simulated images and the real SAR images.",1,0,0,object segmentation+posterior distributions+sar data+sar image segmentation,
76,76,"Speaker clustering based on utterance oriented dirichlet process mixture model. This paper provides the analytical solution and algorithm of UO-DPMM based on a non-parametric Bayesian manner, and thus realizes fully Bayesian speaker clustering. We carried out preliminary speaker clustering experiments by using a TIMIT database to compare the proposed method with the conventional Bayesian Information Criterion (BIC) based method, which is an approximate Bayesian approach. The results showed that the proposed method outperformed the conventional one in terms of both computational cost and robustness to changes in tuning parameters.",1,0,0,maximum likelihood estimator+non-parametric bayesian+hierarchical dirichlet process,
77,77,"Dere a task and domain independent slot filling framework for declarative relation extraction. Most machine learning systems for natural language processing are tailored to specific tasks. As a result, comparability of models across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel framework for declarative specification and compilation of template-based information extraction. It uses a generic specification language for the task and for data annotations in terms of spans and frames. This formalism enables the representation of a large variety of natural language processing challenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software.",1,0,0,natural language processing+information extraction+relation extraction,
78,78,"Joint laplacian feature weights learning. Some filter methods stemming from statistics or geometry theory select features individually. Hence they neglect the combination of features and lead to suboptimal subset of features. To address this problem, a joint feature weights learning framework, which automatically determines the optimal size of the feature subset and selects the best features corresponding to a given adjacency graph, is proposed in this paper. In particular, our framework imposes nonnegative and l 2 2 -norm constraints on feature weights and iteratively learns feature weights jointly and simultaneously. A new minimization algorithm with proved convergence is also developed to optimize the non-convex objective function. Utilizing this framework as a tool, we propose a new unsupervised feature selection algorithm called Joint Laplacian Feature Weights Learning. Experimental results on five real-world datasets demonstrate the effectiveness of our algorithm. HighlightsOur proposed framework automatically determines the optimal size of the feature subset.Our proposed framework imposes nonnegative and l 2 2 -norm constraints on feature weights.Our proposed framework iteratively learns feature weights jointly and simultaneously.Our proposed JLFWL selects the best features corresponding to a given Laplacian graph.",1,0,0,genetic selection+local binary patterns+wrapper approach,
79,79,"An effective category classification method based on a language model for question category recommendation on a cqa service. Classiying user's question into several topics helps respondents answering the question in a cQA service. The word weighting method must estimate the appropriate weight of a word to improve the category (or topic) classification. In this paper, we propose a novel effective word weighting method based on a language model for automatic category classification in the cQA service. We first calculate the occurrence probability of a word in each category by using a language model and then the final weight of each word is estimated by ratio of the occurrence probability of the word on a category to the occurrence probability of the word on the other categories. As a result, the proposed method significantly improves the performance of the category classification.",1,0,0,classification methods+part of speech+language model,
80,80,"General derivation and analysis for input output relations in interval type 2 fuzzy logic systems. In this paper, analytical closed-form expressions are derived for the input–output relation related to an interval type-2 fuzzy logic system. It has been assumed that the related fuzzy system possesses diamond-shaped type-2 fuzzy sets for each input and singletons for output. Moreover, the Nie–Tan inference engine that provides a closed-form is preferred. The footprint of uncertainty in diamond-shaped type-2 membership functions generates four times as many regions in analytical closed-form expression as generated by standard triangular type-1 membership functions. The derived mathematical relationships provide a chance to examine the internal structure of an interval type-2 fuzzy system. These extra regions may enhance the performance of an interval type-2 fuzzy logic system over the type-1 counterpart. An important advantage of the proposed technique is that the analytical input–output relations are applicable for any number of input fuzzy sets. Analytical structures of two special cases of interval type-2 fuzzy logic systems which use different number of membership functions for each input are derived in detail.",1,0,0,fuzzy logic system+type-2 fuzzy logic+type-2 fuzzy systems,
81,81,"Low bit quantization and quantization aware training for small footprint keyword spotting. In this paper, we investigate novel quantization approaches to reduce memory and computational footprint of deep neural network (DNN) based keyword spotters (KWS). We propose a new method for KWS offline and online quantization, which we call dynamic quantization, where we quantize DNN weight matrices column-wise, using each column's exact individual min-max range, and the DNN layers' inputs and outputs are quantized for every input audio frame individually, using the exact min-max range of each input and output vector. We further apply a new quantization-aware training approach that allows us to incorporate quantization errors into KWS model during training. Together, these approaches allow us to significantly improve the performance of KWS in 4-bit and 8-bit quantized precision, achieving the end-to-end accuracy close to that of full precision models while reducing the models' on-device memory footprint by up to 80%.",1,0,0,quantization schemes+neural networks+quantizers,
82,82,"Locality preserving matching. Seeking reliable correspondences between two feature sets is a fundamental and important task in computer vision. This paper attempts to remove mismatches from given putative image feature correspondences. To achieve the goal, an efficient approach, termed as locality preserving matching (LPM), is designed, the principle of which is to maintain the local neighborhood structures of those potential true matches. We formulate the problem into a mathematical model, and derive a closed-form solution with linearithmic time and linear space complexities. Our method can accomplish the mismatch removal from thousands of putative correspondences in only a few milliseconds. To demonstrate the generality of our strategy for handling image matching problems, extensive experiments on various real image pairs for general feature matching, as well as for point set registration, visual homing and near-duplicate image retrieval are conducted. Compared with other state-of-the-art alternatives, our LPM achieves better or favorably competitive performance in accuracy while intensively cutting time cost by more than two orders of magnitude.",1,0,0,image features+image matching+image retrieval,
83,83,"Analysis of objectives relationships in multiobjective problems using trade off region maps. Understanding the relationships between objectives in many-objective optimisation problems is desirable in order to develop more effective algorithms. We propose a technique for the analysis and visualisation of complex relationships between many (three or more) objectives. This technique looks at conflicting, harmonious and independent objectives relationships from different perspectives. To do that, it uses correlation, trade-off regions maps and scatter-plots in a four step approach. We apply the proposed technique to a set of instances of the well-known multiobjective multidimensional knapsack problem. The experimental results show that with the proposed technique we can identify local and complex relationships between objectives, trade-offs not derived from pairwise relationships, gaps in the fitness landscape, and regions of interest. Such information can be used to tailor the development of algorithms.",1,0,0,multi-objective optimisation+multi-objective problem+knapsack problems,
84,84,"Joint chinese word segmentation and part of speech tagging via two way attentions of auto analyzed knowledge. Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.",1,0,0,chinese word segmentation+pos tagging+sequence labeling,
85,85,"Efficient parking allocation as online bipartite matching with posted prices. We study online bipartite matching settings inspired by parking allocation problems, where rational agents arrive sequentially and select their most preferred parking slot. In contrast to standard online matching setting where edges incident to each arriving vertex are revealed upon its arrival, agents in our setting have private preferences over available slots. Our focus is on natural and simple pricing mechanisms, in the form of posted prices. On the one hand, the restriction to posted prices imposes new challenges relative to standard online matching.

On the other hand, we employ specific structures on agents' preferences that are natural in many scenarios including parking. We construct optimal and approximate pricing mechanisms under various informational and structural assumptions, and provide approximation upper bounds under the same assumptions. In particular, one of our mechanisms guarantees a better approximation bound than the classical result of Karp et al.[10] for unweighted online matching, under a natural structural restriction.",1,0,0,matching methods+bipartite matchings+approximation bounds,
86,86,"Residual phase cepstrum coefficients with application to cross lingual speaker verification. Speaker identification and verification has received a great deal of attention from the speech community, and significant gains in robustness and accuracy have been obtained over the past decade [1], [2]. However, the features used for identification are still primarily representations of overall spectral characteristics, and thus the models are primarily phonetic in nature, differentiating speakers based on overall pronunciation patterns. This creates difficulties in terms of the amount of enrollment data and complexity of the models required to cover the phonetic space, especially in tasks such as cross-lingual verification where enrollment and testing data may not have similar phonetic coverage. This paper introduces the use of a new feature for speaker verification, residual phase cepstral coefficients (RPCC), to capture speaker characteristics from their vocal excitation patterns. Results on a cross-lingual speaker verification task taken from the NIST 2004 SRE demonstrate that these RPCC features are significantly more accurate than traditional melfrequency cepstral coefficients (MFCC) when the amount of enrollment data available for training is limited. Additionally, because of the significant differences in the nature of the features, combining MFCC and RPCC features shows an improvement in verification results over MFCCs alone.",1,0,0,mfcc+speaker identification+speaker verification,
87,87,"Recurrent neural networks for language understanding. Recurrent Neural Network Language Models (RNN-LMs) have recently shown exceptional performance across a variety of applications. In this paper, we modify the architecture to perform Language Understanding, and advance the state-of-the-art for the widely used ATIS dataset. The core of our approach is to take words as input as in a standard RNN-LM, and then to predict slot labels rather than words on the output side. We present several variations that differ in the amount of word context that is used on the input side, and in the use of non-lexical features. Remarkably, our simplest model produces state-of-the-art results, and we advance state-of-the-art through the use of bagof-words, word embedding, named-entity, syntactic, and wordclass features. Analysis indicates that the superior performance is attributable to the task-specific word representations learned by the RNN.",1,0,0,recurrent neural networks+feedforward neural networks+language model,
88,88,"Early active learning via robust representation and structured sparsity. Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. Selecting the representative samples is an NP-hard problem, thus we employ the structured sparsity-inducing norm to relax the objective to an efficient convex formulation. Meanwhile, the robust sparse representation loss function is utilized to reduce the effect of outliers. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost and proved global convergence. Empirical results on both single-label and multi-label classification benchmark data sets show the promising results of our method.",1,0,0,optimization+sparse approximations+sparse recovery,
89,89,"Aerial grasping of a moving target with a quadrotor uav. For a quadrotor aircraft, we study the problem of planning a trajectory that connects two arbitrary states while allowing the UAV to grasp a moving target at some intermediate time. To this end, two classes of canonical grasping maneuvers are defined and characterized. A planning strategy relying on differential flatness is then proposed to concatenate one or more grasping maneuvers by means of spline-based subtrajectories, with the additional objective of minimizing the total transfer time. The proposed planning algorithm is not restricted to pure hovering-to-hovering motions and takes into account practical constraints, such as the finite duration of the grasping phase. The effectiveness of the proposed approach is shown by means of physically-based simulations.",1,0,0,grasp planning+attitude stabilization+planning algorithms,
90,90,"Multilingual grammar induction with continuous language identification. The key to multilingual grammar induction is to couple grammar parameters of different languages together by exploiting the similarity between languages. Previous work relies on linguistic phylogenetic knowledge to specify similarity between languages. In this work, we propose a novel universal grammar induction approach that represents language identities with continuous vectors and employs a neural network to predict grammar parameters based on the representation. Without any prior linguistic phylogenetic knowledge, we automatically capture similarity between languages with the vector representations and softly tie the grammar parameters of different languages. In our experiments, we apply our approach to 15 languages across 8 language families and subfamilies in the Universal Dependency Treebank dataset, and we observe substantial performance gain on average over monolingual and multilingual baselines.",1,0,0,neural networks+grammar induction+treebanks,
91,91,"Determining the polarity of words through a common online dictionary. Considerable attention has been given to polarity of words and the creation of large polarity lexicons. Most of the approaches rely on advanced tools like part-of-speech taggers and rich lexical resources such as WordNet. In this paper we show and examine the viability to create a moderate-sized polarity lexicon using only a common online dictionary, five positive and five negative words, a set of highly accurate extraction rules, and a simple yet effective polarity propagation algorithm. The algorithm evaluation results show an accuracy of 84.86% for a lexicon of 3034 words.",1,0,0,lexical database+wordnet+pos taggers,
92,92,"Dissimilarity based ensembles for multiple instance learning. In multiple instance learning, objects are sets (bags) of feature vectors (instances) rather than individual feature vectors. In this paper, we address the problem of how these bags can best be represented. Two standard approaches are to use (dis)similarities between bags and prototype bags, or between bags and prototype instances. The first approach results in a relatively low-dimensional representation, determined by the number of training bags, whereas the second approach results in a relatively high-dimensional representation, determined by the total number of instances in the training set. However, an advantage of the latter representation is that the informativeness of the prototype instances can be inferred. In this paper, a third, intermediate approach is proposed, which links the two approaches and combines their strengths. Our classifier is inspired by a random subspace ensemble, and considers subspaces of the dissimilarity space, defined by subsets of instances, as prototypes. We provide insight into the structure of some popular multiple instance problems and show state-of-the-art performances on these data sets.",1,0,0,ensemble classifiers+training sample+random subspaces,
93,93,"Generating events for dynamic social network simulations. Social Network Analysis in the last decade has gained remarkable attention. The current analysis focuses more and more on the dynamic behavior of them. The underlying structure from Social Networks, like facebook, or twitter, can change over time. Groups can be merged or single nodes can move from one group to another. But these phenomenas do not only occur in social networks but also in human brains. The research in neural spike trains also focuses on finding functional communities. These communities can change over time by switching the stimuli presented to the subject. In this paper we introduce a data generator to create such dynamic behavior, with effects in the interactions between nodes. We generate time stamps for events for one-to-one, one-to-many, and many-to-all relations. This data could be used to demonstrate the functionality of algorithms on such data, e.g. clustering or visualization algorithms. We demonstrated that the generated data fulfills common properties of social networks.",1,0,0,twitter+network analysis+clustering algorithms,
94,94,"Evolutionary feature selection for classification a plug in hybrid vehicle adoption application. We present a real-world application utilizing a Genetic Algorithm (GA) for exploratory multivariate association analysis of a large consumer survey designed to assess potential consumer adoption of Plug-in Hybrid Electric Vehicles (PHEVs). The GA utilizes an intersection/union crossover operator, in conjunction with high background mutation rates, to achieve rapid multivariate feature selection. We experimented with two alternative fitness measures based on classification results of a naive Bayes quadratic discriminant analysis; one fitness function rewarded only for correct classifications, and the other penalized for the degree of misclassification using a quadratic penalty function. We achieved high classification accuracy for three different survey outcome questions (with 3-, 5-, and 7- outcome classes, respectively). The quadratic penalty function yielded better overall results, returning smaller feature sets and overall more accurate contingency tables of predicted classes. Our results help to identify what consumer attributes best predict their likelihood of purchasing a PHEV. These findings will be used to better inform an existing agent-based model of PHEV market penetration, with the ultimate aim of helping auto manufacturers and policy makers identify leverage points in the system that will encourage PHEV market adoption.",1,0,0,crossover operator+classification methods+penalty function,
95,95,"Network games with and without synchroneity. To formulate a network security problem, Mavronicholas et al. [6] introduced a strategic game on an undirected graph whose nodes are exposed to infection by attackers, and whose edges are protected by a defender. Subsequently, MedSalem et al. [9] generalized the model so that they have many defenders instead of a single defender. Then in [1], we introduced a new network game with the roles of players interchanged, and obtained a graph-theoretic characterization of its pure Nash equilibria. In this paper we study mixed Nash equilibria for stochastic strategies in this new game, and then we generalize our network game to an asynchronous game, where two players repeatedly execute simultaneous games. Although the asynchronous game is formally an infinite game, we show that it has a stable solution by reducing it to a finite game.",1,0,0,stochastic+nash equilibrium+pure nash equilibrium,
96,96,"Automated detection of polyps in ct colonography images using deep learning algorithms in colon cancer diagnosis. Colon cancer is cancer that is present on the inner side of colon walls or the rectum walls in the large intestine. Most of these types of cancer begin as abnormal growth of tissue called as polyp. Colonography uses low dose radiation Computed tomography (CT) scanning to obtain an interior view of the colon making use of special x-ray machine to view the large intestine for cancer and abnormal growths known as polyps. Radiologists examine these images to find polyp like structure using computer tools. As CT Colonography image contain noise such as lungs, small intestine, instruments during image capturing; segmenting colon from noise is the key task. Polyp occurrence can be detected mainly using shape feature; eliminating shapes similar to polyp is challenging. Hence, to tackle above issues, Image processing techniques are used by applying deep learning algorithm - Convolution Neural Network (CNN) and the results are compared with classical machine learning algorithm. In proposed method, each image is pre-processed to filter air filled dark region that includes colon, lungs etc. Next, each pre-processed CT Image separated into fixed number of blocks. Using pre-trained CNN, each block of ROI is classified as Type 1 (Usually Ascending and descending colon), Type 2 (Usually Traversal and sigmoidal colon) and Type3 (Noise such as lungs, instruments) to segment colon blocks by eliminating noise. Classified Blocks is further diagnosed for polyp like structure using pre-trained CNN by classifying each colon block as normal or abnormal. The experiment is setup with classical machine learning algorithms - Random Forest (RF) and k-nearest neighbor (KNN) by extracting texture feature - Local binary pattern (LBP) and shape feature - Histogram oriented gradient (HOG) for comparison. The experiment results showed the accuracy of proposed method for colon segmentation using CNN (87%) outperforms RF (85%) and KNN (83%). In, addition, the polyp detection accuracy of CNN (88%) is better than Random forest (85%) and KNN (80%). Hence, in the proposed method, there is significant accuracy improvement using deep learning algorithm compared to classical machine learning algorithms. It also provide baseline for automated colon cancer diagnosis using Deep learning algorithms for further research.",1,0,0,document images+neural networks+convolutional neural networks,
97,97,"Camera based document image retrieval system using local features comparing srif with llah sift surf and orb. In this paper, we present camera-based document retrieval systems using various local features as well as various indexing methods. We employ our recently developed features, named Scale and Rotation Invariant Features (SRIF), which are computed based on geometrical constraints between pairs of nearest points around a keypoint. We compare SRIF with state-of-the-art local features. The experimental results show that SRIF outperforms the state-of-the-art in terms of retrieval time with 90.8% retrieval accuracy.",1,0,0,local feature+image retrieval+local invariant features,
98,98,"Generating event causality hypotheses through semantic relations. Event causality knowledge is indispensable for intelligent natural language understanding. The problem is that any method for extracting event causalities from text is insufficient; it is likely that some event causalities that we can recognize in this world are not written in a corpus, no matter its size. We propose a method of hypothesizing unseen event causalities from known event causalities extracted from the web by the semantic relations between nouns. For example, our method can hypothesize deploy a security camera→avoid crimes from deploy a mosquito net→avoid malaria through semantic relation A PREVENTS B. Our experiments show that, from 2.4 million event causalities extracted from the web, our method generated more than 300,000 hypotheses, which were not in the input, with 70% precision. We also show that our method outperforms a state-of-the-art hypothesis generation method.",1,0,0,semantic information+natural language understanding+natural language generation,
99,99,"Adaptive sequential recommendation using context trees. Machine learning is often used to acquire knowledge in domains that undergo frequent changes, such as networks, social media, or markets. These frequent changes pose a challenge to most machine learning methods as they have difficulty adapting. So my thesis topic focus on adaptive machine learning models. At the first step, we consider a forum content recommender system for massive open online courses (MOOCs) as an example of an application where recommendations have to adapt to new items and evolving user preferences. We formalize the recommendation problem as a sequence prediction problem and compare different recommendation methods, including a new method called context tree (CT). The results show that the CT recommender performs much better than other methods. We analyze the reasons for this and demonstrate that it is because of better adaptation to changes in the domain.",1,0,0,collaborative filtering+recommendation methods+machine learning,
100,100,"Towards fast and kernelized orthogonal discriminant analysis on person re identification. Abstract Recognizing a person across different non-overlapping camera views, is the task of person re-identification. For achieving the task, an effective way is to learn a discriminative metric by minimizing the within-class variance and maximizing the between-class variance simultaneously. However, the dimension of sample feature vector is usually greater than the number of training samples, as a result, the within-class scatter matrix is singular and the metric cannot be learned. In this paper, we propose to solve the singularity problem by employing the pseudo-inverse of the within-class scatter matrix and learning an orthogonal transformation for the metric. The proposed method can be effectively solved with a closed-form solution and no parameters required to tune. In addition, we develop a kernel version against non-linearity in person re-identification, and a fast version for more efficient solution. In experiments, we prove the validity and advantage of the proposed method for solving the singularity problem in person re-identification, and analyze the effectiveness of both kernel version and fast version. Extensively comparative experiments on VIPeR, PRID2011, CUHK01 and CUHK03 person re-identification benchmark datasets, show the state-of-the-art results of the proposed method.",1,0,0,training sample+within-class scatter matrix+scatter matrix,
101,101,"A benchmark of recent population based metaheuristic algorithms for multi layer neural network training. Multi-layer neural networks (MLNNs) are extensively used in many industrial applications. Training is the crucial task for MLNNs. While gradient descent-based approaches are most commonly employed here, they suffer from drawbacks such as getting stuck in local optima. One approach to address this is to employ population-based metaheuristic algorithms. Since a variety of such algorithms have been proposed in the literature, in this paper we benchmark the performance of 15 metaheuristic algorithms, including state-of-the-art as well as some of the most recent algorithms, for neural network training. In particular, we evaluate particle swarm optimisation (PSO), differential evolution (DE), artificial bee colony (ABC), imperialist competitive algorithm (ICA), cuckoo search (CS), gravitational search algorithm (GSA), bat algorithm (BA), firefly algorithm (FA), grey wolf optimiser (GWO), ant lion optimiser (ALO), dragonfly algorithm (DA), sine cosine algorithm (SCA), whale optimisation algorithm (WOA), grasshopper optimisation algorithm (GOA), and salp swarm algorithm (SSA), and assess their performance on different classification algorithms.",1,0,0,artificial bee colonies+differential evolution+competitive algorithms,
102,102,"Fictitious play for mean field games continuous time analysis and applications. In this paper, we deepen the analysis of continuous time Fictitious Play learning algorithm to the consideration of various finite state Mean Field Game settings (finite horizon, $\gamma$-discounted), allowing in particular for the introduction of an additional common noise. We first present a theoretical convergence analysis of the continuous time Fictitious Play process and prove that the induced exploitability decreases at a rate $O(\frac{1}{t})$. Such analysis emphasizes the use of exploitability as a relevant metric for evaluating the convergence towards a Nash equilibrium in the context of Mean Field Games. These theoretical contributions are supported by numerical experiments provided in either model-based or model-free settings. We provide hereby for the first time converging learning dynamics for Mean Field Games in the presence of common noise.",1,0,0,continuous time+nash equilibrium+numerical experiments,
103,103,"Decoder integration and expected bleu training for recurrent neural network language models. Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize for a task specific metric, such as BLEU in machine translation. We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion. Furthermore, we tackle the issue of directly integrating a recurrent network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a crossentropy trained model by up to 0.6 BLEU in a single reference setup.",1,0,0,machine translations+recurrent networks+statistical machine translation,
104,104,"Data augmentation using prosody and false starts to recognize non native children s speech. This paper describes AaltoASR's speech recognition system for the INTERSPEECH 2020 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. The task is to recognize non-native speech from children of various age groups given a limited amount of speech. Moreover, the speech being spontaneous has false starts transcribed as partial words, which in the test transcriptions leads to unseen partial words. To cope with these two challenges, we investigate a data augmentation-based approach. Firstly, we apply the prosody-based data augmentation to supplement the audio data. Secondly, we simulate false starts by introducing partial-word noise in the language modeling corpora creating new words. Acoustic models trained on prosody-based augmented data outperform the models using the baseline recipe or the SpecAugment-based augmentation. The partial-word noise also helps to improve the baseline language model. Our ASR system, a combination of these schemes, is placed third in the evaluation period and achieves the word error rate of 18.71%. Post-evaluation period, we observe that increasing the amounts of prosody-based augmented data leads to better performance. Furthermore, removing low-confidence-score words from hypotheses can lead to further gains. These two improvements lower the ASR error rate to 17.99%.",1,0,0,word error rate+acoustic model+data augmentation,
105,105,"Markov chains and rough sets. In this paper, we present a link between markov chains and rough sets. A rough approximation framework (RAF) gives a set of approximations for a subset of universe. Rough approximations using a collection of reference points gives rise to a RAF. We use the concept of markov chains and introduce the notion of a Markov rough approximation framework (MRAF), wherein a probability distribution function is obtained corresponding to a set of rough approximations. MRAF supplements well-known multi-attribute decision-making methods like TOPSIS and VIKOR in choosing initial weights for the decision criteria. Further, MRAF creates a natural route for deeper analysis of data which is very useful when the values of the ranked alternatives are close to each other. We give an extension to Pawlak's decision algorithm and illustrate the idea of MRAF with explicit example from telecommunication networks.",1,0,0,probability+distribution functions+probability distributions,
106,106,"Facial expression recognition based on deep learning convolution neural network a review. Facial emotional processing is one of the most important activities in effective calculations, engagement with people and computers, machine vision, video game testing, and consumer research. Facial expressions are a form of nonverbal communication, as they reveal a person's inner feelings and emotions. Extensive attention to Facial Expression Recognition (FER) has recently been received as facial expressions are considered. As the fastest communication medium of any kind of information. Facial expression recognition gives a better understanding of a person's thoughts or views and analyzes them with the currently trending deep learning methods. Accuracy rate sharply compared to traditional state-of-the-art systems. This article provides a brief overview of the different FER fields of application and publicly accessible databases used in FER and studies the latest and current reviews in FER using Convolution Neural Network (CNN) algorithms. Finally, it is observed that everyone reached good results, especially in terms of accuracy, with different rates, and using different data sets, which impacts the results.",1,0,0,deep learning+neural networks+convolutional neural networks,
107,107,"Reducing complexity in many objective optimization using community detection. Multi-objective optimization problems with many objective functions (≫3) are difficult to solve. This is because the time complexity of optimization algorithms often grows fast with the number of objective functions and the results of many objective optimization algorithms (Pareto front approximation) require large memory and their interpretation can be difficult for the decision maker. It is therefore attractive to reduce complexity of these problems. This can be achieved by decomposing them into a set of independent lower dimensional subproblems, or by aggregating some objective functions into a single objective function. This work introduces a new approach for decomposition and aggregation based on techniques from social network analysis. The key idea is to interpret an objective function as a node (agent) in a social network, and arcs between nodes indicate relationships: Negatively weighted arcs stand for conflicting objectives, zero weighted arcs for independent objectives, and positively weighted arcs for objectives that support each other. Using well-known algorithms BGLL for community detection we show that - given certain preconditions - it is possible to decompose a many objective optimization problem to a set of lower dimensional multi-objective optimization problems. This makes it easier to solve the problem and interpret the resulting trade-off (hyper-)surfaces. In the paper, after introducing the idea of Communtiy Detection for Many-Objective Optimization (CoDeMO), we lay out an interactive workflow and test it on simple, scalable many-objective optimization problems - variants of facility location problems - and thereby provide a prove-of-concept study.",1,0,0,pareto front+single objective+multi-objective optimization problem,
108,108,"Tree interpolation in vampire. We describe new extensions of the Vampire theorem prover for computing tree interpolants. These extensions generalize Craig interpolation in Vampire, and can also be used to derive sequence interpolants. We evaluated our implementation on a large number of examples over the theory of linear integer arithmetic and integer-indexed arrays, with and without quantifiers. When compared to other methods, our experiments show that some examples could only be solved by our implementation.",1,0,0,hardware implementations+interpolation algorithms+floating-point arithmetic,
109,109,"Deep hashing a joint approach for image signature learning. Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions, which separates the procedures of feature extraction and hash function learning. In this paper, we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds: 1) deep network optimization is often achieved by gradient propagation, which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue, we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification, and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks, our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular, our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In comparison, the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively.",1,0,0,hashing+image classification+image hashing,
110,110,"Network pollution games. We introduce a new network model of the pollution control problem and present two applications of this model. On a high level, our model comprises a graph whose nodes represent the agents, that could be thought of as sources of pollution, and edges between agents represent the effect of spread of pollution. The government as the regulator is responsible to maximize the social welfare while setting bounds on the levels of emitted pollution both locally and globally. Our model is inspired by the existing literature in environmental economics that applies game theoretical methodology to control pollution. We study the social welfare maximization problem in our model. Our main results include hardness results for the problem, and in complement, a constant approximation algorithm on planar graphs. Our approximation algorithm leads to a truthful in expectation mechanism, and it is obtained by a novel decomposition technique of planar graphs to deal with constraints on vertices. We note that no known planar decomposition techniques can be used here and our technique can be of independent interest.",1,0,0,general graph+approximation algorithms+control problems,
111,111,"Improving multi label learning with missing labels by structured semantic correlations. Multi-label learning has attracted significant interests in computer vision recently, finding applications in many vision tasks such as multiple object recognition and automatic image annotation. Associating multiple labels to a complex image is very difficult, not only due to the intricacy of describing the image, but also because of the incompleteness nature of the observed labels. Existing works on the problem either ignore the label-label and instance-instance correlations or just assume these correlations are linear and unstructured. Considering that semantic correlations between images are actually structured, in this paper we propose to incorporate structured semantic correlations to solve the missing label problem of multi-label learning. Specifically, we project images to the semantic space with an effective semantic descriptor. A semantic graph is then constructed on these images to capture the structured correlations between them. We utilize the semantic graph Laplacian as a smooth term in the multi-label learning formulation to incorporate the structured semantic correlations. Experimental results demonstrate the effectiveness of the proposed semantic descriptor and the usefulness of incorporating the structured semantic correlations. We achieve better results than state-of-the-art multi-label learning methods on four benchmark datasets.",1,0,0,object recognition+image annotation+automatic image annotation,
112,112,"A hybrid social spider optimization and genetic algorithm for minimizing molecular potential energy function. The minimization of the molecular potential energy function is one of the most important real-life problems which can help to predict the 3D structure of the protein by knowing the steady (ground) state of the molecules of the protein. In this paper, we propose a new hybrid algorithm between the social spider algorithm and the genetic algorithm in order to minimize a simplified model of the energy function of the molecule. We call the proposed algorithm by hybrid social spider optimization and genetic algorithm (HSSOGA). The HSSOGA comprises of three main steps. In the first step, we apply the social spider optimization algorithm to balance between the exploration and the exploitation processes in the proposed algorithm. In the second step, we use the dimensionality reduction process and the population partitioning process by dividing the population into subpopulations and applying the arithmetical crossover operator for each subpopulation order to increase the diversity of the search in the algorithm. In the last steps, we use the genetic mutation operator in the whole population to avoid the premature convergence and avoid trapping in local minima. The combination of three steps helps the proposed algorithm to solve the molecular potential energy function with different molecules size, especially when the problem dimension $$D>100$$D>100 with powerful performance. We test it on 13 large-scale unconstrained global optimization problems to investigate its performance on these functions. In order to investigate the efficiency of the proposed HSSOGA, we compare HSSOGA against eight benchmark algorithms when we minimize the potential energy function problem. The numerical experiment results show that the proposed algorithm is a promising and efficient algorithm and can obtain the global minimum or near global minimum of the large-scale optimization problems up to 1000 dimension and the molecular potential energy function of the simplified model with up to 200 degrees of freedom faster than the other comparative algorithms.",1,0,0,global optimization problems+premature convergence+numerical experiments,
113,113,"Type 2 fuzzy gmm ubm for text independent speaker verification. This paper proposes the use of a type-2 fuzzy framework in the standard GMM-UBM based text-independent speaker verification systems. Based on type-2 fuzzy sets, the framework provides pertinence intervals for the models. The decision process is obtained using a Support Vectors Machine (SVM) that processes the interval likelihoods. A Voice Activity Detection (VAD) algorithm was also used to discard the parts of the speech signal without voice. The proposed method was tested on the MIT Device Speaker Verification Corpus which contains several different mobile devices used in different environments. The result shows the robustness of the system and the improvements in the verification ratios of the T2F-GMM-UBM compared to the classical GMM-UBM based systems.",1,0,0,gaussian mixtures+speaker verification+speaker verification system,
114,114,"Dual encoding for zero example video retrieval. This paper attacks the challenging problem of zero-example video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and end-to-end. As experiments on three benchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.",1,0,0,natural languages+frames+video retrieval,
115,115,"Patterning of writing style evolution by means of dynamic similarity. Abstract This paper suggests a new methodology for patterning writing style evolution using dynamic similarity. We divide a text into sequential, disjoint portions (chunks) of the same size and exploit the Mean Dependence measure, aspiring to model the writing process via association between the current text chunk and its predecessors. To expose the evolution of a style, a new two-step clustering procedure is applied. In the first phase, a distance based on the Mean Dependence between each pair of chunks is evaluated. All document chunks in a pair are embedded in a high dimensional space using a Kuratowski-type embedding procedure and clustered by means of the introduced distance. In the next phase, the rows of the binary cluster classification documents matrix are clustered via the hierarchical single linkage clustering algorithm. By this way, a visualization of the inner stylistic structure of a texts’ collection, the resulting classification tree, is provided by the appropriate dendrogram. The approach applied to studying writing style evolution in the “Foundation Universe” by Isaac Asimov, the “Rama” series by Arthur C. Clarke, the “Forsyte Saga” of John Galsworthy, “The Lord of the Rings” by John Ronald Reuel Tolkien and a collection of books prescribed to Romain Gary demonstrates that the suggested methodology is capable of identifying style development over time. Additional numerical experiments with author determination and author verification tasks exhibit the high ability of the method to provide accurate solutions.",1,0,0,clustering methods+classification models+numerical experiments,
116,116,"Self learning machines using deep networks. Self learning machines as defined in this paper are those learning by observation under limited supervision, and continuously adapt by observing the surrounding environment. The aim is to mimic the behavior of human brain learning from surroundings with limited supervision, and adapting its learning according to input sensory observations. Recently, Deep Belief Nets (DBNs) [1] have made good use of unsupervised learning as pre-training stage, which is equivalent to the observation stage in humans. However, they still need supervised training set to adjust the network parameters, as well as being nonadaptive to real world examples. In this paper, Self Learning Machine (SLM) is proposed based on deep belief networks and deep auto encoders",1,0,0,unsupervised learning method+unsupervised learning+auto encoders,
117,117,"Photon a robust cross domain text to sql system. Natural language interfaces to databases(NLIDB) democratize end user access to relational data. Due to fundamental differences between natural language communication and programming, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying query language. We present PHOTON, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping cannot be immediately determined. PHOTON consists of a strong neural semantic parser (63.2% structure accuracy on the Spider dev benchmark), a human-in-the-loop question corrector, a SQL executor and a response generator. The question corrector isa discriminative neural sequence editor which detects confusion span(s) in the input question and suggests rephrasing until a translatable input is given by the user or a maximum number of iterations are conducted. Experiments on simulated data show that the proposed method effectively improves the robustness of text-to-SQL system against untranslatable user input.The live demo of our system is available at https://www.naturalsql.com",1,0,0,relational data models+natural language questions+natural language understanding,
118,118,"Word level font to font image translation using convolutional recurrent generative adversarial networks. Conversion of one font to another font is very useful in real life applications. In this paper, we propose a Convolutional Recurrent Generative model to solve the word level font transfer problem. Our network is able to convert the font style of any printed text images from its current font to the required font. The network is trained end-to-end for the complete word images. Thus it eliminates the necessary pre-processing steps, like character segmentations. We extend our model to conditional setting that helps to learn one-to-many mapping function. We employ a novel convolutional recurrent model architecture in the Generator that efficiently deals with the word images of arbitrary width. It also helps to maintain the consistency of the final images after concatenating the generated image patches of target font. Besides, the Generator and the Discriminator network, we employ a Classification network to classify the generated word images of converted font style to their subsequent font categories. Most of the earlier works related to image translation are performed on square images. Our proposed architecture is the first work which can handle images of varying widths. Word images generally have varying width depending on the number of characters present. Hence, we test our model on a synthetically generated font dataset. We compare our method with some of the state-of-the-art methods for image translation. The superior performance of our network on the same dataset proves the ability of our model to learn the font distributions.",1,0,0,generative adversarial networks+character segmentation+word images,
119,119,"Metaheuristics for the multiobjective surgery admission planning problem. This paper presents a comparative survey on metaheuristics applied to the multiobjective surgery admission planning problem. Three well-known metaheuristics: genetic algorithm (GA), simulated annealing (SA) and variable neighbourhood descent (VND) are compared using the Wilcoxon signed rank test. The metaheuristics are also benchmarked against a hybrid GA that uses the VND as a local search procedure. The weighted sum method is used to balance five competing objectives: operating room overtime, operating room idle time, surgeon overtime, surgeon idle time and patient waiting time. As a preparation for future multiobjectivity analysis, a simple example shows how several non-dominated trade-off solutions may be presented to the decision maker using the $\epsilon$ -constrained method and the non-dominated sorting genetic algorithm II (NSGA-II). The results are meant to serve as a starting point for further development and testing where the challenge of uncertain surgery durations will be included.",1,0,0,nsga-ii+non-dominated sorting+multi-objective optimisation,
120,120,"Mcts based on simple regret. UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final ""arm pull"" (the actual move selection) that collects a reward, rather than all ""arm pulls"". Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multiarmed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically.

Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is ""aware"" of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms.",1,0,0,mdp+markov decision processes+finite time,
121,121,"Affordance based grasping and manipulation in real world applications. In real world applications, robotic solutions remain impractical due to the challenges that arise in unknown and unstructured environments. To perform complex manipulation tasks in complex and cluttered situations, robots need to be able to identify the interaction possibilities with the scene, i.e. the affordances of the objects encountered. In unstructured environments with noisy perception, insufficient scene understanding and limited prior knowledge, this is a challenging task. In this work, we present an approach for grasping unknown objects in cluttered scenes with a humanoid robot in the context of a nuclear decommissioning task. Our approach combines the convenience and reliability of autonomous robot control with the precision and adaptability of teleoperation in a semi-autonomous selection of grasp affordances. Additionally, this allows exploiting the expert knowledge of an experienced human worker. To evaluate our approach, we conducted 75 real world experiments with more than 660 grasp executions on the humanoid robot ARMAR-6. The results demonstrate that high-level decisions made by the human operator, supported by autonomous robot control, contribute significantly to successful task execution.",1,0,0,robot arms+autonomous mobile robot+scene understanding,
122,122,"Indoor mapping using planes extracted from noisy rgb d sensors. This paper presents a fast and robust plane feature extraction and matching technique for RGB-D type sensors. We propose three algorithm components required to utilize the plane features in an online Simultaneous Localization and Mapping (SLAM) problem: fast plane extraction, frame-to-frame constraint estimation, and plane merging. For the fast plane extraction, we estimate local surface normals and curvatures by a simple spherical model and then segment points using a modified flood fill algorithm. In plane parameter estimation, we suggest a new uncertainty estimation method which is robust against the measurement bias, and also introduce a fast boundary modeling method. We associate the plane features based on both the parameters and the spatial coverage, and estimate the stable constraints by the cost function with a regulation term. Also, our plane merging technique provides a way of generating local maps that are useful for estimating loop closure constraints. We have performed real-world experiments at our lab environment. The results demonstrate the efficiency and robustness of the proposed algorithm.",1,0,0,matching methods+estimation method+loop closure,
123,123,"Nostalgic adam weighting more of the past gradients when designing the adaptive learning rate. First-order optimization algorithms have been proven prominent in deep learning. In particular, algorithms such as RMSProp and Adam are extremely popular. However, recent works have pointed out the lack of ``long-term memory"" in Adam-like algorithms, which could hamper their performance and lead to divergence. In our study, we observe that there are benefits of weighting more of the past gradients when designing the adaptive learning rate. We therefore propose an algorithm called the Nostalgic Adam (NosAdam) with theoretically guaranteed convergence at the best known convergence rate. NosAdam can be regarded as a fix to the non-convergence issue of Adam in alternative to the recent work of [Reddi et al., 2018]. Our preliminary numerical experiments show that NosAdam is a promising alternative algorithm to Adam. The proofs, code and other supplementary materials can be found in an anonymously shared link.",1,0,0,deep learning+adaptive learning rates+numerical experiments,
124,124,"Power quality improvement for a hybrid renewable farm using upqc. This article describes the eminence of power quality improvement in the integrated grid energy system with the solar photovoltaic (SPV) and wind energy (WE) hybridization using unified power quality controller (UPQC). The UPQC comprises of shunt active and series active power converters to enable the control schemes such as discrete 3 phase - phase locked loop (PLL) via a-b-c to d-q transformation and d-q to a-b-c transformation respectively. Subsequently, the system is subjected to the repeated disturbances in AC loads and output power generated from the renewable farm throughout the power transmission. Therefore itis mandatory to overcome the foresaid issues by incorporating a variable reactive power source. Apart from the regular application, UPQC is configured to look up the various concerns in connection to the quality of power such as diluting the harmonic current, voltage imbalance and reactive power compensation, sag and swell phenomena. The control technique for UPQC is implemented through the fuzzy logic controllers. The proposed UPQC also helps in dropping the energylosses that happen in power systems components and also ensures the safety environment. The imposed novel idea is experimented through PSCAD simulation platform and the obtained results certainly justifies the proposed UPQC for trapping the harmonic agents in the distributed renewable energy farms while exposed to non-linear/ susceptible load conditions.",1,0,0,distributed generators+fuzzy logic controllers+distributed generation system+reactive power compensation,
125,125,"Real time human motion capture with multiple depth cameras. Commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras. In this work we present an efficient and inexpensive solution to markerless motion capture using only a few Kinect sensors. Unlike the previous work on 3d pose estimation using a single depth camera, we relax constraints on the camera location and do not assume a co-operative user. We apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data. Our method accurately localizes body parts without requiring an explicit shape model. The body joint locations are then recovered by combining evidence from multiple views in real-time. We also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the Berkeley MHAD dataset.",1,0,0,segmentation methods+3d pose estimation+image segmentation,
126,126,"Towards understanding regularization in batch normalization. Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.",1,0,0,neural networks+convolutional neural networks+regularization,
127,127,"Information theoretic learning auto encoder. We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a paritition function. This paper also formalizes, generative moment matching networks under the ITL framework.",1,0,0,auto encoders+regularization+matching networks,
128,128,"Speaker hand offs in collaborative human agent oral presentations. Prior studies of public speaking behavior have demonstrated that public speaking anxiety monotonically decreases with the number of co-presenters giving an oral presentation and increases with the size of the audience. However, speaker ""hand off"" behavior---the verbal and nonverbal cues used to transition from one speaker to another---and its effect on speaker anxiety and presentation quality has not been systematically studied. In this work we report on two empirical studies of speaker hand-off behavior used during human co-presentations. We find that the cues used for hand-offs during prepared and rehearsed presentations differ significantly from the cues observed in face-to-face conversational turn-taking. We describe two systems that leverage automatic recognition of these verbal and nonverbal cues to drive hand-offs during co-presentations with a life-sized virtual agent.",1,0,0,spoken dialogue+software agents+intelligent agents,
129,129,"Hierarchical clustering identifies hub nodes in a model of resting state brain activity. A novel clustering algorithm is presented for analyzing the temporal dynamics of synchronization in networks of coupled oscillators and applied to a model of resting-state brain activity. Connectivity in the model is based on a human-brain structural connectivity matrix derived from diffusion tensor imaging tractography. We find a strong correspondence between areas of high synchronization and highly connected “hub” nodes, anatomical regions forming the structural core of the network linking all areas of the brain. Such models have the potential to increase our understanding of the constraints placed on brain function by underlying anatomical structure.",1,0,0,tractography+hier-archical clustering+clustering methods,
130,130,"A pheromone based traffic management model for vehicle re routing and traffic light control. We propose a pheromone-based traffic management model, which simultaneously optimizes vehicle re-routing and traffic light control. Specifically, each car agent deposits multiple digital pheromone on its route. The road infrastructure agents fuse the pheromone to forecast traffic conditions. Once a congested road is predicted, we adopt a proactive vehicle re-routing algorithm for assigning alternative routes to cars before they enter the congested road. At the same time, the traffic light control agents use an online strategy for assigning long time duration of green traffic lights to the roads with a large amount of pheromone. Experimental results confirm its superiority in alleviating traffic congestion.",1,0,0,congestion+routing algorithms+intelligent agents,
131,131,"Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces optimal rate and curse of dimensionality. Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any non-adaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.",1,0,0,maximum likelihood estimator+maximum likelihood estimate+rate of convergence,
132,132,"Matrix completion with noisy side information. We study the matrix completion problem with side information. Side information has been considered in several matrix completion applications, and has been empirically shown to be useful in many cases. Recently, researchers studied the effect of side information for matrix completion from a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features. However, since in reality most given features are noisy or only weakly informative, the development of a model to handle a general feature set, and investigation of how much noisy features can help matrix recovery, remains an important issue. In this paper, we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise. Moreover, we study the effect of general features in theory and show that by using our model, the sample complexity can be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight into the usefulness of general side information. Finally, we consider synthetic data and two applications — relationship prediction and semi-supervised clustering — and show that our model outperforms other methods for matrix completion that use features both in theory and practice.",1,0,0,feature space+additive noise+semi-supervised clustering,
133,133,"An improved object detection method based on deep convolution neural network for smoke detection. The smoke detection plays a very important role in fire alarm. However, the accuracy of smoke detection is low and difficult to detect in open space by traditional methods. In this paper, we introduce an improved object detection method based on deep convolution neural network(CNN) to address this issue. Firstly, we substitute the feature extractor (such as Inception Net and Resnet) in Various neural network object detectors for Faster R-CNN, Single Shot MultiBox Detector (SSD), Region based Fully Convolutional Networks (R-FCN). Secondly, the parametersof the object detection algorithm are optimized on MSCOCO dataset. Finally, the experiments are conducted on the smoke detection dataset. The experiments result demonstrate the mAP reached56.04% on the smoke detection dataset. Compared with the existing smoke detection methods, the present method has achieved good results both in accuracy and speed.",1,0,0,convolutional neural networks+object detection method+object detection,
134,134,"Self supervised learning for single view depth and surface normal estimation. In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piece-wise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark. Demo video: this https URL",1,0,0,single images+neural networks+convolutional neural networks,
135,135,"Rgb d point cloud registration based on salient object detection. We propose a robust algorithm for aligning rigid, noisy, and partially overlapping red green blue-depth (RGB-D) point clouds. To address the problems of data degradation and uneven distribution, we offer three strategies to increase the robustness of the iterative closest point (ICP) algorithm. First, we introduce a salient object detection (SOD) method to extract a set of points with significant structural variation in the foreground, which can avoid the unbalanced proportion of foreground and background point sets leading to the local registration. Second, registration algorithms that rely only on structural information for alignment cannot establish the correct correspondences when faced with the point set with no significant change in structure. Therefore, a bidirectional color distance (BCD) is designed to build precise correspondence with bidirectional search and color guidance. Third, the maximum correntropy criterion (MCC) and trimmed strategy are introduced into our algorithm to handle with noise and outliers. We experimentally validate that our algorithm is more robust than previous algorithms on simulated and real-world scene data in most scenarios and achieve a satisfying 3-D reconstruction of indoor scenes.",1,0,0,adaptive algorithms+registration algorithms+object detection,
136,136,"No regret bayesian optimization with unknown hyperparameters. Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.",1,0,0,gaussians+gaussian processes+regularization parameters,
137,137,"Chinese readability assessment using tf idf and svm. This paper proposes a simple yet effective method to automatically determine the readability of Chinese articles. We use mutual information to select the most important terms from the training data, calculate TF-IDF values based on those terms, and use those values as features for SVM to build classification models that identify articles suitable for lower grade students and middle grade students in elementary school. The experiments on elementary school textbooks produce satisfactory results.",1,0,0,svm+classification models+mutual informations,
138,138,"A communication efficient parallel algorithm for decision tree. Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, the indices of these top attributes are aggregated by a server, and the globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency.",1,0,0,probability+boosting+interpretability,
139,139,"Heuristics in permutation gomea for solving the permutation flowshop scheduling problem. The recently introduced permutation Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) has shown to be an effective Model Based Evolutionary Algorithm (MBEA) for permutation problems. So far, permutation GOMEA has only been used in the context of Black-Box Optimization (BBO). This paper first shows that permutation GOMEA can be improved by incorporating a constructive heuristic to seed the initial population. Secondly, the paper shows that hybridizing with job swapping neighborhood search does not lead to consistent improvement. The seeded permutation GOMEA is compared to a state-of-the-art algorithm (VNS4) for solving the Permutation Flowshop Scheduling Problem (PFSP). Both unstructured and structured instances are used in the benchmarks. The results show that permutation GOMEA often outperforms the VNS4 algorithm for the PFSP with the total flowtime criterion.",1,0,0,sequence-dependent setup time+unrelated parallel machines+black-box optimization,
140,140,"Arabic handwritten document preprocessing and recognition. Arabic handwritten documents present specific challenges due to the cursive nature of the writing and the presence of diacritical marks. Moreover, one of the largest labeled database of Arabic handwritten documents, the OpenHart-NIST database includes specific noise, namely guidelines, that has to be addressed. We propose several approaches to process these documents. First a guideline detection approach has been developed, based on K-means, that detects the documents that include guidelines. We then propose a series of preprocessing at text-line level to reduce the noise effects. For text-lines including guidelines, a guideline removal preprocessing is described and existing keystroke restoration approaches are assessed. In addition, we propose a preprocessing that combines noise removal and deskewing by removing line fragments from neighboring text lines, while searching for the principal orientation of the text-line. We provide recognition results, showing the significant improvement brought by the proposed processings.",1,0,0,handwritten document+text lines+handwritten words,
141,141,"Robot learning manipulation action plans by watching unconstrained videos from the world wide web. In order to advance action generation and creation in robots beyond simple learned schemas we need computational tools that allow us to automatically interpret and represent human actions. This paper presents a system that learns manipulation action plans by processing unconstrained videos from the World Wide Web. Its goal is to robustly generate the sequence of atomic actions of seen longer actions in video in order to acquire knowledge for robots. The lower level of the system consists of two convolutional neural network (CNN) based recognition modules, one for classifying the hand grasp type and the other for object recognition. The higher level is a probabilistic manipulation action grammar based parsing module that aims at generating visual sentences for robot manipulation. Experiments conducted on a publicly available unconstrained video dataset show that the system is able to learn manipulation actions by ""watching"" unconstrained videos with high accuracy.",1,0,0,robot learning+human pose+human-action recognition,
142,142,"Cross pose facial expression recognition. In real world facial expression recognition (FER) applications, it is not practical for a user to enroll his/her facial expressions under different pose angles. Therefore, a desirable property of a FER system would be to allow the user to enroll his/her facial expressions under a single pose, for example frontal, and be able to recognize them under different pose angles. In this paper, we address this problem and present a method to recognize six prototypic facial expressions of an individual across different pose angles. We use Partial Least Squares to map the expressions from different poses into a common subspace, in which covariance between them is maximized. We show that PLS can be effectively used for facial expression recognition across poses by training on coupled expressions of the same identity from two different poses. This way of training lets the learned bases model the differences between expressions of different poses by excluding the effect of the identity. We have evaluated the proposed approach on the BU3DFE database and shown that it is possible to successfully recognize expressions of an individual from arbitrary viewpoints by only having his/her expressions from a single pose, for example frontal pose as the most practical case. Overall, we achieved an average recognition rate of 87.6% when using frontal images as gallery and 86.6% when considering all pose pairs.",1,0,0,database systems+facial expression recognition+expression recognition,
143,143,"Customer sharing in economic networks with costs. In an economic market, sellers, infomediaries and customers constitute an economic network. Each seller has her own customer group and the seller's private customers are unobservable to other sellers. Therefore, a seller can only sell commodities among her own customers unless other sellers or infomediaries share her sale information to their customer groups. However, a seller is not incentivized to share others' sale information by default, which leads to inefficient resource allocation and limited revenue for the sale. To tackle this problem, we develop a novel mechanism called customer sharing mechanism (CSM) which incentivizes all sellers to share each other's sale information to their private customer groups. Furthermore, CSM also incentivizes all customers to truthfully participate in the sale. In the end, CSM not only allocates the commodities efficiently but also optimizes the seller's revenue.",1,0,0,economics+sales+revenue,
144,144,Train detection and tracking in optical time domain reflectometry otdr signals. We propose a novel method for the detection of vibrations caused by trains in an optical fiber buried nearby the railway track. Using optical time-domain reflectometry vibrations in the ground caused by different sources can be detected with high accuracy in time and space. While several algorithms have been proposed in the literature for train tracking using OTDR signals they have not been tested on longer recordings. The presented method learns the characteristic pattern in the Fourier domain using a support vector machine (SVM) and it becomes more robust to any kind of noise and artifacts in the signal. The point-based causal train tracking has two stages to minimize the influence of false classifications of the vibration detection. Our technical contribution is the evaluation of the presented algorithm based on two hour long recording and demonstration of open problems for commercial usage.,1,0,0,fourier+svm+fourier domains,
145,145,"Intuitionistic fuzzy bonferroni means. The Bonferroni mean (BM) was originally introduced by Bonferroni and then more recently generalized by Yager. The desirable characteristic of the BM is its capability to capture the interrelationship between input arguments. Nevertheless, it seems that the existing literature only considers the BM for aggregating crisp numbers instead of any other types of arguments. In this paper, we investigate the BM under intuitionistic fuzzy environments. We develop an intuitionistic fuzzy BM (IFBM) and discuss its variety of special cases. Then, we apply the weighted IFBM to multicriteria decision making. Some numerical examples are given to illustrate our results.",1,0,0,numerical example+kripke semantics+ordered weighted averaging,
146,146,"Educational neurogaming eeg controlled videogames as interactive teaching tools for introductory neuroscience. In order to advance the field of neuroscience, we must continue motivating youth to pursue science education. In this report we tested the idea of combining neurogaming with education. We developed a pair of electroencephalography (EEG)-controlled neurogames using inexpensive and/or free tools to teach students about the fundamentals of neuroscience and brain machine interfaces (BMI) through a fun, interactive activity. We report on the particular concepts they allowed us to introduce, the techniques and methods we used, and the effect of the activities on stimulating students’ interest in neuroscience, and discuss how to optimize the learning experience. We conclude that educational neurogames could be a key tool for furthering and motivating neuroscience education.",1,0,0,teaching method+brain machine interface+electroencephalography (eeg),
147,147,Epistemic equilibrium logic. We add epistemic modal operators to the language of here-and-there logic and define epistemic here-and-there models. We then successively define epistemic equilibrium models and autoepistemic equilibrium models. The former are obtained from here-and-there models by the standard minimisation of truth of Pearce's equilibrium logic; they provide an epistemic extension of that logic. The latter are obtained from the former by maximising the set of epistemic possibilities; they provide a new semantics for Gelfond's epistemic specifications. For both definitions we characterise strong equivalence by means of logical equivalence in epistemic here-and-there logic.,1,0,0,dynamic logic+epistemic logic+modal operators,
148,148,"Tensor product generation networks for deep nlp modeling. We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture --- the Tensor Product Generation Network (TPGN) --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.",1,0,0,deep learning+long short term memory neural networks+neural networks,
149,149,"Deep stereo matching with dense crf priors. Stereo reconstruction from rectified images has recently been revisited within the context of deep learning. Using a deep Convolutional Neural Network to obtain patch-wise matching cost volumes has resulted in state of the art stereo reconstruction on classic datasets like Middlebury and Kitti. By introducing this cost into a classical stereo pipeline, the final results are improved dramatically over non-learning based cost models. However these pipelines typically include hand engineered post processing steps to effectively regularize and clean the result. Here, we show that it is possible to take a more holistic approach by training a fully end-to-end network which directly includes regularization in the form of a densely connected Conditional Random Field (CRF) that acts as a prior on inter-pixel interactions. We demonstrate that our approach on both synthetic and real world datasets outperforms an alternative end-to-end network and compares favorably to more hand engineered approaches.",1,0,0,convolutional neural networks+conditional random field+regularization,
150,150,"Ujnlp at semeval 2020 task 12 detecting offensive language using bidirectional transformers. In this paper, we built several pre-trained models to participate SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media. In the common task of Offensive Language Identification in Social Media, pre-trained models such as Bidirectional Encoder Representation from Transformer (BERT) have achieved good results. We preprocess the dataset by the language habits of users in social network. Considering the data imbalance in OffensEval, we screened the newly provided machine annotation samples to construct a new dataset. We use the dataset to fine-tune the Robustly Optimized BERT Pretraining Approach (RoBERTa). For the English subtask B, we adopted the method of adding Auxiliary Sentences (AS) to transform the single-sentence classification task into a relationship recognition task between sentences. Our team UJNLP wins the ranking 16th of 85 in English subtask A (Offensive language identification).",1,0,0,social networks+classification models+classification methods,
151,151,"Approximate optimal control design for nonlinear one dimensional parabolic pde systems using empirical eigenfunctions and neural network. This paper addresses the approximate optimal control problem for a class of parabolic partial differential equation (PDE) systems with nonlinear spatial differential operators. An approximate optimal control design method is proposed on the basis of the empirical eigenfunctions (EEFs) and neural network (NN). First, based on the data collected from the PDE system, the Karhunen-Loeve decomposition is used to compute the EEFs. With those EEFs, the PDE system is formulated as a high-order ordinary differential equation (ODE) system. To further reduce its dimension, the singular perturbation (SP) technique is employed to derive a reduced-order model (ROM), which can accurately describe the dominant dynamics of the PDE system. Second, the Hamilton-Jacobi-Bellman (HJB) method is applied to synthesize an optimal controller based on the ROM, where the closed-loop asymptotic stability of the high-order ODE system can be guaranteed by the SP theory. By dividing the optimal control law into two parts, the linear part is obtained by solving an algebraic Riccati equation, and a new type of HJB-like equation is derived for designing the nonlinear part. Third, a control update strategy based on successive approximation is proposed to solve the HJB-like equation, and its convergence is proved. Furthermore, an NN approach is used to approximate the cost function. Finally, we apply the developed approximate optimal control method to a diffusion-reaction process with a nonlinear spatial operator, and the simulation results illustrate its effectiveness.",1,0,0,riccati equations+optimal control problem+algebraic riccati equations,
152,152,Self segregating and coordinated segregating transformer for focused deep multi modular network for visual question answering. Attention mechanism has gained huge popularity due to its effectiveness in achieving high accuracy in different domains. But attention is opportunistic and is not justified by the content or usability of the content. Transformer like structure creates all/any possible attention(s). We define segregating strategies that can prioritize the contents for the applications for enhancement of performance. We defined two strategies: Self-Segregating Transformer (SST) and Coordinated-Segregating Transformer (CST) and used it to solve visual question answering application. Self-segregation strategy for attention contributes in better understanding and filtering the information that can be most helpful for answering the question and create diversity of visual-reasoning for attention. This work can easily be used in many other applications that involve repetition and multiple frames of features and would reduce the commonality of the attentions to a great extent. Visual Question Answering (VQA) requires understanding and coordination of both images and textual interpretations. Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.,1,0,0,reasoning+frames+question answering,
153,153,"Training recurrent answering units with joint loss minimization for vqa. We propose a novel algorithm for visual question answering based on a recurrent deep neural network, where every module in the network corresponds to a complete answering unit with attention mechanism by itself. The network is optimized by minimizing loss aggregated from all the units, which share model parameters while receiving different information to compute attention probability. For training, our model attends to a region within image feature map, updates its memory based on the question and attended image feature, and answers the question based on its memory state. This procedure is performed to compute loss in each step. The motivation of this approach is our observation that multi-step inferences are often required to answer questions while each problem may have a unique desirable number of steps, which is difficult to identify in practice. Hence, we always make the first unit in the network solve problems, but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model. To implement this idea, we early-stop training each unit as soon as it starts to overfit. Note that, since more complex models tend to overfit on easier questions quickly, the last answering unit in the unfolded recurrent neural network is typically killed first while the first one remains last. We make a single-step prediction for a new question using the shared model. This strategy works better than the other options within our framework since the selected model is trained effectively from all units without overfitting. The proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in VQA dataset.",1,0,0,recurrent neural networks+feedforward neural networks+backpropagation algorithm,
154,154,"Optimizing locations and scales of distribution centers under uncertainty. In supply chain networks of large companies, how to determine the location for establishing distribution centers is an important strategic-level decision problem. This paper proposes a stochastic programming model to determine distribution centers’ locations as well as their scales. The objective of the model is to minimize the expected total transportation cost under uncertain demands of customers. An improved particle swarm optimization algorithm and a Lagrangean relaxation-based solution approach are proposed to solve the model. This paper also performs a case study on applying this model to the largest retailer in China. In addition, some numerical experiments are conducted to validate the effectiveness of the proposed model and the efficiency of the proposed solution methods.",1,0,0,artificial bee colonies+differential evolution+numerical experiments,
155,155,"Learning not to learn training deep neural networks with biased data. We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.",1,0,0,neural networks+feedforward neural networks+regularization,
156,156,"A distributed intrusion detection framework based on evolved specialized ensembles of classifiers. Modern intrusion detection systems must handle many complicated issues in real-time, as they have to cope with a real data stream; indeed, for the task of classification, typically the classes are unbalanced and, in addition, they have to cope with distributed attacks and they have to quickly react to changes in the data. Data mining techniques and, in particular, ensemble of classifiers permit to combine different classifiers that together provide complementary information and can be built in an incremental way. This paper introduces the architecture of a distributed intrusion detection framework and in particular, the detector module based on a meta-ensemble, which is used to cope with the problem of detecting intrusions, in which typically the number of attacks is minor than the number of normal connections. To this aim, we explore the usage of ensembles specialized to detect particular types of attack or normal connections, and Genetic Programming is adopted to generate a non-trainable function to combine each specialized ensemble. Non-trainable functions can be evolved without any extra phase of training and, therefore, they are particularly apt to handle concept drifts, also in the case of real-time constraints. Preliminary experiments, conducted on the well-known KDD dataset and on a more up-to-date dataset, ISCX IDS, show the effectiveness of the approach.",1,0,0,genetic programming+generic programming+concept drifts,
157,157,"Genetic algorithms a nature inspired tool review of applications in supply chain management. The use of genetic algorithm for supply chain management with its ability to evolve solutions, handle uncertainty, and perform optimization remains to be a leading field of study. The growing body of publications over the last two decades means that it can be difficult to keep track of what has been done previously, what has worked, and what really needs to be addressed. Hence, this paper presents a review of existing research activities inspired by the genetic algorithm application in supply chain management (SCM) aimed at presenting key research themes, trends, and directions of future research.",1,0,0,optimization+evolutionary programming+particle swarm optimization (pso),
158,158,"Estimation of arbitrary resident locations using data obtained from an infrared sensor array. The goal of the series of our studies is to build a resident-friendly smart home while protecting residents' privacy. According to our previous studies, an array of 15(5 x 3) binary infrared sensors can recognize some pre-defined locations of a resident in high accuracy. The purpose of this study is to investigate the possibility of estimating the coordinates of arbitrary locations, including those not defined during training. Estimation of the coordinate (x, y) is equivalent to regression of two functions x(D) and y(D), where D is a 15-dimensional vector observed by the sensor array. Experimental results illustrate that the estimation is relatively accurate for the y-axis. However, we could not obtain highly accurate results for the x-axis this time. We are going to examine the reasons and improve the performance in the next step.",1,0,0,sub-arrays+sensors+infra-red sensor,
159,159,"Convolutional neural network based video super resolution for action recognition. For video action recognition, convolutional neural networks (CNNs) especially two-stream CNNs have achieved remarkable progress in the recent years. However, most of the CNNs for action recognition are trained with high-resolution videos and not scale invariant, making it problematic to apply the trained CNNs directly on low-resolution videos. One possible solution to the problem is performing super-resolution (SR) prior to action recognition. In this paper, we investigate the effects of CNN-based video SR on the action recognition accuracy. We adopt a well trained two-stream CNN for action recognition, and analyze the spatial and temporal streams separately. For the spatial stream, we observe that video SR may improve the PSNR but may incur drop in recognition accuracy, this phenomenon is further analyzed in this paper. For the temporal stream, we observe that frame-by-frame SR may produce temporal inconsistency between consecutive video frames, which also incurs drop in recognition accuracy. We then propose a temporal consistency-oriented method for video SR, which indeed improves the recognition accuracy. Finally, we perform proper fusion of the two streams, and achieve a recognition accuracy of 88.95% on the UCF101 dataset when the input video is down-sampled by a factor of 4, compared to 93.49% accuracy on the original-resolution videos.",1,0,0,neural networks+convolutional neural networks+superresolution,
160,160,"Matrix factorization as search. Simplex Volume Maximization (SiVM) exploits distance geometry for efficiently factorizing gigantic matrices. It was proven successful in game, social media, and plant mining. Here, we review the distance geometry approach and argue that it generally suggests to factorize gigantic matrices using search-based instead of optimization techniques.",1,0,0,optimization+factorization+matrix factorizations,
161,161,"Quantifying urban canopy cover with deep convolutional neural networks. Urban canopy cover is important to mitigate the impact of climate change. Yet, existing quantification of urban greenery is either manual and not scalable, or use traditional computer vision methods that are inaccurate. We train deep convolutional neural networks (DCNNs) on datasets used for self-driving cars to estimate urban greenery instead, and find that our semantic segmentation and direct end-to-end estimation method are more accurate and scalable, reducing mean absolute error of estimating the Green View Index (GVI) metric from 10.1% to 4.67%. With the revised DCNN methods, the Treepedia project was able to scale and analyze canopy cover in 22 cities internationally, sparking interest and action in public policy and research fields.",1,0,0,deep learning+neural networks+convolutional neural networks,
162,162,"Supporting novice clinicians cognitive strategies system design perspective. Infections occur among all clinical domains. The changing nature of microbes, viruses and infections poses a great threat to the overall well-being in medicine. Clinicians in the infectious disease (ID) domain deal with diagnostic as well as treatment uncertainty in their everyday practice. Our current health information technology (HIT) systems do not consider the level of clinician expertise into the system design process. Thus, information is presented to both novice and expert ID clinicians in identical ways. The purpose of this study was to identify the cognitive strategies novice ID clinicians use in managing complex cases to make better recommendations for system design. In the process, we interviewed 14 ID experts and asked them to give us a detailed description of how novice clinicians would have dealt with complex cases. From the interview transcripts, we identified four major themes that expert clinicians suggested about novices' cognitive strategies including: A) dealing with uncertainty, B) lack of higher macrocognition, C) oversimplification of problems through heuristics and D) dealing with peer pressure. Current and future innovative decision support tools embedded in the electronic health record that can match these cognitive strategies may hold the key to cognitively supporting novice clinicians. The results of this study may open up avenues for future research and suggest design directions for better healthcare systems.",1,0,0,recommendation+health records+decision support systems,
163,163,"Bayesian recurrent neural network for language modeling. A language model (LM) is calculated as the probability of a word sequence that provides the solution to word prediction for a variety of information systems. A recurrent neural network (RNN) is powerful to learn the large-span dynamics of a word sequence in the continuous space. However, the training of the RNN-LM is an ill-posed problem because of too many parameters from a large dictionary size and a high-dimensional hidden layer. This paper presents a Bayesian approach to regularize the RNN-LM and apply it for continuous speech recognition. We aim to penalize the too complicated RNN-LM by compensating for the uncertainty of the estimated model parameters, which is represented by a Gaussian prior. The objective function in a Bayesian classification network is formed as the regularized cross-entropy error function. The regularized model is constructed not only by calculating the regularized parameters according to the maximum  a posteriori  criterion but also by estimating the Gaussian hyperparameter by maximizing the marginal likelihood. A rapid approximation to a Hessian matrix is developed to implement the Bayesian RNN-LM (BRNN-LM) by selecting a small set of salient outer-products. The proposed BRNN-LM achieves a sparser model than the RNN-LM. Experiments on different corpora show the robustness of system performance by applying the rapid BRNN-LM under different conditions.",1,0,0,recurrent neural networks+language model+hidden layers,
164,164,"Pseudo bayesian learning with kernel fourier transform as prior. We revisit Rahimi and Recht (2007)’s kernel random Fourier features (RFF) method through the lens of the PAC-Bayesian theory. While the primary goal of RFF is to approximate a kernel, we look at the Fourier transform as a prior distribution over trigonometric hypotheses. It naturally suggests learning a posterior on these hypotheses. We derive generalization bounds that are optimized by learning a pseudo-posterior obtained from a closed-form expression. Based on this study, we consider two learning strategies: The first one finds a compact landmarks-based representation of the data where each landmark is given by a distribution-tailored similarity measure, while the second one provides a PAC-Bayesian justification to the kernel alignment method of Sinha and Duchi (2016).",1,0,0,fourier transforms+bayesian methods+bayesian learning,
165,165,"Fcm type co clustering of categorical multivariate data with exclusive partition. An FCM-type co-clustering model was proposed for handling cooccurrence matrices, in which co-clusters of objects and items are extracted using two different types of fuzzy memberships. Objects are partitioned into clusters in a similar concept with the conventional FCM, which uses the exclusive condition forcing each object to be exclusively assigned. On the other hand, memberships of items represent only the relative typicality degree in each cluster, and cannot be used for determining the clusters, to which each item belongs. This paper proposes a new approach for deriving the exclusive partition not only of objects but also of items in the FCM-type co-clustering model. In order to avoid each item to belong to multiple clusters, an additional penalty term for evaluating the degree of sharing is introduced into the FCM-type objective function, in which the aggregation degree of each cluster is maximized by forcing all items to be exclusively assigned.",1,0,0,clustering methods+fuzzy c-means+fuzzy clustering algorithms,
166,166,"Stem at semeval 2016 task 4 applying active learning to improve sentiment classification. This paper describes our approach to the SemEval 2016 task 4, “Sentiment Analysis in Twitter”, where we participated in subtask A. Our system relies on AlchemyAPI and SentiWordNet to create 43 features based on which we select a feature subset as final representation. Active Learning then filters out noisy tweets from the provided training set, leaving a smaller set of only 900 tweets which we use
for training a Multinomial Naive Bayes classifier to predict the labels of the test set with an F1 score of 0.478.",1,0,0,test samples+sentiment classification+training sample,
167,167,"Step spatial temporal graph convolutional networks for emotion perception from gaits. We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network (ST-GCN) architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the perceived emotion of the human into one of four emotions: happy, sad, angry, or neutral. We train STEP on annotated real-world gait videos, augmented with annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset (E-Gait), which consists of 4,227 human gaits annotated with perceived emotions along with thousands of synthetic gaits. In practice, STEP can learn the affective features and exhibits classification accuracy of 88% on E-Gait, which is 14–30% more accurate over prior methods.",1,0,0,classifiers+variational auto encoders+regularization,
168,168,"Recipes for building voice search uis for automotive. In this paper we describe a set of techniques we found suitable for building multi-modal search applications for automotive environments. As these applications often search across different topical domains, such as maps, weather or Wikipedia, we discuss the problem of switching focus between different domains. Also, we propose techniques useful for minimizing the response time of the search system in mobile environment. We evaluate some of the proposed techniques by means of usability tests with 10 novice test subjects who drove a simulated lane change test on a driving simulator. We report results describing the induced driving distraction and user acceptance.",1,0,0,mobile users+mobile environments+search engines,
169,169,"Ivovo a new interval valued one vs one approach for multi class classification problems. Decomposition strategies have been shown to be a successful methodology to tackle multi-class classification problems. Among them, One-vs-One approach is a commonly used technique that consists in dividing the original multi-class problem into easier-to-solve binary sub-problems considering each possible pair of classes. However, this methodology is limited to those classifiers returning a single real value for each prediction. In this work, we present a new One-vs-One approach that is able to deal with interval-valued outputs. In order to achieve this goal, we propose applying a normalization method for intervals along with the corresponding extension of three different aggregation strategies: voting, weighted voting, and WinWV. The experimental results show the suitability of the normalization method and the improvement obtained by One-vs-One with respect to a state-of-the-art interval-valued Fuzzy Rule-Based Classification System (IVTURS).",1,0,0,multi-class classification+multi-class problems+multiclass classification problems,
170,170,"Construction of hybrid intelligent system of express diagnostics of information security attackers based on the synergy of several sciences and scientific directions. This paper is devoted to construction of a hybrid intelligent system of express-diagnostics of possible information security attackers (HIS DIVNAR) based on a synergy of several sciences and scientific directions: test pattern recognition; discrete mathematics; threshold and fuzzy logic; artificial intelligence; finite state machines (FSM) theory; reliability; theory of separating systems; theory of probability and mathematical statistics; and cognitive means. The proposed approach and basis of the mathematical apparatus are fragmentarily given for constructing HIS DIVNAR; that consists of four components: the first component, called IS DIOS, is designed for the express-diagnostics of organizational stress of the subject; the second (IS DIAPROD) is for the express-diagnostics and prevention of depression; the third (DIDEV) is for the expressdiagnostics and prevention of deviant behavior; and the fourth, intelligent system of express-diagnostics of information security attackers (IS DINARLOG2) is for making and justification of decisions with the use of cognitive means based on earlier revealed different regularities, including fault-tolerant irredundant unconditional diagnostic tests, fault-tolerant mixed diagnostic tests, regularities, and decision rules, which are built by using the applied IS DINARLOG1 constructed on the basis of intelligent instrumental software IMSLOG. Further development of this approach is proposed.",1,0,0,decision tables+probability+pattern recognition,
171,171,"Multi scale identity preserving image to image translation network for low resolution face recognition. State-of-the-art deep neural network models have reached near perfect face recognition accuracy rates on controlled high-resolution face images. However, their performance is drastically degraded when they are tested with very low-resolution face images. This is particularly critical in surveillance systems, where a low-resolution probe image is to be matched with high-resolution gallery images. super-resolution techniques aim at producing high-resolution face images from low-resolution counterparts. While they are capable of reconstructing images that are visually appealing, the identity-related information is not preserved. Here, we propose an identity-preserving end-to-end image-to-image translation deep neural network which is capable of super-resolving very low-resolution faces to their high-resolution counterparts while preserving identity-related information. We achieved this by training a very deep convolutional encoder-decoder network with a symmetric contracting path between corresponding layers. This network was trained with a combination of a reconstruction and an identity-preserving loss, on multi-scale low-resolution conditions. Extensive quantitative evaluations of our proposed model demonstrated that it outperforms competing super-resolution and low-resolution face recognition methods on natural and artificial low-resolution face data sets and even unseen identities.",1,0,0,face recognition systems+superresolution+image super-resolution,
172,172,"Collision detection for human robot interaction in an industrial setting using force myography and a deep learning approach. By applying robots while collaborating with a human in an industrial setting to provide more flexible and productive industries, safe interaction and collision detection have become an indispensable element of the collaborative robots. In such a dynamic environment, safe collaboration scenarios are needed to be designed using reliable methods to monitor collision-related signals and avoid a dangerous collision. Since human’s hand is the most exposed limb to collision during cooperation with a robot, new flexible methods should be conducted to use in industries by considering hand safety. In this study, collision monitoring is developed using force myography of a worker forearm and robot dynamic parameters. A method based on deep neural network is proposed to distinguish any occurrence of a collision between a worker’s hand and robot’s arm during the collaboration. The proposed approach can be applied to provide a reliable interaction with no unnecessary robot stop during working by classifying unintended collision. Various experiments have been conducted to evaluate the proposed method. The results show that the proposed scheme can successfully detect a collision and classify human intention to provide safe and reliable cooperation with a robot in an industrial environment.",1,0,0,deep learning+neural networks+robot dynamics,
173,173,"Pushing the limits of affine rank minimization by adapting probabilistic pca. Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameterfree probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for nonconvex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark.",1,0,0,optimal solutions+computer vision applications+nonconvex,
174,174,"Cultural communication idiosyncrasies in human computer interaction. Comunicacio presentada a: 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue; celebrada del 13 al 15 de setembre de 2016 a Los Angeles, USA",1,0,0,wireless communications+communication+dialogue,
175,175,"Radar based non intrusive fall motion recognition using deformable convolutional neural network. Radar is an attractive sensing technology for remote and non-intrusive human health monitoring and elderly fall detection due to its ability to work in low lighting conditions, its invariance to the environment, and its ability to operate through obstacles. Radar reflections from humans produce unique micro-Doppler signatures that can be used for classifying human activities and fall motion. However, radar-based elderly fall detection need to handle the indistinctive inter-class differences and large intra-class variations of human fall-motion in a real-world situation. Further, the radar placement in the room and varying aspect angle of the falling subject could result in differing radar micro-Doppler signature of human fall-motion. In this paper, we use a compact short-range 60-GHz frequency modulated continuous wave radar for detecting human fall motion using a novel deformable deep convolutional neural network with novel 1-class contrastive loss function in conjunction to focus loss to recognize elderly fall and address several of these signal processing system challenges. We demonstrate the performance of our proposed system in laboratory conditions under staged fall motion.",1,0,0,neural networks+convolutional neural networks+radar reflection+continuous wave radar,
176,176,"Analysis of thompson sampling for combinatorial multi armed bandit with probabilistically triggered arms. We analyze the regret of combinatorial Thompson sampling (CTS) for the combinatorial multi-armed bandit with probabilistically triggered arms under the semi-bandit feedback setting. We assume that the learner has access to an exact optimization oracle but does not know the expected base arm outcomes beforehand. When the expected reward function is Lipschitz continuous in the expected base arm outcomes, we derive $O(\sum_{i =1}^m \log T / (p_i \Delta_i))$ regret bound for CTS, where $m$ denotes the number of base arms, $p_i$ denotes the minimum non-zero triggering probability of base arm $i$ and $\Delta_i$ denotes the minimum suboptimality gap of base arm $i$. We also compare CTS with combinatorial upper confidence bound (CUCB) via numerical experiments on a cascading bandit problem.",1,0,0,optimization+reward function+numerical experiments,
177,177,"Whole word recognition from articulatory movements for silent speech interfaces. Articulation-based silent speech interfaces convert silently produced speech movements into audible words. These systems are still in their experimental stages, but have significant potential for facilitating oral communication in persons with laryngectomy or speech impairments. In this paper, we report the result of a novel, real-time algorithm that recognizes wholewords based on articulatory movements. This approach differs from prior work that has focused primarily on phoneme-level recognition based on articulatory features. On average, our algorithm missed 1.93 words in a sequence of twenty-five words with an average latency of 0.79 seconds for each word prediction using a data set of 5,500 isolated word samples collected from ten speakers. The results demonstrate the effectiveness of our approach and its potential for building a real-time articulationbased silent speech interface for health applications.",1,0,0,natural speech+speech signals+part of speech,
178,178,"Automatic generation of contrast sets from scene graphs probing the compositional consistency of gqa. Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models’ performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA’s compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models.",1,0,0,question answering+training sample+image representations,
179,179,"Efficient methods for overlapping group lasso. The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms.",1,0,0,optimization problems+optimization+multi-objective optimisation,
180,180,"Learning the stein discrepancy for training and evaluating energy based models without sampling. We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model's log-density. We estimate the Stein discrepancy between the data density $p(x)$ and the model density $q(x)$ defined by a vector function of the data. We parameterize this function with a neural network and fit its parameters to maximize the discrepancy. This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing $q(x)$ to minimize this discrepancy produces a novel method for training unnormalized models which scales more gracefully than existing methods. The ability to both learn and compare models is a unique feature of the proposed method.",1,0,0,stochastic processes+high dimensional data+neural networks,
181,181,"Corpus based augmented media posts with density based clustering for community detection. This paper proposes a corpus-based media posts expansion technique with a density-based clustering method for community detection. To enrich the user content information, firstly all (short-text) media posts of a user are combined with hash tags and URLs available with the posts. The expanded content view is further augmented by the virtual words inferred using the novel concept of matrix factorization based topic proportion vector approximation. This expansion technique deals with the extreme sparseness of short text data which otherwise leads to insufficient word co-occurrence and, in hence, inaccurate outcome. We then propose to group these augmented posts which represent users by identifying the density patches and form user communities. The remaining isolated users are then assigned to communities to which they are found most similar using a distance measure. Experimental results using several Twitter datasets show that the proposed approach is able to deal with common issues attached with (short-text) media posts to form meaningful communities and attain high accuracy compared to relevant benchmarking methods.",1,0,0,community detection+factorization+matrix factorizations,
182,182,"Multilinear compressive learning with prior knowledge. The recently proposed Multilinear Compressive Learning (MCL) framework combines Multilinear Compressive Sensing and Machine Learning into an end-to-end system that takes into account the multidimensional structure of the signals when designing the sensing and feature synthesis components. The key idea behind MCL is the assumption of the existence of a tensor subspace which can capture the essential features from the signal for the downstream learning task. Thus, the ability to find such a discriminative tensor subspace and optimize the system to project the signals onto that data manifold plays an important role in Multilinear Compressive Learning. In this paper, we propose a novel solution to address both of the aforementioned requirements, i.e., How to find those tensor subspaces in which the signals of interest are highly separable? and How to optimize the sensing and feature synthesis components to transform the original signals to the data manifold found in the first question? In our proposal, the discovery of a high-quality data manifold is conducted by training a nonlinear compressive learning system on the inference task. Its knowledge of the data manifold of interest is then progressively transferred to the MCL components via multi-stage supervised training with the supervisory information encoding how the compressed measurements, the synthesized features, and the predictions should be like. The proposed knowledge transfer algorithm also comes with a semi-supervised adaption that enables compressive learning models to utilize unlabeled data effectively. Extensive experiments demonstrate that the proposed knowledge transfer method can effectively train MCL models to compressively sense and synthesize better features for the learning tasks with improved performances, especially when the complexity of the learning task increases.",1,0,0,inference+probabilistic inference+data manifolds,
183,183,"Mulgan facial attribute editing by exemplar. Recent studies on face attribute editing by exemplars have achieved promising results due to the increasing power of deep convolutional networks and generative adversarial networks. These methods encode attribute-related information in images into the predefined region of the latent feature space by employing a pair of images with opposite attributes as input to train model, the face attribute transfer between the input image and the exemplar can be achieved by exchanging their attribute-related latent feature region. However, they suffer from three limitations: (1) the model must be trained using a pair of images with opposite attributes as input; (2) weak capability of editing multiple attributes by exemplars; (3) poor quality of generating image. Instead of imposing opposite-attribute constraints on the input image in order to make the attribute information of images be encoded in the predefined region of the latent feature space, in this work we directly apply the attribute labels constraint to the predefined region of the latent feature space. Meanwhile, an attribute classification loss is employed to make the model learn to extract the attribute-related information of images into the predefined latent feature region of the corresponding attribute, which enables our method to transfer multiple attributes of the exemplar simultaneously. Besides, a novel model structure is designed to enhance attribute transfer capabilities by exemplars while improve the quality of the generated image. Experiments demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA dataset.",1,0,0,latent factor+latent variable+generative adversarial networks,
184,184,"An efficient circle detection scheme in digital images using ant system algorithm. Detection of geometric features in digital images is an important exercise in image analysis and computer vision. The Hough Transform techniques for detection of circles require a huge memory space for data processing hence requiring a lot of time in computing the locations of the data space, writing to and searching through the memory space. In this paper we propose a novel and efficient scheme for detecting circles in edge-detected grayscale digital images. We use Ant-system algorithm for this purpose which has not yet found much application in this field. The main feature of this scheme is that it can detect both intersecting as well as nonintersecting circles with a time efficiency that makes it useful in real time applications. We build up an ant system of new type which finds out closed loops in the image and then tests them for circles. Key Words-circle detection, edge detection, ant system algorithm, gray scale image, image processing.",1,0,0,edge detection+circle detection+ant systems,
185,185,"Human derived heuristic enhancement of an evolutionary algorithm for the 2d bin packing problem. The 2D Bin-Packing Problem (2DBPP) is an NP-Hard combinatorial optimisation problem with many real-world analogues. Fully deterministic methods such as the well-known Best Fit and First Fit heuristics, stochastic methods such as Evolutionary Algorithms (EAs), and hybrid EAs that combine the deterministic and stochastic approaches have all been applied to the problem. Combining derived human expertise with a hybrid EA offers another potential approach. In this work, the moves of humans playing a gamified version of the 2DBPP were recorded and four different Human-Derived Heuristics (HDHs) were created by learning the underlying heuristics employed by those players. Each HDH used a decision tree in place of the mutation operator in the EA. To test their effectiveness, these were compared against hybrid EAs utilising Best Fit or First Fit heuristics as well as a standard EA using a random swap mutation modified with a Next Fit heuristic if the mutation was infeasible. The HDHs were shown to outperform the standard EA and were faster to converge than – but ultimately outperformed by – the First Fit and Best Fit heuristics. This shows that humans can create competitive heuristics through gameplay and helps to understand the role that heuristics can play in stochastic search.",1,0,0,stochastic approach+stochastic+evolutionary algorithms,
186,186,"Leveraging filter correlations for deep model compression. We present a filter correlation based model compression approach for deep convolutional neural networks. Our approach iteratively identifies pairs of filters with the largest pairwise correlations and drops one of the filters from each such pair. However, instead of discarding one of the filters from each such pair na\""{i}vely, the model is re-optimized to make the filters in these pairs maximally correlated, so that discarding one of the filters from the pair results in minimal information loss. Moreover, after discarding the filters in each round, we further finetune the model to recover from the potential small loss incurred by the compression. We evaluate our proposed approach using a comprehensive set of experiments and ablation studies. Our compression method yields state-of-the-art FLOPs compression rates on various benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving excellent predictive performance for tasks such as object detection on benchmark datasets.",1,0,0,neural networks+convolutional neural networks+object detection,
187,187,"Route planning for unmanned aerial vehicle uav on the sea using hybrid differential evolution and quantum behaved particle swarm optimization. This paper presents a hybrid differential evolution (DE) with quantum-behaved particle swarm optimization (QPSO) for the unmanned aerial vehicle (UAV) route planning on the sea. The proposed method, denoted as DEQPSO, combines the DE algorithm with the QPSO algorithm in an attempt to further enhance the performance of both algorithms. The route planning for UAV on the sea is formulated as an optimization problem. A simple method of pretreatment to the terrain environment is proposed. A novel route planner for UAV is designed to generate a safe and flyable path in the presence of different threat environments based on the DEQPSO algorithm. To show the high performance of the proposed method, the DEQPSO algorithm is compared with the real-valued genetic algorithm, DE, standard particle swarm optimization (PSO), hybrid particle swarm with differential evolution operator, and QPSO in terms of the solution quality, robustness, and the convergence property. Experimental results demonstrate that the proposed method is capable of generating higher quality paths efficiently for UAV than any other tested optimization algorithms.",1,0,0,particle swarm optimization (pso)+differential evolution+de algorithms,
188,188,"A survey on spoken language understanding recent advances and new frontiers. Spoken Language Understanding (SLU) aims to extract the semantics frame of user queries, which is a core component in a task-oriented dialog system. With the burst of deep neural networks and the evolution of pre-trained language models, the research of SLU has obtained significant breakthroughs. However, there remains a lack of a comprehensive survey summarizing existing approaches and recent trends, which motivated the work presented in this article. In this paper, we survey recent advances and new frontiers in SLU. Specifically, we give a thorough review of this research field, covering different aspects including (1) new taxonomy: we provide a new perspective for SLU filed, including single model vs. joint model, implicit joint modeling vs. explicit joint modeling in joint model, non pre-trained paradigm vs. pre-trained paradigm;(2) new frontiers: some emerging areas in complex SLU as well as the corresponding challenges; (3) abundant open-source resources: to help the community, we have collected, organized the related papers, baseline projects and leaderboard on a public website where SLU researchers could directly access to the recent progress. We hope that this survey can shed a light on future research in SLU field.",1,0,0,language model+spoken language understanding+phrase-based statistical machine translation,
189,189,"Automatic image annotation using semi supervised generative modeling. Image annotation approaches need an annotated dataset to learn a model for the relation between images and words. Unfortunately, preparing a labeled dataset is highly time consuming and expensive. In this work, we describe the development of an annotation system in semi-supervised learning framework which by incorporating unlabeled images into training phase reduces the system demand to labeled images. Our approach constructs a generative model for each semantic class in two main steps. First, based on Gamma distribution, a generative model is constructed for each semantic class using labeled images in that class. The second step incorporates the unlabeled images by using a modified EM algorithm to update parameters of the constructed generative models. Performance evaluation of the proposed method on a standard dataset reveals that using unlabeled images will result in considerable improvement in accuracy of the annotation systems when a limited number of labeled images for each semantic class are available. We propose a modified EM algorithm to incorporate unlabeled images in training phase.Grouping images using spectral clustering improves prototypes and models of concepts.For noisy annotated images, semi-supervised mixture model outperforms graph learning.Incorporating unlabeled images will improve annotation performance significantly.",1,0,0,spectral clustering+image classification+automatic image annotation,
190,190,"Tweester at semeval 2016 task 4 sentiment analysis in twitter using semantic affective model adaptation. We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the subtask B. Our system comprises of multiple independent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way. The novelty of the system is the employment of a topic modeling approach in order to adapt the semantic-affective space for each tweet. In addition, significant enhancements were made in the main system dealing with the data preprocessing and feature extraction including the employment of word embeddings. Each model is used to predict a tweet’s sentiment (positive, negative or neutral) and a late fusion scheme is adopted for the final decision.",1,0,0,sentiment analysis+neural networks+word embedding,
191,191,"Imitation learning for object manipulation based on position force information using bilateral control. This study proposes an imitation learning method based on force and position information. Force information is required for precise object manipulation but is difficult to obtain because the acting and reaction forces cannot be separated. To separate the forces, we proposed to introduce bilateral control, in which the acting and reaction forces are divided using two robots. In the proposed method, two models of neural networks learn a task; to draw a line along a ruler. We verify the possibility that force information is essential to imitate the human skill of object manipulation.",1,0,0,robots+neural networks+imitation learning,
192,192,"Corrlog correlated logistic models for joint prediction of multiple labels. In this paper, we present a simple but effective method for multi-label classification (MLC), termed Correlated Logistic Models (Corrlog), which extends multiple Independent Logistic Regressions (ILRs) by modeling the pairwise correlation between labels. Algorithmically, we propose an efficient method for learning parameters of Corrlog, which is based on regularized maximum pseudolikelihood estimation and has a linear computational complexity with respect to the number of labels. Theoretically, we show that Corrlog enjoys a satisfying generalization bound which is independent of the number of labels. The effectiveness of Corrlog on modeling label correlations is illustrated by a toy example, and further experiments on real data show that Corrlog achieves competitive performance compared with popular MLC algorithms.",1,0,0,logistic regression+logistics+variational approximation,
193,193,"A modified fast recursive hidden nodes selection algorithm for elm. Extreme Learning Machine (ELM) is a new paradigm for using Single-hidden Layer Feedforward Networks (SLFNs) with a much simpler training method. The input weights and the bias of the hidden layer are randomly chosen and output weights are analytically determined. One of the open problems in ELM research is how to automatically determine network architectures for given tasks. In this paper, it is taken as a model selection problem, a modified fast recursive algorithm (MFRA) is introduced to quickly and efficiently estimate the contribution of each hidden layer node to the decrease of the net function, and then a leave one out (LOO) cross validation is used to select the optimal number of hidden layer nodes. Simulation results on both artificial and real world benchmark datasets indicate the effectiveness of the proposed method.",1,0,0,extreme learning machine+hidden layers+feed-forward network,
194,194,"The computational rise and fall of fairness. The fair division of indivisible goods has long been an important topic in economics and, more recently, computer science. We investigate the existence of envyfree allocations of indivisible goods, that is, allocations where each player values her own allocated set of goods at least as highly as any other player's allocated set of goods. Under additive valuations, we show that even when the number of goods is larger than the number of agents by a linear fraction, envy-free allocations are unlikely to exist.We then show that when the number of goods is larger by a logarithmic factor, such allocations exist with high probability. We support these results experimentally and show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small. We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations, and that on average the computational problem is hardest at that transition.",1,0,0,purchase+purchasing+economics,
195,195,"Unsupervised deep metric learning via auxiliary rotation loss. Deep metric learning is an important area due to its applicability to many domains such as image retrieval and person re-identification. The main drawback of such models is the necessity for labeled data. In this work, we propose to generate pseudo-labels for deep metric learning directly from clustering assignment and we introduce unsupervised deep metric learning (UDML) regularized by a self-supervision (SS) task. In particular, we propose to regularize the training process by predicting image rotations. Our method (UDML-SS) jointly learns discriminative embeddings, unsupervised clustering assignments of the embeddings, as well as a self-supervised pretext task. UDML-SS iteratively cluster embeddings using traditional clustering algorithm (e.g., k-means), and sampling training pairs based on the cluster assignment for metric learning, while optimizing self-supervised pretext task in a multi-task fashion. The role of self-supervision is to stabilize the training process and encourages the model to learn meaningful feature representations that are not distorted due to unreliable clustering assignments. The proposed method performs well on standard benchmarks for metric learning, where it outperforms current state-of-the-art approaches by a large margin and it also shows competitive performance with various metric learning loss functions.",1,0,0,traditional clustering+unsupervised learning+image retrieval,
196,196,"Comparing and combining lexicase selection and novelty search. Lexicase selection and novelty search, two parent selection methods used in evolutionary computation, emphasize exploring widely in the search space more than traditional methods such as tournament selection. However, lexicase selection is not explicitly driven to select for novelty in the population, and novelty search suffers from lack of direction toward a goal, especially in unconstrained, highly-dimensional spaces. We combine the strengths of lexicase selection and novelty search by creating a novelty score for each test case, and adding those novelty scores to the normal error values used in lexicase selection. We use this new novelty-lexicase selection to solve automatic program synthesis problems, and find it significantly outperforms both novelty search and lexicase selection. Additionally, we find that novelty search has very little success in the problem domain of program synthesis. We explore the effects of each of these methods on population diversity and long-term problem solving performance, and give evidence to support the hypothesis that novelty-lexicase selection resists converging to local optima better than lexicase selection.",1,0,0,search process+evolutionary programming+evolutionary computation,
197,197,"Tbi contusion segmentation from mri using convolutional neural networks. Traumatic brain injury (TBI) is caused by a sudden trauma to the head that may result in hematomas and contusions and can lead to stroke or chronic disability. An accurate quantification of the lesion volumes and their locations is essential to understand the pathophysiology of TBI and its progression. In this paper, we propose a fully convolutional neural network (CNN) model to segment contusions and lesions from brain magnetic resonance (MR) images of patients with TBI. The CNN architecture proposed here was based on a state of the art CNN architecture from Google, called Inception. Using a 3-layer Inception network, lesions are segmented from multi-contrast MR images. When compared with two recent TBI lesion segmentation methods, one based on CNN (called DeepMedic) and another based on random forests, the proposed algorithm showed improved segmentation accuracy on images of 18 patients with mild to severe TBI. Using a leave-one-out cross validation, the proposed model achieved a median Dice of 0.75, which was significantly better (p<0.01) than the two competing methods.",1,0,0,neural networks+convolutional neural networks+image segmentation,
198,198,"Person recognition based on fusion of iris and periocular biometrics. In this paper we proposed a novel multimodal biometric approach using iris and periocular biometrics to improve the performance of iris recognition in case of non-ideal iris images. Though iris recognition has the highest accuracy among all the available biometrics, still the noises at the image acquisition stage degrade the recognition accuracy. The periocular region can act as a supporting biometric, in case the iris is obstructed by several noises. The periocular region is the part of the face immediately surrounding the eye. The approach is based on fusion of features of iris and periocular region. The approach has shown significant improvement in the performance of iris recognition. The evaluation was done on a test database created from the images of UBIRIS V2 and CASIA iris interval database. We achieved identification accuracy upto 96 % on the test database.",1,0,0,iris images+person recognition+iris recognition,
199,199,"Open domain short text conceptualization a generative descriptive modeling approach. Concepts embody the knowledge to facilitate our cognitive processes of learning. Mapping short texts to a large set of open domain concepts has gained many successful applications. In this paper, we unify the existing conceptualization methods from a Bayesian perspective, and discuss the three modeling approaches: descriptive, generative, and discriminative models. Motivated by the discussion of their advantages and shortcomings, we develop a generative + descriptive modeling approach. Our model considers term relatedness in the context, and will result in disambiguated conceptualization. We show the results of short text clustering using a news title data set and a Twitter message data set, and demonstrate the effectiveness of the developed approach compared with the state-of-the-art conceptualization and topic modeling approaches.",1,0,0,clustering algorithms+text clustering+bayesian methods,
200,200,"Towards lifestyle understanding predicting home and vacation locations from user s online photo collections. Semantic place labeling has been actively studied in the past few years due to its importance in understanding human mobility and lifestyle patterns. In the last decade, the rapid growth of geotagged multimedia data from online social networks provides a valuable opportunity to predict people's POI locations from temporal, spatial and visual cues. Among the massive amount of social media data, one important type of data is the geotagged web images from image-sharing websites. In this paper, we develop a reliable photo classifier based on the Convolutional Neutral Networks to classify the photo-taking scene of real-life photos. We then present a novel approach to home location and vacation locations prediction by fusing together the visual content of photos and the spatiotemporal features of people's mobility patterns. Using a well-trained classifier, we showed that the robust fusion of visual and spatiotemporal features achieves significant accuracy improvement over each of the features alone for both home and vacation detection.",1,0,0,network architecture+convolutional neural networks+web images,
201,201,"Active learning for visual question answering an empirical study. We present an empirical study of active learning for Visual Question Answering, where a deep VQA model selects informative question-image pairs from a pool and queries an oracle for answers to maximally improve its performance under a limited query budget. Drawing analogies from human learning, we explore cramming (entropy), curiosity-driven (expected model change), and goal-driven (expected error reduction) active learning approaches, and propose a fast and effective goal-driven active learning scoring function to pick question-image pairs for deep VQA models under the Bayesian Neural Network framework. We find that deep VQA models need large amounts of training data before they can start asking informative questions. But once they do, all three approaches outperform the random selection baseline and achieve significant query savings. For the scenario where the model is allowed to ask generic questions about images but is evaluated only on specific questions (e.g., questions whose answer is either yes or no), our proposed goal-driven scoring function performs the best.",1,0,0,bayesian methods+question answering+neural networks,
202,202,"A latent space based estimation of distribution algorithm for large scale global optimization. Large-scale global optimization problems (LSGOs) have received considerable attention in the field of meta-heuristic algorithms. Estimation of distribution algorithms (EDAs) are a major branch of meta-heuristic algorithms. However, how to effectively build the probabilistic model for EDA in high dimensions is confronted with obstacle, making them less attractive due to the large computational requirements. To overcome the shortcomings of EDAs, this paper proposed a latent space-based EDA (LS-EDA), which transforms the multivariate probabilistic model of Gaussian-based EDA into its principal component latent subspace with lower dimensionality. LS-EDA can efficiently reduce the complexity of EDA while maintaining its probability model without losing key information to scale up its performance for LSGOs. When the original dimensions are projected to the latent subspace, those dimensions with larger projected value make more contribution to the optimization process. LS-EDA can also help recognize and understand the problem structure, especially for black-box optimization problems. Due to dimensionality reduction, its computational budget and population size can be effectively reduced while its performance is highly competitive in comparison with the state-of-the-art meta-heuristic algorithms for LSGOs. In order to understand the strengths and weaknesses of LS-EDA, we have carried out extensive computational studies. Our results revealed LS-EDA outperforms the others on the benchmark functions with overlap and nonseparate variables.",1,0,0,global optimization problems+distribution algorithms+black-box optimization,
203,203,"Revisiting projection free optimization for strongly convex constraint sets. We revisit the Frank-Wolfe (FW) optimization under strongly convex constraint sets. We provide a faster convergence rate for FW without line search, showing that a previously overlooked variant of FW is indeed faster than the standard variant. With line search, we show that FW can converge to the global optimum, even for smooth functions that are not convex, but are quasi-convex and locally-Lipschitz. We also show that, for the general case of (smooth) non-convex functions, FW with line search converges with high probability to a stationary point at a rate of O(1/t), as long as the constraint set is strongly convex—one of the fastest convergence rates in non-convex optimization.",1,0,0,optimization+nonconvex+rate of convergence,
204,204,"Forward and reverse gradient based hyperparameter optimization. We study two procedures (reverse-mode and
forward-mode) for computing the gradient of the
validation error with respect to the hyperparameters
of any iterative learning algorithm such as
stochastic gradient descent. These procedures
mirror two methods of computing gradients for
recurrent neural networks and have different
trade-offs in terms of running time and space requirements.
Our formulation of the reverse-mode
procedure is linked to previous work by Maclaurin
et al. (2015) but does not require reversible
dynamics. The forward-mode procedure is suitable
for real-time hyperparameter updates, which
may significantly speed up hyperparameter optimization
on large datasets. We present experiments
on data cleaning and on learning task interactions.
We also present one large-scale experiment
where the use of previous gradient-based
methods would be prohibitive.",1,0,0,recurrent neural networks+regularization parameters+stochastic gradient descent,
205,205,"Feature learning for image classification via multiobjective genetic programming. Feature extraction is the first and most critical step in image classification. Most existing image classification methods use hand-crafted features, which are not adaptive for different image domains. In this paper, we develop an evolutionary learning methodology to automatically generate domain-adaptive global feature descriptors for image classification using multiobjective genetic programming (MOGP). In our architecture, a set of primitive 2-D operators are randomly combined to construct feature descriptors through the MOGP evolving and then evaluated by two objective fitness criteria, i.e., the classification error and the tree complexity. After the entire evolution procedure finishes, the best-so-far solution selected by the MOGP is regarded as the (near-)optimal feature descriptor obtained. To evaluate its performance, the proposed approach is systematically tested on the Caltech-101, the MIT urban and nature scene, the CMU PIE, and Jochen Triesch Static Hand Posture II data sets, respectively. Experimental results verify that our method significantly outperforms many state-of-the-art hand-designed features and two feature learning techniques in terms of classification accuracy.",1,0,0,supervised classification+local feature+image classification,
206,206,"Nash equilibrium based semantic cache in mobile sensor grid database systems. Mobile applications are being increasingly deployed on a massive scale in various mobile sensor grid database systems. With limited resources from the mobile devices, how to process the huge number of queries from mobile users with distributed sensor grid databases becomes a critical problem for such mobile systems. While the fundamental semantic cache technique has been investigated for query optimization in sensor grid database systems, the problem is still difficult due to the fact that more realistic multidimensional constraints have not been considered in existing methods. To solve the problem, a new semantic cache scheme is presented in this paper for location-dependent data queries in distributed sensor grid database systems. It considers multidimensional constraints or factors in a unified cost model architecture, determines the parameters of the cost model in the scheme by using the concept of Nash equilibrium from game theory, and makes semantic cache decisions from the established cost model. The scenarios of three factors of semantic, time, and locations are investigated as special cases, which improve existing methods. Experiments are conducted to demonstrate the semantic cache scheme presented in this paper for distributed sensor grid database systems.",1,0,0,game theory+optimization+nash equilibrium,
207,207,"Fine grained generalization analysis of vector valued learning. Many fundamental machine learning tasks can be formulated as a problem of learning with vector-valued functions, where we learn multiple scalar-valued functions together. Although there is some generalization analysis on different specific algorithms under the empirical risk minimization principle, a unifying analysis of vector-valued learning under a regularization framework is still lacking. In this paper, we initiate the generalization analysis of regularized vector-valued learning algorithms by presenting bounds with a mild dependency on the output dimension and a fast rate on the sample size. Our discussions relax the existing assumptions on the restrictive constraint of hypothesis spaces, smoothness of loss functions and low-noise condition. To understand the interaction between optimization and learning, we further use our results to derive the first generalization bounds for stochastic gradient descent with vector-valued functions. We apply our general results to multi-class classification and multi-label classification, which yield the first bounds with a logarithmic dependency on the output dimension for extreme multi-label classification with the Frobenius regularization. As a byproduct, we derive a Rademacher complexity bound for loss function classes defined in terms of a general strongly convex function.",1,0,0,multi-class classification+regularization+stochastic gradient descent,
208,208,"A binary real coded differential evolution for unit commitment problem a preliminary study. Due to its economical importance, the unit commitment problem has become a matter of concern in power systems, and consequently an important area of research. It is a nonlinear mixed-integer optimization problem, in which a given number of power generating units are to be scheduled in such a way that the forecasted demand is met at minimum production cost over a time horizon. In this paper a binary-real-coded differential evolution along with some repairing mechanisms is investigated as the solution technique of the problem. In the computational experiment carried out with a hypothetical 10-unit power system over 24-hour time horizon, available in the literature, the proposed technique is found outperforming all the existing methods.",1,0,0,optimization+particle swarm optimization (pso)+differential evolution,
209,209,"Localization uncertainty estimation for anchor free object detection. Since many safety-critical systems, such as surgical robots and autonomous driving cars, are in unstable environments with sensor noise and incomplete data, it is desirable for object detectors to take into account the confidence of localization prediction. There are three limitations of the prior uncertainty estimation methods for anchor-based object detection. 1) They model the uncertainty based on object properties having different characteristics, such as location (center point) and scale (width, height). 2) they model a box offset and ground-truth as Gaussian distribution and Dirac delta distribution, which leads to the model misspecification problem. Because the Dirac delta distribution is not exactly represented as Gaussian, i.e., for any $\mu$ and $\Sigma$. 3) Since anchor-based methods are sensitive to hyper-parameters of anchor, the localization uncertainty modeling is also sensitive to these parameters. Therefore, we propose a new localization uncertainty estimation method called Gaussian-FCOS for anchor-free object detection. Our method captures the uncertainty based on four directions of box offsets~(left, right, top, bottom) that have similar properties, which enables to capture which direction is uncertain and provide a quantitative value in range~[0, 1]. To this end, we design a new uncertainty loss, negative power log-likelihood loss, to measure uncertainty by weighting IoU to the likelihood loss, which alleviates the model misspecification problem. Experiments on COCO datasets demonstrate that our Gaussian-FCOS reduces false positives and finds more missing-objects by mitigating over-confidence scores with the estimated uncertainty. We hope Gaussian-FCOS serves as a crucial component for the reliability-required task.",1,0,0,gaussians+gaussian distribution+object detection,
210,210,"A state of the art review of biogeography based optimization. Biogeography-based optimization (BBO) is a population-based meta-heuristic evolutionary algorithm proposed by Simon (IEEE Trans Evol Comput 12(6):702–713, 2008 [1]). It is based on the theory of island biogeography which deals with the migration, speciation, and extinction of the species in a habitat. It has excellent exploitation ability but lacks in exploration. After the inception of BBO, a lot of modifications and hybridizations are introduced to enhance its performance. This paper focuses on various modifications and refinements in the migration and mutation operators of the original BBO and its hybridization with other population-based meta-heuristic algorithms. In the end, some open problems related to BBO are highlighted encouraging future research in this novel area.",1,0,0,evolutionary process+evolutionary algorithms+biogeography,
211,211,"Describing body pose feature poselet activity relationship using pachinko allocation model. Understanding video-based activities have remained the challenge regardless of efforts from the image processing and artificial intelligence community. However, the rapid developing of computer vision in 3D area has brought an opportunity for the human pose estimation and so far for the activity recognition. In this research, the authors suggest an impressive approach for understanding daily life activities in the indoor using the skeleton information collected from the Microsoft Kinect device. The approach comprises two significant components as the contribution: the pose-based feature extraction under the spatio-temporal relation and the topic model based learning. For extracting feature, the distance between two articulated points and the angle between horizontal axis and joint vector are measured and normalized on each detected body. A codebook is then constructed using the K-means algorithm to encode the merged set of distance and angle. For modeling activities from sparse features, a hierarchical model developed on the Pachinko Allocation Model is proposed to describe the flexible relationship between features - poselets - activities in the temporal dimension. Finally, the activities are classified by using three different state-of-the-art machine learning techniques: Support Vector Machine, K-Nearest Neighbor, and Random Forest. In the experiment, the proposed approach is benchmarked and compared with existing methods in the overall classification accuracy.",1,0,0,action recognition+human pose+human pose estimations,
212,212,"On using linear diophantine equations for efficient hiding of decision tree rules. Data sharing among organizations has become an increasingly common procedure in several areas like advertising, marketing, e-commerce and banking, but any organization will probably attempt to keep some patterns as hidden as possible when it shares its datasets with others. This paper focuses on preserving the privacy of sensitive patterns when inducing decision trees. We adopt a record augmentation approach for hiding sensitive classification rules in binary datasets. Such a hiding methodology is preferred over other heuristic solutions like output perturbation or crypto-graphic techniques - which restrict the usability of the data - since the raw data itself is readily available for public use. We propose a look-ahead approach using linear Diophantine equations in order to add the appropriate number of instances while maintaining the initial entropy of the nodes. This technique can be used to hide one or more decision tree rules in an optimal way.",1,0,0,decision tree induction+rule induction+classification rules,
213,213,"Exploiting chunk level features to improve phrase chunking. Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases.",1,0,0,parsing algorithm+training algorithms+sequence labeling,
214,214,"From truth degree comparison games to sequents of relations calculi for godel logic. We introduce a game for (extended) Godel logic where the players’ interaction stepwise reduces claims about the relative order of truth degrees of complex formulas to atomic truth comparison claims. Using the concept of disjunctive game states this semantic game is lifted to a provability game, where winning strategies correspond to proofs in a sequents-of-relations calculus.",1,0,0,semantics+sequent calculus+cut elimination,
215,215,"Dynamic texture analysis for detecting fake faces in video sequences. The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection.",1,0,0,classifiers+svm+texture descriptors,
216,216,"Ico2 multi user eco driving training environment based on distributed constraint optimization. Multi-agent systems have already been successfully applied to a variety of traffic control problems and demonstrated the potential to lower travel times and environmental impact. Sharing this goal, we have developed iCO2, an online tool for training eco-friendly driving in a multi-user three-dimensional environment. iCO2 supports eco-driving practice by instructing computer-controlled agents, such as traffic lights and other vehicles, to create traffic situations that make eco-driving more difficult. Hence the agents take the role of ""opponents"" that try to achieve the optimal challenge level for the skill level of each user. The research challenge is to find the optimal challenge level for all user drivers in a shared simulation space that (1) involves both controllable entities (""opponents"") and non-controllable entities (users) and (2) is highly dynamic, with dependencies between entities being created and destroyed in real time. We try to solve this problem by modeling the scenario as a distributed constraint optimization problem (DCOP). The main contribution of our paper is the application of a DCOP algorithm to such a new type of application scenario. We evaluate our approach by running scenarios both in terms of speed and optimality of the solutions proposed by the DCOP algorithm.",1,0,0,optimization+multi-agent+control problems,
217,217,"Asymptotically stable adaptive optimal control algorithm with saturating actuators and relaxed persistence of excitation. This paper proposes a control algorithm based on adaptive dynamic programming to solve the infinite-horizon optimal control problem for known deterministic nonlinear systems with saturating actuators and nonquadratic cost functionals. The algorithm is based on an actor/critic framework, where a critic neural network (NN) is used to learn the optimal cost, and an actor NN is used to learn the optimal control policy. The adaptive control nature of the algorithm requires a persistence of excitation condition to be  a priori  validated, but this can be relaxed using previously stored data concurrently with current data in the update of the critic NN. A robustifying control term is added to the controller to eliminate the effect of residual errors, leading to the asymptotically stability of the closed-loop system. Simulation results show the effectiveness of the proposed approach for a controlled Van der Pol oscillator and also for a power system plant.",1,0,0,optimal control problem+asymptotically stable+adaptive dynamic programming,
218,218,"A wireless surface electromyogram monitoring system using smartphone and its application to maintain biceps muscle. Recently, there are few monitoring systems using wireless technology for surface electromyogram (sEMG). Muscle fatigue detection is one of the important applications of sEMG monitoring during exercise in maintaining muscle. In this paper, a wireless sEMG system for monitoring and guiding people to maintain biceps muscle based on muscle fatigue is proposed. The android smartphone can record and send the sEMG signal to the Personal Computer (PC) using wireless technology via Hypertext Transfer Protocol (HTTP). Furthermore, a Hypertext Preprocessor (PHP) will invoke Mat Lab as PC's application. The sEMG signal is computed by Mat Lab to calculate and analyze muscle fatigue then send the result back to android smartphone to be displayed. In addition, the system can help to maintain biceps muscle until exercise complete. Detailed information about the design and implementation result is reported in this paper. The testing and implementation results have proven that the system is correct and feasible.",1,0,0,wireless communications+wireless+http,
219,219,"Transductive zero shot hashing for multilabel image retrieval. Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data, hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels, which we call unseen images, zero-shot hashing (ZSH) techniques have been studied for retrieval. However, existing ZSH methods mainly focus on the retrieval of single-label images and cannot handle multilabel ones. In this article, for the first time, a novel transductive ZSH method is proposed for multilabel unseen image retrieval. In order to predict the labels of the unseen/target data, a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then, pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multilabel data sets demonstrate that the proposed hashing method achieves significantly better results than the comparison methods.",1,0,0,image retrieval+class labels+nearest neighbor search,
220,220,"Active viscoelastic legged rimless wheel with upper body and its adaptability to irregular terrain. It was clarified that a rimless wheel with viscoelastic legs generates stable passive-dynamic gaits including measurable period of double-limb support motion. This paper then investigates the effect of the leg viscoelasticity on the adaptation ability to irregular terrain through numerical simulations and experiments. We introduce the model of an active viscoelastic-legged rimless wheel (VRW) that consists of eight identical viscoelastic legs and an upper body for analysis. We then develop the mathematical model and numerically examine the adaptation ability to irregular terrain. In this paper, we consider the two situations; one is overcoming steps and the other is sustaining stable walking on a flexible surface. The adaptability of the active VRW is compared with that of the rigid-legged model.",1,0,0,walking robots+quadruped robots+stable walking,
221,221,"Road lane landmark extraction a state of the art review. In this paper we present a state-of-the-art review about road lane landmark extraction. Automatic lane landmark extraction has been studied during the last decade for different practical applications. The purpose of this paper is to gather and discuss methodologies of road lane landmark extraction based on signals from different sensors in order to automate the extraction of horizontal road surface lane signs and get an accurate map of road lane landmarks. Specific algorithms for each kind of sensors are analyzed, describing their basic ideas, and discussing their pros and cons.",1,0,0,sensors+lane markings+information extraction,
222,222,"Offline sentence processing measures for testing readability with users. While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. In this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues. We find, most importantly, that the two correlate. Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. The sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise.",1,0,0,syntactics+syntactic structure+part of speech,
223,223,"Parallelizable sparse inverse formulation gaussian processes spingp. We propose a parallelizable sparse inverse formulation Gaussian process (SpInGP) for temporal models. It uses a sparse precision GP formulation and sparse matrix routines to speed up the computations. Due to the state-space formulation used in the algorithm, the time complexity of the basic SpInGP is linear, and because all the computations are parallelizable, the parallel form of the algorithm is sublinear in the number of data points. We provide example algorithms to implement the sparse matrix routines and experimentally test the method using both simulated and real data.",1,0,0,sparse approximations+gaussians+gaussian processes,
224,224,"The role of intention recognition in the evolution of cooperative behavior. Given its ubiquity, scale and complexity, few problems have created the combined interest of so many unrelated areas as the evolution of cooperation. Using the tools of evolutionary game theory, here we address, for the first time, the role played by intention recognition in the final outcome of cooperation in large populations of self-regarding individuals. By equipping individuals with the capacity of assessing intentions of others in the course of repeated Prisoner's Dilemma interactions, we show how intention recognition opens a window of opportunity for cooperation to thrive, as it precludes the invasion of pure cooperators by random drift while remaining robust against defective strategies. Intention recognizers are able to assign an intention to the action of their opponents based on an acquired corpus of possible intentions. We show how intention recognizers can prevail against most famous strategies of repeated dilemmas of cooperation, even in the presence of errors. Our approach invites the adoption of other classification and pattern recognition mechanisms common among Humans, to unveil the evolution of complex cognitive processes in the context of social dilemmas.",1,0,0,evolutionary game theory+evolutionary games+pattern recognition,
225,225,"Approximate linear programming for constrained partially observable markov decision processes. In many situations, it is desirable to optimize a sequence of decisions by maximizing a primary objective while respecting some constraints with respect to secondary objectives. Such problems can be naturally modeled as constrained partially observable Markov decision processes (CPOMDPs) when the environment is partially observable. In this work, we describe a technique based on approximate linear programming to optimize policies in CPOMDPs. The optimization is performed offline and produces a finite state controller with desirable performance guarantees. The approach outperforms a constrained version of point-based value iteration on a suite of benchmark problems.",1,0,0,optimization+value iteration+performance guarantees,
226,226,"Pairwise weak geometric consistency for large scale image search. State-of-the-art image search systems mostly build on bag-of-features (BOF) representation. As BOF ignores geometric relationships among local features, geometric consistency constraints have been proposed to improve search precision. However, exploiting full geometric constraints are too computational expensive. Weak geometric constraints have strong assumptions and can only deal with uniform transformations. To handle view point changes and nonrigid deformations, in this paper we present a novel pairwise weak geometric consistency constraint (P-WGC) method. It utilizes the local similarity characteristic of deformations, and measures the pairwise geometric similarity of matches between two sets of local features. Experiments performed on four famous datasets and a dataset of one million of images show a significant improvement due to P-WGC as well as its efficiency. Further improvement of search accuracy is obtained when it is combined with full geometric verification.",1,0,0,search engines+image search+local feature,
227,227,"Modelling temporal legal rules. Legal reasoning involves multiple temporal dimensions but the existing state of the art of legal representation languages does not allow us to easily combine expressiveness, performance and legal reasoning requirements. Moreover we also aim at the combination of legal temporal reasoning with the defeasible logic approach, maintaining a computable complexity. The contribution of this work is to extend LKIF-rules with temporal dimensions and defeasible tools, extending our previous work [17].",1,0,0,defeasible logic+defeasible reasoning+default logic,
228,228,"Predicting annotation difficulty to improve task routing and model performance for biomedical information extraction. Modern NLP systems require high-quality annotated data. For specialized domains, expert annotations may be prohibitively expensive; the alternative is to rely on crowdsourcing to reduce costs at the risk of introducing noise. In this paper we demonstrate that directly modeling instance difficulty can be used to improve model performance and to route instances to appropriate annotators. Our difficulty prediction model combines two learned representations: a ‘universal’ encoder trained on out of domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that: (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance; (ii) using difficulty scores to weight instances during training provides further, consistent gains; (iii) assigning instances predicted to be difficult to domain experts is an effective strategy for task routing. Further, our experiments confirm the expectation that for such domain-specific tasks expert annotations are of much higher quality and preferable to obtain if practical and that augmenting small amounts of expert data with a larger set of lay annotations leads to further improvements in model performance.",1,0,0,natural language processing+information extraction+biomedical literature,
229,229,"Graphcodebert pre training code representations with data flow. Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of ""where-the-value-comes-from"" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",1,0,0,semantics+syntactics+language model,
230,230,"Unsupervised feature selection by regularized self representation. By removing the irrelevant and redundant features, feature selection aims to find a compact representation of the original feature with good generalization ability. With the prevalence of unlabeled data, unsupervised feature selection has shown to be effective in alleviating the curse of dimensionality, and is essential for comprehensive analysis and understanding of myriads of unlabeled high dimensional data. Motivated by the success of low-rank representation in subspace clustering, we propose a regularized self-representation (RSR) model for unsupervised feature selection, where each feature can be represented as the linear combination of its relevant features. By using L 2 , 1 -norm to characterize the representation coefficient matrix and the representation residual matrix, RSR is effective to select representative features and ensure the robustness to outliers. If a feature is important, then it will participate in the representation of most of other features, leading to a significant row of representation coefficients, and vice versa. Experimental analysis on synthetic and real-world data demonstrates that the proposed method can effectively identify the representative features, outperforming many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy, redundancy reduction and classification accuracy. HighlightsA regularized self-representation (RSR) model is proposed for unsupervised feature selection.An iterative reweighted least-squares algorithm is proposed to solve the RSR model.The proposed method shows superior performance to state-of-the-art.",1,0,0,high dimensional data+clustering accuracy+unsupervised learning,
231,231,"Minimax rates for homology inference. Often, high dimensional data lie close to a low-dimensional submanifold and it is of interest to understand the geometry of these submanifolds. The homology groups of a manifold are important topological invariants that provide an algebraic summary of the manifold. These groups contain rich topological information, for instance, about the connected components, holes, tunnels and sometimes the dimension of the manifold. In this paper, we consider the statistical problem of estimating the homology of a manifold from noisy samples under several dierent noise models. We derive upper and lower bounds on the minimax risk for this problem. Our upper bounds are based on estimators which are constructed from a union of balls of appropriate radius around carefully selected points. In each case we establish complementary lower bounds using Le Cam’s lemma.",1,0,0,cam+high dimensional data+inference,
232,232,"Driving experience transfer method for end to end control of self driving cars. In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",1,0,0,network architecture+neural networks+convolutional neural networks,
233,233,"Scalable collaborative bayesian preference learning. Learning about users’ utilities from preference, discrete choice or implicit feedback data is of integral importance in e-commerce, targeted advertising and web search. Due to the sparsity and diffuse nature of data, Bayesian approaches hold much promise, yet most prior work does not scale up to realistic data sizes. We shed light on why inference for such settings is computationally difficult for standard machine learning methods, most of which focus on predicting explicit ratings only. To simplify the difficulty, we present a novel expectation maximization algorithm, driven by expectation propagation approximate inference, which scales to very large datasets without requiring strong factorization assumptions. Our utility model uses both latent bilinear collaborative filtering and non-parametric Gaussian process (GP) regression. In experiments on large real-world datasets, our method gives substantially better results than either matrix factorization or GPs in isolation, and converges significantly faster.",1,0,0,approximate inference+variational inference+expectation propagation,
234,234,"Music genre classification using dynamic selection of ensemble of classifiers. This paper presents a dynamic ensemble selection method for music genre classification which employs two pools of diverse classifiers. The pools of classifiers are created by using different features types extracted from three distinct segments of each music piece. From these initial pools of weak classifiers, ensembles of classifiers are dynamically selected for each test pattern using the k-nearest oracles method. The experiments compare the performance of different selection strategies on the Latin Music Database to those related to the use of best single classifier, and to the combination of all classifiers in the pool. It was possible to observe that the most promising selection strategy evaluated allows improving the classification accuracy from 63.71% to 70.31%.",1,0,0,ensemble classifiers+weak classifiers+music genre classification,
235,235,"Optimal voting in groups with convergent interests. Decision-making is crucially important at all levels of biological complexity, from within single-celled organisms, through neural populations within the vertebrate brain, to collections of social organisms such as colonies of ants and honeybees, or societies of humans. What are the prospects for unifying the study of these apparently disparate systems? All can be conceptualised as voting systems at the appropriate level. In this review I will argue that optimality theory can be of fundamental importance in understanding all these systems. In particular I will argue that for groups without conflict of interests, such as neurons and social insect colonies, similar mechanisms could implement statistically optimal decision-making in apparently highly different systems at different levels of biological complexity. I will consider what currency these systems should optimize, and speculate about the possible application of this understanding to the design of voting systems where individual group members' interests are aligned, such as in certain types of human group, and in collectives of robots. I will also consider how established results from economics and political science, notably Arrow's Impossibility Theorem and Condorcet’s ‘jury theorem’, might relate to what we know of social insect voting systems, where interesting effects such as the emergence of collective rationality from the voting of irrational individuals have recently been demonstrated.",1,0,0,economics+robots+foraging behaviors,
236,236,"Vehicle instance segmentation from aerial image and video using a multi task learning residual fully convolutional network. Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks (CNNs). In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel-level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multi-task learning network. More specifically, we utilize the philosophy of residual learning (ResNet) to construct a fully convolutional network that is capable of harnessing multi-level contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixel-wise segmentation tasks. Then, based on this network architecture, we propose a unified multi-task learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating closely spaced vehicles, which are usually not correctly separated into instances. Currently, datasets with pixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS DFC2015 dataset over Zeebrugge, which specializes in semantic segmentation. Therefore, we built a new, more challenging dataset for vehicle instance segmentation, called the Busy Parking Lot UAV Video dataset, and we make our dataset available at this http URL so that it can be used to benchmark future vehicle instance segmentation algorithms.",1,0,0,remote sensing images+object detection+high resolution remote sensing,
237,237,"Connecting community grids by supporting job negotiation with coevolutionary fuzzy systems. We utilize a competitive coevolutionary algorithm (CA) in order to optimize the parameter set of a Fuzzy-System for job negotiation between Community-Grids. In a Community-Grid, users are submitting jobs to their local High Performance Computing (HPC) sites over time. Now, we assume that Community-Grids are interconnected such that the exchange of jobs becomes possible: Each Community strives for minimizing the response time for their own members by trying to distribute workload to other communities in the Grid environment. For negotiation purpose, a Fuzzy-System is used to steer each site’s decisions whether to distribute or accept workload in a beneficial, yet egoistic direction. In such a system, it is essential that communities can only benefit if the workload is equitably (not necessarily equally) portioned among all participants. That is, if one community egoistically refuses to execute foreign jobs regularly, other HPC sites suffer from overloading. This, on the long run, deteriorates the opportunity to utilize them for job delegation. Thus, the egoistic community will degrade its own average performance. This scenario is particularly suited for the application of a competitive CA: the Fuzzy-Systems of the participating communities are modeled as species, which evolve in different populations while having to compete within the commonly shared ecosystem. Using real workload traces and Grid setups, we show that the opportunistic cooperation leads to significant improvements for both each community and the overall system.",1,0,0,computational grids+grid node+grid environments,
238,238,"Stabilization of nonlinear time delay systems with input saturation via anti windup fuzzy design. This investigation considers stability analysis and control design for nonlinear time-delay systems subject to input saturation. An anti-windup fuzzy control approach, based on fuzzy modeling of nonlinear systems, is developed to deal with the problems of stabilization of the closed-loop system and enlargement of the domain of attraction. To facilitate the designing work, the nonlinearity of saturation is first characterized by sector conditions, which provide a basis for analysis and synthesis of the anti-windup fuzzy control scheme. Then, the Lyapunov–Krasovskii delay-independent and delay-dependent functional approaches are applied to establish sufficient conditions that ensure convergence of all admissible initial states within the domain of attraction. These conditions are formulated as a convex optimization problem with constraints provided by a set of linear matrix inequalities. Finally, numeric examples are given to validate the proposed method.",1,0,0,delay-dependent+linear matrix+delay independent+nonlinear time-delay systems,
239,239,"Taskonomy disentangling task transfer learning. Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. 
We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.",1,0,0,data points+latent factor+latent variable,
240,240,"Outsourcing rework of imperfect items in the economic production quantity epq inventory model with backordered demand. This paper deals with an economic production quantity (EPQ) model with imperfect items and backordered demand. The manufacturing system is imperfect and generates a fraction of imperfect items, which is a random variable with a distribution function known. All manufactured items are screened and the imperfect items are identified. The rework of imperfect items cannot be done in the same manufacturing system. The company decides to outsource the rework in a repair store. Therefore, after the screen period all imperfect items are withdrawn and sent to the repair store; and at the end of the reparation process all the repaired items come back to the company and are received by the inventory. There are three cases that can occur depending on when the repaired items enter the inventory. Numerical examples and sensitivity analysis are presented to show the applicability of the three cases of the proposed inventory model.",1,0,0,probability density function+random variables+numerical example,
241,241,"Offline handwritten malayalam word recognition using a deep architecture. Handwriting recognition is an important application of pattern recognition subject. Although some research studies of handwriting recognition of a few major Indian scripts can be found in the literature, the same is not true for many of the Indian scripts. Malayalam is one such script, and automatic recognition issues of this script remain largely unexplored till date. On the other hand, there are nearly 40 million people mainly living in the southern part of India whose native language is Malayalam. In the present article, we present our recent study of Malayalam offline handwritten word recognition. The main contributions of the present study are (a) pioneering development of a database for offline handwritten word samples of Malayalam script and (b) its benchmark recognition results based on a transfer learning strategy which involves a deep convolutional neural network (CNN) architecture for feature extraction and a support vector machine (SVM) for classification. Recognition result of the proposed architecture on the writer independent test set of Malayalam handwritten word sample database is quite satisfactory. Moreover, the same architecture has been found to improve the existing state of the art of offline handwriting recognition of several major Indian scripts.",1,0,0,convolutional neural networks+handwritten words+handwritten word recognition,
242,242,"Information gathering actions over human internal state. Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planning actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver's lane to test their driving style. Results in simulation and in a user study suggest that active information gathering significantly outperforms passive state estimation.",1,0,0,robots+estimation method+human actions,
243,243,"A continuous time markov decision process based method with application in a pursuit evasion example. This paper presents a novel method—continuous-time Markov decision process (CTMDP)—to address the uncertainties in pursuit-evasion problem. The primary difference between the CTMDP and the Markov decision process (MDP) is that the former takes into account the influence of the transition time between the states. The policy iteration method-based potential performance for solving the CTMDP and its convergence are also presented. The results obtained by MDP-based method demonstrate that it is a special case of CTMDP-based method involving the identity transition rate matrix. To compare the methods, a well-known pursuit-evasion problem, involving two identical cars, is solved as a benchmark. The CTMDP-based method can provide a discretization solution that is close to the analytical solution obtained by the differential game method. Besides, it shows strong robustness against changes in the transition probability, as compared with the traditional MDP-based method. To the best of our knowledge, this is the first attempt to validate the influence of the transition time between the states in such a pursuit-evasion scenario, or in a similar application, solved by an MDP-related model. The CTMDP-based method offers a new approach to solving the pursuit-evasion problem and can be extended to similar optimization applications.",1,0,0,markov decision processes+optimization+policy iteration,
244,244,"Covsel a new approach for ensemble selection applied to symbolic regression problems. Ensemble methods combine the predictions of a set of models to reach a better prediction quality compared to a single model's prediction. The ensemble process consists of three steps: 1) the generation phase where the models are created, 2) the selection phase where a set of possible ensembles is composed and one is selected by a selection method, 3) the fusion phase where the individual models' predictions of the selected ensemble are combined to an ensemble's estimate. This paper proposes CovSel, a selection approach for regression problems that ranks ensembles based on the coverage of adequately estimated training points and selects the ensemble with the highest coverage to be used in the fusion phase. An ensemble covers a training point if at least one of its models produces an adequate prediction for this training point. The more training points are covered this way, the higher is the ensemble's coverage. The selection of the ""right"" ensemble has a large impact on the final prediction. Results for two symbolic regression problems show that CovSel improves the predictions for various state-of-the-art fusion methods for ensembles composed of independently evolved GP models and also beats approaches based on single GP models.",1,0,0,ensemble classifiers+ensemble learning+symbolic regression,
245,245,"Real time 6d object pose estimation on cpu. We propose a fast and accurate 6D object pose estimation from a RGB-D image. Our proposed method is template matching based and consists of three main technical components, PCOF-MOD (multimodal PCOF), balanced pose tree (BPT) and optimum memory rearrangement for a coarse-to-fine search. Our model templates on densely sampled viewpoints and PCOF-MOD which explicitly handles a certain range of 3D object pose improve the robustness against background clutters. BPT which is an efficient tree-based data structures for a large number of templates and template matching on rearranged feature maps where nearby features are linearly aligned accelerate the pose estimation. The experimental evaluation on tabletop and bin-picking dataset showed that our method achieved higher accuracy and faster speed in comparison with state-of-the-art techniques including recent CNN based approaches. Moreover, our model templates can be trained solely from 3D CAD in a few minutes and the pose estimation run in near real-time (23 fps) on CPU. These features are suitable for any real applications.",1,0,0,pose estimation+convolutional neural networks+object pose,
246,246,"Discriminative learning of bayesian networks via factorized conditional log likelihood. We propose an efficient and parameter-free scoring criterion, the factorized conditional log-likelihood (fCLL), for learning Bayesian network classifiers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efficient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classifiers. Results on a large suite of benchmark data sets from the UCI repository show that fCLL-trained classifiers achieve at least as good accuracy as the best compared classifiers, using significantly less computational resources.",1,0,0,bayesian methods+posterior probability+bayesian network structure,
247,247,"Ihelp an intelligent online helpdesk system. Due to the importance of high-quality customer service, many companies use intelligent helpdesk systems (e.g., case-based systems) to improve customer service quality. However, these systems face two challenges: 1) Case retrieval measures: most case-based systems use traditional keyword-matching-based ranking schemes for case retrieval and have difficulty to capture the semantic meanings of cases and 2) result representation: most case-based systems return a list of past cases ranked by their relevance to a new request, and customers have to go through the list and examine the cases one by one to identify their desired cases. To address these challenges, we develop iHelp, an intelligent online helpdesk system, to automatically find problem-solution patterns from the past customer-representative interactions. When a new customer request arrives, iHelp searches and ranks the past cases based on their semantic relevance to the request, groups the relevant cases into different clusters using a mixture language model and symmetric matrix factorization, and summarizes each case cluster to generate recommended solutions. Case and user studies have been conducted to show the full functionality and the effectiveness of iHelp.",1,0,0,factorization+matrix factorizations+language model,
248,248,"Human vocal tract growth a longitudinal study of the development of various anatomical structures. The growth of the head and neck and its components, including that of the vocal tract, is not homothetic but appears rather as an anamorphosis. The growth of various structures presents a phenomenon of heterochrony. Another important issue in vocal tract growth is sexual dimorphism. It was first claimed that sexual dimorphism appears at puberty, but a recent study has suggested that some prepubertal differences exist. To study these two phenomena, we used longitudinal radiographic data of sixty-eight typical subjects (966 radiographs, taken from 1 month to 25 years) and twelve fetuses (anatomical sections). In this study, we analyzed the growth curves and growth types of the hard and soft palate, the pharyngeal cavity and the estimated length of the whole vocal tract using non-linear mixed-effect models, in order to take advantage of our unique longitudinal dataset. Results indicate that most of the structures follow a neural/somatic growth type, while the pharyngeal cavity follows a more somatic growth type. As concerns sexual dimor-phism, no prepubertal differences were found, suggesting that the sexual dimorphism is likely to begin at puberty. These results have implications for the acoustics of speech production during development and should lead to improvements in vocal tract growth modeling.",1,0,0,acoustics+natural speech+speech signals,
249,249,"Interactive control of computational power in a model of the basal ganglia thalamocortical circuit by a supervised attractor based learning procedure. The attractor-based complexity of a Boolean neural network refers to its ability to discriminate among the possible input streams, by means of alternations between meaningful and spurious attractor dynamics. The higher the complexity, the greater the computational power of the network. The fine tuning of the interactivity – the network’s feedback output combined with the current input stream – can maintain a high degree of complexity within stable domains of the parameters’ space. In addition, the attractor-based complexity of the network is related to the degree of discrimination of specific input streams. We present a novel supervised attractor-based learning procedure aimed at achieving a maximal discriminability degree of a selected input stream. With a predefined target value of discriminability degree and in the absence of changes in the internal connectivity matrix of the network, the learning procedure updates solely the weights of the feedback projections. In a Boolean model of the basal ganglia-thalamocortical circuit, we show how the learning trajectories starting from different configurations can converge to final configurations associated with same high discriminability degree. We discuss the possibility that the limbic system may play the role of the interactive feedback to the network studied here.",1,0,0,boolean functions+network architecture+neural networks,
250,250,"Learning to navigate the energy landscape. In this paper, we present a novel and efficient architecture for addressing computer vision problems that use `Analysis by Synthesis'. Analysis by synthesis involves the minimization of the reconstruction error which is typically a non-convex function of the latent target variables. State-of-the-art methods adopt a hybrid scheme where discriminatively trained predictors like Random Forests or Convolutional Neural Networks are used to initialize local search algorithms. While these methods have been shown to produce promising results, they often get stuck in local optima. Our method goes beyond the conventional hybrid architecture by not only proposing multiple accurate initial solutions but by also defining a navigational structure over the solution space that can be used for extremely efficient gradient-free local search. We demonstrate the efficacy of our approach on the challenging problem of RGB Camera Relocalization. To make the RGB camera relocalization problem particularly challenging, we introduce a new dataset of 3D environments which are significantly larger than those found in other publicly-available datasets. Our experiments reveal that the proposed method is able to achieve state-of-the-art camera relocalization results. We also demonstrate the generalizability of our approach on Hand Pose Estimation and Image Retrieval tasks.",1,0,0,neural networks+convolutional neural networks+image retrieval,
251,251,"Incorporating named entity recognition into the speech transcription process. Named Entity Recognition (NER) from speech usually involves two sequential steps: transcribing the speech using Automatic Speech Recognition (ASR) and annotating the outputs of the ASR process using NER techniques. Recognizing named entities in automatic transcripts is difficult due to the presence of transcription errors and the absence of some important NER clues, such as capitalization and punctuation. In this paper, we describe a methodology for speech NER which consists of incorporating NER into the ASR process so that the ASR system generates transcripts annotated with named entities. The combination is achieved by adapting ASR language models and pre-annotating the pronunciation dictionary. We evaluate this method on Ester 2 corpus, and show significant improvements over traditional approaches.",1,0,0,speech recognition systems+language model+named entity recognition,
252,252,"Colshade for real world single objective constrained optimization problems. In this paper we present the COLSHADE algorithm for real parameter constrained optimization problems. COLSHADE evolved from the basic L-SHADE algorithm by introducing significant features such as adaptive Levy flights and dynamic tolerance (included in the constraint handling technique). Levy flights mainly perform the exploration phase in algorithms such as the Firefly algorithm and Cuckoo search; in COLSHADE, however, the goal of the Levy flights is to administer the selection pressure exerted over the population as to find the feasible region and keeping diversity. Thus, a new adaptive Levy flight mutation operator is introduced here and called the levy/1/bin. In many problems the levy/1/bin excels during the exploration phase whilst the exploitation phase is performed by current-to-pbest mutation. However, the adaptive strategy propitiates the emergence of these two mutation approaches at different rates and times. The proposed method is tested on 57 constrained optimization functions of the benchmark provided for the CEC 2020 realworld single-objective constrained optimization competition.",1,0,0,single objective+constrained optimization problems+feasible regions,
253,253,"Rabin mostowski index problem a step beyond deterministic automata. For a given regular language of infinite trees, one can ask about the minimal number of priorities needed to recognise this language with a non-deterministic or alternating parity automaton. These questions are known as, respectively, the non-deterministic and the alternating Rabin-Mostowski index problems. Whether they can be answered effectively is a long-standing open problem, solved so far only for languages recognisable by deterministic automata (the alternating variant trivialises). We investigate a wider class of regular languages, recognisable by so-called game automata, which can be seen as the closure of deterministic ones under complementation and composition. Game automata are known to recognise languages arbitrarily high in the alternating Rabin-Mostowski index hierarchy, i.e., the alternating index problem does not trivialise any more. Our main contribution is that both index problems are decidable for languages recognisable by game automata. Additionally, we show that it is decidable whether a given regular language can be recognised by a game automaton.",1,0,0,automata+regular languages+deterministic,
254,254,"Sparse text generation. Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: $\epsilon$-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.",1,0,0,n-gram language models+language model+statistical language models,
255,255,"Energy efficient scheduling in multiprocessor systems using archived multi objective simulated annealing. In this paper, we have proposed an archived simulated annealing based novel approach for solving multi-objective energy-efficient scheduling on heterogeneous DVS activated processors in high-performance real-time systems. Real-time task scheduling problem is a well-known NP-hard problem. In these systems, tasks are usually associated with deadlines and represented by directed acyclic graphs since they depend on each other. So, system designers face difficulty in finding suitable solutions that can satisfy all the objectives of task scheduling, as warranted for proficient operations of such systems. Hence, this paper introduces a novel algorithm, called archived multi-objective simulated annealing for energy-efficient real-time scheduling (AMOSA-E2RTS) that finds an optimal schedule satisfying the precedence and deadline constraints. In the proposed algorithm, a domination concept leads towards finding the optimal trade-off solutions and tasks are prioritized according to three different policies i.e., latest deadline first (LDF), execution ranking and energy ranking policy. A suitable numerical example is used to demonstrate the working of the proposed approach. Experimental findings suggest that the proposed algorithm is capable of producing energy efficient scheduling decisions which satisfy all related constraints. Statistical analysis of the results has been conducted.",1,0,0,directed acyclic graphs+task scheduling problem+numerical example,
256,256,"Evolved decision trees as conformal predictors. In conformal prediction, predictive models output sets of predictions with a bound on the error rate. In classification, this translates to that the probability of excluding the correct class is lower than a predefined significance level, in the long run. Since the error rate is guaranteed, the most important criterion for conformal predictors is efficiency. Efficient conformal predictors minimize the number of elements in the output prediction sets, thus producing more informative predictions. This paper presents one of the first comprehensive studies where evolutionary algorithms are used to build conformal predictors. More specifically, decision trees evolved using genetic programming are evaluated as conformal predictors. In the experiments, the evolved trees are compared to decision trees induced using standard machine learning techniques on 33 publicly available benchmark data sets, with regard to predictive performance and efficiency. The results show that the evolved trees are generally more accurate, and the corresponding conformal predictors more efficient, than their induced counterparts. One important result is that the probability estimates of decision trees when used as conformal predictors should be smoothed, here using the Laplace correction. Finally, using the more discriminating Brier score instead of accuracy as the optimization criterion produced the most efficient conformal predictions.",1,0,0,genetic programming+generic programming+evolutionary algorithms,
257,257,"A linear multivariate binary decision tree classifier based on k means splitting. Abstract A novel linear multivariate decision tree classifier, Binary Decision Tree based on K-means Splitting (BDTKS), is presented in this paper. The unsupervised K-means clustering is recursively integrated into the binary tree, building a hierarchical classifier. The introduction of the unsupervised K-means clustering provides the powerful generalization ability for the resulting BDTKS model. Then, the good generalization ability of BDTKS ensures the classification performance. A novel non-split condition with an easy-setting hyperparameter which focuses more on minority classes of the current node is proposed and applied in the BDTKS model, avoiding ignoring the minority classes in the class imbalance cases. Furthermore, the K-means centroid based BDTKS model is converted into the hyperplane based decision tree, speeding up the process of classification. Extensive experiments on the publicly available data sets have demonstrated that the proposed BDTKS matches or outperforms the previous decision trees.",1,0,0,k-nn classifier+classification models+classification methods,
258,258,"Efficient anomaly detection using bipartite k nn graphs. Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1O-kNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.",1,0,0,anomaly detection methods+k-nearest neighbors+bipartite graphs,
259,259,"Automatic face image quality prediction. Face image quality can be defined as a measure of the utility of a face image to automatic face recognition. In this work, we propose (and compare) two methods for automatic face image quality based on target face quality values from (i) human assessments of face image quality (matcher-independent), and (ii) quality values computed from similarity scores (matcher-dependent). A support vector regression model trained on face features extracted using a deep convolutional neural network (ConvNet) is used to predict the quality of a face image. The proposed methods are evaluated on two unconstrained face image databases, LFW and IJB-A, which both contain facial variations with multiple quality factors. Evaluation of the proposed automatic face image quality measures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two face matchers (a COTS matcher and a ConvNet matcher) by using the proposed face quality to select subsets of face images and video frames for matching templates (i.e., multiple faces per subject) in the IJB-A protocol. To our knowledge, this is the first work to utilize human assessments of face image quality in designing a predictor of unconstrained face quality that is shown to be effective in cross-database evaluation.",1,0,0,neural networks+convolutional neural networks+face recognition,
260,260,"Mlne multi label network embedding. Network embedding aims to preserve topological structures of a network using low-dimensional vectors and has shown to be effective for driving a myriad of graph mining tasks (e.g., link prediction or classification) free of the stressful feature extraction procedure. Many methods have been proposed to integrate node content and/or label information, with nodes sharing similar content/labels being close to each other in the learned latent space. To date, existing methods either consider networked instances with a single label or consider a set of labels as a whole for node representation learning. Therefore, they cannot handle network of instances containing multiple labels (i.e. multi-labels), which are ubiquitous in describing complex concepts of instances. In this article, we formulate a new multi-label network embedding (MLNE) problem to learn feature representation for networked multi-label instances. We argue that the key to MLNE learning is to aggregate node topology structures, node content, and multi-label correlations. We propose a two-layer network embedding framework to couple information for effective learning. To capture higher order label correlations, we use labels to form a high-level label–label network over a low-level node–node network, in which the label network interacts with the node network through multi-labeling relations. The low-level node–node network can be enhanced by latent label-specific features from high-level label network with well-captured high-order correlations between labels. To enable the multi-label informed network embedding, we force both node and label representations being optimized under the same low-dimensional latent space by a unified training objective. Experiments on real-world data sets demonstrate that MLNE achieves better performance compared with methods with or without considering label information.",1,0,0,latent variable+class labels+link prediction,
261,261,"Leveraging deep visual features for content based movie recommender systems. The movie domain is one of the most common scenarios to test and evaluate recommender systems. These systems are often implemented through a collaborative filtering model, which relies exclusively on the user's feedback on items, ignoring content features. Content-based filtering models are nevertheless a potentially good strategy for recommendation, even though identifying relevant semantic representation of items is not a trivial task. Several techniques have been employed to continuously improve the content representation of items in content-based recommender systems, including low-level and high-level features, text analysis, and social tags. Recent advances on deep learning, particularly on convolutional neural networks, are paving the way for better representations to be extracted from unstructured data. In this work, our main goal is to understand whether these networks can extract sufficient semantic representation from items so we can better recommend movies in content-based recommender systems. For that, we propose DeepRecVis, a novel approach that represents items through features extracted from keyframes of the movie trailers, leveraging these features in a content-based recommender system. Experiments shows that our proposed approach outperforms systems that are based on low-level feature representations.",1,0,0,deep learning+neural networks+convolutional neural networks,
262,262,"Sparse low rank fusion based deep features for missing modality face recognition. Multi-modality data recently attract more and more research attention. In this paper, we concentrate on a very interesting problem — image classification with missing modality. Specifically, only images in one modality as well as a relevant auxiliary database are accessible during the training phase, which is significantly different from general image classification under the same modality. To this end, we propose a novel framework integrating multiple deep autoencoders with bagging strategy. For each autoencoder, we generate its input by randomly sampling data from other modality and the auxiliary database, and enforce its output to lie in a common feature space through Robust PCA. Finally, a novel sparse low-rank feature fusion approach is proposed in the test phase to integrate multiple features learned from different autoencoders, followed by a decision voting. Extensive experiments on two databases, i.e., BUAA-NIRVIS, Oulu-CASIA NIRVIS databases demonstrate the effectiveness of the proposed framework when there is only one modality available for training.",1,0,0,face recognition+image classification+auto encoders,
263,263,"Indefinite kernel spectral learning. Abstract The use of indefinite kernels has attracted many research interests in recent years due to their flexibility. They do not possess the usual restrictions of being positive definite as in the traditional study of kernel methods. This paper introduces the indefinite unsupervised and semi-supervised learning in the framework of least squares support vector machines (LS-SVM). The analysis is provided for both unsupervised and semi-supervised models, i.e., Kernel Spectral Clustering (KSC) and Multi-Class Semi-Supervised Kernel Spectral Clustering (MSS-KSC). In indefinite KSC models one solves an eigenvalue problem whereas indefinite MSS-KSC finds the solution by solving a linear system of equations. For the proposed indefinite models, we give the feature space interpretation, which is theoretically important, especially for the scalability using Nystrom approximation. Experimental results on several real-life datasets are given to illustrate the efficiency of the proposed indefinite kernel spectral learning.",1,0,0,unsupervised learning+spectral clustering+kernel method,
264,264,"Inferring binary relation schemas for open information extraction. This paper presents a framework to model the semantic representation of binary relations produced by open information extraction systems. For each binary relation, we infer a set of preferred types on the two arguments simultaneously, and generate a ranked list of type pairs which we call schemas. All inferred types are drawn from the Freebase type taxonomy, which are human readable. Our system collects 171,168 binary relations from ReVerb, and is able to produce top-ranking relation schemas with a mean reciprocal rank of 0.337.",1,0,0,galois connection+information extraction+information extraction systems,
265,265,"Learnable subspace clustering. This article studies the large-scale subspace clustering (LS²C) problem with millions of data points. Many popular subspace clustering methods cannot directly handle the LS²C problem although they have been considered to be state-of-the-art methods for small-scale data points. A simple reason is that these methods often choose all data points as a large dictionary to build huge coding models, which results in high time and space complexity. In this article, we develop a learnable subspace clustering paradigm to efficiently solve the LS²C problem. The key concept is to learn a parametric function to partition the high-dimensional subspaces into their underlying low-dimensional subspaces instead of the computationally demanding classical coding models. Moreover, we propose a unified, robust, predictive coding machine (RPCM) to learn the parametric function, which can be solved by an alternating minimization algorithm. Besides, we provide a bounded contraction analysis of the parametric function. To the best of our knowledge, this article is the first work to efficiently cluster millions of data points among the subspace clustering methods. Experiments on million-scale data sets verify that our paradigm outperforms the related state-of-the-art methods in both efficiency and effectiveness.",1,0,0,data points+clustering methods+hierarchical clustering algorithms,
266,266,"Reference point specification in hypervolume calculation for fair comparison and efficient search. Hypervolume has been frequently used as a performance indicator for comparing evolutionary multiobjective optimization (EMO) algorithms. Hypervolume has been also used in indicator-based algorithms. Whereas a reference point is needed for hypervolume calculation, its specification has not been discussed in detail from a viewpoint of fair comparison. This may be because a slightly worse reference point than the nadir point seems to work well. In this paper, we tackle this issue: How to specify a reference point for fair comparison. First we discuss an appropriate specification of a reference point for multiobjective problems. Our discussions are based on the well-known theoretical results about the optimal solution distribution for hypervolume maximization. Next we examine various specifications by computational experiments. Experimental results show that a slightly worse reference point than the nadir point works well only for test problems with triangular Pareto fronts. Then we explain why this specification is not always appropriate for test problems with inverted triangular Pareto fronts. We also report a number of solution sets obtained by SMS-EMOA with various specifications of a reference point.",1,0,0,multi-objective evolutionary algorithms+hypervolume+evolutionary multiobjective optimization,
267,267,"Perceptions of domestic robots normative behavior across cultures. As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.",1,0,0,robot system+robots+mobile robots,
268,268,"Computational representation of medical concepts a semiotic and fuzzy logic approach. Medicine and biology are among the fastest growing application areas of computer-based systems. Nonetheless, the creation of a computerized support for the health systems presents manifold challenges. One of the major problems is the modeling and interpretation of heterogeneous concepts used in medicine. The medical concepts such as, for example, specific symptoms and their etiologies, are described using terms from diverse domains - some concepts are described in terms of molecular biology and genetics, some concepts use models from chemistry and physics; yet some, for example, mental disorders, are defined in terms of particular feelings, behaviours, habits, and life events. Moreover, the computational representation of medical concepts must be (1) formally or rigorously specified to be processed by a computer, (2) human-readable to be validated by humans, and (3) sufficiently expressive to model concepts which are inherently complex, multi-dimensional, goal-oriented, and, at the same time, evolving and often imprecise. In this chapter, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on semiotics and fuzzy logic to explicitly model two important characteristics of medical concepts: changeability and imprecision. Furthermore, the framework uses a multi-layered specification linking together three domains: medical, computational, and implementational. We describe the framework using an example of mental disorders, specifically, the concept of clinical depression. To exemplify the changeable character of medical concepts, we discuss the evolution of the diagnostic criteria for depression. We discuss the computational representation for polythetic and categorical concepts and for multi-dimensional and noncategorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the contextual interpretation and dimensional nature of the symptoms.",1,0,0,formal framework+layered+fuzzy logic controllers,
269,269,"Enabling incremental knowledge transfer for object detection at the edge. Object detection using deep neural networks (DNNs) involves a huge amount of computation which impedes its implementation on resource/energy-limited user-end devices. The reason for the success of DNNs is due to having knowledge over all different domains of observed environments. However, we need a limited knowledge of the observed environment at inference time which can be learned using a shallow neural network (SHNN). In this paper, a system-level design is proposed to improve the energy consumption of object detection on the user-end device. An SHNN is deployed on the user-end device to detect objects in the observing environment. Also, a knowledge transfer mechanism is implemented to update the SHNN model using the DNN knowledge when there is a change in the object domain. DNN knowledge can be obtained from a powerful edge device connected to the user-end device through LAN or Wi-Fi. Experiments demonstrate that the energy consumption of the user-end device and the inference time can be improved by 78% and 71% compared with running the deep model on the user-end device.",1,0,0,neural networks+feedforward neural networks+object detection,
270,270,"Frame semantic tree kernels for social network extraction from text. In this paper, we present work on extracting social networks from unstructured text. We introduce novel features derived from semantic annotations based on FrameNet. We also introduce novel semantic tree kernels that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system.",1,0,0,semantics+semantic information+syntactics,
271,271,"Competence based multimodal curriculum learning for medical report generation. Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model’s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.",1,0,0,online learning+color images+medical images,
272,272,"Replicate walk and stop on syntax an effective neural network model for aspect level sentiment classification. Aspect-level sentiment classification (ALSC) aims at predicting the sentiment polarity of a specific aspect term occurring in a sentence. This task requires learning a representation by aggregating the relevant contextual features concerning the aspect term. Existing methods cannot sufficiently leverage the syntactic structure of the sentence, and hence are difficult to distinguish different sentiments for multiple aspects in a sentence. We perceive the limitations of the previous methods and propose a hypothesis about finding crucial contextual information with the help of syntactic structure. For this purpose, we present a neural network model named RepWalk which performs a replicated random walk on a syntax graph, to effectively focus on the informative contextual words. Empirical studies show that our model outperforms recent models on most of the benchmark datasets for the ALSC task. The results suggest that our method for incorporating syntactic structure enriches the representation for the classification.",1,0,0,syntactic structure+sentiment classification+neural networks,
273,273,"Legal judgment prediction via topological learning. Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the legal field. In real-world scenarios, legal judgment usually consists of multiple subtasks, such as the decisions of applicable law articles, charges, fines, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TopJudge, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several real-world large-scale datasets of criminal cases in the civil law system. Experimental results show that our model achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from https://github.com/thunlp/TopJudge.",1,0,0,artificial intelligence+directed graphs+directed acyclic graphs,
274,274,"Gans can play lottery tickets too. Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator plays a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",1,0,0,sparse approximations+gan+generative adversarial networks,
275,275,"Training deep learning based denoisers without ground truth data. Recent deep learning based denoisers are trained to minimize the mean squared error (MSE) between the output of a network and the ground truth noiseless image in the training data. Thus, it is crucial to have high quality noiseless training data for high performance denoisers. Unfortunately, in some application areas such as medical imaging, it is expensive or even infeasible to acquire such a clean ground truth image. We propose a Stein's Unbiased Risk Estimator (SURE) based method for training deep learning based denoisers without ground truth data. We demonstrated that our SURE based method only with noisy input data was able to train CNN based denoising networks that yielded performance close to that of the original MSE based deep learning denoisers with ground truth data.",1,0,0,mean square error+deep learning+convolutional neural networks,
276,276,"A robust bayesian truth serum for small populations. Peer prediction mechanisms allow the truthful elicitation of private signals (e.g., experiences, or opinions) in regard to a true world state when this ground truth is unobservable. The original peer prediction method is incentive compatible for any number of agents n ≥ 2, but relies on a common prior, shared by all agents and the mechanism. The Bayesian Truth Serum (BTS) relaxes this assumption. While BTS still assumes that agents share a common prior, this prior need not be known to the mechanism. However, BTS is only incentive compatible for a large enough number of agents, and the particular number of agents required is uncertain because it depends on this private prior. In this paper, we present a robust BTS for the elicitation of binary information which is incentive compatible for every n ≥ 3, taking advantage of a particularity of the quadratic scoring rule. The robust BTS is the first peer prediction mechanism to provide strict incentive compatibility for every n ≥ 3 without relying on knowledge of the common prior. Moreover, and in contrast to the original BTS, our mechanism is numerically robust and ex post individually rational.",1,0,0,bayesian methods+incentive compatible+incentive compatibility,
277,277,"Learning skeleton representations for human action recognition. Abstract Automatic interpretation of human actions gained strong interest among researchers in patter recognition and computer vision because of its wide range of applications, such as in social and home robotics, elderly people health care, surveillance, among others. In this paper, we propose a method for recognition of human actions by analysis of skeleton poses. The method that we propose is based on novel trainable feature extractors, which can learn the representation of prototype skeleton examples and can be employed to recognize skeleton poses of interest. We combine the proposed feature extractors with an approach for classification of pose sequences based on string kernels. We carried out experiments on three benchmark data sets (MIVIA-S, MSRSDA and MHAD) and the results that we achieved are comparable or higher than the ones obtained by other existing methods. A further important contribution of this work is the MIVIA-S dataset, that we collected and made publicly available.",1,0,0,pattern recognition+action recognition+human-action recognition,
278,278,"Diachronic word embeddings and semantic shifts a survey. Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.",1,0,0,lexical semantics+syntactic parsing+word embedding,
279,279,"Forecasting hands and objects in future frames. This paper presents an approach to forecast future presence and location of human hands and objects. Given an image frame, the goal is to predict what objects will appear in the future frame (e.g., 5 seconds later) and where they will be located at, even when they are not visible in the current frame. The key idea is that (1) an intermediate representation of a convolutional object recognition model abstracts scene information in its frame and that (2) we can predict (i.e., regress) such representations corresponding to the future frames based on that of the current frame. We design a new two-stream convolutional neural network (CNN) architecture for videos by extending the state-of-the-art convolutional object detection network, and present a new fully convolutional regression network for predicting future scene representations. Our experiments confirm that combining the regressed future representation with our detection network allows reliable estimation of future hands and objects in videos. We obtain much higher accuracy compared to the state-of-the-art future object presence forecast method on a public dataset.",1,0,0,neural networks+convolutional neural networks+object detection,
280,280,"A simulation optimization approach for design space exploration of soft real time embedded systems. In this work, the problem of design space exploration of soft real-time embedded systems is formulated as a stochastic simulation optimization problem. A novel multiobjective genetic algorithm is proposed to address this problem. In the proposed algorithm, design metrics such as price and size are optimized while deadline violations are minimized. Experimental results show the advantages of our approach over previous work that consider no deadline violation is tolerable.",1,0,0,stochastic+multi-objective optimisation+multi-objective genetic algorithm,
281,281,"Adaptive weighted learning for linear regression problems via kullback leibler divergence. In this paper, we propose adaptive weighted learning for linear regression problems via the Kullback-Leibler (KL) divergence. The alternative optimization method is used to solve the proposed model. Meanwhile, we theoretically demonstrate that the solution of the optimization algorithm converges to a stationary point of the model. In addition, we also fuse global linear regression and class-oriented linear regression and discuss the problem of parameter selection. Experimental results on face and handwritten numerical character databases show that the proposed method is effective for image classification, particularly for the case that the samples in the training and testing set have different characteristics.",1,0,0,optimization+classification methods+image classification,
282,282,"Iso 24617 2 a semantically based standard for dialogue annotation. This paper summarizes the latest, final version of ISO standard 24617-2 “Semantic annotation framework, Part 2: Dialogue acts”. Compared to the preliminary version ISO DIS 24617-2:2010, described in Bunt et al. (2010), the final version additionally includes concepts for annotating rhetorical relations between dialogue units, defines a full-blown compositional semantics for the Dialogue Act Markup Language DiAML (resulting, as a side-effect, in a different treatment of functional dependence relations among dialogue acts and feedback dependence relations); and specifies an optimally transparent XML-based reference format for the representation of DiAML annotations, based on the systematic application of the notion of ‘ideal concrete syntax’. We describe these differences and briefly discuss the design and implementation of an incremental method for dialogue act recognition, which proves the usability of the ISO standard for automatic dialogue annotation.",1,0,0,functional dependency+semantics+semantic information,
283,283,"Finding unexplained activities in video. Consider a video surveillance application that monitors some location. The application knows a set of activity models (that are either normal or abnormal or both), but in addition, the application wants to find video segments that are unexplained by any of the known activity models -- these unexplained video segments may correspond to activities for which no previous activity model existed. In this paper, we formally define what it means for a given video segment to be unexplained (totally or partially) w.r.t. a given set of activity models and a probability threshold. We develop two algorithms - FindTUA and FindPUA - to identify Totally and Partially Unexplained Activities respectively, and show that both algorithms use important pruning methods. We report on experiments with a prototype implementation showing that the algorithms both run efficiently and are accurate.",1,0,0,digital videos+video surveillance+video-surveillance applications,
284,284,"Improving lexical semantics for sentential semantics modeling selectional preference and similar words in a latent variable model. Sentence Similarity [SS] computes a similarity score between two sentences. The SS task differs from document level semantics tasks in that it features the sparsity of words in a data unit, i.e. a sentence. Accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets.",1,0,0,latent variable+latent variable models+sentence similarity,
285,285,"Random forest based ensemble system for short term load forecasting. The short term load forecasting plays an essential role in the operation of electric power systems. Plenty of features involved in the forecasting cause a complex system and the long training time. The curse of dimensionality also downgrades the generalization capability of the predictor. This paper applies the random forest based ensemble system to load forecasting application. Rather than selecting a subset of features, which may cause the information lost, all features are considered in the proposed method. Different feature sets are used to construct regression systems and the average method is used as a fusion. The performance of the proposed model is compared with another existing method based on mutual information feature selection using real load datasets in New York and PJM. Experimental results show our method achieves a better result in term of higher accuracy.",1,0,0,feature space+ensemble classifiers+mutual informations,
286,286,"Conceptualizing social power for agents. One of the most pervasive concepts in human interactions is social power since many social situations entail disputes of social power. These disputes are power games and range from simple personal reasoning to the exercise of specific power strategies, which enhance or assert one’s power. Recognizing the importance of such interactions and how they can enhance autonomous agents’ socially intelligent behaviors, we present a formalization of the fundamental bases of power and conceptualize the diverse forces that should underlie an agent’s deliberative decision process. Different bases of power stem from diverse factors and have particular dynamics and effects. The objective of this work is to establish a theoretical basis for social intelligent agents capable of both being aware of and manipulating social power.",1,0,0,intelligent agents+autonomous agents+multiagent system,
287,287,"Mad g multilingual adapter generation for efficient cross lingual transfer. Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (MMTs) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose MAD-G (Multilingual ADapter Generation), which contextually generates language adapters from language representations based on typological features. In contrast to prior work, our time- and space-efficient MAD-G approach enables (1) sharing of linguistic knowledge across languages and (2) zero-shot inference by generating language adapters for unseen languages. We thoroughly evaluate MAD-G in zero-shot cross-lingual transfer on part-of-speech tagging, dependency parsing, and named entity recognition. While offering (1) improved fine-tuning efficiency (by a factor of around 50 in our experiments), (2) a smaller parameter budget, and (3) increased language coverage, MAD-G remains competitive with more expensive methods for language-specific adapter training across the board. Moreover, it offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available task-specific training data; and (ii) by further fine-tuning generated MAD-G adapters for languages with monolingual data.",1,0,0,parsing algorithm+named entity recognition+dependency parsing,
288,288,Deepanalyzer at semeval 2019 task 6 a deep learning based ensemble method for identifying offensive tweets. This paper describes the system we developed for SemEval 2019 on Identifying and Categorizing Offensive Language in Social Media (OffensEval - Task 6). The task focuses on offensive language in tweets. It is organized into three sub-tasks for offensive language identification; automatic categorization of offense types and offense target identification. The approach for the first subtask is a deep learning-based ensemble method which uses a Bidirectional LSTM Recurrent Neural Network and a Convolutional Neural Network. Additionally we use the information from part-of-speech tagging of tweets for target identification and combine previous results for categorization of offense types.,1,0,0,convolutional neural networks+recurrent neural networks+feedforward neural networks,
289,289,"Multi type attributes driven multi camera person re identification. Abstract One of the major challenges in person Re-Identification (ReID) is the inconsistent visual appearance of a person. Current works on visual feature and distance metric learning have achieved significant achievements, but still suffer from the limited robustness to pose variations, viewpoint changes, etc., and the high computational complexity. This makes person ReID among multiple cameras still challenging. This work is motivated to learn mid-level human attributes which are robust to visual appearance variations and could be used as efficient features for person matching. We propose a weakly supervised multi-type attribute learning framework which considers the contextual cues among attributes and progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely deep attributes exhibit promising generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained competitive accuracy on four person ReID datasets. Experiments also show that a simple distance metric learning modular further boosts our method, making it outperform many recent works.",1,0,0,pose variation+neural networks+convolutional neural networks,
290,290,"New biosensors and wearables for cardiorespiratory telemonitoring. Many cardiorespiratory physiological signals can be measured from the chest skin. When measured at many spots, some of them like EIT (Electrical Impedance Tomography) produce images. Classical approaches cannot practically be used to make wearables with a large number of sensors, because the sensing points are connected to a bulky centralized electronics box in a star arrangement. The solution is to use cooperative sensors. A vest embedding 51 multi-signal sensors designed for COPD (Chronic Obstructive Pulmonary Disease) patients is described as example.",1,0,0,sensors+electric impedance tomography+electrical impedance tomography+eit,
291,291,"A clustering based approach for the identification of a class of temporally switched linear systems. The behaviours of hybrid dynamic systems (HDS) are determined by combining continuous variables with discrete switching logic. The identification of a HDS aims to find an accurate model of the system's dynamics based on its past inputs and outputs. In pattern recognition (PR) methods, each mode is represented by a set of similar patterns that form restricted regions in the feature space. These sets of patterns are called classes. A pattern is a vector built from past inputs and outputs. HDS identification is a challenging problem since it involves the estimation of different sets of parameters without knowing in advance which sections of the measured data correspond to the different modes of the system. Therefore, HDS identification can be achieved by combining two steps: clustering and parameter estimation. In the clustering step, the number of discrete modes (i.e., the classes that input-output data points belong) is estimated. The parameter estimation step finds the parameters of the models that govern the continuous dynamics in each mode. In this paper, an unsupervised PR method is proposed to achieve the clustering step of the identification of temporally switched linear HDS. The determination of the number of modes does not require prior information about the modes or their number.",1,0,0,pattern recognition+prior information+switched linear system,
292,292,"One shot gesture recognition one step towards adaptive learning. User's intentions may be expressed through spontaneous gesturing, which have been seen only a few times or never before. Recognizing such gestures involves one shot gesture learning. While most research has focused on the recognition of the gestures themselves, recently new approaches were proposed to deal with gesture perception and production as part of the recognition problem. The framework presented in this work focuses on learning the process that leads to gesture generation, rather than treating the gestures as the outcomes of a stochastic process only. This is achieved by leveraging kinematic and cognitive aspects of human interaction. These factors enable the artificial production of realistic gesture samples originated from a single observation, which in turn are used as training sets for state-of-the-art classifiers. Classification performance is evaluated in terms of recognition accuracy and coherency; the latter being a novel metric that determines the level of agreement between humans and machines. Specifically, the referred machines are robots which perform artificially generated examples. Coherency in recognition was determined at 93.8%, corresponding to a recognition accuracy of 89.2% for the classifiers and 92.5% for human participants. A proof of concept was performed towards the expansion of the proposed one shot learning approach to adaptive learning, and the results are presented and the implications discussed.",1,0,0,classification methods+stochastic+training sample,
293,293,"Learning 2 opt heuristics for the traveling salesman problem via deep reinforcement learning. Recent works using deep learning to solve the Traveling Salesman Problem (TSP) have focused on learning construction heuristics. Such approaches find TSP solutions of good quality but require additional procedures such as beam search and sampling to improve solutions and achieve state-of-the-art performance. However, few studies have focused on improvement heuristics, where a given solution is improved until reaching a near-optimal one. In this work, we propose to learn a local search heuristic based on 2-opt operators via deep reinforcement learning. We propose a policy gradient algorithm to learn a stochastic policy that selects 2-opt operations given a current solution. Moreover, we introduce a policy neural network that leverages a pointing attention mechanism, which unlike previous works, can be easily extended to more general k-opt moves. Our results show that the learned policies can improve even over random initial solutions and approach near-optimal solutions at a faster rate than previous state-of-the-art deep learning methods.",1,0,0,reinforcement learning+reinforcement learning method+multi-agent reinforcement learning,
294,294,"Fuzze fuzzy fairness evaluation of offensive language classifiers on african american english. Hate speech and offensive language are rampant on social media. Machine learning has provided a way to moderate foul language at scale. However, much of the current research focuses on overall performance. Models may perform poorly on text written in a minority dialectal language. For instance, a hate speech classifier may produce more false positives on tweets written in African-American Vernacular English (AAVE). To measure these problems, we need text written in both AAVE and Standard American English (SAE). Unfortunately, it is challenging to curate data for all linguistic styles in a timely manner—especially when we are constrained to specific problems, social media platforms, or by limited resources. In this paper, we answer the question, “How can we evaluate the performance of classifiers across minority dialectal languages when they are not present within a particular dataset?” Specifically, we propose an automated fairness fuzzing tool called FuzzE to quantify the fairness of text classifiers applied to AAVE text using a dataset that only contains text written in SAE. Overall, we find that the fairness estimates returned by our technique moderately correlates with the use of real ground-truth AAVE text. Warning: Offensive language is displayed in this manuscript.",1,0,0,classifiers+linguistics+text classifiers,
295,295,"Authorship attribution of polish newspaper articles. This paper examines the machine learning approach to authorship attribution of articles in the Polish language. The focus is on the effect of the data volume, number of authors and thematic homogeneity on authorship attribution quality. We study the impact of feature selection under various feature selection criteria, mainly chi square and information gain measures, as well as the effect of combining features of different types. Results are reported for the Rzeczpospolita corpus in terms of the \(F_1\) measure.",1,0,0,local binary patterns+feature space+text classification,
296,296,"Scoring lexical entailment with a supervised directional similarity network. We present the Supervised Directional Similarity Network, a novel neural architecture for learning task-specific transformation functions on top of general-purpose word embeddings. Relying on only a limited amount of supervision from task-specific scores on a subset of the vocabulary, our architecture is able to generalise and transform a general-purpose distributional vector space to model the relation of lexical entailment. Experiments show excellent performance on scoring graded lexical entailment, raising the state-of-the-art on the HyperLex dataset by approximately 25%.",1,0,0,first order logic+syntactics+word embedding,
297,297,"Discriminative training of 150 million translation parameters and its application to pruning. Until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. In this paper, we propose to scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline.",1,0,0,machine translations+statistical machine translation+discriminative training,
298,298,"Unsupervised intuitive physics from visual observations. While learning models of intuitive physics is an increasingly active area of research, current approaches still fall short of natural intelligences in one important regard: they require external supervision, such as explicit access to physical states, at training and sometimes even at test times. Some authors have relaxed such requirements by supplementing the model with an handcrafted physical simulator. Still, the resulting methods are unable to automatically learn new complex environments and to understand physical interactions within them. In this work, we demonstrated for the first time learning such predictors directly from raw visual observations and without relying on simulators. We do so in two steps: first, we learn to track mechanically-salient objects in videos using causality and equivariance, two unsupervised learning principles that do not require auto-encoding. Second, we demonstrate that the extracted positions are sufficient to successfully train visual motion predictors that can take the underlying environment into account. We validate our predictors on synthetic datasets; then, we introduce a new dataset, ROLL4REAL, consisting of real objects rolling on complex terrains (pool table, elliptical bowl, and random height-field). We show that in all such cases it is possible to learn reliable extrapolators of the object trajectories from raw videos alone, without any form of external supervision and with no more prior knowledge than the choice of a convolutional neural network architecture.",1,0,0,convolutional neural networks+feedforward neural networks+unsupervised learning,
299,299,"Likelihood ratio approaches to automatic modulation classification. Adaptive modulation and automatic modulation classification are highly demanded in software-defined radio (SDR) for both commercial and military applications. Various design options of automatic classifiers have attracted researchers in developing 3G and 4G wireless communication systems. There is an urgent need to investigate the different methods of coherent and noncoherent modulation estimations, discuss the challenges in cooperative and noncooperative communication environment, and understand the distinct requirements in real-time modulation classifications. This survey paper focuses on the automatic modulation classification methods based on likelihood functions, studies various classification solutions derived from likelihood ratio test, and discusses the detailed characteristics associated with all major algorithms.",1,0,0,likelihood ratios+likelihood functions+adaptive modulation,
300,300,"An integrated agent based approach for modeling disease spread in large populations to support health informatics. Disease spread has a much broader impact on public health than the important issues of illnesses and deaths. Among these are chronic health problems, reductions in national wealth, government instability, and crime. Here, we describe an integrated approach for computational health informatics that includes individual-based population construction, and agent-based modeling of dynamics. We restrict dynamics modeling to epidemiology. We itemize technical challenges and provide a case study of the Ebola outbreak in Monrovia, Liberia, with discussion of mobile treatment centers.",1,0,0,intelligent agents+autonomous agents+agent-based approach,
301,301,"Specification and derivation of key performance indicators for business analytics. Key Performance Indicators (KPI) measure the performance of an enterprise relative to its objectives thereby enabling corrective action where there are deviations. In current practice, KPIs are manually integrated within dashboards and scorecards used by decision makers. This practice entails various shortcomings. First, KPIs are not related to their business objectives and strategy. Consequently, decision makers often obtain a scattered view of the business status and business concerns. Second, while KPIs are defined by decision makers, their implementation is performed by IT specialists. This often results in discrepancies that are difficult to identify. In this paper, we propose an approach that provides decision makers with an integrated view of strategic business objectives and conceptual data warehouse KPIs. The main benefit of our proposal is that it links strategic business models to the data for monitoring and assessing them. In our proposal, KPIs are defined using a modeling language where decision makers specify KPIs using business terminology, but can also perform quick modifications and even navigate data while maintaining a strategic view. This enables monitoring and what-if analysis, thereby helping analysts to compare expectations with reported results. HighlightsNovel approach for conceptualizing and specifying Key Performance Indicators.Transforms strategic models into analytic tools to aid in decision making.Enables the user to analyze data subspaces from a strategic point of view.Based on the Semantics for Business Vocabulary and Rules specification.Implemented to support the whole process from definition to data extraction.",1,0,0,warehouses+vocabulary+semantics,
302,302,"Extending agm contraction to arbitrary logics. Classic entrenchment-based contraction is not applicable to many useful logics, such as description logics. This is because the semantic construction refers to arbitrary disjunctions of formulas, while many logics do not fully support disjunction. In this paper, we present a new entrenchment-based contraction which does not rely on any logical connectives except conjunction. This contraction is applicable to all fragments of first-order logic that support conjunction. We provide a representation theorem for the contraction which shows that it satisfies all the AGM postulates except for the controversial Recovery Postulate, and is a natural generalisation of entrenchment-based contraction.",1,0,0,first order logic+dynamic logic+description logic,
303,303,"An approximate solution method for large risk averse markov decision processes. Stochastic domains often involve risk-averse decision makers. While recent work has focused on how to model risk in Markov decision processes using risk measures, it has not addressed the problem of solving large risk-averse formulations. In this paper, we propose and analyze a new method for solving large risk-averse MDPs with hybrid continuous-discrete state spaces and continuous action spaces. The proposed method iteratively improves a bound on the value function using a linearity structure of the MDP. We demonstrate the utility and properties of the method on a portfolio optimization problem.",1,0,0,action spaces+value functions+portfolio optimization,
304,304,"Transfer learning of the expressivity using flow metric learning in multispeaker text to speech synthesis. In this paper, we present a novel deep metric learning architecture along with variational inference incorporated in a paramet-ric multispeaker expressive text-to-speech (TTS) system. We proposed inverse autoregressive flow (IAF) as a way to perform the variational inference, thus providing flexible approximate posterior distribution. The proposed approach condition the text-to-speech system on speaker embeddings so that latent space represents the emotion as semantic characteristics. For representing the speaker, we extracted speaker em-beddings from the x-vector based speaker recognition model trained on speech data from many speakers. To predict the vocoder features, we used the acoustic model conditioned on the textual features as well as on the speaker embedding. We transferred the expressivity by using the mean of the latent variables for each emotion to generate expressive speech in different speaker's voices for which no expressive speech data is available. We compared the results obtained using flow-based variational inference with variational autoencoder as a base-line model. The performance measured by mean opinion score (MOS), speaker MOS, and expressive MOS shows that N-pair loss based deep metric learning along with IAF model improves the transfer of expressivity in the desired speaker's voice in synthesized speech.",1,0,0,speaker recognition+posterior distributions+variational inference,
305,305,"An evolutionary transfer reinforcement learning framework for multiagent systems. In this paper, we present an evolutionary transfer reinforcement learning framework (eTL) for developing intelligent agents capable of adapting to the dynamic environment of multiagent systems (MASs). Specifically, we take inspiration from Darwin’s theory of natural selection and Universal Darwinism as the principal driving forces that govern the evolutionary knowledge transfer process. The essential backbone of our proposed eTL comprises several meme-inspired evolutionary mechanisms, namely  meme representation ,  meme expression ,  meme assimilation ,  meme internal evolution , and  meme external evolution . Our proposed approach constructs social selection mechanisms that are modeled after the principles of human learning to identify appropriate interacting partners. eTL also models the intrinsic parallelism of natural evolution and errors that are introduced due to the physiological limits of the agents’ ability to perceive differences, so as to generate “growth” and “variation” of knowledge that agents have of the world, thus exhibiting higher adaptivity capabilities on solving complex problems. To verify the efficacy of the proposed paradigm, comprehensive investigations of the proposed eTL against existing state-of-the-art TL methods in MAS, are conducted on the “minefield navigation tasks” platform and the “Unreal Tournament 2004” first person shooter computer game, in which homogeneous and heterogeneous learning machines are considered.",1,0,0,evolutionary process+multi-agent+reinforcement learning,
306,306,"Unsupervised multi hop question answering by question generation. Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.",1,0,0,question answering+unsupervised learning+qa system,
307,307,"Discriminative probabilistic prototype learning. In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where an input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people's work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmarks.",1,0,0,vector quantization+learning vector quantization+hyperspectral image classification,
308,308,"Incremental construction of robust but deep semantic representations for use in responsive dialogue systems. It is widely acknowledged that current dialogue systems are held back by a lack of flexibility, both in their turn-taking model (typically, allowing only a strict back-and-forth between user and system) and in their interpretation capabilities (typically, restricted to slot filling). We have developed a component for NLU that attempts to address both of these challenges, by a) constructing robust but deep meaning representations that support a range of further user intention determination techniques from inference / reasoning-based ones to ones based on more basic structures, and b) constructing these representations incrementally and hence providing semantic information on which system reactions can be based concurrently to the ongoing user utterance. The approach is based on an existing semantic representation formalism, Robust Minimal Recursion Semantics, which we have modified to suit incremental construction. We present the modifications, our implementation, and discuss applications within a dialogue system context, showing that the approach indeed promises to meet the requirements for more flexibility.",1,0,0,semantic information+dialogue systems+inference,
309,309,"Adaptive memetic algorithm for the vehicle routing problem with time windows. This paper presents an adaptive memetic algorithm (AMA) to minimize the total travel distance in the NP-hard vehicle routing problem with time windows (VRPTW). Although memetic algorithms (MAs) have been proven to be very efficient in solving the VRPTW, their main drawback is an unclear tuning of their numerous parameters. Here, we introduce the AMA in which the selection scheme and the population size are adjusted during the search. We propose a new adaptive selection scheme to balance the exploration and exploitation of the search space. An extensive experimental study confirms that the AMA outperforms a standard MA in terms of the convergence capabilities.",1,0,0,memetic algorithms+memetic+vehicle routing problem with time windows,
310,310,"Orthogonal rotations in latent semantic analysis an empirical study. Abstract   The Latent Semantic Analysis (LSA) literature has recently started to address the issue of interpretability of the extracted dimensions. On the software implementation front, recent versions of SAS Text Miner ® started incorporating Varimax rotations. Considering open source software such as R, when it comes to rotation procedures the user has many more options. However, there is a little work in providing guidance for selecting an appropriate rotation procedure. In this paper we further previous research on LSA rotations by introducing two well-known orthogonal rotations, namely Quartimax and Equamax, and comparing them to Varimax. We present a study that empirically tests the influence of the chosen orthogonal rotations on the extraction and interpretation of LSA factors. Our results indicate that, in most cases, Varimax and Equamax produce factors with similar interpretation, while Quartimax tends to produce a single factor. We conclude with recommendations on how these rotation procedures should be used and suggestions for future research. We note that orthogonal rotations can be used to improve the interpretability of other SVD-based models, such as COALS.",1,0,0,semantics+semantic information+interpretability,
311,311,"Learning description logic concepts when can positive and negative examples be separated. Learning description logic (DL) concepts from positive and negative examples given in the form of labeled data items in a KB has received significant attention in the literature. We study the fundamental question of when a separating DL concept exists and provide useful model-theoretic characterizations as well as complexity results for the associated decision problem. For expressive DLs such as ALC and ALCQI, our characterizations show a surprising link to the evaluation of ontology-mediated conjunctive queries. We exploit this to determine the combined complexity (between ExpTime and NExpTime) and data complexity (second level of the polynomial hierarchy) of separability. For the Horn DL EL, separability is ExpTime-complete both in combined and in data complexity while for its modest extension ELI it is even undecidable. Separability is also undecidable when the KB is formulated in ALC and the separating concept is required to be in EL or ELI.",1,0,0,description logic+complexity results+conjunctive queries,
312,312,"Mining brokers in dynamic social networks. The theory of brokerage in sociology suggests if contacts between two parties are enabled through a third party, the latter occupies a strategic position of controlling information flows. Such individuals are called brokers and they play a key role in disseminating information. However, there is no systematic approach to identify brokers in online social networks. In this paper, we formally define the problem of detecting top-$k$ brokers given a social network and show that it is NP-hard. We develop a heuristic algorithm to find these brokers based on the weak tie theory. In order to handle the dynamic nature of online social networks, we design incremental algorithms: WeakTie-Local for unidirectional networks and WeakTie-Bi for bidirectional networks. We use two real world datasets, DBLP and Twitter, to evaluate the proposed methods. We also demonstrate how the detected brokers are useful in diffusing information across communities and propagating tweets to reach more distinct users.",1,0,0,social networks+online social networks+twitter,
313,313,"Underdetermined blind source separation using sparse coding. In an underdetermined mixture system with    $n$    unknown sources, it is a challenging task to separate these sources from their    $m$    observed mixture signals, where    $m  . By exploiting the technique of sparse coding, we propose an effective approach to discover some 1-D subspaces from the set consisting of all the time-frequency (TF) representation vectors of observed mixture signals. We show that these 1-D subspaces are associated with TF points where only single source possesses dominant energy. By grouping the vectors in these subspaces via hierarchical clustering algorithm, we obtain the estimation of the mixing matrix. Finally, the source signals could be recovered by solving a series of least squares problems. Since the sparse coding strategy considers the linear representation relations among all the TF representation vectors of mixing signals, the proposed algorithm can provide an accurate estimation of the mixing matrix and is robust to the noises compared with the existing underdetermined blind source separation approaches. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed method.",1,0,0,convolutive mixture+source separation+mixing matrix,
314,314,"Mixture kernel least mean square. Instead of using single kernel, different approaches of using multiple kernels have been proposed recently in kernel learning literature, one of which is multiple kernel learning (MKL). In this paper, we propose an alternative to MKL in order to select the appropriate kernel given a pool of predefined kernels, for a family of online kernel filters called kernel adaptive filters (KAF). The need for an alternative is that, in a sequential learning method where the hypothesis is updated at every incoming sample, MKL would provide a new kernel, and thus a new hypothesis in the new reproducing kernel Hilbert space (RKHS) associated with the kernel. This does not fit well in the KAF framework, as learning a hypothesis in a fixed RKHS is the core of the KAF algorithms. Hence, we introduce an adaptive learning method to address the kernel selection problem for the KAF, based on competitive mixture of models. We propose mixture kernel least mean square (MxKLMS) adaptive filtering algorithm, where the kernel least mean square (KLMS) filters learned with different kernels, act in parallel at each input instance and are competitively combined such that the filter with the best kernel is an expert for each input regime. The competition among these experts is created by using a performance based gating, that chooses the appropriate expert locally. Therefore, the individual filter parameters as well as the weights for combination of these filters are learned simultaneously in an online fashion. The results obtained suggest that the model not only selects the best kernel, but also significantly improves the prediction accuracy.",1,0,0,kernel function+single kernel+multiple kernels+kernel learning,
315,315,Bat call identification with gaussian process multinomial probit regression and a dynamic time warping kernel. We study the problem of identifying bat species from echolocation calls in order to build automated bioacoustic monitoring algorithms. We employ the Dynamic Time Warping algorithm which has been successfully applied for bird ight calls identication and show that classication performance is superior to hand crafted call shape parameters used in previous research. This highlights that generic bioacoustic software with good classication rates can be constructed with little domain knowledge. We conduct a study with eld data of 21 bat species from the north and central Mexico using a multinomial probit regression model with Gaussian process prior and a full EP approximation of the posterior of latent function values. Results indicate high classication accuracy across almost all classes while misclassication rate across families of species is low highlighting the common evolutionary path of echolocation in bats.,1,0,0,gaussians+latent variable+gaussian processes,
316,316,"Video captioning with boundary aware hierarchical language decoding and joint video prediction. The explosion of video data on the internet requires effective and efficient technology to generate captions automatically for people who are not able to watch the videos. Despite the great progress of video captioning research, particularly on video feature encoding, the language decoder is still largely based on the prevailing RNN decoder such as LSTM, which tends to prefer the frequent word that aligns with the video. In this paper, we propose a boundary-aware hierarchical language decoder for video captioning, which consists of a high-level GRU based language decoder, working as a global (caption-level) language model, and a low-level GRU based language decoder, working as a local (phrase-level) language model. Most importantly, we introduce a binary gate into the low-level GRU language decoder to detect the language boundaries. Together with other advanced components including joint video prediction, shared soft attention, and boundary-aware video encoding, our integrated video captioning framework can discover hierarchical language information and distinguish the subject and the object in a sentence, which are usually confusing during the language generation. Extensive experiments on two widely-used video captioning datasets, MSR-Video-to-Text (MSR-VTT) \cite{xu2016msr} and YouTube-to-Text (MSVD) \cite{chen2011collecting} show that our method is highly competitive, compared with the state-of-the-art methods.",1,0,0,long short term memory neural networks+recurrent neural networks+language model,
317,317,"Ego2hands a dataset for egocentric two hand segmentation and detection. Hand segmentation and detection in truly unconstrained RGB-based settings is important for many applications. However, existing datasets are far from sufficient both in terms of size and variety due to the infeasibility of manual annotation of large amounts of segmentation and detection data. As a result, current methods are limited by many underlying assumptions such as constrained environment, consistent skin color and lighting. In this work, we present a large-scale RGB-based egocentric hand segmentation/detection dataset Ego2Hands that is automatically annotated and a color-invariant compositing-based data generation technique capable of creating unlimited training data with variety. For quantitative analysis, we manually annotated an evaluation set that significantly exceeds existing benchmarks in quantity, diversity and annotation accuracy. We show that our dataset and training technique can produce models that generalize to unseen environments without domain adaptation. We introduce Convolutional Segmentation Machine (CSM) as an architecture that better balances accuracy, size and speed and provide thorough analysis on the performance of state-of-the-art models on the Ego2Hands dataset.",1,0,0,segmentation methods+image segmentation+object segmentation,
318,318,"Inclusion dependency discovery an experimental evaluation of thirteen algorithms. Inclusion dependencies are an important type of metadata in relational databases, because they indicate foreign key relationships and serve a variety of data management tasks, such as data linkage, query optimization, and data integration. The discovery of inclusion dependencies is, therefore, a well-studied problem and has been addressed by many algorithms. Each of these discovery algorithms follows its own strategy with certain strengths and weaknesses, which makes it difficult for data scientists to choose the optimal algorithm for a given profiling task. This paper summarizes the different state-of-the-art discovery approaches and discusses their commonalities. For evaluation purposes, we carefully re-implemented the thirteen most popular discovery algorithms and discuss their individual properties. Our extensive evaluation on several real-world and synthetic datasets shows the unbiased performance of the different discovery approaches and, hence, provides a guideline on when and where each approach works best. Comparing the different runtimes and scalability graphs, we identify the best approaches for certain situations and demonstrate where certain algorithms fail.",1,0,0,database systems+linkage analysis+optimization,
319,319,"Bsfcos block and sparse principal component analysis based fast co saliency detection method. Co-saliency detection, an emerging research area in saliency detection, aims to extract the common saliency from the multi images. The extracted co-saliency map has been utilized in various applications, such as in co-segmentation, co-recognition and so on. With the rapid development of image acquisition technology, the original digital images are becoming more and more clearly. The existing co-saliency detection methods processing these images need enormous computer memory along with high computational complexity. These limitations made it hard to satisfy the demand of real-time user interaction. This paper proposes a fast co-saliency detection method based on the image block partition and sparse feature extraction method (BSFCoS). Firstly, the images are divided into several uniform blocks, and the low-level features are extracted from Lab and RGB color spaces. In order to maintain the characteristics of the original images and reduce the number of feature points as well as possible, Truncated Power for sparse principal components method are employed to extract sparse features. Furthermore, K-Means method is adopted to cluster the extracted sparse features, and calculate the three salient feature weights. Finally, the co-saliency map was acquired from the feature fusion of the saliency map for single image and multi images. The proposed method has been tested and simulated on two benchmark datasets: Co-saliency Pairs and CMU Cornell iCoseg datasets. Compared with the existing co-saliency methods, BSFCoS has a significant running time improvement in multi images processing while ensuring detection results. Lastly, the co-segmentation method based on BSFCoS is also given and has a better co-segmentation performance.",1,0,0,single images+k-means method+segmentation performance,
320,320,"Pc2wf 3d wireframe reconstruction from raw point clouds. We introduce PC2WF, the first end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a wireframe of that object, i.e., a sparse set of corner points linked by line segments. Recovering the wireframe is a challenging task, where the numbers of both vertices and edges are different for every instance, and a-priori unknown. Our architecture gradually builds up the model: It starts by encoding the points into feature vectors. Based on those features, it identifies a pool of candidate vertices, then prunes those candidates to a final set of corner vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence. We validate the proposed model on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, as well as on a new real-world dataset. Our model produces wireframe abstractions of good quality and outperforms several baselines.",1,0,0,point cloud+network architecture+corner point,
321,321,"Biva a very deep hierarchy of latent variables for generative modeling. With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.",1,0,0,latent variable models+generative adversarial networks+variational auto encoders,
322,322,"Best arm identification for contaminated bandits. This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for \emph{contaminated bandits}, where each arm pull has probability $\varepsilon$ of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor).",1,0,0,probability+distribution functions+probability distributions,
323,323,"On the compactness efficiency and representation of 3d convolutional networks brain parcellation as a pretext task. Deep convolutional neural networks are powerful tools for learning visual representations from images. However, designing efficient deep architectures to analyse volumetric medical images remains challenging. This work investigates efficient and flexible elements of modern convolutional networks such as dilated convolution and residual connection. With these essential building blocks, we propose a high-resolution, compact convolutional network for volumetric image segmentation. To illustrate its efficiency of learning 3D representation from large-scale image data, the proposed network is validated with the challenging task of parcellating 155 neuroanatomical structures from brain MR images. Our experiments show that the proposed network architecture compares favourably with state-of-the-art volumetric segmentation networks while being an order of magnitude more compact. We consider the brain parcellation task as a pretext task for volumetric image segmentation; our trained network potentially provides a good starting point for transfer learning. Additionally, we show the feasibility of voxel-level uncertainty estimation using a sampling approximation through dropout.",1,0,0,neural networks+convolutional neural networks+image segmentation,
324,324,"Local block wise self attention for normal organ segmentation. We developed a new and computationally simple local block-wise self attention based normal structures segmentation approach applied to head and neck computed tomography (CT) images. Our method uses the insight that normal organs exhibit regularity in their spatial location and inter-relation within images, which can be leveraged to simplify the computations required to aggregate feature information. We accomplish this by using local self attention blocks that pass information between each other to derive the attention map. We show that adding additional attention layers increases the contextual field and captures focused attention from relevant structures. We developed our approach using U-net and compared it against multiple state-of-the-art self attention methods. All models were trained on 48 internal headneck CT scans and tested on 48 CT scans from the external public domain database of computational anatomy dataset. Our method achieved the highest Dice similarity coefficient segmentation accuracy of 0.85$\pm$0.04, 0.86$\pm$0.04 for left and right parotid glands, 0.79$\pm$0.07 and 0.77$\pm$0.05 for left and right submandibular glands, 0.93$\pm$0.01 for mandible and 0.88$\pm$0.02 for the brain stem with the lowest increase of 66.7\% computing time per image and 0.15\% increase in model parameters compared with standard U-net. The best state-of-the-art method called point-wise spatial attention, achieved \textcolor{black}{comparable accuracy but with 516.7\% increase in computing time and 8.14\% increase in parameters compared with standard U-net.} Finally, we performed ablation tests and studied the impact of attention block size, overlap of the attention blocks, additional attention layers, and attention block placement on segmentation performance.",1,0,0,image segmentation+object segmentation+segmentation performance,
325,325,"Dependent normalized random measures. In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modeling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process.",1,0,0,inference+marginals+hierarchical dirichlet process,
326,326,"Style generator inversion for image enhancement and animation. One of the main motivations for training high quality image generative models is their potential use as tools for image manipulation. Recently, generative adversarial networks (GANs) have been able to generate images of remarkable quality. Unfortunately, adversarially-trained unconditional generator networks have not been successful as image priors. One of the main requirements for a network to act as a generative image prior, is being able to generate every possible image from the target distribution. Adversarial learning often experiences mode-collapse, which manifests in generators that cannot generate some modes of the target distribution. Another requirement often not satisfied is invertibility i.e. having an efficient way of finding a valid input latent code given a required output image. In this work, we show that differently from earlier GANs, the very recently proposed style-generators are quite easy to invert. We use this important observation to propose style generators as general purpose image priors. We show that style generators outperform other GANs as well as Deep Image Prior as priors for image enhancement tasks. The latent space spanned by style-generators satisfies linear identity-pose relations. The latent space linearity, combined with invertibility, allows us to animate still facial images without supervision. Extensive experiments are performed to support the main contributions of this paper.",1,0,0,latent variable+markov random fields+generative adversarial networks,
327,327,"Dropout model evaluation in moocs. The field of learning analytics needs to adopt a more rigorous approach for predictive model evaluation that matches the complex practice of model-building. In this work, we present a procedure to statistically test hypotheses about model performance which goes beyond the state-of-the-practice in the community to analyze both algorithms and feature extraction methods from raw data. We apply this method to a series of algorithms and feature sets derived from a large sample of Massive Open Online Courses (MOOCs). While a complete comparison of all potential modeling approaches is beyond the scope of this paper, we show that this approach reveals a large gap in dropout prediction performance between forum-, assignment-, and clickstream-based feature extraction methods, where the latter is significantly better than the former two, which are in turn indistinguishable from one another. This work has methodological implications for evaluating predictive or AI-based models of student success, and practical implications for the design and targeting of at-risk student models and interventions.",1,0,0,hierarchical model+student modeling+feature space,
328,328,"Deep learning for tomographic image reconstruction. Deep-learning-based tomographic imaging is an important application of artificial intelligence and a new frontier of machine learning. Deep learning has been widely used in computer vision and image analysis, which deal with existing images, improve these images, and produce features from them. Since 2016, deep learning techniques have been actively researched for tomographic imaging, especially in the context of biomedicine, with impressive results and great potential. Tomographic reconstruction produces images of multi-dimensional structures from externally measured ‘encoded’ data in the form of various tomographic transforms (integrals, harmonics, echoes and so on). In this Review, we provide a general background, highlight representative results with an emphasis on medical imaging, and discuss key issues that need to be addressed in this emerging field. In particular, tomographic imaging is an integral part of modern medicine, and will play a key role in personalized, preventive and precision medicine and make it intelligent, inexpensive and indiscriminate. The popularity of deep learning is leading to new areas in biomedical applications. Wang and colleagues summarize in this Review the recent development and future directions of deep neural networks for superior image quality in the tomographic imaging field.",1,0,0,deep learning+image analysis+neural networks,
329,329,"Efficient model based diagnosis with maximum satisfiability. Model-Based Diagnosis (MBD) finds a growing number of uses in different settings, which include software fault localization, debugging of spreadsheets, web services, and hardware designs, but also the analysis of biological systems, among many others. Motivated by these different uses, there have been significant improvements made to MBD algorithms in recent years. Nevertheless, the analysis of larger and more complex systems motivates further improvements to existing approaches. This paper proposes a novel encoding of MBD into maximum satisfiability (MaxSAT). The new encoding builds on recent work on using Propositional Satisfiability (SAT) for MBD, but identifies a number of key optimizations that are very effective in practice. The paper also proposes a new set of challenging MBD instances, which can be used for evaluating new MBD approaches. Experimental results obtained on existing and on the new MBD problem instances, show conclusive performance gains over the current state of the art.",1,0,0,propositional formulas+satisfiability problems+propositional satisfiability,
330,330,"Mapping for planetary rovers from terramechanics perspective. In an autonomous scientific exploration system, the terrain map generated from mapping process integrates sensing information from multiple aspects and lays the base for decision making processes. With the increasing challenges in planetary exploration, equipping planetary rovers with the principles of terramechanics is becoming more and more common, especially on rough or intricate terrain. However, it is difficult for conventional maps with elevation information only to reflect terrain mechanical properties, which play important roles in terramechanics-based simulation or motion control. This study extracts the dominant parameters in terrain bearing and shearing models, and presents a multi-layered grid map with fundamental geometric and mechanical elements. A corresponding mapping scheme based on dense visual input is designed to reconstruct elevation in the map and predict terrain mechanical parameters of the entire visual field. Experiments are conducted to verify the practicability of the approach proposed in a Mars emulation yard with a rover prototype.",1,0,0,grid node+motion control+decision support systems,
331,331,"Domain specific sentiment dictionary for opinion mining of vietnamese text. Knowing public opinions from subjective text messages vastly available on the Web is very useful for many different purposes. Technically, extracting efficiently and accurately the opinions from a huge amount of unstructured text messages is challenging. For English language, a common approach to this problem is using sentiment dictionaries. However, building a sentiment dictionary for less popular languages, such as Vietnamese, is difficult and time consuming. This paper proposes an approach to mining public opinions from Vietnamese text using a domain specific sentiment dictionary in order to improve the accuracy. The sentiment dictionary is built incrementally using statistical methods for a specific domain. The efficiency of the approach is demonstrated through an application which is built to extract public opinions on online products and services. Even though this approach is designed initially for Vietnamese text, we believe that it is also applicable to other languages.",1,0,0,sentiment analysis+opinion mining+semantic orientation,
332,332,"Topology learnable graph convolution for skeleton based action recognition. Abstract Graph convolutional networks (GCNs) generalize convolutional neural networks into irregular graph-like structures. Generally, graph topologies are set by hand and fixed over all layers. Handcrafted connections may not be optimal and cannot fully use the self-learning ability of deep learning. In this work, we explore a topology-learnable graph convolution for skeleton-based action recognition. Specifically, a spatial graph convolution can be decomposed into a feature learning component that evolves the features of each graph vertex, and a graph vertex fusion component in which the latent graph topologies can be learned adaptively. Different initialization strategies for the learnable fusion matrix are evaluated. Experimental results that are based on the spatial-temporal GCNs for skeleton-based action recognition, demonstrate that convolution can work on graphs like on images, even if only a specific fusion matrix initialization that uses adjacency matrices is applied. Moreover, the self-learning process can learn the latent topology of a graph beyond the handcrafted topology, thereby making graph convolution flexible and universal.",1,0,0,latent variable+neural networks+convolutional neural networks,
333,333,"Morefusion multi object reasoning for 6d pose estimation from volumetric fusion. Robots and other smart devices need efficient object-based scene representations from their on-board vision systems to reason about contact, physics and occlusion. Recognized precise object models will play an important role alongside non-parametric reconstructions of unrecognized structures. We present a system which can estimate the accurate poses of multiple known objects in contact and occlusion from real-time, embodied multi-view vision. Our approach makes 3D object pose proposals from single RGB-D views, accumulates pose estimates and non-parametric occupancy information from multiple views as the camera moves, and performs joint optimization to estimate consistent, non-intersecting poses for multiple objects in contact. 
We verify the accuracy and robustness of our approach experimentally on 2 object datasets: YCB-Video, and our own challenging Cluttered YCB-Video. We demonstrate a real-time robotics application where a robot arm precisely and orderly disassembles complicated piles of objects, using only on-board RGB-D vision.",1,0,0,optimization+joint optimization+object pose,
334,334,"Lost in translation authorship attribution using frame semantics. We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that frame-based classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that---contrary to current belief---naive classifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.",1,0,0,syntactics+svm+training sample,
335,335,"Parasite worm egg automatic detection in microscopy stool image based on faster r cnn. This paper proposed a method based on Faster R-CNN for detection of human parasite eggs in stool images. The shapes, and patterns of parasite worm in egg micro images are very diversity, therefore proposing and choosing the good model to detect them is necessary to help the doctors discover the potential disease by worm in human. To be sure for the proposal, we executed many various experiments, and retrieved dataset from two independent resources. The training set is retrieved in standard biology image library, meanwhile the evaluation image set is retrieved from real patients. The precision, recall and other values evaluated in the experiments represented the effectiveness of the method. The various experiments with the outstanding results proved the correctness of the proposal.",1,0,0,digital image+convolutional neural networks+training sample,
336,336,"Unsupervised identii cation of translationese. Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that  unsupervised  classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy.",1,0,0,text classification+machine translations+statistical machine translation,
337,337,"How much knowledge can you pack into the parameters of a language model. It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.",1,0,0,n-gram language models+language model+statistical language models,
338,338,"Gans for semi supervised opinion spam detection. Online reviews have become a vital source of information in purchasing a service (product). Opinion spammers manipulate reviews, affecting the overall perception of the service. A key challenge in detecting opinion spam is obtaining ground truth. Though there exists a large set of reviews online, only a few of them have been labeled spam or non-spam. In this paper, we propose spamGAN, a generative adversarial network which relies on limited set of labeled data as well as unlabeled data for opinion spam detection. spamGAN improves the state-of-the-art GAN based techniques for text classification. Experiments on TripAdvisor dataset show that spamGAN outperforms existing spam detection techniques when limited labeled data is used. Apart from detecting spam reviews, spamGAN can also generate reviews with reasonable perplexity.",1,0,0,text classification+gan+generative adversarial networks,
339,339,"Improving target driven visual navigation with attention on 3d spatial relationships. Embodied artificial intelligence (AI) tasks shift from tasks focusing on internet images to active settings involving embodied agents that perceive and act within 3D environments. In this paper, we investigate the target-driven visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes, whose navigation task aims to train an agent that can intelligently make a series of decisions to arrive at a pre-specified target location from any possible starting positions only based on egocentric views. However, most navigation methods currently struggle against several challenging problems, such as data efficiency, automatic obstacle avoidance, and generalization. Generalization problem means that agent does not have the ability to transfer navigation skills learned from previous experience to unseen targets and scenes. To address these issues, we incorporate two designs into classic DRL framework: attention on 3D knowledge graph (KG) and target skill extension (TSE) module. On the one hand, our proposed method combines visual features and 3D spatial representations to learn navigation policy. On the other hand, TSE module is used to generate sub-targets which allow agent to learn from failures. Specifically, our 3D spatial relationships are encoded through recently popular graph convolutional network (GCN). Considering the real world settings, our work also considers open action and adds actionable targets into conventional navigation situations. Those more difficult settings are applied to test whether DRL agent really understand its task, navigating environment, and can carry out reasoning. Our experiments, performed in the AI2-THOR, show that our model outperforms the baselines in both SR and SPL metrics, and improves generalization ability across targets and scenes.",1,0,0,visual feature+obstacle avoidance+reinforcement learning,
340,340,"Measurement of analytical knowledge based corporate memory and its application. In the current knowledge-driven economy, businesses are increasingly required to function as knowledge-based organizations. In these organizations, knowledge usually serves as the means for attainment of competitive advantage. It is clear that organizational knowledge has to be carefully managed, and knowledge management measurement is important to businesses. In this paper, corporate memory (CM) is viewed as an organization memory for managing knowledge. Generically and concretely, CM is constructed using analytical knowledge (AK), which is defined as the knowledge formatted with 5W1H (who, when, where, what, why, and how). AK is extracted from data storage systems and domain experts by aggregating information, where data analysts, knowledge workers, and knowledge users are involved in a knowledge discovery process. The research objective of this study is to propose a measurement approach, which provides a generic and applicable methodology for measuring the performance and quality of CM. To represent the uncertainty and fuzzy terms in the evaluation environments, and to explicate the invisible impact induced by information technology (IT), the fuzzy set theory is applied. An effective procedure is also proposed to apply the measurement approach in practice. Highlights? A generic and applicable approach for measuring the performance and quality of CM ? The fuzzy set theory and the S-shaped logistic model are employed. ? A definite and effective procedure is able to support the measurement in practice. ? The method is able to manage knowledge and then develop strategies in businesses. ? The method facilitates the development of strategies for intellectual capital.",1,0,0,tacit knowledge+knowledge management+organizational memory,
341,341,"Frequent direction algorithms for approximate matrix multiplication with applications in cca. Approximate matrix multiplication (AMM) becomes increasingly popular because it makes matrix computation suitable for large-scale datasets. Most previous AMM methods are based on the idea of random selection or random projection. In this paper, we propose a deterministic algorithm FD-AMM for computing an approximation to the product of two given matrices. Moreover, the algorithm works in a streaming manner. In particular, our approach is inspired by a recently proposed matrix sketching algorithm called Frequent Directions (FD). FD-AMM has stronger error bound than both random selection and random projection algorithms with respect to the same space complexity. Our approach also leads to an algorithm for computing the Canonical Correlation Analysis (CCA) of two matrices exactly in a streaming way, which takes less space than the classical method. Experimental results validate the effectiveness of our method.",1,0,0,adaptive algorithms+multiplication+random projections,
342,342,"Can your face detector do anti spoofing face presentation attack detection with a multi channel face detector. In a typical face recognition pipeline, the task of the face detector is to localize the face region. However, the face detector localizes regions that look like a face, irrespective of the liveliness of the face, which makes the entire system susceptible to presentation attacks. In this work, we try to reformulate the task of the face detector to detect real faces, thus eliminating the threat of presentation attacks. While this task could be challenging with visible spectrum images alone, we leverage the multi-channel information available from off the shelf devices (such as color, depth, and infrared channels) to design a multi-channel face detector. The proposed system can be used as a live-face detector obviating the need for a separate presentation attack detection module, making the system reliable in practice without any additional computational overhead. The main idea is to leverage a single-stage object detection framework, with a joint representation obtained from different channels for the PAD task. We have evaluated our approach in the multi-channel WMCA dataset containing a wide variety of attacks to show the effectiveness of the proposed framework.",1,0,0,face recognition+face detector+object detection,
343,343,"Globalphone pronunciation dictionaries in 20 languages. This paper describes the advances in the multilingual text and speech database GLOBALPHONE a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GLOBALPHONE was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GLOBALPHONE supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages. Very recently the GLOBALPHONE pronunciation dictionaries have been made available for research and commercial purposes by the European Language Resources Association (ELRA).",1,0,0,speech data+speech quality+speaker recognition,
344,344,"Attentron few shot text to speech utilizing attention based variable length embedding. On account of growing demands for personalization, the need for a so-called few-shot TTS system that clones speakers with only a few data is emerging. To address this issue, we propose Attentron, a few-shot TTS model that clones voices of speakers unseen during training. It introduces two special encoders, each serving different purposes. A fine-grained encoder extracts variable-length style information via an attention mechanism, and a coarse-grained encoder greatly stabilizes the speech synthesis, circumventing unintelligible gibberish even for synthesizing speech of unseen speakers. In addition, the model can scale out to an arbitrary number of reference audios to improve the quality of the synthesized speech. According to our experiments, including a human evaluation, the proposed model significantly outperforms state-of-the-art models when generating speech for unseen speakers in terms of speaker similarity and quality.",1,0,0,tts systems+human evaluation+speech synthesis system,
345,345,"Using an autoencoder in the design of an anomaly detector for smart manufacturing. Abstract According to the smart manufacturing paradigm, the analysis of assets’ time series with a machine learning approach can effectively prevent unplanned production downtimes by detecting assets’ anomalous operational conditions. To support smart manufacturing operators with no data science background, we propose an anomaly detection approach based on deep learning and aimed at providing a manageable machine learning pipeline and easy to interpret outcome. To do so we combine (i) an autoencoder, a deep neural network able to produce an anomaly score for each provided time series, and (ii) a discriminator based on a general heuristics, to automatically discern anomalies from regular instances. We prove the convenience of the proposed approach by comparing its performances against isolation forest with different case studies addressing industrial laundry assets’ power consumption and bearing vibrations.",1,0,0,anomaly detection+deep learning+neural networks,
346,346,"Knowledge flow improve upon your teachers. A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves 'knowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other 'knowledge exchange' methods.",1,0,0,tacit knowledge+explicit knowledge+reinforcement learning,
347,347,"An effective two branch model based deep network for single image deraining. Removing rain effects from an image is of importance for various applications such as autonomous driving, drone piloting, and photo editing. Conventional methods rely on some heuristics to handcraft various priors to remove or separate the rain effects from an image. Recent deep learning models are proposed to learn end-to-end methods to complete this task. However, they often fail to obtain satisfactory results in many realistic scenarios, especially when the observed images suffer from heavy rain. Heavy rain brings not only rain streaks but also haze-like effect caused by the accumulation of tiny raindrops. Different from the existing deep learning deraining methods that mainly focus on handling the rain streaks, we design a deep neural network by incorporating a physical raining image model. Specifically, in the proposed model, two branches are designed to handle both the rain streaks and haze-like effects. An additional submodule is jointly trained to finally refine the results, which give the model flexibility to control the strength of removing the mist. Extensive experiments on several datasets show that our method outperforms the state-of-the-art in both objective assessments and visual quality.",1,0,0,document images+neural networks+markov random fields,
348,348,"Bootstrap estimated uncertainty of the environment model for model based reinforcement learning. Model-based reinforcement learning (RL) methods attempt to learn a dynamics model to simulate the real environment and utilize the model to make better decisions. However, the learned environment simulator often has more or less model error which would disturb making decision and reduce performance. We propose a bootstrapped model-based RL method which bootstraps the modules in each depth of the planning tree. This method can quantify the uncertainty of environment model on different state-action pairs and lead the agent to explore the pairs with higher uncertainty to reduce the potential model errors. Moreover, we sample target values from their bootstrap distribution to connect the uncertainties at current and subsequent time-steps and introduce the prior mechanism to improve the exploration efficiency. Experiment results demonstrate that our method efficiently decreases model error and outperforms TreeQN and other stateof-the-art methods on multiple Atari games.",1,0,0,reinforcement learning+reinforcement learning method+multi-agent reinforcement learning,
349,349,"Learning agents in financial markets consensus dynamics on volatility. Black-Scholes (BS) is the standard mathematical model for European option pricing in financial markets. Option prices are calculated using an analytical formula whose main inputs are strike (at which price to exercise) and volatility. The BS framework assumes that volatility remains constant across all strikes, however, in practice it varies. How do traders come to learn these parameters? We introduce and analyze the convergence properties of natural models of learning agents, in which they update their beliefs about the true implied volatility based on the opinions of other traders.",1,0,0,retail price+financial market+learning agents,
350,350,"On real time detecting passenger flow anomalies. In large and medium-sized cities, detecting unusual changes of crowds of people on the streets is needed for public security, transportation management, emergency control, and terrorism prevention. As public transportation has the capability to bring a large number of people to an area in a short amount of time, real-time discovery of anomalies in passenger numbers is an effective way to detect crowd anomalies. In this paper, we devise an approach called Kochab. Kochab adopts a generative model and combines the prior knowledge about passenger flows. Hence, it can detect anomalies in the numbers of incoming and outgoing passengers within a certain time and spatial area, including anomalous events along with their durations and severities. Through well-designed inference algorithms, Kochab requires only a moderate amount of historical data to be sample data. As such, Kochab shows good performance in real time and makes prompt responses to user' s interactive analysis requests. In particular, based on the recognized anomalous events, we capture event patterns which give us hints to link to activities or status in cities. In addition, for the convenience of method evaluation and comparison, we create an open Stream Anomaly Benchmark on the basis of large-scale real-world data. This benchmark will prove useful for other researchers too. Using this benchmark, we compare Kochab with four other methods. The experimental results show that Kochab is sensitive to population flow anomalies and has superior accuracy in detecting anomalies in terms of precision, recall and the F1 score.",1,0,0,data stream+inference+inference algorithm,
351,351,"Distributed fair rate congestion control for vehicular networks. Vehicular ad hoc networks (VANETs) are self-organizing communication networks, which principally consist of vehicles that broadcast beacons with relevant real time traffic information. These continuous exchanges of information allow the development of applications that drastically improve the road traffic safety and efficiency. Such services suffer from network congestion problems when the road traffic density increases. This may cause VANET malfunction, and thus, the increase of hazardous road situations. In this study, we present a family of fully distributed intelligent light-weight congestion control algorithms (executed by each node), i.e., the Distributed Intelligent Fair Rate Adaptation (DIFRA) family. These methods accurately estimate the channel load in a distributed manner and dynamically adapt the beacon rate of each node. Experimental analyses show the effectiveness of DIFRA methods in increasing the amount of data exchanged between the vehicles and the balance in the channel usage, while avoiding network congestion.",1,0,0,self organizing+network architecture+rate adaptation,
352,352,"Show and speak directly synthesize spoken description of images. This paper proposes a new model, referred to as the show and speak (SAS) model that, for the first time, is able to directly synthesize spoken descriptions of images, bypassing the need for any text or phonemes. The basic structure of SAS is an encoder-decoder architecture that takes an image as input and predicts the spectrogram of speech that describes this image. The final speech audio is obtained from the predicted spectrogram via WaveNet. Extensive experiments on the public benchmark database Flickr8k demonstrate that the proposed SAS is able to synthesize natural spoken descriptions for images, indicating that synthesizing spoken descriptions for images while bypassing text and phonemes is feasible.",1,0,0,color images+reference image+digital image,
353,353,"Unified preview control for humanoid postural stability and upper limb interaction adaptation. This paper proposes a robust whole-body control formulation for biped balance in disturbed conditions by manipulation tasks. In order to include the effects of the interaction of the robot with its environment, required by the manipulation task in the balance control, we introduce a distributed preview control which captures both balance and manipulation behaviors and enables the regulation of the interaction impedance. The initial ZMP preview control is extended to take into account the disturbance resulting from the manipulation task and the preview control of adaptive impedances used to drive the upper-limbs. The resulting behavior is illustrated in a simple scenario. Its aptitude to dynamically extract an optimal control strategy improving tracking performances of both manipulation and balance tasks is also assessed when complex perturbations have to be compensated.",1,0,0,biped robot+optimal control+zmp,
354,354,"Soft robotic burrowing device with tip extension and granular fluidization. Mobile robots of all shapes and sizes move through the air, water, and over ground. However, few robots can move through the ground. Not only are the forces resisting movement much greater than in air or water, but the interaction forces are more complicated. Here we propose a soft robotic device that burrows through dry sand while requiring an order of magnitude less force than a similarly sized intruding body. The device leverages the principles of both tip-extension and granular fluidization. Like roots, the device extends from its tip; the principle of tip-extension eliminates skin drag on the sides of the body, because the body is stationary with respect to the medium. We implement this with an everting, pressure-driven thin film body. The second principle, granular fluidization, enables a granular medium to adopt a dynamic fluid-like state when pressurized fluid is passed through it, reducing the forces acting on an object moving through it. We realize granular fluidization with a flow of air through the core of the body that mixes with the medium at the tip. The proposed device could lead to applications such as search and rescue in mudslides or shallow subterranean exploration. Further, because it creates a physical conduit with its body, electrical lines, fluids, or even tools could be passed through this channel.",1,0,0,robots+mobile robots+autonomous mobile robot,
355,355,"Humanoid learns to detect its own hands. Robust object manipulation is still a hard problem in robotics, even more so in high degree-of-freedom (DOF) humanoid robots. To improve performance a closer integration of visual and motor systems is needed. We herein present a novel method for a robot to learn robust detection of its own hands and fingers enabling sensorimotor coordination. It does so solely using its own camera images and does not require any external systems or markers. Our system based on Cartesian Genetic Programming (CGP) allows to evolve programs to perform this image segmentation task in real-time on the real hardware. We show results for a Nao and an iCub humanoid each detecting its own hands and fingers.",1,0,0,generic programming+image segmentation+cartesian genetic programming,
356,356,"Practical and rigorous uncertainty bounds for gaussian process regression. Gaussian Process regression is a popular nonparametric regression method based on Bayesian principles that provides uncertainty estimates for its predictions. However, these estimates are of a Bayesian nature, whereas for some important applications, like learning-based control with safety guarantees, frequentist uncertainty bounds are required. Although such rigorous bounds are available for Gaussian Processes, they are too conservative to be useful in applications. This often leads practitioners to replacing these bounds by heuristics, thus breaking all theoretical guarantees. To address this problem, we introduce new uncertainty bounds that are rigorous, yet practically useful at the same time. In particular, the bounds can be explicitly evaluated and are much less conservative than state of the art results. Furthermore, we show that certain model misspecifications lead to only graceful degradation. We demonstrate these advantages and the usefulness of our results for learning-based control with numerical examples.",1,0,0,gaussians+gaussian processes+numerical example,
357,357,"Efficient stochastic gradient hard thresholding. Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods.",1,0,0,thresholding methods+morphological filtering+wavelet thresholding,
358,358,"Morphological query expansion and language filtering words for improving basque web retrieval. The experience of a user of major search engines or other web information retrieval services looking for information in the Basque language is far from satisfactory: they only return pages with exact matches but no inflections (necessary for an agglutinative language like Basque), many results in other languages (no search engine gives the option to restrict its results to Basque), etc. This paper proposes using morphological query expansion and language-filtering words in combination with the APIs of search engines as a very cost-effective solution to build appropriate web search services for Basque. The implementation details of the methodology (choosing the most appropriate language-filtering words, the number of them, the most frequent inflections for the morphological query expansion, etc.) have been specified by corpora-based studies. The improvements produced have been measured in terms of precision and recall both over corpora and real web searches. Morphological query expansion can improve recall up to 47 % and language-filtering words can raise precision from 15 % to around 90 %, although with a loss in recall of about 30–35 %. The proposed methodology has already been successfully used in the Basque search service Elebila (http://www.elebila.eu) and the web-as-corpus tool CorpEus (http://www.corpeus.org), and the approach could be applied to other morphologically rich or under-resourced languages as well.",1,0,0,information retrieval+web searches+query expansion,
359,359,"Prof life log audio environment detection for naturalistic audio streams. In this study, we develop a new system for real world audio environment matching. Environment detection within unknown audio streams requires a system that operates in an unsupervised manner since it will be faced with unknown environments without prior information. In addition, the overall solution should be computationally efficient for large audio collection. In the proposed approach, a Gaussian mixture model(GMM) is trained on large amounts of unlabeled audio data and used as a background acoustic model. Subsequently, an acoustic signature vector (ASV) is computed for each environment. Here, the ASV vector is designed to capture the unique acoustic characteristics of an environment. Using the ASV vectors, we demonstrate that it is possible to compute an effective similarity measure between two acoustic environments. We demonstrate the performance of the proposed system on real-world audio data, and compare it to a traditional GMM-UBM (Universal Background Model) system. Experiments show that our system achieves an equal error rate (EER) that is +35% better than a baseline GMM-UBM system.",1,0,0,prior information+background model+acoustic model,
360,360,"Learning to rank with cross entropy. Learning to rank algorithms are usually grouped into three types: the point wise approach, the pairwise approach, and the listwise approach, according to the input spaces. Much of the prior work is based on the three approaches to learn the ranking model to predict the relevance of a document to a query. In this paper, we focus on the problem of constructing new input space based on groups of documents with the same relevance judgment. A novel approach is proposed based on cross entropy to improve the existing ranking method. The experimental results show that our approach leads to significant improvements in retrieval effectiveness.",1,0,0,feature space+relevance judgment+retrieval effectiveness,
361,361,"A multi branch deep learning network for automated detection of covid 19. Fast and affordable solutions for COVID-19 testing are necessary to contain the spread of the global pandemic and help relieve the burden on medical facilities. Currently, limited testing locations and expensive equipment pose difficulties for individuals seeking testing, especially in low-resource settings. Researchers have successfully presented models for detecting COVID-19 infection status using audio samples recorded in clinical settings, suggesting that audio-based Artificial Intelligence models can be used to identify COVID-19. Such models have the potential to be deployed on smartphones for fast, widespread, and low-resource testing. However, while previous studies have trained models on cleaned audio samples collected mainly from clinical settings, audio samples collected from average smartphones may yield suboptimal quality data that is different from the clean data that models were trained on. This discrepancy may add a bias that affects COVID-19 status predictions. To tackle this issue, we propose a multi-branch deep learning network that is trained and tested on crowdsourced data where most of the data has not been manually processed and cleaned. Furthermore, the model achieves state-of-art results for the COUGHVID dataset. After breaking down results for each category, we have shown an AUC of 0.99 for audio samples with COVID-19 positive labels. © 2021 ISCA",1,0,0,standard model+stochastic processes+deep learning,
362,362,"Test time training with self supervision for generalization under distribution shifts. In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a single unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts.",1,0,0,classification models+test samples+image classification,
363,363,"Robustness guarantees for bayesian inference with gaussian processes. Bayesian inference and Gaussian processes are widely used in applications ranging from robotics and control to biological systems. Many of these applications are safety-critical and require a characterization of the uncertainty associated with the learning model and formal guarantees on its predictions. In this paper we define a robustness measure for Bayesian inference against input perturbations, given by the probability that, for a test point and a compact set in the input space containing the test point, the prediction of the learning model will remain δ−close for all the points in the set, for δ > 0. Such measures can be used to provide formal probabilistic guarantees for the absence of adversarial examples. By employing the theory of Gaussian processes, we derive upper bounds on the resulting robustness by utilising the Borell-TIS inequality, and propose algorithms for their computation. We evaluate our techniques on two examples, a GP regression problem and a fully-connected deep neural network, where we rely on weak convergence to GPs to study adversarial examples on the MNIST dataset.",1,0,0,gaussians+neural networks+gaussian processes,
364,364,"Random forest classification based acoustic event detection utilizing contextual information and bottleneck features. Abstract The variety of event categories and event boundary information have resulted in limited success for acoustic event detection systems. To deal with this, we propose to utilize the long contextual information, low-dimensional discriminant global bottleneck features and category-specific bottleneck features. By concatenating several adjacent frames together, the use of contextual information makes it easier to cope with acoustic signals with long duration. Global and category-specific bottleneck features can extract the prior knowledge of the event category and boundary, which is ideally matched by the task of an event detection system. Evaluations on the UPC-TALP and ITC-IRST databases of highly variable acoustic events demonstrate the effectiveness of the proposed approaches by achieving a 5.30% and 4.44% absolute error rate improvement respectively compared to the state of art technique.",1,0,0,random forests+acoustics+frames,
365,365,"Discovering combos in fighting games with evolutionary algorithms. In fighting games, players can perform many different actions at each instant of time, leading to an exponential number of possible sequences of actions. Some of these combinations can lead to unexpected behaviors, which can compromise the game design. One example of these unexpected behaviors is the occurrence of long or infinite combos, a long sequence of actions that does not allow any reactions from the opponent. Finding these sequences is essential to ensure fairness in fighting games, but evaluating all possible sequences is a time consuming task. In this paper, we propose the use of an evolutionary algorithm to find combos on a fighting game. The main idea is to use a genetic algorithm to evolve a population composed of sequences of inputs and, using an adequate fitness function, select the ones that are more suitable to be considered combos. We performed a series of experiments and the results show that the proposed approach was not only successful in finding combos, managing to find unexpected sequences, but also superior to previous methods.",1,0,0,evolutionary computation+multi-objective evolutionary algorithms+differential evolution,
366,366,"Synchronization error estimation and controller design for delayed lur e systems with parameter mismatches. This paper investigates the problem of master-slave synchronization of two delayed Lur'e systems in the presence of parameter mismatches. First, by analyzing the corresponding synchronization error system, synchronization with an error level, which is referred to as quasi-synchronization, is established. Some delay-dependent quasi-synchronization criteria are derived. An estimation of the synchronization error bound is given, and an explicit expression of error levels is obtained. Second, sufficient conditions on the existence of feedback controllers under a predetermined error level are provided. The controller gains are obtained by solving a set of linear matrix inequalities. Finally, a delayed Chua's circuit is chosen to illustrate the effectiveness of the derived results.",1,0,0,linear matrix inequalities+delay-dependent+linear matrix,
367,367,"Nrtr a no recurrence sequence to sequence model for scene text recognition. Scene text recognition has attracted a great many researches due to its importance to various applications. Existing methods mainly adopt recurrence or convolution based networks. Though have obtained good performance, these methods still suffer from two limitations: slow training speed due to the internal recurrence of RNNs, and high complexity due to stacked convolutional layers for long-term feature extraction. This paper, for the first time, proposes a no-recurrence sequence-to-sequence text recognizer, named NRTR, that dispenses with recurrences and convolutions entirely. NRTR follows the encoder-decoder paradigm, where the encoder uses stacked self-attention to extract image features, and the decoder applies stacked self-attention to recognize texts based on encoder output. NRTR relies solely on self-attention mechanism thus could be trained with more parallelization and less complexity. Considering scene image has large variation in text and background, we further design a modality-transform block to effectively transform 2D input images to 1D sequences, combined with the encoder to extract more discriminative features. NRTR achieves state-of-the-art or highly competitive performance on both regular and irregular benchmarks, while requires only a small fraction of training time compared to the best model from the literature (at least 8 times faster).",1,0,0,text recognition+text detection+scene text,
368,368,"Deep learning models delineates multiple nuclear phenotypes in h e stained histology sections. Nuclear segmentation is an important step for profiling aberrant regions of histology sections. However, segmentation is a complex problem as a result of variations in nuclear geometry (e.g., size, shape), nuclear type (e.g., epithelial, fibroblast), and nuclear phenotypes (e.g., vesicular, aneuploidy). The problem is further complicated as a result of variations in sample preparation. It is shown and validated that fusion of very deep convolutional networks overcomes (i) complexities associated with multiple nuclear phenotypes, and (ii) separation of overlapping nuclei. The fusion relies on integrating of networks that learn region- and boundary-based representations. The system has been validated on a diverse set of nuclear phenotypes that correspond to the breast and brain histology sections.",1,0,0,deep learning+automatic segmentations+segmentation methods,
369,369,"Extrinsic and temporal calibration of automotive radar and 3d lidar. While automotive radars are widely used in most assisted and autonomous driving systems, only a few works were proposed to tackle the calibration problems of automotive radars with other perception sensors. One of the key calibration challenges of automotive planar radars with other sensors is the missing elevation angle in 3D space. In this paper, extrinsic calibration is accomplished based on the observation that the radar cross section (RCS) measurements have different value distributions across radar’s vertical field of view. An approach to accurately and efficiently estimate the time delay between radars and LiDARs based on spatial-temporal relationships of calibration target positions is proposed. In addition, a localization method for calibration target detection and localization in pre-built maps is proposed to tackle insufficient LiDAR measurements on calibration targets. The experimental results show the feasibility and effectiveness of the proposed Radar-LiDAR extrinsic and temporal calibration approaches.",1,0,0,autonomous driving+target detection+radar cross section,
370,370,"Legibility and aesthetic analysis of handwriting. This paper deals with computer-based cognitive analysis towards legibility and aesthetics of a handwritten document. The legible text creates a human perception that the writing can be read effortlessly because of its orthographic clarity. The aesthetic property relates to the beautiful appearance of a handwritten document. In this study, we deal with these properties on offline Bengali handwriting. We formulate both legibility and aesthetic analysis tasks as machine learning problems supervised by the human cognitive system. We employ automatically derived feature-based recurrent neural networks to investigate writing legibility. For aesthetics evaluation, we employ hand-crafted feature-based support vector machines (SVMs). We have collected contemporary Bengali handwritings, on which the subjective legibility and aesthetic scores are provided by human readers. On this corpus containing legibility and aesthetic ground-truth information, we executed our experiments. The experimental results obtained on various handwritings are encouraging.",1,0,0,neural networks+recurrent neural networks+handwritten document,
371,371,"Word like character n gram embedding. We propose a new word embedding method called word-like character n-gram embedding, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus. However, its n-gram vocabulary tends to contain too many non-word n-grams. We solved this problem by introducing an idea of expected word frequency. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset). Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.",1,0,0,vocabulary+word embedding+word segmentation,
372,372,"An unknown environment exploration strategy for swarm robotics based on brain storm optimization algorithm. In this paper, a distributed algorithm used to solve the swarm robotic exploration problem with communication constraint conditions is proposed. The swarm robotics exploration problem is represented as an optimization problem in this paper, and a modified Brain Storm Optimization algorithm is utilized to solve this problem. This swarm robotic exploration algorithm has several advantages compared to traditional strategies. Firstly, it is fully decentralized, which suits for swarm robotic application. What is more, it is easy to combine this algorithm with many existing frontier-based exploration methods to improve robots’ cooperation ability. Finally, the proposed method has been tested in several different simulation environments, and the experimental results demonstrate its advantages over other approaches.",1,0,0,optimization+swarm robotics+communication constraints,
373,373,"Learning more with less conditional pggan based data augmentation for brain metastases detection using highly rough annotation on mr images. Accurate Computer-Assisted Diagnosis, associated with proper data wrangling, can alleviate the risk of overlooking the diagnosis in a clinical environment. Towards this, as a Data Augmentation (DA) technique, Generative Adversarial Networks (GANs) can synthesize additional training data to handle the small/fragmented medical imaging datasets collected from various scanners; those images are realistic but completely different from the original ones, filling the data lack in the real image distribution. However, we cannot easily use them to locate disease areas, considering expert physicians' expensive annotation cost. Therefore, this paper proposes Conditional Progressive Growing of GANs (CPGGANs), incorporating highly-rough bounding box conditions incrementally into PGGANs to place brain metastases at desired positions/sizes on 256 × 256 Magnetic Resonance (MR) images, for Convolutional Neural Network-based tumor detection; this first GAN-based medical DA using automatic bounding box annotation improves the training robustness. The results show that CPGGAN-based DA can boost 10% sensitivity in diagnosis with clinically acceptable additional False Positives. Surprisingly, further tumor realism, achieved with additional normal brain MR images for CPGGAN training, does not contribute to detection performance, while even three physicians cannot accurately distinguish them from the real ones in Visual Turing Test.",1,0,0,generative adversarial networks+data augmentation+computer assisted diagnosis,
374,374,"Sequential independent component analysis density estimation. A problem of multivariate probability density function estimation by exploiting linear independent components analysis (ICA) is addressed. Historically, ICA density estimation was initially proposed under the name projection pursuit density estimation (PPDE) and two basic methods, named forward and backward, were published. We derive a modification of the forward PPDE method, which avoids a computationally demanding optimization involving Monte Carlo sampling of the original method. The results of the experiments show that the proposed method presents an attractive choice for density estimation, which is pronounced for a small number of training observations. Under such conditions, our method usually outperforms model-based Gaussian mixture model. We also found that our method obtained better results than the backward PPDE methods in the situation of nonfactorizable underlying density functions. The proposed method has demonstrated a competitive performance compared with the support vector machine and the extreme learning machine in some real classification tasks.",1,0,0,extreme learning machine+gaussian mixtures+gaussians,
375,375,"Copy and paste networks for deep video inpainting. We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes affine matrices between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos.",1,0,0,line detection+optimization+deep learning,
376,376,"Entity tracking improves cloze style reading comprehension. Recent work has improved on modeling for reading comprehension tasks with simple approaches such as the Attention Sum-Reader; however, automatic systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset by 8 pts, particularly on difficult entity examples. We also effectively match the performance of more complicated models on the named entity portion of the CBT dataset.",1,0,0,tracking algorithm+object tracking+named entity recognition,
377,377,"Effective decomposition of large scale separable continuous functions for cooperative co evolutionary algorithms. In this paper we investigate the performance of cooperative co-evolutionary (CC) algorithms on large-scale fully-separable continuous optimization problems. We have shown that decomposition can have significant impact on the performance of CC algorithms. The empirical results show that the subcomponent size should be chosen small enough so that the subcomponent size is within the capacity of the subcomponent optimizer. In practice, determining the optimal size is difficult. Therefore, adaptive techniques are desired by practitioners. Here we propose an adaptive method, MLSoft, that uses widely-used techniques in reinforcement learning such as the value function method and softmax selection rule to adapt the subcomponent size during the optimization process. The experimental results show that MLSoft is significantly better than an existing adaptive algorithm called MLCC on a set of large-scale fully-separable problems.",1,0,0,reinforcement learning+evolutionary algorithms+value functions,
378,378,"Gaussian based particle swarm optimisation and statistical clustering for feature selection. Feature selection is an important but difficult task in classification, which aims to reduce the number of features and maintain or even increase the classification accuracy. This paper proposes a new particle swarm optimisation (PSO) algorithm using statistical clustering information to solve feature selection problems. Based on Gaussian distribution, a new updating mechanism is developed to allow the use of the clustering information during the evolutionary process of PSO based on which a new algorithm (GPSO) is developed. The proposed algorithm is examined and compared with two traditional algorithms and a PSO based algorithm which does not use clustering information on eight benchmark datasets of varying difficulty. The results show that GPSO can be successfully used for feature selection to reduce the number of features and achieve similar or even better classification performance than using all features. Meanwhile, it achieves better performance than the two traditional feature selection algorithms. It maintains the classification performance achieved by the standard PSO for feature selection algorithm, but significantly reduces the number of features and the computational cost.",1,0,0,particle swarm optimization (pso)+gaussian distribution+standard pso,
379,379,"A k prototypes algorithm based on adaptive determination of the initial centroids. K-prototypes is an algorithm that deals with mixed data clustering. However, the clustering parameter k needs to be manually set and the initial centroids are randomly selected, therefore, it can lead to the blindness of cluster number as well as the problems of low clustering accuracy and unstable clustering results. Regarding the issue above, this paper proposes a strategy to determine the initial centroids adaptively based on density and distance, which can determine the number of clusters adaptively and choose the initial centroids better than the original algorithm. According to the experiments of UCI datasets, this algorithm is superior to the traditional k-prototypes algorithm and fuzzy k-prototypes algorithm in clustering quality and stability.",1,0,0,cluster centers+clustering quality+cluster numbers,
380,380,"Model based identification of anatomical boundary conditions in living tissues. In this paper, we present a novel method dealing with the identification of boundary conditions of a deformable organ, a particularly important step for the creation of patient-specific biomechani-cal models of the anatomy. As an input, the method requires a set of scans acquired in different body positions. Using constraint-based finite element simulation, the method registers the two data sets by solving an optimization problem minimizing the energy of the deformable body while satisfying the constraints located on the surface of the registered organ. Once the equilibrium of the simulation is attained (i.e. the organ registration is computed), the surface forces needed to satisfy the constraints provide a reliable estimation of location, direction and magnitude of boundary conditions applied to the object in the deformed position. The method is evaluated on two abdominal CT scans of a pig acquired in flank and supine positions. We demonstrate that while computing a physically admissible registration of the liver, the resulting constraint forces applied to the surface of the liver strongly correlate with the location of the anatomical boundary conditions (such as contacts with bones and other organs) that are visually identified in the CT images.",1,0,0,ct image+optimization problems+optimization,
381,381,"Embedding time expressions for deep temporal ordering models. Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these models often overlook explicit temporal signals, such as dates and time windows. Rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes’ interactions with events and are hard to integrate with the distributed representations of neural net models. In this paper, we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes. We generate synthetic data consisting of pairs of timexes, then train a character LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.",1,0,0,stochastic processes+temporal correlations+long short term memory neural networks,
382,382,"On decomposing the proximal map. The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory.",1,0,0,closed-form solution+closed form solutions+regularization,
383,383,"Analyzing and storing network intrusion detection data using bayesian coresets a preliminary study in offline and streaming settings. In this paper we offer a preliminary study of the application of Bayesian coresets to network security data. Network intrusion detection is a field that could take advantage of Bayesian machine learning in modelling uncertainty and managing streaming data; however, the large size of the data sets often hinders the use of Bayesian learning methods based on MCMC. Limiting the amount of useful data is a central problem in a field like network traffic analysis, where large amount of redundant data can be generated very quickly via packet collection. Reducing the number of samples would not only make learning more feasible, but would also contribute to reduce the need for memory and storage. We explore here the use of Bayesian coresets, a technique that reduces the amount of data samples while guaranteeing the learning of an accurate posterior distribution using Bayesian learning. We analyze how Bayesian coresets affect the accuracy of learned models, and how time-space requirements are traded-off, both in a static scenario and in a streaming scenario.",1,0,0,mcmc+bayesian learning+posterior distributions,
384,384,"A spatial oligopolistic electricity model under uncertain demands. This paper mainly investigates a single-period spatial oligopolistic model of electricity generators under uncertain demands. We maximize the $$\beta $$-optimistic value of each generator’s uncertain profit function in this game. Emphasis is first put on computing the $$\beta $$-optimistic value of the uncertain profit function through the inverse uncertainty distribution of the uncertain variable in the inverse demand function. We later focus on transforming the single-period spatial oligopolistic game, which is a generalized Nash equilibrium (short for GNE ) problem, into a variational inequality problem. Then the model is applied to simulate the transmission price of the electricity system of America, which reveals that our model is effective.",1,0,0,electricity+expected profits+nash equilibrium,
385,385,"The empathy machine. Empathy is a key component of interpersonal interactions that is often neglected by modern communication technologies. This paper presents the theoretical basis, prototype, and preliminary user testing for an application of affective computing in mediating live human-to-human interactions. The proposed system uses facial expression recognition to identify the emotional state of a user's conversation partner. It algorithmically generates emotional music to match the expressive state of the partner and plays the music to the user in a non-disruptive manner. Preliminary user studies indicate that the prototype system can generate music that users reliably associate with the emotions of anger, happiness, fear, and sadness and that the presence of emotional music augments the emotional response generated by visual cues.",1,0,0,facial expression analysis+facial expression recognition+expression recognition,
386,386,"Evaluation of a predictive approach in steering the human locomotion via haptic feedback. In this paper, we present a haptic guidance policy to steer the user along predefined paths, and we evaluate a predictive approach to compensate actuation delays that humans have when they are guided along a given trajectory via sensory stimuli. The proposed navigation policy exploits the nonholonomic nature of human locomotion in goal directed paths, which leads to a very simple guidance mechanism. The proposed method has been evaluated in a real scenario where seven human subjects were asked to walk along a set of predefined paths, and were guided via vibrotactile cues. Their poses as well as the related distances from the path have been recorded using an accurate optical tracking system. Results revealed that an average error of 0.24 m is achieved by using the proposed haptic policy, and that the predictive approach does not bring significant improvements to the path following problem for what concerns the distance error. On the contrary, the predictive approach achieved a definitely lower activation time of the haptic interfaces.",1,0,0,haptic guidance+tracking system+human locomotions,
387,387,"Performance comparison of iot communication protocols. The amount of systems based on Internet of Things (IoT) has grown at an unprecedented rate over the last years and such an expansion tends to continue. As a consequence, billions of devices are expected to be deployed on diverse industries (e.g., healthcare, automotive) during the next decade. Due to its heterogeneity, the communication of IoT devices is a prominent system function and, thus, distinct communication protocols have been proposed for these systems. This paper presents a performance evaluation of IoT communication protocols for the application layer: AMQP, CoAP, and MQTT. Throughput, message size, and packet loss are the adopted metrics and experiments indicate CoAP protocol provides the best results.",1,0,0,communication protocols+communication+packet loss,
388,388,"Adaptive convolution for multi relational learning. We consider the problem of learning distributed representations for entities and relations of multi-relational data so as to predict missing links therein. Convolutional neural networks have recently shown their superiority for this problem, bringing increased model expressiveness while remaining parameter efficient. Despite the success, previous convolution designs fail to model full interactions between input entities and relations, which potentially limits the performance of link prediction. In this work we introduce ConvR, an adaptive convolutional network designed to maximize entity-relation interactions in a convolutional fashion. ConvR adaptively constructs convolution filters from relation representations, and applies these filters across entity representations to generate convolutional features. As such, ConvR enables rich interactions between entity and relation representations at diverse regions, and all the convolutional features generated will be able to capture such interactions. We evaluate ConvR on multiple benchmark datasets. Experimental results show that: (1) ConvR performs substantially better than competitive baselines in almost all the metrics and on all the datasets; (2) Compared with state-of-the-art convolutional models, ConvR is not only more effective but also more efficient. It offers a 7% increase in MRR and a 6% increase in Hits@10, while saving 12% in parameter storage.",1,0,0,convolutional neural networks+link prediction+relational learning,
389,389,"A novel connectivity preserving control design for rendezvous problem of networked uncertain nonlinear systems. This article proposes a novel and robust connectivity-preserving rendezvous control design for a group of uncertain nonlinear multiagent systems with communication constraint where each agent has a limited sensing range. The control design can work under the assumption that the communication network is initially connected and is characterized by two distinguishing features. First, a new potential function is provided not only to maintain the existing and newly added links by the hysteresis rule but also to overcome the difficulty imposed by the nonlinear terms from system dynamics. Second, by constructing a series of lemmas, a connectivity-preserving stabilizing control law is presented to solve the robust stabilization problem with connectivity preservation for a time-varying nonlinear system, which is a special case of the augmented system with both dynamic and static uncertainties obtained via internal model design. After further incorporating the adaptive control technique, regardless of uncertain parameters and external disturbances in the multiple nonlinear subsystems, the leader-following rendezvous with connectivity preservation problem is finally solved by a distributed connectivity-preserving controller with parameter update law.",1,0,0,uncertain nonlinear systems+feedback linearization+robust stabilization,
390,390,"Multiclass from binary expanding one versus all one versus one and ecoc based approaches. Recently, there has been a lot of success in the development of effective binary classifiers. Although many statistical classification techniques have natural multiclass extensions, some, such as the support vector machines, do not. The existing techniques for mapping multiclass problems onto a set of simpler binary classification problems run into serious efficiency problems when there are hundreds or even thousands of classes, and these are the scenarios where this paper's contributions shine. We introduce the concept of correlation and joint probability of base binary learners. We learn these properties during the training stage, group the binary leaner's based on their independence and, with a Bayesian approach, combine the results to predict the class of a new instance. Finally, we also discuss two additional strategies: one to reduce the number of required base learners in the multiclass classification, and another to find new base learners that might best complement the existing set. We use these two new procedures iteratively to complement the initial solution and improve the overall performance. This paper has two goals: finding the most discriminative binary classifiers to solve a multiclass problem and keeping up the efficiency, i.e., small number of base learners. We validate and compare the method with a diverse set of methods of the literature in several public available datasets that range from small (10 to 26 classes) to large multiclass problems (1000 classes) always using simple reproducible scenarios.",1,0,0,initial solution+multi-class classification+multi-class problems,
391,391,"Tafssl task adaptive feature sub space learning for few shot classification. The field of Few-Shot Learning (FSL), or learning from very few (typically $1$ or $5$) examples per novel class (unseen during training), has received a lot of attention and significant performance advances in the recent literature. While number of techniques have been proposed for FSL, several factors have emerged as most important for FSL performance, awarding SOTA even to the simplest of techniques. These are: the backbone architecture (bigger is better), type of pre-training on the base classes (meta-training vs regular multi-class, currently regular wins), quantity and diversity of the base classes set (the more the merrier, resulting in richer and better adaptive features), and the use of self-supervised tasks during pre-training (serving as a proxy for increasing the diversity of the base set). In this paper we propose yet another simple technique that is important for the few shot learning performance - a search for a compact feature sub-space that is discriminative for a given few-shot test task. We show that the Task-Adaptive Feature Sub-Space Learning (TAFSSL) can significantly boost the performance in FSL scenarios when some additional unlabeled data accompanies the novel few-shot task, be it either the set of unlabeled queries (transductive FSL) or some additional set of unlabeled data samples (semi-supervised FSL). Specifically, we show that on the challenging miniImageNet and tieredImageNet benchmarks, TAFSSL can improve the current state-of-the-art in both transductive and semi-supervised FSL settings by more than $5\%$, while increasing the benefit of using unlabeled data in FSL to above $10\%$ performance gain.",1,0,0,adaptive algorithms+video shots+classification models,
392,392,"Altair supervised methodology to obtain retinal vessels caliber. A back of the eye examination allows performing a noninvasive evaluation of the retinal microcirculation, as well as of the vascular damage induced by multiple cardiovascular risk factors. The objective of this work is to study the existing needs to lead to the development and validation (reliability and validity) of a methodology able to extract all the information from the images of the back of the eye to solve the studied needs. Its development will subsequently allow analyzing its utility in various clinical environments. Currently there are different works that evaluate the thickness of the retinal veins and arteries, but they require either full intervention by an observer or no intervention at all, so when facing incorrect analysis (none of them achieves a 100 % accuracy in automatic analysis) erroneous results can be a serious problem when drawing conclusions. The proposed solution refers to the second group (automatic), but providing a supervisor the possibility to interfere with the analysis when any kind of error is produced, which ideally will not happen many times. Thanks to this the possible subjectivity that can be introduced by the supervisor does not affect the final result of the analysis.",1,0,0,structural damages+observer+correlation analysis,
393,393,"You re making me depressed leveraging texts from contact subsets to predict depression. Depression, a prevalent and debilitating mental illness, is frequently undiagnosed. Diagnosis is an important step towards treatment. Currently screening tools, such as the Patient Health Questionnaire-9 (PHQ-9), require patient input. Many studies have used a variety of data types and features to predict depression scores for individuals. In this study, we focus on the predictive ability of a single under-utilized modality indicative of the impact of social interactions: received text messages. Our approach encompasses creating subsets of influential contacts for each participant and engineering features from the text messages of those contact subsets. Overall, our study demonstrates that received text communications are a promising modality when predicting depression scores. Specifically, we found that the F1 score of Gaussian Naive Bayes models leveraging just the text messages from a subset of top contacts performed statistically significantly better by 13.2 percent than the models leveraging text messages from all contacts.",1,0,0,communication+naive bayes+gaussians,
394,394,"Toward the automated analysis of complex diseases in genome wide association studies using genetic programming. Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods. Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets. We demonstrate TPOT-MDR's capabilities using a combination of simulated and real world data sets from human genetics and find that TPOT-MDR significantly outperforms modern machine learning methods such as logistic regression and eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's ability to produce a high-accuracy solution that is also easily interpretable.",1,0,0,genetic programming+generic programming+evolutionary algorithms,
395,395,"Data augmentation instead of explicit regularization. Contrary to most machine learning models, modern deep artificial neural networks typically include multiple components that contribute to regularization. Despite the fact that some (explicit) regularization techniques, such as weight decay and dropout, require costly fine-tuning of sensitive hyperparameters, the interplay between them and other elements that provide implicit regularization is not well understood yet. Shedding light upon these interactions is key to efficiently using computational resources and may contribute to solving the puzzle of generalization in deep learning. Here, we first provide formal definitions of explicit and implicit regularization that help understand essential differences between techniques. Second, we contrast data augmentation with weight decay and dropout. Our results show that visual object categorization models trained with data augmentation alone achieve the same performance or higher than models trained also with weight decay and dropout, as is common practice. We conclude that the contribution on generalization of weight decay and dropout is not only superfluous when sufficient implicit regularization is provided, but also such techniques can dramatically deteriorate the performance if the hyperparameters are not carefully tuned for the architecture and data set. In contrast, data augmentation systematically provides large generalization gains and does not require hyperparameter re-tuning. In view of our results, we suggest to optimize neural networks without weight decay and dropout to save computational resources, hence carbon emissions, and focus more on data augmentation and other inductive biases to improve performance and robustness.",1,0,0,regularization technique+data augmentation+regularization,
396,396,"On the appropriateness of complex valued neural networks for speech enhancement. Although complex-valued neural networks (CVNNs) â?? networks which can operate with complex arithmetic â?? have been around for a while, they have not been given reconsideration since the breakthrough of deep network architectures. This paper presents a critical assessment whether the novel tool set of deep neural networks (DNNs) should be extended to complex-valued arithmetic. Indeed, with DNNs making inroads in speech enhancement tasks, the use of complex-valued input data, specifically the short-time Fourier transform coefficients, is an obvious consideration. In particular when it comes to performing tasks that heavily rely on phase information, such as acoustic beamforming, complex-valued algorithms are omnipresent. In this contribution we recapitulate backpropagation in CVNNs, develop complex-valued network elements, such as the split-rectified non-linearity, and compare real- and complex-valued networks on a beamforming task. We find that CVNNs hardly provide a performance gain and conclude that the effort of developing the complex-valued counterparts of the building blocks of modern deep or recurrent neural networks can hardly be justified.",1,0,0,short time fourier transforms+backpropagation algorithm+speech enhancement,
397,397,"D 2 mopso multi objective particle swarm optimizer based on decomposition and dominance. D2MOPSO is a multi-objective particle swarm optimizer that incorporates the dominance concept with the decomposition approach. Whilst decomposition simplifies the multi-objective problem (MOP) by rewriting it as a set of aggregation problems, solving these problems simultaneously, within the PSO framework, might lead to premature convergence because of the leader selection process which uses the aggregation value as a criterion. Dominance plays a major role in building the leader's archive allowing the selected leaders to cover less dense regions avoiding local optima and resulting in a more diverse approximated Pareto front. Results from 10 standard MOPs show D2MOPSO outperforms two state-of-the-art decomposition based evolutionary methods.",1,0,0,pareto front+premature convergence+particle swarm optimizers,
398,398,"Automatic keyword extraction on twitter. In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.",1,0,0,social media+twitter+micro-blog,
399,399,"Data augmentation for discrimination prevention and bias disambiguation. Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ""ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.",1,0,0,data points+data augmentation+submodularity,
400,400,"Local discriminative based sparse subspace learning for feature selection. Abstract Subspace learning is a matrix decomposition method. Some algorithms apply subspace learning to feature selection, but they ignore the local discriminative information contained in data. In this paper, we propose a new unsupervised feature selection algorithm to address this issue, which is called local discriminative based sparse subspace learning for feature selection (LDSSL). We first introduce a local discriminant model in our feature selection framework of subspace learning. This model preserves both the local discriminant structure and local geometric structure of the data, simultaneously. It can not only improve the discriminate ability of the algorithm, but also utilize the local geometric structure information contained in data. Local discriminant model is a linear model, which cannot deal with nonlinear data effectively. Therefore, we need to kernelize the local discriminant model to get a nonlinear version. We next introduce the L1-norm to constrain the feature selection matrix, and this can ensure the sparsity of the feature selection matrix and improve the algorithm's discriminate ability. Then we give the objective function, convergence proof and iterative update rules of the algorithm. We compare LDSSL with eight state-of-the-art algorithms on six datasets. The experimental results show that LDSSL is more effective than eight other feature selection algorithms.",1,0,0,fisher discriminant analysis+sparse approximations+subspace learning,
401,401,"Dynamic characteristic of a multiple chaotic neural network and its application. Based on chaotic neural network, a multiple chaotic neural network algorithm combining two different chaotic dynamics sources in each neuron is proposed. With the effect of self-feedback connection and non-linear delay connection weight, the new algorithm can contain more powerful chaotic dynamics to search the solution domain globally in the beginning searching period. By analyzing the dynamic characteristic and the influence of cooling schedule in simulated annealing, a flexible parameter tuning strategy being able to promote chaotic dynamics convergence quickly is introduced into our algorithm. We show the effectiveness of the new algorithm in two difficult combinatorial optimization problems, i.e., a traveling salesman problem and a maximum clique problem.",1,0,0,optimization problems+optimization+neural networks,
402,402,"Ucolorado_som extraction of drug drug interactions from biomedical text using knowledge rich and knowledge poor features. In this paper, we present our approach to SemEval-2013 Task 9.2. It is a feature rich classification using LIBSVM for Drug-Drug Interactions detection in the BioMedical domain. The features are extracted considering morphosyntactic, lexical and semantic concepts. Tools like openDMAP and TEES are used to extract semantic concepts from the corpus. The best F-score that we got for DrugDrug Interaction (DDI) detection is 50% and 61% and the best F-score for DDI detection and classification is 34% and 48% for test and development data respectively.",1,0,0,semantic gap+classification models+biomedical text,
403,403,"Individual based stability in hedonic diversity games. In hedonic diversity games (HDGs), recently introduced by Bredereck, Elkind, and Igarashi (2019), each agent belongs to one of two classes (men and women, vegetarians and meat-eaters, junior and senior researchers), and agents' preferences over coalitions are determined by the fraction of agents from their class in each coalition. Bredereck et al. show that while an HDG may fail to have a Nash stable (NS) or a core stable (CS) outcome, every HDG in which all agents have single-peaked preferences admits an individually stable (IS) outcome, which can be computed in polynomial time. In this work, we extend and strengthen these results in several ways. First, we establish that the problem of deciding if an HDG has an NS outcome is NP-complete, but admits an XP algorithm with respect to the size of the smaller class. Second, we show that, in fact, all HDGs admit IS outcomes that can be computed in polynomial time; our algorithm for finding such outcomes is considerably simpler than that of Bredereck et al. We also consider two ways of generalizing the model of Bredereck et al. to k ≥ 2 classes. We complement our theoretical results by empirical analysis, comparing the IS outcomes found by our algorithm, the algorithm of Bredereck et al. and a natural better-response dynamics.",1,0,0,software agents+intelligent agents+theoretical result,
404,404,"Studying human ai collaboration protocols the case of the kasparov s law in radiological double reading. The integration of Artificial Intelligence into medical practices has recently been advocated for the promise to bring increased efficiency and effectiveness to these practices. Nonetheless, little research has so far been aimed at understanding the best human-AI interaction protocols in collaborative tasks, even in currently more viable settings, like independent double-reading screening tasks. To this aim, we report about a retrospective case–control study, involving 12 board-certified radiologists, in the detection of knee lesions by means of Magnetic Resonance Imaging, in which we simulated the serial combination of two Deep Learning models with humans in eight double-reading protocols. Inspired by the so-called Kasparov’s Laws, we investigate whether the combination of humans and AI models could achieve better performance than AI models alone, and whether weak reader, when supported by fit-for-use interaction protocols, could out-perform stronger readers. We discuss two main findings: groups of humans who perform significantly worse than a state-of-the-art AI can significantly outperform it if their judgements are aggregated by majority voting (in concordance with the first part of the Kasparov’s law); small ensembles of significantly weaker readers can significantly outperform teams of stronger readers, supported by the same computational tool, when the judgments of the former ones are combined within “fit-for-use” protocols (in concordance with the second part of the Kasparov’s law). Our study shows that good interaction protocols can guarantee improved decision performance that easily surpasses the performance of individual agents, even of realistic super-human AI systems. This finding highlights the importance of focusing on how to guarantee better co-operation within human-AI teams, so to enable safer and more human sustainable care practices.",1,0,0,expert systems+artificial intelligence+deep learning,
405,405,"Datasetgan efficient labeled data factory with minimal human effort. We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods, which in some cases require as much as 100x more annotated data as our method.",1,0,0,latent factor+latent variable+image segmentation,
406,406,Decision support through argumentation based practical reasoning. This extended research abstract describes an argumentation-based approach to modelling articulated decision making contexts. The approach encompasses a variety of argument and attack schemes aimed at representing basic knowledge and reasoning patterns for decision support.,1,0,0,reasoning+decision tables+argumentation,
407,407,"Multi agent learner based online feature selection system. Online Feature Selection (OFS) is an important technique in pattern recognition and machine learning. Our challenge is how to enhance the classification performance in real contexts where the large-scale training data arrive sequentially with a big number of features. The major problem is how to choose the best accurate and efficient state-of-the art OFS method that can select the relevant features or if we do a combination between these methods can we improve the classification performance? In this paper, we propose a framework of OFS using the characteristics of multi-agent systems (MAS) to overcome this challenge. We propose firstly a new OFS model; Agent-Learner based OFS (ALOFS) which represents each agent in our MAS. ALOFS is a generalization of first-order and second-order online learning methods based feature selection. Secondly, we propose the Multi Agent-Learner based OFS (MALOFS) system which is our MAS. MALOFS uses two levels of selection: the first level aims to select the more confident learners and the second level has as object to select the relevant features using a proposed negotiation method (MANOFS). MALOFS is applicable to different domains successfully and achieves highly accuracy with some real world applications.",1,0,0,pattern recognition+classification methods+multi-agent,
408,408,"Fluctuation dissipation relations for stochastic gradient descent. The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.",1,0,0,ensemble classifiers+stochastic+stochastic gradient descent,
409,409,"Drogon a trajectory prediction model based on intention conditioned behavior reasoning. We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON) for accurate vehicle trajectory prediction by considering behavioral intentions of vehicles in traffic scenes. Our main insight is that the behavior (i.e., motion) of drivers can be reasoned from their high level possible goals (i.e., intention) on the road. To succeed in such behavior reasoning, we build a conditional prediction model to forecast goal-oriented trajectories with the following stages: (i) relational inference where we encode relational interactions of vehicles using the perceptual context; (ii) intention estimation to compute the probability distributions of intentional goals based on the inferred relations; and (iii) behavior reasoning where we reason about the behaviors of vehicles as trajectories conditioned on the intentions. To this end, we extend the proposed framework to the pedestrian trajectory prediction task, showing the potential applicability toward general trajectory prediction.",1,0,0,probability+probability distributions+inference,
410,410,"From big to small multi scale local planar guidance for monocular depth estimation. Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction. In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.",1,0,0,depth estimation+neural networks+convolutional neural networks,
411,411,"Deeppainter painter classification using deep convolutional autoencoders. In this paper we describe the problem of painter classification, and propose a novel approach based on deep convolutional autoencoder neural networks. While previous approaches relied on image processing and manual feature extraction from paintings, our approach operates on the raw pixel level, without any preprocessing or manual feature extraction. We first train a deep convolutional autoencoder on a dataset of paintings, and subsequently use it to initialize a supervised convolutional neural network for the classification phase.",1,0,0,convolutional neural networks+feedforward neural networks+auto encoders,
412,412,"Simple video generation using neural odes. Despite having been studied to a great extent, the task of conditional generation of sequences of frames, or videos, remains extremely challenging. It is a common belief that a key step towards solving this task resides in modelling accurately both spatial and temporal information in video signals. A promising direction to do so has been to learn latent variable models that predict the future in latent space and project back to pixels, as suggested in recent literature. Following this line of work and building on top of a family of models introduced in prior work, Neural ODE, we investigate an approach that models time-continuous dynamics over a continuous latent space with a differential equation with respect to time. The intuition behind this approach is that these trajectories in latent space could then be extrapolated to generate video frames beyond the time steps for which the model is trained. We show that our approach yields promising results in the task of future frame prediction on the Moving MNIST dataset with 1 and 2 digits.",1,0,0,ode+ordinary differential equations+latent variable models,
413,413,"Multi label learning based on iterative label propagation over graph. Abstract   One key challenge in multi-label learning is how to exploit label dependency effectively, and existing methods mainly address this issue via training a prediction model for each label based on the combination of original features and the labels on which it depends on. However, the influence of label dependency might be depressed due to the significant imbalance in dimensionality of feature set and dependent label set in this way, also the dynamic interaction between labels cannot be utilized effectively. In this paper, we propose a new framework to exploit the dependencies between labels iteratively and interactively. Every label’s prediction will be updated through iterative process of propagation, other than being determined directly by a prediction model. Specifically, we utilize a graph model to encode the dependencies between labels, and employ the random-walk with restart (RWR) strategy to propagate the dependency among all labels iteratively until the predictions for all the labels converge. We validate our approach by experiments, and the results demonstrate that it yields significant improvements compared with several state-of-the-art algorithms.",1,0,0,random walk+class labels+label propagation,
414,414,"The automatic design of multiobjective ant colony optimization algorithms. Multiobjective optimization problems are problems with several, typically conflicting, criteria for evaluating solutions. Without any a priori preference information, the Pareto optimality principle establishes a partial order among solutions, and the output of the algorithm becomes a set of nondominated solutions rather than a single one. Various ant colony optimization (ACO) algorithms have been proposed in recent years for solving such problems. These multiobjective ACO (MOACO) algorithms exhibit different design choices for dealing with the particularities of the multiobjective context. This paper proposes a formulation of algorithmic components that suffices to describe most MOACO algorithms proposed so far. This formulation also shows that existing MOACO algorithms often share equivalent design choices, but they are described in different terms. Moreover, this formulation is synthesized into a flexible algorithmic framework, from which not only existing MOACO algorithms may be instantiated, but also combinations of components that were never studied in the literature. In this sense, this paper goes beyond proposing a new MOACO algorithm, but it rather introduces a family of MOACO algorithms. The flexibility of the proposed MOACO framework facilitates the application of automatic algorithm configuration techniques. The experimental results presented in this paper show that the automatically configured MOACO framework outperforms the MOACO algorithms that inspired the framework itself. This paper is also among the first to apply automatic algorithm configuration techniques to multiobjective algorithms.",1,0,0,pareto+multi-objective optimization problem+nondominated solutions,
415,415,"Using different cost functions to train stacked auto encoders. Deep neural networks comprise several hidden layers of units, which can be pre-trained one at a time via an unsupervised greedy approach. A whole network can then be trained (fine-tuned) in a supervised fashion. One possible pre-training strategy is to regard each hidden layer in the network as the input layer of an auto-encoder. Since auto-encoders aim to reconstruct their own input, their training must be based on some cost function capable of measuring reconstruction performance. Similarly, the supervised fine-tuning of a deep network needs to be based on some cost function that reflects prediction performance. In this work we compare different combinations of cost functions in terms of their impact on layer-wise reconstruction performance and on supervised classification performance of deep networks. We employed two classic functions, namely the cross-entropy (CE) cost and the sum of squared errors (SSE), as well as the exponential (EXP) cost, inspired by the error entropy concept. Our results were based on a number of artificial and real-world data sets.",1,0,0,hidden layers+auto encoders+output layer,
416,416,"Delving into transferable adversarial examples and black box attacks. An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.",1,0,0,supervised classification+feedforward neural networks+image classification,
417,417,"Pct point cloud transformer. The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",1,0,0,natural language processing+neural networks+nearest neighbor search,
418,418,"Zoom net mining deep feature interactions for visual relationship recognition. Recognizing visual relationships among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features.The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net.We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin. The source code will be released on this https URL.",1,0,0,linguistics+image understanding+markov random fields,
419,419,"Neural network pmv estimation for model based predictive control of hvac systems. Heating, Ventilating and Air Conditioning (HVAC) systems are used to provide adequate comfort to occupants of spaces within buildings. One important aspect of comfort, the thermal sensation, is commonly assessed by computation of the Predicted Mean Vote (PMV) index. Model-based predictive control may be applied to HVAC systems in existing buildings in order to provide a desired degree of thermal comfort and simultaneously achieve significant energy savings. This control strategy may be formulated as a discrete optimisation problem and solved by means of structured search techniques. Finding the optimal solution depends on the ability of computing many PMV values in a small amount of time. As the PMV formulation involves iterative computations consuming variable time, it is crucial to have a method for fast, possibly constant execution time, computation of the PMV index. In this paper it is experimentally shown that an Artificial Neural Network (ANN) can estimate the PMV index with varying degrees of efficiency over the trade-off of accuracy versus computational speed-up.",1,0,0,ann+multi-objective optimisation+neural networks,
420,420,"Adaboost with neural networks for yield and protein prediction in precision agriculture. Adaptive Boosting, or AdaBoost, is an algorithm aimed at improving the performance of ensembles of weak learners by weighing the data itself as well as the learners. Two versions of AdaBoost—AdaBoost-R2 and AdaBoost R∆—are applied in this project, as well as a third novel algorithm combining ideas of these two methods, to the problem of predicting crop yield and protein content in support of precision agriculture. All three algorithms use Feedforward Neural Networks (FFNN) trained with backpropagation as the weak model. Data from four different fields were gathered as a result of on-farm experiments of different nitrogen rate applications using randomly stratified trials based on previous years’ yield and protein. The three AdaBoost algorithms are compared to a simple FFNN with a single hidden layer. The results confirm previous findings in different fields, where ensemble methods outperform single models. The results are improved by 3 to 10 units for yield prediction, and by a small percentage for protein prediction.",1,0,0,feedforward neural networks+backpropagation algorithm+hidden layers,
421,421,"Stability of solutions of sir model under constant vaccination. Vaccination is effective to the control of the epidemics. In this paper we discuss the stability of the equilibriums of SIR model under constant vaccination. It is proved that with the constant vaccination percentage p, if p > p 0  (a constant), the “infection-free” equilibrium is stable. Otherwise, it is unstable and the endemic equilibrium is stable.",1,0,0,asymptotical stability+nash equilibrium+endemic equilibrium,
422,422,"Assessment of service protocol adaptability based on novel walk computation. In recent years, we witness the increasing trend that more applications are developed by composing Web services. Services interact with each other in ways not necessarily foreseen during their development phase. In this setting, mismatches usually exist between services, and adapters are typically synthesized to reconcile mismatches occurring in certain interactions. The technique that identifies the most suitable provider service from a set of functionally equivalent candidates with respect to certain requirements specified by the requester is essential. To address this challenge, we propose a technique called adaptability assessment , which 1) provides a set of conditions that determines when service interactions can be conducted and 2) computes an adaptation degree that specifies to what extent a service protocol is adaptable to another service protocol. Adaptability assessment complements the techniques that synthesize adapters. Specifically, when adaptability assessment suggests that two service protocols can conduct some interactions according to the adaptation mechanisms of a certain adapter and these interactions can fulfill the requester's requirements, then the effort of synthesizing an adapter is beneficial to potential service interactions. As such, the requester can acknowledge whether his/her expected interactions can be conducted or not and under which conditions. This is important before conducting an interaction, particularly when this interaction is critical, long running, and nonrepeatable.",1,0,0,network interface+communication protocols+transport protocols,
423,423,"Detecting malicious accounts in online developer communities using deep learning. Online developer communities like GitHub provide services such as distributed version control and task management, which allow a massive number of developers to collaborate online. However, the openness of the communities makes themselves vulnerable to different types of malicious attacks, since the attackers can easily join and interact with legitimate users. In this work, we formulate the malicious account detection problem in online developer communities, and propose GitSec, a deep learning-based solution to detect malicious accounts. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles as well as dynamic activity characteristics. On one hand, GitSec makes use of users' descriptive features from the profiles. On the other hand, GitSec processes users' dynamic behavioral data by constructing two user activity sequences and applying a parallel neural network design to deal with each of them, respectively. An attention mechanism is used to integrate the information generated by the parallel neural networks. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our extensive evaluations show that GitSec is an accurate detection system, with an F1-score of 0.922 and an AUC value of 0.940.",1,0,0,detection problems+deep learning+neural networks,
424,424,"Three module lumped element model of a continuum arm section. In this paper, a section of a continuum arm is modeled using lumped model elements (masses, springs and dampers). The model, although an approximation for a continuum structure, can be used to conveniently analyze the dynamics of the arm with selectable tradeoff in accuracy of modeling. Principles of lagrangian dynamics are used to derive the expressions for the generalized forces in the system. Simulation results using the model are compared with the physical measurements of a continuum arm prototype built using McKibben actuators. A brief discussion on how this relatively simple model can be more realizable when compared to other techniques of modeling continuum arms is also presented in the paper.",1,0,0,hierarchical model+sma actuators+continuum structures,
425,425,"Spatio temporal scene graphs for video dialog. The Audio-Visual Scene-aware Dialog (AVSD) task requires an agent to indulge in a natural conversation with a human about a given video. Specifically, apart from the video frames, the agent receives the audio, brief captions, and a dialog history, and the task is to produce the correct answer to a question about the video. Due to the diversity in the type of inputs, this task poses a very challenging multimodal reasoning problem. Current approaches to AVSD either use global video-level features or those from a few sampled frames, and thus lack the ability to explicitly capture relevant visual regions or their interactions for answer generation. To this end, we propose a novel spatio-temporal scene graph representation (STSGR) modeling fine-grained information flows within videos. Specifically, on an input video sequence, STSGR (i) creates a two-stream visual and semantic scene graph on every frame, (ii) conducts intra-graph reasoning using node and edge convolutions generating visual memories, and (iii) applies inter-graph aggregation to capture their temporal evolutions. These visual memories are then combined with other modalities and the question embeddings using a novel semantics-controlled multi-head shuffled transformer, which then produces the answer recursively. Our entire pipeline is trained end-to-end. We present experiments on the AVSD dataset and demonstrate state-of-the-art results. A human evaluation on the quality of our generated answers shows 12% relative improvement against prior methods.",1,0,0,semantics+frames+human evaluation,
426,426,"Deep ranking model by large adaptive margin learning for person re identification. Abstract Person re-identification aims to match images of the same person across disjoint camera views, which is a challenging problem in video surveillance. The major challenge of this task lies in how to preserve the similarity of the same person against large variations caused by complex backgrounds, mutual occlusions and different illuminations, while discriminating the different individuals. In this paper, we present a novel deep ranking model with feature learning and fusion by learning a large adaptive margin between the intra-class distance and inter-class distance to solve the person re-identification problem. Specifically, we organize the training images into a batch of pairwise samples. Treating these pairwise samples as inputs, we build a novel part-based deep convolutional neural network (CNN) to learn the layered feature representations by preserving a large adaptive margin. As a result, the final learned model can effectively find out the matched target to the anchor image among a number of candidates in the gallery image set by learning discriminative and stable feature representations. Overcoming the weaknesses of conventional fixed-margin loss functions, our adaptive margin loss function is more appropriate for the dynamic feature space. On four benchmark datasets, PRID2011, Market1501, CUHK01 and 3DPeS, we extensively conduct comparative evaluations to demonstrate the advantages of the proposed method over the state-of-the-art approaches in person re-identification.",1,0,0,digital image+neural networks+convolutional neural networks,
427,427,"Noise estimation using density estimation for self supervised multimodal learning. One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https://github.com/elad-amrani/ssml.",1,0,0,noise estimation+question answering+video retrieval,
428,428,"Agentpolis towards a platform for fully agent based modeling of multi modal transportation demonstration. AgentPolis is a fully agent-based platform for modeling multi-modal transportation systems. It comprises a high-performance discrete-event simulation core, a cohesive set of high-level abstractions for building extensible agent-based models and a library of predefined components frequently used in transportation and mobility models. Together with a suite of supporting tools, AgentPolis enables rapid prototyping and execution of data-driven simulations of a wide range of mobility and transportation phenomena. We illustrate the capabilities of the platform on a model of fare inspection in public transportation networks.",1,0,0,software agents+agent based+intelligent agents,
429,429,"Enzymatic numerical p systems for basic operations and sorting. Membrane computing, which is a computational model inspired by the structures and behaviors of living cells, has considerable attention as one of non-silicon based computing. In the present paper, we propose EN P systems for basic operations, and sorting. We first propose three EN P systems for computing three logic operations, OR, AND, and EX-OR functions. All of the EN P systems work in a constant number of steps. Next, we propose two EN P systems that operates as a half adder and a full adder, and then, propose two EN P system for additions of two binary numbers and n binary numbers. We show the EN P systems for two additions work in O(m) steps and O(nm) steps, respectively. Finally, we propose two EN P systems for sorting. We propose an EN P system for compare-and-exchange operation. Then, using the EN P system as sub-systems, we propose an EN P system for sorting n numbers, and show that the EN P system works in O(n) steps.",1,0,0,multiplication+multiplier+ripple carry adders,
430,430,"Recent advances in age and height estimation from still images and video. Soft-biometrics such as gender, age, race, etc have been found to be useful characterizations that enable fast pre-filtering and organization of data for biometric applications. In this paper, we focus on two useful soft-biometrics — age and height. We discuss their utility and the factors involved in their estimation from images and videos. In this context, we highlight the role that geometric constraints such as multiview-geometry, and shape-space geometry play. Then, we present methods based on these geometric constraints for age and height-estimation. These methods provide a principled means by fusing image-formation models, multi-view geometric constraints, and robust statistical methods for inference.",1,0,0,biometric applications+multi-views+inference,
431,431,"Local linear neighbor reconstruction for multi view data. We propose to learn a unified similarity matrix by using multi-view information.The unified similarity matrix interacts with multi-view original local data.The learned similarity matrix enhances both spectral clustering and label propagation.Results on several datasets show the effectiveness of the learned similarity matrix. Graph based multi-view data analysis has become a hot topic in the past decade, and multi-view similarity matrix is fundamental for such tasks. Existing multi-view similarity matrix construction methods cannot learn local geometrical information in the original data space from multiple views simultaneously. Considering the fact that an appropriate similarity matrix is block-wise with intra-class similarity, it is more reasonable to learn a similarity matrix by using local geometrical information in multiple original data space. In this paper, we propose to construct a unified similarity matrix by using local linear neighbors in multiple views. In each view, the similarity matrix can be reconstructed with the weights of the neighbors of each data point in the original space. In multiple views, we seek for a unified similarity matrix which consists of the similarity matrix in each view. The unified similarity matrix can be used for spectral clustering, label propagation and other graph based learning algorithms. Experimental results show that spectral clustering and label propagation algorithms using the unified similarity matrix outperform those using other multi-view similarity matrices, they also outperform typical multi-view spectral clustering algorithms and typical multi-view label propagation algorithms.",1,0,0,similarity matrix+spectral clustering+label propagation,
432,432,"Experimenting multiresolution analysis for identifying regions of different classification complexity. Systems for assessing the classification complexity of a dataset have received increasing attention in research activities on pattern recognition. These systems typically aim at quantifying the overall complexity of a domain, with the goal of comparing different datasets. In this work, we propose a method for partitioning a dataset into regions of different classification complexity, so to highlight sources of complexity inside the dataset. Experiments have been carried out on relevant datasets, proving the effectiveness of the proposed method.",1,0,0,pattern recognition+classification methods+supervised classification,
433,433,"Semantic interpretation of requirements through cognitive grammar and configuration. Many attempts have been made to apply Natural Language Processing to requirements specifications. However, typical approaches rely on shallow parsing to identify object-oriented elements of the specifications (e.g. classes, attributes, and methods). As a result, the models produced are often incomplete, imprecise, and require manual revision and validation. In contrast, we propose a deep Natural Language Understanding approach to create complete and precise formal models of requirements specifications. We combine three main elements to achieve this: (1) acquisition of lexicon from a user-supplied glossary requiring little specialised prior knowledge; (2) flexible syntactic analysis based purely on word-order; and (3) Knowledge-based Configuration unifies several semantic analysis tasks and allows the handling of ambiguities and errors. Moreover, we provide feedback to the user, allowing the refinement of specifications into a precise and unambiguous form. We demonstrate the benefits of our approach on an example from the PROMISE requirements corpus.",1,0,0,natural language understanding+syntactic analysis+parsing algorithm,
434,434,"Unlocking neural population non stationarity using a hierarchical dynamics model. Neural population activity often exhibits rich variability. This variability can arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as from modulations of neural firing properties on long time-scales, often referred to as neural non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics. We derive a Bayesian Laplace propagation algorithm for joint inference of parameters and population states. On neural population recordings from primary visual cortex, we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models.",1,0,0,inference+probabilistic inference+stochasticity,
435,435,"Fbnetv3 joint architecture recipe search using neural acquisition function. Neural Architecture Search (NAS) yields state-of-the-art neural networks that outperform their best manually-designed counterparts. However, previous NAS methods search for architectures under one training recipe (i.e., training hyperparameters), ignoring the significance of training recipes and overlooking superior architectures under other training recipes. Thus, they fail to find higher-accuracy architecture-recipe combinations. To address this oversight, we present JointNAS to search both (a) architectures and (b) their corresponding training recipes. To accomplish this, we introduce a neural acquisition function that scores architectures and training recipes jointly. Following pre-training on a proxy dataset, this acquisition function guides both coarse-grained and fine-grained searches to produce FBNetV3. FBNetV3 is a family of state-of-the-art compact ImageNet models, outperforming both automatically and manually-designed architectures. For example, FBNetV3 matches both EfficientNet and ResNeSt accuracy with 1.4x and 5.0x fewer FLOPs, respectively. Furthermore, the JointNAS-searched training recipe yields significant performance gains across different networks and tasks.",1,0,0,novel architecture+network architecture+neural networks,
436,436,"Augfpn improving multi scale feature learning for object detection. Current state-of-the-art detectors typically exploit feature pyramid to detect objects at different scales. Among them, FPN is one of the representative works that build a feature pyramid by multi-scale features summation. However, the design defects behind prevent the multi-scale features from being fully exploited. In this paper, we begin by first analyzing the design defects of feature pyramid in FPN, and then introduce a new feature pyramid architecture named AugFPN to address these problems. Specifically, AugFPN consists of three components: Consistent Supervision, Residual Feature Augmentation, and Soft RoI Selection. AugFPN narrows the semantic gaps between features of different scales before feature fusion through Consistent Supervision. In feature fusion, ratio-invariant context information is extracted by Residual Feature Augmentation to reduce the information loss of feature map at the highest pyramid level. Finally, Soft RoI Selection is employed to learn a better RoI feature adaptively after feature fusion. By replacing FPN with AugFPN in Faster R-CNN, our models achieve 2.3 and 1.6 points higher Average Precision (AP) when using ResNet50 and MobileNet-v2 as backbone respectively. Furthermore, AugFPN improves RetinaNet by 1.6 points AP and FCOS by 0.9 points AP when using ResNet50 as backbone. Codes will be made available.",1,0,0,information loss+semantic gap+object detection,
437,437,"Structured prediction using cgans with fusion discriminator. We propose the fusion discriminator, a single unified framework for incorporating conditional information into a generative adversarial network (GAN) for a variety of distinct structured prediction tasks, including image synthesis, semantic segmentation, and depth estimation. Much like commonly used convolutional neural network -- conditional Markov random field (CNN-CRF) models, the proposed method is able to enforce higher-order consistency in the model, but without being limited to a very specific class of potentials. The method is conceptually simple and flexible, and our experimental results demonstrate improvement on several diverse structured prediction tasks.",1,0,0,conditional random field+markov random fields+generative adversarial networks,
438,438,"Counting complexity for reasoning in abstract argumentation. In this paper, we consider counting and projected model counting of extensions in abstract argumentation for various semantics. When asking for projected counts we are interested in counting the number of extensions of a given argumentation framework while multiple extensions that are identical when restricted to the projected arguments count as only one projected extension. We establish classical complexity results and parameterized complexity results when the problems are parameterized by treewidth of the undirected argumentation graph. To obtain upper bounds for counting projected extensions, we introduce novel algorithms that exploit small treewidth of the undirected argumentation graph of the input instance by dynamic programming (DP). Our algorithms run in time double or triple exponential in the treewidth depending on the considered semantics. Finally, we take the exponential time hypothesis (ETH) into account and establish lower bounds of bounded treewidth algorithms for counting extensions and projected extension.",1,0,0,argumentation frameworks+bounded treewidth+abstract argumentation,
439,439,"Two generator game learning to sample via linear goodness of fit test. Learning the probability distribution of high-dimensional data is a challenging problem. To solve this problem, we formulate a deep energy adversarial network (DEAN), which casts the energy model learned from real data into an optimization of a goodness-of-fit (GOF) test statistic. DEAN can be interpreted as a GOF game between two generative networks, where one explicit generative network learns an energy-based distribution that fits the real data, and the other implicit generative network is trained by minimizing a GOF test statistic between the energy-based distribution and the generated data, such that the underlying distribution of the generated data is close to the energy-based distribution. We design a two-level alternative optimization procedure to train the explicit and implicit generative networks, such that the hyper-parameters can also be automatically learned. Experimental results show that DEAN achieves high quality generations compared to the state-of-the-art approaches.",1,0,0,high dimensional data+optimization+network architecture,
440,440,A deep learning method for pathological voice detection using convolutional deep belief networks. Automatically detecting pathological voice disorders such as vocal cord paralysis or Reinke’s edema is a challenging and important medical classification problem. While deep learning techniques have achieved significant progress in the speech recognition field there has been less research work in the area of pathological voice disorders detection. A novel system for pathological voice detection using convolutional neural network (CNN) as the basic architecture is presented in this work. The novel system uses spectrograms of normal and pathological speech recordings as the input to the network. Initially Convolutional deep belief network (CDBN) are used to pre-train the weights of CNN system. This acts as a generative model to explore the structure of the input data using statistical methods. Then a CNN is trained using supervised back-propagation learning algorithm to fine tune the weights. It will be shown that a small amount of data can be used to achieve good results in classification with this deep learning approach. A performance analysis of the novel method is provided using real data from the Saarbrucken Voice database,1,0,0,neural networks+convolutional neural networks+backpropagation algorithm,
441,441,"Wsabie scaling up to large vocabulary image annotation. Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory.",1,0,0,reference image+digital image+image annotation,
442,442,"Top n recommender system via matrix completion. Top-N recommender systems have been investigated widely both in industry and academia. However, the recommendation quality is far from satisfactory. In this paper, we propose a simple yet promising algorithm. We fill the user-item matrix based on a low-rank assumption and simultaneously keep the original information. To do that, a nonconvex rank relaxation rather than the nuclear norm is adopted to provide a better rank approximation and an efficient optimization strategy is designed. A comprehensive set of experiments on real datasets demonstrates that our method pushes the accuracy of Top-N recommendation to a new level.",1,0,0,optimization+real data sets+nonconvex,
443,443,"Vision based autonomous mapping and exploration using a quadrotor mav. In this paper, we describe our autonomous vision-based quadrotor MAV system which maps and explores unknown environments. All algorithms necessary for autonomous mapping and exploration run on-board the MAV. Using a front-looking stereo camera as the main exteroceptive sensor, our quadrotor achieves these capabilities with both the Vector Field Histogram+ (VFH+) algorithm for local navigation, and the frontier-based exploration algorithm. In addition, we implement the Bug algorithm for autonomous wall-following which could optionally be selected as the substitute exploration algorithm in sparse environments where the frontier-based exploration under-performs. We incrementally build a 3D global occupancy map on-board the MAV. The map is used by the VFH+ and frontier-based exploration in dense environments, and the Bug algorithm for wall-following in sparse environments. During the exploration phase, images from the front-looking camera are transmitted over Wi-Fi to the ground station. These images are input to a large-scale visual SLAM process running off-board on the ground station. SLAM is carried out with pose-graph optimization and loop closure detection using a vocabulary tree. We improve the robustness of the pose estimation by fusing optical flow and visual odometry. Optical flow data is provided by a customized downward-looking camera integrated with a microcontroller while visual odometry measurements are derived from the front-looking stereo camera. We verify our approaches with experimental results.",1,0,0,ego-motion estimation+loop closure+robot localization,
444,444,"Computing theoretically sound upper bounds to expected support for frequent pattern mining problems over uncertain big data. Frequent pattern mining aims to discover implicit, previously unknown, and potentially useful knowledge in the form of sets of frequently co-occurring items, events, or objects. To mine frequent patterns from probabilistic datasets of uncertain data, where each item in a transaction is usually associated with an existential probability expressing the likelihood of its presence in that transaction, the UF-growth algorithm captures important information about uncertain data in a UF-tree structure so that expected support can be computed for each pattern. A pattern is considered frequent if its expected support meets or exceeds the user-specified threshold. However, a challenge is that the UF-tree can be large. To handle this challenge, several algorithms use smaller trees such that upper bounds to expected support can be computed. In this paper, we examine these upper bounds, and determine which ones provide tighter upper bounds to expected support for frequent pattern mining of uncertain big data.",1,0,0,frequent patterns+association rules+frequent itemsets,
445,445,"Gaze tracking based on pupil estimation using multilayer perception. Most accurate gaze trackers commonly use near IR (infrared ray) illuminators to detect a pupil rather than an iris because the pupil detection provides higher accuracy for implementing a gaze tracker and it is easier to detect the pupil under IR illumination. However, the active IR illuminating methods directly emit energies to human eyes and also generate heats to an embedded mobile device. Thus, it may be uncomfortable and unstable to utilize an active IR illuminating method in an embedded mobile device as a gaze tracker for a long time. In this paper, we propose a new gaze tracking method using a common USB camera, in which a multilayer perceptron is applied to estimate the pupil's location using iris area information localized in a face area detected from a captured image. The pupil location information as teaching target signals for the neural network is obtained from off-line experiments using an IR camera with an illuminator. And localized iris area information obtained from on-line experiments using a common USB camera is used as input signals of the neural network. Experimental results show that the proposed method plausibly performs the pupil estimation by the multilayer perceptron and successfully generates gaze tracking by an additional calibration process.",1,0,0,multilayer perceptrons+feedforward neural networks+backpropagation algorithm,
446,446,"Mechanism design in social networks. This paper studies an auction design problem for a seller to sell a commodity in a social network, where each individual (the seller or a buyer) can only communicate with her neighbors. The challenge to the seller is to design a mechanism to incentivize the buyers, who are aware of the auction, to further propagate the information to their neighbors so that more buyers will participate in the auction and hence, the seller will be able to make a higher revenue. We propose a novel auction mechanism, called information diffusion mechanism (IDM), which incentivizes the buyers to not only truthfully report their valuations on the commodity to the seller, but also further propagate the auction information to all their neighbors. In comparison, the direct extension of the well-known Vickrey-Clarke-Groves (VCG) mechanism in social networks can also incentivize the information diffusion, but it will decrease the seller’s revenue or even lead to a deficit sometimes. The formalization of the problem has not yet been addressed in the literature of mechanism design and our solution is very significant in the presence of large-scale online social networks",1,0,0,information diffusion+mechanism design+auction mechanisms,
447,447,"Maximal output admissible set for trajectory tracking control of biped robots and its application to falling avoidance control. Humanoid robots have been considered as a universal machine which can operate in place of human. This kind of universal machine requires human-like biped walking capability. In particular, it is important to avoid falling by appropriately switching behaviors even if there are unknown disturbances. The authors proposed the maximal output admissible (MOA) set for the center of gravity (COG) regulator in the upright position. Based on the MOA set, we can switch feedback gains with the Zero Moment Point (ZMP) constraint satisfied. In this paper, the author extends MOA set framework to trajectory tracking controller. This extension makes it possible to switch controllers: regulator in the upright position and tracking controller of a stepping motion in order to avoid falling. The effectiveness of the proposed method is verified with a simulation.",1,0,0,trajectory tracking control+zmp+tracking controls,
448,448,"Narrowing the gap between saturated and optimal cost partitioning for classical planning. In classical planning, cost partitioning is a method for admissibly combining a set of heuristic estimators by distributing operator costs among the heuristics. An optimal cost partitioning is often prohibitively expensive to compute. Saturated cost partitioning is an alternative that is much faster to compute and has been shown to offer high-quality heuristic guidance on Cartesian abstractions. However, its greedy nature makes it highly susceptible to the order in which the heuristics are considered. We show that searching in the space of orders leads to significantly better heuristic estimates than with previously considered orders. Moreover, using multiple orders leads to a heuristic that is significantly better informed than any single-order heuristic. In experiments with Cartesian abstractions, the resulting heuristic approximates the optimal cost partitioning very closely.",1,0,0,heuristic approaches+maximum likelihood estimator+tabu search,
449,449,"Kergm kernelized graph matching. Graph matching plays a central role in such fields as computer vision, pattern recognition, and bioinformatics. Graph matching problems can be cast as two types of quadratic assignment problems (QAPs): Koopmans-Beckmann's QAP or Lawler's QAP. In our paper, we provide a unifying view for these two problems by introducing new rules for array operations in Hilbert spaces. Consequently, Lawler's QAP can be considered as the Koopmans-Beckmann's alignment between two arrays in reproducing kernel Hilbert spaces (RKHS), making it possible to efficiently solve the problem without computing a huge affinity matrix. Furthermore, we develop the entropy-regularized Frank-Wolfe (EnFW) algorithm for optimizing QAPs, which has the same convergence rate as the original FW algorithm while dramatically reducing the computational burden for each outer iteration. We conduct extensive experiments to evaluate our approach, and show that our algorithm significantly outperforms the state-of-the-art in both matching accuracy and scalability.",1,0,0,graph matching+quadratic assignment problems+affinity matrix,
450,450,"Multi objective bayesian optimization algorithm for real time task scheduling on heterogeneous multiprocessors. In this paper, we have proposed a Bayesian optimization based novel approach for multi-objective task scheduling in real-time heterogeneous multiprocessor systems. Task scheduling problem in multi-processor real-time systems is a NP-hard problem. In such systems, scheduling of tasks becomes a huge challenge for the scheduler designers; especially when tasks are inter-dependent and have deadline constraints. Interdependent or precedence-constrained tasks are often represented as directed acyclic graphs. Most of the real life applications require state of the art planning and scheduling schemes for safer and efficient operations. Thus, we propose the algorithm ‘moBOA-RTS’ (multi-objective Bayesian optimization algorithm for real time scheduling) to find an optimal schedule satisfying all the constraints within reasonable time. Here, learning of task graph is made through Bayesian networks. At first, tasks are allocated to different processors and then LDF (latest deadline first) based priority is used to determine the task execution on individual processors. The proposed approach can be applied on many processor real-time systems, where both the scenarios viz. homogenous and heterogeneous processing environments are prevalent. Experimental analysis shows that our approach produces optimal decisions for feasible scheduling that ensures the compliance of all real time and precedence related constraints.",1,0,0,optimization+bayesian methods+task scheduling problem,
451,451,"The smart classifier for arabic fine grained dialect identification. This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a Multinomial Naive Bayes classifier based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73% in terms of Macro accuracy and a macro-averaged F1-score of 67.31%",1,0,0,bayes classifier+classifiers+language model,
452,452,"Point linking network for object detection. Object detection is a core problem in computer vision. With the development of deep ConvNets, the performance of object detectors has been dramatically improved. The deep ConvNets based object detectors mainly focus on regressing the coordinates of bounding box, e.g., Faster-R-CNN, YOLO and SSD. Different from these methods that considering bounding box as a whole, we propose a novel object bounding box representation using points and links and implemented using deep ConvNets, termed as Point Linking Network (PLN). Specifically, we regress the corner/center points of bounding-box and their links using a fully convolutional network; then we map the corner points and their links back to multiple bounding boxes; finally an object detection result is obtained by fusing the multiple bounding boxes. PLN is naturally robust to object occlusion and flexible to object scale variation and aspect ratio variation. In the experiments, PLN with the Inception-v2 model achieves state-of-the-art single-model and single-scale results on the PASCAL VOC 2007, the PASCAL VOC 2012 and the COCO detection benchmarks without bells and whistles. The source code will be released.",1,0,0,computer vision+corner point+object detection,
453,453,"Deep structured mixtures of gaussian processes. Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii) when used as GP approximation, captures predictive uncertainties consistently better than previous expert-based approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and often perform competitive or outperform prior work.",1,0,0,gaussians+non-parametric bayesian+gaussian processes,
454,454,"Challenges in patrolling to maximize pristine forest area position paper. Illegal extraction of forest resources is fought, in many developing countries, by patrols through the forest that seek to deter such activity by decreasing its profitability. With limited resources for performing such patrols, a patrol strategy will seek to distribute the patrols throughout the forest, in space and time, in order to minimize the resulting amount of extraction that occurs or maximize the degree of forest protection, according to one of several potential metrics. We pose this problem as a Stackelberg game. We adopt and extend the simple, geometrically elegant model of (Albers 2010). First, we study optimal allocations of patrol density under generalizations of this model, relaxing several of its assumptions. Second, we pose the problem of generating actual schedules whose site visit frequencies are consistent with the analytically computed optimal patrol densities.",1,0,0,stackelberg equilibrium+stackelberg+stackelberg games,
455,455,"A fine grained distribution approach for etl processes in big data environments. Abstract   Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “ Volume ” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.",1,0,0,big data+parallelizations+etl process,
456,456,"Stock market prediction using hidden markov model. Stock market is the most popular investment scheme promising high returns albeit some risks. An intelligent stock prediction model would thus be desirable. So, this paper aims at surveying recent literature in the area of Neural Network, Hidden Markov Model and Support Vector Machine used to predict the stock market fluctuation. Neural networks and SVM are identified to be the leading machine learning techniques in stock market prediction area. Also, a model for predicting stock market using HMM is presented. Traditional techniques lack in covering stock price fluctuations and so new approaches have been developed for analysis of stock price variations. Markov Model is one such recent approach promising better results. In this paper a predicting method using Hidden Markov Model is proposed to provide better accuracy and a comparison of the existing techniques is also done.",1,0,0,stock market+stock price+neural networks,
457,457,"Fuzzy sets and cut systems in a category of sets with similarity relations. Let $$\Upomega$$ be a complete residuated lattice. Let $${\mathbf{SetR}}(\Upomega)$$ be the category of sets with similarity relations with values in $$\Upomega$$ (called $$\Upomega$$ -sets), which is an analogy of the category of classical sets with relations as morphisms. A fuzzy set in an $$\Upomega$$ -set in the category $${\mathbf{SetR}}(\Upomega)$$ is a morphism from $$\Upomega$$ -set to a special $$\Upomega$$ -set $$(\Upomega,\leftrightarrow),$$ where $$\leftrightarrow$$ is the biresiduation operation in $$\Upomega.$$ In the paper, we prove that fuzzy sets in $$\Upomega$$ -sets in the category $${\mathbf{SetR}}(\Upomega)$$ can be expressed equivalently as special cut systems $$(C_{\alpha})_{\alpha\in\Upomega}.$$",1,0,0,boundary values+semantic similarity+functors,
458,458,"Deep transfer learning with joint adaptation networks. Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.",1,0,0,stochastic+backpropagation algorithm+stochastic gradient descent,
459,459,"Demographic differences in using knowledge creation tools among faculty members. Purpose




The purpose of this study is to investigate and analyze demographic differences in using knowledge creation tools among faculty members. It also attempts to identify the most knowledge creation tool used by the participants. The tools comprised of 13 items including data mining, metadata, classifications, expert profiling, Mashup and blogs.




Design/methodology/approach




Data were collected through an online survey questionnaire. A total of 300 faculty members from 26 universities and colleges accredited by the UAE Ministry of High Education participated in the study. The t-test and analysis of variance (ANOVA) test are used to validate the stated hypotheses.




Findings




The study found personal knowledge management to be the most used knowledge creation tool among the faculty members, followed by authoring tools and templates. Findings of the study indicate statistically no significant difference in using knowledge creation tools with respect to gender, qualification, academic rank, teaching experience and institutional affiliation. These findings support the stated null hypotheses (H1, H3, H4, H6 and H8) and suggest that the use of knowledge creation tools is independent from these variables. However, the results showed statistically a significant age group difference, academic specialization and research experience in using knowledge creation tools. The findings reject the assumed hypotheses (H2, H5 and H7) and suggest the impact of these variables on the use of knowledge creation tools.




Research limitations/implications




The paper is based on the data collected through a survey questionnaire. Future studies may combine quantitative and qualitative data collection methods for the purpose of comparison and in-depth analysis.




Practical implications




Findings could be an important reference for knowledge management officers and knowledge intensive organizations and institutions to develop knowledge creation tools and promote usage among knowledge workers.




Originality/value




The paper represents one of the very few empirical studies conducted on the use of knowledge creation tools. Findings of the study may contribute to the process of knowledge creation among faculty members and also to the improvement of knowledge management in the academic environment and other knowledge organizations.",1,0,0,data mining+explicit knowledge+knowledge management,
460,460,"Dynamite a dynamic local motion model with temporal constraints for robust real time feature matching. Feature based visual odometry and SLAM methods require accurate and fast correspondence matching between consecutive image frames for precise camera pose estimation in real-time. Current feature matching pipelines either rely solely on the descriptive capabilities of the feature extractor or need computationally complex optimization schemes. We present the lightweight pipeline DynaMiTe, which is agnostic to the descriptor input and leverages spatial-temporal cues with efficient statistical measures. The theoretical backbone of the method lies within a probabilistic formulation of feature matching and the respective study of physically motivated constraints. A dynamically adaptable local motion model encapsulates groups of features in an efficient data structure. Temporal constraints transfer information of the local motion model across time, thus additionally reducing the search space complexity for matching. DynaMiTe achieves superior results both in terms of matching accuracy and camera pose estimation with high frame rates, outperforming state-of-the-art matching methods while being computationally more efficient.",1,0,0,pose estimation+visual odometry+odometry,
461,461,"Categorical data clustering a correlation based approach for unsupervised attribute weighting. The interest in attribute weighting, in clustering tasks, have been increasing in the last years. However, few attempts have been made to apply automated attribute weighting to categorical data clustering. Most of the existing approaches computes the weights based on the frequency of the mode category or according to the average distance of data objects from the mode of a cluster. In this paper, we adopt a different approach, investigating how to use the correlation among categorical attributes for measuring their relevancies in clustering tasks. As a result, we propose a correlation-based attribute weighting approach for categorical attributes.",1,0,0,clustering algorithms+clustering methods+data clustering,
462,462,"Learning a super mario controller from examples of human play. Imitating human-like behaviour in action games is a challenging but intriguing task in Artificial Intelligence research, with various strategies being employed to solve the human-like imitation problem. In this research we consider learning human-like behaviour via Markov decision processes without being explicitly given a reward function, and learning to perform the task by observing expert's demonstration. Individual players often have characteristic styles when playing the game, and this method attempts to find the behaviours which make them unique. During play sessions of Super Mario we calculate player's behaviour policies and reward functions by applying inverse reinforcement learning to the player's actions in game. We conduct an online questionnaire which displays two video clips, where one is played by a human expert and the other is played by the designed controller based on the player's policy. We demonstrate that by using apprenticeship learning via Inverse Reinforcement Learning, we are able to get an optimal policy which yields performance close to that of an human expert playing the game, at least under specific conditions.",1,0,0,reward function+optimal policies+imitation learning,
463,463,"Linearly convergent frank wolfe with backtracking line search. Structured constraints in Machine Learning have recently brought the Frank-Wolfe (FW) family of algorithms back in the spotlight. While the classical FW algorithm has poor local convergence properties, the Away-steps and Pairwise FW variants have emerged as improved variants with faster convergence. However, these improved variants suffer from two practical limitations: they require at each iteration to solve a 1-dimensional minimization problem to set the step-size and also require the Frank-Wolfe linear subproblems to be solved exactly. In this paper, we propose variants of Away-steps and Pairwise FW that lift both restrictions simultaneously. The proposed methods set the step-size based on a sufficient decrease condition, and do not require prior knowledge of the objective. Furthermore, they inherit all the favorable convergence properties of the exact line-search version, including linear convergence for strongly convex functions over polytopes. Benchmarks on different machine learning problems illustrate large performance gains of the proposed variants.",1,0,0,local convergence+rate of convergence+step size,
464,464,"Up or down adaptive rounding for post training quantization. When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%.",1,0,0,quantization+neural networks+quantizers,
465,465,"Enhancing weak signal transmission through a feedforward network. The ability to transmit and amplify weak signals is fundamental to signal processing of artificial devices in engineering. Using a multilayer feedforward network of coupled double-well oscillators as well as Fitzhugh-Nagumo oscillators, we here investigate the conditions under which a weak signal received by the first layer can be transmitted through the network with or without amplitude attenuation. We find that the coupling strength and the nodes' states of the first layer act as two-state switches, which determine whether the transmission is significantly enhanced or exponentially decreased. We hope this finding is useful for designing artificial signal amplifiers.",1,0,0,network architecture+multilayer perceptrons+feed-forward network,
466,466,"Focalmix semi supervised learning for 3d medical image detection. Applying artificial intelligence techniques in medical imaging is one of the most promising areas in medicine. However, most of the recent success in this area highly relies on large amounts of carefully annotated data, whereas annotating medical images is a costly process. In this paper, we propose a novel method, called FocalMix, which, to the best of our knowledge, is the first to leverage recent advances in semi-supervised learning (SSL) for 3D medical image detection. We conducted extensive experiments on two widely used datasets for lung nodule detection, LUNA16 and NLST. Results show that our proposed SSL methods can achieve a substantial improvement of up to 17.3% over state-of-the-art supervised learning approaches with 400 unlabeled CT scans.",1,0,0,digital image+medical image processing+unsupervised learning,
467,467,"Identification of emergency blood donation request on twitter. Social media-based text mining in healthcare has received special attention in recent times due to the enhanced accessibility of social media sites like Twitter. The increasing trend of spreading important information in distress can help patients reach out to prospective blood donors in a time bound manner. However such manual efforts are mostly inefficient due to the limited network of a user. In a novel step to solve this problem, we present an annotated Emergency Blood Donation Request (EBDR) dataset to classify tweets referring to the necessity of urgent blood donation requirement. Additionally, we also present an automated feature-based SVM classification technique that can help selective EBDR tweets reach relevant personals as well as medical authorities. Our experiments also present a quantitative evidence that linguistic along with handcrafted heuristics can act as the most representative set of signals this task with an accuracy of 97.89%.",1,0,0,linguistics+svm+classification models,
468,468,"Developing new fitness functions in genetic programming for classification with unbalanced data. Machine learning algorithms such as genetic programming (GP) can evolve biased classifiers when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class) while other classes make up the majority. In this scenario, classifiers can have good accuracy on the majority class but very poor accuracy on the minority class(es) due to the influence that the larger majority class has on traditional training criteria in the fitness function. This paper aims to both highlight the limitations of the current GP approaches in this area and develop several new fitness functions for binary classification with unbalanced data. Using a range of real-world classification problems with class imbalance, we empirically show that these new fitness functions evolve classifiers with good performance on both the minority and majority classes. Our approaches use the original unbalanced training data in the GP learning process, without the need to artificially balance the training examples from the two classes (e.g., via sampling).",1,0,0,binary classification+genetic programming+generic programming,
469,469,"Modeling competition in the telecommunications market based on concepts of population biology. Based on concepts of ecology modeling and specifically on population biology, a methodology for describing a high-technology market's dynamics is developed and presented. The importance of the aforementioned methodology is its capability to estimate and forecast the degree of competition, market equilibrium, and market concentration, the latter expressed by corresponding market shares, in the high-technology environment. Evaluation of the presented methodology in the area of telecommunications led to accurate results, as compared to historical data, in a specific case study. Apart from a very good estimation of the market's behavior, this methodology presents a very good forecasting ability, which can provide valuable inputs for managerial and regulatory decisions and strategic planning, to the players of a high-technology market, described by high entry barriers.",1,0,0,telecommunication networks+strategic planning+strategic management,
470,470,"Population independent pairwise proportionally imitative dynamics for multipopulation games. Multipopulation games model interactions among many agents who have various roles. The imitative protocol describes when and how each agent changes her action in multipopulation games. Under existing imitative protocols, each agent imitates only agents who belong to the population that she belongs to. In other words, it is assumed that each agent knows the populations that the other agents belong to. However, the agents cannot always know the populations that the other agents belong to. In multipopulation games that consist of agents whose senses of values are different from one another, we consider situations where agents cannot know populations that the other agents belong to. We propose a population-independent imitative protocol, and derive the mean dynamics that is generated by the protocol.",1,0,0,intelligent agents+autonomous agents+multiagent system,
471,471,"Filter grafting for deep neural networks. This paper proposes a new learning paradigm called filter grafting, which aims to improve the representation capability of Deep Neural Networks (DNNs). The motivation is that DNNs have unimportant (invalid) filters (e.g., l1 norm close to 0). These filters limit the potential of DNNs since they are identified as having little effect on the network. While filter pruning removes these invalid filters for efficiency consideration, filter grafting re-activates them from an accuracy boosting perspective. The activation is processed by grafting external information (weights) into invalid filters. To better perform the grafting process, we develop an entropy-based criterion to measure the information of filters and an adaptive weighting strategy for balancing the grafted information among networks. After the grafting operation, the network has very few invalid filters compared with its untouched state, enpowering the model with more representation capacity. We also perform extensive experiments on the classification and recognition tasks to show the superiority of our method. For example, the grafted MobileNetV2 outperforms the non-grafted MobileNetV2 by about 7 percent on CIFAR-100 dataset. Code is available at this https URL.",1,0,0,boosting+network architecture+neural networks,
472,472,"An active learning approach to audio to score alignment using dynamic time warping. We propose an integrated system using active learning for audio-to-score alignment. Audio-to-score alignment is a fundamental task in music information retrieval. Although various machine learning techniques have been applied to this task, it is not the case for active learning. To show how beneficial active learning is in audio-to-score alignment, we demonstrate a system that integrates it with dynamic time warping, a commonly used algorithm for time series alignment. We propose a simple parametric model for selecting queries—a crucial step in active learning. We evaluate the system using synthesized audio as well as real performances. The alignment accuracy is improved with a range from 20% to 50% using only less than 10% query instances, a promising result that hopefully can inspire the creation of a collaborative framework between human and machine for audio-to-score alignment in the future.",1,0,0,information retrieval+music information retrieval+machine learning,
473,473,"Applying regional level set formulation to postsawing four element led wafer inspection. With level-set formulation, new contours can emerge during the evolution of contours. A defect inspection system that utilizes the evolution of zero-level contours for segmenting postsawing wafer is proposed in this study. The system utilizes a regional formulation, which improves the level-set segmentation in images with intensity inhomogeneity. An automatic threshold is used to set the initial contour to a contour near the die region. Fewer iterations are thus required to evolve the zero-level set to segment the wafer. Without the needs for filtering in advance, the inspection can be performed directly on the segmented results. The proposed approach outperforms other postsawing inspection methods in terms of accuracy.",1,0,0,initial contour+automatic segmentations+level set segmentation+chan-vese model+bias field+intensity inhomogeneity,
474,474,"Classifying oscillatory signatures of expert vs nonexpert meditators. EEG oscillatory correlates of expert meditators have been studied in the time-frequency domain. Machine Learning techniques are required to expand the understanding of oscillatory signatures. In this work, we propose a methodological pipeline to develop machine learning models for the classification between expert and nonexpert meditative state. We carried out this study utilizing the online repository consisting of EEG dataset of 24 meditators that categorized as 12 experts and 12 nonexperts meditators. The pipeline consists of four stages that include feature engineering, machine learning classifiers, feature selection, and visualization. We decomposed signals using five wavelet families consisting of Haar, Biorthogonal(1.3-6.8), Daubechies( orders 2-10), Coiflet(orders 1-5), and Symlet(2-8), followed by feature extraction using relative entropy and power. We classified the meditative state between expert and non-expert meditators employing twelve classifiers to build machine learning models. Wavelet coefficients d8 shows the maximum classification accuracy in all the wavelet families. Wavelet orders Bior3.5 and Coif3 produce the maximum classification performance with the detail coefficient d8 using relative power. We have successfully classified the meditative state between expert and non-expert with 100% accuracy using d5,d6,d7,d8,a8 coefficients. Multi-Layer Perceptron and Quadratic Discriminant Analysis attain the highest accuracy. We have figured out the most discriminating channels during classification and reported 20 channels involving frontal, central and parietal regions. We plot the high dimensional structure of data by utilizing two feature reduction techniques PCA and t-SNE.",1,0,0,supervised classification+multilayer perceptrons+feedforward neural networks,
475,475,"Open domain fine grained class extraction from web search queries. This paper introduces a method for extracting fine-grained class labels ( “countries with double taxation agreements with india” ) from Web search queries. The class labels are more numerous and more diverse than those produced by current extraction methods. Also extracted are representative sets of instances (singapore, united kingdom) for the class labels.",1,0,0,web searches+web search queries+class labels,
476,476,"Harmful comments extraction from a bulletin board system using word meaning and impression on thread context. Harmful documents make readers unpleasant on the Web. In order to hide the harmful documents from the public, machine learning methods have been proposed, which learn words used in harmful documents and hide them automatically. The learned words often have bad meanings. Though word meanings are not changed, word impression may be changed on context. Even if a word with bad impression is contained in a document, the previous learning methods can not learn the word, and fail to hide documents. We select the following approach: word impression may be changed on context. If a word has been used with other words of good meaning, it is considered that impression of the word is also good. In contrast, if a word has been used with others of bad meaning, impression of the word may be bad. This paper proposes a new extraction method of harmful comments in a thread of a Bulletin Board System. The proposed method extracts comments using word meanings and word impression on thread context. We evaluated the proposed method using comments collected from four threads in Japanese BBS ""2-channel."" The averaged precision of extraction was 0.47, and the averaged recall was 0.68. We verified that the proposed method was suitable for extraction of harmful comments from a thread of a BBS.",1,0,0,text document+electronic document+part of speech,
477,477,"Typicality distribution function a new density based data analytics tool. In this paper a new density-based, non-frequentistic data analytics tool, called typicality distribution function (TDF) is proposed. It is a further development of the recently introduced typicality- and eccentricity-based data analytics (TEDA) framework. The newly introduced TDF and its standardized form offer an effective alternative to the widely used probability distribution function (pdf), however, remaining free from the restrictive assumptions made and required by the latter. In particular, it offers an exact solution for any (except a single point) amount of non-coinciding data samples. For a comparison, that the well developed and widely used traditional probability theory and related statistical learning approaches require (theoretically) an infinitely large amount of data samples/ observations, although, in practice this requirement is often ignored. Furthermore, TDF does not require the user to pre-select or assume a particular distribution (e.g. Gaussian or other) or a mixture of such distributions or to pre-define the number of such distributions in a mixture. In addition, it does not require the individual data items to be independent. At the same time, the link with the traditional statistical approaches such as the well-known “nσ” analysis, Chebyshev inequality, etc. offers the interesting conclusion that without the restrictive prior assumptions listed above to which these traditional approaches are tied up the same type of analysis can be made using TDF automatically. TDF can provide valuable information for analysis of extreme processes, fault detection and identification were the amount of observations of extreme events or faults is usually disproportionally small. The newly proposed TDF offers a non-parametric, closed form analytical (quadratic) description extracted from the real data realizations exactly in contrast to the usual practice where such distributions are being pre-assumed or approximated. For example, so called particle filters are also a non-parametric approximation of the traditional statistics; however, they suffer from computational complexity and introduce a large number of dummy data. In addition to that, for several types of proximity/similarity measures (such as Euclidean, Mahalonobis, cosine) it can be calculated recursively, thus, computationally very efficiently and is suitable for real time and online algorithms. Moreover, with a very simple example, it has been illustrated that while traditional probability theory and related statistical approaches can lead in some cases to paradoxically incorrect results and/or to the need for hard prior assumptions to be made. In contrast, the newly proposed TDF can offer a logically meaningful result and an intuitive interpretation automatically and exactly without any prior assumptions. Finally, few simple univariate examples are provided and the process of inference is discussed and the future steps of the development of TDF and TEDA are outlined. Since it is a new fundamental theoretical innovation the areas of applications of TDF and TEDA can span from anomaly detection, clustering, classification, prediction, control, regression to (Kalman-like) filters. Practical applications can be even wider and, therefore, it is difficult to list all of them.",1,0,0,gaussians+on-line algorithms+random variables,
478,478,"Typenet scaling up keystroke biometrics. We study the suitability of keystroke dynamics to authenticate 100K users typing free-text. For this, we first analyze to what extent our method based on a Siamese Recurrent Neural Network (RNN) is able to authenticate users when the amount of data per user is scarce, a common scenario in free-text keystroke authentication. With 1K users for testing the network, a population size comparable to previous works, TypeNet obtains an equal error rate of 4.8% using only 5 enrollment sequences and 1 test sequence per user with 50 keystrokes per sequence. Using the same amount of data per user, as the number of test users is scaled up to 100K, the performance in comparison to 1K decays relatively by less than 5%, demonstrating the potential of TypeNet to scale well at large scale number of users. Our experiments are conducted with the Aalto University keystroke database. To the best of our knowledge, this is the largest free-text keystroke database captured with more than 136M keystrokes from 168K users.",1,0,0,recurrent neural networks+feedforward neural networks+population structure,
479,479,"Casting device for search and rescue aiming higher and faster access in disaster site. This paper discusses a casting device for search and rescue operation to aid higher and faster access for mobile robots. To realize higher performance, the design concept of the device is examined from the following three aspects. First, after the arrangements of the stored tether are categorized, the method of selecting the optimal one is considered to maximize the kinetic energy on the tether. Second, to minimize the resistance caused by the tether, a tapered case for the storage is proposed, expected to mitigate the friction between the tether and the case and to prevent the twist on the tether. Third, the way of anchoring the end of the cast tube is proposed. Finally, experimental results show the validity of the device designed on the proposed concepts, after the device is combined with an unmanned vehicle and a ropeway type robot.",1,0,0,robots+mobile robots+autonomous mobile robot,
480,480,"A multimodal eye movement dataset and a multimodal eye movement segmentation analysis. We present a new dataset with annotated eye movements. The dataset consists of over 800,000 gaze points recorded during a car ride in the real world and in the simulator. In total, the eye movements of 19 subjects were annotated. In this dataset there are several data sources such as the eyelid closure, the pupil center, the optical vector, and a vector into the pupil center starting from the center of the eye corners. These different data sources are analyzed and evaluated individually as well as in combination with respect to their goodness of fit for eye movement classification. These results will help developers of real-time systems and algorithms to find the best data sources for their application. Also, new algorithms can be trained and evaluated on this data set. The data and the Matlab code can be downloaded here this https URL",1,0,0,integrated data+matlab+classification models,
481,481,Extracting glossary sentences from scholarly articles a comparative evaluation of pattern bootstrapping and deep analysis. The paper reports on a comparative study of two approaches to extracting definitional sentences from a corpus of scholarly discourse: one based on bootstrapping lexico-syntactic patterns and another based on deep analysis. Computational Linguistics was used as the target domain and the ACL Anthology as the corpus. Definitional sentences extracted for a set of well-defined concepts were rated by domain experts. Results show that both methods extract high-quality definition sentences intended for automated glossary construction.,1,0,0,linguistics+computational linguistics+syntactic structure,
482,482,"Diverse few shot text classification with multiple metrics. We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning to image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties to existing state-of-the-art metric-based algorithms since a single metric is insufficient to capture complex task variations in natural language domain. To alleviate the problem, we propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy. We make our code and data available for further study.",1,0,0,classification models+sentiment classification+text classification,
483,483,"A comparisons of bkt rnn and lstm for learning gain prediction. The objective of this study is to develop effective computational models that can predict student learning gains, preferably as early as possible. We compared a series of Bayesian Knowledge Tracing (BKT) models against vanilla RNNs and Long Short Term Memory (LSTM) based models. Our results showed that the LSTM-based model achieved the highest accuracy and the RNN based model have the highest F1-measure. Interestingly, we found that RNN can achieve a reasonably accurate prediction of student final learning gains using only the first 40% of the entire training sequence; using the first 70% of the sequence would produce a result comparable to using the entire sequence.",1,0,0,bayesian methods+long short term memory neural networks+recurrent neural networks,
484,484,"Flat2sphere learning spherical convolution for fast features from 360 imagery. While 360° cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield “flat"" filters, yet 360° images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360° imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360° data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360° images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art “flat"" object detector to 360° data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.",1,0,0,neural networks+convolutional neural networks+training sample,
485,485,"Odin automated drift detection and recovery in video analytics. Recent advances in computer vision have led to a resurgence of interest in visual data analytics. Researchers are developing systems for effectively and efficiently analyzing visual data at scale. A significant challenge that these systems encounter lies in the drift in real-world visual data. For instance, a model for self-driving vehicles that is not trained on images containing snow does not work well when it encounters them in practice. This drift phenomenon limits the accuracy of models employed for visual data analytics. In this paper, we present a visual data analytics system, called ODIN, that automatically detects and recovers from drift. ODIN uses adversarial autoencoders to learn the distribution of high-dimensional images. We present an unsupervised algorithm for detecting drift by comparing the distributions of the given data against that of previously seen data. When ODIN detects drift, it invokes a drift recovery algorithm to deploy specialized models tailored towards the novel data points. These specialized models outperform their non-specialized counterpart on accuracy, performance, and memory footprint. Lastly, we present a model selection algorithm for picking an ensemble of best-fit specialized models to process a given input. We evaluate the efficacy and efficiency of ODIN on high-resolution dashboard camera videos captured under diverse environments from the Berkeley DeepDrive dataset. We demonstrate that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x smaller memory footprint compared to a baseline system without automated drift detection and recovery.",1,0,0,computer vision+mode selection+auto encoders,
486,486,An agent model of pedestrian and group dynamics experiments on group cohesion. The simulation of pedestrian dynamics is a consolidated area of application for agent-based based models; however generally the presence of groups and particular relationships among pedestrians is treated in a simplistic way. This work describes an innovative agent-based based approach encapsulating in the pedestrian's behavioural model effects representing both proxemics and a simplified account of influences related to the presence of groups in the crowd. The model is tested in a simple scenario to evaluate the effectiveness of mechanisms to preserve groups cohesion maintaining a plausible overall crowd dynamic.,1,0,0,intelligent agents+autonomous agents+agent model,
487,487,"Adversarially trained deep nets transfer better illustration on image classification. Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.",1,0,0,neural networks+image classification+influence functions,
488,488,"Order compression schemes. Sample compression schemes are schemes for “encoding” a set of examples in a small subset of examples. The long-standing open sample compression conjecture states that, for any concept class \(\mathcal{C}\) of VC-dimension d, there is a sample compression scheme in which samples for concepts in \(\mathcal{C}\) are compressed to samples of size at most d.",1,0,0,lossless compression+compression algorithms+compression methods,
489,489,"A cross conformal predictor for multi label classification. Unlike the typical classification setting where each instance is associated with a single class, in multi-label learning each instance is associated with multiple classes simultaneously. Therefore the learning task in this setting is to predict the subset of classes to which each instance belongs. This work examines the application of a recently developed framework called Conformal Prediction (CP) to the multi-label learning setting. CP complements the predictions of machine learning algorithms with reliable measures of confidence. As a result the proposed approach instead of just predicting the most likely subset of classes for a new unseen instance, also indicates the likelihood of each predicted subset being correct. This additional information is especially valuable in the multi-label setting where the overall uncertainty is extremely high.",1,0,0,classification models+classification methods+supervised classification,
490,490,"Bart goes multilingual the unitn essex submission to the conll 2012 shared task. This paper describes the UniTN/Essex submission to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution. We have extended our CoNLL-2011 submission, based on BART, to cover two additional languages, Arabic and Chinese. This paper focuses on adapting BART to new languages, discussing the problems we have encountered and the solutions adopted. In particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system.",1,0,0,named entity recognition+dependency parsing+relation extraction,
491,491,"Determination of pedestrian flow performance based on video tracking and microscopic simulations. One of the objectives of understanding pedestrian behavior is to predict the effect of proposed changes in the design or evaluation of pedestrian facilities. We want to know the impact to the user of the facilities, as the design of the facilities change. That impact was traditionally evaluated by level of service standards. Another design criterion to measure the impact of design change is measured by the pedestrian flow performance index. This paper describes the determination of pedestrian flow performance based video tracking or any microscopic pedestrian simulation models. Most of pedestrian researches have been done on a macroscopic level, which is an aggregation of all pedestrian movement in pedestrian areas into flow, average speed and area module. Macroscopic level, however, does not consider the interaction between pedestrians. It is also not well suited for prediction of pedestrian flow performance in pedestrian areas or in buildings with some obstruction, that reduces the effective width of the walkways. On the other hand, the microscopic level has a more general usage and considers detail in the design. More efficient pedestrian flow can even be reached with less space. Those results have rejected the linearity assumption of space and flow in the macroscopic level.",1,0,0,architecture designs+video streams+digital videos,
492,492,"Light verb constructions in the szegedparalellfx english hungarian parallel corpus. In this paper, we describe the first English–Hungarian parallel corpus annotated for light verb constructions, which contains 14,261 sentence alignment units. Annotation principles and statistical data on the corpus are also provided, and English and Hungarian data are contrasted. On the basis of corpus data, a database containing pairs of English–Hungarian light verb constructions has been created as well. The corpus and the database can contribute to the automatic detection of light verb constructions and they can enhance performance in several fields of NLP (e.g. parsing, information extraction/retrieval and machine translation).",1,0,0,information extraction+parsing algorithm+machine translations,
493,493,"Graduate employment prediction with bias. The failure of landing a job for college students could cause serious social consequences such as drunkenness and suicide. In addition to academic performance, unconscious biases can become one key obstacle for hunting jobs for graduating students. Thus, it is necessary to understand these unconscious biases so that we can help these students at an early stage with more personalized intervention. In this paper, we develop a framework, i.e., MAYA (Multi-mAjor emploYment stAtus) to predict students' employment status while considering biases. The framework consists of four major components. Firstly, we solve the heterogeneity of student courses by embedding academic performance into a unified space. Then, we apply a generative adversarial network (GAN) to overcome the class imbalance problem. Thirdly, we adopt Long Short-Term Memory (LSTM) with a novel dropout mechanism to comprehensively capture sequential information among semesters. Finally, we design a bias-based regularization to capture the job market biases. We conduct extensive experiments on a large-scale educational dataset and the results demonstrate the effectiveness of our prediction framework.",1,0,0,gan+generative adversarial networks+regularization,
494,494,"Time complexity reduction in efficient global optimization using cluster kriging. Efficient Global Optimization (EGO) is an effective method to optimize expensive black-box functions and utilizes Kriging models (or Gaussian process regression) trained on a relatively small design data set. In real-world applications, such as experimental optimization, where a large data set is available, the EGO algorithm becomes computationally infeasible due to the time and space complexity of Kriging. Recently, the so-called Cluster Kriging methods have been proposed to reduce such complexities for the big data, where data sets are clustered and Kriging models are built on each cluster. Furthermore, Kriging models are combined in an optimal way for the prediction. In addition, we analyze the Cluster Kriging landscape to adopt the existing infill-criteria, e.g., the expected improvement. The approach is tested on selected global optimization problems. It is shown by the empirical studies that this approach significantly reduces the CPU time of the EGO algorithm while maintaining the convergence rate of the algorithm.",1,0,0,gaussians+gaussian processes+global optimization problems,
495,495,"Tajik farsi persian transliteration using statistical machine translation. Tajik Persian is a dialect of Persian spoken primarily in Tajikistan and written with a modified Cyrillic alphabet. Iranian Persian, or Farsi, as it is natively called, is the lingua franca of Iran and is written with the Persian alphabet, a modified Arabic script. Although the spoken versions of Tajik and Farsi are mutually intelligible to educated speakers of both languages, the difference between the writing systems constitutes a barrier to text compatibility between the two languages. This paper presents a system to transliterate text between these two different Persian dialects that use incompatible writing systems. The system also serves as a mechanism to facilitate sharing of computational linguistic resources between the two languages. This is relevant because of the disparity in resources for Tajik versus Farsi.",1,0,0,computational linguistics+machine translations+statistical machine translation,
496,496,"From fingerprint to footprint revealing physical world privacy leakage by cyberspace cookie logs. It is well-known that online services resort to various cookies to track users through users' online service identifiers (IDs) - in other words, when users access online services, various ""fingerprints"" are left behind in the cyberspace. As they roam around in the physical world while accessing online services via mobile devices, users also leave a series of ""footprints"" -- i.e., hints about their physical locations - in the physical world. This poses a potent new threat to user privacy: one can potentially correlate the ""fingerprints"" left by the users in the cyberspace with ""footprints"" left in the physical world to infer and reveal leakage of user physical world privacy, such as frequent user locations or mobility trajectories in the physical world - we refer to this problem as user physical world privacy leakage via user cyberspace privacy leakage. In this paper we address the following fundamental question: what kind - and how much - of user physical world privacy might be leaked if we could get hold of such diverse network datasets even without any physical location information. In order to conduct an in-depth investigation of these questions, we utilize the network data collected via a DPI system at the routers within one of the largest Internet operator in Shanghai, China over a duration of one month. We decompose the fundamental question into the three problems: i) linkage of various online user IDs belonging to the same person via mobility pattern mining; ii) physical location classification via aggregate user mobility patterns over time; and iii) tracking user physical mobility. By developing novel and effective methods for solving each of these problems, we demonstrate that the question of user physical world privacy leakage via user cyberspace privacy leakage is not hypothetical, but indeed poses a real potent threat to user privacy.",1,0,0,ids+linkage analysis+fingerprint,
497,497,"On top k selection in multi armed bandits and hidden bipartite graphs. This paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric, up to a small relative error. We study the topic under two standard settings—multi-armed bandits and hidden bipartite graphs—which differ in the nature of the input distributions. In the former setting, each distribution can be sampled (in the i.i.d. manner) an arbitrary number of times, whereas in the latter, each distribution is defined on a population of a finite size m (and hence, is fully revealed after m samples). For both settings, we prove lower bounds on the total number of samples needed, and propose optimal algorithms whose sample complexities match those lower bounds.",1,0,0,weighted graph+bipartite graphs+general graph,
498,498,"Knowledge management practices in a public sector organisation the role of leaders cognitive styles. Purpose – The purpose of this paper is to investigate the impact of the cognitive styles of leaders on knowledge management practices in a public sector organisation in India.Design/methodology/approach – The data were gathered from 210 middle and senior managers who were employed in different projects across the country. Self‐reported questionnaires were administered to collect the data on the cognitive styles of leaders and knowledge management practices.Findings – The results of exploratory factor analysis showed three significant factors of cognitive styles – i.e. radical, innovative‐collaborator, and adaptor. The knowledge management questionnaire had five dimensions – i.e. KM process, KM leadership, KM culture, KM technology, and KM measurement. The results of regression analysis showed a negative impact of the radical and innovative‐collaborator styles, while the adaptor style had a positive impact on knowledge management practices.Research limitations/implications – This study was conducted in a l...",1,0,0,knowledge management+knowledge management system+organizational memory,
499,499,"Investigating the role of entropy in sentence processing. We outline four ways in which uncertainty might affect comprehension difficulty in human sentence processing. These four hypotheses motivate a self-paced reading experiment, in which we used verb subcategorization distributions to manipulate the uncertainty over the next step in the syntactic derivation (single step entropy) and the surprisal of the verb’s complement. We additionally estimate wordby-word surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG). Surprisal and total entropy, but not single step entropy, were significant predictors of reading times in different parts of the sentence. This suggests that a complete model of sentence processing should incorporate both entropy and surprisal.",1,0,0,syntactics+syntactic structure+part of speech,
500,500,"Slampa recommending code snippets with statistical language model. Programming is typically a difficult and repetitive task. Programmers will encounter endless problems during programming, and they often need to write similar code over and over again. Over the years, many tools have been proposed to support programming. However, to the best of our knowledge, these approaches require high-quality queries or programming contexts, which are often difficult to be built or even unavailable. To address this challenge, we propose SLAMPA, a novel tool which takes advantage of statistical language model and clone detection techniques to recommend code snippets during programming. Given a piece of incomplete code, SLAMPA first infers its intention using a neural language model. Then it retrieves code snippets from codebase with the support of an efficient clone detection technology Hybrid-CD we proposed. Finally, it recommends the most similar code snippets to programmers. Our evaluation results demonstrate that Hybrid-CD precisely detects similar code snippets and it outperforms previous techniques. Our results also show that the snippets recommended by SLAMPA catch the intention of programmers and SLAMPA is capable of finding potential code reuse opportunities during programming.",0,1,0,statistical language modeling+source codes+clone detection,
501,501,"Robustness of networks with skewed degree distributions under strategic node protection. Previous studies on the robustness of networks against intentional attacks have suggested that protecting a small fraction of important nodes in a network significantly improves its robustness. In this paper, we analyze the robustness of networks under several strategic node protection schemes. Strategic node protection schemes select a small fraction of nodes as important nodes, using a network measure such as node centrality, and protect the important nodes to prevent them from being removed by intentional attacks. Our simulation results indicate that (1) strategic node protection significantly improves the robustness of networks with skewed degree distributions, (2) the efficiency of strategic node protection schemes is affected by the strength of community structure of the network being protected, and (3) strategic node protection based on betweenness centrality can effectively improve the robustness of networks regardless of the strength of community structure.",0,1,0,computational efficiency+degree distributions+centrality measures,
502,502,"Software reliability model with coxian distribution of length of intervals between errors detection and fixing moments. The generalized software reliability model on the basis of non-stationary Markovian service system is proposed. Approximation by Coxian distribution allows investigating software reliability growth for any kinds of distribution (for example, Weibull, Gamma) of time between the moments of program errors detection and fixing. The model enables to forecast important software reliability characteristics, such as number of corrected errors, number of errors to be fixed, required debugging time, etc. The diagram of transitions between states of the generalized model and differential equations system are presented. The example of calculation with use of the offered model is considered, research of influence of Coxian distributions variation coefficients of duration of intervals between the error detection moments and error correction time distributions on values of look-ahead characteristics is executed.",0,1,0,software reliability+reliability growth+software reliability models,
503,503,"Ausum approach for unsupervised bug report summarization. In most software projects, resolved bugs are archived for future reference. These bug reports contain valuable information on the reported problem, investigation and resolution. When bug triaging, developers look for how similar problems were resolved in the past. Search over bug repository gives the developer a set of recommended bugs to look into. However, the developer still needs to manually peruse the contents of the recommended bugs which might vary in size from a couple of lines to thousands. Automatic summarization of bug reports is one way to reduce the amount of data a developer might need to go through. Prior work has presented learning based approaches for bug summarization. These approaches have the disadvantage of requiring large training set and being biased towards the data on which the model was learnt. In fact, maximum efficacy was reported when the model was trained and tested on bug reports from the same project. In this paper, we present the results of applying four unsupervised summarization techniques for bug summarization. Industrial bug reports typically contain a large amount of noise---email dump, chat transcripts, core-dump---useless sentences from the perspective of summarization. These derail the unsupervised approaches, which are optimized to work on more well-formed documents. We present an approach for noise reduction, which helps to improve the precision of summarization over the base technique (4% to 24% across subjects and base techniques). Importantly, by applying noise reduction, two of the unsupervised techniques became scalable for large sized bug reports.",0,1,0,software bug+software defects+bug reports,
504,504,"Super dense computation in verification of hybrid csp processes. Hybrid Communicating Sequential Processes (HCSP) extends CSP to include differential equations and interruptions. We feel comfortable in our experience with HCSP to model scenarios of the Level 3 of Chinese Train Control System (CTCS-3), and to define a formal semantics for Simulink. The Hoare style calculus of [5] proposes a calculus to verify HCSP processes. However it has an error with respect to super-dense computation. This paper is to establish another calculus for a subset of HCSP, which uses Duration Calculus formulas to record program history, negligible time state to denote super-dense computation and semantic continuation to avoid infinite interval. It is compositional and sound.",0,1,0,formal semantics+verification+communicating sequential process,
505,505,"Improving software bug specific named entity recognition with deep neural network. Abstract There is a large volume of bug data in the bug repository, which contains rich bug information. Existing studies on bug data mining mainly rely on using information retrieval (IR) technology to search relevant historical bug reports. These studies basically treat a bug report as a closed unit, ignoring the semantic and structural information within it. Named-entity recognition (NER) is an important task of information extraction (IE) technology. Based on NER, fine-grained factual information could be comprehensively extracted to further form structured data, which provides a new way to improve the accessibility of bug information. However, bug NER is different from general NER tasks. Bug reports are free-form text, which include a mixed language environment studded with code, abbreviations and software-specific vocabularies. In this paper, we propose a deep neural network approach for bug-specific entity recognition called DBNER using bidirectional long short-term memory (LSTM) with Conditional Random Fields decoding model (CRF). DBNER extracts multiple features from the massive bug data and uses attention mechanism to improve the consistency of entity tags in the bug reports. Experiment results show that the F1-score reaches an average of 91.19%. In addition, in cross-project experiments, the DBNER’s F1-score reaches an average of 84%.",0,1,0,software+software bug+bug reports,
506,506,"Extracting domain models from natural language requirements approach and industrial evaluation. Domain modeling is an important step in the transition from natural-language requirements to precise specifications. For large systems, building a domain model manually is a laborious task. Several approaches exist to assist engineers with this task, whereby candidate domain model elements are automatically extracted using Natural Language Processing (NLP). Despite the existing work on domain model extraction, important facets remain under-explored: (1) there is limited empirical evidence about the usefulness of existing extraction rules (heuristics) when applied in industrial settings; (2) existing extraction rules do not adequately exploit the natural-language dependencies detected by modern NLP technologies; and (3) an important class of rules developed by the information retrieval community for information extraction remains unutilized for building domain models.   Motivated by addressing the above limitations, we develop a domain model extractor by bringing together existing extraction rules in the software engineering literature, extending these rules with complementary rules from the information retrieval literature, and proposing new rules to better exploit results obtained from modern NLP dependency parsers. We apply our model extractor to four industrial requirements documents, reporting on the frequency of different extraction rules being applied. We conduct an expert study over one of these documents, investigating the accuracy and overall effectiveness of our domain model extractor.",0,1,0,software engineering+software reengineering+requirements document,
507,507,"A fine grained analysis of the support provided by uml class diagrams and er diagrams during data model maintenance. This paper presents the results of an empirical study aiming at comparing the support provided by ER and UML class diagrams during maintenance of data models. We performed one controlled experiment and two replications that focused on comprehension activities (the first activity in the maintenance process) and another controlled experiment on modification activities related to the implementation of given change requests. The results achieved were analyzed at a fine-grained level aiming at comparing the support given by each single building block of the two notations. Such an analysis is used to identify weaknesses (i.e., building blocks not easy to comprehend) in a notation and/or can justify the need of preferring ER or UML for data modeling. The analysis revealed that the UML class diagrams generally provided a better support for both comprehension and modification activities performed on data models as compared to ER diagrams. Nevertheless, the former has some weaknesses related to three building blocks, i.e., multi-value attribute, composite attribute, and weak entity. These findings suggest that an extension of UML class diagrams should be considered to overcome these weaknesses and improve the support provided by UML class diagrams during maintenance of data models.",0,1,0,class diagrams+uml class diagrams+unified modeling language,
508,508,"Heterogeneous defect prediction with two stage ensemble learning. Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In the EDS stage, we employ RESample with replacement (RES) technique to learn multiple different EMKCA predictors and use average ensemble to combine them together. These two stages create an ensemble of defect predictors. Extensive experiments on 30 public projects show that the proposed TSEL approach outperforms a range of competing methods. The improvement is 20.14–33.92% in AUC, 36.05–54.78% in f-measure, and 5.48–19.93% in balance, respectively.",0,1,0,ensemble classifiers+base classifiers+ensemble learning,
509,509,"A complete run time overhead aware schedulability analysis for mrsp under nested resources. Abstract Multiprocessor Resource Sharing Protocol (MrsP) is a hard real-time multiprocessor resource sharing protocol for fully partitioned fixed-priority systems, and adopts a novel helping mechanism to allow task migrations during resource accessing. Previous research focusing on analysing MrsP systems have delivered two forms of timing analysis which effectively bound response time and migration cost of tasks under MrsP, and have demonstrated advantages of this protocol. An adjustable non-preemptive section is also introduced that effectively reduces the number of migrations needed during each resource access. However, these analysis methods are only applicable if a non-nested resource accessing model is assumed. In addition, there is no clear approach towards the configuration of the non-preemptive section length, and the computation cost for applying the analysis remains unknown. In this paper, we extend the MrsP analysis for systems with nested resources. Major run-time costs incurred by MrsP tasks are also taken into account to form a complete run-time cost-aware schedulability analysis. In addition, recommendations towards non-preemptive section configuration are given from both analytic and empiric perspectives. Finally, a set of evaluations are conducted to investigate schedulability of MrsP under nested resources and the cost for applying the proposed analysis. As a result of this paper, the schedulability test for MrsP is complete and the computation costs of its use are now understood.",0,1,0,schedulability test+schedulability analysis+timing analysis,
510,510,"How practitioners perceive the relevance of software engineering research. The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.",0,1,0,requirements engineering+software engineering education+software development practices,
511,511,"Interactive query suggestion in thai library automation system. This paper introduces interactive query expansion for Thai library automation system. The approach aims to expand original Thai-language user queries to a new set of more meaningful queries in the same language. The purpose is to improve the quality of Thai user query and suggest the user with a new set of more meaningful queries. User satisfaction with retrieved results should be improved. The approach employs the information in MARC 21 bibliographic format (title access field 245) in order to generate a new list of candidate terms. The genetic process is then augmented to select new expanded terms from those candidates. The approach is evaluated on the collection of Thai educational materials such as book, journal and magazine. The performance is measured by the accuracy of new expanded terms. We asked 87 participants whether our generated terms are related to their original query. The experiment shows that users quite agree that our technique is able to generate useful terms for searching their materials in library system.",0,1,0,keyword queries+query processing+query results,
512,512,"Querypoint moving backwards on wrong values in the buggy execution. As developers debug, they often have to seek the origins of wrong values they see in their debugger. This search must be performed backwards in time since the code causing the wrong value is executed before the wrong value appears. Therefore, locating the origin of wrong values with breakpoint- or log- based debuggers demands persistence and significant experience.   Querypoint, is a Firefox plugin that enhances the popular Firebug JavaScript debugger with a new, practical feature called lastChange. lastChange automatically locates the last point at which a variable or an object property has been changed. Starting from a program suspended on a breakpoint, the lastChange algorithm applies queries to the live program during re-execution, recording the call stack and limited program state each time the property value changes. When the program halts again on the breakpoint, it shows the call stack and program state at the last change point. To evaluate the usability and effectiveness of Querypoint we studied four experienced JavaScript developers applying the tool to two test cases.",0,1,0,program debugging+firefox+software developer,
513,513,"Safety add a tool for safety contract based design. Safety ADD is a tool for working with safety contracts for software components. Safety contracts tie safety related properties, in the form of guarantees and assumptions, to a component. A guarantee is a property the component promises to hold, on the premise that the environment provides its associated assumptions. When multiple software components are integrated in a system, Safety ADD is used to verify that the guarantees and assumptions match when there are safety-related dependencies between the components. The initial goal of Safety ADD is to investigate how safety contracts can be managed and used efficiently within the software design process. It is implemented as an Eclipse plug in. The tool has two main functions. It gives designers of software components a way to specify safety contracts, which are stored in an XML format and shall be distributed together with the component. It also gives developers who integrate multiple software components in their systems a tool to verify that the safety contracts are fulfilled. A graphical editor is used to connect guarantees and assumptions for dependent components, and an algorithm traverses all such connections to make sure they match.",0,1,0,software component+software design+software,
514,514,"A systematic mapping study of the current practice of indian software engineering. Systematic mapping studies are an important research method and have been used in software engineering to provide an overview of a research area by a process of classification and counting of the outputs in a particular area. They have also been used to examine the outputs found in specific publication outlets. In this paper, we report on the results of a systematic mapping study conducted to review the entire publication output of the Indian Software Engineering Conference (ISEC) series. We use the outputs of the study to present visual depictions of the nature of Indian Software Engineering academic research from 2008--2015. A second contribution of the work reports on comparison of the ISEC series with that of the pre-eminent international conference in software engineering (ICSE). We contextualise the results within the wider picture of the national Indian IT community.",0,1,0,software+software engineering+software reengineering,
515,515,"Automatically classifying user requests in crowdsourcing requirements engineering. Abstract In order to make a software project succeed, it is necessary to determine the requirements for systems and to document them in a suitable manner. Many ways for requirements elicitation have been discussed. One way is to gather requirements with crowdsourcing methods, which has been discussed for years and is called crowdsourcing requirements engineering. User requests forums in open source communities, where users can propose their expected features of a software product, are common examples of platforms for gathering requirements from the crowd. Requirements collected from these platforms are often informal text descriptions and we name them user requests. In order to transform user requests into structured software requirements, it is better to know the class of requirements that each request belongs to so that each request can be rewritten according to corresponding requirement templates. In this paper, we propose an effective classification methodology by employing both project-specific and non-project-specific keywords and machine learning algorithms. The proposed strategy does well in achieving high classification accuracy by using keywords as features, reducing considerable manual efforts in building machine learning based classifiers, and having stable performance in finding minority classes no matter how few instances they have.",0,1,0,requirements engineering+requirements specifications+requirements engineering process,
516,516,"Verifying linearizability via optimized refinement checking. Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and a combination of both for refinement checking. We have built the method into the PAT model checker, and used PAT to automatically check a variety of implementations of concurrent objects, including the first algorithm for scalable nonzero indicators. Our system is able to find all known and injected bugs in these implementations.",0,1,0,state-space explosion+model checker+partial order reductions,
517,517,"A model for the evaluation of educational games for teaching software engineering. Teaching software engineering through educational games is expected to have several benefits. Various games have already been developed in this context, yet there is still a lack of assessment models to measure the real benefits and quality of these educational resources. This article presents the development of a model for assessing the quality of educational games for teaching software engineering. The model has been systematically derived from literature and evaluated in terms of its applicability, usefulness, validity and reliability through a series of case studies, applying educational board games in software engineering courses. Early results indicate that the model can be used to assess the aspects of motivation, user experience and learning of educational SE games.",0,1,0,software engineering+software reengineering+software engineering education,
518,518,"Rethinking pointer reasoning in symbolic execution. Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises. In this paper we introduce MemSight, a new approach to symbolic memory that reduces the need for concretization, hence offering the opportunity for broader state explorations and more precise pointer reasoning. Rather than mapping address instances to data as previous tools do, our technique maps symbolic address expressions to data, maintaining the possible alternative states resulting from the memory referenced by a symbolic address in a compact, implicit form. A preliminary experimental investigation on prominent benchmarks from the DARPA Cyber Grand Challenge shows that MemSight enables the exploration of states unreachable by previous techniques.",0,1,0,memory access+program analysis+symbolic execution,
519,519,"Embedded controlled gardening an academically based service course. A course intended to integrate concepts of basic physics, biology, electronics, and systems engineering for the benefit of University of Pennsylvania engineering students, plus teachers and students from three community public schools located in Philadelphia. The course engaged participants in the design and the implementation of an indoor cultivating system using photo-voltaic technology to energize Light Emitting Diodes emulating the needed solar radiation for plant growth, a liquid nutrient distribution system, sensors / actuators capable of selecting the harvestable plants and keeping track of overall system parameters.",0,1,0,systems design+university+engineering,
520,520,"Symbolic and statistical theories of cognition towards integrated artificial intelligence. There are two types of approaches to Artificial Intelligence, namely Symbolic AI and Statistical AI. The symbolic and statistical paradigms of cognition may be considered to be in conflict with each other; the recent debate between Chomsky and Norvig exemplifies a fundamental tension between the two paradigms (esp. on language), which is arguably in parallel with a conflict on interpretations of quantum theory as seen between Bohr and Einstein, one side arguing for the probabilist or empiricist view and the other for the universalist or rationalist view. In the present paper we explicate and articulate the fundamental discrepancy between them, and explore how a unifying theory could be developed to integrate them, and what sort of cognitive roles Integrated AI could play in comparison with present-day AI. We give, inter alia, a classification of Integrated AI, and argue that Integrated AI serves the purpose of humanising AI in terms of making AI more verifiable, more explainable, more causally accountable, more ethical, and thus closer to general intelligence. We especially emphasise the ethical advantage of Integrated AI. We also briefly touch upon the Turing Test for Ethical AI, and the pluralistic nature of Turing-type Tests for Integrated AI. Overall, we believe that the integrated approach to cognition gives the key to the next generation paradigm for AI and Cognitive Science in general, and that Categorical Integrated AI or Categorical Integrative AI Robotics would be arguably the most promising approach to it.",0,1,0,robotics+artificial intelligence+intelligent systems,
521,521,"Test case prioritization of build acceptance tests for an enterprise cloud application. A novel process for prioritizing Build Acceptance Tests using historical failures.A set of black box prioritization heuristics for SaaS Build Acceptance Testing.An industrial case study on a large enterprise cloud production application.Data showing the complexity of service combinations leading to field failures.Empirical evidence that the order of running Build Acceptance tests in SaaS matters. The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't.",0,1,0,saas+software+test case prioritization,
522,522,"Toward e health applications for suicide prevention. E-health applications are methods used by medical practitioners to assess and follow their patients' mental and physical conditions. They may also provide feedback to patients and help them to change their behavior. Our research project aims to propose a mobile application (app in short) for suicide prevention. The principle is to develop a connected tool used by the patient to report about his/her health status. The use of a connected tool at home has the potential to overcome well-known limitations of self-reporting in a clinical context. However, developing an efficient e-health app is challenging, especially for sensible topics such as suicide prevention. The application must be developed carefully (data protection, HMI, ergonomic, question choice) in orderto increase its acceptance by both patients and practitioners. Here, we relate ongoing world-wide initiatives and we propose some requirements towards the development of an efficient intelligent-health (i-health) application for suicide prevention.",0,1,0,smart phones+data protection+mobile applications,
523,523,Mechanisms to handle structural variability in matlab simulink models. Systematically postponing variability binding is an important design concept in Software Product Line Engineering in order to increase flexibility. One major challenge is the technical implementation of respective binding mechanisms in different tool environments and artifacts.,0,1,0,product lines+software product line+software product line engineerings,
524,524,"Java enterprise edition support in search based junit test generation. Many different techniques and tools for automated unit test generation target the Java programming languages due to its popularity. However, a lot of Java’s popularity is due to its usage to develop enterprise applications with frameworks such as Java Enterprise Edition (JEE) or Spring. These frameworks pose challenges to the automatic generation of JUnit tests. In particular, code units (“beans”) are handled by external web containers (e.g., WildFly and GlassFish). Without considering how web containers initialize these beans, automatically generated unit tests would not represent valid scenarios and would be of little use. For example, common issues of bean initialization are dependency injection, database connection, and JNDI bean lookup. In this paper, we extend the EvoSuite search-based JUnit test generation tool to provide initial support for JEE applications. Experiments on 247 classes (the JBoss EAP tutorial examples) reveal an increase in code coverage, and demonstrate that our techniques prevent the generation of useless tests (e.g., tests where dependencies are not injected).",0,1,0,enterprise information system+java+test generations,
525,525,"An automated approach to assess the similarity of github repositories. Open source software (OSS) allows developers to study, change, and improve the code free of charge. There are several high-quality software projects which deliver stable and well-documented products. Most OSS forges typically sustain active user and expert communities which in turn provide decent levels of support both with respect to answering user questions as well as to repairing reported software bugs. Code reuse is an intrinsic feature of OSS, and developing a new system by leveraging existing open source components can reduce development effort, and thus it can be beneficial to at least two phases of the software life cycle, i.e., implementation and maintenance. However, to improve software quality, it is essential to develop a system by learning from well-defined, mature projects. In this sense, the ability to find similar projects that facilitate the undergoing development activities is of high importance. In this paper, we address the issue of mining open source software repositories to detect similar projects, which can be eventually reused by developers. We propose CrossSim as a novel approach to model the OSS ecosystem and to compute similarities among software projects. An evaluation on a dataset collected from GitHub shows that our proposed approach outperforms three well-established baselines.",0,1,0,software quality+software bug+software repositories,
526,526,"A metrics model to measure the impact of an agile transformation in large software development organizations. As the adoption of agile and lean methods continues to grow, measuring the effects of such a transformation can be valuable but challenging due to the many variables influencing the outcome of a software project. In this paper we present a metrics model developed for measuring the effects of an agile and lean transformation on software development organizations. The model was developed iteratively in cooperation with industry partners within the Cloud Software Finland research project. The resulting metrics model is applicable to projects of any size, complexity and scope, using metrics that support agile and lean values. The model can be used to measure both past and ongoing projects, regardless of whether the process model used is plan driven or agile. In order to evaluate the metrics model, the proposed model has been piloted in an industry setting.",0,1,0,software development+software project+software development organizations,
527,527,"A symbolic analysis framework for static analysis of imperative programming languages. Highlights? We present a generic symbolic analysis framework for imperative programming languages. ? Our framework computes all valid variable bindings at a given program point. ? Variable bindings are kept in an algebraic structure called supercontext. ? Path expression algebra models control flow information of programs. ? A homomorphism maps path expressions into the symbolic domain. We present a generic symbolic analysis framework for imperative programming languages. Our framework is capable of computing all valid variable bindings of a program at given program points. This information is invaluable for domain-specific static program analyses such as memory leak detection, program parallelization, and the detection of superfluous bound checks, variable aliases and task deadlocks.We employ path expression algebra to model the control flow information of programs. A homomorphism maps path expressions into the symbolic domain. At the center of the symbolic domain is a compact algebraic structure called supercontext. A supercontext contains the complete control and data flow analysis information valid at a given program point.Our approach to compute supercontexts is based purely on algebra and is fully automated. This novel representation of program semantics closes the gap between program analysis and computer algebra systems, which makes supercontexts an ideal symbolic intermediate representation for all domain-specific static program analyses.Our approach is more general than existing methods because it can derive solutions for arbitrary (even intra-loop and nested loop) nodes of reducible and irreducible control flow graphs. We prove the correctness of our symbolic analysis method. Our experimental results show that the problem sizes arising from real-world applications such as the SPEC95 benchmark suite are tractable for our symbolic analysis framework.",0,1,0,program analysis+static analysis+static program analysis,
528,528,"Libdx a cross platform and accurate system to detect third party libraries in binary code. With the development of the open-source movement, third-party library reuse is commonly practiced in programming. Application developers can reuse the code to save time and development costs. However, there are some hidden risks in misusing third-party libraries such as license violation and security vulnerability. The identification of libraries written in C or C++ is impeded by compilation process which hides most features of code. The same open-source package can be compiled into different binary code by different compilation processes. Therefore, this paper proposes LibDX, a platform-independent and fully-automated system, to detect reused libraries in binary files. With a well-designed feature extractor, LibDX can overcome compilation diversity between binary files. LibDX novelly introduces the logic feature block concept which is applied to deal with the feature duplication challenge in a large-scale feature database. We built a large test data set covering multiple platforms and evaluated LibDX with 9.5K packages including 25.8K C/C++ binary files. Our results show that LibDX achieves a precision of 92% and a recall of 97%, and outperforms state-of-the-art tools. We have validated the performance of the system with closed source commercial applications and found some license violation cases.",0,1,0,libraries+binary codes+software developer,
529,529,"Semantic program repair using a reference implementation. Automated program repair has been studied via the use of techniques involving search, semantic analysis and artificial intelligence. Most of these techniques rely on tests as the correctness criteria, which causes the test overfitting problem. Although various approaches such as learning from code corpus have been proposed to address this problem, they are unable to guarantee that the generated patches generalize beyond the given tests. This work studies automated repair of errors using a reference implementation. The reference implementation is symbolically analyzed to automatically infer a specification of the intended behavior. This specification is then used to synthesize a patch that enforces conditional equivalence of the patched and the reference programs. The use of the reference implementation as an implicit correctness criterion alleviates overfitting in test-based repair. Besides, since we generate patches by semantic analysis, the reference program may have a substantially different implementation from the patched program, which distinguishes our approach from existing techniques for regression repair like Relifix. Our experiments in repairing the embedded Linux Busybox with GNU Coreutils as reference (and vice-versa) revealed that the proposed approach scales to real-world programs and enables the generation of more correct patches.",0,1,0,embedded linux+formal proofs+linux,
530,530,"On construction of a library of formally verified low level arithmetic functions. Arithmetic functions are used in many important computer programs such as computer algebra systems and cryptographic software. The latter are critical applications whose correct implementation deserves to be formally guaranteed. They are also computation-intensive applications, so that programmers often resort to low-level assembly code to implement arithmetic functions. We propose an approach for the construction of a library of formally verified low-level arithmetic functions. To build our library, we first introduce a formalization of data structures for signed multi-precision arithmetic in low-level programs. We use this formalization to verify the implementation of several primitive arithmetic functions using Separation logic, an extension of Hoare logic to deal with pointers. Since this direct style of formal verification leads to technically involved specifications, we also propose for larger functions to show a formal simulation relation between pseudo-code and assembly. This style of verification is illustrated with a concrete implementation of the binary extended gcd algorithm.",0,1,0,separation logic+formal verifications+verification techniques,
531,531,"A knowledge mapping technique for project level knowledge flow analysis. A pilot study was conducted within a software development domain using ""knowledge mapping"" (K-mapping) as a research technique to locate and indicate improvements to problematic software project-level knowledge flows (K-flows). The goal of the study is to show that the mitigation or removal of obstacles to project-level K-flow will result in software process improvement (SPI) by lowering project labor cost, improving schedule adherence, and/or enhancing end-product quality. Results show support for the efficacy of the study's K-mapping technique for the identification and management of software project K-flow obstacles. Additionally, the suggested alleviation of such obstacles was acknowledged by project team members as having the potential for a positive effect on SPI.",0,1,0,software development+software project+software process improvement,
532,532,"Cross lifecycle variability analysis utilizing requirements and testing artifacts. Abstract Variability analysis is an essential activity that supports increasing and systemizing reuse across similar software products. Current studies use different types of artifacts for analyzing variability, most notably are architecture or design, requirements, and code. While architecture, design, and code help understand and model the differences in solutions and realizations, requirements enable capturing differences in a higher level of abstraction through the intended use of the software products or their behavior. However, analyzing variability based on requirements may result in inaccurate outcomes, due to the informal and incomplete nature of requirements. To tackle this deficiency, we call for augmenting requirements-based variability analysis with other behavior-related cross-lifecycle artifacts. Particularly, we extend an approach that compares and analyzes software behaviors based on requirements taking into account both ontological and semantic considerations. Using test cases and their relations to requirements, our extension, named SOVA R-TC, extract software behaviors more comprehensively, including their preconditions, post-conditions, and expected results. The outputs of SOVA R-TC are feature diagrams, which group similar behaviors and present variability of software products in a tree structure. Empirically evaluating outcomes of SOVA R-TC, they seem to be perceived as significantly better than outcomes generated based on requirements only.",0,1,0,software+system requirements+software project,
533,533,"Second international workshop on software architecture and metrics sam 2015. Software engineers and architects of complex software systems need to balance hard quality attribute requirements while at the same time manage risks and make decisions with a system-wide and long-lasting impact. To achieve these tasks efficiently, they need quantitative information about design-time and run-time system aspects through usable and quick tools. While there is body of work focusing on code quality and metrics, their applicability at the design and architecture level and at scale are inconsistent and not proven. We are interested in exploring whether architecture can assist with better contextualizing existing system and code quality and metrics approaches. Furthermore, we ask whether we need additional architecture-level metrics to make progress and whether something as complex and subtle as software architecture can be quantified. The goal of this workshop is to discuss progress, gather empirical evidence, and identify priorities for a research agenda on architecture and metrics in the software engineering field.",0,1,0,software systems+software engineering+software reengineering,
534,534,"Migrationminer an automated detection tool of third party java library migration at the method level. In this paper we introduce, MigrationMiner, an automated tool that detects code migrations performed between Java third-party library. Given a list of open source projects, the tool detects potential library migration code changes and collects the specific code fragments in which the developer replaces methods from the retired library with methods from the new library. To support the migration process, MigrationMiner collects the library documentation that is associated with every method involved in the migration. We evaluate our tool on a benchmark of manually validated library migrations. Results show that MigrationMiner achieves an accuracy of 100%. A demo video of Migration-Miner is available at https://youtu.be/sAlR1HNetXc.",0,1,0,java+code fragments+open source projects,
535,535,"Continuous test generation on guava. Search-based testing can be applied to automatically generate unit tests that achieve high levels of code coverage on object-oriented classes. However, test generation takes time, in particular if projects consist of many classes, like in the case of the Guava library. To allow search-based test generation to scale up and to integrate it better into software development, continuous test generation applies test generation incrementally during continuous integration. In this paper, we report on the application of continuous test generation with EvoSuite at the SSBSE’15 challenge on the Guava library. Our results show that continuous test generation reduces the time spent on automated test generation by 96 %, while increasing code coverage by 13.9 % on average.",0,1,0,test generations+atpg+automatic test pattern generation,
536,536,"Emerging trends challenges and experiences in devops and microservice apis. In August 2019, we organized the second Vienna Software Seminar (VSS) with the topic ""DevOps and Microservice APIs.""1 Embracing the positive reception of its first iteration in 2017,2 VSS is an opportunity for attendees to discuss recent software technologies, practices, and related research. The seminar's 34 participants included a mix of practitioners and academics, who were invited based on their roles and experiences. The explicit intention of the seminar was to provide ample opportunities for exchange and communication: six themed sessions consisted of one invited keynote and two lightning talks, giving different perspectives on the session?s topic and (ideally) sparking ideas for follow-up discussions. After the talks, all participants decided on subtopics for two to three breakout sessions (i.e., informal, self-organized discussions among interested participants). Breakout session topics included microservice security, tooling for application programming interface (API) evolution, serverless programming models, and identification of microservices using domaindriven design. The sessions provided opportunities for detailed discussions and identifying challenges to address in future collaborations. Toward the end of each session, all participants gathered once more to summarize the breakout discussions. Additional opportunities for communication were provided during shared lunch breaks and social events in the evenings.",0,1,0,software+microservices+devops,
537,537,Context bounded model checking with esbmc 1 17. ESBMC is a context-bounded symbolic model checker for single- and multi-threaded ANSI-C code. It converts the verification conditions using different background theories and passes them directly to an SMT solver.,0,1,0,model checking+bounded model checking+model checker,
538,538,"A history based matching approach to identification of framework evolution. In practice, it is common that a framework and its client programs evolve simultaneously. Thus, developers of client programs may need to migrate their programs to the new release of the framework when the framework evolves. As framework developers can hardly always guarantee backward compatibility during the evolution of a framework, migration of its client program is often time-consuming and error-prone. To facilitate this migration, researchers have proposed two categories of approaches to identification of framework evolution: operation-based approaches and matching-based approaches. To overcome the main limitations of the two categories of approaches, we propose a novel approach named HiMa, which is based on matching each pair of consecutive revisions recorded in the evolution history of the framework and aggregating revision-level rules to obtain framework-evolution rules. We implemented our HiMa approach as an Eclipse plug-in targeting at frameworks written in Java using SVN as the version-control system. We further performed an experimental study on HiMa together with a state-of-art approach named AURA using six tasks based on three subject Java frameworks. Our experimental results demonstrate that HiMa achieves higher precision and higher recall than AURA in most circumstances and is never inferior to AURA in terms of precision and recall in any circumstances, although HiMa is computationally more costly than AURA.",0,1,0,formal framework+software developer+java,
539,539,"On the relationship between comment update practices and software bugs. When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners.",0,1,0,software reengineering+software bug+software defects,
540,540,"Behind the scenes an approach to incorporate context in gui test case generation. Graphical user interfaces (GUIs) are a common way to interact with software. To ensure the quality of such software it is important to test the possible interactions with its user interface. GUI testing is a challenging task as they can allow, in general, infinitely many different sequences of interactions with the software. As it is only possible to test a limited amount of possible user interactions, it is crucial for the quality of GUI testing to identify relevant sequences and avoid improper ones. In this paper we propose a model for better GUI testing. Our model is created based on two observations. It is a common case that different user interactions result in the execution of the same code fragments. That is, it is sufficient to test only interactions that execute different code fragments. Our second observation is that user interactions are context-sensitive. That is, the control flow that is taken in a program fragment handling a user interaction depends on the order of some preceding user interactions. We show that these observations are relevant in practice. We present a preliminary implementation that utilizes these observations for test case generation.",0,1,0,code fragments+test case generation+gui testing,
541,541,"A graph based dataset of commit history of real world android apps. Obtaining a good dataset to conduct empirical studies on the engineering of Android apps is an open challenge. To start tackling this challenge, we present AndroidTimeMachine, the first, self-contained, publicly available dataset weaving spread-out data sources about real-world, open-source Android apps. Encoded as a graph-based database, AndroidTimeMachine concerns 8,431 real open-source Android apps and contains: (i) metadata about the apps' GitHub projects, (ii) Git repositories with full commit history and (iii) metadata extracted from the Google Play store, such as app ratings and permissions.",0,1,0,iphone+android+engineering,
542,542,"Handling refinement of continuous behaviors a proof based approach with event b. Cyber-physical systems (CPS) are taking a crucial role in various areas of our society and industry. Yet, because of their hybrid nature (i.e. the integration of both continuous and discrete features), their design and verification are not easy to handle, in particular when they are part of a critical system. Their certification requires to exhibit a formal argumentation that formal methods should be able to provide. This paper addresses the formal development of CPS using correct-by-construction refinement and proof based approaches. It relies on the Event-B formal method. In addition to modeling both the discrete and continuous parts of a CPS, this paper presents a novel approach in two steps. First it shows that the generic formal model we have defined, integrating both discrete and continuous behaviors, can be instantiated by various kinds of CPS. Fundamentally, continuous behaviors modeled by differential equations mingle with discrete transition systems (mode automaton), which model discrete behaviors. Here, refinement is used as a decomposition mechanism. Second, it expands the refinement operation, well mastered in the discrete world, to cover continuous behaviors. We show that different levels of abstraction of continuous aspects can be glued in a refinement chain. The proposed approach has been completely formalized using Event-B on the Rodin platform and a case study based on water tanks is used to illustrate it.",0,1,0,formal specification+formal methods+event-b,
543,543,"Industry trends 2017. A survey of software industry professionals revealed trends involving efficiency and cost, security and safety, innovation, the digital transformation, connectivity, and governance and compliance. On the basis of the survey and personal experience in the industry, researchers have developed recommendations on how software development organizations can deal with these trends.",0,1,0,software development+software industry+software development organizations,
544,544,"End to end formal verification of ethereum 2 0 deposit smart contract. We report our experience in the formal verification of the deposit smart contract, whose correctness is critical for the security of Ethereum 2.0, a new Proof-of-Stake protocol for the Ethereum blockchain. The deposit contract implements an incremental Merkle tree algorithm whose correctness is highly nontrivial, and had not been proved before. We have verified the correctness of the compiled bytecode of the deposit contract to avoid the need to trust the underlying compiler. We found several critical issues of the deposit contract during the verification process, some of which were due to subtle hidden bugs of the compiler.",0,1,0,formal verifications+ethereum+verification techniques,
545,545,"Fault based testing of combining algorithms in xacml3 0 policies. With the increasing complexity of software, new access control methods have emerged to deal with attribute- based authorization. As a standard language for attribute-based access control policies, XACML offers a number of rule and policy combining algorithms to meet different needs of policy composition. Due to their variety and complexity, however, it is not uncommon to apply combining algorithms incorrectly, which can lead to unauthorized access or denial of service. To solve this problem, this paper presents a fault-based testing approach for determining incorrect combining algorithms in XACML 3.0 policies. It exploits an efficient constraint solver to generate queries to which a given policy produces different responses than its combining algorithm-based mutants. Such queries can determine whether or not the given combining algorithm is used correctly. Our empirical studies using sizable XACML policies have demonstrated that our approach is effective.",0,1,0,constraint solvers+xacml+software,
546,546,"A systematic mapping study on modeling for industry 4 0. Industry 4.0 is a vision of manufacturing in which smart, interconnected production systems optimize the complete value-added chain to reduce cost and time-to-market. At the core of Industry 4.0 is the smart factory of the future, whose successful deployment requires solving challenges from many domains. Model-based systems engineering (MBSE) is a key enabler for such complex systems of systems as can be seen by the increased number of related publications in key conferences and journals. This paper aims to characterize the state of the art of MBSE for the smart factory through a systematic mapping study on this topic. Adopting a detailed search strategy, 1466 papers were initially identified. Of these, 222 papers were selected and categorized using a particular classification scheme. Hence, we present the concerns addressed by the modeling community for Industry 4.0, how these are investigated, where these are published, and by whom. The resulting research landscape can help to understand, guide, and compare research in this field. In particular, this paper identifies the Industry 4.0 challenges addressed by the modeling community, but also the challenges that seem to be less investigated.",0,1,0,engineering+model-based systems engineering+requirements engineering,
547,547,"What s spain s paris mining analogical libraries from q a discussions. Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with for different programming languages or different mobile platforms. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&A posts, which often contain overwhelming or out-of-date information. In this paper, we present a new approach to recommend analogical libraries based on a knowledge base of analogical libraries mined from tags of millions of Stack Overflow questions. The novelty of our approach is to solve analogical-library questions by combining state-of-the-art word embedding technique and domain-specific relational and categorical knowledge mined from Stack Overflow. Given a library and a recommended analogical library, our approach further extracts questions and answer snippets in Stack Overflow about comparison of analogical libraries, which can potentially offer useful information scents for developers to further their investigation of the recommended analogical libraries. We implement our approach in a proof-of-concept web application and more than 34.8 thousands of users visited our website from November 2015 to August 2017. Our evaluation shows that our approach can make accurate recommendation of analogical libraries. We also demonstrate the usefulness of our analogical-library recommendations by using them to answer analogical-library questions in Stack Overflow. Google Analytics of our website traffic and analysis of the visitors’ interaction with website contents provide the insights into the usage patterns and the system design of our web application.",0,1,0,web application+software+software project,
548,548,"Where was this sql query executed a static concept location approach. Concept location in software engineering is the process of identifying where a specific concept is implemented in the source code of a software system. It is a very common task performed by developers during development or maintenance, and many techniques have been studied by researchers to make it more efficient. However, most of the current techniques ignore the role of a database in the architecture of a system, which is also an important source of concepts or dependencies among them. In this paper, we present a concept location technique for data-intensive systems, as systems with at least one database server in their architecture which is intensively used by its clients. Specifically, we present a static technique for identifying the exact source code location from where a given SQL query was sent to the database. We evaluate our technique by collecting and locating SQL queries from testing scenarios of two open source Java systems under active development. With our technique, we are able to successfully identify the source of most of these queries.",0,1,0,software systems+software engineering+software reengineering,
549,549,"Watch out for extrinsic bugs a case study of their impact in just in time bug prediction models on the openstack project. Intrinsic bugs are bugs for which a bug-introducing change can be identified in the version control system of a software. In contrast, extrinsic bugs are caused by external changes to a software, such as errors in external APIs; thereby they do not have an explicit bug-introducing change in the version control system. Although most previous research literature has assumed that all bugs are of intrinsic nature, in a previous study, we show that not all bugs are intrinsic. This paper shows an example of how considering extrinsic bugs can affect software engineering research. Specifically, we study the impact of extrinsic bugs in Just-In-Time bug prediction by partially replicating a recent study by McIntosh and Kamei on JIT models. These models are trained using properties of earlier bug-introducing changes. Since extrinsic bugs do not have bug-introducing changes in the version control system, we manually curate McIntosh and Kamei's dataset to distinguish between intrinsic and extrinsic bugs. Then, we address their original research questions, this time removing extrinsic bugs, to study whether bug-introducing changes are a moving target in Just-In-Time bug prediction. Finally, we study whether characteristics of intrinsic and extrinsic bugs are different. Our results show that intrinsic and extrinsic bugs are of different nature. When removing extrinsic bugs the performance is different up to 16 % Area Under the Curve points. This indicates that our JIT models obtain a more accurate representation of the real world. We conclude that extrinsic bugs negatively impact Just-In-Time models. Furthermore, we offer evidence that extrinsic bugs should be further investigated, as they can significantly impact how software engineers understand bugs.",0,1,0,engineering research+software bug+software defects,
550,550,"A metrics suite for code annotation assessment. Abstract Code annotation is a language feature that enables the introduction of custom metadata on programming elements. In Java, this feature was introduced on version 5, and today it is widely used by main enterprise application frameworks and APIs. Although this language feature potentially simplifies metadata configuration, its abuse and misuse can reduce source code readability and complicate its maintenance. The goal of this paper is to propose software metrics regarding annotations in the source code and analyze their distribution in real-world projects. We have defined a suite of metrics to assess characteristics of the usage of source code annotations in a code base. Our study collected data from 24947 classes extracted from open source projects to analyze the distribution of the proposed metrics. We developed a tool to automatically extract the metrics and provide a full report on annotations usage. Based on the analysis of the distribution, we defined an appropriate approach for the calculation of thresholds to interpret the metric values. The results allow the assessment of annotated code characteristics. Using the thresholds values, we proposed a way to interpret the use of annotations, which can reveal potential problems in the source code.",0,1,0,java+software metrics+open source projects,
551,551,"Deriving detailed design models from an aspect oriented adl using mdd. Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the ''aspects'' identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented UML 2.0 models, closing the gap between ADLs and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to UML 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level.",0,1,0,model driven development+crosscutting concern+aspect-orientation+aspect-oriented software,
552,552,"Synonym suggestion for tags on stack overflow. The amount of diverse tags used to classify posts on Stack Overflow increased in the last years to more than 38,000 tags. Many of these tags have the same or similar meaning. Stack Overflow provides an approach to reduce the amount of tags by allowing privileged users to manually create synonyms. However, currently exist only 2,765 synonym-pairs on Stack Overflow that is quite low compared to the total number of tags.   To comprehend how synonym-pairs are built, we manually analyzed the tags and how the synonyms could be created automatically. Based on our findings, we then present TSST, a tag synonym suggestion tool, that outputs a ranked list of possible synonyms for each input tag.   We first evaluated TSST with the 2,765 approved synonym-pairs of Stack Overflow. For 88.4% of the tags TSST finds the correct synonyms, for 72.2% the correct synonym is within the top 10 suggestions. In addition, we applied TSST to 10 randomly selected Android related tags and evaluated the suggested synonyms with 20 Android app developers in an online survey. Overall, in 80% of their ratings, developers found an adequate synonym suggested by TSST.",0,1,0,android+android platforms+software developer,
553,553,"Proving the fidelity of simulations of event b models. A major hindrance to the use of formal methods is the difficulty to validate the models, particularly at the early stages of the development. We propose to build simulations: programs automatically generated from the specifications but with user-provided implementations of the non-executable traits of the models. We present such a simulation. Of course, the question of the fidelity of the simulation to the model is raised in such a setting. We provide a formal definition of fidelity and the proof obligations that can be attached to each hand-coded element so that fidelity can be proven.",0,1,0,formal specification+formal methods+proof obligations,
554,554,"Soqde a supervised learning based question difficulty estimation model for stack overflow. StackOverflow (SO), the most popular community Q&A site rewards answerers with reputation scores to encourage answers from volunteer participants. However, irrespective of the difficulty of a question, the contributor of an accepted answer is awarded with the same 'reputation' score, which may demotivate an user's additional efforts to answer a difficult question. To facilitate a question difficulty aware rewarding system, this study proposes SOQDE (Stack Overflow Question Difficulty Estimation), a supervised learning based Question difficulty estimation model for the StackOverflow. To design SOQDE, we randomly selected 936 questions from a SO datadump exported during September 2017. Two of the authors independently labeled those questions into three categories (basic, intermediate, or advanced), where conflicting labels were resolved through tie-breaking votes from a third author. We performed an empirical study to determine how the difficulty of a question impacts its outcomes, such as number of votes, resolution time, and number of votes. Our results suggest that the answers of a basic question receive more votes and therefore would generate more reputation points for an answerer. Due to less incentives relative to efforts spent by an answerer, intermediate and advanced questions encounter significantly more delays than the basic questions, which further validates the need of a model like SOQDE. To build our model, we have identified textual and contextual features of a question and divided them into two categories-pre-hoc and post-hoc features. We observed a model based on Random Forest achieving the highest mean accuracy (67.6%), using only answer-independent pre-hoc features. Accommodating answer-dependent post-hoc features, we were able to improve the mean accuracy of our model to 75.2%.",0,1,0,trust evaluation+hierarchical model+random forests,
555,555,"Malware analysis with tree automata inference. The underground malware-based economy is flourishing and it is evident that the classical ad-hoc signature detection methods are becoming insufficient. Malware authors seem to share some source code and malware samples often feature similar behaviors, but such commonalities are difficult to detect with signature-based methods because of an increasing use of numerous freelyavailable randomized obfuscation tools. To address this problem, the security community is actively researching behavioral detection methods that commonly attempt to understand and differentiate how malware behaves, as opposed to just detecting syntactic patterns. We continue that line of research in this paper and explore how formal methods and tools of the verification trade could be used for malware detection and analysis. We propose a new approach to learning and generalizing from observed malware behaviors based on tree automata inference. In particular, we develop an algorithm for inferring k-testable tree automata from system call dataflow dependency graphs and discuss the use of inferred automata in malware recognition and classification.",0,1,0,system calls+source codes+formal methods,
556,556,"Clustering glossary terms extracted from large sized software requirements using fasttext. Specialized terms used in the requirements document should be defined in a glossary. We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents. We use text chunking combined with WordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster. We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements.",0,1,0,software requirements specifications+requirements specifications+requirements document,
557,557,"Mdroid a mutation testing framework for android. Mutation testing has shown great promise in assessing the effectiveness of test suites while exhibiting additional applications to test-case generation, selection, and prioritization. Traditional mutation testing typically utilizes a set of simple language specific source code transformations, called operators, to introduce faults. However, empirical studies have shown that for mutation testing to be most effective, these simple operators must be augmented with operators specific to the domain of the software under test. One challenging software domain for the application of mutation testing is that of mobile apps. While mobile devices and accompanying apps have become a mainstay of modern computing, the frameworks and patterns utilized in their development make testing and verification particularly difficult. As a step toward helping to measure and ensure the effectiveness of mobile testing practices, we introduce MDroid+, an automated framework for mutation testing of Android apps. MDroid+ includes 38 mutation operators from ten empirically derived types of Android faults and has been applied to generate over 8,000 mutants for more than 50 apps. Video URL: https://youtu.be/yzE5_-zN5GA",0,1,0,test case selection+test case generation+regression testing,
558,558,"An approach to translating ocl invariants into owl 2 dl axioms for checking inconsistency. Checking the design specification for contradictions at the early phase of the software development process is crucial to ensure that the design is implementable. However, the high expressivity of OCL makes manual inconsistency checking a difficult task. In addition, the developers cannot detect these problems by OCL itself due to its lack of automated reasoning support. We investigate an approach to translating OCL invariants into OWL 2 DL axioms. We do this where the OCL expression contained in an invariant is converted to the corresponding OWL 2 DL class expression in a compositional way. Our approach covers the OCL expressions including four: PrimaryExp, RelationalExp, LogicalExp and IfExp types. Considering the distinction between the CWA and OWA, we achieve correct translation from RelationalExp using closure axiom. Also, we employ an ontology design pattern to overcome the limitations of OWL 2 DL expressivity when translating IfExp. Then inconsistency checking is done through description logic reasoning by the OWL 2 DL high-performance reasoner. We construct an inductive proof to establish the correctness of our translation approach. Moreover, we evaluate our approach using the implemented TUCO tool prototype.",0,1,0,automated reasoning+design patterns+software development,
559,559,"Model refactoring using interactive genetic algorithm. Refactoring aims at improving the quality of design while preserving its semantic. Providing an automatic support for refactoring is a challenging problem. This problem can be considered as an optimization problem where the goal is to find appropriate refactoring suggestions using a set of refactoring examples. However, some of the refactorings proposed using this approach do not necessarily make sense depending on the context and the semantic of the system under analysis. This paper proposes an approach that tackles this problem by adapting the Interactive Genetic Algorithm IGA which enables to interact with users and integrate their feedbacks into a classic GA. The proposed algorithm uses a fitness function that combines the structural similarity between the analyzed design model and models from a base of examples, and the designers' ratings of the refactorings proposed during execution of the classic GA. Experimentation with the approach yielded interesting and promising results.",0,1,0,fitness functions+semantics+refactorings,
560,560,"Towards safety risk assessment of socio technical systems via failure logic analysis. A thorough understanding of the safety risks of a system requires an understanding of its human and organizational factors, as well as its technical components. Analysis approaches that focus only on the latter without considering, for example, how human decision makers may respond to a technical failure, are not able to adequately capture the wide variety of safety risk scenarios that need to be considered. In this paper, we propose a model-based analysis approach that allows analysts to interpret humans and organizations in terms of components and their behavior in terms of failure logic. Our approach builds on top of CHESS-FLA, which is a tool-supported failure logic analysis technique that supports analysis of component-based system architectures to understand what can go wrong at the system level and to identify the causes (i.e. Faulty components). However, CHESS-FLA currently deals only with hardware and software components and thus it is not adequate to reason about socio-technical systems. We therefore provide an extension based on a pre-existing classification of socio-failures and combine it with the one used in CHESS-FLA for technical failures, thereby giving birth to a novel approach to analysis of socio-technical systems. We demonstrate our approach on an example from the petroleum domain.",0,1,0,software component+software+permanent faults,
561,561,"Task assignment in heterogeneous computing systems using an effective iterated greedy algorithm. A fundamental issue affecting the performance of a parallel application running on a heterogeneous computing system is the assignment of tasks to the processors in the system. The task assignment problem for more than three processors is known to be NP-hard, and therefore satisfactory suboptimal solutions obtainable in an acceptable amount of time are generally sought. This paper proposes a simple and effective iterative greedy algorithm to deal with the problem with goal of minimizing the total sum of execution and communication costs. The main idea in this algorithm is to improve the quality of the assignment in an iterative manner using results from previous iterations. The algorithm first uses a constructive heuristic to find an initial assignment and iteratively improves it in a greedy way. Through simulations over a wide range of parameters, we have demonstrated the effectiveness of our algorithm by comparing it with recent competing task assignment algorithms in the literature.",0,1,0,communication overheads+computing systems+parallel application,
562,562,"Managing requirements change in global software development. Requirements change is an inevitable activity and can occur due to changes in user demands, increased understanding of the stakeholders, customer organization, project vision, requirements specification and availability of technological solutions. Although requirements change management (RCM) is not a straightforward process in collocated software development, the presence of geographical, social, cultural and temporal variations makes the process more difficult for software teams in Global Software Development (GSD). Existing methods do not adequately address many of the GSD issues and challenges. In this paper, we present a graph-based method for RCM.",0,1,0,software teams+global software development+requirements specifications,
563,563,"Migrating the android apo games into an annotation based software product line. Most organizations start to reuse software by cloning complete systems and adapting them to new customer requirements. However, with an increasing number of cloned systems, the problems of this approach become severe, due to synchronization efforts. In such cases, organizations often decide to extract a software product line, which promises to reduce development and maintenance costs. While this scenario is common in practice, the research community is still missing knowledge about best practices and needs datasets to evaluate supportive techniques. In this paper, we report our experiences with extracting a preprocessor-based software product line from five cloned Android games of the Apo-Games challenge. Besides the process we employed, we also discuss lessons learned and contribute corresponding artifacts, namely a feature model and source code. The insights into the processes help researchers and practitioners to improve their understanding of extractive software-product-line adoption. Our artifacts can serve as a valuable dataset for evaluations and can be extended in the future to support researchers as a real-world baseline.",0,1,0,software product line+core asset+software product line engineerings,
564,564,"Healing data loss problems in android apps. Android apps should be designed to cope with stop-start events, which are the events that require stopping and restoring the execution of an app while leaving its state unaltered. These events can be caused by run-time configuration changes, such as a screen rotation, and by context-switches, such as a switch from one app to another. When a stop-start event occurs, Android saves the state of the app, handles the event, and finally restores the saved state. To let Android save and restore the state correctly, apps must provide the appropriate support. Unfortunately, Android developers often implement this support incorrectly, or do not implement it at all. This bad practice makes apps to incorrectly react to stop-start events, thus generating what we defined data loss problems, that is Android apps that lose user data, behave unexpectedly, and crash due to program variables that lost their values. Data loss problems are difficult to detect because they might be observed only when apps are in specific states and with specific inputs. Covering all the possible cases with testing may require a large number of test cases whose execution must be checked manually to discover whether the app under test has been correctly restored after each stop-start event. It is thus important to complement traditional in-house testing activities with mechanisms that can protect apps as soon as a data loss problem occurs in the field. In this paper we present DataLossHealer, a technique for automatically identifying and healing data loss problems in the field as soon as they occur. DataLossHealer is a technique that checks at run-time whether states are recovered correctly, and heals the app when needed. DataLossHealer can learn from experience, incrementally reducing the overhead that is introduced avoiding to monitor interactions that have been managed correctly by the app in the past.",0,1,0,ios+android+android platforms,
565,565,"Program characterization using runtime values and its application to software plagiarism detection. Illegal code reuse has become a serious threat to the software community. Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various  automated   code transformation or obfuscation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (i) most of them cannot handle advanced obfuscation techniques, and (ii) the methods based on source code analysis are not practical since the source code of suspicious programs typically cannot be obtained until strong evidences have been collected. Based on the observation that some critical runtime values of a program are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our value-based plagiarism detection method (VaPD) uses the longest common subsequence based similarity measuring algorithms to check whether two code fragments belong to the same lineage. We evaluate our proposed method through a set of real-world automated obfuscators. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.",0,1,0,code transformation+code fragments+source code analysis,
566,566,"Estimating the return on investment of defect taxonomy supported system testing in industrial projects. Defect taxonomies collect and organize the domain knowledge and project experience of experts and are a valuable instrument of system testing for several reasons. They provide systematic backup for the design of tests, support decisions for the allocation of testing resources and provide a suitable basis for measuring the product and test quality. In this paper, we present a method of system testing based on defect taxonomies and an appropriate estimation procedure for its return on investment depending on several parameters like the average test design time or the number of test cycles and experience values of a test organization. The estimated return on investment provides decision support whether to apply defect taxonomy supported system testing for a specific product or not. We develop the estimation procedure in the context of an industrial project from a public health insurance institution where the return on investment was positive after the first test cycle. From the experience of this project we extract guidelines and heuristics for precise estimation and interpretation of the return on investment of defect taxonomy supported system testing in the context of other projects.",0,1,0,capital investment+investments+system testing,
567,567,"Automatic generation of quality specifications. The logic ${\ensuremath{\rm LTL}} ^{\triangledown } $ extends ${\ensuremath{\rm LTL}} $ by quality operators. The satisfaction value of an ${\ensuremath{\rm LTL}} ^{\triangledown } $ formula in a computation refines the 0/1 value of ${\ensuremath{\rm LTL}} $ formulas to a real value in [0,1]. The higher the value is, the better is the quality of the computation. The quality operator ∇λ, for a quality constant λ∈[0,1], enables the designer to prioritize different satisfaction possibilities. Formally, the satisfaction value of a sub-formula ∇λϕ is λ times the satisfaction value of ϕ. For example, the ${\ensuremath{\rm LTL}} ^{\triangledown } $ formula $G({\it req} \rightarrow (X {\it grant} \vee \triangledown _{\frac{1}{2}} F{\it grant}))$ has value 1 in computations in which every request is immediately followed by a grant, value $\frac{1}{2}$ if grants to some requests involve a delay, and value 0 if some request is not followed by a grant.

The design of an ${\ensuremath{\rm LTL}} ^{\triangledown } $ formula typically starts with an ${\ensuremath{\rm LTL}} $ formula on top of which the designer adds the parameterized ∇ operators. In the Boolean setting, the problem of automatic generation of specifications from binary-tagged computations is of great importance and is a very challenging one. Here we consider the quantitative counterpart: an ${\ensuremath{\rm LTL}} ^{\triangledown } $ query is an ${\ensuremath{\rm LTL}} ^{\triangledown } $ formula in which some of the quality constants are replaced by variables. Given an ${\ensuremath{\rm LTL}} ^{\triangledown } $ query and a set of computations tagged by satisfaction values, the goal is to find an assignment to the variables in the query so that the obtained ${\ensuremath{\rm LTL}} ^{\triangledown } $ formula has the given satisfaction values, or, if this is impossible, best approximates them. The motivation to solving ${\ensuremath{\rm LTL}} ^{\triangledown } $ queries is that in practice it is easier for a designer to provide desired satisfaction values in representative computations than to come up with quality constants that capture his intuition of good and bad quality.

We study the problem of solving ${\ensuremath{\rm LTL}} ^{\triangledown } $ queries and show that while the problem is NP-hard, interesting fragments can be solved in polynomial time. One such fragment is the case of a single tagged computation, which we use for introducing a heuristic for the general case. The polynomial solution is based on an analysis of the search space, showing that reasoning about the infinitely many possible assignments can proceed by reasoning about their partition into finitely many classes. Our experimental results show the effectiveness and favorable outcome of the heuristic.",0,1,0,linear temporal logic+linear time temporal logic+model checking,
568,568,"Retrieving sequence diagrams using genetic algorithm. The benefits of software reuse are multiplied if it is carried out in the early stages of software development. Sequence diagrams are commonly used to model the functionality of software systems at the initial stages (e.g. during requirements analysis) of the software development life cycle. In this work we utilize Genetic Algorithm (GA) for determining the similarity of graphical representations of sequence diagrams, in order to aid the retrieval of similar sequence diagrams from a repository. Experimental results show that the introduction of GA in the graph similarity computation leads to very significant improvements in retrieval quality compared to the existing method which utilizes a deterministic graph similarity algorithm.",0,1,0,software reuse+software systems+software development,
569,569,"A gamified tutorial for learning about security requirements engineering. Thanks to the advent of interactive technologies, education institutions are looking for innovative teaching methods to increase the engagement and reach of students. Besides the uprise of MOOCs, gamification has been shown to produce positive results when it comes to increasing people's engagement and interest in conducting tasks. Unfortunately, the application and benefits of these technologies in teaching requirements engineering remain largely unexplored. In this paper we introduce the STS-Tooltorial, an interactive gamified platform that executes within a security requirements modeling tool and helps learners apprehend the STS-ml language and basic notions about security requirements. We present the design principles of our functional prototype: its educational content and the embedded game elements. Furthermore, we report on an early evaluation with IT professionals and postgraduate information science students focused on the platform's effectiveness and usability.",0,1,0,requirements modeling+requirements engineering+security requirements engineering,
570,570,"Endogenous metamodeling semantics for structural uml 2 concepts. A lot of work has been done in order to put the Unified Modeling Language UML on a formal basis by translating concepts into various formal languages, e.g., set theory or graph transformation. While the abstract UML syntax is defined by using an endogenous approach, i. e., UML describes its abstract syntax using UML, this approach is rarely used for its semantics. This paper shows how to apply an endogenous approach called metamodeling semantics for central parts of the UML standard. To this end, we enrich existing UML language elements with constraints specified in the Object Constraint Language OCL in order to describe a semantic domain model. The UML specification explicitly states that complete runtime semantics is not included in the standard because it would be a major amount of work. However, we believe that certain central concepts, like the ones used in the UML standard and in particular property features as subsets, union and derived, need to be explicitly modeled to enforce a common understanding. Using such an endogenous approach enables the validation and verification of the UML standard by using off-the-shelf UML and OCL tools.",0,1,0,object constraint language+ocl+unified modeling language,
571,571,"Icoq regression proof selection for large scale verification projects. Proof assistants such as Coq are used to construct and check formal proofs in many large-scale verification projects. As proofs grow in number and size, the need for tool support to quickly find failing proofs after revising a project increases. We present a technique for large-scale regression proof selection, suitable for use in continuous integration services, e.g., Travis CI. We instantiate the technique in a tool dubbed iCoq . iCoq tracks fine-grained dependencies between Coq definitions, propositions, and proofs, and only checks those proofs affected by changes between two revisions. iCoq additionally saves time by ignoring changes with no impact on semantics. We applied iCoq to track dependencies across many revisions in several large Coq projects and measured the time savings compared to proof checking from scratch and when using Coq's timestamp-based toolchain for incremental checking. Our results show that proof checking with iCoq is up to 10 times faster than the former and up to 3 times faster than the latter.",0,1,0,verification method+verification+tool support,
572,572,"Parameterized model checking of timed systems with conjunctive guards. In this work we extend the Emerson and Kahlon’s cutoff theorems for process skeletons with conjunctive guards to Parameterized Networks of Timed Automata, i.e. systems obtained by an apriori unknown number of Timed Automata instantiated from a finite set \(U_1, \dots , U_n\) of Timed Automata templates. In this way we aim at giving a tool to universally verify software systems where an unknown number of software components (i.e. processes) interact with continuous time temporal constraints. It is often the case, indeed, that distributed algorithms show an heterogeneous nature, combining dynamic aspects with real-time aspects. In the paper we will also show how to model check a protocol that uses special variables storing identifiers of the participating processes (i.e. PIDs) in Timed Automata with conjunctive guards. This is non-trivial, since solutions to the parameterized verification problem often relies on the processes to be symmetric, i.e. indistinguishable. On the other side, many popular distributed algorithms make use of PIDs and thus cannot directly apply those solutions.",0,1,0,model checking+timed automata+software systems,
573,573,"Sydit creating and applying a program transformation from an example. Bug fixes and feature additions to large code bases often require systematic edits-similar, but not identical, coordinated changes to multiple places. This process is tedious and error-prone. Our prior work introduces a systematic editing approach that creates generalized edit scripts from exemplar edits and applies them to user-selected targets. This paper describes how the Sydit plug-in integrates our technology into the Eclipse integrated development environment. A programmer provides an example edit to Sydit that consists of an old and new version of a changed method. Based on this one example, Sydit generates a context-aware, abstract edit script. To make transformations applicable to similar but not identical methods, Sydit encodes control, data, and containment dependences and abstracts position, type, method, and variable names. Then the programmer selects target methods and Sydit customizes the edit script to each target and displays the results for the programmer to review and approve. Sydit thus automates much of the systematic editing process. To fully automate systematic editing, future tool enhancements should include automated selection of targets and testing of Sydit generated edits.",0,1,0,partial evaluation+scripting languages+software developer,
574,574,"Ai for localizing faults in spreadsheets. Localizing faults in programs is considered a demanding task. A lot of effort is usually spent in finding the root cause of a misbehavior and correcting the program such that it fulfills its intended behavior. The situation is even worse in case of end user programming like spreadsheet development where more or less complex spreadsheets are developed only with little knowledge in programming and also testing. In order to increase quality of spreadsheets and also efficiency of spreadsheet development, tools for testing and debugging support are highly required. In this paper, we focus on the latter and show that approaches originating from Artificial Intelligence can be adapted for (semi-) automated fault localization in spreadsheets in an interactive manner. In particular, we introduce abstract models that can be automatically obtained from spreadsheets enabling the computation of diagnoses within a fraction of a second. Besides the basic foundations, we discuss empirical results using artificial and real-world spreadsheet examples. Furthermore, we show that the abstract models have a similar accuracy to models of spreadsheets capturing their semantics.",0,1,0,programming languages+program debugging+fault localization,
575,575,"Real time flocking of multiple quadrotor system of systems. The subject of this paper is a real-time flocking control of multiple quadrotors in the context of system of systems. We believe that the most challenging aspect in multiple-quadrotor control is the interaction between quadrotors through sensing and preserving safe interdistances. The final objective is a collision-free flock of multiple quadrotors while navigating to a predefined destination. For this purpose, we develop control laws that are based on the consensus theory introduced by Olfati-Saber in [1]. Our control laws are designed in order to be compatible with experimental implementation and nonlinear model of quadrotors. Simulations and experiments using four quadrotors validate the performance of the proposed control laws. The convergence of interdistances between quadrotors to a desired value are maintained while navigating to a destination point.",0,1,0,feedback controller+control systems+nonlinear control laws,
576,576,"A quantitative analysis framework for recurrent neural network. Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework --- DeepStellar --- to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.",0,1,0,software+software systems+test generations,
577,577,"Confiddent a model driven consistent and non redundant layer 3 firewall acl design development and maintenance framework. Design, development, and maintenance of firewall ACLs are very hard and error-prone tasks. Two of the reasons for these difficulties are, on the one hand, the big gap that exists between the access control requirements and the complex and heterogeneous firewall platforms and languages and, on the other hand, the absence of ACL design, development and maintenance environments that integrate inconsistency and redundancy diagnosis. The use of modelling languages surely helps but, although several ones have been proposed, none of them has been widely adopted by industry due to a combination of factors: high complexity, unsupported firewall important features, no integrated model validation stages, etc. In this paper, CONFIDDENT, a model-driven design, development and maintenance framework for layer-3 firewall ACLs is proposed. The framework includes different modelling stages at different abstraction levels. In this way, non-experienced administrators can use more abstract models while experienced ones can refine them to include platform-specific features. CONFIDDENT includes different model diagnosis stages where the administrators can check the inconsistencies and redundancies of their models before the automatic generation of the ACL to one of the many of the market-leader firewall platforms currently supported.",0,1,0,access control+maintenance process+modeling languages,
578,578,"Heuristics for composite web service decentralization. A composite service is usually specified by means of a process model that captures control-flow and data-flow relations between activities that are bound to underlying component services. In mainstream service orchestration platforms, this process model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for executing composite services in a decentralized manner. However, this and similar methods for decentralized composite service execution do not optimize the communication overhead between the services participating in the composition. This paper studies the problem of optimizing the selection of services assigned to activities in a decentralized composite service, both in terms of communication overhead and overall quality of service, and taking into account collocation and separation constraints that may exist between activities in the composite service. This optimization problem is formulated as a quadratic assignment problem. The paper puts forward a greedy algorithm to compute an initial solution as well as a tabu search heuristic to identify improved solutions. An experimental evaluation shows that the tabu search heuristic achieves significant improvements over the initial greedy solution. It is also shown that the greedy algorithm combined with the tabu search heuristic scale up to models of realistic size.",0,1,0,composite services+service execution+service orchestration,
579,579,"Software testing design techniques used in automated vehicle simulations. Research and development in the field of automated vehicles has increased along with related works about its software. Software testing in automated vehicles is key to launching safe and reliable vehicles. Several issues in the software testing of automated vehicles have been raised including extremely large space of test input, the high cost of test executions in a physical environment, test oracles not being simple Boolean properties, and so on. Automated vehicle simulations are a solution of cost reduction in test execution. However, space of test input is extremely large. Extremely large space of test input comes from sensing data. In this paper, we discuss the software architecture of automated vehicle simulation as the target software. We raise issues on software testing in these simulations on the basis of the related works. We then discuss test design techniques used in automated vehicle simulations.",0,1,0,boolean functions+software+software architecture,
580,580,"Automatic exploit generation for buffer overflow vulnerabilities. Buffer overflow vulnerabilities are widely found in software. Finding these vulnerabilities and identifying whether these vulnerabilities can be exploit is very important. However, it is not easy to find all of the buffer overflow vulnerabilities in software programs, and it is more difficult to find and exploit these vulnerabilities in binary programs. This paper proposes a method and a corresponding tool that automatically finds buffer overflow vulnerabilities in binary programs, and then automatically generate exploit for the vulnerability. The tool uses symbolic execution to search the target software and find potential buffer overflow vulnerabilities, then try to bypass system protection by choosing different exploiting method according to the different level of protections. Finally, the exploit of software vulnerability is generated using constraint solver. The method and tool can automatically find vulnerabilities and generate exploits for three kinds of protection: without system protection, with address space layout randomization protection, and with stack non-executable protection.",0,1,0,software vulnerabilities+buffer overflows+symbolic execution,
581,581,"On the use of grey literature a survey with the brazilian software engineering research community. Background: The use of Grey Literature (GL) has been investigated in diverse research areas. In Software Engineering (SE), this topic has an increasing interest over the last years. Problem: Even with the increase of GL published in diverse sources, the understanding of their use on the SE research community is still controversial. Objective: To understand how Brazilian SE researchers use GL, we aimed to become aware of the criteria to assess the credibility of their use, as well as the benefits and challenges. Method: We surveyed 76 active SE researchers participants of a flagship SE conference in Brazil, using a questionnaire with 11 questions to share their views on the use of GL in the context of SE research. We followed a qualitative approach to analyze open questions. Results: We found that most surveyed researchers use GL mainly to understand new topics. Our work identified new findings, including: 1) GL sources used by SE researchers (e.g., blogs, community website); 2) motivations to use (e.g., to understand problems and to complement research findings) or reasons to avoid GL (e.g., lack of reliability, lack of scientific value); 3) the benefit that is easy to access and read GL and the challenge of GL to have its scientific value recognized; and 4) criteria to assess GL credibility, showing the importance of the content owner to be renowned (e.g., renowned author and institutions). Conclusions: Our findings contribute to form a body of knowledge on the use of GL by SE researchers, by discussing novel (some contradictory) results and providing a set of lessons learned to both SE researchers and practitioners.",0,1,0,software engineering+software reengineering+engineering research,
582,582,"Toward learning teams. Today's software development challenges require learning teams that can continuously apply new engineering and management practices, new and complex technical skills, cross-functional skills, and experiential lessons learned. The pressure of delivering working software often forces software teams to sacrifice learning-focused practices. Effective learning under pressure involves conscious efforts to implement original agile practices such as retrospectives and adapted strategies such as learning spikes. Teams, their management, and customers must all recognize the importance of creating learning teams as the key to braving the erratic climates and uncharted territories of future software development.",0,1,0,software development+software teams+agile practices,
583,583,"Quality metrics and oracles for autonomous vehicles testing. The race for deploying AI-enabled autonomous vehicles (AVs) on public roads is based on the promise that such self-driving cars will be as safe as or safer than human drivers. Numerous techniques have been proposed to test AVs, which however lack oracle definitions that account for the quality of driving, due to the lack of a commonly used set of metrics.Towards filling this gap, we first performed a systematic analysis of the literature concerning the assessment of the quality of driving of human drivers and extracted 126 metrics. Then, we measured the correlation between such metrics and the human perception of driving quality when AVs are driving. Lastly, we performed a study based on mutation analysis to assess whether the 26 metrics that best capture the quality of AV driving according to the human study can be used as functional oracles. Our results, targeting the Udacity platform, indicate that our automated oracles can kill a high proportion of mutants at a zero or very low false alarm rate, and therefore can be used as effective functional oracles for the quality of driving of AVs.",0,1,0,asian continental ancestry group+mutation operators+complexity metrics,
584,584,"A preliminary study of knowledge sharing related to covid 19 pandemic in stack overflow. The Covid-19 outbreak has changed to an unprecedented extent almost every aspect of human activity. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open ‘question & answer’ communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. Through the study of 464 Stack Overflow questions posted in February and March 2020 and leveraging the power of text mining, we attempt to shed light into the interest of developers in Covid-19 related topics and the most popular problems for which the users seek information. The findings reveal that indeed this global crisis sparked off an intense activity in Stack Overflow with most post topics reflecting a strong interest on the analysis of Covid- 19 data, primarily using Python technologies.",0,1,0,python+software+software developer,
585,585,"Application of model oriented security requirements engineering framework for secure e voting. The election system is in need of a secure electronic web application that voters can rely and have trust in. E-Voting is the most security sensitive processes handled electronically. The highest achievable security is never too much for an E-Voting application. So when the web application is being built, tasks such as Security Requirements elicitation, specification and validation are essential to assure the security of the resulting E-Voting web application. By considering the Security requirements as functional requirements in the Requirement phase, the complete specification of Security Requirements for E-Voting application can be developed and flaws can be reduced. In this paper we propose to use Model Oriented Security Requirements Engineering (MOSRE) Framework in the early phases of E-Voting application development so as to identify assets, threats and vulnerabilities. This helps the developers to analyze and elicit the Security Requirements in the early stage of secure E-Voting application development.",0,1,0,requirements specifications+requirements engineering process+security requirements engineering,
586,586,"Invited paper what is ai software testing and why. Abstract— With the fast advance of artificial intelligence technology and data-driven machine learning techniques, building high-quality AI-based software in different application domains is becoming a very hot research topic in both academic and industry communities. Today, many machine learning models and artificial technologies have been developed to build smart application systems based on multimedia inputs to achieve intelligent functional features, such as recommendation, object detection, classification, and prediction, natural language processing and translation, and so on. This brings strong demand in quality validation and assurance for AI software systems. Current research work seldom discusses AI software testing questions, challenges, and validation approaches with clear quality requirements and criteria. This paper focuses on AI software quality validation, including validation focuses, features, and process, and potential testing approaches. Moreover, it presents a test process and a classification-based test modeling for AI classification function testing. Finally, it discusses the challenges, issues, and needs in AI software testing.",0,1,0,quality requirements+software systems+software quality,
587,587,"State of the art big data security taxonomies. Today's businesses accumulate an astonishing amount of digital data, which can be leveraged to unlock new sources of economic value and provide fresh insights into business trends. The real challenge in this process is the design of computing, storage infrastructure and algorithms needed to handle this ""Big Data"". Hence, organizations are looking at different ways in which they can make use of Big Data in their business. There's no doubt that the creation of a Hadoop-powered Data Lake can provide a robust foundation for a new generation of analytics and intuitive results. At the same time, it is also very necessary to consider security before launching or expanding a Hadoop initiative. As we move towards a stage where Hadoop is considered for real-time production scenarios rather than just experimentation levels, a major chunk of production data is normally sensitive, or subject to many industry regulations and governance controls. This paper analyzes the current security challenges in big data implementations based on state-of-the-art big data security taxonomies.",0,1,0,security challenges+hadoop+data security,
588,588,"Clami defect prediction on unlabeled datasets t. Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort.   In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.",0,1,0,software engineering+software reengineering+open source projects,
589,589,"Automated product line test case selection industrial case study and controlled experiment. Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM_T) to capture commonalities and variabilities of a product line and a component family model for testing (CFM_T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort significantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efficiency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efficiency is not affected by the increased complexity of products.",0,1,0,feature models+test case selection+product lines,
590,590,"175 modeling for product line evolution of domain artifacts. Software evolution is an inevitable process in the development of long-living software systems as, e.g., changes of requirements demand corresponding adaptations. For software product lines, the incorporation of evolution in the development process gets even more complex due to the vast number of potential variants and the set of reusable domain artifacts and their interrelations. To allow for the application of existing analyses also for combined dimensions of variants and versions, recent evolution-aware variability modeling techniques are insufficient for capturing both version and variant information by the same means. In this paper, we propose an extension of annotative variability modeling, also known as 150% modeling, to tackle evolution and variability by the same means. The so called 175% modeling formalism allows for the development and documentation of evolving product lines. A 175% model combines all variant-specific models of all versions of a product line, where elements are mapped to features and versions to specify which version of a variant contains the element. We discuss potential application scenarios for 175% modeling. Furthermore, we propose a bidirectional transformation between 175% and higher-order delta models to exploit the benefits of both modeling formalisms, when solely one type is available.",0,1,0,product lines+software product line+software evolution,
591,591,"A heterogeneous image fusion algorithm based on llc coding. Most image fusion algorithms are not good at batch processing. To address this, we propose a LLC coding based image fusion algorithm, by which multiple infrared and visible light images can be fused and identified. The images were encoded and several image features were extracted by those codes. It was judged whether the images could be merged by the coincidence of the non-zero coding counterpart obtained from comparing the LLC coding of two heterogeneous images. The max-pooling criterion was employed to fuse the features extracted from images by maximizing the complementary information and minimizing the redundant information. Consequently the SVM classifier was used to classify and identify the target. The simulated results show the accuracy of our proposed method.",0,1,0,image fusion techniques+support vector machine+adaptive algorithms,
592,592,"A review of meta ethnographies in software engineering. Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.",0,1,0,software+software engineering+software reengineering,
593,593,"Lattice based semantics for combinatorial model evolution. Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates.",0,1,0,software defects+bug reports+bug-fixing,
594,594,"Booster an acceleration based verification framework for array programs. We present Booster, a new framework developed for verifiying programs handling arrays. Booster integrates new acceleration features with standard verification techniques, like Lazy Abstraction with Interpolants (extended to arrays). The new acceleration features are the key for scaling-up in the verification of programs with arrays, allowing Booster to efficiently generate required quantified safe inductive invariants attesting the safety of the input code.",0,1,0,verification+formal verifications+verification techniques,
595,595,"A systematic mapping study of value based software engineering. Integrating value-oriented perspectives into the principles and practices of software engineering is critical to ensure that software development and management activities address all key stakeholders' views and also balance short-and-long-term goals. This is put forward in the discipline of Value-Based Software Engineering (VBSE). In this paper, a mapping study of VBSE is detailed. We classify evidence on VBSE principles and practices, research methods, and the research types. This mapping study includes 134 studies located from online searches, and backward snowballing of references. Our results show that VB Requirements Engineering (22%) and VB Planning and Control (19%) were the two principles and practices mostly investigated in the VBSE literature, whereas VB Risk Management, VB People Management and Value Creation (3% respectively) were the three less researched. In terms of the research method, the most commonly employed method is case-study research. In terms of research types, most of the studies (28%) proposed solution technique(s) without empirical validation.",0,1,0,software engineering+software reengineering+requirements engineering,
596,596,Predicting faculty performance using regression model in data mining. This paper investigates the different attributes used in evaluating faculty performance to come up with a regression model that predicts faculty performance. The main objective of this paper is to develop a model for predicting faculty performance and design a framework of data mining implementing ETL. The outcome of this research could be used as basis in improving the instruction in an academic institution.,0,1,0,faculty+association rules+etl,
597,597,"Gossiping components for cyber physical systems. Developing software for dynamic cyber-physical systems (CPS) is a complex task. One has to deal with the dynamicity and unreliability of the physical environment where the software resides in, while, at the same time, provide sufficient levels of dependability and scalability. Although emerging software engineering abstractions, such as dynamic ad-hoc component ensembles, provide a convenient way to structure software for dynamic CPS, they need to be mapped to robust decentralized execution schemes in real-life settings. A particular challenge in this context is the robust distributed data dissemination in dynamic networks. Gossip-based communication stands as a promising solution to this challenge. We argue, that exploitation of application-specific information, software architecture in particular, has a large potential for improving the robustness and performance of gossip-based communication. This paper proposes a synergy between high-level architectural models and low-level communication models to effectively enable application-specific gossiping in component-based systems. The synergy is exemplified on the DEECo component model which is tailored to the needs and specifics of CPS, and evaluated on an emergency coordination case study with realistic network configurations.",0,1,0,software architecture+software engineering+software reengineering,
598,598,"Motivating by examples an empirical study of teaching an introductory software engineering course in brazil. Educators in emerging and developing countries face the challenge of conveying international software engineering concepts within their local contexts, cultures and environments. In Brazil, where the local software industry is domestically focused, this becomes a factor in the development and expansion of the local software industries. This paper investigates the issue of localization of software engineering education in the context of using localized examples in the teaching of an introductory software engineering course in Brazil. Results from student questionnaires illustrate that the use of examples familiar to the students' environment increased student motivation, participation and general positive perception and attitudes towards the teaching material and the learning experience. This paper contributes empirical evidence to justify further research into contextualization and localization of software engineering education.",0,1,0,software engineering+software reengineering+software engineering education,
599,599,"Automatic test case generation and optimization based on mutation testing. Based on defect implantation mutation testing technique not only serves as a standard for evaluating test cases but also guides how to generate high-quality test case sets. In order to reduce the number of mutants, we propose a mutation operator selection strategy according to Selective Mutation. From 19 mutation operators of Mujava we select 5 mutation operators to obtain a subset. Test cases using this subset are able to achieve an average variation score of more than 95% on the variants of the complete set. Then we propose a test case generation method combining mutation testing with a genetic algorithm. The crossover, insertion, change, and deletion operators of the test case set are redefined, and the test cases are optimized. Finally compared with some algorithms and tools we obtain a set of test cases with higher coverage and higher mutation score.",0,1,0,genetic selection+mutation operators+test case generation,
600,600,"Incentivizing the dissemination of truth versus fake news in social networks. The concept of “truth,” as a public good is the production of a collective understanding, which emerges from a complex network of social interactions. The recent impact of social networks on shaping the perception of truth in political arena shows how such perception is corroborated and established by the online users, collectively. However, investigative journalism for discovering truth is a costly option, given the vast spectrum of online information. In some cases, both journalist and online users choose not to investigate the authenticity of the news they receive, because they assume other actors of the network had carried the cost of validation. Therefore, the new phenomenon of “fake news” has emerged within the context of social networks. The online social networks, similarly to System of Systems, cause emergent properties, which makes authentication processes difficult, given availability of multiple sources. In this study, we show how this conflict can be modeled as a volunteer's dilemma. We also show how the public contribution through news subscription (shared rewards) can impact the dominance of truth over fake news in the network.",0,1,0,complex networks+authentication+validation,
601,601,"Social and technical evolution of software ecosystems a case study of rails. Software ecosystems evolve through an active community of developers who contribute to projects within the ecosystem. However, development teams change over time, suggesting a potential impact on the evolution of the technical parts of the ecosystem. The impact of such modifications has been studied by previous works, but only temporary changes have been investigated, while the long-term effect of permanent changes has yet to be explored. In this paper, we investigate the evolution of the ecosystem of Ruby on Rails in GitHub in terms of such temporary and permanent changes of the development team. We use three viewpoints of the Rails ecosystem evolution to discuss our preliminary findings: (1) the base project; (2) the forks; and (3) the entire ecosystem containing both base project and forks.",0,1,0,information systems+software+ruby,
602,602,"An aspect oriented reference architecture for software engineering environments. Abstract: Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs.",0,1,0,software systems+software engineering+software reengineering,
603,603,"Real time recognition of human faces. A simple yet efficient face detection and recognition system is proposed in this paper which has the capability to recognize human faces in single as well as multiple face images in a database in real time. Preprocessing of the proposed frame work includes noise removal and hole filling in color images. After preprocessing, face detection is performed by using viola jones algorithm. Detected faces are cropped out of the input image to make computation fast. SURF features are extracted from the cropped image. For face matching, putative feature matching is carried out and outliers are removed using M-estimator Sample Consensus (MSAC) algorithm. Single as well as multiple person color images from class persons of Graz 01 dataset are used to evaluate the system.",0,1,0,matching algorithm+database systems+face detection algorithm,
604,604,"Persistence of relevance a missing issue in imperfect coverage models. In traditional imperfect fault coverage models, simply coverage models (CMs), the coverage (including identification and isolation) is typically limited to the faulty components regardless of their relevance. The relevance is typically defined in the context of perfect fault coverage, and a component is called irrelevant if its (covered) failure does not affect the system state, otherwise it is relevant. Although it is generally assumed that all components are initially relevant in these models, such an assumption does not consider the fact that an initially relevant component could become irrelevant afterwards due to the failures of other components, and we call it a non-persistent component. A system with only persistent components is called persistent, otherwise it is called non-persistent. For a non-persistent system, it is important to cover (identify and isolate) the non-persistent components in time whenever they become irrelevant, such that their future uncovered failures will not affect the system anymore. This paper formalizes the concept of persistence and analyzes its impact on reliability of the systems subjected to imperfect fault coverage. It is demonstrated that with the coverage of irrelevant components (in addition to the faulty components), the reliability of a non-persistent system can be (significantly) improved without increasing redundancy.",0,1,0,software component+permanent faults+fault coverages,
605,605,"Different classifiers find different defects although with different level of consistency. BACKGROUND -- During the last 10 years hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall.   OBJECTIVE -- We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers.   METHOD -- We perform a sensitivity analysis to compare the performance of Random Forest, Naive Bayes, RPart and SVM classifiers when predicting defects in 12 NASA data sets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty is compared against different classifiers.   RESULTS -- Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others.   CONCLUSIONS -- Our results confirm that a unique sub-set of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Classifier ensembles with decision making strategies not based on majority voting are likely to perform best.",0,1,0,random forests+naive bayes+prediction modes,
606,606,"Complete and interpretable conformance checking of business processes. This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical evaluation shows that the proposed method can handle real datasets and produces more concise and higher-level difference descriptions than state-of-the-art conformance checking methods. In a survey designed according to the technology acceptance model, practitioners showed a preference towards the proposed method with respect to a state-of-the-art baseline.",0,1,0,business process+process models+conformance checking,
607,607,"Simultaneous support vector selection and parameter optimization using support vector machines for sentiment classification. Sentiment classification is widely used in some areas, such as product reviews, movie reviews, and micro-blogging reviews. Sentiment classification method is mainly bag of words model, Naive Bayes and Support Vector Machine. In recent years, the machine learning method represented by support vector machine (SVM) is widely used in the field of sentiment classification. There are more and more experiments show that support vector machine (SVM) performs better than the traditional bag of words model in the field of sentiment classification. However, more researches mainly focus on semantic analysis and feature extraction on sentiment, but also did not consider the case of sample imbalance. The purpose of this study was to test the feasibility of sentiment classification based on the genetic algorithm to optimize SVM model. Genetic algorithm is an optimization algorithm, which often used for selecting the feature subset and the optimization of the SVM parameters. This paper presents a novel optimization method, which select the optimal support vector subset by genetic algorithm and optimize SVM parameters. We construct the experiment show that the proposed method has improved significantly on sentiment classification than the traditional SVM modeling capabilities.",0,1,0,genetic selection+semantics+naive bayes,
608,608,"Towards an evidence based theoretical framework on factors influencing the software development productivity. Productivity refers to the rate at which a company produces goods, and its observation takes into account the number of people and the amount of other necessary resources to deliver such goods. However, it is not clear how to observe productivity and what influences it when the product is software since most effort spent in software development is creative and human-dependent. Besides, the outputs vary from each instance of software solutions throughout the software development process. To characterize software development productivity and investigate evidence-based factors aiming at understanding their influence on software development productivity. To evolve and replicate a systematic literature review (SLR) on software development productivity measurement and prediction methods. Next, to use the Structured Synthesis Method to aggregate and describe the relationships among software productivity and correspondingly influence factors according to the results of primary studies selected by SLR protocol. The study allowed organizing a body of knowledge through a model obtained from empirical evidence comprising 25 factors and 33 relationships regarding software development productivity based on the technical literature over the last 30 years. It uses a taxonomy for describing observations and for supporting the reasoning of uncertainty on the evidence regarding software development productivity in Software Engineering. The acquired knowledge may represent a first try towards a well-grounded theoretical framework regarding software development productivity. Based on a methodically selected set of evidence, the proposed framework intends to support practitioners and researchers on observing, deciding, and controlling software development productivity in software projects. Additionally, it can encourage researchers to identify which phenomena deserve better understanding and explanation through further empirical studies.",0,1,0,software project+software engineering+software reengineering,
609,609,"Runtime monitoring of behavioral properties in dynamically adaptive systems. A Dynamically adaptive System (DAS) enable adaptations at runtime based on context information. DAS can be developed following the same approach used in Dynamic Software Product Lines (DSPL). Then, software engineers design the behavioral adaptations of DAS modeling context-aware features, which can be activated/deactivated at runtime. In our previous work, we proposed a model checking technique to verify behavioral properties in the specification of a DAS adaptation at design time. However, once this kind of system deals with reconfiguration at runtime, the inherent dynamism of context information and defects in the adaptation mechanism may cause unexpected behaviors, such as incorrect activation of adaptation rules. So, runtime monitoring activities are necessary to ensure the satisfaction of properties during the system execution. In this paper, we address this issue by proposing an approach to verify behavioral properties with a monitor framework during the system operation. To evaluate the approach, we perform a proof of concept with two mobile DAS by previously injecting faults in the source code. The injected faults were successfully detected when they turn into failures at system runtime.",0,1,0,self-adaptive system+software product line engineerings+runtime monitoring,
610,610,"Carial cost aware software reliability improvement with active learning. In the context of field testing (operational testing) of software, we address the problem of balancing the potential reduction in failure risk that developers may achieve by reviewing captured test executions to identify failures (and by successfully debugging their causes) against the cost of reviewing the tests. To achieve a desirable balance, we propose a cost-sensitive active learning strategy. Our approach guides developers in selecting a sample of test executions to review and label, and it calls for them to profile execution dynamics and characterize the symptoms and relative severity levels of failures. Profiles, labels, failure symptoms, and severity levels are used by the active learner to construct and refine a mapping between examined and unexamined tests, on one hand, and possible defects, on the other hand. This mapping is used together with estimates of test review costs to guide the selection of additional tests. We evaluate our approach on three subject programs and show that it (1) produces reasonable predictions of risk reduction and (2) significantly improves severity-weighted reliability for each subject program, with relatively low developer effort.",0,1,0,program debugging+software developer+software reliability,
611,611,"Quantifying conformance using the skorokhod metric. The conformance testing problem for dynamical systems asks, given two dynamical models (e.g., as Simulink diagrams), whether their behaviors are “close” to each other. In the semi-formal approach to conformance testing, the two systems are simulated on a large set of tests, and a metric, defined on pairs of real-valued, real-timed trajectories, is used to determine a lower bound on the distance. We show how the Skorokhod metric on continuous dynamical systems can be used as the foundation for conformance testing of complex dynamical models. The Skorokhod metric allows for both state value mismatches and timing distortions, and is thus well suited for checking conformance between idealized models of dynamical systems and their implementations. We demonstrate the robustness of the metric by proving a transference theorem: trajectories close under the Skorokhod metric satisfy “close” logical properties in the timed linear time logic TLTL augmented with a rich class of temporal and spatial constraint predicates. We provide an efficient window-based streaming algorithm to compute the Skorokhod metric, and use it as a basis for a conformance testing tool for Simulink. We experimentally demonstrate the effectiveness of our tool in finding discrepant behaviors on a set of control system benchmarks, including an industrial challenge problem. 
Open image in new window",0,1,0,formal approach+conformance testing+model based testing,
612,612,"From proprietary to open source growing an open source ecosystem. In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem.",0,1,0,open source software+open source system+software project,
613,613,"Acco a novel approach to measure cohesion using hierarchical slicing of java programs. Maintainability of program parts refers to the ease with which these parts can be modified. Many existing metrics of internal quality attributes such as cohesion, coupling, etc. have been used in this regard. Cohesiveness among program parts is considered as a strong measure for the maintainability of object-oriented components and to predict the probability of being erroneous. Our objective is to propose a novel graph-based cohesion metric to measure the maintainability of different program parts in an object-oriented program and predict their fault proneness. We compute the cohesion of the sliced component as a measure to predict its correctness and preciseness. In addition, we wish to theoretically validate the proposed technique against the existing guidelines of cohesion measurement and compare it with some existing techniques. We propose a new cohesion metric named affected component cohesion (ACCo) to measure the maintainability of different program parts and predict their fault proneness. This metric is based on the hierarchical decomposition slice of an object-oriented program that comprises all the affected program parts. The slices are obtained with respect to some modification made to the program under consideration. It is essential to consider all possible dependence relationships along with the control and data dependences that exist between different program parts of an object-oriented program for a better program comprehension. To represent these dependences, we construct a suitable intermediate graph for an object-oriented program. Then, we compute the slice of the object-oriented program using the intermediate graph to extract the affected program parts. These extracted affected program parts are represented as nodes in the proposed affected slice graph. The critical and sub-critical nodes that require thorough testing are determined by estimating their cohesion measure. The theoretical validation signifies that ACCo satisfies all the existing properties for cohesion measurement. The results obtained are more precise and comparable with other existing approaches. ACCo is a more precise and practical technique to measure the inter-relatedness of affected program parts in an object-oriented program. The discussion on possible threats to its validity demonstrates the scope for improvement of this approach.",0,1,0,quality attributes+java program+program comprehension,
614,614,"The making of cloud applications an empirical study on software development for the cloud. Cloud computing is gaining more and more traction as a deployment and provisioning model for software. While a large body of research already covers how to optimally operate a cloud system, we still lack insights into how professional software engineers actually use clouds, and how the cloud impacts development practices. This paper reports on the first systematic study on how software developers build applications for the cloud. We conducted a mixed-method study, consisting of qualitative interviews of 25 professional developers and a quantitative survey with 294 responses. Our results show that adopting the cloud has a profound impact throughout the software development process, as well as on how developers utilize tools and data in their daily work. Among other things, we found that (1) developers need better means to anticipate runtime problems and rigorously define metrics for improved fault localization and (2) the cloud offers an abundance of operational data, however, developers still often rely on their experience and intuition rather than utilizing metrics. From our findings, we extracted a set of guidelines for cloud development and identified challenges for researchers and tool vendors.",0,1,0,software developer+software development+fault localization,
615,615,"A systematic review of model driven security. To face continuously growing security threats and requirements, sound methodologies for constructing secure systems are required. In this context, Model-Driven Security (MDS) has emerged since more than a decade ago as a specialized Model-Driven Engineering approach for supporting the development of secure systems. MDS aims at improving the productivity of the development process and quality of the resulting secure systems, with models as the main artifact. This paper presents how we systematically examined existing published work in MDS and its results. The systematic review process, which is based on a formally designed review protocol, allowed us to identify, classify, and evaluate different MDS approaches. To be more specific, from thousands of relevant papers found, a final set of the most relevant MDS publications has been identified, strictly selected, and reviewed. We present a taxonomy for MDS, which is used to synthesize data in order to classify and evaluate the selected MDS approaches. The results draw a wide picture of existing MDS research showing the current status of the key aspects in MDS as well as the identified most relevant MDS approaches. We discuss the main limitations of the existing MDS approaches and suggest some potential research directions based on these insights.",0,1,0,engineering+development processes+model-driven engineering,
616,616,"Scalable software merging studies with merganser. Software merging researchers constantly need empirical data of real-world merge scenarios to analyze. Such data is currently extracted through individual and isolated efforts, often with non-systematically designed scripts that may not easily scale to large studies. This hinders replication and proper comparison of results. In this paper, we introduce MERGANSER, a scalable and easy-to-use tool for extracting and analyzing merge scenarios in Git repositories. In addition to extracting basic information about merge scenarios from Git history, our tool also replays each merge to detect conflicts and stores the corresponding information of conflicting files and regions. We design a normalized and extensible SQL data schema to store the information of the analyzed repositories, merge scenarios and involved commits, and merge replays and conflicts. By running only one command, our proposed tool clones the target repositories, detects their merge scenarios, and stores their information in a SQL database. MergAnser is written in Python and released under the MIT license. In this tool paper, we describe MergAnser's architecture and provide guidance for its usage in practice.",0,1,0,python+sql+software,
617,617,"Experience report verifying data interaction coverage to improve testing of data intensive systems the norwegian customs and excise case study. Testing data-intensive systems is paramount to increase our reliance on information processed in e-governance, scientific/ medical research, and social networks. A common practice in the industrial testing process is to use test databases copied from live production streams to test functionality of complex database applications that manage well-formedness of data and its adherence to business rules in these systems. This practice is often based on the assumption that the test database adequately covers realistic scenarios to test, hopefully, all functionality in these applications. There is a need to systematically evaluate this assumption. We present a tool-supported method to model realistic scenarios and verify whether copied test databases actually cover them and consequently facilitate adequate testing. We conceptualize realistic scenarios as data interactions between fields cross-cutting a complex database schema and model them as test cases in a classification tree model. We present a human-in the-loop tool, DEPICT, that uses the classification tree model as input to (a) facilitate interactive selection of a connected sub graph from often many possible paths of interactions between tables specified in the model (b) automatically generate SQL queries to create an inner join between tables in the connected sub graph (c) extract records from the join and generate a visual report of satisfied and unsatisfied interactions hence quantifying test adequacy of the test database. We report our experience as a qualitative evaluation of approach and with a large industrial database from the Norwegian Customs and Excise information system TVINN featuring large and complex databases with millions of records.",0,1,0,sql+business rules+cross-cutting,
618,618,"Securing a connected mobile system for healthcare. Connected mobile systems are useful to provide remote and continuous monitoring, which has been increasingly applied in healthcare. In order to protect patients' privacy based on HIPPA regulation, the security of connected mobile systems decides if users can trust and adopt the system. This paper proposes a novel approach to protect a connected mobile system in a mobile cloud environment. The novel approach is able to provide connected mobile systems with authentication, encryption, and usable security that counter attacks such as sniffing, replay, and man-in-the-middle attacks. The performance including encryption, decryption, and key generation time has been evaluated based on a real-world mobile cloud environment. Security robustness has been analyzed based on several threat models as well.",0,1,0,cloud environments+cloud services+security and privacy,
619,619,"Fuzzy control based software self adaptation a case study in mission critical systems. Self-adaptation ability is particularly desirable for mission critical software (MCS). This paper proposes a fuzzy control-based approach to provide a systematic, engineering, and intuitive way for programmers to achieve software self adaptation. This approach uses fuzzy logic to handle uncertainty in software, uses natural-language style to describe self-adaptation logic, and makes control visible in software. These greatly shorten the knowledge and semantics gap between control engineering and software engineering, and reduce the realization difficulty of software self-adaptation. We illustrate our approach through the development of an adaptive MCS application in process control systems.",0,1,0,software+software engineering+software reengineering,
620,620,"Sustainability guidelines for long living software systems. Economically sustainable software systems must be able to cost-effectively evolve in response to changes in their environment, their usage profile, and business demands. However, in many software development projects, sustainability is treated as an afterthought, as developers are driven by time-to-market pressure and are often not educated to apply sustainability-improving techniques. While software engineering research and practice has suggested a large amount of such techniques, a holistic overview is missing and the effectiveness of individual techniques is often not sufficiently validated. On this behalf we created a catalog of “software sustainability guidelines” to support project managers, software architects, and developers during system design, development, operation, and maintenance. This paper describes how we derived these guidelines and how we applied selected techniques from them in two industrial case studies. We report several lessons learned about sustainable software development.",0,1,0,software engineering+software reengineering+engineering research,
621,621,"Bug prediction metrics based decision support for preventive software maintenance. There exist a number of large legacy systems that still undergo continuous maintenance and enhancement. Due to the sheer size and complexity of the software systems and limited resources, managers are confronted with crucial decisions regarding allocation and training of new engineers, intelligent allocation of testing personnel, assessment of release readiness of the software and so on. While the area of bug prediction by mining software repositories holds promise, and is a worthwhile endeavor, the current state of the art techniques are not accurate enough in predicting bugs and hence are of limited usefulness to managers. So instead of predicting files as buggy or not we take a different viewpoint and focus on providing decision support for managers. In this paper we present a set of metrics to guide the managers in taking these decisions. These metrics are evaluated using 4 open source systems and 2 proprietary systems.",0,1,0,software systems+software maintenance+software repositories,
622,622,"A situation centric approach to identifying new user intentions using the mtl method. Human factors have been increasingly recognized as one of the major driving forces of requirement changes. We believe that the requirements elicitation (RE) process should largely embrace human-centered perspectives, and this paper focuses on changing human intentions and desires over time. To support software evolution due to requirement changes, Situ framework has been proposed to model and detect human intentions by inferring their desires through monitoring environmental and human behavioral contexts prior to or after system deployment. Researchers have reported that Situ is able to infer users' desires with high accuracy using the Conditional Random Fields method. However, manual analysis is still needed for new intention identification and new requirements elicitation. This work attempts to find a computable way to identify users' new intentions with minimal help from human oracle. We discuss the feasibility of implementing the concept of DIKW (Data, Information, Knowledge, Wisdom) to bridge the gap between user behavioral & contextual data and requirements, and propose a situation-centric approach using the Multi-strategy, Task-adaptive Learning (MTL) method. A case study shows that the proposed approach is able to identify users' new intentions, and is especially effective to capture alternatives of low-level task.",0,1,0,requirements analysis+software+software evolution,
623,623,"Software bug localization with markov logic. Software bug localization is the problem of determining buggy statements in a software system. It is a crucial and expensive step in the software debugging process. Interest in it has grown rapidly in recent years, and many approaches have been proposed. However, existing approaches tend to use isolated information to address the problem, and are often ad hoc. In particular, most existing approaches predict the likelihood of a statement being buggy sequentially and separately.     This paper proposes a well-founded, integrated solution to the software bug localization problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and views them as templates for features of Markov networks. We show how a number of salient program features can be seamlessly combined in Markov logic, and how the resulting joint inference can be solved.     We implemented our approach in a debugging system, called MLNDebugger, and evaluated it on 4 small programs. Our initial results demonstrated that our approach achieved higher accuracy than a previous approach.",0,1,0,debugging process+software systems+software bug,
624,624,"A new model based framework for testing security of iot systems in smart cities using attack trees and price timed automata. In this paper we propose a new model-based framework for testing security properties of Internet of Things in Smart Cities. In general a model-based approach consists in extracting test cases from a formal specification either of the system under test or the environment of the considered system in an automatic fashion. Our framework is mainly built on the use of two formalisms namely Attack Trees and Price Timed Automata. An attack tree allows to describe the strategy adopted by the malicious party which intends to violate the security of the considered IOT system. An attack tree is translated into a network of price timed automata. The product of the constructed price timed automata is then computed using the well known UPPAALL platform. The obtained timed automata product serves as input for the adopted test generation algorithm. Moreover our framework takes advantage of the use of the standardized specification and execution testing language TTCN-3. With this respect, the obtained abstract tests are translated into the TTCN-3 format. Finally we propose a cloud-oriented architecture in order to ensure test execution and to collect the generated verdicts.",0,1,0,security protocols+timed automata+test generations,
625,625,"A heuristic based approach to refactor crosscutting behaviors in uml state machines. UML state machines are commonly used to model the state-based behavior of communication and control systems to support various activities such as test cases and code generation. Standard UML state machines are well suited to model functional behavior, however extra-functional behavior such as robustness and security can also be directly modeled on them, but this often results in cluttered models since extra-functional behaviors are often crosscutting. Such modeling crosscutting behavior directly on UML state machines is a common practice. Aspect-Oriented Modeling (AOM) allows systematically modeling of crosscutting behavior and has shown to provide a scalable solution in the recent years. However, due to lack of familiarity of AOM in both academic and industry, extra-functional behavior is often modeled directly on UML state machines and as a result those UML state machines are difficult to read and maintain. To improve the readability of already developed UML state machines and ease maintenance, we propose a set of heuristics, derived from two industrial cases studies, implemented in a tool to automatically identify commonly observed crosscutting behaviors in UML state machines and refactor them as Aspect State Machines. Such refactoring makes the state machines easier to maintain and comprehend. We present the results of applying our proposed heuristics to the existing UML state machines of two industrial case studies developed for model-based testing.",0,1,0,model based testing+uml 2.0+uml profiles,
626,626,"Early software defect prediction a systematic map and review. Abstract Context Software defect prediction is a trending research topic, and a wide variety of the published papers focus on coding phase or after. A limited number of papers, however, includes the prior (early) phases of the software development lifecycle (SDLC). Objective The goal of this study is to obtain a general view of the characteristics and usefulness of Early Software Defect Prediction (ESDP) models reported in scientific literature. Method A systematic mapping and systematic literature review study has been conducted. We searched for the studies reported between 2000 and 2016. We reviewed 52 studies and analyzed the trend and demographics, maturity of state-of-research, in-depth characteristics, success and benefits of ESDP models. Results We found that categorical models that rely on requirement and design phase metrics, and few continuous models including metrics from requirements phase are very successful. We also found that most studies reported qualitative benefits of using ESDP models. Conclusion We have highlighted the most preferred prediction methods, metrics, datasets and performance evaluation methods, as well as the addressed SDLC phases. We expect the results will be useful for software teams by guiding them to use early predictors effectively in practice, and for researchers in directing their future efforts.",0,1,0,software defect prediction+software development+software teams,
627,627,"Improving cybersecurity hygiene through jit patching. Vulnerability patch management remains one of the most complex issues facing modern enterprises; companies struggle to test and deploy new patches across their networks, often leaving myriad attack vectors vulnerable to exploits. This problem is exacerbated by enterprise server applications, which expose tremendous amounts of information about their security postures, greatly expediting attackers' reconnaissance incursions (e.g., knowledge gathering attacks). Unfortunately, current patching processes offer no insights into attacker activities, and prompt attack remediation is hindered by patch compatibility considerations and deployment cycles. To reverse this asymmetry, a patch management model is proposed to facilitate the rapid injection of software patches into live, commodity applications without disruption of production workflows, and the transparent sandboxing of suspicious processes for counterreconnaissance and threat information gathering. Our techniques improve workload visibility and vulnerability management, and overcome perennial shortcomings of traditional patching methodologies, such as proneness to attacker fingerprinting, and the high cost of deployment. The approach enables a large variety of novel defense scenarios, including rapid security patch testing with prompt recovery from defective patches and the placement of exploit sensors inlined into production workloads. An implementation for six enterprise-grade server programs demonstrates that our approach is practical and incurs minimal runtime overheads. Moreover, four use cases are discussed, including a practical deployment on two public cloud environments.",0,1,0,cloud environments+enterprise information system+software,
628,628,"Toward the characterization of software testing practices in south america looking at brazil and uruguay. Software testing is an important activity in the software development life cycle. Several previous studies reported the results of surveys on software testing practices among practitioners from different countries. In this paper, we analyze these surveys aiming to get their main questions, and replicate a survey with practitioners from Brazil and Uruguay, two emerging South American software development scenarios. This survey was previously conducted in Manaus/Brazil in 2006 and Buenos Aires/Argentina in 2013. The replication’s scope includes three regions (Northern Brazil, Southern Brazil, and Uruguay). A total of 150 software testing practitioners responded to the survey. Its results are compared with the previous executions and other software testing surveys identified in the technical literature, strengthening previous findings. The Brazilian and Uruguayan participants indicate that: (1) documentation of test artifacts (plan, cases, procedures, results) are useful and important for software testing practitioners; (2) system and regression testing are the two test types deemed most useful and important; (3) tools for monitoring and management of test process tasks and bug reports are considered useful and important; (4) it is usual for software companies to have a definition of a testing process and to have a dedicated testing team; (5) there is a lack of measurement of test tasks and coverage in the industry; and (6) tools to support automation of test case generation and execution or code coverage are still poorly used in their organizations.",0,1,0,test case generation+regression testing+bug reports,
629,629,"Identifying strategies on god class detection in two controlled experiments. Context: “Code smell” is commonly presented as indicative of problems in design of object-oriented systems. However, some empirical studies have presented findings refuting this idea. One of the reasons of the misunderstanding is the low number of studies focused on the role of human on code smell detection. Objective: Our aim is to build empirical support to exploration of the human role on code smell detection. Specifically, we investigated strategies adopted by developers on god class detection. God class is one of the most known code smell. Method: We performed a controlled experiment and replicated it. We explored the strategies from the participant’s actions logged during the detection of god classes. Result: One of our findings was that the observation of coupling is more relevant than the observation of attributes like LOC or complexity and the hierarchical relation among these. We also noted that reading source code is important, even with visual resources enhancing the general comprehension of the software. Conclusion: This study contributes to expand the comprehension of the human role on code smell detection through the use of automatic logging. We suggest that this approach brings a complementary perspective of analysis in discussions about the topic.",0,1,0,software+object-oriented system+code smell,
630,630,"Poster unit tests and component tests do make a difference on fault localisation effectiveness. Agile testers distinguish between unit tests and component tests as a way to automate the bulk of the developer tests. Research on fault localisation largely ignores this distinction, evaluating the effectiveness of these techniques irrespective of whether the fault is exposed by unit tests---where the search space to locate the fault is constrained to the unit under test---or by component tests---where the search space expands to all objects involved in the test. Based on a comparison of sixteen spectrum based fault localisation techniques, we show that there is indeed a big difference in performance when facing unit tests and component tests. Consequently, researchers should distinguish between easy and difficult to locate faults when evaluating new fault localisation techniques.",0,1,0,agile methods+software developer+fault localization,
631,631,"Runtime monitoring of software energy hotspots. GreenIT has emerged as a discipline concerned with the optimization of software solutions with regards to their energy consumption. In this domain, most of the state-of-the-art solutions concentrate on coarse-grained approaches to monitor the energy consumption of a device or a process. However, none of the existing solutions addresses in-process energy monitoring to provide in-depth analysis of a process energy consumption. In this paper, we therefore report on a fine-grained runtime energy monitoring framework we developed to help developers to diagnose energy hotspots with a better accuracy than the state-of-the-art.     Concretely, our approach adopts a 2-layer architecture including OS-level and process-level energy monitoring. OS-level energy monitoring estimates the energy consumption of processes according to different hardware devices (CPU, network card). Process-level energy monitoring focuses on Java-based applications and builds on OS-level energy monitoring to provide an estimation of energy consumption at the granularity of classes and methods. We argue that this per-method analysis of energy consumption provides better insights to the application in order to identify potential energy hotspots. In particular, our preliminary validation demonstrates that we can monitor energy hotspots of Jetty web servers and monitor their variations under stress scenarios.",0,1,0,in-process+java+runtime monitoring,
632,632,"Hybrid regression test selection. Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.",0,1,0,regression testing+test case prioritization+regression test selection,
633,633,"Collaborative development of safety critical automotive systems exchange views and metrics. Automotive system development involves a large set of organizations and disciplines. In particular, vehicle manufacturers rely on a large set of suppliers to provide components and systems. To successfully develop and integrate these components, stakeholders exchange requirement specifications that define in detail the component properties. Because of the complexity of a typical automotive system, requirement specifications are error prone and time consuming to negotiate with a correct result. In addition, most systems have safety implications and require rigorous means to achieve and argue safety. Recent autonomous and semi-autonomous systems are particularly complex and critical.",0,1,0,system requirements+software requirements specifications+requirements specifications,
634,634,"Automated analysis of css rules to support style maintenance. CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called Cilla, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.",0,1,0,css+web application+web developers,
635,635,"Managing performance vs accuracy trade offs with loop perforation. Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are designed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. Loop perforation provides a general technique to trade accuracy for performance by transforming loops to execute a subset of their iterations. A criticality testing phase filters out critical loops (whose perforation produces unacceptable behavior) to identify tunable loops (whose perforation produces more efficient and still acceptably accurate computations). A perforation space exploration algorithm perforates combinations of tunable loops to find Pareto-optimal perforation policies. Our results indicate that, for a range of applications, this approach typically delivers performance increases of over a factor of two (and up to a factor of seven) while changing the result that the application produces by less than 10%.",0,1,0,machine learning+trade+parallel computation,
636,636,"Object naming service supporting heterogeneous object code identification for iot system. The Electronic Product Code (EPC) network is a collection of industrial standards designed to build an Internet of physical objects. The Object Naming Service (ONS), a directory based on the Domain Name System (DNS) is one of the important components of the EPC network. ONS provides a connection between the product code and Information Services (IS) in IoT systems. However, object identification standards used by these IoT information systems are usually customized and inhomogeneous. Therefore it is difficult to integrate these systems for real-time data sharing. To solve this problem, in this paper, we propose an extension of ONS architecture to dynamically support heterogeneous object code identification. A syntax called Formal Decoding Rule (FDR) is designed to define the rules of parsing object code, so that decoding rules can be decoupled from ONS hardcode program. Object Identifier (OID) is imported to manage heterogeneous code rules, which offers open registration of new identification standards by IoT subscribers and allows real-time updates in the ONS network. And there is a one-to-one correspondence between FDR and OID. In a typical query, ONS first detects the OID of an object, finds its corresponding FDR and then resolves the object code to corresponding resource address according to this FDR. By this mechanism, IoT applications with different object code identification standards is possible to be integrated into a larger network to implement real-time data sharing. In addition, this extended ONS architecture can dynamically support heterogeneous web service interface resolving. This paper presents experimental results to illustrate a better performance of ONS using FDR than usual ONS.",0,1,0,binary codes+source codes+code generation,
637,637,"Defining architectural viewpoints for quality concerns. A common practice in software architecture design is to apply architectural views to model the design decisions for the various stakeholder concerns. When dealing with quality concerns, however, it is more difficult to address these explicitly in the architectural views. This is because quality concerns do not easily match the architectural elements that seem to be primarily functional in nature. As a result, the communication and analysis of these quality concerns becomes more problematic in practice. We introduce a general and practical approach for supporting architects to model quality concerns by extending the architectural viewpoints of the so-called V&B approach. We illustrate the approach for defining recoverability and adaptability viewpoints for an open source software architecture.",0,1,0,open source software+software architecture+quality concerns,
638,638,"Satisfiability checking theory and applications. Satisfiability checking aims to develop algorithms and tools for checking the satisfiability of existentially quantified logical formulas. Besides powerful SAT solvers for solving propositional logic formulas, sophisticated SAT-modulo-theories (SMT) solvers are available for a wide range of theories, and are applied as black-box engines for many techniques in different areas. In this paper we give a short introduction to the theoretical foundations of satisfiability checking, mention some of the most popular tools, and discuss the successful embedding of SMT solvers in different technologies.",0,1,0,propositional formulas+sat+satisfiability,
639,639,"Grey box concolic testing on binary code. We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools.",0,1,0,execution paths+test case generation+symbolic execution,
640,640,"A novel multi dimensional encryption technique to secure the grayscale images and color images in public cloud storage. Image encryption is one of the techniques which is used to maintain the image confidentiality. Trust is needed to be created and retained in the cloud between the service provider and the end user. The existing image encryption methods are using a map structure, Rubik cube method, DCT-based approach and s-box designs. The existing algorithms are mostly designed with the concept of offline image encryption. While coming to the online encryption, the image encryption algorithms needed to be redefined with a lightweight approach with an improved security level or same security level of existing algorithms. These things are taken into consideration in the proposed multi-dimensional encryption technique design. The proposed technique is using two phases to perform the encryption. The proposed technique’s two phases are image pixel shuffling phase and image pixel rearrange phase. The proposed technique is already tested successfully on the standard grayscale images, and the results were obtained and it satisfied the objectives. In this paper, the proposed technique was tested on the standard color images and the gained results. Those results have been used to analyze the performance and efficiency of proposed technique with the existing technique, by using different parameters which include PSNR, MSE, information entropy, coefficient correlation, NPCR and UPCI.",0,1,0,image encryptions+cloud storage+security analysis,
641,641,"Peer to peer load testing. Nowadays the large-scale systems are common-place in any kind of applications. The popularity of the web created a new environment in which the applications need to be highly scalable due to the data tsunami generated by a huge load of requests (i.e., connections and business operations). In this context, the main question is to validate how far the web applications can deal with the load generated by the clients. Load testing is a technique to analyze the behavior of the system under test upon normal and heavy load conditions. In this work we present a peer-to-peer load testing approach to isolate bottleneck problems related to centralized testing drivers and to scale up the load. Our approach was tested in a DBMS as study case and presents satisfactory results.",0,1,0,ajax+web application+system testing,
642,642,"Bounded verification with on the fly discrepancy computation. Simulation-based verification algorithms can provide formal safety guarantees for nonlinear and hybrid systems. The previous algorithms rely on user provided model annotations called discrepancy function, which are crucial for computing reachtubes from simulations. In this paper, we eliminate this requirement by presenting an algorithm for computing piecewise exponential discrepancy functions. The algorithm relies on computing local convergence or divergence rates of trajectories along a simulation using a coarse over-approximation of the reach set and bounding the maximal eigenvalue of the Jacobian over this over-approximation. The resulting discrepancy function preserves the soundness and the relative completeness of the verification algorithm. We also provide a coordinate transformation method to improve the local estimates for the convergence or divergence rates in practical examples. We extend the method to get the input-to-state discrepancy of nonlinear dynamical systems which can be used for compositional analysis. Our experiments show that the approach is effective in terms of running time for several benchmark problems, scales reasonably to larger dimensional systems, and compares favorably with respect to available tools for nonlinear models.",0,1,0,local convergence+verification method+verification,
643,643,"Unleashing the potentials of immersive augmented reality for software engineering. In immersive augmented reality (IAR), users can wear a head-mounted display to see computer-generated images superimposed to their view of the world. IAR was shown to be beneficial across several domains, e.g., automotive, medicine, gaming and engineering, with positive impacts on, e.g., collaboration and communication. We think that IAR bears a great potential for software engineering but, as of yet, this research area has been neglected. In this vision paper, we elicit potentials and obstacles for the use of IAR in software engineering. We identify possible areas that can be supported with IAR technology by relating commonly discussed IAR improvements to typical software engineering tasks. We further demonstrate how innovative use of IAR technology may fundamentally improve typical activities of a software engineer through a comprehensive series of usage scenarios outlining practical application. Finally, we reflect on current limitations of IAR technology based on our scenarios and sketch research activities necessary to make our vision a reality. We consider this paper to be relevant to academia and industry alike in guiding the steps to innovative research and applications for IAR in software engineering.",0,1,0,software+software engineering+software reengineering,
644,644,"Capturing cost avoidance through reuse systematic literature review and industrial evaluation. Background: Cost avoidance through reuse shows the benefits gained by the software organisations when reusing an artefact. Cost avoidance captures benefits that are not captured by cost savings e.g. spending that would have increased in the absence of the cost avoidance activity. This type of benefit can be combined with quality aspects of the product e.g. costs avoided because of defect prevention. Cost avoidance is a key driver for software reuse. Objectives: The main objectives of this study are: (1) To assess the status of capturing cost avoidance through reuse in the academia; (2) Based on the first objective, propose improvements in capturing of reuse cost avoidance, integrate these into an instrument, and evaluate the instrument in the software industry. Method: The study starts with a systematic literature review (SLR) on capturing of cost avoidance through reuse. Later, a solution is proposed and evaluated in the industry to address the shortcomings identified during the systematic literature review. Results: The results of a systematic literature review describe three previous studies on reuse cost avoidance and show that no solution, to capture reuse cost avoidance, was validated in industry. Afterwards, an instrument and a data collection form are proposed that can be used to capture the cost avoided by reusing any type of reuse artefact. The instrument and data collection form (describing guidelines) were demonstrated to a focus group, as part of static evaluation. Based on the feedback, the instrument was updated and evaluated in industry at 6 development sites, in 3 different countries, covering 24 projects in total. Conclusion: The proposed solution performed well in industrial evaluation. With this solution, practitioners were able to do calculations for reuse costs avoidance and use the results as decision support for identifying potential artefacts to reuse.",0,1,0,software reuse+software industry+software project,
645,645,"Predictors of well being and productivity among software professionals during the covid 19 pandemic a longitudinal study. The COVID-19 pandemic has forced governments worldwide to impose movement restrictions on their citizens. Although critical to reducing the virus’ reproduction rate, these restrictions come with far-reaching social and economic consequences. In this paper, we investigate the impact of these restrictions on an individual level among software engineers who were working from home. Although software professionals are accustomed to working with digital tools, but not all of them remotely, in their day-to-day work, the abrupt and enforced work-from-home context has resulted in an unprecedented scenario for the software engineering community. In a two-wave longitudinal study (N = 192), we covered over 50 psychological, social, situational, and physiological factors that have previously been associated with well-being or productivity. Examples include anxiety, distractions, coping strategies, psychological and physical needs, office set-up, stress, and work motivation. This design allowed us to identify the variables that explained unique variance in well-being and productivity. Results include (1) the quality of social contacts predicted positively, and stress predicted an individual’s well-being negatively when controlling for other variables consistently across both waves; (2) boredom and distractions predicted productivity negatively; (3) productivity was less strongly associated with all predictor variables at time two compared to time one, suggesting that software engineers adapted to the lockdown situation over time; and (4) longitudinal analyses did not provide evidence that any predictor variable causal explained variance in well-being and productivity. Overall, we conclude that working from home was per se not a significant challenge for software engineers. Finally, our study can assess the effectiveness of current work-from-home and general well-being and productivity support guidelines and provides tailored insights for software professionals.",0,1,0,software project+software engineering+software reengineering,
646,646,"Automating unit and integration testing with partial oracles. The oracle problem is an essential part in current research on automating software tests. Partial oracles seem to be a viable solution, but their suitability for different testing steps and general applicability for various systems remains still to be shown. This paper presents a study in which partial oracles are applied in order to automatically test a jpeg2000 encoder as an example for a modular software system with several integrated units and components. The effectiveness of the partial oracles is measured by means of mutation analysis to determine their adequacy for both unit and integration testing. Additionally, the paper presents possibilities of improving the effectiveness as well as the efficiency of the employed partial oracles. It shows how the knowledge of certain characteristics of the system to be tested, such as linearity or time-invariance, may lead to a better choice of partial oracles and thus to an improved effectiveness and efficiency.",0,1,0,software component+software+software systems,
647,647,"Reduce before you localize delta debugging and spectrum based fault localization. Spectrum-based fault localization (SBFL) is one of the most popular and studied methods for automated debugging. Many formulas have been proposed to improve the accuracy of SBFL scores. Many of these improvements are either marginal or context-dependent. This paper proposes that, independent of the scoring method used, the effectiveness of spectrum-based localization can usually be dramatically improved by, when possible, delta-debugging failing test cases and basing localization only on the reduced test cases. We show that for programs and faults taken from the standard localization literature, a large case study of Mozilla's JavaScript engine using 10 real faults, and mutants of various open-source projects, localizing only after reduction often produces much better rankings for faults than localization without reduction, independent of the localization formula used, and the improvement is often even greater than that provided by changing from the worst to the best localization formula for a subject.",0,1,0,fault localization+open source projects+automated debugging,
648,648,"Soft skills in software engineering a study of its demand by software companies in uruguay. Software development requires professionals with knowledge and experience on many different methodologies, tools, and techniques. However, the so-called soft skills, such as interpersonal skills, teamwork, problem solving and customer orientation to name just a few, are as important as, or even more important, than traditional qualifications and technical skills. In this paper we review a set of jobs advertisements offering job positions related to software engineering in order to identify what soft skills are most in demand by software companies in Uruguay. We also compare our findings with the ones reported in other recent studies carried out with data from other countries. This comparison shows that evidence exists about a common set of basic soft skills software companies demand when looking for new staff for software engineering activities.",0,1,0,software development+software engineering+software reengineering,
649,649,"The drupal framework a case study to evaluate variability testing techniques. Variability testing techniques search for effective but manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in real settings is a must but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this paper, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability as a feature model. Then, we report on extensive data extracted from the Drupal git repository and the Drupal issue tracking system. Among other results, we identified 378 faults in single features and 11 faults triggered by the interaction between two of the features of Drupal v7.23, reported during a one-year period. These data may give a new insight into the distribution of faults in variability-intensive systems and the fault propensity of features. To show the feasibility of our work, we used the case study to evaluate the effectiveness of a history-based test case prioritization criterion. Results suggest that this technique could contribute to accelerate the detection of faults of test suites based on combinatorial testing.",0,1,0,feature models+test case generation+test case prioritization,
650,650,"Automotive software. This theme issue addresses automotive IT and software development. What technologies and principles deliver value, and how can you introduce them at a fast pace?",0,1,0,software+software development+software project,
651,651,"Cktail model learning of communicating systems. Event logs are helpful to figure out what is happening in a system or to diagnose the causes that led to an unexpected crash or security issue. Unfortunately, their growing sizes and lacks of abstraction make them difficult to interpret, especially when a system integrates several communicating components. This paper proposes to learn models of communicating systems, e.g., Web service compositions, distributed applications, or IoT systems, from their event logs in order to help engineers understand how they are functioning and diagnose them. Our approach, called CkTail, generates one Input Output Labelled Transition System (IOLTS) for every component participating in the communications and dependency graphs illustrating another viewpoint of the system architecture. Compared to other model learning approaches, CkTail improves the precision of the generated models by better recognising sessions in event logs. Experimental results obtained from 9 case studies show the effectiveness of CkTail to recover accurate and general models along with component dependency graphs.",0,1,0,transition system+distributed applications+engineers,
652,652,"Cloud robotics architecture trends and challenges. Cloud robotics is a field of robotics that attempts to use Cloud technologies for robotics. The use of the Cloud for robotics and automation brings some potential benefits largely ameliorating the performance of robotic systems. However, there are also some challenges. First of all, from the viewpoint of architecture, how to model and describe the architectures of Cloud robotic systems? How to deploy the architecture in Cloud? Our proposed approach in this paper is based on the principles of software architectures. In particular, we leverage our lightweight architectural solution to solve the architecture problem of Cloud robotics and discuss different solutions to deploy Cloud robotics system in Cloud.",0,1,0,automation+software+software architecture,
653,653,"It is almost all about human safety a novel paradigm for robot design control and planning. In this paper we review our work on safe control, acting, and planning in human environments. In order for a robot to be able to safely interact with its environment it is necessary to be able to react to unforeseen events in real-time on basically all levels of abstraction. Having this goal in mind, our contributions reach from fundamental understanding of human injury due to robot-human collisions as the underlying metric for ""safe"" behavior, various interaction control schemes that ground on the basic components impedance control and collision behavior, to safe real-time motion planning and behavior based control as an interface level for task planning. Based on this foundation, we also developed joint interaction planners for role allocation in human-robot collaborative assembly, as well as reactive safety oriented replanning algorithms. A very recent step was the development of novel programming paradigms that act as a simple yet powerful interface between programmer, automatic planning, and the robot. A significant amount of our work on robot safety and control has found found its way into international standardization committees, products, and was applied in numerous real-world applications.",0,1,0,robots+robot designs+task planning,
654,654,"Funfrog bounded model checking with interpolation based function summarization. This paper presents FunFrog, a tool that implements a function summarization approach for software bounded model checking. It uses interpolation-based function summaries as over-approximation of function calls. In every successful verification run, FunFrog generates function summaries of the analyzed program functions and reuses them to reduce the complexity of the successive verification. To prevent reporting spurious errors , the tool incorporates a counter-example-guided refinement loop. Experimental evaluation demonstrates competitiveness of FunFrog with respect to state-of-the-art software model checkers.",0,1,0,model checking+bounded model checking+model checker,
655,655,"A data replication strategy with tenant performance and provider economic profit guarantees in cloud data centers. Abstract Meeting tenant performance requirements through data replication while ensuring an economic profit is very challenging for cloud providers. For this purpose, we propose a data Replication Strategy that satisfies Performance tenant objective and provider profit in Cloud data centers (RSPC). Before the execution of each tenant query Q, data replication is considered only if: (i) the estimated Response Time of Q (RTQ) exceeds a critical RT threshold (per-query replication), or (ii) more often, if RTQ exceeds another (lower) RT threshold for a given number of times (replication per set of queries). Then, a new replica is really created only if a suitable replica placement is heuristically found so that the RT requirement is satisfied again while ensuring an economic profit for the provider. Both the provider's revenues and expenditures are also estimated while penalties and replication costs are taken into account. Furthermore, the replica factor is dynamically adjusted in order to reduce the resource consumption. Compared to four other strategies, RSPC best satisfies the RT requirement under high loads, complex queries and strict RT thresholds. Moreover, penalty and data transfer costs are significantly reduced, which impacts the provider profit.",0,1,0,complex queries+cloud providers+replica placement,
656,656,"Poster guiding developers to make informative commenting decisions in source code. Code commenting is a common programming practice of practical importance to help developers review and comprehend source code. However, there is a lack of thorough specifications to help developers make their commenting decisions in current practice. To reduce the effort of making commenting decisions, we propose a novel method, CommentSuggester, to guide developers regarding appropriate commenting locations in the source code. We extract context information of source code and employ machine learning techniques to identify possible commenting locations in the source code. The encouraging experimental results demonstrated the feasibility and effectiveness of our commenting suggestion method.",0,1,0,binary codes+source codes+software developer,
657,657,"On the need of understanding the failures of smart contracts. When the execution of smart contracts fails, the transaction will not be recorded to provide hints for analysts to improve their automated analyzers. To mitigate this, we present ExecuWatch to watch the execution of smart contracts and report the execution details.",0,1,0,contracts+service contract+smart contracts,
658,658,"Using smt for solving fragments of parameterised boolean equation systems. Fixpoint logics such as parameterised Boolean equation systems (PBESs) provide a unifying framework in which a number of practical decision problems can be encoded. Efficient evaluation methods (solving methods in the terminology of PBESs) are needed to solve the encoded decision problems. We present a sound pseudo-decision procedure that uses SMT solvers for solving conjunctive and disjunctive PBESs. These are important fragments, allowing to encode typical verification problems and planning problems. Our experiments, conducted with a prototype implementation, show that the new solving procedure is complementary to existing techniques for solving PBESs.",0,1,0,verification problems+verification method+verification,
659,659,"Using bayesian network to estimate the value of decisions within the context of value based software engineering. The software industry's current decision-making relating to product/project management and development is largely done in a value neutral setting, in which cost is the primary driver for every decision taken. However, numerous studies have shown that the primary critical success factor that differentiates successful products/projects from failed ones lie in the value domain. Therefore, to remain competitive, innovative and to grow, companies must change from cost-based to value-based decisionmaking where the decisions taken are the best for that company's overall value creation. This paper details a case study where value-based decisions made by key stakeholders to select features for the next sprint of an Internet of Things (IoT) project, stored in a decisions database, were used to build and validate a value estimation model. This model's goal was to estimate the overall value contribution that each feature being discussed during a decision-making meeting would bring to the company, if selected for implementation. The estimation technique employed was Bayesian Network, and validation results were quite positive.",0,1,0,software industry+software engineering+software reengineering,
660,660,"Earmo an energy aware refactoring approach for mobile apps. The energy consumption of mobile apps is a trending topic and researchers are actively investigating the role of coding practices on energy consumption. Recent studies suggest that design choices can conflict with energy consumption. Therefore, it is important to take into account energy consumption when evolving the design of a mobile app. In this paper, we analyze the impact of eight type of anti-patterns on a testbed of 20 android apps extracted from F-Droid. We propose EARMO, a novel anti-pattern correction approach that accounts for energy consumption when refactoring mobile anti-patterns. We evaluate EARMO using three multiobjective search-based algorithms. The obtained results show that EARMO can generate refactoring recommendations in less than a minute, and remove a median of 84 percent of anti-patterns. Moreover, EARMO extended the battery life of a mobile phone by up to 29 minutes when running in isolation a refactored multimedia app with default settings (no Wi-Fi, no location services, and minimum screen brightness). Finally, we conducted a qualitative study with developers of our studied apps, to assess the refactoring recommendations made by EARMO. Developers found 68 percent of refactorings suggested by EARMO to be very relevant.",0,1,0,iphone+energy aware+android,
661,661,"Dads dynamic slicing continuously running distributed programs with budget constraints. We present Dads, the first distributed, online, scalable, and cost-effective dynamic slicer for continuously-running distributed programs with respect to user-specified budget constraints. Dads is distributed by design to exploit distributed and parallel computing resources. With an online analysis, it avoids tracing hence the associated time and space costs. Most importantly, Dads achieves and maintains practical scalability and cost-effectiveness tradeoffs according to a given budget on analysis time by continually and automatically adjusting the configuration of its analysis algorithm on the fly via reinforcement learning. Against eight real-world Java distributed systems, we empirically demonstrated the scalability and cost-effectiveness merits of Dads. The open-source tool package of Dads with a demo video is publicly available.",0,1,0,distributed program+java+dynamic slicing,
662,662,"Evaluating representation learning of code changes for predicting patch correctness in program repair. A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.",0,1,0,logistic regression+reasoning+formal proofs,
663,663,"Exploring the relationship between software modularity and technical debt. Modularity is one of the key principles of software design. In order for a software system to be modular, it should be organized into modules that are highly coherent internally, whereas at the same time as independent from other modules as possible. In this paper we explore coupling and cohesion metrics at the software package level—i.e., one of most basic levels of software functional decomposition in object-oriented (OO) systems, with the aim of investigating their relation to the technical debt of each package. Current state-of-the-art tools in TD measurement are working on the source code level, and the extent to which they can unveil limitations at the architecture level (e.g., violations of the modularity principle), has not been explored so far. To achieve this goal, we conducted a case study on 1,200 packages retrieved from 20 well-known open source software projects. The results of the study suggested that current measures of technical debt are able to identify / predict modules that lack modularity, and therefore suffer from Architectural Technical Debt (ATD). The results of the study are discussed both from the practitioners' and re-searchers' point of view.",0,1,0,software project+architectural technical debt+technical debt,
664,664,"Advancement of decision making in agile projects by applying logistic regression on estimates. With the advent of iterative software development methodologies such as Agile the emphasis is on light weight software development methodologies. Emphasis is on accommodating frequent changes and also relies on individuals and interaction over processes and tools. Estimation methods used rely on expert-judgment and methods such as planning-poker. During project execution, when changes arise due to dynamic nature of the project, individuals' interaction, mode of communication and expert judgment decide the actions and decision-making in distributed teams. In this paper logistic regression equation method is suggested to capture the schedule changes that are possible due to the dynamic changes to the project. Logistic regression provides the change in probability of completing the feature for an expected change in any of the explanatory variables. Due to the quantified output from logistic regression it augments shared decision-making in distributed environment resulting in better actions.",0,1,0,agile methods+software development+software development methodologies,
665,665,"Runtime prevention of concurrency related type state violations in multithreaded applications. We propose a new method for runtime prevention of type state violations in multithreaded applications due to erroneous thread interleavings. The new method employs a combination of static and dynamic program analysis techniques to control the execution order of the method calls to suppress illegal call sequences. The legal behavior of a shared object is specified by a type-state automaton, which serves as the guidance for our method to delay certain method calls at run time. Our main contribution is a new theoretical framework for ensuring that the runtime prevention strategy is always safe, i.e., they do not introduce new erroneous interleavings. Furthermore, whenever the static program analysis is precise enough, our method guarantees to steer the program to a failurefree interleaving as long as such interleaving exists. We have implemented the new method in a tool based on the LLVM compiler framework. Our experiments on a set of multithreaded C/C++ applications show that the method is both efficient and effective in suppressing concurrency related type-state violations.",0,1,0,static analysis+static program analysis+multi-threaded application+dynamic program analysis,
666,666,"A diagnosis framework for critical systems verification short paper. For critical systems design, the verification tasks play a crucial role. If abnormalities are detected, a diagnostic process must be started to find and understand the root causes before corrective actions are applied. Detection and diagnosis are notions that overlap in common speech. Detection basically means to identify something as unusual, diagnosis means to investigate its root cause. The meaning of diagnosis is also fuzzy, because diagnosis is either an activity - an investigation - or an output result - the nature or the type of a problem. This paper proposes an organizational framework for structuring diagnoses around three principles: that propositional data (including detection) are the inputs of the diagnostic system; that activities are made of methods and techniques; and that associations specialize that relationships between the two preceding categories.",0,1,0,systems design+verification method+verification,
667,667,"Boat an experimental platform for researchers to comparatively and reproducibly evaluate bug localization techniques. Bug localization refers to the process of identifying source code files that contain defects from descriptions of these defects which are typically contained in bug reports. There have been many bug localization techniques proposed in the literature. However, often it is hard to compare these techniques since different evaluation datasets are used. At times the datasets are not made publicly available and thus it is difficult to reproduce reported results. Furthermore, some techniques are only evaluated on small datasets and thus it is not clear whether the results are generalizable. Thus, there is a need for a platform that allows various techniques to be compared with one another on a common pool containing a large number of bug reports with known defective source code files. In this paper, we address this need by proposing our Bug lOcalization experimental plATform (BOAT). BOAT is an extensible web application that contains thousands of bug reports with known defective source code files. Researchers can create accounts in BOAT, upload executables of their bug localization techniques, and see how these techniques perform in comparison with techniques uploaded by other researchers, with respect to some standard evaluation measures. BOAT is already preloaded with several bug localization techniques and thus researchers can directly compare their newly proposed techniques against these existing techniques. BOAT has been made available online since October 2013, and researchers could access the platform at: http://www.vlis.zju.edu.cn/blp.",0,1,0,web application+source codes+bug reports,
668,668,"Search based software testing past present and future. Search-Based Software Testing is the use of a meta-heuristic optimizing search technique, such as a Genetic Algorithm, to automate or partially automate a testing task, for example the automatic generation of test data. Key to the optimization process is a problem-specific fitness function. The role of the fitness function is to guide the search to good solutions from a potentially infinite search space, within a practical time limit. Work on Search-Based Software Testing dates back to 1976, with interest in the area beginning to gather pace in the 1990s. More recently there has been an explosion of the amount of work. This paper reviews past work and the current state of the art, and discusses potential future research areas and open problems that remain in the field.",0,1,0,genetic algorithms+fitness functions+search process,
669,669,"Reliability and temperature constrained task scheduling for makespan minimization on heterogeneous multi core platforms. Abstract   We study the problem of scheduling tasks onto a heterogeneous multi-core processor platform for makespan minimization, where each cluster on the platform has a probability of failure governed by an exponential law and the processor platform has a thermal constraint specified by a peak temperature threshold. The goal of our work is to design algorithms that optimize makespan under the constraints of reliability and temperature. We first provide a mixed-integer linear programming (MILP) formulation for assigning and scheduling independent tasks with reliability and temperature constraints on the heterogeneous platform to minimize the makespan. However, MILP takes exponential time to finish. We then propose a two-stage heuristic that determines the assignment, replication, operating frequency, and execution order of tasks to minimize the makespan while satisfying the real-time, reliability, and temperature constraints based on the analysis of the effects of task assignment on makespan, reliability, and temperature. We finally carry out extensive simulation experiments to validate our proposed MILP formulation and two-stage heuristic. Simulation results demonstrate that the proposed MILP formulation can achieve the best performance in reducing makespan among all the methods used in the comparison. The results also show that the proposed two-stage heuristic has a close performance as the representative existing approach ESTS and a better performance when compared to the representative existing approach RBSA, in terms of reducing makespan. In addition, the proposed two-stage heuristic has the highest feasibility as compared to RBSA and ESTS.",0,1,0,multi core+multi-core processor+multi-core platforms,
670,670,"A call trace based technique for regression test selection of enterprise web applications sortea. Regression testing is applied to a modified program to ensure no new errors are introduced in previously tested code. This testing is computationally intensive and time consuming. Hence, Regression Test Selection (RTS) techniques are used to select necessary test cases for testing the modified program. The test cases selected from the test suite should potentially identify same errors as identified by running the complete test suite. The existing RTS techniques address only the web services and standalone applications. In this paper, we present a technique to select regression test cases for web based Java applications. Majority of the web applications follow Service Oriented Architecture (SOA). The server side components are often distributed and work independently on different nodes. Testing these components is difficult and time consuming. The distributed and batch nature of application leads to major challenges in testing them effectively. We list out some of these challenges and present our tool SoRTEA (Selection of Regression Test for Enterprise Application), which addresses few of them in selection of the regression test cases.",0,1,0,regression testing+test case prioritization+regression test selection,
671,671,"Could we predict the result of a continuous integration build an empirical study. Software build integrates modules developed and maintained by different developers in parallel, tests the result of integration, and serves as a crucial step in cooperatiive software development. Predicting the result of build has drawn the interest of academia and industry. In spite of many previous researches, the generalizability of build failure prediction over a wide range of open-source projects remains unclear.In this paper, we used 9 classifiers to construct prediction models and investigated the performance of both cross-validation and on-line predictions on 126 open-source projects available on TravisTorrent with nearly 300,000 build records. We found that for most projects, (a) the prediction performance in cross-validation scenario is pretty well (especial under AUC); (b) when it comes to on-line scenario, the prediction performance falls to a fairly low level.",0,1,0,software+software development+open source projects,
672,672,"A controlled experiment to evaluate the understandability of kaos and i for modeling teleo reactive systems. A novel method to model software requirements for Teleo-Reactive systems using i*.A novel method to model software requirements for Teleo-Reactive systems using KAOS.i* notation has higher understandability level for modeling Teleo-Reactive systems than KAOS. ContextTeleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment. ObjectiveThis article evaluates two different Goal Oriented Requirements Engineering notations, i* and KAOS, to determine their understandability level for specifying TR systems. MethodA controlled experiment was performed by two groups of Bachelor students. Each group first analyzed a requirements model of a mobile robotic system, specified using one of the evaluated languages, and then they filled in a questionnaire to evaluate its understandability. Afterwards, each group proceeded similarly with the model of another system specified with the second language. ResultsThe statistical analysis of the data obtained by means of the experiment showed that the understandability of i* is higher than that of KAOS when modeling TR systems. ConclusionBoth languages are suitable for specifying TR systems although their notations should be specialized to maximize the understandability attribute. i* surpasses KAOS due to two main reasons: i* models represent dependencies between agents and goals or tasks; and notational differences between tasks and goals in i* are more evident than those between goals and requirements in KAOS.",0,1,0,requirements models+requirements engineering+requirements specifications,
673,673,"The awareness network to whom should i display my actions and whose actions should i monitor. The concept of awareness plays a pivotal role in research in Computer-Supported Cooperative Work. Recently, software engineering researchers interested in the collaborative nature of software development have explored the implications of this concept in the design of software development tools. A critical aspect of awareness is the associated coordinative work practices of displaying and monitoring actions. This aspect concerns how colleagues monitor one another's actions to understand how these actions impact their own work and how they display their actions in such a way that others can easily monitor them while doing their own work. In this paper, we focus on an additional aspect of awareness: the identification of the social actors who should be monitored and the actors to whom their actions should be displayed. We address this aspect by presenting software developers' work practices based on ethnographic data from three different software development teams. In addition, we illustrate how these work practices are influenced by different factors, including the organizational setting, the age of the project, and the software architecture. We discuss how our results are relevant for both CSCW and software engineering researchers.",0,1,0,software development+software engineering+software reengineering,
674,674,"Formal specifications better than function points for code sizing. Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing.",0,1,0,formal verifications+function point+microkernel,
675,675,"Properties of effective metrics for coverage based statistical fault localization. In this paper, we investigate several coverage-based statistical fault localization metrics that have performed well in recent comparisons of many metrics, in order to better understand the properties of effective metrics. We first algebraically and probabilistically analyze the metrics to identify their key elements. Then we report on an empirical study we conducted to assess the relative importance of those elements. The results suggest that the most effective metrics contain a product of two terms: one that estimates the failure-causing effect of a program element (possibly with confounding bias) and one that weights the first term based on the evidence for the existence of faults in other program elements.",0,1,0,complexity metrics+fault location+fault localization,
676,676,"Efsm oriented minimal traces set generation approach for web applications. Most of web applications models focus on sequencing of events, where the ignored parameters or DOM elements changes and the relationship between the execution conditions and web states are crucial for analyzing and testing the behavior of client-side of client-server web applications. In this paper, we first define a novel trace, which can represent dynamic behaviors of web applications more accurately. Then an EFSM-oriented minimal traces set generation approach is proposed for modelling web applications. In order to ensure the integrity of the EFSM model and improve the effectiveness of the modelling process, three adequacy criteria with respect to all events, JS branches and DOM structures, are applied to compensate the traces and to guide the minimal traces set generation by greedy algorithm. Finally, the minimal traces set is abstracted into an EFSM as the behavior model for web applications. We implement a prototype tool for the proposed approach and empirically evaluate that the minimal traces set generation approach based on two web applications. The results show that the traces generated by the approach is effective and the all JavaScript branches coverage criteria is most appropriate to select the minimal traces set used for modelling.",0,1,0,dom tree+web application+javascript,
677,677,"Implicit gender biases in professional software development an empirical study. It has been well-known that the software development profession lacks gender diversity, particularly in the technical leadership positions. Researchers and practitioners have spent tremendous efforts on identifying the problems and finding solutions. However, most of the existing software engineering literature focuses on the explicit gender biases but ignores implicit gender biases. To fill this gap, the study sought to empirically investigate whether professional software engineers hold implicit gender biases related to women in the software development profession, and examine whether these implicit biases predict discriminatory decision-making. Using data from 142 professional software engineers in seven organizations, our study yields a rich set of concerning findings. First, we find that implicit biases were pervasive-both male and female software engineers implicitly associated software development professions, particular technical leadership roles, with men, not women, and also associated women with the home and family. Besides, people often cannot resist their implicit gender biases and make decisions in gender-neutral ways while they do well in resisting their explicit gender biases.",0,1,0,software development+software engineering+software reengineering,
678,678,"How do developers utilize source code from stack overflow. Technical question and answer Q&A platforms, such as Stack Overflow, provide a platform for users to ask and answer questions about a wide variety of programming topics. These platforms accumulate a large amount of knowledge, including hundreds of thousands lines of source code. Developers can benefit from the source code that is attached to the questions and answers on Q&A platforms by copying or learning from (parts of) it. By understanding how developers utilize source code from Q&A platforms, we can provide insights for researchers which can be used to improve next-generation Q&A platforms to help developers reuse source code fast and easily. In this paper, we first conduct an exploratory study on 289 files from 182 open-source projects, which contain source code that has an explicit reference to a Stack Overflow post. Our goal is to understand how developers utilize code from Q&A platforms and to reveal barriers that may make code reuse more difficult. In 31.5% of the studied files, developers needed to modify source code from Stack Overflow to make it work in their own projects. The degree of required modification varied from simply renaming variables to rewriting the whole algorithm. Developers sometimes chose to implement an algorithm from scratch based on the descriptions from Stack Overflow answers, even if there was an implementation readily available in the post. In 35.5% of the studied files, developers used Stack Overflow posts as an information source for later reference. To further understand the barriers of reusing code and to obtain suggestions for improving the code reuse process on Q&A platforms, we conducted a survey with 453 open-source developers who are also on Stack Overflow. We found that the top 3 barriers that make it difficult for developers to reuse code from Stack Overflow are: (1) too much code modification required to fit in their projects, (2) incomprehensive code, and (3) low code quality. We summarized and analyzed all survey responses and we identified that developers suggest improvements for future Q&A platforms along the following dimensions: code quality, information enhancement & management, data organization, license, and the human factor. For instance, developers suggest to improve the code quality by adding an integrated validator that can test source code online, and an outdated code detection mechanism. Our findings can be used as a roadmap for researchers and developers to improve code reuse.",0,1,0,source codes+software developer+open source projects,
679,679,"Automated tagging of software projects using bytecode and dependencies n. Several open and closed source repositories group software systems and libraries to allow members of particular organizations or the open source community to take advantage of them. However, to make this possible, it is necessary to have effective ways of searching and browsing the repositories. Software tagging is the process of assigning terms (i.e., tags or labels) to software assets in order to describe features and internal details, making the task of understanding software easier and potentially browsing and searching through a repository more effective. We present Sally, an automatic software tagging approach that is able to produce meaningful tags for Maven-based software projects by analyzing their bytecode and dependency relations without any special requirements from developers. We compared tags generated by Sally to the ones in two widely used online repositories, and the tags generated by a state-of-the-art categorization approach. The results suggest that Sally is able to generate expressive tags without relying on machine learning-based models.",0,1,0,software systems+software project+bytecodes,
680,680,"Doomsday equilibria for omega regular games. Two-player games on graphs provide the theoretical framework for many important problems such as reactive synthesis. While the traditional study of two-player zero-sum games has been extended to multi-player games with several notions of equilibria, they are decidable only for perfect-information games, whereas several applications require imperfect-information games.

In this paper we propose a new notion of equilibria, called doomsday equilibria, which is a strategy profile such that all players satisfy their own objective, and if any coalition of players deviates and violates even one of the players objective, then the objective of every player is violated.

We present algorithms and complexity results for deciding the existence of doomsday equilibria for various classes of ω-regular objectives, both for imperfect-information games, and for perfect-information games.We provide optimal complexity bounds for imperfect-information games, and in most cases for perfect-information games.",0,1,0,role-playing game+zero-sum game+complexity results,
681,681,"Empirical assessment of the effort needed to attack programs protected with client server code splitting. Code hardening is meant to fight malicious tampering with sensitive code executed on client hosts. Code splitting is a hardening technique that moves selected chunks of code from client to server. Although widely adopted, the effective benefits of code splitting are not fully understood and thoroughly assessed. The objective of this work is to compare non protected code vs. code splitting protected code, considering two levels of the chunk size parameter, in order to assess the effectiveness of the protection - in terms of both attack time and success rate - and to understand the attack strategy and process used to overcome the protection. We conducted an experiment with master students performing attack tasks on a small application hardened with different levels of protection. Students carried out their task working at the source code level. We observed a statistically significant effect of code splitting on the attack success rate that, on the average, was reduced from 89% with unprotected clear code to 52% with the most effective protection. The protection variant that moved some small-sized code chunks turned out to be more effective than the alternative moving fewer but larger chunks. Different strategies were identified yielding different success rates. Moreover we discovered that successful attacks exhibited different process w.r.t. failed ones. We found empirical evidence of the effect of code splitting, assessed the relative magnitude, and evaluated the influence of the chunk size parameter. Moreover we extracted the process used to overcome such obfuscation technique.",0,1,0,binary codes+source codes+code generation,
682,682,"Xen2mx high performance communication in virtualized environments. Cloud computing infrastructures provide vast processing power and host a diverse set of computing workloads, ranging from service-oriented deployments to high-performance computing (HPC) applications. As HPC applications scale to a large number of VMs, providing near-native network I/O performance to each peer VM is an important challenge. In this paper we present Xen2MX, a paravirtual interconnection framework over generic Ethernet, binary compatible with Myrinet/MX and wire compatible with MXoE. Xen2MX combines the zero-copy characteristics of Open-MX with Xen's memory sharing techniques. Experimental evaluation of our prototype implementation shows that Xen2MX is able to achieve nearly the same raw performance as Open-MX running in a non-virtualized environment. On the latency front, Xen2MX performs as close as 96% to the case where virtualization layers are not present. Regarding throughput, Xen2MX saturates a 10Gbps link, achieving 1159MB/s, compared to 1192MB/s of the non-virtualized case. Scales efficiently with the number of VMs, saturating the link for even smaller messages when 40 single-core VMs put pressure on the network adapters.",0,1,0,virtual machine monitors+virtualized environment+virtualization layers,
683,683,"Process mining as a modelling tool beyond the domain of business process management. Process mining emerged in the field of business process management BPM as an innovative technique to exploit the large amount of data recorded by information systems in the form of event logs. It allows to discover not only relations and structure in data but also control flow, and produces a process model, which can then be visualised as a process map. In addition to discovery, process mining supports conformance analysis, a technique to compare an a priori model with the event logs to detect deviations and inconsistencies.

In this paper we go beyond the domain of BPM and illustrate how process mining and conformance analysis can be used in a number of contexts, in and across the areas of human-computer interaction and learning.",0,1,0,business process management+bpm+process management,
684,684,"The seed is strong seeding strategies in search based software testing. Search-based techniques have been shown useful for the task of generating tests, for example in the case of object-oriented software. But, as for any meta-heuristic search, the efficiency is heavily dependent on many different factors, seeding is one such factor that may strongly influence this efficiency. In this paper, we evaluate new and typical strategies to seed the initial population as well as to seed values introduced during the search when generating tests for object-oriented code. We report the results of a large empirical analysis carried out on 20 Java projects (for a total of 1,752 public classes). Our experiments show with strong statistical confidence that, even for a testing tool that is already able to achieve high coverage, the use of appropriate seeding strategies can further improve performance.",0,1,0,software+object oriented software+java,
685,685,"Synthesizing qualitative research in software engineering a critical review. Synthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confirm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a significant amount of missing knowledge and incomplete understanding of the defined QRS methods in the community. Our effort also identifies an initial set factors that may influence the selection and use of appropriate QRS methods in SE.",0,1,0,software+software engineering+software reengineering,
686,686,"Time to pay up technical debt from a software quality perspective. Software companies need to produce high-quality software and support continuous and fast delivery of customer value both in the short and long term. However, this can be hindered by compromised software quality attributes 
that have an important influence on the overall software development lifecycle. 
The aim of this study is threefold: to understand which quality issues have the most negative impact on the software development lifecycle process, to deter-
mine the association of these quality issues in relation to the age of the software, and relate each of these quality issues to the impact of different Technical Debt types. This paper reports the results of six initial group interviews with in total 43 practitioners, an online web-survey provided quantitative data from 258 participants and seven follow-up group interviews with in total 32 industrial software practitioners. First, this study shows that practitioners identified maintenance difficulties, a limited ability to add new features, restricted reusability, and poor reliability, and performance degradation issues as the quality issues having the most negative effect on the software development lifecycle. Secondly, we found no evidence for the generally held view that the Technical Debt increases with age of the software. Thirdly, we show that Technical Debt affects not only 
productivity but also several other quality attributes of the system.",0,1,0,high-quality software+software quality+technical debt,
687,687,"Learning to rank vs ranking to learn strategies for regression testing in continuous integration. In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.",0,1,0,system testing+test case generation+regression testing,
688,688,"Predictors of success in applied stem education through guitar building. This research examined the predictors of success for secondary and postsecondary students taught by STEM Guitar Project-trained instructors. Each instructor participated in a 50-hour STEM Guitar Project development institute between 2013 and 2016 focusing on the manufacture of a solid-body electric guitar and received instruction focused on how to teach integrated STEM Modular Learning Activities (MLAs), which are aligned with the Common Core mathematics standards and the Next Generation Science Standards (NGSS). The data collected include pre- and post-assessment scores from 769 students in three grade bands (grades 6–8, 9–12, and undergraduate level from 15 states). Student mastery of the 12 core MLA concepts was measured through the deployment of pre- and post-assessments evaluating student knowledge across the 12 core concepts. Analysis of student scores showed significant improvement between pre- and post-assessment scores. The significant predictors of success included the percentage of minority students at the school and the STEM Guitar curriculum being taught by a science instructor. These findings indicate that students attending schools with a high percentage of minority students are more likely to increase their assessment score from the pre-assessment to the post-assessment. These data show encouraging results for using the electric guitar as a vehicle to teach integrated STEM concepts to secondary and postsecondary students, particularly at institutions with a high percentage of minority students.",0,1,0,social sciences+education+curricula,
689,689,"Kinect with ros interact with oculus towards dynamic user interfaces for robotic teleoperation. Teleoperation remains an important aspect for robotic systems especially when deployed in unstructured environments. While a range of research strives for robots that are completely autonomous, many robotic applications still require some level of human-in-the-loop control. In any situation where teleoperation is required an effective User Interface (UI) remains a key component within the systems design. Current advancements in Virtual Reality (VR) software and hardware such as the Oculus Rift, HTC Vive and Google Cardboard combined with greater transparency to robotic systems afforded by middleware such as the Robot Operating System (ROS) provides an opportunity to rapidly improve traditional teleoperation interfaces. This paper uses a System of System (SoS) approach to present the concept of a Virtual Reality Dynamic User Interface (VRDUI) for the teleoperation of heterogeneous robots. Different geometric virtual workspaces are discussed and a cylindrical workspace aligned with interactive displays is presented as a virtual control room. A presentation mode within the proposed VRDUI is also detailed, this shows how point cloud information obtained from the Microsoft Kinect can be incorporated within the proposed virtual workspace. This point cloud data is successfully processed into an OctoMap utilizing the octree data structure to create a voxelized representation of the 3D scanned environment. The resulting OctoMap is then displayed to an operator as a 3D point cloud using the Oculus Rift Head Mounted Display (HMD).",0,1,0,cloud services+operating systems+software,
690,690,"Why don t software developers use static analysis tools to find bugs. Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs, or software defects, faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers fix defects.",0,1,0,code inspection+static analysis+software defects,
691,691,"A counting semantics for monitoring ltl specifications over finite traces. We consider the problem of monitoring a Linear Time Logic (LTL) specification that is defined on infinite paths, over finite traces. For example, we may need to draw a verdict on whether the system satisfies or violates the property “p holds infinitely often.” The problem is that there is always a continuation of a finite trace that satisfies the property and a different continuation that violates it.",0,1,0,formal specification+linear time temporal logic+model checking,
692,692,"Benchmarking the ibm power8 processor. This paper discusses the performance of IBM's Power8 CPUs, on a number of skeleton, financial and CFD benchmarks and applications. Implicitly, the performance of the software toolchain is also tested - the bare-bones Little-Endian Ubuntu, the XL compilers and OpenMP runtimes. First, we aim to establish some roofline numbers on bandwidth and compute throughput, then move on to benchmark explicit and implicit one-/three-factor Black-Scholes computations, and CFD applications based on the OP2 and OPS frameworks, such as Airfoil, CloverLeaf, CloverLeaf 3D. These applications all exhibit different characteristics in terms of computations, communications, memory access patterns, etc. Both absolute and relative performance metrics are computed and compared to NVIDIA GPUs and Intel Xeon CPUs.",0,1,0,memory access patterns+memory access+compiler,
693,693,"An industrial case study on the use of uml in software maintenance and its perceived benefits and hurdles. UML is a commonly-used graphical language for the modelling of software. Works regarding UML’s effectiveness have studied projects that develop software systems from scratch. Yet the maintenance of software consumes a large share of the overall time and effort required to develop software systems. This study, therefore, focuses on the use of UML in software maintenance. We wish to elicit the practices of the software modelling used during maintenance in industry and understand what are perceived as hurdles and benefits when using modelling. In order to achieve a high level of realism, we performed a case study in a multinational company’s ICT department. The analysis is based on 31 interviews with employees who work on software maintenance projects. The interviewees played different roles and provided complementary views about the use, hurdles and benefits of software modelling and the use of UML. Our study uncovered a broad range of modelling-related practices, which are presented in a theoretical framework that illustrates how these practices are linked to the specific goals and context of software engineering projects. We present a list of recommended practices that contribute to the increased effectiveness of software modelling. The use of software modelling notations (like UML) is considered beneficial for software maintenance, but needs to be tailored to its context. Various practices that contribute to the effective use of modelling are commonly overlooked, suggesting that a more conscious holistic approach with which to integrate modelling practices into the overall software engineering approach is required.",0,1,0,software engineering+software reengineering+software maintenance,
694,694,"On learning meaningful code changes via neural machine translation. Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.",0,1,0,maintenance tasks+bug-fixing+refactorings,
695,695,"Pruning architectural models of automotive embedded systems via dependency analysis. Dependency analysis techniques are widely used to understand software implementations, and reduce their verification efforts. Recently, architectural languages have started to be integrated in the development of complex embedded systems. Such languages provide early development artifacts, which can be used to specify the structure and functionality of a system, and can be also analyzed in order to provide early information regarding the system's correctness. By performing dependency analysis on architectural languages, crucial dependencies can surface earlier in the life cycle. Once computed, these dependencies can be used to prune the architectural models in an attempt to reduce the early design-stage verification efforts. In this paper, we propose a dependency analysis-based technique that can be applied to prune models in EAST-ADL, an architectural description language tailored to automotive systems development. To achieve correct pruning, we investigate the types of dependencies that can appear in an architectural model, and how these dependencies create dependency chains within the model. Next, we investigate how such dependency chains can be exploited in formal verification in order to reduce the verified state-spaces during model-checking. Assuming a given requirement, our pruning method entails that only the relevant dependency chains are examined during EAST-ADL model-checking against that particular requirement. We validate our analysis results by comparing them to those obtained by applying an analytical approach for end-to-end timing analysis in EAST-ADL models. The methodology is illustrated on a Brake-by-Wire industrial system.",0,1,0,timing analysis+architecture description languages+east-adl,
696,696,"Scalable code clone detection and search based on adaptive prefix filtering. Abstract Code clone detection is a well-known software engineering problem that aims to detect all the groups of code blocks or code fragments that are functionally equivalent in a code base. It has numerous and wide ranging important uses in areas such as software metrics, plagiarism detection, aspect mining, copyright infringement investigation, code compaction, virus detection, and detecting bugs. A scalable code clone detection technique, able to process large source code repositories, is crucial in the context of multi-project or Internet-scale code clone detection scenarios. In this paper, we focus on improving the scalability of code clone detection, relative to current state of the art techniques. Our adaptive prefix filtering technique improves the performance of code clone detection for many common execution parameters, when tested on common benchmarks. The experimental results exhibit improvements for commonly used similarity thresholds of between 40% and 80%, in the best case decreasing the execution time up to 11% and increasing the number of filtered candidates up to 63%.",0,1,0,code fragments+code clone+clone detection,
697,697,"Linking source code to untangled change intents. Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: the pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.",0,1,0,source codes+software+software change,
698,698,"Autonomous online expansion technology for zigbee wireless sensor networks. In Zig Bee, the router capable devices have restriction to accept a number of devices as children devices. A router capable device can not allow any new device to join as a child device if it reaches to the maximum capacity of children or depth limit. If a device can not join the network, it isolates from the network and becomes an orphan node even though address spaces are available in the network. The orphan problem becomes worse when the topology of the network changes dynamically. In this paper we propose an autonomous online expansion technology for Zig Bee networks that shares available address spaces by router devices to connect maximum number of devices. Our simulation results show that the proposed online expansion technology significantly reduces the orphan nodes in the network.",0,1,0,wireless+wireless sensor networks+routers,
699,699,"A goal oriented approach for representing and using design patterns. We focus on cognitive aspects of software patterns.We propose goal-oriented format for documenting patterns.We revised the process for using patterns.We conducted an experiment to validate format and process. Design patterns are known as proven solutions to recurring design problems. The role of pattern documentation format is to transfer experience thus making pattern employment a viable technique. This research line proposes a goal-oriented pattern documentation that highlights decision-relevant information. The contribution of this paper is twofold. First, it presents a semi-structural visual notation that visualizes context, forces, alternative solutions and consequences in a compact format. Second, it introduces a systematic reuse process, in which the use of goal-oriented patterns aids the practitioner in selecting and customizing design patterns. An empirical study has been conducted the results of which supports the hypothesis that the goal-oriented format provides benefits for the practitioner. The experiment revealed a trend in which solutions better address requirements when the subjects are equipped with the new pattern documentation.",0,1,0,architecture designs+design patterns+software,
700,700,"Metrics to measure the change impact in atl model transformations. The Model-Driven Development (MDD) shifts the focus on code to models in the software development process. In MDD, model transformations are elements that play important role. In the software process, MDD projects evolve as changes in their transformations are frequent. Before applying changes it is important to measure their impacts in the transformation. However, currently no technique helps practitioners in this direction. In this work, we conducted an exploratory study to identify the criteria used by practitioners to measure the impact of changes in model transformations. As a result, we propose a set of metrics to measure such impacts. By measuring the change impact, practitioners can (i) save effort and development time for estimating costs to apply changes; and (ii) better schedule and prioritize changes according to the impact.",0,1,0,software project+model transformation+model driven development,
701,701,"Atrina inferring unit oracles from gui test cases. Testing JavaScript web applications is challenging due to its complex runtime interaction with the Document Object Model (DOM). Writing unit-level assertions for JavaScript applications is even more tedious as the tester needs to precisely understand the interaction between the DOM and the JavaScript code, which is responsible for updating the DOM. In this work, we propose to leverage existing DOM-dependent assertions in a human-written UI-based test cases as well as useful execution information inferred from the UI-based test suite to automatically generate assertions used for unit-level testing of the JavaScript code of the application. Our approach is implemented in a tool called ATRINA. We evaluate our approach to assess its effectiveness. The results indicate that ATRINA maps DOM-based assertions to the corresponding JavaScript code with high accuracy (99% precision, 92% recall). In terms of fault finding capability, the assertions generated by ATRINA outperform human-written DOM-based assertions by 31% on average. It also surpasses the state-of-the-art mutation-based assertion generation technique by 26% on average in detecting faults.",0,1,0,dom tree+web application+javascript,
702,702,"Conducting systematic literature reviews and systematic mapping studies. Context: An essential part of conducting software engineering (SE) research is the ability to identify extant research on tools, technologies, concepts and methods in order to evaluate and make rational and scientific decisions. The domain from which such knowledge is extracted is typically existing research literature found in journals, conference proceedings, books and gray literature. Empirical approaches that include various systematic review (SR) methodologies such as systematic literature review (SLR) and systematic mapping study (SMS) are found to be effective in this context. They adopt rigorous planning, follow repeatable and well-defined processes, and produce unbiased and evidence-based outcomes. Despite these significant benefits, the general trend on using these systematic review (SR) methodologies is not encouraging in SE research. The primary reasons emerging are twofold - a) SR methodologies are largely cited as time-consuming activities and b) lack of guidance to conduct systematic reviews. This tutorial discusses these concerns and describes an effective way of using SR methodologies for SE research.   Objectives: Attendees will be introduced to the key concepts, methods and processes for conducting systematic literature reviews (SLR) and systematic mapping studies (SMS). The benefits, limitations, guidelines for using SR methodologies in an effective manner will discussed in the session. Attendees will be guided on the appropriate formulation of a research question and sub questions; the development of a review protocol such as inclusion criteria, exclusion criteria, quality criteria and classification structures; and execution of review protocol using digital libraries and syntheses of review data. A web based software tool1, for supporting the systematic literature review process will be demonstrated and attendees will get the opportunity to use the tool to conduct the review to help in identification of relevant research and extraction and synthesis of data.   Method: We will use a blend of information presentation, interactive hands-on session and knowledge sharing session. The presentation will introduce the key concepts, benefits, limitations and how to overcome the limitations; hands on session will illustrate a review process with a case study, and finally the knowledge sharing session will discuss the experiences, best practices and the lesson learnt.",0,1,0,software+software engineering+software reengineering,
703,703,"Sign language recognition using image processing. One of the major drawback of our society is the barrier that is created between disabled or handicapped persons and the normal person. Communication is the only medium by which we can share our thoughts or convey the message but for a person with disability (deaf and dumb) faces difficulty in communication with normal person. For many deaf and dumb people , sign language is the basic means of communication.     Sign language recognition (SLR) aims to interpret sign languages automatically by a computer in order to help the deaf communicate with hearing society conveniently.  Our aim is to design a system  to help the person who trained the hearing impaired to communicate with the rest of the world using sign language or hand gesture recognition techniques. In this system, feature detection and feature extraction of hand gesture is done with the help of SURF algorithm using image processing. All this work is done using MATLAB software. With the help of this algorithm, a person can easily trained a deaf and dumb.",0,1,0,user with disabilities+communication+software,
704,704,Validating software measures using action research a method and industrial experiences. Validating software measures for using them in practice is a challenging task. Usually more than one complementary validation methods are applied for rigorously validating software measures: Theoretical methods help with defining the measures with expected properties and empirical methods help with evaluating the predictive power of measures. Despite the variety of these methods there still remain cases when the validation of measures is difficult. Particularly when the response variables of interest are not accurately measurable and the practical context cannot be reduced to an experimental setup the abovementioned methods are not effective. In this paper we present a complementary empirical method for validating measures. The method relies on action research principles and is meant to be used in combination with theoretical validation methods. The industrial experiences documented in this paper show that in many practical cases the method is effective.,0,1,0,software component+software+software project,
705,705,"Towards providing automated supports to developers on writing logging statements. Developers write logging statements to generate logs and record system execution behaviors. Such logs are widely used for a variety of tasks, such as debugging, testing, program comprehension, and performance analysis. However, there exists no practical guidelines on how to write logging statements; hence, making the logging decision a very challenging task. There are two main challenges that developers are facing while making logging decisions: 1) Difficult to accurately and succinctly record execution behaviors; and 2) Hard to decide where to write logging statements. This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations. Through case studies on large-scale open source and commercial systems, we anticipate that our study will provide useful suggestions and supports to developers for writing better logging statements.",0,1,0,web developers+software developer+program comprehension,
706,706,"A multihop broadcast mechanism for emergency messages dissemination in vanets. In Vehicular Ad Hoc Networks (VANETs), using multihop broadcast to achieve fast and reliable dissemination of event-driven emergency messages among vehicles has positive significance. Most existing solutions make relay selection by either senders or receivers. However, the former would fall into invalidation due to the unstable links, while the latter would generate extra delay. In this paper, we propose a new mechanism named MBM-EMD (i.e., multihop broadcast mechanism for emergency messages dissemination) to solve these problems. Our contributions are two-fold. i) We integrate multiple influencing factors such as vehicle density, relative movement, and channel quality into a new metric named ETGPH (i.e., the expected transmission gain per hop), and use it to select the optimal relays. ii) After acquiring the candidate set for further rebroadcast, the current sender firstly appoints an optimal relay from it to rebroadcast immediately. Other candidates will assist in forwarding according to our rules if this relay fails. The simulation results demonstrate the feasibility and effectiveness of the proposed solution.",0,1,0,vanet+ad hoc networks+relay selection,
707,707,"Layered reduction for modal specification theories. Modal transition systems (MTSs) are a well-known formalism used as an abstraction theory for labeled transition systems (LTSs). MTS specifications support compositionality together with a step-wise refinement methodology, and thus are useful for component-oriented design and analysis of distributed systems. This paper proposes a state-space reduction technique for such systems that are modeled as a network of acyclic MTSs. Our technique is based on the notion of layered transformation. We propose a layered composition operator for acyclic MTSs, and prove the communication closed layer (CCL) laws. Next, we define a partial order (po) equivalence between acyclic MTSs, and show that it enables performing layered transformation within the framework of CCL laws. We also show the preservation of existential (\(\exists \)) and universal (\(\forall \)) reachability properties under this transformation.",0,1,0,reachability problem+transition system+state-space reduction,
708,708,"Automatically assessing crashes from heap overflows. Heap overflow is one of the most widely exploited vulnerabilities, with a large number of heap overflow instances reported every year. It is important to decide whether a crash caused by heap overflow can be turned into an exploit. Efficient and effective assessment of exploitability of crashes facilitates to identify severe vulnerabilities and thus prioritize resources. In this paper, we propose the first metrics to assess heap overflow crashes based on both the attack aspect and the feasibility aspect. We further present HCSIFTER, a novel solution to automatically assess the exploitability of heap overflow instances under our metrics. Given a heap-based crash, HCSIFTER accurately detects heap overflows through dynamic execution without any source code or debugging information. Then it uses several novel methods to extract program execution information needed to quantify the severity of the heap overflow using our metrics. We have implemented a prototype HCSIFTER and applied it to assess nine programs with heap overflow vulnerabilities. HCSIFTER successfully reports that five heap overflow vulnerabilities are highly exploitable and two overflow vulnerabilities are unlikely exploitable. It also gave quantitatively assessments for other two programs. On average, it only takes about two minutes to assess one heap overflow crash. The evaluation result demonstrates both effectiveness and efficiency of HC Sifter.",0,1,0,program execution+security vulnerabilities+software vulnerabilities,
709,709,"Optimizing the expected mean payoff in energy markov decision processes. Energy Markov Decision Processes (EMDPs) are finite-state Markov decision processes where each transition is assigned an integer counter update and a rational payoff. An EMDP configuration is a pair s(n), where s is a control state and n is the current counter value. The configurations are changed by performing transitions in the standard way. We consider the problem of computing a safe strategy (i.e., a strategy that keeps the counter non-negative) which maximizes the expected mean payoff.",0,1,0,markov decision processes+mdp+markov chains,
710,710,"Software components beyond programming from routines to services. Software engineering (SE) conference in 1968, Doug Mc Ilroy introduced the concept of software components during his keynote speech, ""Mass-Produced Software Components."" That components hold such an esteemed place in SE history should come as no surprise: componentization is a fundamental engineering principle. Top-down approaches decompose large systems into smaller parts-components and bottom-up approaches compose smaller parts components into larger systems. Since 1968, components have played a role in both SE research and practice. For example, components have been an immanent part of software architecture from its early days.2 In 1998, the In ternational Conference on Software Engineering introduced component based software engineering (CBSE) as a specific area within SE at the first workshop on CBSE.",0,1,0,software architecture+software engineering+software reengineering,
711,711,"A controlled experiment in testing of safety critical embedded software. In engineering of safety critical systems, regulatory standards often put requirements on both traceable specification-based testing, and structural coverage on program units. Automated test generation techniques can be used to generate inputs to cover the structural aspects of a program. However, there is no conclusive evidence on how automated test generation compares to manual test design, or how testing based on the program implementation relates to specification-based testing. In this paper, we investigate specification -- and implementation-based testing of embedded software written in the IEC 61131-3 language, a programming standard used in many embedded safety critical software systems. Further, we measure the efficiency and effectiveness in terms of fault detection. For this purpose, a controlled experiment was conducted, comparing tests created by a total of twenty-three software engineering master students. The participants worked individually on manually designing and automatically generating tests for two IEC 61131-3 programs. Tests created by the participants in the experiment were collected and analyzed in terms of mutation score, decision coverage, number of tests, and testing duration. We found that, when compared to implementation-based testing, specification-based testing yields significantly more effective tests in terms of the number of faults detected. Specifically, specification-based tests more effectively detect comparison and value replacement type of faults, compared to implementation-based tests. On the other hand, implementation-based automated test generation leads to fewer tests (up to 85% improvement) created in shorter time than the ones manually created based on the specification.",0,1,0,test generations+safety critical software+automatic test pattern generation,
712,712,"Mining static code metrics for a robust prediction of software defect proneness. Defect-proneness prediction is affected by multiple aspects including sampling bias, non-metric factors, uncertainty of models etc. These aspects often contribute to prediction uncertainty and result in variance of prediction. This paper proposes two methods of data mining static code metrics to enhance defect-proneness prediction. Given little non-metric or qualitative information extracted from software codes, we first suggest to use a robust unsupervised learning method, shared nearest neighbors (SNN) to extract the similarity patterns of the code metrics. These patterns indicate similar characteristics of the components of the same cluster that may result in introduction of similar defects. Using the similarity patterns with code metrics as predictors, defect-proneness prediction may be improved. The second method uses the Occam's windows and Bayesian model averaging to deal with model uncertainty: first, the datasets are used to train and cross-validate multiple learners and then highly qualified models are selected and integrated into a robust prediction. From a study based on 12 datasets from NASA, we conclude that our proposed solutions can contribute to a better defect-proneness prediction.",0,1,0,data mining+software+software defects,
713,713,"On the fly decomposition of specifications in software model checking. Major breakthroughs have increased the efficiency and effectiveness of software model checking considerably, such that this technology is now applicable to industrial-scale software. However, verifying the full formal specification of a software system is still considered too complex, and in practice, sets of properties are verified one by one in isolation. We propose an approach that takes the full formal specification as input and first tries to verify all properties simultaneously in one verification run. Our verification algorithm monitors itself and detects situations for which the full set of properties is too complex. In such cases, we perform an automatic decomposition of the full set of properties into smaller sets, and continue the verification seamlessly. To avoid state-space explosion for large sets of properties, we introduce on-the-fly property weaving: properties get weaved into the program's transition system on the fly, during the analysis; which properties to weave and verify is determined dynamically during the verification process. We perform an extensive evaluation based on verification tasks that were derived from 4336 Linux kernel modules, and a set of properties that define the correct usage of the Linux API. Checking several properties simultaneously can lead to a significant performance gain, due to the fact that abstract models share many parts among different properties.",0,1,0,model checking+software systems+software model checking,
714,714,"Risk measure propagation through organisational network. In this paper we present a risk measurement methodology by incorporating actors' dependency relationships with each other across the whole organisational model. We argue that any actor associated with a vulnerable actor holds certain degree of vulnerability as a result of its association with the vulnerable actor. The degree of vulnerability becomes more if the actor is associated with more vulnerable actors. Similarly, we argue that any actor associated with a critical actor holds certain degree of criticality as a result of its association with the critical actor. The degree of criticality becomes more if the actor is associated with more critical actors. We believe our novel approach is capable of guiding the analyst to analyse vulnerability and criticality levels that exists among the actors' interdependencies across the whole organisational model, hence enabling him/her to design or redesign risk aware business processes.",0,1,0,business activities+business process+business process management,
715,715,"Experiences and challenges in building a data intensive system for data migration. Data Intensive (DI) applications are becoming more and more important in several fields of science, economy, and even in our normal life. Unfortunately, even if some technological frameworks are available for their development, we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc.. In this paper we report our action research experience in developing-testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases. We highlight the issues we have faced during our experience and we show how cumbersome, expensive and time-consuming the developing-testing-reengineering approach can be in this specific case. Also, we analyse the state of the art in the light of our experience and identify weaknesses and open challenges that could generate new research in the areas of software design and verification.",0,1,0,software development+software engineering+software reengineering,
716,716,"A comparative study for estimating software development effort intervals. Software cost/effort estimation is still an open challenge. Many researchers have proposed various methods that usually focus on point estimates. Until today, software cost estimation has been treated as a regression problem. However, in order to prevent overestimates and underestimates, it is more practical to predict the interval of estimations instead of the exact values. In this paper, we propose an approach that converts cost estimation into a classification problem and that classifies new software projects in one of the effort classes, each of which corresponds to an effort interval. Our approach integrates cluster analysis with classification methods. Cluster analysis is used to determine effort intervals while different classification algorithms are used to find corresponding effort classes. The proposed approach is applied to seven public datasets. Our experimental results show that the hit rate obtained for effort estimation are around 90---100%, which is much higher than that obtained by related studies. Furthermore, in terms of point estimation, our results are comparable to those in the literature although a simple mean/median is used for estimation. Finally, the dynamic generation of effort intervals is the most distinctive part of our study, and it results in time and effort gain for project managers through the removal of human intervention.",0,1,0,software development+software project+software cost estimation,
717,717,"Schrodinger s security opening the box on app developers security rationale. Research has established the wide variety of security failures in mobile apps, their consequences, and how app developers introduce or exacerbate them. What is not well known is why developers do so-what is the rationale underpinning the decisions they make which eventually strengthen or weaken app security? This is all the more complicated in modern app development's increasingly diverse demographic: growing numbers of independent, solo, or small team developers who do not have the organizational structures and support that larger software development houses enjoy. Through two studies, we open the box on developer rationale, by performing a holistic analysis of the rationale underpinning various activities in which app developers engage when developing an app. The first study does so through a task-based study with app developers ( $\mathrm{N}=44$ ) incorporating six distinct tasks for which this developer demographic must take responsibility: setting up a development environment, reviewing code, seeking help, seeking testers, selecting an advertisement SDK, and software licensing. We found that, while on first glance in several activities participants seemed to prioritize security, only in the code task such prioritization was underpinned by a security rationale-indicating that development behavior perceived to be secure may only be an illusion until the box is opened on their rationale. The second study confirms these findings through a wider survey of app developers ( $\mathrm{N}=274$ ) investigating to what extent they find the activities of the task-based study to affect their app's security. In line with the task-based study, we found that developers perceived actively writing code and actively using external SDKs as the only security-relevant, while similarly disregarding other activities having an impact on app security. Our results suggest the need for a stronger focus on the tasks and activities surrounding the coding task - all of which need to be underpinned by a security rationale. Without such a holistic focus, developers may write “secure code” but not produce “secure apps”.",0,1,0,software+software developer+software development,
718,718,"Automated resolution of connector architectures using constraint solving arcas method. In current software systems, connectors play an important role by encapsulating the communication and coordination logic. Since they share common patterns (elements) depending on characteristics of the connections, the elements can be predefined and reused. A method of connector implementation based on a composition of predefined elements naturally comprises two steps: resolution of the connector architecture, and creation of the actual connector code based on the architecture. However, manual resolution of a connector architecture is very difficult due to the number of factors to be considered. Thus, the challenge is to come up with an automated method, able to address all the important factors. In this paper, we present a method for automated resolution of connector architectures based on constraint solving techniques. We exploit a propositional logic with relational calculus for defining a connector theory, a constraint specification reflecting both the predefined parts and the important resolution factors, and employ a constraint solver to find a suitable connector architecture as a model of the theory. As a proof of the concept, we show how the theory can be captured in the Alloy language and resolved via the Alloy Analyzer.",0,1,0,constraint solvers+software+software systems,
719,719,A quantitative analysis of developer information needs in software ecosystems. We present the results of an investigation into the nature of information needs of software developers who work in projects that are part of larger ecosystems. This work is based on a quantitative survey of 75 professional software developers. We corroborate the results identified in the survey with needs and motivations proposed in a previous survey and discover that tool support for developers working in an ecosystem context is even more meager than we thought: mailing lists and internet search are the most popular tools developers use to satisfy their ecosystem-related information needs.,0,1,0,software+software developer+tool support,
720,720,"Importance splitting for statistical model checking rare properties. Statistical model checking avoids the intractable growth of states associated with probabilistic model checking by estimating the probability of a property from simulations. Rare properties are often important, but pose a challenge for simulation-based approaches: the relative error of the estimate is unbounded. A key objective for statistical model checking rare events is thus to reduce the variance of the estimator. Importance splitting achieves this by estimating a sequence of conditional probabilities, whose product is the required result. To apply this idea to model checking it is necessary to define a score function based on logical properties, and a set of levels that delimit the conditional probabilities.

In this paper we motivate the use of importance splitting for statistical model checking and describe the necessary and desirable properties of score functions and levels. We illustrate how a score function may be derived from a property and give two importance splitting algorithms: one that uses fixed levels and one that discovers optimal levels adaptively.",0,1,0,correlation analysis+model checking+probabilistic model checking,
721,721,"Dynamic deployment of context aware access control policies for constrained security devices. Abstract: Securing the access to a server, guaranteeing a certain level of protection over an encrypted communication channel, executing particular counter measures when attacks are detected are examples of security requirements. Such requirements are identified based on organizational purposes and expectations in terms of resource access and availability and also on system vulnerabilities and threats. All these requirements belong to the so-called security policy. Deploying the policy means enforcing, i.e., configuring, those security components and mechanisms so that the system behavior be finally the one specified by the policy. The deployment issue becomes more difficult as the growing organizational requirements and expectations generally leave behind the integration of new security functionalities in the information system: the information system will not always embed the necessary security functionalities for the proper deployment of contextual security requirements. To overcome this issue, our solution is based on a central entity approach which takes in charge unmanaged contextual requirements and dynamically redeploys the policy when context changes are detected by this central entity. We also present an improvement over the OrBAC (Organization-Based Access Control) model. Up to now, a controller based on a contextual OrBAC policy is passive, in the sense that it assumes policy evaluation triggered by access requests. Therefore, it does not allow reasoning about policy state evolution when actions occur. The modifications introduced by our work overcome this limitation and provide a proactive version of the model by integrating concepts from action specification languages.",0,1,0,access control policies+security policy+security requirements,
722,722,"Revisiting goal oriented requirements engineering with a regulation view. Goal-Oriented Requirements Engineering (GORE) is considered to be one of the main achievements that the Requirements Engineering field has produced since its inception. Several GORE methods were designed in the last twenty years in both research and industry. In analyzing individual and organizational behavior, goals appear as a natural element. There are other organizational models that may better explain human behavior, albeit at the expense of more complex models. We present one such alternative model that explains individual and organizational survival through continuous regulation. We give our point of view of the changes needed in GORE methods in order to support this alternative view through the use of maintenance goals and beliefs. We illustrate our discussion with the real example of a family practitioner association that needed a new information system.",0,1,0,engineering+goal-oriented requirements engineering+requirements engineering,
723,723,"Do programmers work at night or during the weekend. Abnormal working hours can reduce work health, general well-being, and productivity, independent from a profession. To inform future approaches for automatic stress and overload detection, this paper establishes empirically collected measures of the work patterns of software engineers. To this aim, we perform the first large-scale study of software engineers' working hours by investigating the time stamps of commit activities of 86 large open source software projects, both containing hired and volunteer developers. We find that two thirds of software engineers mainly follow typical office hours, empirically established to be from 10h to 18h, and do not usually work during nights and weekends. Large variations between projects and individuals exist. Surprisingly, we found no support that project maturation would decrease abnormal working hours. In the Firefox case study, we found that hired developers work more during office hours while seniority, either in terms of number of commits or job status, did not impact working hours. We conclude that the use of working hours or timestamps of work products for stress detection requires establishing baselines at the level of individuals.",0,1,0,firefox+software developer+software project,
724,724,"Gate game based testing environment. In this paper, we propose a game-based public testing mechanism called GATE. The purpose of GATE is to make use of the rich human resource on the Internet to help increase effectiveness in software testing and improve test adequacy. GATE facilitates public testing in three main steps: 1) decompose the test criterion satisfaction problem into many smaller sub-model satisfaction problems; 2) construct games for each individual sub-models and presenting the games to the public through web servers; 3) collect and convert public users' action sequence data into real test cases which guarantee to cover not adequately tested elements.   A preliminary study on apache-commons-math library shows that 44% of the branches have not been adequately tested by state of the art automatic test generation techniques. Among these branches, at least 42% are decomposable by GATE into smaller sub-problems. These elements naturally become the potential targets of GATE for public game-based testing.",0,1,0,test generations+atpg+automatic test pattern generation,
725,725,"Output oriented refactoring in php based dynamic web applications. Refactoring is crucial in the development process of traditional programs as well as advanced Web applications. In a dynamic Web application, multiple versions of client code in HTML and JavaScript are dynamically generated from server-side code at run time for different usage scenarios. Toward understanding refactoring for dynamic Web code, we conducted an empirical study on several PHP-based Web applications. We found that Web developers perform a new type of refactoring that is specific to PHP-based dynamic Web code and pertain to output client-side code. After such a refactoring, the server-side code is more compact and modular with less amount of embedded and inline client-side HTML/JS code, or produces more standard-conforming client-side code. However, the corresponding output client-side code of the server code before and after the refactoring provides the same external behavior. We call it output-oriented refactoring. Our finding in the study motivates us to build WebDyn, an automatic tool for dynamicalizing refactorings. When performing on a portion of server-side code (which might contain both PHP and embedded/inline HTML/JS code), WebDyn detects the repeated and varied parts in that code portion and produces dynamic PHP code that creates the same client-side code. Our empirical evaluation on several projects showed WebDyn's accuracy in such automated refactorings.",0,1,0,php+web developers+dynamic web applications,
726,726,"A study on the use of ide features for debugging. Integrated development environments (IDEs) provide features to help developers both create and understand code. As maintenance and bug repair are time-consuming and costly activities, IDEs have long integrated debugging features to simplify these tasks. In this paper we investigate the impact of using IDE debugger features on different aspects of programming and debugging. Using the data set provided by MSR challenge track, we compared debugging tasks performed with or without the IDE debugger. We find, on average, that developers spend more time and effort on debugging when they use the debugger. Typically, developers start using the debugger early, at the beginning of a debugging session, and that their editing behavior does not appear to significantly change when they are debugging regardless of whether debugging features are in use.",0,1,0,program debugging+web developers+software developer,
727,727,"What causes merge conflicts. During the software development process, several developers commonly change artifacts in parallel. A merge process can combine parallel changes. In the case of changes that cannot be automatically combined, the developer responsible for the merge must reconcile decisions and resolve conflicts. Some studies are concerned with investigating ways to deal with merge conflicts and measuring the effort that this activity may require. However, the investigation of factors that may reduce the occurrence of conflicts needs more and deeper attention. This paper aims at identifying and analyzing attributes of past merges with and without conflicts to understand what may induce physical conflicts. We analyzed 182,273 merge scenarios from 80 projects written in eight different programming languages to find characteristics that increase the chances of a merge to have a conflict. We found that attributes such as the number of committers, the number of commits, and the number of changed files seem to have the biggest influence in the occurrence of merge conflicts. Moreover, attributes in the branch that is being integrated seem to be more influential than the same attributes in the other branch. Additionally, we discovered positive correlations between the occurrence of conflicts and both the duration of the branch and the intersection of developers.",0,1,0,software+software developer+software development,
728,728,"Accelerated simulated fault injection testing. Fault injection testing approaches assess the reliability of execution environments for critical software. They support the early testing of safety concepts that mitigate the impact of hardware failures on software behavior. The growing use of platform software for embedded systems raises the need to verify safety concepts that execute on top of operating systems and middleware platforms. Current fault injection techniques consider the resulting software stack as one black box and attempt to test the reaction of all components in the context of faults. This leads to very high software complexity and consequently requires a very high number of fault injection experiments. Testing the software components, such as control functions, operating systems, and middleware, individually would lead to a significant reduction of the number of experiments required. In this paper, we illustrate our novel approach for fault injection testing, which considers the components of a software stack, enables re-use of previously collected evidences, allows focusing testing on highly critical parts of the control software, and significantly lowers the number of experiments required.",0,1,0,execution environments+software complexity+fault injection,
729,729,"Practical ajax race detection for javascript web applications. Asynchronous client-server communication is a common source of errors in JavaScript web applications. Such errors are difficult to detect using ordinary testing because of the nondeterministic scheduling of AJAX events. Existing automated event race detectors are generally too imprecise or too inefficient to be practically useful. To address this problem, we present a new approach based on a lightweight combination of dynamic analysis and controlled execution that directly targets identification of harmful AJAX event races. We experimentally demonstrate using our implementation, AjaxRacer, that this approach is capable of automatically detecting harmful AJAX event races in many websites, and producing informative error messages that support diagnosis and debugging. Among 20 widely used web pages that use AJAX, AjaxRacer discovers harmful AJAX races in 12 of them, with a total of 72 error reports, and with very few false positives.",0,1,0,dynamic analysis+program debugging+race detection,
730,730,"Symbolic verification of regular properties. Verifying the regular properties of programs has been a significant challenge. This paper tackles this challenge by presenting symbolic regular verification (SRV) that offers significant speedups over the state-of-the-art. SRV is based on dynamic symbolic execution (DSE) and enabled by novel techniques for mitigating path explosion: (1) a regular property-oriented path slicing algorithm, and (2) a synergistic combination of property-oriented path slicing and guiding. Slicing prunes redundant paths, while guiding boosts the search for counterexamples. We have implemented SRV for Java and evaluated it on 15 real-world open-source Java programs (totaling 259K lines of code). Our evaluation results demonstrate the effectiveness and efficiency of SRV. Compared with the state-of-the-art --- pure DSE, pure guiding, and pure path slicing --- SRV achieves average speedups of more than 8.4X, 8.6X, and 7X, respectively, making symbolic regular property verification significantly more practical.",0,1,0,java+java program+symbolic execution,
731,731,"Xacmet xacml testing modeling. In the context of access control systems, testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. In XACML-based access control systems, incoming access requests are transmitted to the policy decision point (PDP) that grants or denies the access based on the defined XACML policies. The criticality of a PDP component requires an intensive testing activity consisting in probing such a component with a set of requests and checking whether its responses grant or deny the requested access as specified in the policy. Existing approaches for improving manual derivation of test requests such as combinatorial ones do not consider policy function semantics and do not provide a verdict oracle. In this paper, we introduce XACMET, a novel approach for systematic generation of XACML requests as well as automated model-based oracle derivation. The main features of XACMET are as follows: (i) it defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation; (ii) it derives a set of test requests via full-path coverage of this graph; (iii) it derives automatically the expected verdict of a specific request execution by executing the corresponding path in such graph; (iv) it allows us to measure coverage assessment of a given test suite. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach.",0,1,0,xacml+system testing+policy language,
732,732,"Unified ltl verification and embedded execution of uml models. The increasing complexity of embedded systems leads to uncertain behaviors, security flaws, and design mistakes. With model-based engineering, early diagnosis of such issues is made possible by verification tools working on design models. However, three severe drawbacks remain to be fixed. First, transforming design models into executable code creates a semantic gap between models and code. Furthermore, for formal verification, a second transformation (towards a formal language) is generally required, which complicates the diagnosis process. Finally, an equivalence relation between verified formal models and deployed code should be built, proven, and maintained. To tackle these issues, we introduce a UML interpreter that fulfills multiple purposes: simulation, formal verification, and execution on both desktop computer and bare-metal embedded target. Using a single interpreter for all these activities ensures operational semantics consistency. We illustrate our approach on a level crossing example, showing verification of LTL properties on a desktop computer, as well as execution on a stm32 embedded target.",0,1,0,unified modeling language+verification tools+uml model,
733,733,"Bridging java annotations and uml profiles with jump. UML profiles support annotations at the modeling level. However, current modeling tools lack the capabilities to generate such annotations required for the programming level, which is desirable for reverse engineering and forward engineering scenarios. To overcome this shortcoming, we defined an effective conceptual mapping between Java annotations and UML profiles as a basis for implementing the JUMP tool. It automates the generation of profiles from annotation-based libraries and their application to generate profiled UML models. In this demonstration, we (i) compare our mapping with the different representational capabilities of current UML modeling tools, (ii) apply our tool to a model-based software modernization scenario, and (iii) evaluate its scalability with real-world libraries and applications.",0,1,0,sysml+uml 2.0+uml profiles,
734,734,"The first twenty five years of industrial use of the b method. The B-Method has an interesting history, where language and tools have evolved over the years. This not only led to considerable research and progress in the area of formal methods, but also to numerous industrial applications, in particular in the railway domain. We present a survey of the industrial usage of the B-Method since the first toolset in 1993 and the inauguration of the driverless metro line 14 in Paris in 1999. We discuss the various areas of applications, from software development to data validation and on to systems modelling. The evolution of the tooling landscape is also analysed, and we present an assessment of the current situation, lessons learned and possible new directions.",0,1,0,software+formal methods+software development,
735,735,"A modular approach to calculate service based maintainability metrics from runtime data of microservices. While several service-based maintainability metrics have been proposed in the scientific literature, reliable approaches to automatically collect these metrics are lacking. Since static analysis is complicated for decentralized and technologically diverse microservice-based systems, we propose a dynamic approach to calculate such metrics from runtime data via distributed tracing. The approach focuses on simplicity, extensibility, and broad applicability. As a first prototype, we implemented a Java application with a Zipkin integrator, 23 different metrics, and five export formats. We demonstrated the feasibility of the approach by analyzing the runtime data of an example microservice-based system. During an exploratory study with six participants, 14 of the 18 services were invoked via the system’s web interface. For these services, all metrics were calculated correctly from the generated traces.",0,1,0,java+java applications+static analysis,
736,736,"Exploring how to support software revision in software non intensive projects using existing techniques. Most industrial products are developed based on their former products including software. Revising existing software according to new requirements is thus an important issue. However, innovative techniques for software revision cannot be easily introduced to projects where software is not a central part. In this paper, we report how to explore and apply software engineering techniques to such non-ideal projects to encourage technology transfer to industry. We first show our experiences with industrial partners to explore which tasks could be supported in such projects and which techniques could be applied to such tasks. As a result, we found change impact analysis could be technically supported, and traceability techniques using information retrieval seemed to be suitable for it. We second had preliminary experiences of a method using such techniques with data in industry and evaluated them with our industrial partners. Based on the evaluation, we third improved such a method by using following techniques, indexing of technical documents for characterizing requirements changes, machine learning on source codes for validating predicted traceability and static source code analysis for finding indirect impacts. Our industrial partners finally evaluated the improved method, and they confirmed the improved method worked better than ever.",0,1,0,software evolution+refactorings+change impact analysis,
737,737,"Extending complex event processing to graph structured information. Complex Event Processing (CEP) is a powerful technology in realtime distributed environments for analyzing fast and distributed streams of data, and deriving conclusions from them. CEP permits defining complex events based on the events produced by the incoming sources in order to identify complex meaningful circumstances and to respond to them as quickly as possible. However, in many situations the information that needs to be analyzed is not structured as a mere sequence of events, but as graphs of interconnected data that evolve over time. This paper proposes an extension of CEP systems that permits dealing with graph-structured information. Two case studies are used to validate the proposal and to compare its performance with traditional CEP systems. We discuss the benefits and limitations of the CEP extensions presented.",0,1,0,user information+data stream+distributed environments,
738,738,"Model driven development of mobile applications allowing role driven variants. Rapidly increasing numbers of applications and users make the development of mobile applications to one of the most promising fields in software engineering. Due to short time-to-market, differing platforms and fast emerging technologies, mobile application development faces typical challenges where model-driven development can help. We present a modeling language and an infrastructure for the model-driven development (MDD) of Android apps supporting the specification of different app variants according to user roles. For example, providing users may continuously configure and modify custom content with one app variant whereas end users are supposed to use provided content in their variant. Our approach allows a flexible app development on different abstraction levels: compact modeling of standard app elements, detailed modeling of individual elements, and separate provider models for specific custom needs. We demonstrate our MDD-approach at two apps: a phone book manager and a conference guide being configured by conference organizers for participants.",0,1,0,software engineering+software reengineering+model driven development,
739,739,"Statistical learning approach for mining api usage mappings for code migration. The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy.",0,1,0,software+java+software project,
740,740,"Exploring the use of automated api migrating techniques in practice an experience report on android. In recent years, open source software libraries have allowed developers to build robust applications by consuming freely available application program interfaces (API). However, when these APIs evolve, consumers are left with the difficult task of migration. Studies on API migration often assume that software documentation lacks explicit information for migration guidance and is impractical for API consumers. Past research has shown that it is possible to present migration suggestions based on historical code-change information. On the other hand, research approaches with optimistic views of documentation have also observed positive results. Yet, the assumptions made by prior approaches have not been evaluated on large scale practical systems, leading to a need to affirm their validity. This paper reports our recent practical experience migrating the use of Android APIs in FDroid apps when leveraging approaches based on documentation and historical code changes. Our experiences suggest that migration through historical code-changes presents various challenges and that API documentation is undervalued. In particular, the majority of migrations from removed or deprecated Android APIs to newly added APIs can be suggested by a simple keyword search in the documentation. More importantly, during our practice, we experienced that the challenges of API migration lie beyond migration suggestions, in aspects such as coping with parameter type changes in new API. Future research may aim to design automated approaches to address the challenges that are documented in this experience report.",0,1,0,android+software+open source software,
741,741,"Contract based modeling and verification of timed safety requirements within sysml. In order to cope with the growing complexity of critical real-time embedded systems, systems engineering has adopted a component-based design technique driven by requirements. Yet, such an approach raises several issues since it does not explicitly prescribe how system requirements can be decomposed on components nor how components contribute to the satisfaction of requirements. The envisioned solution is to design, with respect to each requirement and for each involved component, an abstract specification, tractable at each design step, that models how the component is concerned by the satisfaction of the requirement and that can be further refined toward a correct implementation. In this paper, we consider such specifications in the form of contracts. A contract for a component consists in a pair (assumption, guarantee) where the assumption models an abstract behavior of the component's environment and the guarantee models an abstract behavior of the component given that the environment behaves according to the assumption. Therefore, contracts are a valuable asset for the correct design of systems, but also for mapping and tracing requirements to components, for tracing the evolution of requirements during design and, most importantly, for compositional verification of requirements. The aim of this paper is to introduce contract-based reasoning for the design of critical real-time systems made of reactive components modeled with UML and/or SysML. We propose an extension of UML and SysML languages with a syntax and semantics for contracts and the refinement relations that they must satisfy. The semantics of components and contracts is formalized by a variant of timed input/output automata on top of which we build a formal contract-based theory. We prove that the contract-based theory is sound and can be applied for a relatively large class of SysML system models. Finally, we show on a case study extracted from the automated transfer vehicle (http://www.esa.int/ATV) that our contract-based theory allows to verify requirement satisfaction for previously intractable models.",0,1,0,unified modeling language+real-time embedded systems+sysml,
742,742,"The enterprise service bus as integration architecture in heterogeneous systems. The standardization of the architecture of electronic healthcare records is very important from several point of view. To design a reference architecture for personal healthcare, ensuring interoperability between heterogeneous devices and services, as well as a reliable and secure patient data management and a seamless integration with the clinical workflow, is the point of starting of our research. In this paper we present ESB, as the core of the Middleware subsystem for interaction among different and heterogeneous systems.",0,1,0,architectural models+service-oriented architecture (soa)+reference architecture,
743,743,"Investigating program behavior using the texada ltl specifications miner. Temporal specifications, relating program events through time, are useful for tasks ranging from bug detection to program comprehension. Unfortunately, such specifications are often lacking from system descriptions, leading researchers to investigate methods for inferring these specifications from code, execution traces, code comments, and other artifacts. This paper describes Texada, a tool to dynamically mine temporal specifications in LTL from traces of program activity. We review Texada's key features and demonstrate how it can be used to investigate program behavior through two scenarios: validating an implementation that solves the dining philosophers problem and supporting comprehension of a stack implementation. We also detail Texada's other, more advanced, usage options. Texada is an open source tool: https://bitbucket.org/bestchai/texada",0,1,0,program execution+execution trace+program comprehension,
744,744,"Array folds logic. We present an extension to the quantifier-free theory of integer arrays which allows us to express counting. The properties expressible in Array Folds Logic (AFL) include statements such as “the first array cell contains the array length,” and “the array contains equally many minimal and maximal elements.” These properties cannot be expressed in quantified fragments of the theory of arrays, nor in the theory of concatenation. Using reduction to counter machines, we show that the satisfiability problem of AFL is PSPACE-complete, and with a natural restriction the complexity decreases to NP. We also show that adding either universal quantifiers or concatenation leads to undecidability.",0,1,0,dynamic logic+satisfiability problems+satisfiability,
745,745,"On the use of uml documentation in software maintenance results from a survey in industry. This paper presents the findings of a survey on the use of UML in software maintenance, carried out with 178 professionals working on software maintenance projects in 12 different countries. As part of long-term research we are carrying out to investigate the benefits of using UML in software maintenance, the main objectives of this survey are: 1) to explore whether UML diagrams are being used in software industry maintenance projects; 2) to see what UML diagrams are the most effective for software maintenance; 3) to find out what the perceived benefits of using UML diagrams are; and 4) to contextualize the kind of companies that use UML documentation in software maintenance. Some complementary results based on the way the documentation is used (whether it is UML-based or not) during software maintenance are also presented.",0,1,0,unified modeling language+software industry+software maintenance,
746,746,"Co evolution of project documentation and popularity within github. Github is a very popular collaborative software-development platform that provides typical source-code management and issue tracking features augmented by strong social-networking features such as following developers and watching projects. These features help ``spread the word'' about individuals and projects, building the reputation of the former and increasing the popularity of the latter. In this paper, we investigate the relation between project popularity and regular, consistent documentation updates. We found strong indicators that consistently popular projects exhibited consistent documentation effort and that this effort tended to attract more documentation collaborators. We also found that frameworks required more documentation effort than libraries to achieve similar adoption success, especially in the initial phase.",0,1,0,software+collaborative software development+software development,
747,747,"The mystery of the writing that isn t on the wall differences in public representations in traditional and agile software development. This paper considers the use of public displays, such as whiteboards and papers pinned to walls, by different software development teams, based on evidence from a number of empirical studies. This paper outlines differences in use observed between traditional and agile teams and begins to identify the implications that they may have for software development.",0,1,0,software development teams+software development+agile software development,
748,748,"Automated api documentation with tutorials generated from stack overflow. Software reuse provides benefits during the software development and maintenance processes. The use of APIs is one of the most common ways to reuse. However, obtaining an easy-to-understand documentation is a challenge faced by developers. Several papers have proposed alternatives to make API documentation more understandable, or even more detailed. However, these studies have not taken into account the complexity of examples in order to make documentation adaptable to different levels of developer experience. In this work, we developed and evaluated four different methodologies to generate tutorials for APIs from the contents of Stack Overflow and organize them according to the complexity of understanding. The methodologies were evaluated through tutorials generated for the Swing API. A survey was conducted to evaluate eight different features of the generated tutorials. The overall outcome was positive on several characteristics, showing the feasibility of automatically generated tutorials. In addition, the adoption of features for presenting tutorial elements in order of complexity, for separating the tutorial in basic and advanced parts, for selecting posts with a tutorial nature and with didactic source code had significantly better results regarding the generation methodology.",0,1,0,software developer+software reuse+software development,
749,749,"Dsibin identifying dynamic data structures in c c binaries. Reverse engineering binary code is notoriously difficult and, especially, understanding a binary’s dynamic data structures. Existing data structure analyzers are limited wrt. program comprehension: they do not detect complex structures such as skip lists, or lists running through nodes of different types such as in the Linux kernel’s cyclic doubly-linked list. They also do not reveal complex parent-child relationships between structures. The tool DSI remedies these shortcomings but requires source code, where type information on heap nodes is available.     We present DSIbin, a combination of DSI and the type excavator Howard for the inspection of C/C++ binaries. While a naive combination already improves upon related work, its precision is limited because Howard’s inferred types are often too coarse. To address this we auto-generate candidates of refined types based on speculative nested-struct detection and type merging; the plausibility of these hypotheses is then validated by DSI. We demonstrate via benchmarking that DSIbin detects data structures with high precision.",0,1,0,reverse engineering+linux+program comprehension,
750,750,"Automated extraction of conceptual models from user stories via nlp. Natural language (NL) is still the predominant notation that practitioners use to represent software requirements. Albeit easy to read, NL does not readily highlight key concepts and relationships such as dependencies and conflicts. This contrasts with the inherent capability of graphical conceptual models to visualize a given domain in a holistic fashion. In this paper, we propose to automatically derive conceptual models from a concise and widely adopted NL notation for requirements: user stories. Due to their simplicity, we hypothesize that our approach can improve on the low accuracy of previous works. We present an algorithm that combines state-of-the-art heuristics and that is implemented in our Visual Narrator tool. We evaluate our work on two case studies wherein we obtained promising precision and recall results (between 80% and 92%). The creators of the user stories perceived the generated models as a useful artifact to communicate and discuss the requirements, especially for team members who are not yet familiar with the project.",0,1,0,functional requirement+software requirements+requirements specifications,
751,751,"Supervised representation learning approach for cross project aging related bug prediction. Software aging, which is caused by Aging-Related Bugs (ARBs), tends to occur in long-running systems and may lead to performance degradation and increasing failure rate during software execution. ARB prediction can help developers discover and remove ARBs, thus alleviating the impact of software aging. However, ARB-prone files occupy a small percentage of all the analyzed files. It is usually difficult to gather sufficient ARB data within a project. To overcome the limited availability of training data, several researchers have recently developed cross-project models for ARB prediction. A key point for cross-project models is to learn a good representation for instances in different projects. Nevertheless, most of the previous approaches neither consider the reconstruction property of new representation nor encode source samples' label information in learning representation. To address these shortcomings, we propose a Supervised Representation Learning Approach (SRLA), which is based on double encoding-layer autoencoder, to perform cross-project ARB prediction. Moreover, we present a transfer cross-validation framework to select the hyper-parameters of cross-project models. Experiments on three large open-source projects demonstrate the effectiveness and superiority of our approach compared with the state-of-the-art approach TLAP.",0,1,0,software+software aging+open source projects,
752,752,"Model driven techniques to enhance architectural languages interoperability. The current practice of software architecture modeling and analysis would benefit of using different architectural languages, each specialized on a particular view and each enabling specific analysis. Thus, it is fundamental to pursue architectural language interoperability. An approach for enabling interoperability consists in defining a transformation from each single notation to a pivot language, and vice versa. When the pivot assumes the form of a small and abstract kernel, extension mechanisms are required to compensate the loss of information. The aim of this paper is to enhance architectural languages interoperability by means of hierarchies of pivot languages obtained by systematically extending a root pivot language. Model-driven techniques are employed to support the creation and the management of such hierarchies and to realize the interoperability by means of model transformations. Even though the approach is applied to the software architecture domain, it is completely general.",0,1,0,software architecture+architecture description languages+model transformation,
753,753,"The problems with eclipse modeling tools a topic analysis of eclipse forums. Eclipse offers a wide range of tools supporting various aspects of modeling and Model-Driven Engineering (MDE). Arguably, the Eclipse ecosystem has been and continues to be one of the most important modeling tool repositories and sources of information about these tools, with, for example, more than 180,000 posts in the modeling forums since 2002. In this paper, we collect and analyze the content of the 30 most widely used Eclipse forums associated with different modeling and MDE tools, such as EMF, Xtext, ATL, Epsilon, and GMF. Using state-of-the-art text mining techniques coupled with manual analysis, we explore these forums with respect to two important questions: What are the primary issues, problems, and challenges raised in the use of these tools? And, perhaps even more important: Which of these issues are most commonly faced by ""newbies"" in the MDE community? Our study provides supporting evidence for some commonly held but unproven beliefs, such that plug-ins and documentation issues are the most common, and suggests which issues actually present the biggest ""barriers to entry"" for new users of MDE tools, and how they might be addressed.",0,1,0,model driven development+domain specific modeling+model-driven software development,
754,754,"App genome callback sequencing in android. Recent analysis shows that the callback sequences are of great importance in the analysis of Android applications (apps for short), due to the app's event-driven nature. However, existing works only extract a part of the callback sequences, depending on the need for their specific properties. We propose App Genome sequencing, an automatic fine-grained callback extraction, covering lifecycle and non-lifecycle, inner- and inter-component callback relations, as well as related attributes, including global objects and operations, along the callback sequences. The extracted App Genome facilitates more complete analysis of Android apps, since it contains more callback sequences and data information, than existing works. We use a process algebra called CSP# to represent the App Genome. We implement our method as a tool, which takes an app as input, automatically generates the CSP# model of the App Genome and automatically invokes the model checker to verify a given property.",0,1,0,android+android applications+model checker,
755,755,"Sound and complete monitoring of sequential consistency for relaxed memory models. We present a technique for verifying that a program has no executions violating sequential consistency (SC) when run under the relaxed memory models Total Store Order (TSO) and Partial Store Order (PSO). The technique works by monitoring sequentially consistent executions of a program to detect if similar program executions could fail to be sequentially consistent under TSO or PSO.We propose novel monitoring algorithms that are sound and complete for TSO and PSO--if a program can exhibit an SC violation under TSO or PSO, then the corresponding monitor can detect this on some SC execution. The monitoring algorithms arise naturally from the operational definitions of these relaxed memory models, highlighting an advantage of viewing relaxed memory models operationally rather than axiomatically. We apply our technique to several concurrent data structures and synchronization primitives, detecting a number of violations of sequential consistency.",0,1,0,program execution+sequential consistency+concurrent data structures,
756,756,"A case study on consistency management of business and it process models in banking. Organizations that adopt process modeling often maintain several co-existing models of the same business process. These models target different abstraction levels and stakeholder perspectives. Maintaining consistency among these models has become a major challenge for such organizations. Although several academic works have discussed this challenge, little empirical investigation exists on how people perform process model consistency management in practice. This paper aims to address this lack by presenting an in-depth empirical study of a business-driven engineering process deployed at a large company in the banking sector. We analyzed more than 70 business process models developed by the company, including their change history, with over 1,000 change requests. We also interviewed 9 business and IT practitioners and surveyed 23 such practitioners to understand concrete difficulties in consistency management, the rationales for the specification-to-implementation refinements found in the models, strategies that the practitioners use to detect and fix inconsistencies, and how tools could help with these tasks. Our contribution is a set of eight empirical findings, some of which confirm or contradict previous works on process model consistency management found in the literature. The findings provide empirical evidence of (1) how business process models are created and maintained, including a set of recurrent patterns used to refine business-level process specifications into IT-level models; (2) what types of inconsistencies occur; how they are introduced; and what problems they cause; and (3) what stakeholders expect from tools to support consistency management.",0,1,0,process modeling+bpm+engineering,
757,757,"Avoiding finding and fixing spreadsheet errors a survey of automated approaches for spreadsheet qa. Spreadsheet programs can be found everywhere in organizations and they are used for a variety of purposes, including financial calculations, planning, data aggregation and decision making tasks. A number of research surveys have however shown that such programs are particularly prone to errors. Some reasons for the error-proneness of spreadsheets are that spreadsheets are developed by end users and that standard software quality assurance processes are mostly not applied. Correspondingly, during the last two decades, researchers have proposed a number of techniques and automated tools aimed at supporting the end user in the development of error-free spreadsheets. In this paper, we provide a review of the research literature and develop a classification of automated spreadsheet quality assurance (QA) approaches, which range from spreadsheet visualization, static analysis and quality reports, over testing and support to model-based spreadsheet development. Based on this review, we outline possible opportunities for future work in the area of automated spreadsheet QA.",0,1,0,software quality assurance+static analysis+software quality,
758,758,"Software engineering for data analytics. We are at an inflection point where software engineering meets the data-centric world of big data, machine learning, and artificial intelligence. In this article, I summarize findings from studies of professional data scientists and discuss my perspectives on open research problems to improve data-centric software development.",0,1,0,software development+software engineering+software reengineering,
759,759,"Insights into the perceived benefits of kanban in software companies practitioners views. In the last decade, Kanban has been promoted as a means for bringing visibility to work while improving the software development flow, team communication and collaboration. However, little empirical evidence exists regarding Kanban use in the software industry. This paper aims to investigate the factors that users perceive to be important for Kanban use. We conducted a survey in 2015 among Kanban practitioners in the LeanKanban LinkedIn community. The survey results consist of 146 responses from 27 different organisations, with all respondents being experienced in using Kanban. The results show that practitioners perceived Kanban as easy to learn and useful in individual and team work. They also consider organisational support and social influence to be important determinants for Kanban use. Respondents noted various perceived benefits for using Kanban, such as bringing visibility to work, helping to reduce work in progress, improving development flow, increasing team communication and facilitating coordination. Despite the benefits, participants also identified challenges to using Kanban, such as organisational support and culture, difficulties in Kanban implementation, lack of training and misunderstanding of key concepts. The paper summarises the results and includes a discussion of implications for effective deployment of Kanban before describing future research needs.",0,1,0,software+software development+software industry,
760,760,"Automated trainability evaluation for smart software functions. More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)---which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs.",0,1,0,machine learning+recommendation+software,
761,761,"Snoring a noise in defect prediction datasets. In order to develop and train defect prediction models, researchers rely on datasets in which a defect is often attributed to a release where the defect itself is discovered. However, in many circumstances, it can happen that a defect is only discovered several releases after its introduction. This might introduce a bias in the dataset, i.e., treating the intermediate releases as defect-free and the latter as defect-prone. We call this phenomenon as ""sleeping defects"". We call ""snoring"" the phenomenon where classes are affected by sleeping defects only, that would be treated as defect-free until the defect is discovered. In this paper we analyze, on data from 282 releases of six open source projects from the Apache ecosystem, the magnitude of the sleeping defects and of the snoring classes. Our results indicate that 1) on all projects, most of the defects in a project slept for more than 20% of the existing releases, and 2) in the majority of the projects the missing rate is more than 25% even if we remove the last 50% of releases.",0,1,0,open source system+software defect prediction+open source projects,
762,762,"Classification model of network intrusion using weighted extreme learning machine. The development of a model classification intrusion detection using Weighted Extreme Learning Machine was examined with KDD'99 data set ad 4 types of main attack : Denial of Service Attack (DoS), User to Root Attack (U2R), Remote to Local Attack (R2L), and Probing Attack, when comparing the effectiveness of working process of the method presented to SVM+GA[6] and ELM, found that weighted technique using RBF Kernel activation function which the value of trade-off constant C was at 25, which was presented the average effectiveness of accuracy to be more effective than other 2 techniques, giving accuracy effectiveness value of DoS = 99.95%, U2R = 99.97%, R2L = 93.64% and Probing = 96.64 %, meanwhile it used less time for working. This could be an interesting technique to be applied to enhance the effectiveness of security of system surveillances in monitoring to be able to remedy the situations on time.",0,1,0,support vector machine+network intrusion detection+intrusion detection,
763,763,"Purity a planning based security testing tool. Despite sophisticated defense mechanisms security testing still plays an important role in software engineering. Because of their latency, security flaws in web applications always bear the risk of being exploited sometimes in the future. In order to avoid potential damage, appropriate prevention measures should be incorporated in time and in the best case already during the software development cycle. In this paper, we contribute to this this goal and present the PURITY tool for testing web applications. PURITY executes test cases against a given website. It detects whether the website is vulnerable against some of the most common vulnerabilities, i.e., SQL injections and cross-site scripting. The goal is to resemble a malicious activity by following typical sequences of actions potentially leading to a vulnerable state. The test execution proceeds automatically. In contrast to other penetration testing tools, PURITY relies on planning. Concrete test cases are obtained from a plan, which in turn is generated from specific initial values and given actions. The latter are intended to mimic actions usually performed by an attacker. In addition, PURITY also allows a tester to configure input parameters and also tests a website in a manual manner.",0,1,0,software engineering+software reengineering+cross site scripting,
764,764,"Multi dimensional assessment of risks in a distributed software development course. The organizational shift from local to global settings in many software development initiatives has triggered the need for entailing it when educating the future software engineers. Several educational institutions have embraced this need and started collaborating for the provision of global software engineering courses. The rather complex nature of such courses results in a wider range of risks, in comparison to standard software engineering courses, that arise in different dimensions, ranging from course-to result-related, and for different reasons. In this work we provide an assessment of such a variety of risks as well as their causes, and we give a hint on how they may affect each other based on our 10-year-long experience with a tightly integrated GSD course.",0,1,0,distributed software development+software engineering education+global software engineering,
765,765,"Expected termination time in bpa games. We consider the problem of computing the value and finding the epsilon-optimal strategies for concurrent Basic Process Algebra games, which is a subclass of two-player infinite-state stochastic games with imperfect information. These games are played on the transition graph of stateless pushdown systems, or equivalently 1-exit recursive state machines, and can model recursive procedural program execution with probabilistic transitions. The objective of one player in these games is to minimise the expected termination time of such a program, while the objective of the other is to maximise it. We show that the quantitative decision questions regarding the value of the game as well as checking whether this value is infinite can be answered in PSPACE. We also show the latter problem to be as hard as the square root sum, whose containment even in the polynomial hierarchy is an open problem since the 1970s. Furthermore, an optimal strategy may require an infinite amount of memory in general, but we show that both player have epsilon-optimal stackless&memoryless strategies (i.e. strategies that do not use memory nor depend on the stack content). Finally, we show how to find such strategies using a strategy improvement algorithm.",0,1,0,stochastic game+pushdown systems+program execution,
766,766,"Automated api migration in a user extensible refactoring tool for erlang programs. Wrangler is a refactoring and code inspection tool for Erlang programs. Apart from providing a set of built-in refactorings and code inspection functionalities, Wrangler allows users to define refactorings, code inspections, and general program transformations for themselves to suit their particular needs. These are defined using a template- and rule-based program transformation and analysis framework built into Wrangler.     This paper reports an extension to Wrangler's extension framework, supporting the automatic generation of API migration refactorings from a user-defined adapter module.",0,1,0,software evolution+refactorings+refactoring tools,
767,767,"Seed an easy to use random generator of recursive data structures for testing. Random testing represents a simple and tractable way for software assessment. This paper presents the Seed tool that can be used for the uniform random generation of recursive data structures such as labelled trees and logical formulas. We show how Seed can be used in several testing contexts, from model based testing to performance testing. Generated data structures are defined by grammar-like rules, given in an XML format, multiplying Seed possible applications. Seed is based on combinatorial techniques, and can generate uniformly at random k structures of size n with an efficient time complexity. Finally, Seed is available as a free Java application and a great effort has been made to make it easy-to-use.",0,1,0,java+java applications+model based testing,
768,768,"An rbm anomaly detector for the cloud. Failures are unavoidable in complex software systems, and the intrinsic characteristics of cloud systems amplify the problem. Predicting failures before their occurrence by detecting anomalies in system metrics is a viable solution to enable failure preventing or mitigating actions. The most promising approaches for predicting failures exploit statistical analysis or machine learning to reveal anomalies and their correlation with possible failures. Statistical analysis approaches result in far too many false positives, which severely hinder their practical applicability, while accurate machine learning approaches need extensive training with seeded faults, which is often impossible in operative cloud systems. In this paper, we propose EmBeD, Energy-Based anomaly Detection in the cloud, an approach to detect anomalies at runtime based on the free energy of a Restricted Boltzmann Machine (RBM) model. The free energy is a stochastic function that can be used to efficiently score anomalies for detecting outliers. EmBeD analyzes the system behavior from raw metric data, does not require extensive training with seeded faults, and classifies the relation of anomalous behaviors with future failures with very few false positives. The experimental results presented in this paper confirm that EmBeD can precisely predict failure-prone behavior without training with seeded faults, thus overcoming the main limitations of current approaches.",0,1,0,permanent faults+complex software systems+software systems,
769,769,"Getting rid of store buffers in tso analysis. We propose an approach for reducing the TSO reachability analysis of concurrent programs to their SC reachability analysis, under some conditions on the explored behaviors. First, we propose a linear code-to-code translation that takes as input a concurrent program P and produces a concurrent program P′ such that, running P′ under SC yields the same set of reachable (shared) states as running P under TSO with at most k context-switches for each thread, for a fixed k. Basically, we show that it is possible to use only O(k) additional copies of the shared variables of P as local variables to simulate the store buffers, even if they are unbounded. Furthermore, we show that our translation can be extended so that an unbounded number of context-switches is possible, under the condition that each write operation sent to the store buffer stays there for at most k contextswitches of the thread. Experimental results show that bugs due to TSO can be detected with small bounds, using off-the-shelf SC analysis tools.",0,1,0,reachability analysis+sequential programs+concurrent program,
770,770,"Reporting experiments to satisfy professionals information needs. Although the aim of empirical software engineering is to provide evidence for selecting the appropriate technology, it appears that there is a lack of recognition of this work in industry. Results from empirical research only rarely seem to find their way to company decision makers. If information relevant for software managers is provided in reports on experiments, such reports can be considered as a source of information for them when they are faced with making decisions about the selection of software engineering technologies. To bridge this communication gap between researchers and professionals, we propose characterizing the information needs of software managers in order to show empirical software engineering researchers which information is relevant for decision-making and thus enable them to make this information available. We empirically investigated decision makers' information needs to identify which information they need to judge the appropriateness and impact of a software technology. We empirically developed a model that characterizes these needs. To ensure that researchers provide relevant information when reporting results from experiments, we extended existing reporting guidelines accordingly. We performed an experiment to evaluate our model with regard to its effectiveness. Software managers who read an experiment report according to the proposed model judged the technology's appropriateness significantly better than those reading a report about the same experiment that did not explicitly address their information needs. Our research shows that information regarding a technology, the context in which it is supposed to work, and most importantly, the impact of this technology on development costs and schedule as well as on product quality is crucial for decision makers.",0,1,0,software+software engineering+software reengineering,
771,771,"Software improvement with gin a case study. We provide a case study for the usage of Gin, a genetic improvement toolbox for Java. In particular, we implemented a simple GP search and targeted two software optimisation properties: runtime and repair. We ran our search algorithm on Gson, a Java library for converting Java objects to JSON and vice-versa. We report on runtime improvements and fixes found. We provide all the new code and data on the dedicated website: https://github.com/justynapt/ssbseChallenge2019.",0,1,0,software+java+java virtual machines,
772,772,"Schemaanalyst search based test data generation for relational database schemas. Data stored in relational databases plays a vital role in many aspects of society. When this data is incorrect, the services that depend on it may be compromised. The database schema is the artefact responsible for maintaining the integrity of stored data. Because of its critical function, the proper testing of the database schema is a task of great importance. Employing a search-based approach to generate high-quality test data for database schemas, SchemaAnalyst is a tool that supports testing this key software component. This presented tool is extensible and includes both an evaluation framework for assessing the quality of the generated tests and full-featured documentation. In addition to describing the design and implementation of SchemaAnalyst and overviewing its efficiency and effectiveness, this paper coincides with the tool's public release, thereby enhancing practitioners' ability to test relational database schemas.",0,1,0,software component+software+test data generation,
773,773,"The rise and evolution of agile software development. Agile software development has dominated the second half of the past 50 years of software engineering. Retrospectives, one of the most common agile practices, enables reflection on past performance, discussion of current progress, and charting forth directions for future improvement. Because of agile’s burgeoning popularity as the software development model of choice and a significant research subdomain of software engineering, it demands a retrospective of its own. This article provides a historical overview of agile’s main focus areas and a holistic synthesis of its trends, their evolution over the past two decades, agile’s current status, and, forecast from these, agile’s likely future. This article is part of a theme issue on software engineering’s 50th anniversary.",0,1,0,software reengineering+agile software development+agile practices,
774,774,"Using psycho physiological measures to assess task difficulty in software development. Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem --- detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code.     In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99% precision and 64.58% recall, and for a new task with 84.38% precision and 69.79% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools.",0,1,0,software+software developer+software development,
775,775,"An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods. Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessment methods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience. The paper is revised based on previous reviewers comments.Literature is reviewed to identify best practices related to SPA methods.A set of 38 best practices has been collected and classified.Classes include SPA methods, support tools, procedures, documentation, and users.The collected set is of utmost importance in designing lightweight SPA methods.",0,1,0,software project+software process improvement+software process modeling+software process assessment,
776,776,"Runtime verification of ltl based declarative process models. Linear Temporal Logic (LTL) on finite traces has proven to be a good basis for the analysis and enactment of flexible constraint-based business processes. The Declare language and system benefit from this basis. Moreover, LTL-based languages like Declare can also be used for runtime verification. As there are often many interacting constraints, it is important to keep track of individual constraints and combinations of potentially conflicting constraints . In this paper, we operationalize the notion of conflicting constraints and demonstrate how innovative automata-based techniques can be applied to monitor running process instances. Conflicting constraints are detected immediately and our toolset (realized using Declare and ProM) provides meaningful diagnostics.",0,1,0,linear temporal logic+linear time temporal logic+run-time verification,
777,777,"An integrated model based tool chain for managing variability in complex system design. Software-intensive systems in the automotive domain are often built in different variants, notably in order to support different market segments and legislation regions. Model-based concepts are frequently applied to manage complexity in such variable systems. However, the considered approaches are often focused on single-product development. In order to support variable products in a model-based systems engineering environment, we describe a tool-supported approach that allows us to annotate SysML models with variability data. Such variability information is exchanged between the system modeling tool and variability management tools through the Variability Exchange Language. The contribution of the paper includes the introduction of the model-based product line engineering tool chain and its application on a practical case study at Volvo Construction Equipment. Initial results suggest an improved efficiency in developing such a variable system.",0,1,0,sysml+software product line engineerings+variability management,
778,778,"Oink an implementation and evaluation of modern parity game solvers. Parity games have important practical applications in formal verification and synthesis, especially to solve the model-checking problem of the modal mu-calculus. They are also interesting from the theory perspective, as they are widely believed to admit a polynomial solution, but so far no such algorithm is known. In recent years, a number of new algorithms and improvements to existing algorithms have been proposed. We implement a new and easy to extend tool Oink, which is a high-performance implementation of modern parity game algorithms. We further present a comprehensive empirical evaluation of modern parity game algorithms and solvers, both on real world benchmarks and randomly generated games. Our experiments show that our new tool Oink outperforms the current state-of-the-art.",0,1,0,model checking problem+verification+formal verifications,
779,779,"On the effects of programming and testing skills on external quality and productivity in a test driven development context. Background: In previous studies, a model was proposed that investigated how the developers' unit testing effort impacted their productivity as well as the external quality of the software they developed.   Goal: The aim of this study is to enhance the proposed model by considering two additional factors related to the expertise of developers: programming and unit testing skills. The possibility of including such skills in a model that represents the relationship that testing effort has with the developer's productivity and the product's external quality was investigated.   Method: Data collected from a test-first development task in academic setting was used in order to gauge the relationship between testing effort, external quality, and productivity. Furthermore, Analysis of Covariance (ANCOVA) was utilized to check the impact of developers' skills on productivity and quality.   Result: The results obtained in previous studies were confirmed: there exists a positive effect of testing effort on productivity, but not on quality. Moreover, the developers' skills have an impact on productivity but none on external quality.   Conclusion: Productivity improves with testing effort, a result consistent across previous, similar studies. The role of existing skills is a relevant factor in studying the effects of developers' unit testing effort on productivity. Nevertheless, more investigations are needed regarding the relationship between unit testing effort and external quality.",0,1,0,system testing+web developers+software developer,
780,780,"Toward the use of blog articles as a source of evidence for software engineering research. Background: Blog articles have potential value as a source of practitioner-generated evidence to complement already accepted sources of evidence in software engineering research e.g. interviews and surveys. To be valuable to research, a method for extracting the high quality articles from the vast quantity available needs to be developed. Objective: To better define the benefits and challenges, scope the problem, develop a set of criteria for evaluating blog articles to be used in the method, and propose research questions. Method: We conducted a two-phase pilot study, using a preliminary set of criteria, to explore the challenges of classifying blog articles. We analyse credibility criteria that have been used in previous research, and cross reference those criteria with previous research in evidence-based software engineering. Results: Based on our analysis, we decide that blog articles need to be rigorous, relevant, well written and experience based for them to be considered credible to researchers. Conclusion: Our work provides an overview of the problem domain, as well as presenting criteria and suggested measurements for these criteria. These can be used by others to find blog articles of potential value to their research.",0,1,0,software reengineering+engineering research+requirements engineering,
781,781,"Large scale agile transformation a case study of transforming business development and operations. Today, product development organizations are adopting agile methods in units outside the software development unit, such as in sales, market, legal, operations working with the customer. This broader adoption of agile methods has been labeled large-scale agile transformation and is considered a particular type of organizational change, originating in the software development units. So far, there is little research-based advice on conducting such transformations. Aiming to contribute towards providing relevant research advice on large-scale agile transformation, we apply a research-based framework for evaluating organizational agility on a product development program in a maritime service provider organization. We found that doing a large-scale agile transformation involves many significant challenges, such as having a shared understanding of the problem, getting access to users, and getting commitment to change that needs to be done. In order to overcome such challenges, we discuss the need for a holistic and integrated approach to agile transformation involving all the units linked to software development.",0,1,0,software+agile methods+software development,
782,782,"Bridging proprietary modelling and open source model management tools the case of ptc integrity modeller and epsilon. While the majority of research on Model-Based Software Engineering revolves around open-source modelling frameworks such as the Eclipse Modelling Framework, the use of commercial and closed-source modelling tools such as RSA, Rhapsody, MagicDraw and Enterprise Architect appears to be the norm in industry at present. This technical gap can prohibit industrial users from reaping the benefits of state-of-the-art research-based tools in their practice. In this paper, we discuss an attempt to bridge a proprietary UML modelling tool (PTC Integrity Modeller), which is used for model-based development of safety-critical systems at Rolls-Royce, with an open-source family of languages for automated model management (Epsilon). We present the architecture of our solution, the challenges we encountered in developing it, and a performance comparison against the tool’s built-in scripting interface. In addition, we use the bridge in a real-world industrial case study that involves the coordination with other bridges between proprietary tools and Epsilon.",0,1,0,software engineering+software reengineering+uml modeling,
783,783,"Open issues in reliability safety and efficiency of connected health. This paper reviews the state of the art on several issues in connected health networks and devices, such as testing and verification of these systems, security and privacy issues, efficiency and analysis of connected health. Then several open issues and challenges in the area are presented. Due to the increasing interest of medical and health related devices and networks, a review that summarizes the state of the art on the subject will be helpful to the community.",0,1,0,privacy concerns+privacy+verification,
784,784,"Software testing software quality and trust in software based systems. In our daily life we increasingly depend on software-based systems deployed as embedded software control systems in the automotive domain, or the numerous health or government applications. Software-based systems are more and more developed by reusable components available as commercial off-the-shelf components or open source components. The successful introduction of such integrated systems into businesses however does depend whether we trust the system or not. Trust and therewith the quality of software-based systems is determined by many properties such as completeness, consistency, maintainability, security, safety, reliability, and usability, among others. However during the development of software-based systems there are many opportunities to introduce errors in the different phases of the software development lifecycle. Testing is commonly applied as the predominant activity in industry to ensure high software quality providing a wide variety of methods and techniques to detect different types of errors in software-based systems.",0,1,0,software development+software quality+embedded software,
785,785,"Interactive code review for systematic changes. Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes---similar, related changes to multiple contexts. We present C ritics , an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, C ritics  allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated C ritics  using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used C ritics  to investigate diff patches authored by their own team. After using C ritics , all six participants indicated that they would like C ritics  to be integrated into their current code review environment. This also attests to the fact that C ritics  scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using C ritics  and Eclipse diff. The results show that human subjects using C ritics  answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that C ritics  should improve developer productivity in inspecting systematic changes during peer code reviews.",0,1,0,engineers+code generation+software developer,
786,786,"Crowdsourcing user reviews to support the evolution of mobile apps. Abstract In recent software development and distribution scenarios, app stores are playing a major role, especially for mobile apps. On one hand, app stores allow continuous releases of app updates. On the other hand, they have become the premier point of interaction between app providers and users. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we empirically investigate—by performing a study on the evolution of 100 open source Android apps and by surveying 73 developers—to what extent app developers take user reviews into account, and whether addressing them contributes to apps’ success in terms of ratings. In order to perform the study, as well as to provide a monitoring mechanism for developers and project managers, we devised an approach, named CRISTAL , for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results of our study indicate that (i) on average, half of the informative reviews are addressed, and over 75% of the interviewed developers claimed to take them into account often or very often, and that (ii) developers implementing user reviews are rewarded in terms of significantly increased user ratings.",0,1,0,source codes+software+software development,
787,787,"Cooperative clustering for software modularization. Clustering is a useful technique to group data entities. Many different algorithms have been proposed for software clustering. To combine the strengths of various algorithms, researchers have suggested the use of Consensus Based Techniques (CBTs), where more than one actors (e.g. algorithms) work together to achieve a common goal. Although the use of CBTs has been explored in various disciplines, no work has been done for modularizing software. In this paper, the main research question we investigate is whether the Cooperative Clustering Technique (CCT), a type of CBT, can improve software modularization results. The main contributions of this paper are as follows. First, we propose our CCT in which more than one similarity measure cooperates during the hierarchical clustering process. To this end, we present an analysis of well-known measures. Second, we present a cooperative clustering approach for two types of well-known agglomerative hierarchical software clustering algorithms, for binary as well as non-binary features. Third, to evaluate our proposed CCT, we conduct modularization experiments on five software systems. Our analysis identifies certain cases that reveal weaknesses of the individual similarity measures. The experimental results support our hypothesis that these weaknesses may be overcome by using more than one measure, as our CCT produces better modularization results for test systems in which these cases occur. We conclude that CCTs are capable of showing significant improvement over individual clustering algorithms for software modularization.",0,1,0,non-binary+software+software systems,
788,788,"Remi defect prediction for efficient api testing. Quality assurance for common APIs is important since the the reliability of APIs affects the quality of other systems using the APIs. Testing is a common practice to ensure the quality of APIs, but it is a challenging and laborious task especially for industrial projects. Due to a large number of APIs with tight time constraints and limited resources, it is hard to write enough test cases for all APIs. To address these challenges, we present a novel technique, REMI that predicts high risk APIs in terms of producing potential bugs. REMI allows developers to write more test cases for the high risk APIs. We evaluate REMI on a real-world industrial project, Tizen-wearable, and apply REMI to the API development process at Samsung Electronics. Our evaluation results show that REMI predicts the bug-prone APIs with reasonable accuracy (0.681 f-measure on average). The results also show that applying REMI to the Tizen-wearable development process increases the number of bugs detected, and reduces the resources required for executing test cases.",0,1,0,system testing+software bug+software defects,
789,789,"A test scenario generation method for high requirement coverage by using kaos method. To achieve high traceability from requirements to test scenarios, a test scenario generation method has been widely recognized as being an integral aspect of model-based development. In this paper, we propose a test scenario generation method from formalized functional requirements which employs the KAOS-based approach with the fluent proposition. Based on safety requirements, this technique is able to generate the scenario that reveals hazardous situations by specifying a hazard state as the end state of a scenario. By using exhaustive pathfinding approach, our results show the ability to achieve the high requirement coverage compared to existing approaches.",0,1,0,functional requirement+goal-oriented requirements engineering+requirements models,
790,790,"Improving software performance and reliability in a distributed and concurrent environment with an architecture based self adaptive framework. We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.",0,1,0,concurrent systems+software architecture patterns+software systems,
791,791,"Does quality of requirements specifications matter combined results of two empirical studies. [Background] Requirements Engineering is crucial for project success, and to this end, many measures for quality assurance of the software requirements specification (SRS) have been proposed. [Goal] However, we still need an empirical understanding on the extent to which SRS are created and used in practice, as well as the degree to which the quality of an SRS matters to subsequent development activities. [Method] We studied the relevance of SRS by relying on survey research and explored the impact of quality defects in SRS by relying on a controlled experiment. [Results] Our results suggest that the relevance of SRS quality depends both on particular project characteristics and what is considered as a quality defect; for instance, the domain of safety critical systems seems to motivate for an intense usage of SRS as a means for communication whereas defects hampering the pragmatic quality do not seem to be relevant as initially thought. [Conclusion] Efficient and effective quality assurance measures must be specific for carefully characterized contexts and carefully select defect classes.",0,1,0,requirements engineering+software requirements specifications+requirements specifications,
792,792,"Research preview supporting end user requirements elicitation using product line variability models. [Context and motivation] Product line variability models have been primarily used for product configuration purposes. We suggest that such models contain information that is relevant for early software engineering activities too. [Question/Problem] So far, the knowledge contained in variability models has not been used to improve requirements elicitation activities. State-of-the-art requirements elicitation approaches furthermore do not focus on the cost-effective identification of individual end-user needs, which, for example, is highly relevant for the customization of service-oriented systems. [Principal idea/results] The planned research will investigate how end-users can be empowered to document their individual needs themselves. We propose a tentative solution which facilitates end-users requirements elicitation by providing contextual information codified in software product line variability models. [Contribution] We present the idea of a ""smart"" tool for end-users allowing them to specify their needs and to customize, for example, a service-oriented system based on contextual information in variability models.",0,1,0,variability modeling+product lines+software product line,
793,793,"Regiondroid a tool for detecting android application repackaging based on runtime ui region features. With the rapid development of mobile devices, Android applications (apps) are universally used. However, attackers repackage Android apps and release them to the markets for illegal purposes, which brings great threats to the Android ecosystem. To leverage the popularity of original apps, they keep similar software behaviors to confuse app users. Furthermore, repackaged apps can be obfuscated or encrypted to avoid being detected. Besides, hybrid mobile apps, built by combining web technology and native elements, are becoming a preferred choice for developers. The structure of hybrid apps differs a lot from that of native apps which would raise great challenges to repackaging detection. Existing works still have some limitations in detecting repackaging from obfuscated and encrypted apps. Besides, few of them can deal with hybrid apps. In this paper, we proposed an approach based on the app UI regions extracted from app's runtime UI traces. We also implement a tool named RegionDroid based on the approach. We apply RegionDroid to tree datasets with totally 369 apps. It successfully finds all the 98 obfuscated or encrypted repackaged pairs in dataset S1. It also shows good credibility in distinguishing another 114 commercial apps in dataset S2. We also test our approach in dataset S3 with 157 hybrid apps by comparing them pairwisely and the false positive rate is 0.016%.",0,1,0,android+android applications+software,
794,794,"Rapid multi purpose multi commit code analysis. Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.",0,1,0,software project+software maintenance+software evolution,
795,795,"Status report on software testing test comp 2021. This report describes Test-Comp 2021, the 3rd edition of the Competition on Software Testing. The competition is a series of annual comparative evaluations of fully automatic software test generators for C programs. The competition has a strong focus on reproducibility of its results and its main goal is to provide an overview of the current state of the art in the area of automatic test-generation. The competition was based on 3 173 test-generation tasks for C programs. Each test-generation task consisted of a program and a test specification (error coverage, branch coverage). Test-Comp 2021 had 11 participating test generators from 6 countries.",0,1,0,automatic test generation+c programs+test generations,
796,796,"Tipmerge recommending developers for merging branches. Development in large projects often involves branches, where changes are performed in parallel and merged periodically. This merge process often combines two independent and long sequences of commits that may have been performed by multiple, different developers. It is nontrivial to identify the right developer to perform the merge, as the developer must have enough understanding of changes in both branches to ensure that the merged changes comply with the objective of both lines of work (branches), which may have been active for several months. We designed and developed TIPMerge, a novel tool that recommends developers who are best suited to perform the merge between two given branches. TIPMerge does so by taking into consideration developers’ past experience in the project, their changes in the branches, and the dependencies among modified files in the branches. In this paper we demonstrate TIPMerge over a real merge case from the Voldemort project.",0,1,0,parallel processing+web developers+software developer,
797,797,"Measurable concepts for the usability of software components. While usability has proven to be an important software quality attribute, its application to APIs is still rather uncommon. Available methods for measuring software usability show significant disadvantages when applied to APIs, like the need for test users and experienced evaluators. This makes it difficult to evaluate the usability of software components, as well as to compare different software components. An API usability measurement method is needed that is both machine-computable and objective. This paper takes a first step in the direction of such a measure by identifying measurable concepts for the usability of software components, and validating these concepts against existing studies and guidelines for usability and API design.",0,1,0,software+quality attributes+software quality,
798,798,"Measuring the intelligence of an idealized mechanical knowing agent. We define a notion of the intelligence level of an idealized mechanical knowing agent. This is motivated by efforts within artificial intelligence research to define real-number intelligence levels of complicated intelligent systems. Our agents are more idealized, which allows us to define a much simpler measure of intelligence level for them. In short, we define the intelligence level of a mechanical knowing agent to be the supremum of the computable ordinals that have codes the agent knows to be codes of computable ordinals. We prove that if one agent knows certain things about another agent, then the former necessarily has a higher intelligence level than the latter. This allows our intelligence notion to serve as a stepping stone to obtain results which, by themselves, are not stated in terms of our intelligence notion (results of potential interest even to readers totally skeptical that our notion correctly captures intelligence). As an application, we argue that these results comprise evidence against the possibility of intelligence explosion (that is, the notion that sufficiently intelligent machines will eventually be capable of designing even more intelligent machines, which can then design even more intelligent machines, and so on).",0,1,0,software agents+artificial intelligence+intelligent systems,
799,799,"Verdict machinery on the need to automatically make sense of test results. Along with technological developments and increasing competition there is a major incentive for companies to produce and market high quality products before their competitors. In order to conquer a bigger portion of the market share, companies have to ensure the quality of the product in a shorter time frame. To accomplish this task companies try to automate their test processes as much as possible. It is critical to investigate and understand the problems that occur during different stages of test automation processes. In this paper we report on a case study on automatic analysis of non-functional test results. We discuss challenges in the face of continuous integration and deployment and provide improvement suggestions based on interviews at a large company in Sweden. The key contributions of this work are filling the knowledge gap in research about performance regression test analysis automation and providing warning signs and a road map for the industry.",0,1,0,automation+industrial automation+regression testing,
800,800,Predator a shape analyzer based on symbolic memory graphs. Predator is a shape analyzer that uses the abstract domain of symbolic memory graphs in order to support various forms of low-level memory manipulation commonly used in optimized C code. This paper briefly describes the verification approach taken by Predator and its strengths and weaknesses revealed during its participation in the Software Verification Competition (SV-COMP’14).,0,1,0,abstract domains+software+software verification,
801,801,"Fixpoint computation in the polyhedra abstract domain using convex and numerical analysis tools. Polyhedra abstract domain is one of the most expressive and used abstract domains for the static analysis of programs. Together with Kleene algorithm, it computes precise yet costly program invariants. Widening operators speed up this computation and guarantee its termination, but they often induce a loss of precision, especially for numerical programs. In this article, we present a process to accelerate Kleene iteration with a good trade-off between precision and computation time. For that, we use two tools: convex analysis to express the convergence of convex sets using support functions, and numerical analysis to accelerate this convergence applying sequence transformations. We demonstrate the efficiency of our method on benchmarks.",0,1,0,numerical methods+abstract domains+static analysis,
802,802,"Decentralized self adaptive computing at the edge. Nowadays, computing infrastructures are usually deployed in fully controlled environments and managed in a centralized fashion. Leveraging on centralized infrastructures prevent the system to deal with scalability and performance issues, which are inherent to modern large-scale data-intensive applications. On the other hand, we envision fully decentralized computing infrastructures deployed at the edge of the network providing the required support for operating data-intensive systems. However, engineering such systems raises many challenges, as decentralization introduces uncertainty, which in turn may harm the dependability of the system. To this end, self-adaptation is a key approach to manage uncertainties at runtime and satisfy the requirements of decentralized data-intensive systems. This paper shows the research directions and current contributions towards this vision by (i) evaluating the impact of the distribution of computational entities, (ii) engineering decentralized computing through self-adaptation and, (iii) evaluating decentralized and self-adaptive applications.",0,1,0,cloud infrastructures+dependability+engineering,
803,803,"Understanding android app piggybacking. The Android packaging model offers adequate opportunities for attackers to inject malicious code into popular benign apps, attempting to develop new malicious apps that can then be easily spread to a large user base. Despite the fact that the literature has already presented a number of tools to detect piggybacked apps, there is still lacking a comprehensive investigation on the piggybacking processes. To fill this gap, in this work, we collect a large set of benign/piggybacked app pairs that can be taken as benchmark apps for further investigation. We manually look into these benchmark pairs for understanding the characteristics of piggybacking apps and eventually we report 20 interesting findings. We expect these findings to initiate new research directions such as practical and scalable piggybacked app detection, explainable malware detection, and malicious code location.",0,1,0,iphone+malwares+android,
804,804,"Using knowledge elicitation to improve web effort estimation lessons from six industrial case studies. This paper details our experience building and validating six different expert-based Web effort estimation models for ICT companies in New Zealand and Brazil. All models were created using Bayesian networks, via eliciting knowledge from domain experts, and validated using data from past finished projects. Post-mortem interviews with the participating companies showed that they found the entire process extremely beneficial and worthwhile, and that all the models created remained in use by those companies.",0,1,0,information and communication technologies+domain knowledge+ict,
805,805,"Psi exact symbolic inference for probabilistic programs. Probabilistic inference is a key mechanism for reasoning about probabilistic programs. Since exact inference is theoretically expensive, most probabilistic inference systems today have adopted approximate inference techniques, which trade precision for better performance (but often without guarantees). As a result, while desirable for its ultimate precision, the practical effectiveness of exact inference for probabilistic programs is mostly unknown.",0,1,0,inference+probabilistic inference+reasoning,
806,806,"Autoconfig automatic configuration tuning for distributed message systems. Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.",0,1,0,real time streaming+prediction modes+cloud services,
807,807,"Ethical issues in empirical studies using student subjects re visiting practices and perceptions. Using student subjects in empirical studies has been discussed extensively from a methodological perspective in Software Engineering (SE), but there is a lack of similar discussion surrounding ethical aspects of doing so. As students are in a subordinate relationship to their instructors, such a discussion is needed. We aim to increase the understanding of practices and perceptions SE researchers have of ethical issues with student participation in empirical studies. We conducted a systematic mapping study of 372 empirical SE studies involving students, following up with a survey answered by 100 SE researchers regarding their current practices and opinions regarding student participation. The mapping study shows that the majority of studies does not report conditions regarding recruitment, voluntariness, compensation, and ethics approval. In contrast, the majority of survey participants supports reporting these conditions. The survey further reveals that less than half of the participants require ethics approval. Additionally, the majority of participants recruit their own students on a voluntary basis, and use informed consent with withdrawal options. There is disagreement among the participants whether course instructors should be involved in research studies and if they should know who participates in a study. It is a positive sign that mandatory participation is rare, and that informed consent and withdrawal options are standard. However, we see immediate need for action, as study conditions are under-reported, and as opinions on ethical practices differ widely. In particular, there is little regard in SE on the power relationship between instructors and students.",0,1,0,software+software engineering+software reengineering,
808,808,"Towards automating dynamic analysis for behavioral design pattern detection. The detection of behavioral design patterns is more accurate when a dynamic analysis is performed on the candidate instances identified statically. Such a dynamic analysis requires the monitoring of the candidate instances at run-time through the execution of a set of test cases. However, the definition of such test cases is a time-consuming task if performed manually, even more, when the number of candidate instances is high and they include many false positives. In this paper we present the results of an empirical study aiming at assessing the effectiveness of dynamic analysis based on automatically generated test cases in behavioral design pattern detection. The study considered three behavioral design patterns, namely State, Strategy, and Observer, and three publicly available software systems, namely JHotDraw 5.1, QuickUML 2001, and MapperXML 1.9.7. The results show that dynamic analysis based on automatically generated test cases improves the precision of design pattern detection tools based on static analysis only. As expected, this improvement in precision is achieved at the expenses of recall, so we also compared the results achieved with automatically generated test cases with the more expensive but also more accurate results achieved with manually built test cases. The results of this analysis allowed us to highlight costs and benefits of automating dynamic analysis for design pattern detection.",0,1,0,dynamic analysis+software systems+static analysis,
809,809,"An empirical investigation on software practices in growth phase startups. Context: Software startups are software-intensive early-stage companies with high growth rates. We notice little evidence in the literature concerning engineering practices when startups transition to the growth phase. Aim: Our goal is to evaluate how software startups embrace software engineering practices. Methodology: We conduct a survey guided by semi-structured interviews as an initial step, to be followed by field questionnaires as part of a future exploratory study. We use open coding to identify patterns leading to themes we use to state our hypotheses. To identify our samples, we use purposive sampling. Results: Specifically, we analyze seven startup cases during the first qualitative phase. We obtain five anti-patterns (no-documentation, no-agile, no-code intellectual property protection, cowboy programming, no-automated testing) and corresponding patterns (readable code, ad-hoc project management, private code repositories, paired and individual programming, ad-hoc testing) in adopting software engineering practices. We state 10 corresponding hypotheses we intend to corroborate by surveying a more significant number of software startups. Contribution: This study, throughout its recommendations, provides an initial road map for software startups in the growth phase, allowing future researchers and practitioners to make educated recommendations.",0,1,0,software+software engineering+software reengineering,
810,810,"An approach to t way test sequence generation with constraints. In this paper we address the problem of constraint handling in t-way test sequence generation. We develop a notation for specifying sequencing constraints and present a t-way test sequence generation that handles the constraints specified in this notation. We report a case study in which we applied our notation and test generation algorithm to a real-life communication protocol. Our experience indicates that our notation is intuitive to use and allows us to express important sequencing constraints for the protocol. However, the test generation algorithm takes a significant amount of time. This work is part of our larger effort to make t-way sequence testing practically useful.",0,1,0,communication protocols+test generations+automatic test pattern generation,
811,811,"Task recommendation with developer social network in software crowdsourcing. Recently, crowdsourcing has been increasingly used in software industry to lower costs and increase innovations, by utilizing experiences, labor, or creativity of developers worldwide. In software crowdsourcing platforms, developers expect to find suitable tasks for their interests and abilities. So it is significant for software crowdsourcing to build a recommender system to match developers with suitable tasks. However, there are a significant number of inactive developers who have very sparse historical behavior records in the platform, and thus state-of-the-art recommendation approaches in software crowdsourcing, such as collaborative filtering, suffer from this cold-start problem. In this paper, a social influence-based method is proposed to recommend suitable tasks for both active and inactive developers. The essential idea of the novel method is (1) to construct developer social network from developer behaviors, such as browsing and bidding for tasks, (2) to calculate the influence degrees between developers using developer social network, (3) to recommend tasks to active developers using SiSVD, and (4) to recommend tasks to inactive developers by combining the recommended tasks of their friends. We have evaluated our method on a large real data set from the JointForce, a popular software crowdsourcing platform in China. The results show that our method is feasible and practical for recommendation in software crowdsourcing. In particular, the F1-Measure of our method for inactive developers with task-bidding friends is increased by 16.7% than other previous approaches averagely.",0,1,0,software+software developer+software industry,
812,812,"Refactorings without names. As with design patterns before, the naming and cataloguing of refactorings has contributed significantly to the recognition of the discipline. However, in practice concrete refactoring needs may deviate from what has been distilled as a named refactoring, and mapping these needs to a series of such refactorings &#x2014; if at all possible &#x2014; can be difficult. To address this, we propose a framework of specifying refactorings in an ad hoc fashion, and demonstrate its feasibility by presenting an implementation. Evaluation is done by simulating application through a user on a set of given sample programs. Results suggest that our proposal of ad hoc refactoring is, for the investigated scenarios at least, viable.",0,1,0,refactorings+code smell+refactoring tools,
813,813,"Generic and effective specification of structural test objectives. A large amount of research has been carried out to automate white-box testing. While a wide range of different and sometimes heterogeneous code-coverage criteria have been proposed, there exists no generic formalism to describe them all, and available test automation tools usually support only a small subset of them. We introduce a new specification language, called HTOL (Hyperlabel Test Objectives Language), providing a powerful generic mechanism to define a wide range of test objectives. HTOL comes with a formal semantics, and can encode all standard criteria but full mutations. Besides specification, HTOL is appealing in the context of test automation as it allows handling criteria in a unified way.",0,1,0,formal semantics+formal specification+test case generation,
814,814,"An empirical study of internationalization failures in the web. Web application internationalization frameworks allow businesses to more easily market and sell their products and services around the world. However, internationalization can lead to problems. Text expansion and contraction after translation may result in a distortion of the layout of the translated versions of a webpage, which can reduce their usability and aesthetics. In this paper, we investigate and report on the frequency and severity of different types of failures in webpages' user interfaces that are due to internationalization. In our study, we analyzed 449 real world internationalized webpages. Our results showed that internationalization failures occur frequently and they range significantly in terms of severity and impact on the web applications. These findings motivate and guide future work in this area.",0,1,0,user interfaces+ajax+web application,
815,815,"The work life of developers activities switches and perceived productivity. Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day  in situ . Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers’ work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers’ work day and improving their personal productivity.",0,1,0,tool support+software development+software development organizations,
816,816,"Forecast horizon for automated safety actions in automated driving systems. Future Automated Driving Systems (ADS) will ultimately take over all driving responsibilities from the driver. This will as well include the overall safety goal of avoiding hazards on the road by executing automated safety actions (ASA). It is the purpose of this paper to address the general properties of the ASA. One property of particular interest is the forecast horizon (FH) that defines how much in advance a hazard has to be identified in order to ensure the timely execution of an ASA. For the estimation of the FH, we study the fault-tolerant time interval concept defined by the ISO 26262 and extend it for the use case of fail-operational ADS. We then perform a thorough study on all parameters contributing to the FH, assign typical values for each parameter for a running example, and formalize our work by a set of equations. The set of equations are then applied to two specific driving scenarios, and based on the running example values, the FH is estimated. We conclude our work with a summary of the estimated FH for each of the specific driving scenarios at different road conditions and the recommended road speed limits. Such a scientific way of deciding optimal bounds on the FH is essential to ensure the safety of the future autonomous vehicles and can be a major requirement for clearing the regulatory needs on certification.",0,1,0,vehicles+autonomous vehicles+fault-tolerant,
817,817,"Ccfindersw clone detection tool with flexible multilingual tokenization. So far, many tools have been developed for the detection of code clones in source code. The existing clone detection tools support only a limited number of programming languages and do not provide any easy extension mechanism to handle additional language. However, from our experience in industry/university collaboration, we found that many practitioners need to analyze source code written in various languages. In this paper, we propose a clone detection tool CCFinderSW that has extension mechanism to handle addition language on demand from practitioners.",0,1,0,source codes+code clone+clone detection,
818,818,"Towards an operationalization of the physics of notations for the analysis of visual languages. We attempt to validate the conceptual framework ""Physics of Notation"" PoN as a means for analysing visual languages by applying it to UML Use Case Diagrams. We discover that the PoN, in its current form, is neither precise nor comprehensive enough to be applied in an objective way to analyse practical visual software engineering notations. We propose an operationalization of a part of the PoN, highlight conceptual shortcomings of the PoN, and explore ways to address them.",0,1,0,software engineering+software reengineering+epon+wdm-pon,
819,819,"Oo spl modelling of the focused case study. This document overviews an object-oriented (OO) modeling approach and a software product line (SPL) methodology used to model the Barbados Crash Management System Product Line (referred to as bCMS-SPL), as well as for a reference variant of such a product line (referred to as bCMS). The approaches and the modeling languages used have been chosen in order to comply with widely used practices and/or (de facto) standards. The starting point for this effort is a set of requirements described in a brief requirements document [5]. Products of bCMS-SPL are intended to support distributed crisis management by police and fire personnel for automotive accidents on public roadways. While police and fire personnel have complementary responsibilities to be done concurrently, these efforts need to be coordinated in order to ensure efficient and effective management of a given crisis. As such, the bCMS-SPL models focus on the functionality of the Police Station Coordinator (PS coordinator) and the Fire Station Coordinator (FS coordinator) and their interactions. The scope of the bCMS-SPL starts with the notification of a crisis to the PS coordinator and FS coordinator concludes at the point when all fire and police personnel have been released from the given crisis. The assumption is that high-level requirements have been gathered and stated in the form of use cases for the bCMS. The modeling of the bCMS and bCMS-SPL target the late requirements briefly overviews the refinement to system architecture. The OOmodeling approach provides structural context information and behavior information to be used to provide a late requirements specification of the bCMS. The structural context information is captured in terms of a domain model using the class diagram notation, including a data dictionary. The behavior information is described in terms of sequence diagrams that model specific scenarios between the key elements of a system, and interacting state diagrams that describe the behavior of a given element as it behaves across multiple scenarios. In order to create a SPL model for the bCMS-SPL, we use model elements from the bCMS late requirements OO model and the variabilities described in the high-level requirements document [5]. The SPL model comprises a feature diagram and OO model fragments, each of which describes the structural and behavioral",0,1,0,software product line+requirements specifications+requirements document,
820,820,"A special edutainment system based on somatosensory game. Teaching system mode that combines virtual reality technology and electronic entertainment has rapidly developed in recent years. In the virtual environment where is constructed by computers and related electronic equipment, the hearing-impaired people can learn knowledge by playing the somatosensory entertainment games. In this paper, we proposed a special education and entertainment system framework based on somatosensory game. We designed the strategy of somatosensory control and other key technologies of the edutainment system. The experimental results show that the SESS edutainment system can effectively help hearing-impaired children to learn social knowledge and other knowledge in related subjects.",0,1,0,teaching systems+edutainment+education,
821,821,"Is mining software repositories data science keynote. Trick question: what is Data Science? The collection and use of low-veracity data in software repositories and other operational support systems is exploding. It is, therefore, imperative to elucidate basic principles of how such data comes into being and what it means. Are there practices of constructing software data analysis tools that could raise the integrity of their results despite the problematic nature of the underlying data? The talk explores the basic nature of data in operational support systems and considers approaches to develop engineering practices for software mining tools.",0,1,0,engineering+software+software repositories,
822,822,"Stcml an extensible xml based language for socio technical modeling. Understanding the complex dependencies between the technical artifacts of software engineering and the social processes involved in their development has the potential to improve the processes we use to engineer software as well as the eventual quality of the systems we produce. A foundational capability in grounding this study of socio-technical concerns is the ability to explicitly model technical and social artifacts as well as the dependencies between them. This paper presents the STCML language, intended to support the modeling of core socio-technical aspects in software development in a highly extensible fashion. We present the basic structure of the language, discuss important language design principles, and offer an example of its application.",0,1,0,software development+software engineering+software reengineering,
823,823,"Improving dual hop amplify and forward cooperative mobile network based on path selection and stbc with pre coding scheme. This paper presents a path selection algorithm in conjunction with (2,2) Alamouti's space-time block code with pre-coding scheme (STBC-PC) to improve the network performance of a dual-hop amplify-and-forward cooperative mobile network. STBC-PC is applied to this study in order to employ STBC with single antenna which is incorporated in any wireless portable devices, e.g., mobile stations and to achieve high gain. The path selection algorithm is designed to be used at the mobile station where is assumed to have the knowledge of all channel gains, hence it can be designed to maximize the channel capacity and to provide better network performance. The simulation results have been shown that the cooperative transmission with STBC-PC can achieve BER of 10–4 at the level of SNR about 17 dB which is lower level than the cooperative transmission without STBC-PC. Moreover, the proposed method can provide BER of 10–2 at SNR of 3 dB which is lower level the STBC-PC-based cooperative transmission without path selection.",0,1,0,stbc+amplify-and-forward (af)+alamouti+alamouti codes+transmit diversity+orthogonal space-time block codes,
824,824,"Blending design patterns with aspects. HighlightsWe investigated the scalability of AOP for composing GoF design patterns.We study the benefits and drawbacks of AOP for composing GoF design patterns.Category of the pattern is a dominant factor impacting the pattern composability.Programming abstractions of AOP languages also impact the patterns composability. Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classes' core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how AOP models influence the composability of widely used design patterns. This paper investigates the influence of using AOP models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of AOP in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two AOP models, namely, AspectJ and Compose*. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition.",0,1,0,software systems+aspect-oriented programming+crosscutting concern,
825,825,"Recommending automated extract method refactorings. Extract Method is a key refactoring for improving program comprehension. However, recent empirical research shows that refactoring tools designed to automate Extract Methods are often underused. To tackle this issue, we propose a novel approach to identify and rank Extract Method refactoring opportunities that are directly automated by IDE-based refactoring tools. Our approach aims to recommend new methods that hide structural dependencies that are rarely used by the remaining statements in the original method. We conducted an exploratory study to experiment and define the best strategies to compute the dependencies and the similarity measures used by the proposed approach. We also evaluated our approach in a sample of 81 extract method opportunities generated for JUnit and JHotDraw, achieving a precision of 48% (JUnit) and 38% (JHotDraw).",0,1,0,refactorings+change impact analysis+refactoring tools,
826,826,"Program analysis with local policy iteration. We present local policy iterationi¾?LPI, a new algorithm for deriving numerical invariants that combines the precision of max-policy iteration with the flexibility and scalability of conventional Kleene iterations. It is defined in the Configurable Program Analysis CPA framework, thus allowing inter-analysis communication.

LPI uses adjustable-block encoding in order to traverse loop-free program sections, possibly containing branching, without introducing extra abstraction. Our technique operates over any template linear constraint domain, including the interval and octagon domains; templates can also be derived from the program source.

The implementation is evaluated on a set of benchmarks from the International Competition on Software Verification SV-COMP. It competes favorably with state-of-the-art analyzers.",0,1,0,verification techniques+software verification+program analysis,
827,827,"Multi objective optimal test suite computation for software product line pairwise testing. Software Product Lines (SPLs) are families of related software products, which usually provide a large number of feature combinations, a fact that poses a unique set of challenges for software testing. Recently, many SPL testing approaches have been proposed, among them pair wise combinatorial techniques that aim at selecting products to test based on the pairs of feature combinations such products provide. These approaches regard SPL testing as an optimization problem where either coverage (maximize) or test suite size (minimize) are considered as the main optimization objective. Instead, we take a multi-objective view where the two objectives are equally important. In this exploratory paper we propose a zero-one mathematical linear program for solving the multi-objective problem and present an algorithm to compute the true Pareto front, hence an optimal solution, from the feature model of a SPL. The evaluation with 118 feature models revealed an interesting trade-off between reducing the number of constraints in the linear program and the runtime which opens up several venues for future research.",0,1,0,software project+product lines+software product line,
828,828,"Exploring the relationships between the understandability of components in architectural component models and component level metrics. Architectural component models represent high level designs and are frequently used as a central view of architectural descriptions of software systems. The components in those models represent important high level organization units that group other components and classes in object-oriented design views. Hence, understandability of components and their interactions plays a key role in supporting the architectural understanding of a software system. In this paper we present a study we carried out to examine the relationships between the effort required to understand a component, measured through the time that participants spent on studying a component, and component level metrics that describe component's size, complexity and coupling in terms of the number of classes in a component and the classes' relationships. The participants were 49 master students, and they had to fully understand the components' functionalities in order to answer 4 true/false questions for each of the 7 components in the architecture of the Soomla Android store system. Correlation, collinearity and multivariate regression analysis were performed. The results of the analysis show a statistically significant correlation between three of the metrics, number of classes, number of incoming dependencies, and number of internal dependencies, on one side, and the effort required to understand a component, on the other side. In a multivariate regression analysis we obtained 3 reasonably well-fitting models that can be used to estimate the effort required to understand a component. In our future work we plan to study more components and investigate more metrics and their relationships to the understandability of components and architectural component models.",0,1,0,component model+software+software systems,
829,829,"Probabilistic bisimulation for parameterized systems with applications to verifying anonymous protocols. Probabilistic bisimulation is a fundamental notion of process equivalence for probabilistic systems. It has important applications, including the formalisation of the anonymity property of several communication protocols. While there is a large body of work on verifying probabilistic bisimulation for finite systems, the problem is in general undecidable for parameterized systems, i.e., for infinite families of finite systems with an arbitrary number n of processes. In this paper we provide a general framework for reasoning about probabilistic bisimulation for parameterized systems. Our approach is in the spirit of software verification, wherein we encode proof rules for probabilistic bisimulation and use a decidable first-order theory to specify systems and candidate bisimulation relations, which can then be checked automatically against the proof rules.",0,1,0,software+probabilistic systems+software verification,
830,830,"Multi view refactoring of class and activity diagrams using a multi objective evolutionary algorithm. To improve the quality of software systems, one of the widely used techniques is refactoring defined as the process of improving the design of an existing system by changing its internal structure without altering the external behavior. The majority of existing refactoring work focuses mainly on the source code level. The suggestion of refactorings at the model level is more challenging due to the difficulty to evaluate: (a) the impact of the suggested refactorings applied to a diagram on other related diagrams to improve the overall system quality, (b) their feasibility, and (c) interdiagram consistency. We propose, in this paper, a novel framework that enables software designers to apply refactoring at the model level. To this end, we used a multi-objective evolutionary algorithm to find a trade-off between improving the quality of class and activity diagrams. The proposed multi-objective approach provides a multi-view for software designers to evaluate the impact of suggested refactorings applied to class diagrams on related activity diagrams in order to evaluate the overall quality, and check their feasibility and behavior preservation. The statistical evaluation performed on models extracted from four open-source systems confirms the efficiency of our approach.",0,1,0,software evolution+refactorings+refactoring tools,
831,831,"Score a scalable concolic testing tool for reliable embedded software. Current industrial testing practices often generate test cases in a manual manner, which degrades both the effectiveness and efficiency of testing. To alleviate this problem, concolic testing generates test cases that can achieve high coverage in an automated fashion. One main task of concolic testing is to extract symbolic information from a concrete execution of a target program at runtime. Thus, a design decision on how to extract symbolic information affects efficiency, effectiveness, and applicability of concolic testing. We have developed a Scalable COncolic testing tool for REliable embedded software (SCORE) that targets embedded C programs. SCORE instruments a target C program to extract symbolic information and applies concolic testing to a target program in a scalable manner by utilizing a large number of distributed computing nodes. In this paper, we describe our design decisions that are implemented in SCORE and demonstrate the performance of SCORE through the experiments on the SIR benchmarks.",0,1,0,c programs+embedded software+symbolic execution,
832,832,"Toward el dorado for cloud computing lightweight vms containers meta containers and oracles. Cloud computing offers glimpses of the straight road toward hypothetical El Dorado for distributed and grid computing, but the state of the art is currently unclear and the road is anything but straight. The goal of this paper is to dive deep into technologies and techniques underpinning cloud computing and try to determine the viability of concepts like meta-containers, lightweight VMs, DIME network cognitive architecture, Straight Road, and Mesos DCOS. We also try to answer the question to which degree cloud computing requires meta-layers with power of Turing's Oracles.",0,1,0,grid computing+cloud services+cloud computing,
833,833,"Emerging perspectives of application programming interface strategy a framework to respond to business concerns. Software specialists increasingly find themselves in situations where their application programming interface (API)-related decisions have implications on software business. We present a strategic API framework to aid in consideration of business concerns when designing, updating, or maintaining APIs.",0,1,0,business activities+business process+software,
834,834,"Varys an agnostic model driven monitoring as a service framework for the cloud. Cloud systems are large scalable distributed systems that must be carefully monitored to timely detect problems and anomalies. While a number of cloud monitoring frameworks are available, only a few solutions address the problem of adaptively and dynamically selecting the indicators that must be collected, based on the actual needs of the operator. Unfortunately, these solutions are either limited to infrastructure-level indicators or technology-specific, for instance, they are designed to work with OpenStack but not with other cloud platforms. This paper presents the VARYS monitoring framework, a technology-agnostic Monitoring-as-a-Service solution that can address KPI monitoring at all levels of the Cloud stack, including the application-level. Operators use VARYS to indicate their monitoring goals declaratively, letting the framework to perform all the operations necessary to achieve a requested monitoring configuration automatically. Interestingly, the VARYS architecture is general and extendable, and can thus be used to support increasingly more platforms and probing technologies.",0,1,0,private clouds+cloud providers+distributed systems,
835,835,"A survey of secure multiparty computation protocols for privacy preserving genetic tests. We discuss several protocols that apply secure multiparty computation to privacy preserving genetic testing. We categorize methods into those using oblivious finite automata, additive homomorphic encryption, garbled circuits, and private set intersection. Through comparison of performance and security metrics, we aim to make recommendations for efficient and secure multiparty computation protocols for various genetic tests including edit distance, disease susceptibility, identity/paternity/- common ancestry testing, medicine and treatment efficacy for personalized medicine, and genetic compatibility.",0,1,0,encryption+finite automata+automata,
836,836,"Crossrec supporting software developers by recommending third party libraries. Abstract When creating a new software system, or when evolving an existing one, developers do not reinvent the wheel but, rather, seek available libraries that suit their purpose. In such a context, open source software repositories contain rich resources that can provide developers with helpful advice to support their tasks. However, the heterogeneity of resources and the dependencies among them are the main obstacles to the effective mining and exploitation of the available data. In this sense, advanced techniques and tools are needed to mine the metadata to bring in meaningful recommendations. In this paper, we present CrossRec, a recommender system to assist open source software developers in selecting suitable third-party libraries. CrossRec exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies, which are currently included in the project being developed. We perform an empirical evaluation to compare the proposed approach with three state-of-the-art baselines, i.e., LibRec, LibFinder, and LibCUP on three considerably large datasets. The experimental results show that CrossRec overcomes the limitation of the baselines by recommending also libraries with a specific version. More importantly, it outperforms LibRec and LibCUP with respect to various quality metrics.",0,1,0,software developer+software systems+software repositories,
837,837,"A dataset for dynamic discovery of semantic changes in version controlled software histories. Over the last few years, researchers proposed several semantic history slicing approaches that identify the set of semantically-related commits implementing a particular software functionality. However, there is no comprehensive benchmark for evaluating these approaches, making it difficult to assess their capabilities.   This paper presents a dataset of 81 semantic change data collected from 8 real-world projects. The dataset is created for benchmarking semantic history slicing techniques. We provide details on the data collection process and the storage format. We also discuss usage and possible extensions of the dataset.",0,1,0,storage formats+software component+software,
838,838,"Effect analysis for programs with callbacks. We introduce a precise interprocedural effect analysis for programs with mutable state, dynamic object allocation, and dynamic dispatch. Our analysis is precise even in the presence of dynamic dispatch where the context-insensitive estimate on the number of targets is very large. This feature makes our analysis appropriate for programs that manipulate first-class functions callbacks. We present a framework in which programs are enriched with special effect statements, and define the semantics of both program and effect statements as relations on states. Our framework defines a program composition operator that is sound with respect to relation composition. Computing the summary of a procedure then consists of composing all its program statements to produce a single effect statement. We propose a strategy for applying the composition operator in a way that balances precision and efficiency.

We instantiate this framework with a domain for tracking read and write effects, where relations on program states are abstracted as graphs. We implemented the analysis as a plugin for the Scala compiler. We analyzed the Scala standard library containing 58000 methods and classified them into several categories according to their effects. Our analysis proves that over one half of all methods are pure, identifies a number of conditionally pure methods, and computes summary graphs and regular expressions describing the side effects of non-pure methods.",0,1,0,haskell+compiler+inter-procedural,
839,839,"Are php applications ready for hack. PHP is by far the most popular WEB scripting language, accounting for more than 80% of existing websites. PHP is dynamically typed, which means that variables take on the type of the objects that they are assigned, and may change type as execution proceeds. While some type changes are likely not harmful, others involving function calls and global variables may be more difficult to understand and the source of many bugs. Hack, a new PHP variant endorsed by Facebook, attempts to address this problem by adding static typing to PHP variables, which limits them to a single consistent type throughout execution.",0,1,0,scripting languages+javascript+php,
840,840,Sound non statistical clustering of static analysis alarms. We present a sound method for clustering alarms from static analyzers. Our method clusters alarms by discovering sound dependencies between them such that if the dominant alarm of a cluster turns out to be false (respectively true) then it is assured that all others in the same cluster are also false (respectively true). We have implemented our clustering algorithm on top of a realistic buffer-overflow analyzer and proved that our method has the effect of reducing 54% of alarm reports. Our framework is applicable to any abstract interpretation-based static analysis and orthogonal to abstraction refinements and statistical ranking schemes.,0,1,0,program analysis+static analysis+static program analysis,
841,841,"A user survey of configuration challenges in linux and ecos. Operating systems expose sophisticated configurability to handle variability in hardware platforms like mobile devices, desktops, and servers. The variability model of an operating system kernel like Linux contains thousands of options guarded by hundreds of complex constraints. To guide users throughout the configuration and ensure the validity of their decisions, specialized tools known as configurators have been developed. Despite these tools, configuration still remains a difficult and challenging process. To better understand the challenges faced by users during configuration, we conducted two surveys, one among Linux users and another among eCos users. This paper presents the results of the surveys along three dimensions: configuration practice; user guidance; and language expressiveness. We hope that these results will help researchers and tool builders focus their efforts to improve tool support for software configuration.",0,1,0,linux+tool support+variability modeling,
842,842,"An analysis of a project reuse approach in an industrial setting. We performed an industrial exploratory case study to analyze the software reuse process of a medium size company which is a technology leader in a niche market. Two unstructured interviews and code duplication analyses of four SVN dumps report about a development practice that resulted in more efficient maintenance, due to archiving complete versions of every shipped software, and more efficient development, due to duplication and modification of the most similar program, instead of implementing a new program from scratch.",0,1,0,reusability+software+software reuse,
843,843,"Bio inspired model for data distribution in fog and mist computing. Fog and Mist computing exploits computational resources of IoT devices located at the edge of the network. This new infrastructure brings benefits related to latency and Internet bandwidth utilization when compared with Cloud computing. However, this scenario is quite challenging mainly due to the considerable heterogeneity of the devices and to the dynamicity of the network topology what adds new issues related to security, privacy, data availability, and service availability, among others. Despite the theme of data dissemination and data storage had been relatively explored in fields of Sensor Wireless Network (WSN) and Vehicular Ad Hoc Networks (VANETs), we found few studies in the Fog computing research area. Then, this work focuses on the problem of data availability and dissemination in this environment by proposing a distributed algorithm using ideas from evolutionary computation and epidemic models. Results obtained through simulation suggest that the algorithm presents proper results in maintaining data availability in the Fog and Mist Computing environments, even under severe conditions.",0,1,0,security and privacy+cloud computing+mobile cloud computing,
844,844,"A novel touchscreen based authentication scheme using static and dynamic hand biometrics. With the booming of smart phone and high-speed wireless networks in recent years, applications and data have been shifting from desktop to mobile devices at a vigorous pace. Although mobile computing provides great convenience in daily life, it becomes vulnerable to various types of emerging attacks. User authentication plays an indispensable role in protecting computer systems and applications, but the development of touch screen hardware and user habit change post requirements for new authentication methods for mobile and tablets devices. In this paper, we present a robust user authentication scheme using both static and dynamic features of touch gestures. We take advantage of the pressure sensitivity of multi-touch screens to obtain irreproducible biometric patterns. Discriminative features such as distance, angle, and pressure are extracted from the touch-point data, and used in statistical analysis for verification. We tested our scheme in a variety of experiments that involved multiple volunteers to perform various gestures. The analysis of experimental results and user feedback indicate the proposed scheme delivers comprehensive measurements and accurate pattern classification for touch gestures. Based on these results, we conclude that the proposed scheme overcomes the limitations of the existing user authentication methods, and shows great potential to provide robust protection against unauthorized access.",0,1,0,wireless networks+computer hardware+verification,
845,845,"A smart city environmental monitoring network and analysis relying on big data techniques. A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.",0,1,0,data analytics+4g+cloud services,
846,846,"Safe regression test selection based on program dependence graphs. Regression test selection is to select a subset of existing tests to run, so as to identify the possible faults in the modified program. A promising regression test selection technique needs to be safe, that is, to select tests from the original test suite that can expose faults in the modified program under controlled regression testing. Existing safe regression test selection techniques however may rerun some tests that do not expose faults. To address this problem, we present in this paper a new regression test selection technique based on program dependence graphs of a program and its modified version. Comparing with previous techniques, our technique can eliminate some unnecessary tests to rerun. We show the validity and feasibility of our approach through a running example.",0,1,0,regression testing+test case prioritization+regression test selection,
847,847,"Communication robot for elderly based on robotic process automation. Currently, a communication robot like an AI speaker has become popular as one of consumer services. As realized high functions seen in cooperation of network and home appliances by them, hurdles for the elderly in operating such advanced IT devices are still high. On the other hand, RPA (Robotic Process Automation) attracts attention for productivity improvement of business processing. However, there are few examples that applied RPA to consumer services. It is caused that there is not common sense about an application method of RPA for consumer services. Therefore, if we could define the application method of RPA for consumer services, we could develop consumer services familiar with the elderly. This paper gives some examples of our developed consumer services for the elderly by communication robots after summarizing requirements for applying RPA to consumer services. Then, we inspect the effectiveness of the consumer services based on RPA. Finally, we make clear a RPA basic model for consumer services.",0,1,0,home appliances+business processing+automation,
848,848,Managing copyrights and moral rights of service based software. Service orientation of software raises new challenges for representing the intellectual rights associated with services and requirements for developing service licensing strategies.,0,1,0,access rights+service-oriented architecture (soa)+software,
849,849,"Program analysis for overlaid data structures. We call a data structure overlaid, if a node in the structure includes links for multiple data structures and these links are intended to be used at the same time. In this paper, we present a static program analysis for overlaid data structures. Our analysis implements two main ideas. The first is to run multiple sub-analyses that track information about non-overlaid data structures, such as lists. Each sub-analysis infers shape properties of only one component of an overlaid data structure, but the results of these sub-analyses are later combined to derive the desired safety properties about the whole overlaid data structure. The second idea is to control the communication among the sub-analyses using ghost states and ghost instructions. The purpose of this control is to achieve a high level of efficiency by allowing only necessary information to be transferred among sub-analyses and at as few program points as possible. Our analysis has been successfully applied to prove the memory safety of the Linux deadline IO scheduler and AFS server.",0,1,0,safety properties+program analysis+static program analysis,
850,850,"Trade off oriented development making quality attribute trade offs first class. Implementing a solution for a design decision that precisely satisfies the trade-off between quality attributes can be extremely challenging. Further, typically quality attribute trade-offs are not represented as first-class entities in development artifacts. Hence, decisions might be sub-optimal and lack requirements traceability as well as changeability. We propose Trade-off-oriented Development (ToD), a new concept to automate the selection and integration of reusable implementations for a given design decision based on quality attribute trade-offs. Implementations that vary in quality attributes and that solve reoccurring design decisions are collected in a design decision library. Developers declaratively specify the quality attribute trade-off, which is then used to automatically select the best fitting implementation. We argue that thereby, software could satisfy the trade-offs more precisely, requirements are traceable and changeable, and advances in implementations automatically improve existing software.",0,1,0,software+quality attributes+requirements traceability,
851,851,"Concept based failure clustering. When attempting to determine the number and set of execution failures that are caused by particular faults, developers must perform an arduous task of investigating and diagnosing each individual failure. Researchers proposed failure-clustering techniques to automatically categorize failures, with the intention of isolating each culpable fault. The current techniques utilize dynamic control flow to characterize each failure to then cluster them. These existing techniques, however, are blind to the intent or purpose of each execution, other than what can be inferred by the control-flow profile. We hypothesize that semantically rich execution information can aid clustering effectiveness by categorizing failures according to which functionality they exhibit in the software. This paper presents a novel clustering method that utilizes latent-semantic-analysis techniques to categorize each failure by the semantic concepts that are expressed in the executed source code. We present an experiment comparing this new technique to traditional control-flow-based clustering. The results of the experiment showed that the semantic-concept clustering was more precise in the number of clusters produced than the traditional approach, without sacrificing cluster accuracy.",0,1,0,semantics+source codes+software,
852,852,"A mechanised proof of an adaptive state counting algorithm. In this paper it is demonstrated that the capabilities of state-of-the-art proof assistant tools are sufficient to present mechanised and, at the same time, human-readable proofs establishing completeness properties of test methods and the correctness of associated test generation algorithms. To this end, the well-known Isabelle/HOL proof assistant is used to mechanically verify a complete test theory elaborated by the second author for checking the reduction conformance relation between a possibly nondeterministic finite state machine (FSM) serving as reference model and an implementation whose behaviour can also be represented by an FSM. The formalisation also helps to clarify an ambiguity in the original test generation algorithm which was specified in natural language and could be misinterpreted in a way leading to insufficient fault coverage.",0,1,0,isabelle/hol+fault coverages+test generations,
853,853,"Model driven allocation engineering t. Cyber-physical systems (CPSs) provide sophisticated functionality and are controlled by networked electronic control units (ECUs). Nowadays, software engineers use component-based development approaches to develop their software. Moreover, software components have to be allocated to an ECU to be executed. Engineers have to cope with topology-, software-, and timing-dependencies and memory-, scheduling-, and routing-constraints. Currently, engineers use linear programs to specify allocation constraints and to derive a feasible allocation automatically. However, encoding the allocation problem as a linear program is a complex and error-prone task. This paper contributes a model-driven, OCL-based allocation engineering approach for reducing the engineering effort and to avoid failures. We validate our approach with an automotive case study modeled with MechatronicUML. Our validation shows that we can specify allocation constraints with less engineering effort and are able to derive feasible allocations automatically.",0,1,0,software component+software+ocl,
854,854,"A dissection of the test driven development process does it really matter to test first or to test last. Background : Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature.  Aim : We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations.  Method : We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness.  Result : Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring.  Conclusion : The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.",0,1,0,development processes+refactorings+tdd,
855,855,"Enablers and inhibitors of experimentation in early stage software startups. Software startups are temporary organizations that develop innovative software-intensive products or services. Despite of numerous successful stories, most startups fail. Several methodologies were proposed both in the scientific and commercial literature to improve their success rate, and a common element among them is the idea of experimentation. This concept was brought to software development as an approach focused on taking critical product assumptions as hypotheses and developing experiments to support or refute them. Although well-known methodologies are based on this idea, the literature shows that software startups still do not follow this approach. The goal of this paper is to identify the enablers and inhibitors of experimentation in early-stage software startups. To achieve the goal, we performed a multiple-case study of four software startups. The results comprise a set of enablers and inhibitors divided into the categories of individual, organizational context, and environment.",0,1,0,software+software development+software project,
856,856,"Automated verification of interactive rule based configuration systems. Rule-based specifications of systems have again become common in the context of product line variability modeling and configuration systems. In this paper, we define a logical foundation for rule-based specifications that has enough expressivity and operational behavior to be practically useful and at the same time enables decidability of important overall properties such as consistency or cycle-freeness. Our logic supports rule-based interactive user transitions as well as the definition of a domain theory via rule transitions. As a running example, we model DOPLER, a rule-based configuration system currently in use at Siemens.",0,1,0,variability modeling+product lines+software product line,
857,857,"A smart city application modeling framework a case study on re engineering a smart retail platform. Smart City Application Engineering is a challenging task due to the constantly evolving environment in which these applications operate and the variability of the different types of technologies that synthesize them. Therefore, flexibility and extendibility are two important quality attributes that should be taken into consideration when designing Smart City Applications. In this paper, we propose the Smart City Application Modeling Framework (SCAMF) for analyzing and designing Smart City applications that is based on the concept of Clean Architecture and adopts the representation formalism of feature models. SCAMF methodology is evaluated through a case study on a Smart Retail Platform. Quality indices like flexibility, extendibility along with metrics as complexity, cohesion and design size are compared to the initial version of the application that was completely re-engineered due to maintenance problems. The results of the study suggest that the proposed methodology improves quality indices like flexibility and extendibility up to 120%.",0,1,0,application engineering+quality attributes+software product line,
858,858,"Ar tracker track the dynamics of mobile apps via user review mining. User-generated reviews on mobile applications (apps) are a valuable source of data for developers to improve the quality of their apps. But the reviews are usually massive in size and span over multiple topics, thus leading to great challenges for developers to efficiently identify the key reviews of interest. In recent studies, automatic user review mining has been recognized as a key solution to address this challenge. The existing methods, however, require extensive human efforts to manually label the training data. Besides, they only analyze the static characteristics over the whole set of collected reviews, while ignoring the dynamic information embedded in the reviews of different time periods. In this paper, we propose 'AR-Tracker', a new framework to mine user reviews without the need of human labeling and track the dynamics from the top-ranked reviews. Through extensive experiments on the reviews of four popular mobile apps collected over 7 months, we show that AR-Tracker can still achieve comparable accuracy with the state-of-the-art methods, e.g., AR-Miner. Additionally, a case study on Facebook reviews further validates the effectiveness of 'AR-Tracker' in tracking the dynamics.",0,1,0,user information+mobile devices+mobile applications,
859,859,"An empirical investigation on the relationship between design and architecture smells. Architecture of a software system represents the key design decisions and therefore its quality plays an important role to keep the software maintainable. Code smells are indicators of quality issues in a software system and are classified based on their granularity, scope, and impact. Despite a plethora of existing work on smells, a detailed exploration of architecture smells, their characteristics, and their relationships with smells in other granularities is missing. The paper aims to study architecture smells characteristics, investigate correlation, collocation, and causation relationships between architecture and design smells. We implement smell detection support for seven architecture smells. We mine 3 073 open-source repositories containing more than 118 million lines of C# code and empirically investigate the relationships between seven architecture and 19 design smells. We find that smell density does not depend on repository size. Cumulatively, architecture smells are highly correlated with design smells. Our collocation analysis finds that the majority of design and architecture smell pairs do not exhibit collocation. Finally, our causality analysis reveals that design smells cause architecture smells.",0,1,0,software systems+refactorings+code smell,
860,860,"Software reliability assessment modeling and algorithms. Non-homogeneous Poisson process (NHPP) software reliability growth models (SRGM) enable quantitative assessment of the software testing process. Software reliability models ranging from simple to complex have been proposed to characterize failure data that results from a variety of testing factors as well as non-uniform expenditure of testing effort. In order to predict the reliability of software accurately, it is important to apply models that both characterize the observed failure data well and make accurate predictions of the future. Efficient and robust algorithms to quickly estimate the model parameters despite inaccuracy in the initial estimates are also highly desirable. Ultimately, emphasis should be placed on predictive accuracy over complexity to best serve users of the research. This work presents the results of the preliminary contributions of the proposal including: (i) a heterogeneous single changepoint framework considering different models before and after the changepoint and (ii) comparison of testing effort models with a simple model as well as a testing effort model fit with an ECM algorithm to emphasize the importance of model predictive accuracy over increased model complexity. The preliminary findings will be used to serve as the basis of the overall contributions of the dissertation.",0,1,0,software reliability+reliability growth+software reliability models,
861,861,"Evolving a method framework for engineering process assessment models. In 2009 a research team analyzed how Process Assessment Models (PAM) had been produced. As a consequence, MFMOD was developed as a Method Framework for engineering Process Assessment Models (MFMOD). Producing PAMs using an engineering approach is a desirable research topic in Software Process Improvement due to the need for distinct and more specialized PAMs and the publication of ISO/IEC 15504 International Standard that includes requirements for PAMs. Lately MFMOD has been used as reference for defining processes for engineering three different PAMs and for defining a specific method for customizing PAMs. An analysis of how MFMOD was used confirmed its usefulness and indicated improvement opportunities for evolving it. This article introduces a work in progress to evolve it towards an improved version.",0,1,0,software development+software project+software process improvement,
862,862,"Neural detection of semantic code clones via tree based convolution. Code clones are similar code fragments that share the same semantics but may differ syntactically to various degrees. Detecting code clones helps reduce the cost of software maintenance and prevent faults. Various approaches of detecting code clones have been proposed over the last two decades, but few of them can detect semantic clones, i.e., code clones with dissimilar syntax. Recent research has attempted to adopt deep learning for detecting code clones, such as using tree-based LSTM over Abstract Syntax Tree (AST). However, it does not fully leverage the structural information of code fragments, thereby limiting its clone-detection capability. To fully unleash the power of deep learning for detecting code clones, we propose a new approach that uses tree-based convolution to detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical information from code tokens. Additionally, our approach addresses the limitation that source code has an unlimited vocabulary of tokens and models, and thus exploiting lexical information from code tokens is often ineffective when dealing with unseen tokens. Particularly, we propose a new embedding technique called position-aware character embedding (PACE), which essentially treats any token as a position-weighted combination of character one-hot embeddings. Our experimental results show that our approach substantially outperforms an existing state-of-the-art approach with an increase of 0.42 and 0.15 in F1-score on two popular code-clone benchmarks (OJClone and BigCloneBench), respectively, while being more computationally efficient. Our experimental results also show that PACE enables our approach to be substantially more effective when code clones contain unseen tokens.",0,1,0,software maintenance+code clone+clone detection,
863,863,"Star a specialized tagging approach for docker repositories. Docker images, having the idea of ""build once, run anywhere"", are widely used as the reusable delivery artifacts. Currently, there are a huge number of online Docker repositories that provide images as the off-the-shelf blocks to construct large and complicated systems. Tags would improve the reusability as they provide concise semantics. However, tags are not well supported for Docker images, and manual tagging is still exhausting. We propose STAR, a Specialized Tagging Approach for Docker Repositories, to address the problem of automatically multi-labeling the large number of repositories. STAR takes Dockerfiles of the repositories as the primary input, which because a Dockerfile contains all the instructions for building a Docker image. STAR is based on two prediction models. By taking a Dockerfile as the specific text description, we model a repository with its labeled tags and Dockerfile terms, and use Labeled Latent Dirichlet Allocation algorithm to recommend tags. By regarding a Dockerfile as the configuration code, we construct a feature model based on Dockerfile key instructions and use a similarity-based ranking algorithm to recommend tags. Given an untagged repository, STAR outputs two probability scores for each tag with the two models and takes a weighted sum of them as the final score. Finally, STAR ranks all the tags according to their scores and recommends the top K ones. We evaluate STAR on over 100,000 repositories of Docker Hub. The experimental results show that STAR outperforms the state-of-the-art approaches in terms of Recall@5 and Recall@10.",0,1,0,prediction modes+reusability+feature models,
864,864,"Bidirectional symbolic analysis for effective branch testing. Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the  frontier  between symbolic execution and symbolic reachability analysis, to guide the alternation and the progress of bidirectional analysis towards the coverage targets. The experimental results presented in the paper indicate that the proposed approach can both find test inputs that exercise rare execution conditions that are not identified with state-of-the-art approaches and eliminate many infeasible branches from the coverage measurement. It can thus produce a modified branch coverage metric that indicates the amount of feasible branches covered during testing, and helps team leaders and developers in estimating the amount of not-yet-covered feasible branches. The approach proposed in this paper suffers less than the other approaches from particular cases that may trap the analysis in unbounded loops.",0,1,0,test case generation+regression testing+symbolic execution,
865,865,"Symbolic optimal reachability in weighted timed automata. Weighted timed automata have been defined in the early 2000 s for modelling resource-consumption or -allocation problems in real-time systems. Optimal reachability is decidable in weighted timed automata, and a symbolic forward algorithm has been developed to solve that problem. This algorithm uses so-called priced zones, an extension of standard zones with cost functions. In order to ensure termination, the algorithm requires clocks to be bounded. For unpriced timed automata, much work has been done to develop sound abstractions adapted to the forward exploration of timed automata, ensuring termination of the model-checking algorithm without bounding the clocks. In this paper, we take advantage of recent developments on abstractions for timed automata, and propose an algorithm allowing for symbolic analysis of all weighted timed automata, without requiring bounded clocks.",0,1,0,push-down automata+model checking algorithm+timed automata,
866,866,"Analyzing the context of bug fixing changes in the openstack cloud computing platform. Many research areas in software engineering, such as mutation testing, automatic repair, fault localization, and fault injection, rely on empirical knowledge about recurring bug-fixing code changes. Previous studies in this field focus on what has been changed due to bug-fixes, such as in terms of code edit actions. However, such studies did not consider where the bug-fix change was made (i.e., the context of the change), but knowing about the context can potentially narrow the search space for many software engineering techniques (e.g., by focusing mutation only on specific parts of the software). Furthermore, most previous work on bug-fixing changes focused on C and Java projects, but there is little empirical evidence about Python software. Therefore, in this paper we perform a thorough empirical analysis of bug-fixing changes in three OpenStack projects, focusing on both the what and the where of the changes. We observed that all the recurring change patterns are not oblivious with respect to the surrounding code, but tend to occur in specific code contexts.",0,1,0,fault localization+fault injection+bug-fixing,
867,867,"Key challenges in software startups across life cycle stages. Software startups are challenging endeavours, with various road blocks on their path to success. The current understanding of the challenges that software startups may encounter is very limited. In this paper, we use the research framework of learning and product development stages to analyse the key challenges that software startups have to deal with at different life cycle stages, from problem definition to solution validation and from concept to mature product. Based on an analysis of the empirical data collected by a large survey of 4100 startups, we find out that what perceived as biggest challenges by software startups do vary across different life cycle stages. Building product is the biggest obstacle for software startups, even though its significance decreases when the learning focuses of the startups move from problem to solution and their products mature. Business related challenges such as customer acquisition and scaling are more noticeable at the later stages. Our study raises the awareness of these challenges and suggests to tackle right challenges at the right time.",0,1,0,software component+software+software project,
868,868,"Mapreduce join strategies for key value storage. This paper analyses MapReduce join strategies used for big data analysis and mining known as map-side and reduce-side joins. The most used joins will be analysed in this paper, which are theta-join algorithms including all pair partition join, repartition join, broadcasting join, semi join, per-split semi join. This paper can be considered as a guideline for MapReduce application developers for the selection of join strategies. The analysis of several join strategies for big data analysis and mining is accompanied by comprehensive examples.",0,1,0,data analytics+map-reduce+software developer,
869,869,"Automatic documentation generation via source code summarization of method context. A documentation generator is a programming tool that creates documentation for software by analyzing the statements and comments in the software's source code. While many of these tools are manual, in that they require specially-formatted metadata written by programmers, new research has made inroads towards automatic generation of documentation. These approaches work by stitching together keywords from the source code into readable natural language sentences. These approaches have been shown to be effective, but carry a key limitation: the generated documents do not explain the source code's context. They can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a technique that includes this context by analyzing how the Java methods are invoked. In a user study, we found that programmers benefit from our generated documentation because it includes context information.",0,1,0,source codes+software+java,
870,870,"Building a body of knowledge on model checking for software development. Formal Methods has been recognized as a rigorous development methodology for hardware and software systems. In particular, model checking is well accepted as an effective verification method for hardware systems, safety/missioncritical systems and embedded systems. To foster this technology in industry, we recognize a need to develop educational materials to enhance learning the technology by students and practitioners. However, there are neither standard guidelines nor instructions how to teach this technology. In this paper, we will present the first draft of a body of knowledge on model checking called MCBOK to address this issue, and present lessons learned from its development experience.",0,1,0,model checking+software systems+software development,
871,871,"The key technology research on certain missile weapon system simulation based on hla rti. To resolve some bottleneck of certain missile weapon system simulation, simulating architecture, distributed simulation engine based-on HLA/RTI, simulation database management based on Oracle Berkeley DB, techniques of ocean waves synthesis based on directional spectrum and helicopter anti-submarine visual simulations are researched. The simulating result indicates that above research has great promoted the usage and deployment of certain missile weapon system.",0,1,0,engine+data-base management systems+database management,
872,872,"On the relation of test smells to software code quality. Test smells are sub-optimal design choices in the implementation of test code. As reported by recent studies, their presence might not only negatively affect the comprehension of test suites but can also lead to test cases being less effective in finding bugs in production code. Although significant steps toward understanding test smells, there is still a notable absence of studies assessing their association with software quality. In this paper, we investigate the relationship between the presence of test smells and the change-and defect-proneness of test code, as well as the defect-proneness of the tested production code. To this aim, we collect data on 221 releases of ten software systems and we analyze more than a million test cases to investigate the association of six test smells and their co-occurrence with software quality. Key results of our study include:(i) tests with smells are more change-and defect-prone, (ii) ""Indirect Testing"", ""Eager Test"", and ""Assertion Roulette"" are the most significant smells for change-proneness and, (iii) production code is more defect-prone when tested by smelly tests.",0,1,0,software systems+software quality+code smell,
873,873,"Implementing database access control policy from unconstrained natural language text. Although software can and does implement access control at the application layer, failure to enforce data access at the data layer often allows uncontrolled data access when individuals bypass application controls. The goal of this research is to improve security and compliance by ensuring access controls rules explicitly and implicitly defined within unconstrained natural language texts are appropriately enforced within a system's relational database. Access control implemented in both the application and data layers strongly supports a defense in depth strategy. We propose a tool-based process to 1) parse existing, unaltered natural language documents; 2) classify whether or not a statement implies access control and whether or not the statement implies database design; and, as appropriate, 3) extract policy elements; 4) extract database design; 5) map data objects found in the text to a database schema; and 6) automatically generate the necessary SQL commands to enable the database to enforce access control. Our initial studies of the first three steps indicate that we can effectively identify access control sentences and extract the relevant policy elements.",0,1,0,application layers+access control policies+role-based access control,
874,874,"Investigating next steps in static api misuse detection. Application Programming Interfaces (APIs) often impose constraints such as call order or preconditions. API misuses, i.e., usages violating these constraints, may cause software crashes, data-loss, and vulnerabilities. Researchers developed several approaches to detect API misuses, typically still resulting in low recall and precision. In this work, we investigate ways to improve API-misuse detection. We design MuDetect, an API-misuse detector that builds on the strengths of existing detectors and tries to mitigate their weaknesses. MuDetect uses a new graph representation of API usages that captures different types of API misuses and a systematically designed ranking strategy that effectively improves precision. Evaluation shows that MuDetect identifies real-world API misuses with twice the recall of previous detectors and 2.5x higher precision. It even achieves almost 4x higher precision and recall, when mining patterns across projects, rather than from only the target project.",0,1,0,programming languages+misuse detection+software,
875,875,"Enabling the runtime assertion checking of concurrent contracts for the java modeling language. Though there exists ample support for Design by Contract (DbC) for sequential programs, applying DbC to concurrent programs presents several challenges. In previous work, we extended the Java Modeling Language (JML) with constructs to specify concurrent contracts for Java programs. We present a runtime assertion checker (RAC) for the expanded JML capable of verifying assertions for concurrent Java programs. We systematically evaluate the validity of system testing results obtained via runtime assertion checking using actual concurrent and functional faults on a highly concurrent industrial system from the telecommunications domain.",0,1,0,sequential programs+java program+concurrent program,
876,876,"Party parameterized synthesis of token rings. Synthesis is the process of automatically constructing an implementation from a specification. In parameterized synthesis, we construct a single process such that the distributed system consisting of an arbitratry number of copies of the process satisfies a parameterized specification. In this paper, we present Party, a tool for parameterized synthesis from specifications in indexed linear temporal logic. Our approach extends SMT-based bounded synthesis, a flexible method for distributed synthesis, to parameterized specifications. In the current version, Party can be used to solve the parameterized synthesis problem for token-ring architectures. The tool can also synthesize monolithic systems, for which we provide a comparison to other state-of-the-art synthesis tools.",0,1,0,linear temporal logic+formal specification+linear time temporal logic,
877,877,"Exploring mobile end user development existing use and design factors. Mobile devices are everywhere, and the scope of their use is growing from simple calling and texting through Internet browsing to more technical activities such as creating message processing filters and connecting different apps. However, building tools which provide effective support for such advanced technical use of mobile devices by non-programmers (mobile end user development or  mEUD ) requires thorough understanding of user needs and motivations, including factors which can impact user intentions regarding mEUD activities. We propose a model linking these mEUD factors with mobile users’ attitudes towards, and intent of doing mEUD, and discuss a number of implications for supporting mEUD. Our research process is user-centered, and we formulate a number of hypotheses by fusing results from an exploratory survey which gathers facts about mEUD motivations and activities, and from a focus group study, which delivers deeper understanding of particular mEUD practices and issues. We then test the hypothesized relationships through a follow-up enquiry mixing quantitative and qualitative techniques, leading to the creation of a preliminary mEUD model. Altogether we have involved 275 mobile users in our research. Our contribution links seven mEUD factors with mEUD intentions and attitudes, and highlights a number of implications for mEUD support.",0,1,0,mobile terminal+mobile networks+internet,
878,878,"A theory for control flow graph exploration. Detection of infeasible code has recently been identified as a scalable and automated technique to locate likely defects in software programs. Given the (acyclic) control-flow graph of a procedure, infeasible code detection depends on an exhaustive search for feasible paths through the graph. A number of encodings of control-flow graphs into logic (understood by theorem provers) have been proposed in the past for this application. In this paper, we compare the performance of these different encodings in terms of runtime and the number of queries processed by the prover. We present a theory of acyclic control-flow as an alternative method of handling control-flow graphs. Such a theory can be built into theorem provers by means of theory plug-ins. Our experiments show that such native handling of control-flow can lead to significant performance gains, compared to previous encodings.",0,1,0,program dependence graph+execution trace+execution paths,
879,879,"Analysing the usage of tools in pair programming sessions. In this study we observe the daily work of nineteen software developers of a large Italian manufacturing company for a period of ten months to determine the effects of pair programming on the use of tools. Fifteen developers are existing team members and four have recently joined the team. They practice Pair Programming spontaneously, that is, when they feel it is needed. We identify the tools the developers use, how they distribute their time among these tools when working alone and when doing Pair Programming. The data have been extracted non-invasively by means of PROM – tool for automated data collection and analysis. The preliminary results indicate that developers working in pairs devote significantly more time to programming activities than developers working alone.",0,1,0,programming languages+software+software developer,
880,880,"Three cases of feature based variability modeling in industry. Large software product lines need to manage complex variability. A common approach is variability modeling—creating and maintaining models that abstract over the variabilities inherent in such systems. While many variability modeling techniques and notations have been proposed, little is known about industrial practices and how industry values or criticizes this class of modeling. We attempt to address this gap with an exploratory case study of three companies that apply variability modeling. Among others, our study shows that variability models are valued for their capability to organize knowledge and to achieve an overview understanding of codebases. We observe centralized model governance, pragmatic versioning, and surprisingly little constraint modeling, indicating that the effort of declaring and maintaining constraints does not always pay off.",0,1,0,product lines+software product line+software product line engineerings,
881,881,"Hybrid program dependence approximation for effective dynamic impact prediction. Impact analysis determines the effects that program entities of interest, or changes to them, may have on the rest of the program for software measurement, maintenance, and evolution tasks. Dynamic impact analysis could be one major approach to impact analysis that computes smaller impact sets than static alternatives for concrete sets of executions. However, existing dynamic approaches often produce impact sets that are too large to be useful, hindering their adoption in practice. To address this problem, we propose to exploit static program dependencies to drastically prune false-positive impacts that are not exercised by the set of executions utilized by the analysis, via hybrid dependence approximation. Further, we present a novel dynamic impact analysis called Diver which leverages both the information provided by the dependence graph and method-execution events to identify runtime method-level dependencies, hence dynamic impact sets, much more precisely without reducing safety and at acceptable costs. We evaluate Diver on ten Java subjects of various sizes and application domains against both arbitrary queries covering entire programs and practical queries based on changes actually committed by developers to actively evolving software repositories. Our extensive empirical studies show that Diver can significantly improve the precision of impact prediction, with 100-186 percent increase, with respect to a representative existing alternative thus provide a far more effective option for dynamic impact prediction. Following a similar rationale to Diver , we further developed and evaluated an online dynamic impact analysis called DiverOnline which produces impact sets immediately upon the termination of program execution. Our results show that compared to the offline approach, for the same precision, the online approach can reduce the time by 50 percent on average for answering all possible queries in the given program at once albeit at the price of possibly significant increase in runtime overhead. For users interested in one specific query only, the online approach may compute the impact set for that query during runtime without much slowing down normal program operation. Further, the online analysis, which does not incur any space cost beyond the static-analysis phase, may be favored against the offline approach when trace storage and/or related file-system resource consumption becomes a serious challenge or even stopper for adopting dynamic impact prediction. Therefore, the online and offline analysis together offer complementary options to practitioners accommodating varied application/task scenarios and diverse budget constraints.",0,1,0,impact analysis+software repositories+change impact analysis,
882,882,"On the impact of sample duplication in machine learning based android malware detection. Malware detection at scale in the Android realm is often carried out using machine learning techniques. State-of-the-art approaches such as DREBIN and MaMaDroid are reported to yield high detection rates when assessed against well-known datasets. Unfortunately, such datasets may include a large portion of duplicated samples, which may bias recorded experimental results and insights. In this article, we perform extensive experiments to measure the performance gap that occurs when datasets are de-duplicated. Our experimental results reveal that duplication in published datasets has a limited impact on supervised malware classification models. This observation contrasts with the finding of Allamanis on the general case of machine learning bias for big code. Our experiments, however, show that sample duplication more substantially affects unsupervised learning models (e.g., malware family clustering). Nevertheless, we argue that our fellow researchers and practitioners should always take sample duplication into consideration when performing machine-learning-based (via either supervised or unsupervised learning) Android malware detections, no matter how significant the impact might be.",0,1,0,malicious codes+malwares+android,
883,883,"What industry needs from architectural languages a survey. Many times we are faced with the proliferation of definitions, concepts, languages, and tools in certain (research) topics. But often there is a gap between what is provided by existing technologies and what is needed by their users. The strengths, limitations, and needs of the available technologies can be dubious. The same applies to software architectures, and specifically to languages designed to represent architectural models. Tens of different architectural languages have been introduced by the research and industrial communities in the last two decades. However, it is unclear if they fulfill the user's perceived needs in architectural description. As a way to plan for next generation languages for architectural description, this study analyzes practitioners' perceived strengths, limitations, and needs associated with existing languages for software architecture modeling in industry. We run a survey by interviewing 48 practitioners from 40 different IT companies in 15 countries. Each participant is asked to fill in a questionnaire of 51 questions. By analyzing the data collected through this study, we have concluded that 1) while practitioners are generally satisfied with the design capabilities provided by the languages they use, they are dissatisfied with the architectural language analysis features and their abilities to define extra-functional properties; 2) architectural languages used in practice mostly originate from industrial development instead of from academic research; 3) more formality and better usability are required of an architectural language.",0,1,0,software+software architecture+architecture description languages,
884,884,"Meminsight platform independent memory debugging for javascript. JavaScript programs often suffer from memory issues that can either hurt performance or eventually cause memory exhaustion. While existing snapshot-based profiling tools can be helpful, the information provided is limited to the coarse granularity at which snapshots can be taken. We present MemInsight, a tool that provides detailed, time-varying analysis of the memory behavior of JavaScript applications, including web applications. MemInsight is platform independent and runs on unmodified JavaScript engines. It employs tuned source-code instrumentation to generate a trace of memory allocations and accesses, and it leverages modern browser features to track precise information for DOM (document object model) objects. It also computes exact object lifetimes without any garbage collector assistance, and exposes this information in an easily-consumable manner for further analysis. We describe several client analyses built into MemInsight, including detection of possible memory leaks and opportunities for stack allocation and object inlining. An experimental evaluation showed that with no modifications to the runtime, MemInsight was able to expose memory issues in several real-world applications.",0,1,0,javascript+program debugging+memory locations,
885,885,"On the impact of software evolution on software clustering. The evolution of a software project is a rich data source for analyzing and improving the software development process. Recently, several research groups have tried to cluster source code artifacts based on information about how the code of a software system evolves. The results of these evolutionary approaches seem promising, but a direct comparison to traditional software clustering approaches based on structural code dependencies is still missing. To fill this gap, we conducted several clustering experiments with an established software clustering tool comparing and combining the evolutionary and the structural approach. These experiments show that the evolutionary approach could produce meaningful clustering results. While the traditional approach provides better results because of a more reliable data density of the structural data, the combination of both approaches is able to improve the overall clustering quality. A review of related studies shows that this approach of combining dependency information is also successful in other software engineering applications.",0,1,0,software engineering+software reengineering+software evolution,
886,886,"Interfaces modeling for product service system integration. Product-Service System (PSS) is proposed as an answer to the increasing need for providing sustainable solutions in nowadays competitive markets. To fulfil the advanced needs of customers, OEMs (Original Equipment Manufacturers) start collaborating with service and technology providers to develop a customised package of heterogeneous tangible and intangible components. Consequently, defining the interfaces is crucial to integrate these components into a unique system. Comparing to the product or service design, modeling interfaces in PSS is challenging because of the heterogeneity of its components, especially, when a tangible component is interacting with an intangible one. This paper aims at proposing a conceptual model supporting the definition and classification of interfaces in PSS architecture. Based on Systems Engineering (SE) recommendations, the model allows the definition of interfaces through the system definition from the functional to the physical architecture. The application of the model is illustrated by a pragmatic use case concerning the self-service and station-less bicycle sharing system.",0,1,0,content providers+systems integration+engineering,
887,887,"Arguing on software level verification techniques appropriateness. In this paper, we investigate the pondered selection of innovative software verification technology in the safety-critical domain and its implications. Verification tools perform analyses, testing or simulation activities. The compliance of the techniques implemented by these tools to fulfill standard-mandated objectives (i.e., to be means of compliance in the context of DO-178C and related supplements) should be explained to the certification body. It is thereby difficult for practitioners to use novel techniques, without a systematic method for arguing their appropriateness. Thus, we offer a method for arguing the appropriate application of a certain verification technique (potentially in combination with other techniques) to produce the evidence needed to satisfy certification objectives regarding fault detection and mitigation in a realistic avionics application via safety cases. We use this method for the choice of an appropriate compiler to support the development of a drone.",0,1,0,verification techniques+software verification+verification tools,
888,888,"An experiment comparing lifted and delayed variability aware program analysis. Today's software systems need to be highly flexible and managing their variability plays an essential role during development. Variability-aware program analysis techniques have been proposed to support developers in understanding code-level variability by analyzing the space of program variants. Such techniques are highly beneficial, e.g., when determining the impact of changes during maintenance and evolution. Two strategies have been proposed in the literature to make existing program analysis techniques variability-aware:(i) program analysis can be lifted by considering variability already in the parsing stage; or(ii) analysis can be delayed by considering and recovering variability only when needed. Both strategies have advantages and disadvantages, however, a systematic comparison is still missing. The contributions of this paper are an in-depth comparison of SPLLIFT and COACH, two existing approaches representing these two strategies, and an analysis and discussion of the trade-offs regarding precision and run-time performance. The results of our experiment show that the delayed strategy is significantly faster but typically less precise. Our findings are intended for researchers and practitioners deciding which strategy to select for their purpose and context.",0,1,0,static program analysis+symbolic execution+pointer analysis,
889,889,"Learning based testing the sliding window behavior of tcp implementations. We develop a learning-based testing framework for register automaton models that can express the windowing behavior of TCP, thereby presenting the first significant application of register automata learning to realistic software for a class of automata with Boolean-arithmetic constraints over data values. We have applied our framework to TCP implementations belonging to different operating systems and have found a violation of the TCP specification in Linux and Windows. The violation has been confirmed by Linux developers.",0,1,0,tcp+software+linux,
890,890,"Semantic modelling of android malware for effective malware comprehension detection and classification. Malware has posed a major threat to the Android ecosystem. Existing malware detection tools mainly rely on signature- or feature- based approaches, failing to provide detailed information beyond the mere detection. In this work, we propose a precise semantic model of Android malware based on Deterministic Symbolic Automaton (DSA) for the purpose of malware comprehension, detection and classification. It shows that DSA can capture the common malicious behaviors of a malware family, as well as the malware variants. Based on DSA, we develop an automatic analysis framework, named SMART, which learns DSA by detecting and summarizing semantic clones from malware families, and then extracts semantic features from the learned DSA to classify malware according to the attack patterns. We conduct the experiments in both malware benchmark and 223,170 real-world apps. The results show that SMART builds meaningful semantic models and outperforms both state-of-the-art approaches and anti-virus tools in malware detection. SMART identifies 4583 new malware in real-world apps that are missed by most anti-virus tools. The classification step further identifies new malware variants and unknown families.",0,1,0,malicious codes+malwares+android,
891,891,"Spt storyboard programming tool. We present Spt, a tool that helps programmers write low-level data-structure manipulations by combining various forms of insights such as abstract and concrete input-output examples as well as implementation skeletons. When programmers write such manipulations, they typically have a clear high-level intuition about how the manipulation should work, but implementing efficient low-level pointer manipulating code is error-prone. Our tool aims to bridge the gap between the intuition and the corresponding implementation by automatically synthesizing the implementation. The tool frames the synthesis problem as a generalization of an abstract-interpretation based shape analysis, and represents the problem as a set of constraints which are solved efficiently by the Sketch solver. We report the successful evaluation of our tool on synthesizing several linked list and binary search tree manipulations.",0,1,0,programming languages+software developer+abstract interpretation,
892,892,"Architectures and experiences in testing iot communications. A typical architecture for Internet of Things (IoT) systems consist of simple embedded systems (e.g., sensor nodes), collecting data, connected to a more traditional backend system providing end-user services. In our experience, compared to more general software testing, the most specific element in IoT testing is that of highly distributed and dynamic communication of IoT nodes, such as sensors. Further challenges for IoT reliability testing come from the architectural elements of IoT gateways connecting the devices to the networks, and the networks themselves which can host all other type of traffic at the same time. In this paper, we describe our experiences with these different aspects of IoT communications testing, and related test architectures.",0,1,0,internet of things+embedded systems+software,
893,893,"Selecting an appropriate framework for value based requirements prioritization. There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.",0,1,0,functional requirement+software systems+requirements engineers,
894,894,"Architecture centric support for adaptive service collaborations. In today's volatile business environments, collaboration between information systems, both within and across company borders, has become essential to success. An efficient supply chain, for example, requires the collaboration of distributed and heterogeneous systems of multiple companies. Developing such collaborative applications and building the supporting information systems poses several engineering challenges. A key challenge is to manage the ever-growing design complexity. In this article, we argue that software architecture should play a more prominent role in the development of collaborative applications. This can help to better manage design complexity by modularizing collaborations and separating concerns. State-of-the-art solutions, however, often lack proper abstractions for modeling collaborations at architectural level or do not reify these abstractions at detailed design and implementation level. Developers, on the other hand, rely on middleware, business process management, and Web services, techniques that mainly focus on low-level infrastructure.   To address the problem of managing the design complexity of collaborative applications, we present Macodo. Macodo consists of three complementary parts: (1) a set of abstractions for modeling adaptive collaborations, (2) a set of architectural views, the main contribution of this article, that reify these abstractions at architectural level, and (3) a proof-of-concept middleware infrastructure that supports the architectural abstractions at design and implementation level. We evaluate the architectural views in a controlled experiment. Results show that the use of Macodo can reduce fault density and design complexity, and improve reuse and productivity. The main contributions of this article are illustrated in a supply chain management case.",0,1,0,engineering+software+software architecture,
895,895,"Towards verification of multicore motor drive controllers in aerospace. It is a known fact that development of models on the design stage of a product, constitutes a highly important stage proving early evidence of error absence for the proposed artifact. Meanwhile, advances in the embedded systems domain push for rapid architecture product changes based on current state-of-the-art solutions. Multicore systems have exhibit enormous benefits due to parallelization of task execution, increasing availability of resources in multiple domains such as the automotive and telecommunication. Such a premise creates the need to invest into new verification methodologies that will re-assure the safe and efficient transition of new solutions like multicores, especially in the demanding aerospace world. In this paper we describe current challenges and trends on the development of safe and efficient methods for power controllers’ verification in multicore-based hardware platforms, such as motor-drive applications. We outline current industrial practices and describe common toolsets, workflows and techniques used in the aerospace domain. Then our discussion focus on formal verification techniques that could provide efficient solutions for verifying power control algorithms in aerospace applications. We conclude with remarks about an ongoing verification effort for power control of a multicore-based motor drive towards producing certification evidence.",0,1,0,formal verifications+verification techniques+power control algorithm,
896,896,"Swarm verification techniques. The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances and new theoretical insights, but it has also benefitted from the steady increase in processing speeds and main memory sizes on standard computers. The steady increase in processing speeds, though, ended when chip-makers started redirecting their efforts to the development of multicore systems. For the near-term future, we can anticipate the appearance of systems with large numbers of CPU cores, but without matching increases in clock-speeds. We will describe a model checking strategy that can allow us to leverage this trend and that allows us to tackle significantly larger problem sizes than before.",0,1,0,model checking+model checker+model checking tools,
897,897,"Smt based checking of soloist over sparse traces. SMT solvers have been recently applied to bounded model checking and satisfiability checking of metric temporal logic. In this paper we consider SOLOIST, an extension of metric temporal logic with aggregate temporal modalities; it has been defined based on a field study on the use of specification patterns in the context of the provisioning of service-based applications. We apply bounded satisfiability checking to perform trace checking of service execution traces against requirements expressed in SOLOIST. In particular, we focus on sparse traces, i.e., traces in which the number of time instants when events occur is very low with respect to the length of the trace.

The main contribution of this paper is an encoding of SOLOIST formulae into formulae of the theory of quantifier-free integer difference logic with uninterpreted function and predicate symbols. This encoding paves the way for efficient checking of SOLOIST formulae over sparse traces using an SMT-based verification toolkit. We report on the evaluation of the proposed encoding, commenting on its scalability and its effectiveness.",0,1,0,model checking+execution trace+bounded model checking,
898,898,"Moped a mobile open platform for experimental design of cyber physical systems. Due to the increasing importance of cyber-physical and embedded systems in industry, there is a strong demand for engineers with an updated knowledge on contemporary technology and methods in the area. This is a challenge for educators, in particular when it comes to creating hands-on experiences of real systems, due to their complexity and the fact that they are usually proprietary. Therefore, a laboratory environment that is representative of the industrial solutions is needed, with a focus on software and systems engineering issues. This paper describes such an environment, called the Mobile Open Platform for Experimental Design (MOPED). It consists of a model car chassis, equipped with a network of three control units based on standard hardware, and running the automotive software standard AUTOSAR, which consists of operating system, middleware, and application software structures. It is equipped with various sensors and actuators, and is open to extensions both in hardware and software. It also contains elements of future systems, since it allows connectivity to cloud services, development of federated embedded systems, and continuous deployment of new functionality. In this way, the platform provides a very relevant learning environment for cyber-physical systems, today and in the future.",0,1,0,engineers+software+autosar,
899,899,"Software cost estimating for cmmi level 5 developers. Modeled final effort with estimated parameters by CMMI Level 5 teams.Examined Pearson and Spearman Rank correlation matrices for relationships.Analyzed single estimated parameters using OLS to predict final effort.When appropriate, transformed variables for OLS using natural logarithms.CMMI Level 5 regression results outperformed lower CMMI levels. This article provides analysis results of Capability Maturity Model Integrated Level 5 projects for developers earning the highest level possible, using actual software data from their initial project estimates. Since there were no measures to verify software performance, this level was used a proxy for high quality software. Ordinary least squares regression was used to predict final effort hours with initially estimated variables obviates the need to estimate growth or shrinkage for typical changes occurring in software projects, regardless of software developer (contracted or in-house). The OLS equations, or cost estimating relationship equations, were evaluated by a series of standards: statistical significance, visual inspection, goodness of fit measures, and academically set thresholds for accuracy measures used in software cost estimating: mean magnitude of relative error and prediction (for determining the percentage of records with 25%, or less, based on their magnitude of relative error score). As several initial estimated variables were strongly correlated to the reported final effort hours and each other, each variable was examined separately. Thirty records from software projects completed in 2003-2008 for the highest process maturity level were used to compute statistically significant equations with implicit growth or shrinkage in their make-up.",0,1,0,high-quality software+software cost estimation+cmmi,
900,900,"Towards artifact models as process interfaces in distributed software projects. Much effort has been spent to investigate the organization of distributed teams and their collaboration patterns. It is, however, not fully understood to which extent and how agile software processes are feasible to support distributed software projects. Practices and challenges that arise from the demands for communication are often in scope of current research. Still, it remains unclear what is necessary to monitor a project and to track its progress from a management perspective. A solution is to monitor projects and their progress on basis of the current quality of the created artifacts according to a given reference model that defines the artifacts and their dependencies. In this paper, we present an artifact model for agile methods that results from of a systematic literature review. The contribution serves as an empirically grounded definition of process interfaces to coordinate projects and to define exchanged artifacts while abstracting from the diverse local software processes.",0,1,0,agile methods+software development+software project,
901,901,"Mechanical properties of pet ptt rayon staple blended fabrics. Abstract: The mechanical properties and strain at applied loads for PET/PTT (polyester/poly trimethylene terephtha-late)/rayon staple blended fabrics, PET/rayon blended fabrics, and PET/rayon containing PU fabrics were studied. Theresult of the research shows that PET/PTT/rayon staple blended fabrics have higher bending and shear properties thanother fabrics in cases regarding the weft direction. Elastic recovery above a 30% strain of PET/PTT/rayon stapleblended fabrics is larger than that of other fabrics. PET/PTT/rayon staple blended fabrics are easily extensible at thesmall as well as the large deformation region.Keywords: mechanical properties, strain, PET/PTT, elastic recovery, elongation, low load, high load 1. 서 론 PET는 분자 구조 상 융점이 높고, 고 결정성을 가지므로기계적 특성 및 화학적 안정성이 우수하고 중합 원료도 저렴하지만, 낮은 신축성이나 염색성의 제한, 흡습성 저하 등의 단점을 가지고 있다. 최근에는 섬유 및 의류 제품 소비자들의 제품에 대한 활동성 선호로 폴리에스터나 나일론의류에 신축성이 우수한 스판덱스 커버링사와 core-spun사를 사용하고 있으나 제직, 염색 가공 공정에서 균일한 장력 관리가 어렵고, 공정 단가 및 불량 발생 비용이 증가하며, pilling 및 공정 시 유해 가스 발생 등의 문제가 발생되고 있다. 따라서 신축성 및 제반 물성이 우수한 새로운 섬유에 대한 기대와 환경오염 문제로 친환경 섬유 제조에 대한 요구가 증가되면서, 옥수수 원료가 일부 사용되는PTT(poly trimethylene terephthalate) 섬유가 상업화되기 시작했다. 이러한 PTT 섬유는 신축성, 염색성, 촉감, 방오성등이 우수하고 PET와 중합 방법 및 화학적 구조가 유사하여 기존의 PET 섬유 생산설비를 이용해 생산할 수 있는강점이 있다. PTT 섬유에 대한 연구들은 주로 연신 및 열처리가 PTT섬유에 미치는 영향[1,2], PTT 섬유의 염색에 관한 연구[3-6], PTT 필라멘트사의 물성 연구[7], PTT 직물의 제조및 물성에 대한 연구[8-11]와 편성물의 물성에 대한 연구[12,13] 등이 이루어지고 있다. 이러한 PTT 섬유 연구의 대부분은 필라멘트에 대한 것으로 스테이플을 이용한 연구는거의 없다. 본 연구에서 사용한 PET/PTT 단섬유는 PET/PTT를 50%/50%로 하여 conjugate 타입으로 복합 방사한것으로 이 섬유와 천연 섬유를 혼방한 후 염색을 하면 적정 염색 온도에서의 PET와 PTT의 수축률 차이에 의해 크림프를 형성하면서 혼방사가 수축되며 이에 따라 직물의신장성이 향상됨은 물론 신장 회복성도 좋아진다. 따라서 본 연구는 친환경적이면서 신축성, 촉감 등의 섬유 물성이 우수하여 인조 섬유와 천연 섬유의 장점과 감성이 모두 발현될 수 있도록, 직물의 혼방률 및 각 방적 공정상 공정별 적정 요인들을 조절한 PET/PTT 스테이플 혼방 직물을 개발하여 PET/PTT 스테이플 혼방 직물의 역학적 특성을 PET 혼방 직물, PET/PU(polyurethane) 혼방 직물과 비교해 보고자 한다. 특히 신축성이 우수한 PET/PTT스테이플 혼방 직물의 하중 변화와 반복 인장에 따른 신장성의 변화를 분석하여 의류 제품 사용 도중 발생될 수 있는 반복 하중 및 하중에 따른 직물의 신장 변형을 살펴보고자 한다.",0,1,0,mechanical systems+primary users+spect imaging,
902,902,"Formal patterns for multi rate distributed real time systems. Distributed real-time systems (DRTSs), such as avionics and automotive systems, are very hard to design and verify. Besides the difficulties of asynchrony, clock skews, and network delays, an additional source of complexity comes from the multirate nature of many such systems, which must implement several levels of hierarchical control at different rates. In this work we present several simple model transformations and a multirate extension of the PALS pattern which can be combined to reduce the design and verification of a virtually synchronous multirate DRTS to the much simpler task of specifying and verifying a single synchronous system. We illustrate the ideas with a multirate hierarchical control system where a central controller orchestrates control systems in the ailerons and tail of an airplane to perform turning maneuvers.",0,1,0,verification+model transformation+clock drift+distributed real time system,
903,903,"Teaching stratego to play ball optimal synthesis for continuous space mdps. Formal models of cyber-physical systems, such as priced timed Markov decision processes, require a state space with continuous and discrete components. The problem of controller synthesis for such systems then can be cast as finding optimal strategies for Markov decision processes over a Euclidean state space. We develop two different reinforcement learning strategies that tackle the problem of continuous state spaces via online partition refinement techniques. We provide theoretical insights into the convergence of partition refinement schemes. Our techniques are implemented in Open image in new window. Experimental results show the advantages of our new techniques over previous optimization algorithms of Open image in new window.",0,1,0,state space+formal model+cyber physical systems (cpss),
904,904,"Recommending code changes for automatic backporting of linux device drivers. Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history.",0,1,0,operating systems+linux+device driver,
905,905,"Teaching autonomous driving using a modular and integrated approach. Introduction: Teaching autonomous driving is a challenging task. Indeed, most existing autonomous driving teaching activities focus on a few of the technologies involved. This not only fails to provide a comprehensive coverage, but also sets a high entry barrier for students with different backgrounds. Objective: The primary objective of this study is to present a modular, integrated approach towards teaching autonomous driving. Methods: We organize the technologies used in autonomous driving into modules. This is described in the textbook we have developed as well as a series of multimedia online lectures designed to provide technical overview for each module. Once the students have understood these modules, the experimental platforms for integration we have developed allow the students to fully understand how the modules interact with each other. Results: To verify this teaching approach, we present three case studies: an introductory class on autonomous driving for students with only a basic technology background; a new session in an existing embedded systems class to demonstrate how embedded system technologies can be applied towards autonomous driving; and an industry professional training session to quickly bring up experienced engineers to work in autonomous driving. The results show that students can maintain a high interest level and make great progress by starting with familiar concepts before moving onto other modules. Conclusions: Autonomous driving is not one single technology, but rather a complex system integrating many technologies. Our modular and integrated approach is an effective method in teaching autonomous driving.",0,1,0,autonomous vehicles+embedded systems+engineers,
906,906,Glr based abstract parsing. Abstract parsing is an important step of the processing of dynamically constructed statements or string-embedded languages (such as embedded or dynamic SQL). Existing LALR-based algorithms have performance issues. To increase performance we propose to use a GLR-algorithm as a base for abstract parsing and to reuse graph-structured stack and shared packed parse forest. RNGLR-algorithm modification for abstract parsing is presented.,0,1,0,adaptive algorithms+hybrid algorithms+sql,
907,907,"Using pig as a data preparation language for large scale mining software repositories studies. Highlights? We evaluate Pig's ability to prepare data in a modular way by performing three large-scale MSR studies in detail. Our implementation can be reused by other MSR researchers. ? We compare the use of Pig and Hadoop for preparing data for MSR studies. ? We report the lessons learnt with Pig in order to assist other researchers who want to use Pig as a data preparation language in their MSR studies. The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses.",0,1,0,software engineering+software reengineering+software repositories,
908,908,"The cost of formal verification in adaptive cps an example of a virtualized server node. Cyber-physical systems (CPS) are large scale systems highly integrated with the physical environment. Given the changing nature of physical environments, CPS must be able to adapt on-line to new situations while preserving their correct operation. Correctness by construction relies on using formal tools, which suffer from a considerable computationaloverhead especially if executed on-line. As the current system model of a CPS may change to adapt to the environment, the new system model has to be verified at run-time prior to its execution to ensure that the system properties are preserved. CPS development has mainly concentrated on the design-time aspects, existing only few contributions that support on-line adaptation. We undertake a practical exercise to research on the pros and cons of formal tools to support dynamic changes at run-time. We formalize the semantics of the adaptation logic of an autonomic manager (OLIVE) that performs on-line verification for a specific application, i.e., a dynamic virtualized server system. We explore the realization of the autonomic manager using formal tools based on CLTLoc to express functional and non-functional properties of the managed system. The on-line verification manager services requests from mobile clients that might require a change inboth the running software components and server services. To establish if the adaptation preserves the temporal constraints provided in the specification, i.e., to decide whether a new client request can be serviced in the modified system, the on-line verification manager employs CLTLoc satisfiability checking. In this scenario, we then provide empirical results showing thetemporal costs of our approach.",0,1,0,software+formal verifications+verification techniques,
909,909,"Lessons in persisting object data using object relational mapping. In this article, object-relational mapping (ORM) engines in object-oriented programming are introduced. The tradeoffs that must be considered when using ORM are discussed and four lessons that will help software developers take advantage of ORM in transaction processing scenarios are provided.",0,1,0,software+software developer+unified modeling language,
910,910,"Aprove proving and disproving termination of memory manipulating c programs. AProVE is a system for automatic termination and complexity analysis of C, Java, Haskell, Prolog, and several forms of rewrite systems. The new contributions in this version of AProVE are its capabilities to prove non-termination of C programs and to handle recursive C programs, even if these programs use pointer arithmetic combined with direct memory accesses. Moreover, in addition to mathematical integers, AProVE can now also handle fixed-width bitvector integers.",0,1,0,haskell+java+c programs,
911,911,"How android developers handle evolution induced api compatibility issues a large scale study. As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them. In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in counter-measure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.",0,1,0,android platforms+software developer+static analysis,
912,912,"Detecting semantic merge conflicts with variability aware execution. In collaborative software development, changes made in parallel by multiple developers may conflict. Previous research has shown that conflicts are common and occur as textual conflicts or semantic conflicts, which manifest as build or test failures. With many parallel changes, it is desirable to identify conflicts early and pinpoint the (minimum) set of changes involved. However, the costs of identifying semantic conflicts can be high because tests need to be executed on many merge scenarios. We propose Semex, a novel approach to detect semantic conflicts using variability-aware execution. We encode all parallel changes into a single program, in which ""if"" statements guard the alternative code fragments. Then, we run the test cases using variability-aware execution, exploring all possible concrete executions of the combined program with regard to all possible merge scenarios, while exploiting similarities among the executions to speed up the process. Variability-aware execution returns a formula describing all failing merge scenarios. In our preliminary experimental study on seven PHP programs with a total of 50 test cases and 19 semantic conflicts, Semex correctly detected all 19 conflicts.",0,1,0,collaborative software development+software development+code fragments,
913,913,"A principled way to use frameworks in architecture design. In the past decade, researchers have devised many methods to support and codify architecture design. However, what hampers such methods' adoption is that these methods employ abstract concepts such as views, tactics, and patterns, whereas practicing software architects choose technical design primitives from the services offered in commercial frameworks. A holistic and more realistic approach to architecture design addresses this disconnect. This approach uses and systematically links both top-down concepts, such as patterns and tactics, and implementation artifacts, such as frameworks, which are bottom-up concepts. The Web extra at http://youtu.be/kygFOV8TqEw is a video in which Humberto Cervantes from Autonomous Metropolitan University interviews Josue Martinez Buenrrostro, a software architect at Quarksoft in Mexico City, about the design process discussed in the article ""A Principled Way to Use Frameworks in Architecture Design"".",0,1,0,http+tactics+software,
914,914,"A categorization scheme for software engineering conference papers and its application. Abstract Background In Software Engineering (SE), conference publications have high importance both in effective communication and in academic careers. Researchers actively discuss how a paper should be organized to be accepted in mainstream conferences. Aiming This work tackles the problem of generalizing and characterizing the type of papers accepted at SE conferences. Method The paper offers a new perspective in the analysis of SE literature: a categorization scheme for SE papers is obtained by merging, extending and revising related proposals from a few existing studies. The categorization scheme is used to classify the papers accepted at three top-tier SE conferences during five years (2012–2016). Results While a broader experience is certainly needed for validation and fine-tuning, preliminary outcomes can be observed relative to what problems and topics are addressed, what types of contributions are presented and how they are validated. Conclusions The results provide insights to paper writers, paper reviewers and conference organizers in focusing their future efforts, without any intent to provide judgments or authoritative guidelines.",0,1,0,software+software engineering+software reengineering,
915,915,"Quantitative projection coverage for testing ml enabled autonomous systems. Systematically testing models learned from neural networks remains a crucial unsolved barrier to successfully justify safety for autonomous vehicles engineered using data-driven approach. We propose quantitative k-projection coverage as a metric to mediate combinatorial explosion while guiding the data sampling process. By assuming that domain experts propose largely independent environment conditions and by associating elements in each condition with weights, the product of these conditions forms scenarios, and one may interpret weights associated with each equivalence class as relative importance. Achieving full k-projection coverage requires that the data set, when being projected to the hyperplane formed by arbitrarily selected k-conditions, covers each class with number of data points no less than the associated weight. For the general case where scenario composition is constrained by rules, precisely computing k-projection coverage remains in NP. In terms of finding minimum test cases to achieve full coverage, we present theoretical complexity for important sub-cases and an encoding to 0-1 integer programming. We have implemented a research prototype that generates test cases for a visual object detection unit in automated driving, demonstrating the technological feasibility of our proposed coverage criterion.",0,1,0,domain knowledge+autonomous vehicles+equivalence classes,
916,916,"The case for knowledge translation. Context: For the outcomes of systematic literature reviews to be of use for practitioners, we need to develop models for addressing the needs of Knowledge Translation (KT). Aim: To identify some of the key issues that need to be addressed by a KT process for software engineering (SE) and possible routes for achieving these. Method: We have examined some of the models used in other disciplines, and suggested a possible interpretation for software engineering. Results: We propose a model for achieving KT. Conclusions: Research with industry and commerce is needed to explore how this can be realised.",0,1,0,software+software engineering+software reengineering,
917,917,"Metamorphic model based testing applied on nasa dat an experience report. Testing is necessary for all types of systems, but becomes difficult when the tester cannot easily determine whether the system delivers the correct result or not. NASA's Data Access Toolkit allows NASA analysts to query a large database of telemetry data. Since the user is unfamiliar with the data and several data transformations can occur, it is impossible to determine whether the system behaves correctly or not in full scale production situations. Small scale testing was already conducted manually by other teams and unit testing was conducted on individual functions. However, there was still a need for full scale acceptance testing on a broad scale.   We describe how we addressed this testing problem by applying the idea of metamorphic testing [1]. Specifically, we base it on equivalence of queries and by using the system itself for testing. The approach is implemented using a model-based testing approach in combination with a test data generation and test case outcome analysis strategy. We also discuss some of the issues that were detected using this approach.",0,1,0,system testing+test data generation+model based testing,
918,918,"Users the hidden software product quality experts a study on how app users report quality aspects in online reviews. [Context and motivation] Research on eliciting requirements from a large number of online reviews using automated means has focused on functional aspects. Assuring the quality of an app is vital for its success. This is why user feedback concerning quality issues should be considered as well [Question/problem] But to what extent do online reviews of apps address quality characteristics? And how much potential is there to extract such knowledge through automation? [Principal ideas/results] By tagging online reviews, we found that users mainly write about ""usability"" and ""reliability"", but the majority of statements are on a subcharacteristic level, most notably regarding ""operability"", ""adaptability"", ""fault tolerance"", and ""interoperability"". A set of 16 language patterns regarding ""usability"" correctly identified 1,528 statements from a large dataset far more efficiently than our manual analysis of a small subset. [Contribution] We found that statements can especially be derived from online reviews about qualities by which users are directly affected, although with some ambiguity. Language patterns can identify statements about qualities with high precision, though the recall is modest at this time. Nevertheless, our results have shown that online reviews are an unused Big Data source for quality requirements.",0,1,0,quality requirements+quality attributes+software product quality,
919,919,"Handling uncertainty in self adaptive software using self learning fuzzy neural network. Uncertainty has posed great challenges to the development and application of self-adaptive software (SAS). To handle uncertainty underneath SAS, the technique of fuzzy control method has been employed to model and develop SASs. Practices prove that fuzzy logic is powerful to handle uncertainty, especially fuzzy uncertainty, within SAS. However, fuzzy control based SAS needs software developers to set fuzzy rules of the system, which is rather experience-dependent and heavily increases development burden of software engineers. To some extent, the effect of handling uncertainty depends on experiences of software engineers. Besides, fuzzy control based SAS realizes self-adaptation logic using fixed fuzzy rules, lacking the ability to adapt to large changes (e.g., scenario switches). In order to make up the above shortages of fuzzy control based SAS, we present the Fuzzy-Learning SAS, attempting to construct self-adaptation logic using self-learning fuzzy neural network. By incorporating the model of fuzzy neural network, Fuzzy-Learning models SAS with two feedback loops, i.e., the self-adaptation loop and the self-learning loop, enabling SASs with the ability of adapting to dynamic changes and the ability of automatically constructing self-adaptation logic. We have experimentally evaluated effectiveness and efficiency of Fuzzy-Learning SAS with a motivating example. The experiment results confirmed that Fuzzy-Learning SAS can improve the effect of handling uncertainty and alleviate the development burden of software engineers with ill knowledge of fuzzy control. Besides, Fuzzy-Learning SAS can adapt to large changes (e.g., scenario switches) with the self-learning ability.",0,1,0,software+software developer+self-adaptive software,
920,920,"Framing the conundrum of total cost of ownership of open source software. This paper reflects the results of phase I of our study on the total cost of ownership (TCO) of open source software adoption. Not only have we found TCO to be an intriguing issue but it is contentious, baffling and each company approaches it in a distinctive manner (and sometimes not at all). In effect it is a conundrum that needs unpacking before it can be explained and understood. Our paper discusses the components of TCO as total cost of ownership and total cost of acquisition (and besides). Using this broad dichotomy and its various components we then analyze our data to make sense of procurement decisions in relation to open source software in the public sector and private companies.",0,1,0,software+open source software+open source system,
921,921,"Behavior verification of autonomous robot vehicle in consideration of errors and disturbances. Reliability of embedded systems has become important along with their popularization in many fields. Model checking is one of formal methods which has been successfully applied to many systems for ensuring reliabilities. In this study, we handle an autonomous robot vehicle in consideration of errors and disturbances, as an concrete example of embedded systems. Behaviors of the vehicle become essentially probabilistic in such an environment. We show that such probabilistic behaviors can be modeled in Markov Decision Processes (MDPs). To demonstrate the applicability of the modeling, we show experimental verification using the probabilistic model checker PRISM. Note that the approaches are still in preliminary stages. It is intended to establish qualitative and quantitative validations from the standpoint of design phase.",0,1,0,formal methods+model checking+model checker,
922,922,"Games for requirements engineers analysis and directions. The requirements engineering (RE) discipline keeps evolving to cope with increasingly complex systems and shorter development cycles. Using a lightweight analysis framework, we review the current landscape of games for RE and provide guidance for the practitioner interested in improving their skills using innovative game-based RE.",0,1,0,engineering+requirements analysis+requirements engineering,
923,923,"Available car parking space detection from webcam by using adaptive mixing features. This paper presents a robust approach for detection of available car parking spaces. With low quality of video camera as webcam and dynamic change of light around the car parking, it is hard to accurately detect or recognize the cars. Moreover the proposed appearance-based approach is efficient than recognition-based approach because it do not need to learn a huge of multi-view objects. In this paper, we propose adaptive background model-based object detection with dynamic mixing features of masked-area and edge orientation histogram (EOH) density. The average variance of variance of intensity change for dynamic background model is used to change ratio of mixing features dynamically. The masked-area density is density of predefined area of a parking slot that is weighted by Gaussian mask to robust density computation and the edge orientation histogram (EOH) density is density of the EOH in the predefined area that can be used under low contrast image as night scene. The experiments are performed both in simulation model and real scenes. The results show the proposed approach can handle dynamic change of light efficiently.",0,1,0,color histogram+background subtraction+background subtraction method,
924,924,"Automatic approval prediction for software enhancement requests. Software applications often receive a large number of enhancement requests that suggest developers to fulfill additional functions. Such requests are usually checked manually by the developers, which is time consuming and tedious. Consequently, an approach that can automatically predict whether a new enhancement report will be approved is beneficial for both the developers and enhancement suggesters. With the approach, according to their available time, the developers can rank the reports and thus limit the number of reports to evaluate from large collection of low quality enhancement requests that are unlikely to be approved. The approach can help developers respond to the useful requests more quickly. To this end, we propose a multinomial naive Bayes based approach to automatically predict whether a new enhancement report is likely to be approved or rejected. We acquire the enhancement reports of open-source software applications from Bugzilla for evaluation. Each report is preprocessed and modeled as a vector. Using these vectors with their corresponding approval status, we train a Bayes based classifier. The trained classifier predicts approval or rejection of the new enhancement reports. We apply different machine learning and neural network algorithms, and it turns out that the multinomial naive Bayes classifier yields the highest accuracy with the given dataset. The proposed approach is evaluated with 40,000 enhancement reports from 35 open source applications. The results of tenfold cross validation suggest that the average accuracy is up to 89.25%.",0,1,0,naive bayes+software+open source software,
925,925,"The role of quality attributes in service based systems architecting a survey. Quality attributes (QA) play a fundamental role when architecting software systems. However, in contrast to QA in traditional software systems, the role of QA when architecting service-based systems (SBS) has not yet been studied in depth. Thus, we conducted a descriptive survey to explore how QA are treated during the architecting of SBS. Data were collected using an online questionnaire targeted at participants with architecting experience. Our survey shows that QA and functional requirements of SBS are mostly considered equally important. Also, QA are usually treated explicitly rather than implicitly. Furthermore, dependability and performance appear to be the most important QA in the context of SBS. Our results partially show that general findings on QA also apply to the domain of SBS. On the other hand, we did not find a confirmation that QA are primary drivers for the architecting of SBS, or that certain application domains would focus on particular QA.",0,1,0,software systems+architecting+service-based systems,
926,926,"A data driven workflow based on structured tokens petri net. Business processes design and implementation within a company are mainly based on the specification of actors and their different tasks. Data and general information are transmitted in a very specific organization among actors, applications and the information system, which constitute a workflow. In this paper, we present an approach for workflow process modeling. The model is in charge of representing both control flow and shared data in the workflow process, and it can be analysed to verify its correctness before implementation. This workflow modeling approach has been implemented into Opus system that provides a set of graphical interfaces to model and execute the business process tasks. The system also provides a workflow engine that grants automatic workflow processing by interpreting the workflow process. Keywords—Workflow modeling; Workflow management sys- tem; Petri Nets; Data-driven approach; Structured token.",0,1,0,business process management+process modeling+business process design,
927,927,"Vision paper towards model based energy testing. Today, energy consumption is one of the major challenges for optimisation of future software applications and ICT infrastructures. To develop software w.r.t. its energy consumption, testing is an essential activity, since testing allows quality assurance and thus, energy consumption reduction during the software's development. Although first approaches measuring and predicting software's energy consumption for its execution on a specific hardware platform exist, no model-based testing approach has been developed, yet. In this paper we present our vision of a model-based energy testing approach that uses a combination of abstract interpretation and run-time profiling to predict the energy consumption of software applications and to derive energy consumption test cases.",0,1,0,software+abstract interpretation+model based testing,
928,928,"Ambient awareness of build status in collocated software teams. We describe the evaluation of a build awareness system that assists agile software development teams to understand current build status and who is responsible for any build breakages. The system uses ambient awareness technologies, providing a separate, easily perceived communication channel distinct from standard team workflow. Multiple system configurations and behaviours were evaluated. An evaluation of the system showed that, while there was no significant change in the proportion of build breakages, the overall number of builds increased substantially, and the duration of broken builds decreased. Team members also reported an increased sense of awareness of, and responsibility for, broken builds and some noted the system dramatically changed their perception of the build process making them more cognisant of broken builds.",0,1,0,software development+software teams+agile software development,
929,929,"Inductive program synthesis over noisy data. We present a new framework and associated synthesis algorithms for program synthesis over noisy data, i.e., data that may contain incorrect/corrupted input-output examples. This framework is based on an extension of finite tree automata called state-weighted finite tree automata. We show how to apply this framework to formulate and solve a variety of program synthesis problems over noisy data. Results from our implemented system running on problems from the SyGuS 2018 benchmark suite highlight its ability to successfully synthesize programs in the face of noisy data sets, including the ability to synthesize a correct program even when every input-output example in the data set is corrupted.",0,1,0,regular languages+tree automata+automata,
930,930,"Two generalisations of rosu and chen s trace slicing algorithm a. Rosu and Chen’s trace analysis algorithm identifies activity streams in a monitored application based on data (such as memory locations) and groups events accordingly into slices. It can be generalised to assign several such activity streams to the same slice, even if data is unrelated. This is useful for monitoring scheduling algorithms, which linearise activity streams that are not necessarily related. The algorithm can be generalised further to impose constraints on the generated slices such that, for example, each trace relates a high-priority activity to a low-priority activity. There are no limitations on constraints other than that constraint solvers efficient enough for runtime analysis need to be available.",0,1,0,constraint solvers+memory locations+execution trace,
931,931,"Software ecosystems architectural health challenges x practices. Over time many software ecosystems have achieved success. Several organizations are opening their software projects for external businesses, creating an multi-organizational government to development their software platform The software architecture has an important participation in this success. In this context, there are some studies describing architectural challenges for software ecosystems, but little research is investigating how these challenges are being faced by software ecosystems organizations. This paper presents an initial investigation how open source software (OSS) ecosystems have faced several architectural challenges. We conducted interviews with three architects of different OSS ecosystems and gathered some architectural practices to lead with challenges. We also analyzed how these architectural practices have influenced the software ecosystem health, introducing the concept of Software Ecosystems Architectural Health.",0,1,0,open source software+software architecture+software project,
932,932,"An application of adaptive random sequence in test case prioritization. Test case prioritization aims to schedule test cases in a certain order such that the effectiveness of regression testing can be improved. Prioritization using random sequence is a basic and simple technique, and normally acts as a benchmark to evaluate other prioritization techniques. Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box code coverage information that is normally related to the test execution history of previous versions. In this paper, we propose several ARS-based prioritization techniques using black-box information. The proposed techniques schedule test cases based on the string distances of the input data, without referring to the execution history. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization. In addition, as compared with an existing blackbox prioritization technique, the new techniques have similar fault-detection effectiveness but much lower computation overhead, and thus are more cost-effective.",0,1,0,regression testing+test case prioritization+regression test selection,
933,933,Scientific software testing goes serverless creating and invoking metamorphic functions. Our function-as-a-service (FaaS) framework transformed end users' questions into automated tests for scientific software. Our case study illustrates the FaaSification of scientific software testing and the importance of value-driven evaluations by focusing on real-world defect detection.,0,1,0,software+system testing+software project,
934,934,"Defects4j as a challenge case for the search based software engineering community. Defects4J is a collection of reproducible bugs, extracted from real-world Java software systems, together with a supporting infrastructure for using these bugs. Defects4J has been widely used to evaluate software engineering research, including research on automated test generation, program repair, and fault localization. Defects4J has recently grown substantially, both in number of software systems and number of bugs. This report proposes that Defects4J can serve as a benchmark for Search-Based Software Engineering (SBSE) research as well as a catalyst for new innovations. Specifically, it outlines the current Defects4J dataset and infrastructure, and details how it can serve as a challenge case to support SBSE research and to expand Defects4J itself.",0,1,0,engineering research+fault localization+test generations,
935,935,"Automatically an automated refactoring method and tool for improving web accessibility. Website is one of the major sources of information today and should be accessible to everyone including people with disabilities. Web accessibility becomes an important aspect of accessing a web that is set in the Web Content Accessibility Guidelines (WCAG) standard. However, so many websites that have not implemented the WCAG standard. Some of the reasons are the length of time to learn about it and difficult to understand. Therefore, many studies have been done to improve this web accessibility. The solution offered by previous researchers still uses manual code fixes. The problem that arises is time-consuming code changes because there are many web pages need to fix. The automated refactoring method is believed to be able to solve the problem. We develop a method and tool that can improve web accessibility automatically (automated refactoring), also improve the accessibility faster than refactoring manually. The implementation of Automatically is done by modifying the existing automated evaluation tool, namely HTML Code Sniffer and implementing 40 techniques from WCAG standard that can be automated. From the functional testing performed, 5 websites chosen to be tested successfully to reduce errors in the source code. Comparison of manual refactoring methods and Automatically tools is also conducted to 5 websites. Total changes or parameters performed using manual refactoring is more than using Automatically. It can be concluded that the tool is able to increase web accessibility automatically and successfully improve the web faster than manual refactoring.",0,1,0,html+source codes+refactorings,
936,936,"Bita coverage guided automatic testing of actor programs. Actor programs are concurrent programs where concurrent entities communicate asynchronously by exchanging messages. Testing actor programs is challenging because the order of message receives depends on the non-deterministic scheduler and because exploring all schedules does not scale to large programs. This paper presents Bita, a scalable, automatic approach for testing non-deterministic behavior of actor programs. The key idea is to generate and explore schedules that are likely to reveal concurrency bugs because these schedules increase the schedule coverage. We present three schedule coverage criteria for actor programs, an algorithm to generate feasible schedules that increase coverage, and a technique to force a program to comply with a schedule. Applying Bita to real-world actor programs implemented in Scala reveals eight previously unknown concurrency bugs, of which six have already been fixed by the developers. Furthermore, we show our approach to find bugs 122× faster than random scheduling, on average.",0,1,0,software bug+software defects+concurrent program,
937,937,"Timed conformance testing for orchestrated service discovery. Orchestrations are systems deployed on the Internet where there is a central component (called orchestrator) coordinating other components (called Web services), pre-existing to the orchestration design phase. Web services are made available through repositories on the Internet to orchestration designers. Service discovery refers to the activity of identifying Web services offered by third parties. We propose an approach to discover Web services by taking into account the intended behaviors of Web services as they can be inferred from the orchestrator specifications. Web services are tested with respect to those behaviors to decide whether or not they can be selected. Specifications of orchestrators are Timed Input/Output Symbolic Transition Systems. Web service intended behaviors are elicited by means of symbolic execution and projection techniques. Those behaviors can be used as test purposes for our timed symbolic conformance testing algorithm.",0,1,0,conformance testing+model based testing+symbolic execution,
938,938,"Applied bounded model checking for interlocking system designs. In this paper the verification and validation of interlocking systems is investigated. Reviewing both geographical and route-related interlocking, the verification objectives can be structured from a perspective of computer science into (1) verification of static semantics, and (2) verification of behavioural (operational) semantics. The former checks that the plant model  that is, the software components reflecting the physical components of the interlocking system  has been set up in an adequate way. The latter investigates trains moving through the network, with the objective to uncover potential safety violations. From a formal methods perspective, these verification objectives can be approached by theorem proving, global, or bounded model checking. This paper explains the techniques for application of bounded model checking techniques, and discusses their advantages in comparison to the alternative approaches.",0,1,0,model checking+bounded model checking+model-checking techniques,
939,939,"On the impact of refactoring operations on code quality metrics. Refactorings are behavior-preserving source code transformations. While tool support exists for (semi) automatically identifying refactoring solutions, applying or not a recommended refactoring is usually up to the software developers, who have to assess the impact that the transformation will have on their system. Evaluating the pros (e.g., the bad smell removal) and cons (e.g., side effects of the change) of a refactoring is far from trivial. We present RIPE (Refactoring Impact Prediction), a technique that estimates the impact of refactoring operations on source code quality metrics. RIPE supports 12 refactoring operations and 11 metrics and it can be used together with any refactoring recommendation tool. RIPE was used to estimate the impact on 8,103 metric values, for 504 refactorings from 15 open source systems. 38% of the estimates are correct, whereas the median deviation of the estimates from the actual values is 5% (with a 31% average).",0,1,0,tool support+code transformation+refactorings,
940,940,"A modular requirements engineering framework for web based toolchain integration. Requirements Engineering (RE) tools and more generally the whole Software Engineering toolchain follow the strong trend towards web-based interface. This allows the analyst to use their tools in a ""Software as a Service"" mode either from a local company server or directly in the Cloud. Such deployments also ease toolchain integration by connecting their respective API through secured web-services, possibly using specific software lifecycle interoperability standards. In this tool demonstration, we illustrate the results of the rewrite process of a major Requirements Engineering tool towards this purpose. Our tooling has the following key features: (i) it supports rich requirements models based on goal-oriented RE, (ii) it is implemented as a collaborative concept server based on Eclipse Modelling technology and (iii) it exposes a REST interface supporting model building, diagram edition, history retrieval, snapshot management, collaborative mode, user authentication and project management. The following scenarios will be demonstrated (1) collaborative edition of a shared RE model, (2) rich service composition with application lifecycle management tools and (3) easy web-component integration in third-party web interfaces.",0,1,0,software reengineering+requirements models+requirements engineering,
941,941,"Domain objects and microservices for systems development a roadmap. This paper discusses a roadmap to investigate Domain Objects being an adequate formalism to capture the peculiarity of microservice architecture, and to support Software development since the early stages. It provides a survey of both Microservices and Domain Objects, and it discusses plans and reflections on how to investigate whether a modeling approach suited to adaptable service-based components can also be applied with success to the microservice scenario.",0,1,0,software+microservices+software development,
942,942,"An axiomatic memory model for power multiprocessors. The growing complexity of hardware optimizations employed by multiprocessors leads to subtle distinctions among allowed and disallowed behaviors, posing challenges in specifying their memory models formally and accurately, and in understanding and analyzing the behavior of concurrent software. This complexity is particularly evident in the IBM® Power Architecture®, for which a faithful specification was published only in 2011 using an operational style. In this paper we present an equivalent axiomatic specification, which is more abstract and concise. Although not officially sanctioned by the vendor, our results indicate that this axiomatic specification provides a reasonable basis for reasoning about current IBM® POWER® multiprocessors. We establish the equivalence of the axiomatic and operational specifications using both manual proof and extensive testing. To demonstrate that the constraint-based style of axiomatic specification is more amenable to computer-aided verification, we develop a SAT-based tool for evaluating possible outcomes of multi-threaded test programs, and we show that this tool is significantly more efficient than a tool based on an operational specification.",0,1,0,software+multithreaded+concurrent program,
943,943,"Collaborative bug triaging using textual similarities and change set analysis. Bug triaging assigns a bug report, which is also known as a work item, an issue, a task or simply a bug, to the most appropriate software developer for fixing or implementing it. However, this task is tedious, time-consuming and error-prone if not supported by effective means. Current techniques either use information retrieval and machine learning to find the most similar bugs already fixed and recommend expert developers, or they analyze change information stemming from source code to propose expert bug solvers. Neither technique combines textual similarity with change set analysis and thereby exploits the potential of the interlinking between bug reports and change sets. In this paper, we present our approach to identify potential experts by identifying similar bug reports and analyzing the associated change sets. Studies have shown that effective bug triaging is done collaboratively in a meeting, as it requires the coordination of multiple individuals, the understanding of the project context and the understanding of the specific work practices. Therefore, we implemented our approach on a multi-touch table to allow multiple stakeholders to interact simultaneously in the bug triaging and to foster their collaboration. In the current stage of our experiments we have experienced that the expert recommendations are more specific and useful when the rationale behind the expert selection is also presented to the users.",0,1,0,software defects+bug reports+bug-fixing,
944,944,"Mining android app descriptions for permission requirements recommendation. During the development or maintenance of an Android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements. Permission requirements include two folds. First, what permissions (i.e., access to sensitive resources, e.g., location or contact list) the app needs to request. Second, how to explain the reason of permission usages to users. In this paper, we focus on the multiple challenges that developers face when creating permission-usage explanations. We propose a novel framework, CLAP, that mines potential explanations from the descriptions of similar apps. CLAP leverages information retrieval and text summarization techniques to find frequent permission usages. We evaluate CLAP on a large dataset containing 1.4 million Android apps. The evaluation results outperform existing state-of-the-art approaches, showing great promise of CLAP as a tool for assisting developers and permission requirements discovery.",0,1,0,privacy requirements+access rights+access permissions,
945,945,"Failure is a four letter word a parody in empirical research. Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though.   Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion.   Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies.   Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice.   Conclusions: With the abundance of software development data, even the simplest methods can produce ""actionable"" results.",0,1,0,software developer+software systems+software development,
946,946,"Checking deadlock freedom of parametric component based systems. We propose an automated method for computing inductive invariants used to proving deadlock freedom of parametric component-based systems. The method generalizes the approach for computing structural trap invariants from bounded to parametric systems with general architectures. It symbolically extracts trap invariants from interaction formulae defining the system architecture. The paper presents the theoretical foundations of the method, including new results for the first order monadic logic and proves its soundness. It also reports on a preliminary experimental evaluation on several textbook examples.",0,1,0,architecture designs+system architectures+software component,
947,947,"Adapting our view of software adaptation an architectural perspective keynote. Engineers frequently neglect to carefully consider the impact of adaptation on a software system. As a result, the software system's architectural design sooner, rather than later, begins to deviate from the original designers' intent and to decay through unplanned introduction of new and/or invalidation of existing design decisions. For systems that are intended to be (self-)adaptive, this problem can be even more pronounced. A solution that was proposed over a decade ago was to keep the architectures of (self-)adaptive systems in sync with their implementations through carefully engineered implementation frameworks, and to allow implementation-level adaptations only via carefully controlled architecture-level operations. However, many approaches to (self-)adaptive software do not explicitly consider the system's architecture as the starting point for adaptation and, more generally, developers change systems in seemingly arbitrary ways all the time. This begs the question: What is the impact of system changes on a system's architecture in a general case? This keynote talk presents the results of an on-going study that has tried to shed light on this question. To date, the study has involved around 30 open-source systems and, in several cases, large numbers of versions of a given system. The keynote discusses and illustrates the challenges in extracting the architecture of a system from its implementation artifacts, the concrete problems posed by architectural decay, the difficulties of tracking the architectural impact of implementation-level changes, and the occasional arbitrariness with which the adaptation of real, widely-used software systems is approached. The keynote then identifies several promising research opportunities that present themselves for dealing with these problems in (self-)adaptive systems.",0,1,0,software+open source system+software systems,
948,948,"Siri write the next method. Code completion is one of the killer features of Integrated Development Environments (IDEs), and researchers have proposed different methods to improve its accuracy. While these techniques are valuable to speed up code writing, they are limited to recommendations related to the next few tokens a developer is likely to type given the current context. In the best case, they can recommend a few APIs that a developer is likely to use next. We present FeaRS, a novel retrieval-based approach that, given the current code a developer is writing in the IDE, can recommend the next complete method (i.e., signature and method body) that the developer is likely to implement. To do this, FeaRS exploits ""implementation patterns"" (i.e., groups of methods usually implemented within the same task) learned by mining thousands of open source projects. We instantiated our approach to the specific context of Android apps. A large-scale empirical evaluation we performed across more than 20k apps shows encouraging preliminary results, but also highlights future challenges to overcome.",0,1,0,software developer+open source system+open source projects,
949,949,"Assessment of overetch and polysilicon film properties through on chip tests. Two main uncertainty sources can affect the response of polysilicon MEMS during standard on-chip measurements: the overetch induced by the deep reactive-ion etching process; the mechanical properties of the aggregate of silicon grains. The former one can be reduced by finer fabrication techniques, not adopted indeed in mass production processes, while the latter one is related to the length-scale of the devices.
Due to the increasing miniaturization, the width of some MEMS components may become comparable to that of a silicon grain and the relevant effective mechanical properties can vary significantly from one device to another. In this work, through on-chip tests we investigate the response of polysilicon films using standard electrostatic actuation/sensing. The results of such experimental campaign are then compared to an analytical reduced-order model of the structure, and to coupled electro-mechanical simulations accounting for the stochastic morphology of the silicon film. These two models are adopted to bilaterally bound the experimental data up to pull-in, and to assess the scattering induced by the random orientation of the crystal lattice of each grain in digital Voronoi tessellations of the slender parts of the devices.",0,1,0,mems+electrostatic actuation+pull-in,
950,950,"Improving context awareness in self adaptation using the dynamico reference model. Self-adaptation mechanisms modify target systems dynamically to address adaptation goals, which may evolve continuously due to changes in system requirements. These changes affect values and thresholds of observed context variables and monitoring logic, or imply the addition and/or deletion of context variables, thus compromising self-adaptivity effectiveness under static monitoring infrastructures. Nevertheless, self-adaptation approaches often focus on adapting target systems only rather than monitoring infrastructures. Previously, we proposed DYNAMICO, a reference model for self-adaptive systems where adaptation goals and monitoring requirements change dynamically. This paper presents an implementation of DYNAMICO comprising our SMARTERCONTEXT monitoring infrastructure and QoS-CARE adaptation framework in a self-adaptation solution that maintains its context-awareness relevance. To evaluate our reference model we use self-adaptive system properties and the Znn.com exemplar to compare the Rainbow system with our DYNAMICO implementation. The results of the evaluation demonstrate the applicability, feasibility, and effectiveness of Dynamico, especially for self-adaptive systems with context-awareness requirements.",0,1,0,quality requirements+functional requirement+self-adaptive system,
951,951,"Measuring the degree of service orientation in proprietary soa systems. According to a survey conducted by Forrester Research in 2008, at least 44% of North American, European, and Asian-Pacific enterprises have adopted SOA (Service-oriented Architecture), and at least 63% would adopt it by the end of 2008. A more recent survey by Forrester also shows that SOA adoption remains strong in 2010. Nevertheless, there are many misconceptions about SOA, which could lead to sub optimal implementation of the paradigm. This paper is not about whether to adopt or not to adopt SOA. Instead, this paper proposes a method on how to judge an SOA implementation from architectural point of view. More specifically, we evaluate the extent to which proprietary SOA systems conform to the principles of service orientation. To this aim, a framework of Degree of Service Orientation (DoSO) has been developed. This framework is applied to nine proprietary SOA systems and the results show that, on average, the degree of service orientation is rather low. Experts' evaluation on the usefulness of the framework is also discussed.",0,1,0,service architecture+architectural models+service-oriented architecture (soa),
952,952,"Smocor a smart mobile contact recommender based on smart phone data. This paper presents SMoCoR, a smart mobile contact recommender based on smart phone data. It recommends the most appropriate way to contact friends according to friends' current condition. In no emergency condition, SMoCoR achieves two goals. First, the recommended contacts disturb friends least, that means, it will tell whether it's suitable to call friends. Second, the recommended contact makes the communication information accessible to the friends and gets replies from the friends as soon as possible. For the goals above, SMoCoR takes friends' calendar data and smart phone data as inputs, and after several steps of calculation it will recommend a list of contacts ranked by intelligent algorithm according to the appropriateness. The experimental results based on real-user data show that SMoCoR provides an efficient and accurate means for contact recommendation.",0,1,0,recommendation+communication+contracts,
953,953,"The role of catalogues of threats and security controls in security risk assessment an empirical study with atm professionals. [Context and motivation] To remedy the lack of security expertise, industrial security risk assessment methods come with catalogues of threats and security controls. [Question/problem] We investigate in both qualitative and quantitative terms whether the use of catalogues of threats and security controls has an effect on the actual and perceived effectiveness of a security risk assessment method. In particular, we assessed the effect of using domain-specific versus domain-general catalogues on the actual and perceived efficacy of a security risk assessment method conducted by non-experts and compare it with the effect of running the same method by security experts but without catalogues.",0,1,0,data security+security analysis+security controls,
954,954,"Expressing aspectual interactions in design experiences in the slot machine domain. In the context of an industrial project we are implementing the software of a casino slot machine. This software has a significant amount of cross-cutting concerns that depend on, and interact with each other, as well as with the modular concerns.We therefore wish to express our design using an appropriate Aspect-Oriented Modeling methodology and notation. We evaluated two of the most mature methodologies: Theme/UML and WEAVR, to establish their suitability. Remarkably, neither of these allow us to express any of the dependencies and interactions to our satisfaction. In both cases, half of the interaction types cannot be expressed at all while the other half need to be expressed using a workaround that hides the intention of the design. As a result, we consider both methodologies and notations unsuitable for expressing the dependencies and interactions present in the slot machine domain. In this paper we describe our evaluation experience.",0,1,0,aspect-oriented+unified modeling language+crosscutting concern,
955,955,"Automatically exploring tradeoffs between software output fidelity and energy costs. Data centers account for a significant fraction of global energy consumption and represent a growing business cost. Most current approaches to reducing energy use in data centers treat it as a hardware, compiler, or scheduling problem. This article focuses instead on the software level, showing how to reduce the energy used by programs when they execute. By combining insights from search-based software engineering, mutational robustness, profile-guided optimization, and approximate computing, the Producing Green Applications Using Genetic Exploration ( PowerGAUGE ) algorithm finds variants of individual programs that use less energy than the original. We apply hardware, software, and statistical techniques to manage the complexity of accurately assigning physical energy measurements to particular processes. In addition, our approach allows, but does not require, relaxing output quality requirements to achieve greater non-functional improvements. PowerGAUGE optimizations are validated using physical performance measurements. Experimental results on PARSEC benchmarks and two larger programs show average energy reductions of 14% when requiring the preservation of original output quality and 41% when allowing for human-acceptable levels of error.",0,1,0,quality requirements+software engineering+software reengineering,
956,956,"On accuracy performance tradeoff frameworks for energy saving models and review. A class of research, which explores energy saving via the accuracy-performance tradeoff ability of applications with soft outputs, is emerging. Many such applications implement various notions of precision improvement in their iterative computations and output approximated results. Computing more precisely often requires greater computational effort, hence there is a tradeoff space between precision and cost as long as the output accuracy remains within a targeted bound. This paper reviews the state-of-the-art accuracy tradeoff frameworks, and presents abstract model representations to facilitate our comparison of these frameworks in terms of robustness, performance, profiling, run-time feedback control, and specification of accuracy metrics.",0,1,0,energy conservation+software frameworks+energy savings,
957,957,"Multimedia based instructional development bubble sort visualization. This paper is intended to develop a bubble sort algorithm visualization for an Algorithm and Programming course in computer education. This learning model should encourage student's ability to learn the variety of sorting, such as selection sort, merge sort. etc. This research consists of 6 steps in visualization development such as need assessment, front-end analysis, design, development, implementation, and evaluation. During the testing step, the application is run and checked to confirm that it performs exactly what the author has intended and the students can learn bubble sorting algorithm by studying the visualization. Research findings indicated that the students increased their motivation and ability to create sorting visualitation application. Based on the data of process and product competencies taken from the evaluation, 85 per cent of the students were competent in sorting visualization programming and there were 15 per cent of the students were not competent.",0,1,0,education+programming course+programming languages,
958,958,"Generating evil test strings for regular expressions. Regular expressions are a powerful string processing tool. However, they are error-prone and receive little error checking from the compiler as most regular expressions are syntactically correct. This paper describes EGRET, a tool for generating evil test strings for regular expressions. EGRET focuses on common mistakes made by developers when creating regular expressions and develops test strings that expose these errors. EGRET has found errors in 284 out of 791 regular expressions. Prior approaches to test string generation have traversed all possible paths in the equivalent nondeterministic finite state automaton leading to the generation of too many strings. EGRET keeps the set of test strings to a manageable number: Fewer than 100 test strings were generated for 96% of the regular expressions, a manageable 307 test strings were generated for the most complex regular expression.",0,1,0,deterministic finite automata+regular-expression matching+compiler,
959,959,"Combining string abstract domains for javascript analysis an evaluation. Strings play a central role in JavaScript and similar scripting languages. Owing to dynamic features such as the eval function and dynamic property access, precise string analysis is a prerequisite for automated reasoning about practically any kind of runtime property. Although the literature presents a considerable number of abstract domains for capturing and representing specific aspects of strings, we are not aware of tools that allow flexible combination of string abstract domains. Indeed, support for string analysis is often confined to a single, dedicated string domain. In this paper we describe a framework that allows us to combine multiple string abstract domains for the analysis of JavaScript programs. It is implemented as an extension of SAFE, an open-source static analysis tool. We investigate different combinations of abstract domains that capture various aspects of strings. Our evaluation suggests that a combination of a few, simple abstract domains suffice to outperform the precision of state-of-the-art static analysis tools for JavaScript.",0,1,0,javascript+abstract domains+static analysis,
960,960,"Sf pmipv6. Highlights? We develop a secure fast handover mechanism for Proxy Mobile IPv6 networks. ? SF-PMIPv6 can resist various attacks. ? SF-PMIPv6 outperforms all existing schemes in terms of packet loss, authentication latency, and handoff latency. An efficient mobility management mechanism is one of the major challenges for ubiquitous computing. Recently, the IETF NETLMM working group proposed Proxy Mobile IPv6 (PMIPv6), a network-based localized mobility management protocol to support mobility management without the participation of mobile nodes (MNs) in any mobility-related signaling. Unfortunately, PMIPv6 still suffers from high packet losses and long authentication latency during handover. To address these issues, we propose a secure authentication mechanism and fast handover scheme called SF-PMIPv6 for PMIPv6 networks. The scheme provides low handover latency, supports local authentication procedures, resolves the packet loss problem, and deals with out-of-sequence packets. Moreover, SF-PMIPv6 is a robust authentication scheme that resists various attacks. Our simulation results demonstrate that it provides a better solution than existing schemes.",0,1,0,handover latency+fast handovers+handover schemes+handoff latency,
961,961,"Broadcast psi calculi with an application to wireless protocols. Psi-calculi is a parametric framework for the extensions of pi-calculus, with arbitrary data structures and logical assertions for facts about data. In this paper we add primitives for broadcast communication in order to model wireless protocols. The additions preserve the purity of the psi-calculi semantics, and we formally prove the standard congruence and structural properties of bisimilarity. We demonstrate the expressive power of broadcast psi-calculi by modelling the wireless ad hoc routing protocol LUNAR and verifying a basic reachability property.",0,1,0,wireless+communication protocols+routing protocols,
962,962,"Tackling the requirements jigsaw puzzle. A key challenge during stakeholder meetings is that of presenting the requirements and conflicts to stakeholders in a way that fosters co-responsibility and co-ownership regarding the conflicts and their resolution. In this paper, we propose a jigsaw puzzle metaphor to make identified conflicts explicit as well as an associated method to utilise this metaphor during stakeholder meetings. The metaphor provides an easy to understand language for stakeholders from otherwise diverse backgrounds. It enables stakeholders to work with a well-understood concept - that of building a system from misshapen pieces. These characteristics foster communication and team work, which improve commitment of stakeholders in co-authoring of requirements and co-responsibility in conflict handling. The gamification of conflict resolution also promotes a relaxed environment, which in turn improves team cooperation and creativity. Our experience in three user studies demonstrates that the jigsaw puzzle indeed improves such co-responsibility and co-ownership when compared with typical text-based representations of requirements.",0,1,0,system requirements+quality requirements+functional requirement,
963,963,"Extending abstract interpretation to dependency analysis of database applications. Dependency information (data- and/or control-dependencies) among program variables and program statements is playing crucial roles in a wide range of software-engineering activities, e.g., program slicing, information flow security analysis, debugging, code-optimization, code-reuse, code-understanding. Most existing dependency analyzers focus on mainstream languages and they do not support database applications embedding queries and data-manipulation commands. The first extension to the languages for relational database management systems, proposed by Willmor et al. in 2004, suffers from the lack of precision in the analysis primarily due to its syntax-based computation and flow insensitivity. Since then no significant contribution is found in this research direction. This paper extends the Abstract Interpretation framework for static dependency analysis of database applications, providing a semantics-based computation tunable with respect to precision. More specifically, we instantiate dependency computation by using various relational and non-relational abstract domains, yielding to a detailed comparative analysis with respect to precision and efficiency. Finally, we present a prototype $\sf{ semDDA}$ semDDA , a sem antics-based D atabase D ependency A nalyzer integrated with various abstract domains, and we present experimental evaluation results to establish the effectiveness of our approach. We show an improvement of the precision on an average of 6 percent in the interval, 11 percent in the octagon, 21 percent in the polyhedra and 7 percent in the powerset of intervals abstract domains, as compared to their syntax-based counterpart, for the chosen set of Java Server Page (JSP)-based open-source database-driven web applications as part of the GotoCode project.",0,1,0,software engineering+software reengineering+program slicing,
964,964,"Diggit automated code review via software repository mining. We present Diggit, a tool to automatically generate code review comments, offering design guidance on prospective changes, based on insights gained from mining historical changes in source code repositories. We describe how the tool was built and tuned for use in practice as we integrated Diggit into the working processes of an industrial development team. We focus on the developer experience, the constraints that had to be met in adapting academic research to produce a tool that was useful to developers, and the effectiveness of the results in practice.",0,1,0,source codes+software+software developer,
965,965,"Precision recall and sensitivity of monitoring partially synchronous distributed systems. Runtime verification focuses on analyzing the execution of a given program by a monitor to determine if it is likely to violate its specifications. There is often an impedance mismatch between the assumptions/model of the monitor and that of the underlying program. This constitutes problems especially for distributed systems, where the concept of current time and state are inherently uncertain. A monitor designed with asynchronous system model assumptions may cause false-positives for a program executing in a partially synchronous system: the monitor may flag a global predicate that does not actually occur in the underlying system. A monitor designed with a partially synchronous system model assumption may cause false negatives as well as false positives for a program executing in an environment where the bounds on partial synchrony differ (albeit temporarily) from the monitor model assumptions.",0,1,0,distributed systems+program execution+run-time verification,
966,966,"An empirical study of patch uplift in rapid release development pipelines. In rapid release development processes, patches that fix critical issues, or implement high-value features are often promoted directly from the development channel to a stabilization channel, potentially skipping one or more stabilization channels. This practice is called patch uplift. Patch uplift is risky, because patches that are rushed through the stabilization phase can end up introducing regressions in the code. This paper examines patch uplift operations at Mozilla, with the aim to identify the characteristics of the uplifted patches that did not effectively fix the targeted problem and that introduced regressions. Through statistical and manual analyses, a series of problems were investigated, including the reasons behind patch uplift decisions, the root causes of ineffective uplifts, the characteristics of uplifted patches that introduced regressions, and whether these regressions can be prevented. Additionally, three Mozilla release managers were interviewed in order to understand organizational factors that affect patch uplift decisions and outcomes. Results show that most patches are uplifted because of a wrong functionality or a crash. Certain uplifts did not effectively address their problems because they did not completely fix the problems or lead to regressions. Uplifted patches that lead to regressions tend to have larger patch size, and most of the faults are due to semantic or memory errors in the patches. Also, release managers are more inclined to accept patch uplift requests that concern certain specific components, and–or that are submitted by certain specific developers. About 25% to 30% of the regressions due to Beta or Release uplifts could have been prevented as they could be reproduced by developers and were found in widely used feature/website/configuration or via telemetry.",0,1,0,semantics+development processes+software developer,
967,967,"Towards a common metamodel for traces of high performance computing systems to enable software analysis tasks. There exist several tools for analyzing traces generated from HPC (High Performance Computing) applications, used by software engineers for debugging and other maintenance tasks. These tools, however, use different formats to represent HPC traces, which hinders interoperability and data exchange. At the present time, there is no standard metamodel that represents HPC trace concepts and their relations. In this paper, we argue that the lack of a common metamodel is a serious impediment for effective analysis for this class of software systems. We aim to fill this void by presenting MTF2 (MPI Trace Format2)—a metamodel for representing HPC system traces. MTF2 is built with expressiveness and scalability in mind. Scalability, an important requirement when working with large traces, is achieved by adopting graph theory concepts to compact large traces. We show through a case study that a trace represented in MTF2 can be in average 49% smaller than a trace represented in a format that does not consider compaction.",0,1,0,program debugging+maintenance tasks+software systems,
968,968,"A scalable and nearly uniform generator of sat witnesses. Functional verification constitutes one of the most challenging tasks in the development of modern hardware systems, and simulation-based verification techniques dominate the functional verification landscape. A dominant paradigm in simulation-based verification is directed random testing, where a model of the system is simulated with a set of random test stimuli that are uniformly or near-uniformly distributed over the space of all stimuli satisfying a given set of constraints. Uniform or near-uniform generation of solutions for large constraint sets is therefore a problem of theoretical and practical interest. For Boolean constraints, prior work offered heuristic approaches with no guarantee of performance, and theoretical approaches with proven guarantees, but poor performance in practice. We offer here a new approach with theoretical performance guarantees and demonstrate its practical utility on large constraint sets.",0,1,0,verification method+verification+verification techniques,
969,969,"Incentives and performance in large scale lean software development an agent based simulation approach. The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. However, a simple transfer of good practices from the automotive industry combined with experiences from agile development on a team level is not possible due to fundamental differences stemming from the particular domain specifics – i.e. different types of products and components (material versus immaterial goods), knowledge work versus production systems as well as established business models. Especially team empowerment and the absence of a a hierarchical control on all levels impacts goal orientation and business optimization. In such settings, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity of central importance. Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design such incentives, what their effect is, and how to chose an adequate development structure to foster overall software product development flow by means of more economic decisions and thus resulting in a shorter time to market. For calibrating our simulation, we rely on practical experience from a very large software company piloting and implementing lean and agile for about three years.",0,1,0,agile methods+software development+software project,
970,970,"Open source software ecosystems towards a modelling framework. Open source software ecosystem modelling has emerged as an important research area in software engineering. Several models have been proposed to identify and analyse the complex relationships in OSS-ecosystems. However, there is a lack of formal models, methodologies, tool support, and standard notations for OSS-ecosystems. In this paper we propose a general framework for support the OSS-ecosystems modelling process. This framework will allow the representation, synthesis, analysis, evaluation, and evolution of OSS-ecosystems. Design science methodology is proposed to create several artefacts and investigating the suitability of these artefacts in the OSS-ecosystem context.",0,1,0,tool support+software engineering+software reengineering,
971,971,"Elarva a monitoring tool for erlang. The Larva monitoring tool has been successfully applied to a number of industrial Java systems, providing extra assurance of behaviour correctness. Given the increased interest in concurrent programming, we propose Elarva, an adaptation of Larva for monitoring programs written in Erlang, an established industry-strength concurrent language. Object-oriented Larva constructs have been translated to process-oriented setting, and the synchronous Larva monitoring semantics was altered to an asynchronous interpretation. We argue how this loosely-coupled runtime verification architecture still permits monitors to actuate recovery actions.",0,1,0,concurrent programming+java+run-time verification,
972,972,"Project achilles a prototype tool for static method level vulnerability detection of java source code using a recurrent neural network. Software has become an essential component of modern life, but when software vulnerabilities threaten the security of users, new ways of analyzing for software security must be explored. Using the National Institute of Standards and Technology's Juliet Java Suite, containing thousands of examples of defective Java methods for a variety of vulnerabilities, a prototype tool was developed implementing an array of Long-Short Term Memory Recurrent Neural Networks to detect vulnerabilities within source code. The tool employs various data preparation methods to be independent of coding style and to automate the process of extracting methods, labeling data, and partitioning the dataset. The result is a prototype command-line utility that generates an n-dimensional vulnerability prediction vector. The experimental evaluation using 44,495 test cases indicates that the tool can achieve an accuracy higher than 90% for 24 out of 29 different types of CWE vulnerabilities.",0,1,0,software+software vulnerabilities+java,
973,973,"Motion deblurring via using generative adversarial networks for space based imaging. In some missions of NanoSats, we find images captured are disturbed by motion blur which caused under the situation that NanoSats work in low-earth orbit at high speeds. In this paper, we address the problem of deblurring images degraded due to space-based imaging system shaking or movements of observing targets. We propose a motion deblurring strategy via using Generative Adversarial Networks(GAN) to realize an end-to-end image processing without kernel estimation in orbit. We combine Wasserstein GAN(WGAN) and loss function based on adversarial loss and perceptual loss to optimize the result of deblurred image. The experimental results on the two different datasets prove the feasibility and effectiveness of the proposed strategy which outperforms the state-of-the-art blind deblurring algorithms using for remote sensing images both quantitatively and qualitatively.",0,1,0,imaging systems+remote sensing images+image processing,
974,974,"Concurrent correctness in vector space. Correctness verification of a concurrent history is challenging and has been proven to be an NP-complete problem. The reason that verifying correctness cannot be solved in polynomial time is a consequence of the way correctness is defined. Traditional correctness conditions require a concurrent history to be equivalent to a legal sequential history. The worst case number of legal sequential histories for a concurrent history is O(n!) with respect to n methods invoked. Existing correctness verification tools improve the time complexity by either reducing the size of the possible legal sequential histories or improving the efficiency of generating the possible legal sequential histories. Further improvements to the time complexity of correctness verification can be achieved by changing the way correctness of concurrent programs is defined. In this paper, we present the first methodology to recast the correctness conditions in literature to be defined in vector space. The concurrent histories are represented as a set of method call vectors, and correctness is defined as properties over the set of vectors. The challenge with defining correctness in vector space is accounting for method call ordering and data structure semantics. We solve this challenge by incorporating a priority assignment scheme to the values of the method call vectors. Using our new definitions of concurrent correctness, we design a dynamic analysis tool that checks the vector space correctness of concurrent data structures in \(O(n^2)\) with respect to n method calls, a significant improvement over O(n!) time required to analyze legal sequential histories. We showcase our dynamic analysis tool by using it to check the vector space correctness of a variety of queues, stacks, and hashmaps.",0,1,0,verification tools+concurrent program+concurrent data structures,
975,975,"Biker a tool for bi information source based api method recommendation. Application Programming Interfaces (APIs) in software libraries play an important role in modern software development. Although most libraries provide API documentation as a reference, developers may find it difficult to directly search for appropriate APIs in documentation using the natural language description of the programming tasks. We call such phenomenon as knowledge gap, which refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes. In this paper, we propose a Java API recommendation tool named BIKER (Bi-Information source based KnowledgE Recommendation) to bridge the knowledge gap. We implement BIKER as a search engine website. Given a query in natural language, instead of directly searching API documentation, BIKER first searches for similar API-related questions on Stack Overflow to extract candidate APIs. Then, BIKER ranks them by considering the query’s similarity with both Stack Overflow posts and API documentation. Finally, to help developers better understand why each API is recommended and how to use them in practice, BIKER summarizes and presents supplementary information (e.g., API description, code examples in Stack Overflow posts) for each recommended API. Our quantitative evaluation and user study demonstrate that BIKER can help developers find appropriate APIs more efficiently and precisely.",0,1,0,software+java+software development,
976,976,"Stateful component based performance models. The accuracy of performance-prediction models is crucial for widespread adoption of performance prediction in industry. One of the essential accuracy-influencing aspects of software systems is the dependence of system behaviour on a configuration, context or history related state of the system, typically reflected with a (persistent) system attribute. Even in the domain of component-based software engineering, the presence of state-reflecting attributes (the so-called internal states) is a natural ingredient of the systems, implying the existence of stateful services, stateful components and stateful systems as such. Currently, there is no consensus on the definition or method to include state-related information in component-based prediction models. Besides the task to identify and localise different types of stateful information across component-based software architecture, the issue is to balance the expressiveness and complexity of prediction models via an effective abstraction of state modelling. In this paper, we identify and classify stateful information in component-based software systems, study the performance impact of the individual state categories, and discuss the costs of their modelling in terms of the increased model size. The observations are formulated into a set of heuristics-guiding software engineers in state modelling. Finally, practical effect of state modelling on software performance is evaluated on a real-world case study, the SPECjms2007 Benchmark. The observed deviation of measurements and predictions was significantly decreased by more precise models of stateful dependencies.",0,1,0,software engineering+software reengineering+component-based software systems,
977,977,"Comparison of the fmea and stpa safety analysis methods a case study. As our society becomes more and more dependent on IT systems, failures of these systems can harm more and more people and organizations. Diligently performing risk and hazard analysis helps to minimize the potential harm of IT system failures on the society and increases the probability of their undisturbed operation. Risk and hazard analysis is an important activity for the development and operation of critical software intensive systems, but the increased complexity and size puts additional requirements on the effectiveness of risk and hazard analysis methods. This paper presents a qualitative comparison of two hazard analysis methods, failure mode and effect analysis (FMEA) and system theoretic process analysis (STPA), using case study research methodology. Both methods have been applied on the same forward collision avoidance system to compare the effectiveness of the methods and to investigate what are the main differences between them. Furthermore, this study also evaluates the analysis process of both methods by using a qualitative criteria derived from the technology acceptance model (TAM). The results of the FMEA analysis were compared to the results of the STPA analysis, which were presented in a previous study. Both analyses were conducted on the same forward collision avoidance system. The comparison shows that FMEA and STPA deliver similar analysis results.",0,1,0,node failure+software+software failure,
978,978,"Security vulnerabilities detection using model inference for applications and security protocols. ""Internet of Services"" (IoS) is a vision of the Internet of the Future where applications are built by combining services provided by a variety of service providers over the network. They are deployed as needed and consumed at run-time in a demand-driven and flexible way. Model-based testing is one method for testing security of applications but it needs formal models and most of the time service providers are not able to provide them. For that, model inference methods adapted to security testing can be used. This document tries to give some directions in order to combine enhanced model inference and model testing to ensure security of services automatically.",0,1,0,security protocols+security vulnerabilities+model based testing,
979,979,"Devops a definition and perceived adoption impediments. As the interest in DevOps continues to grow, there is an increasing need for software organizations to understand how to adopt it successfully. This study has as objective to clarify the concept and provide insight into existing challenges of adopting DevOps. First, the existing literature is reviewed. A definition of DevOps is then formed based on the literature by breaking down the concept into its defining characteristics. We interview 13 subjects in a software company adopting DevOps and, finally, we present 11 impediments for the company’s DevOps adoption that were identified based on the interviews.",0,1,0,microservices+software organization+devops,
980,980,"Leveraging natural language analysis of software achievements challenges and opportunities. Studies continue to report that more time is spent reading, locating, and comprehending code than actually writing code. The increasing size and complexity of software systems makes it significantly more challenging for humans to perform maintenance tasks on software without automated and semi-automated tools to support them, especially in the error-prone tasks. Thus, software engineers increasingly rely on software engineering tools to automate maintenance tasks as much as possible. The program analyses that drive today's software engineering tools have historically focused on analyzing the program's data and control flow, dependencies, and other structural information about the program to uncover and prove program properties. Yet, a software system is more than just the source code and its structure. To build effective software tools, the underlying automated analyses need to use all the information available to make the tools as intelligent and useful as possible. By adapting natural language processing (NLP) to source code analysis, and integrating information retrieval (IR), NLP, and traditional program analyses, we can expect significant improvement in automated and semi-automated software engineering tools for many different software engineering tasks. In this talk, I will overview research in text analysis of software and discuss our achievements to date, the challenges faced in text analysis, and the opportunities for text analysis of software in the future.",0,1,0,software engineering+software reengineering+source code analysis,
981,981,"Design of a uml profile for feature diagrams and its tooling implementation. This paper proposes an instrumented solution to integrate feature diagrams with UML models to be used as part of a general approach for designing software product lines and for product generation. The contribution is implemented in IBM Rational Software Architect (RSA). It is intended to be used in the context of large, complex and multi-domain projects, and at allowing model transformations to derive products. Our RSA implementation makes it possible to link feature diagrams with UML model artifacts. It allows traceability between feature models and other different kinds of models (requirements, class diagrams, sequence or activity diagrams, etc.). It is used in a project dedicated to create smart building optimization systems.",0,1,0,software product line+uml model+uml profiles,
982,982,"Reinforcing software engineering learning through provenance. Software engineering is focused on practical and theoretical aspects of the software production. Teaching software engineering is traditionally done through theoretical classes with some practical exercises. Recently, games and simulators were introduced as a ludic alternative for software engineering learning, where decisions and interactions become key factors to transmit and acquire knowledge. However, mistakes made by wrong decisions may jeopardize the learning process, especially when reproducing its effects is not a viable option due to the non-deterministic nature of games. With this in mind, in a previous work we proposed a novel approach based on provenance concepts in order to present the decisions and effects of such decisions when learning through games. In this work, we present an experimental evaluation of that approach with undergraduate students. The obtained results show that the use of provenance leads to faster and more accurate answers from students, including learning aspects that could not be achieved by a traditional educational game.",0,1,0,software+software engineering+software reengineering,
983,983,"Spotify tailoring for promoting effectiveness in cross functional autonomous squads. Organisations tend to tailor agile methods to scale employed practices to have cross-functional autonomous teams while promoting sustainable creative and productive development at a constant pace. Thus, it is important to investigate how organisations tailor agile practices to get the balance right between teams’ autonomy and alignment. Spotify model is originally introduced to facilitate the development of music streaming services in a very large-scale project with a Business-to-Consumer (B2C) model. However, developing a large-scale mission-critical project with a Business-to-Business (B2B) model is not essentially supported by the Spotify model. Thus, embracing Spotify model for such projects should be concerned about the question of how Spotify practices are adjusted to promote effectiveness of cross-functional autonomous squads in a mission-critical project with B2B model?",0,1,0,b2b+agile methods+agile practices,
984,984,"Robustness of on device models adversarial attack to deep learning models on android apps. Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.",0,1,0,android+engineering+reverse engineering,
985,985,"A learning based approach for automatic construction of domain glossary from source code and documentation. A domain glossary that organizes domain-specific concepts and their aliases and relations is essential for knowledge acquisition and software development. Existing approaches use linguistic heuristics or term-frequency-based statistics to identify domain specific terms from software documentation, and thus the accuracy is often low. In this paper, we propose a learning-based approach for automatic construction of domain glossary from source code and software documentation. The approach uses a set of high-quality seed terms identified from code identifiers and natural language concept definitions to train a domain-specific prediction model to recognize glossary terms based on the lexical and semantic context of the sentences mentioning domain-specific concepts. It then merges the aliases of the same concepts to their canonical names, selects a set of explanation sentences for each concept, and identifies ""is a"", ""has a"", and ""related to"" relations between the concepts. We apply our approach to deep learning domain and Hadoop domain and harvest 5,382 and 2,069 concepts together with 16,962 and 6,815 relations respectively. Our evaluation validates the accuracy of the extracted domain glossary and its usefulness for the fusion and acquisition of knowledge from different documents of different projects.",0,1,0,source codes+software+software development,
986,986,"Identifying implicit architectural dependencies using measures of source code change waves. The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB.",0,1,0,scrum+agile software development+software repositories,
987,987,"Boxing clever practical techniques for gaining insights into training data and monitoring distribution shift. Training data has a significant influence on the behaviour of an artificial intelligence algorithm developed using machine learning techniques. Consequently, any argument that the trained algorithm is, in some way, fit for purpose ought to include consideration of data as an entity in its own right. We describe some simple techniques that can provide domain experts and algorithm developers with insights into training data and which can be implemented without specialist computer hardware. Specifically, we consider sampling density, test case generation and monitoring for distribution shift. The techniques are illustrated using example data sets from the University of California, Irvine, Machine Learning repository.",0,1,0,domain knowledge+computer hardware+test case generation,
988,988,"Challenges in the modelling of sos design alternatives with mbse. A central promise of Model-Based Systems Engineering (MBSE) is to decrease the volume of information produced during the system design lifecycle through improved structure and smarter reuse across the numerous perspectives and architectural levels. The study of variants is a core challenge within MBSE due to associated exponential information growth, and the complexity created by this extra dimensionality. Variant management has been discussed extensively for the physical layer, but lacks research focusing on functional and logical layers, particularly relevant to concept phase engineering. In System of Systems (SoS) problems, the physical choices are typically constrained by the fact that the interacting systems already exist. This paper therefore studies the implications of variant modelling for the behaviour of the SoS. It explores the possibility of developing a functional architecture that can be extended to accommodate changes due to decisions made in the logical layer. It investigates the formal aspects of including variant modelling into the Systems Modelling Language (SysML) and discusses the potential need for a language extension.",0,1,0,modeling languages+model-based systems engineering+sysml,
989,989,"Cyber foraging and offloading framework for internet of things. Computation offloading or cyber foraging is a key capability required to achieve effective resource utilization in mobile cloud computing. It enables the dynamic offloading of computations to either neighboring mobile nodes or remote cloud based servers, retrieve results from the offloaded computations, and thereafter continue execution of the mobile business logic. A number of computational mobility solutions have emerged recently for mobile cloud computing involving smartphones and tablets. However, these solutions incur limitations in the context of Internet of Things (IoT) due to the significant heterogeneity illustrated by the range of objects involved in IoT and the fact that existing solutions tend to be tightly coupled to their underlying frameworks, which makes it hard to seamlessly adapt these solutions to the IoT scenarios. To address these concerns, this paper makes three contributions. First, it presents a novel modular and highly configurable framework for providing seamless computational mobility in the IoT realm. Second, it provides implementation details for key capabilities of this framework. Third, it provides qualitative evaluation of the framework's capabilities.",0,1,0,business process+cyber foraging+business logic,
990,990,"Effective statistical fault localization using program slices. Recent techniques for fault localization statistically analyze coverage information of a set of test runs to measure the correlations between program entities and program failures. However, coverage information cannot identify those program entities whose execution affects the output, which weakens the aforementioned correlations. Thus, this paper proposes a novel statistical fault localization approach to address this problem. Our statistical approach utilizes program slices of a set of test runs to capture the influence of a program entity's execution on the output, and uses statistical analysis to measure the suspiciousness of program entities to be faulty. In addition, this paper presents a new slicing approach called approximate dynamic backward slice to balance the size and accuracy of a slice, and applies this slice to our statistical approach. The experimental results on two standard benchmarks show that our statistical approach significantly outperforms eight representative fault localization techniques.",0,1,0,localization algorithm+localization technique+fault localization,
991,991,"Wb4sp a tool to build the word base for specific programs. Software becomes increasingly complex with its continuous maintenance activities. Given a system under maintenance, developers used to employing code search techniques to locate the code of their interests. However, they may have difficulties in understanding the source code elements and the relationship among them in the searching results. If there is a word base for a specific system, the developers can refer it to help locate and recover the source code elements and their relationships, which can improve the maintenance efficiency. In this paper, we present a tool, WB4SP(Word Base for Specific Programs), which focuses on building the word base for a specific system. WB4SP can retrieve the words, recover the relationship between them, and display the evolution of these words during the software evolution.",0,1,0,source codes+software+software evolution,
992,992,"The state of adoption and the challenges of systematic variability management in industry. Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.",0,1,0,software product line+software product line engineerings+variability management,
993,993,"Product recommendation for e commerce business by applying principal component analysis pca and k means clustering benefit for the society. Recommender system is a computer-based intelligent technique which facilitates the customers to fulfill their purchase requirements. In addition to this, it also helps retailers to manage the supply chain of their business and to develop different business strategies keeping in pace with the current market. Supply chain management (SCM) involves the streamlining of a business’s supply-side activities to remain competitive in the business landscape. Maximizing the customer value is another important activity of SCM to gain an advantage in the market. In this work, the K-Means clustering algorithm has been used for the effective segmentation of customers who have bought apparel items. PCA has been used for dimensionality reduction of different features of products and customers. The main focus of this work is to determine the different possible associations of customers in terms of brand, product, and price from their purchase habits. The result shows that the clusters made by the algorithm based on PCA and K-Means are similar and the results are acceptable on the basis of feedback received from existing customers and satisfies the customers’ requirements based on the amount of money (price range) the customers want to spend while doing online shopping. The features of products purchased by customers were combined together to generate a unique product key for business, and a model was prepared to segment products based on the volume of products sold and revenue generated, and the price of products sold and revenue generated. This work, in the long run, will help business houses to build a sustainable, profitable, and scalable e-commerce business. Environmental, social, and economic aspects are important to make e-commerce more sustainable for the benefit of the society.",0,1,0,business process+customer values+customer requirements,
994,994,"Automatic query rewriting schemes for multitenant saas applications. In Software as a Service (SaaS) environments, designing and realizing multitenant schema-mapping that supports a shared database with custom extensions is a non-trivial task. Universal Table is one promising schema-mapping technique that is commonly used. However, there has been little research devoted to the design and realization of a query rewriting scheme for Universal Table. In this paper, we present a collection of general query rewriting schemes for Universal Table that can transparently transform tenant-specific logical queries into corresponding physical queries. Based on the design, we have developed a Java-based schema-mapping and query rewriting middleware for Universal Table and a sample online shopping SaaS application to verify its feasibility. Additionally, analytical results that can be used to predict the overhead of our schemes are also reported. Finally, we conduct a series of experiments and find that the results not only agree well with our analytical predictions but also show that our schemes are scalable to the number of tenants and the number of concurrent database connections.",0,1,0,cloud services+saas+multi-tenancy,
995,995,"Empirically analyzing and evaluating security features in software requirements. Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements for compliance is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is appealing to apply Natural Language Processing (NLP) and Machine Learning (ML)-based techniques for analyzing these documents. In this paper, we propose a semi-automatic methodology that assesses the security requirements of software systems with respect to completeness and ambiguity, creating a bridge between the requirements documents and being in compliance with standards. Security standards, such as ISO and OWASP, are compared against software project documents for textual entailment relationships. These entailment results along with the document annotations are used to train a Neural Network model to predict whether a given statement in the document is found within the security standard or not. Hence, this approach aims to identify the appropriate structures that underlie software requirements documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirements specification documents as related to security-oriented features.",0,1,0,requirements specifications+software organization+requirements document,
996,996,"Multi core symbolic bisimulation minimisation. Bisimulation minimisation alleviates the exponential growth of transition systems in model checking by computing the smallest system that has the same behavior as the original system according to some notion of equivalence. One popular strategy to compute a bisimulation minimisation is signature-based partition refinement. This can be performed symbolically using binary decision diagrams to allow models with larger state spaces to be minimised.

This paper studies strong and branching symbolic bisimulation for labeled transition systems, continuous-time markov chains, and interactive markov chains. We introduce the notion of partition refinement with partial signatures. We extend the parallel BDD library Sylvan to parallelize the signature refinement algorithm, and develop a new parallel BDD algorithm to refine a partition, which conserves previous block numbers and uses a parallel data structure to store block assignments. We also present a specialized BDD algorithm for the computation of inert transitions. The experimental evaluation, based on benchmarks from the literature, demonstrates a speedup of upi¾?to 95x sequentially. In addition, we find parallel speedups of upi¾?to 17x due to parallelisation with 48 cores. Finally, we present the implementation of these algorithms as a versatile tool that can be customized for bisimulation minimisation in various contexts.",0,1,0,bdd+transition system+model checking,
997,997,"Dynamic analysis of upgrades in c c software. Regression testing techniques are commonly used to validate the correctness of upgrades. When a regression test fails, testers must understand the erroneous behaviors that caused the failure and identify the fault that originated these erroneous behaviors. In many cases, identifying the causes of a failure is difficult and time consuming. The analysis of regression problems provides interesting opportunities to validation and verification techniques. In fact, by comparing the execution of the base version and the upgraded version of the same program it is possible to automatically deduce information about incorrect behavior of the program. In this paper we present RADAR (Regression Analysis with Diff And Recording), a dynamic analysis technique, which analyzes regression problems and automatically identifies the chain of erroneous events that lead to a failure in C/C++ programs. RADAR exploits information about changes and the availability of multiple versions of the same program to automatically distinguish correct and suspicious events. Empirical experience with industrial and open source cases shows that RADAR can effectively support testers in the investigation of regression problems. Thus, RADAR can drive and simplify the debugging process.",0,1,0,debugging process+c programs+regression testing,
998,998,"Empirical evaluation on fbd model based test coverage criteria using mutation analysis. Function Block Diagram (FBD), one of the PLC programming languages, is a graphical modeling language which has been increasingly used to implement safety-critical software such as nuclear reactor protection software. With increased importance of structural testing for FBD models, FBD model-based test coverage criteria have been introduced. In this paper, we empirically evaluate the fault detection effectiveness of the FBD coverage criteria using mutation analysis. We produce 1800 test suites satisfying the FBD criteria and generate more than 600 mutants automatically for the target industrial FBD models. Then we evaluate mutant detection of the test suites to assess the fault detection effectiveness of the coverage criteria. Based on the experimental results, we analyze strengths and weaknesses of the FBD coverage criteria, and suggest possible improvements for the test coverage criteria.",0,1,0,software+modeling languages+safety critical software,
999,999,"A web portal for the certification of open source software. This paper presents a web portal for the certification of open source software. The portal aims at helping programmers in the internet age, when there are too many open source reusable libraries and tools available. Our portal offers programmers a web-based and easy setting to analyze and certify open source software, which is a crucial step to help programmers choosing among many available alternatives, and to get some guarantees before using one piece of software.

The paper presents our first prototype of such web portal. It also describes in detail a domain specific language that allows programmers to describe with a high degree of abstraction specific open source software certifications. The design and implementation of this language is the core of the web portal.",0,1,0,software+open source software+open source system,
1000,1000,"Fostering early literacy skills in children s libraries opportunities for embodied cognition and tangible technologies. Early literacy is a leading indicator of academic success. Recent findings describe the important role that embodied cognition can play in the promotion of early literacy. Libraries---children's libraries in particular---stand to benefit greatly from emerging forms of tangible and embodied interactive technology that can leverage these findings. As informal community-based learning institutions with a mandate to provide user-centered, personalized reading and learning experiences, children's libraries are uniquely positioned to empower young learners through the development of reading skills. Within these institutions, reading skills---particularly those representing embodied cognition---are supported by social interaction with peers, caregivers, and librarians. Through embodied cognition, children develop critical early literacy skills by linking concepts with corresponding physical actions, to establish the foundation of reading comprehension. When such activities are playful, fun, and interactive, learning to read becomes intrinsically motivating. While embodied technology is particularly conducive to creating such novel interactions, few libraries have adopted technology that deliberately channels these phenomena towards literacy development. Through qualitative ethnographic methods this investigation presents opportunities for embodied cognition and tangible embedded interactive play and learning systems within children's libraries.",0,0,1,user-centered+tangible interfaces+tangible user interfaces,
1001,1001,"Virtual assistants for mobile interaction a review from the accessibility perspective. The technology of virtual assistants (VAs) is a powerful option to support the interaction of human users with computational systems. These VAs are able, for example, to identify interaction problems and offer recommendations on the execution of commands. This work analyses the use of VAs as a resource of accessibility for mobile devices. This analysis was carried out by means of a literature review, which considered both academic studies and commercial solutions. The results showed that there are very few researches in this area and this fact motivated the development of an evaluation protocol, and related set of tests cases, which can verify if current VAs are in fact able to support the interaction of motor and visually impaired users with their mobile devices.",0,0,1,virtual spaces+visually-impaired users+mobile interaction,
1002,1002,"Presstures exploring pressure sensitive multi touch gestures on trackpads. In this paper, we present Presstures, an extension to current multi-touch operations that enriches common multi-finger gestures with pressure information. By using the initially applied pressure level for implicit mode switching, a gesture can be enhanced with different functionalities to enlarge the interaction space for multi-touch. To evaluate the feasibility of our concept, we conducted an experiment, which indicates good human sensorimotor skills for performing multi-touch gestures with a few number of pressure levels and without any additional feedback. Based on the experimental results, we discuss implications for the design of pressure-sensitive multi-touch gestures, and propose application scenarios that make optimal use of our concept.",0,0,1,gesture recognition+hand gesture+hand posture,
1003,1003,"Theoretical foundations for developing cybersecurity training. Cybersecurity is a computer term regarding the detection, anticipation, and prevention of computer technologies and peripherals from damage, attack, or unauthorized access. These technologies include the monitoring of networks, programs, applications, and personnel. Cybersecurity can be viewed from both an offensive or defensive posture involving maintaining and proactively assessing security vulnerabilities. In 2013, Edward Snowden used his position as an infrastructure analyst to leak thousands of top-secret classified documents detailing the U.S. Government’s global covert surveillance and eavesdropping undertakings to the public. This incident identified the human threat as a contributing factor that highlighted several weaknesses in the present state of U.S. cybersecurity affairs. In efforts to strengthen cyber defenses, a solid theoretical research foundation regarding cyber vulnerabilities is warranted. Building upon that foundation, training and experimentation can provide insight into current cybersecurity training methods and how they can be transitioned and implemented into future training regimens.",0,0,1,information security+cyber security+cyber threats,
1004,1004,"Forcestamps fiducial markers for pressure sensitive touch surfaces to support rapid prototyping of physical control interfaces. We present ForceStamps, fiducial markers for supporting rapid prototyping of physical control interfaces on pressure-sensitive touch surfaces. We investigate marker design options for supporting various physical controls, with focusing on creating dedicated footprints and maintaining the structural stability. ForceStamps can be persistently tracked on surfaces along with the force information and other attributes. Designers without knowledge of electronics can rapidly prototype physical controls by attaching mechanisms to ForceStamps, while manipulating the haptic feedback with buffer materials. The created control widgets can be spatially configured on the touch surface to make an interface layout. We showcase a variety of example controls created with ForceStamps. In addition, we report on our analysis of a two-day musical instrument design workshop to explore the affordances of ForceStamps for making novel instruments with diverse interaction designs.",0,0,1,haptic systems+haptic feedbacks+interaction design,
1005,1005,"Cyberloafing and job burnout. Cyberloafing in the knowledge-intensive settings has hitherto remained an underexplored research terrain. This is despite that, among others, the implications of cyberloafing in the knowledge-intensive context are more costly to the employers as compared to other less-knowledge-intensive settings. The motivations, moderators and consequences of cyberloafing among knowledge workers appear to have multifarious differences and contextual nuances. One of the significant determinants and increasing vulnerabilities of the knowledge-intensive setting is job burnout. This paper particularly focuses on and studies the inter-relationship of job burnout and cyberloafing. The viewpoints of knowledge-workers at a cross-section of organizational levels have been studied through quantitative surveys. The findings show that both facets of cyberloafing (activities and behaviors) have significant impact on job burnout among knowledge workers. It is also proved that cyberloafing 'behaviors' is a stronger predictor of job burnout than cyberloafing 'activities'. Both dimensions of cyberloafing (activities and behaviors) have significant impact on job burnout among knowledge workers.'Cyberloafing behaviors' are a stronger predictor of job burnout than 'cyberloafing activities'.'Developmental' mode of cyberloafing is negatively related with job burnout.The correlation of job burnout with the 'deviant behavior' dimension of cyberloafing is positive.",0,0,1,tacit knowledge+domain knowledge+viewpoint,
1006,1006,"An evolving museum metaphor applied to cultural heritage for personalized content delivery. The aim of this article concerns adaptive and personalized navigation in a cultural heritage database. The theoretical grounding of the proposition relies on cognitive science, particularly constructivism and enaction. The navigation is conducted via an intelligent interface through a 3D “living” museum metaphor. The purpose of this interface is to recommend dynamic cultural heritage objects according to a user profile that is computed online from the interactions that a user has with these objects. To this end, objects are linked to semantic structures that represent relations between cultural heritage concepts. The user profile is described in terms of cultural heritage interests. A prototype of this principle is used to evaluate some of the basic hypotheses of this proposition.",0,0,1,user profile+navigation systems+cognitive science,
1007,1007,"Channels matter. Multimodal connectedness was associated with bridging and bonding social capital.Playing with existing offline friends was associated with bonding social capital.Playing with offline and online friends was associated with bridging social capital.Multimodal connectedness moderated the relationships between co-players and social capital. The study aimed to examine the roles and interactions of (1) multimodal connectedness and (2) three types of co-player networks in online gamers' social capital acquisition. Over 17,000 players of the popular game League of Legends were surveyed on their playing partners, the media channels used, and social capital. Combined with behavioral data from server logs, the results showed that multimodal connectedness (i.e., the number of communication channels used for social interaction among players) was positively associated with one's bridging and bonding social capital. The frequency of playing with an existing offline friend was positively associated with one's bridging and bonding social capital; the frequency of playing with an online friend first met in the game was positively associated with one's bridging social capital; the frequency of playing with a family member was not a significant predictor of one's social capital outcomes. Moreover, multimodal connectedness magnified the positive relationships found between social capital outcomes and playing with online and offline friends.",0,0,1,social relations+social computing+social aspect,
1008,1008,"Visual data analysis as an integral part of environmental management. The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured current and historical field data. We describe in this article how visual analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites. Lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites.",0,0,1,visualization+visualization tools+visual analysis,
1009,1009,"Depthtouch an elastic surface for tangible computing. In this paper we describe DepthTouch, an installation which explores future interactive surfaces and features elastic feedback, allowing the user to go deeper than with regular multi-touch surfaces. DepthTouch's elastic display allows the user to create valleys and ascending slopes by depressing or grabbing its textile surface. We describe the experimental approach for eliciting appropriate interaction metaphors from interaction with real materials and the resulting digital prototype.",0,0,1,tangible user interfaces+interactive surfaces+tangible interaction,
1010,1010,"Self affirmation underlies facebook use. Social network sites, such as Facebook, have acquired an unprecedented following, yet it is unknown what makes them so attractive to users. Here we propose that these sites’ popularity can be understood through the fulfillment of ego needs. We use self-affirmation theory to hypothesize why and when people spend time on their online profiles. Study 1 shows that Facebook profiles are self-affirming in the sense of satisfying users’ need for self-worth and self-integrity. Study 2 shows that Facebook users gravitate toward their online profiles after receiving a blow to the ego, in an unconscious effort to repair their perceptions of self-worth. In addition to illuminating some of the psychological factors that underlie Facebook use, the results provide an important extension to self-affirmation theory by clarifying how self-affirmation operates in people’s everyday environments.",0,0,1,facebook+social networking sites+social network sites,
1011,1011,"The self on the net. In the present research, two studies examined how self-construal and public self-consciousness jointly influence positive self-presentation in online social networking. Participants' independent and interdependent self-construals were either measured (Study 1, Nź=ź137) or manipulated with priming (Study 2, Nź=ź120). Public self-consciousness was measured with self-report ratings in both studies. Participants self-reported their tendency (Study 1) and behavioral intention (Study 2) to engage in positive self-presentation on Facebook. Both studies were conducted with college student Facebook users in South Korea. Results from both studies demonstrated no main effect of self-construal but a significant interaction between self-construal and public self-consciousness. Specifically, the negative association between interdependent self-construal and positive self-presentation on Facebook was significant among those low in public self-consciousness; the association became less prominent for those higher in public self-consciousness (Study 1). When self-construal was experimentally manipulated, interdependent self-construal priming, as compared with independent self-construal priming, resulted in lower intention to engage in positive self-presentation, particularly among those low in public self-consciousness (Study 2). The implications of the findings were discussed. We examined possible predictors of positive self-presentation (PSP) on Facebook.We examined the interplaying role of self-construal and public self-consciousness.Data came from college-age Facebook users in South Korea.Findings were consistent across two studies: survey and experiment.Interdependent self-construal and public self-consciousness had an interaction effect on PSP.",0,0,1,social networking sites+online social networkings+social network sites,
1012,1012,"A comparative evaluation of touch and pen gestures for adult and child users. In this paper, we present results of two user studies that compared the performance of touch-based and pen-based gesture input on capacitive touchscreens for both adult and 8-11 years old child users. Results showed that inputting gestures with pen was significantly faster and more accurate than touch for adult users. However, no significant effect of input method was observed on performance for child users. Similarly, user experience evaluation showed that a large number of adult users favoured one technique over the other and/or found a technique more comfortable to use than the other, while child users were mostly neutral. This trend, however, was not statistically significant.",0,0,1,hand gesture+mobile users+user experience,
1013,1013,"One led is enough catalyzing face to face interactions at conferences with a gentle nudge. Face-to-face social interactions among strangers today are becoming increasingly rare as people turn towards computer-mediated networking tools. Today's tools, however, are based on the following assumptions: increased information encourages interaction, profiles are good representations of users to other users, and computer-mediated communications prior to face-to-face meetings lead to better outcomes. This paper describes CommonTies, a gentle technological in the form of a wearable accessory, that encourages immediate, face-to-face, organic social interactions among strangers at conferences. By not exposing any profile information, CommonTies preserves an element of mystery and enables self-disclosure of information through conversation. We evaluate our system through a field study at a three-day research conference - CSCW 2014. We find that despite our information-scarce design, users were willing to interact with strangers and 74% of the interactions initiated by CommonTies were reported as novel and useful.",0,0,1,social relationships+wearable computers+cscw,
1014,1014,"Increasing user engagement with distributed public displays through the awareness of peer interactions. Recent developments have shown a growing interest in interactive pervasive computing scenarios supported by public displays as well as their introduction into educational environments. Still, one of the biggest challenges in the design of public display systems is to engage users to interact and be motivated to do so. In this paper, we report a study exploring the potential effect of the awareness of peers' interactions with an educational video installation and the popularity of the display system on the user engagement. The awareness is facilitated by pop-up notifications and visualizations of interactions on the display screen. We conducted a six day long deployment of our system which included a diary study, during which we altered the display's dynamic behavior in order to test different conditions. The analysis of the diary reports and the progression of the users' interactions showed that the users found the presentations of peer interactions to be engaging, both with the display system as well as the social context around it.",0,0,1,display devices+image display+display system,
1015,1015,"Tilepop tile type pop up prop for virtual reality. We present TilePoP, a new type of pneumatically-actuated interface deployed as floor tiles which dynamically pop up by inflating into large shapes constructing proxy objects for whole-body interactions in Virtual Reality. TilePoP consists of a 2D array of stacked cube-shaped airbags designed with specific folding structures, enabling each airbag to be inflated into a physical proxy and then deflated down back to its original tile shape when not in use. TilePoP is capable of providing haptic feedback for the whole body and can even support human body weight. Thus, it allows new interaction possibilities in VR. Herein, the design and implementation of TilePoP are described in detail along with demonstrations of its applications and the results of a preliminary user evaluation conducted to understand the users' experience with TilePoP.",0,0,1,virtual environments+haptic feedbacks+virtual reality,
1016,1016,"Real time facial feature tracking in poor quality thermal imagery. Recently, facial feature tracking systems have become more and more popular because of many possible use cases. Especially in medical applications location of the face and facial features are very useful. Many researches have presented methods to detect and track facial features in visible light. However, facial feature analysis in thermography may also be very advantageous. Some examples of using infrared imagery in medicine include the estimation of the respiration rate using an analysis of temperature changes in the area below nose region. Moreover, due to technological development small thermal cameras may be embedded into wearable devices, like smart glasses and used to support remote patient monitoring. Therefore, in this paper, we focused on face tracking in low quality thermal images. Especially, we compared four interest points detectors for facial feature tracking in thermal images. All methods were tested for processing time, displacement of detected areas and errors of calculated mean value of pixel intensities in the detected nose region. Finally, we presented a fully automatic system for facial features tracking, which allows to process one frame in about 27.7ms (Harris), 23.9ms (ORB), 19.7ms (SIFT), 27.6ms (SURF) with acceptable accuracy (Harris - 7.2±4.3%, ORB 9.9±2.2%, SIFT 7.0±1.9%, SURF 8.9±2.7%).",0,0,1,face tracking+wearable computers+wearable devices,
1017,1017,"Audio augmented paper for the therapy of low functioning autism children. In this paper, we present a prototype and an initial pilot study of audio-augmented paper to support the therapy of low-functioning autism children. The prototype supports the recording of audio on standard sheets of paper by using tangible tools that can be shared among the therapist and the child. The prototype is designed as tool for the therapist to engage a child in a storytelling activity.",0,0,1,usability studies+tangible interfaces+tangible user interfaces,
1018,1018,"Collision prediction and prevention in a simultaneous two user immersive virtual environment. Head-mounted display (HMD) based immersive virtual environment (VE) systems that incorporate a wearable rendering unit allow users to navigate within virtual worlds through natural walking. Redirected walking (RDW) is a technique that allows users to explore virtual worlds which are larger than the physical tracking area. It involves imperceptibly rotating the VE that the user sees, which causes the user to subconsciously compensate by physically turning. This work extends generalized RDW techniques to allow two immersed users to share a tracking area. The extension forecasts potential collisions so they can be avoided by using RDW techniques. In simulations based on recorded user-data, unsafe situations which could result in a collision occurred at a rate of 31.5/hr. in baseline experiments. The algorithm presented here resulted in all potential collisions being predicted in advance. Once predicted, most future collisions could be avoided using RDW techniques. Some, however, were resolved by stopping one or both users. These instances occurred at a rate of only 2.5/hr.",0,0,1,virtual environments+head mounted displays+immersive virtual environments,
1019,1019,"Multiray multi finger raycasting for large displays. We explore and evaluate a multi-finger raycasting design space that we call ""multiray"". Each finger projects a ray on to the display, so the user is interacting from a distance using a form of direct input. Specifically, we propose techniques, where patterns of ray intersections created by hand postures form 2D geometric shapes to trigger actions and perform direct manipulations that go beyond single-point selections. Two formative studies examine characteristics of multi-finger raycasting for different projection methods, shapes, and tasks. Based on the results of those investigations, we demonstrate a number of dynamic UI controls and operations that utilise multiray points and shapes.",0,0,1,hand posture+display system+large displays,
1020,1020,"Unlimited corridor a visuo haptic redirection system. The Unlimited Corridor is a virtual reality system that enables users to walk in an ostensibly straight direction around a virtual corridor within a small tracked space. Unlike other redirected walking systems, the Unlimited Corridor allows users to keep walking around without interruptions or resetting phases. This is made possible by combining a redirected walking technique with visuo-haptic interaction and a path planning algorithm. The Unlimited Corridor produces passive haptic feedback using semi-circular handrails; that is, when users grip a straight handrail in the virtual environment, they simultaneously grip a corresponding curved handrail in the physical world. These stimuli enable visuo-haptic interaction, with the user perceiving the gripped handrail as straight, and this sensation enhances the effects of redirected walking. Furthermore, we developed an algorithm that dynamically modifies the amount of distortion to allow a user to walk ostensibly straight and turn at intersections in any direction. We evaluated the Unlimited Corridor using a virtual space of approximately 400 m2 in a physical space of approximately 60 m2. According to a user study, the median value of the straightness sensation of walking when users grip the handrails (5.13) was significantly larger than that of the sensation felt without gripping the handrails (3.38).",0,0,1,haptic interactions+haptic sensation+virtual reality,
1021,1021,"Female by default exploring the effect of voice assistant gender and pitch on trait and trust attribution. Gendered voice based on pitch is a prevalent design element in many contemporary Voice Assistants (VAs) but has shown to strengthen harmful stereotypes. Interestingly, there is a dearth of research that systematically analyses user perceptions of different voice genders in VAs. This study investigates gender-stereotyping across two different tasks by analyzing the influence of pitch (low, high) and gender (women, men) on stereotypical trait ascription and trust formation in an exploratory online experiment with 234 participants. Additionally, we deploy a gender-ambiguous voice to compare against gendered voices. Our findings indicate that implicit stereotyping occurs for VAs. Moreover, we can show that there are no significant differences in trust formed towards a gender-ambiguous voice versus gendered voices, which highlights their potential for commercial usage.",0,0,1,trust evaluation+trust relationship+telephone,
1022,1022,"What you expect is what you get potential use of contingent negative variation for passive bci systems in gaze based hci. When using eye movements for cursor control in humancomputer interaction (HCI), it may be difficult to find an appropriate substitute for the click operation. Most approaches make use of dwell times. However, in this context the so-called Midas-Touch-Problem occurs which means that the system wrongly interprets fixations due to long processing times or spontaneous dwellings of the user as command. Lately it has been shown that brain-computer interface (BCI) input bears good prospects to overcome this problem using imagined hand movements to elicit a selection. The current approach tries to develop this idea further by exploring potential signals for the use in a passive BCI, which would have the advantage that the brain signals used as input are generated automatically without conscious effort of the user. To explore event-related potentials (ERPs) giving information about the user's intention to select an object, 32-channel electroencephalography (EEG) was recorded from ten participants interacting with a dwell-time-based system. Comparing ERP signals during the dwell time with those occurring during fixations on a neutral cross hair, a sustained negative slow cortical potential at central electrode sites was revealed. This negativity might be a contingent negative variation (CNV) reflecting the participants' anticipation of the upcoming selection. Offline classification suggests that the CNV is detectable in single trial (mean accuracy 74.9 %). In future, research on the CNV should be accomplished to ensure its stable occurence in human-computer interaction and render possible its use as a potential substitue for the click operation.",0,0,1,brain computer interface+user-computer interface+human computer interaction,
1023,1023,"Co design futures for ai and space a workbook sprint. Artificial Intelligence (AI) is continuously moving into our surroundings. In its various forms, it has the potential to disrupt most aspects of human life. Yet, the discourse around AI has long been by experts and for experts. In this paper, we argue for a participatory approach towards designing human-AI interactions. We outline how we used design methodology to organise an interdisciplinary workshop with a diverse group of students – a workbook sprint with 45 participants from four different programs and 13 countries – to develop speculative design futures in five focus areas. We then provide insights into our findings and share our lessons learned regarding our workshop topic – AI and Space – our process, and our research. We learned that involving non-experts in complex technical discourses – such as AI – through the structural rigour of design methodology is a viable approach. We then conclude by laying out how others might use our findings and initiate their own workbook sprint to explore complex technologies in a human-centred way.",0,0,1,expert systems+intelligent systems+distributed artificial intelligence,
1024,1024,"Rethinking financial inclusion from access to autonomy. Financial inclusion has been defined and understood primarily in terms of access, thereby constituting ‘inclusion’/‘exclusion’ as a binary. This paper argues such a view to be myopic that risks treating financial inclusion as an end in itself, and not as means to a larger end. ‘Access’ oriented perspectives also fail to take into account considerations of structural factors like power asymmetries and pay inadequate attention to user practices. Through the case of auto-rickshaw drivers in Bangalore, India, and their use of Ola, a peer-to-peer taxi hailing service similar to Uber, we show that access is a necessary, but not sufficient condition to achieve financial inclusion in a substantive sense. By examining in detail, the financial needs and practices of rickshaw drivers, we identify the opportunities and constraints for digital technology to better support their financial practices and enhance their wellbeing. The paper proposes adding ‘autonomy’ and ‘affordances’ as two crucial factors to be included in the discourse on financial inclusion. Finally, we outline design implications for P2P technologies to contribute towards the financial inclusion of drivers.",0,0,1,viewpoint+peer-to-peer+innovative technologies,
1025,1025,"Virtual reality wound care training for clinical nursing education an initial user study. Wound care is an essential nursing competency, where dressing change is an important component. Compliance with aseptic procedures and techniques is necessary to reduce the risk of infection. Proficiency in the skills can be developed through adequate practice. In this paper, use of virtual reality is proposed to provide more practice opportunity. An immersive virtual environment is developed to simulate the steps of changing a simple wound dressing. Positive comments are obtained from an initial user study on usability with an experienced nurse and an undergraduate nursing student. Comprehensive evaluation will be conducted to further improve the simulation.",0,0,1,virtual environments+virtual reality+immersive virtual environments,
1026,1026,"The pedagogical project of education for sustainable development in 3d virtual space. Nowadays there is no day without some thought- provoking, shocking news about the global climate crisis. Although efforts to achieve environmental education and sustainable development have been present since the 1970s, today we see that an increasing emphasis is put on them not only in the scientific world but also in our daily lives. One of the biggest challenges nowadays is the effective implementation of sustainability education in the school system. In the process of this, the formation of environmentally conscious behavior of young people participating in education and the development of a sense of responsibility are central objectives. We have a guiding role in this during teacher training. It is necessary to equip prospective teachers for the most important activities of environmental education and to make them acquire the effective environmental education methods necessary for successful work. All this in a way that, in addition to direct nature experiences, we adjust to the needs and interests of the digital generation. The study presents a sustainability pedagogical project realized in teacher education by using 3D VR space.",0,0,1,educational technology+virtual spaces+virtual environments,
1027,1027,"Virtual reality technology used as a learning tool in civil engineering training. In the execution of bridge or overpass decks several construction processes are applied. A geometric model 4D (3D + time) in a Virtual Reality (VR) environment which simulates the construction of a bridge deck composed of precast beams was implemented. The model allows viewing and interaction with the various steps and the main elements involved in the construction process. In order to develop the virtual model, the components of the construction, the steps inherent in the process and its follow-up and the type and mode of operation of the required equipment were initially examined, in detail. Based on this study, the 3D geometric modelling of the different elements that make up the site was created and a schedule that would simulate an interactive mode of construction activity was established. As the model is interactive, it allows the user to have access to different stages of the construction process, thus allowing different views in time and in space throughout the development of construction work, thereby supporting understanding of this constructive method. Since the model is didactic in character it can be used to support the training of students and professionals in the field of Bridge Construction. The VR application is currently used to support Bridge classes.",0,0,1,virtual spaces+virtual environments+virtual reality,
1028,1028,"A virtual reality simulator for training gaze control of wheeled tele robots. People who cannot use their hands may use eye-gaze to interact with robots. Emerging virtual reality head-mounted displays (HMD) have built-in eye-tracking sensors. Previous studies suggest that users need substantial practice for gaze steering of wheeled robots with an HMD. In this paper, we propose to apply a VR-based simulator for training of gaze-controlled robot steering. The simulator and preliminary test results are presented.",0,0,1,virtual environments+virtual reality+head mounted displays,
1029,1029,"Visualizing museum visitors behavior where do they go and what do they do there. Museum curators and personnel are interested in understanding what is happening at their museum: what exhibitions and exhibits do visitors attend to, what exhibits visitors spend most time at, what hours of the day are most busy at certain areas in the museum and more. We use automatic tracking of visitors' position, movements and interaction at the museum to log visitor information. Using this information, we provide an interface that visualizes individual and small group movement patterns, presentations watched, and aggregated information of overall visitor engagement at the museum. We utilized a user centered design approach in which we gathered requirements, iteratively designed and implemented a working prototype and evaluated it with the help of domain experts (museum curators and other museum personnel). We describe our efforts and provide insights from the design and evaluation of our system, and outline how it might be generalized for other indoor domains such as supermarkets or shopping malls.",0,0,1,supermarket+automatic tracking+user centered designs,
1030,1030,"On research ethics and representation in virtual reality. This position paper approaches issues in the ethics of virtual reality particularly around notions of art as academic research, collaborative ethnographic media production, consent, and representation. These issues are explored through examples of research with the Yong community in Thailand, investigating the use of 360° video for cultural documentation and preservation. Possible ways of addressing some of the ethical issues in virtual reality are suggested, drawing upon collaborative ethnography, collaborative media production, and Value-Sensitive Design.",0,0,1,virtual environments+virtual reality+mixed reality,
1031,1031,"Magnetoelectric sensors for biomagnetic measurements. Magnetoelectric sensors are assumed to be capable of biomagnetic measurements like magnetoencephalography (MEG) and magnetocardiography (MCG). In this paper a thin film sensor with a ME coefficient of α ME  = 450 V/cmOe at a resonance frequency of f res  = 678.5 Hz is investigated. Measurements show that this sensor reaches a noise level of 7.5 pT/√Hz at resonance and that field strengths in the picotesla range can be detected, which is well in the range of biomagnetic signals.",0,0,1,wireless sensor+sensors+sensor data,
1032,1032,"Nonvisual presentation and navigation within the structure of digital text documents on mobile devices. This paper introduces a novel concept for an assistive technology in support of blind and visually impaired persons for nonvisual presentation and navigation within the structure of digital text-documents on mobile devices (smart phones, internet tablets, etc.) which enables them to get a fast overview over the structure of an entire document. The advantages compared to state of the art screen readers are that it enables them to identifying the type, position, length and context of each logical structure element, the current cursor position within the document as well as to accessing any structure element at any time in an arbitrary order. For the nonvisual presentation of the document structure elements auditory icons, tones and vibration feedback are used. Navigation and cursor routing is provided by performing gestures on the touch screen. There is no expensive special hardware required.",0,0,1,assistive technology+visually impaired+screen readers,
1033,1033,"Interweaving multimodal interaction with flexible unit visualizations for data exploration. Multimodal interfaces that combine direct manipulation and natural language have shown great promise for data visualization. Such multimodal interfaces allow people to stay in the flow of their visual exploration by leveraging the strengths of one modality to complement the weaknesses of others. In this article, we introduce an approach that interweaves multimodal interaction combining direct manipulation and natural language with flexible unit visualizations. We employ the proposed approach in a proof-of-concept system, DataBreeze. Coupling pen, touch, and speech-based multimodal interaction with flexible unit visualizations, DataBreeze allows people to create and interact with both systematically bound (e.g., scatterplots, unit column charts) and manually customized views, enabling a novel visual data exploration experience. We describe our design process along with DataBreeze's interface and interactions, delineating specific aspects of the design that empower the synergistic use of multiple modalities. We also present a preliminary user study with DataBreeze, highlighting the data exploration patterns that participants employed. Finally, reflecting on our design process and preliminary user study, we discuss future research directions.",0,0,1,data visualization+visual exploration+multi-modal interfaces,
1034,1034,"Incorporating constraints into matrix factorization for clothes package recommendation. Recommender systems have been widely applied in the literature to suggest individual items to users. In this paper, we consider the harder problem of package recommendation, where items are recommended together as a package. We focus on the clothing domain, where a package recommendation involves a combination of a 'top' (e.g. a shirt) and a 'bottom' (e.g. a pair of trousers). The novelty in this work is that we combined matrix factorisation methods for collaborative filtering with hand-crafted and learnt fashion constraints on combining item features such as colour, formality and patterns. Finally, to better understand where the algorithms are underperforming, we conducted focus groups, which lead to deeper insights into how to use constraints to improve package recommendation in this domain.",0,0,1,collaborative filtering+recommender systems+recommendation,
1035,1035,"Towards person centered anomaly detection and support system for home dementia care. Anomaly detection is a crucial issue for people with dementia and their families to live a safe and comfortable life at home. The elderly monitoring system is a promising solution. However, the conventional systems have limitations in detectable anomalies and support actions, which cannot fully cover individual needs. To achieve more person-centered home care for people with dementia, our research group has been studying environmental sensing with IoT. In this paper, using the environmental sensing, we propose a new service that allows individual users to customize definition of anomaly and corresponding actions. Specifically, borrowing a mechanism of context-aware services, we regard every anomaly observed within the house as a context. We then define every care as an action bound to an anomaly context. This achieves the personalized anomaly detection and care. To demonstrate the feasibility, we implement a prototype system and conduct a practical case study.",0,0,1,anomalous behavior+internet of things+context aware services,
1036,1036,"Connect and connectivity revealing a world of interactions. Connectivity is embedded into our modern day society. Devices increasingly rely on permanent network connections, and people keep connected through social networks. Technological advances allow everyday objects to become part of large networks of interconnected entities. Connectivity within these networks allows for the design of novel interaction methods that utilise the digital input and output capabilities of connected entities. However, when specifically designing for interaction, entities become entangled and remain oblivious of each other's features. In this paper we report on the current progress in opening up the space of connectivity in order to reveal and make use of the available technological capabilities. We describe how this will open channels for new synergy and novel interaction methods. We conclude by discussing the preparation of a case study which incorporates our initial designs and proof of concepts.",0,0,1,communication networks+social networks+multi-modal interactions,
1037,1037,"Danger from the deep a gap affordance study in augmented reality. It is an open question as to whether people perceive and act in augmented reality environments in the same way that they do in real environments. The current work investigated participants' judgments of whether or not they could act on an obstacle portrayed with augmented reality. Specifically, we presented gaps of varying widths and depths to participants in augmented reality using the Microsoft HoloLens. We asked users to indicate whether or not they believed that they could step across the virtual gaps given their widths and depths. Averaging across changes in width and depth, users generally underestimated their abilities to cross gaps. However, this underestimation was significantly greater when the gaps were deep. Thus, our findings suggest that users in augmented reality respond with more conservative judgments when presented with risky stimuli—a response that mimics real world behavior. Their altered reactions to deeper gaps may provide early evidence for augmented reality's capacity to evoke a sense of realism or immersion and its use in evaluating perception and action.",0,0,1,virtual spaces+augmented reality+head mounted displays,
1038,1038,"Interpretation and construction of meaning of bliss words in children. Blissymbolics as a graphic symbol system has the potential to represent a large number of vocabulary items using a small number of basic Bliss-characters. The aim of this project was to investigate how children with typical development, aged 3 years to 7 years 11 months, interpreted Bliss-characters and compound Bliss-words and then constructed their own Bliss-words. Children participated in a teaching procedure that included explanations and feedback on the structure and meaning of Bliss-characters. Their spontaneous interpretations of Bliss-characters and their ability to construct new Bliss-words were explored. Results suggest that Bliss-characters, although not very transparent, evoked spontaneous linguistic activities and were retained after brief explanations. Children aged 5 years and older appeared to discover the logic of the structure of Bliss-words. Children of all ages used Bliss-characters to represent new ideas. Some chose Bliss-characters resembling adult representations of concepts, others chose Bliss-characters representing personal associations. In sum, children retained many of the Bliss-characters after a relatively brief exposure and demonstrated semantic creativity in interpretation and construction of Bliss-words.",0,0,1,semantics+linguistics+vocabulary,
1039,1039,"Psychometric properties of measurements obtained with the marlowe crowne social desirability scale in an icelandic probability based internet sample. Previously, measurements obtained with the MCSDS in Internet mode have not been validated.The present study relies on a population representative sample, previous studies have relied on non-probability samples.Reliability estimates were in line with results from previous studies of MCSDS data.The CFA findings generally support the unidimensionality assumption of the MCSDS.Results indicate a need to focus on the content and quality of individual items. Internet surveys have become a very popular research tool. Relatively little attention has, however, been devoted to the possible changes in psychometric properties when measurements are obtained with Internet surveys. The Marlowe-Crowne Social Desirability Scale (MCSDS) is the most widely used instrument for measuring the tendency to respond in a socially desirable way and is often used to validate other measures. The purpose of the current research is to evaluate the dimensionality and reliability of measurements obtained with the MCSDS and short forms of the scale in an Internet sample of the general public in Iceland. An e-mail invitation was sent to a sample of 1200 panel members drawn from the Social Science Research Institute (SSRI) probability based panel, of those 536 participants completed all items on the MCSDS. Reliability estimates were in line with results from previous studies (α=.81 for the MCSDS data and α ranging from .59 to .75 for short forms). Using confirmatory factor analysis, a good fit was obtained for a one-factor model of measurements obtained with the MCSDS and its short forms (apart from significant chi square values in all cases but one), which generally supports the assumption of unidimensionality.",0,0,1,internet+world wide web+intranets,
1040,1040,"Euclidean distance transform shadow mapping. The high-quality simulation of the penumbra effect in real-time shadows is a challenging problem in shadow mapping. The existing shadow map filtering techniques are prone to aliasing and light leaking artifacts which decrease the shadow visual quality. In this paper, we aim to minimize both problems with the Euclidean distance transform shadow mapping. To reduce the perspective aliasing artifacts generated by shadow mapping, we revectorize the hard shadow boundaries using the revectorization-based shadow mapping. Then, an exact normalized Euclidean distance transform is computed in the user-defined penumbra region to simulate the penumbra effect. Finally, a mean filter is applied to further suppress skeleton artifacts generated by the distance transform. The results obtained show that our technique runs entirely on the GPU, produces less artifacts than related work, and provides real-time performance.",0,0,1,visual qualities+gpu+interreflections,
1041,1041,"Characteristics of single case designs used to assess intervention effects in 2008. This article reports the results of a study that located, digitized, and coded all 809 single-case designs appearing in 113 studies in the year 2008 in 21 journals in a variety of fields in psychology and education. Coded variables included the specific kind of design, number of cases per study, number of outcomes, data points and phases per case, and autocorrelations for each case. Although studies of the effects of interventions are a minority in these journals, within that category, single-case designs are used more frequently than randomized or nonrandomized experiments. The modal study uses a multiple-baseline design with 20 data points for each of three or four cases, where the aim of the intervention is to increase the frequency of a desired behavior; but these characteristics vary widely over studies. The average autocorrelation is near to but significantly different from zero; but autocorrelations are significantly heterogeneous. The results have implications for the contributions of single-case designs to evidence-based practice and suggest a number of future research directions.",0,0,1,education+systems design+cad tool,
1042,1042,"The pen is mightier understanding stylus behaviour while inking on tablets. Although pens and paper are pervasive in the analog world, their digital counterparts, styli and tablets, have yet to achieve the same adoption and frequency of use. To date, little research has identified why inking experiences differ so greatly between analog and digital media or quantified the varied experiences that exist with stylus-enabled tablets. By observing quantitative and behavioural data in addition to querying preferential opinions, the experimentation reaffirmed the significance of accuracy, latency, and unintended touch, whilst uncovering the importance of friction, aesthetics, and stroke beautification to users. The observed participant behaviour and recommended tangible goals should enhance the development and evaluation of future systems.",0,0,1,digital media+tangible interfaces+tangible user interfaces,
1043,1043,"A foundation for the persuasive gameplay experience. Games are increasingly used for purposes that stretch be-yond their primary strength as medium for entertainment.The interactive nature of games provide players with variousopportunity to deal with complex (societal) issues on a moreinvolved and personal level, far more than any other mediumaﬀords. As such it is not surprising to see that games oﬀer agreat platform for persuading players to adopt a particularperspective on events that occur in the real, physical, world.Games for persuasion, or games for attitude-change, havebeen a topic of discussion over the past decade. Concretedesign strategies however, to analyze persuasive gameplayor guide the designer in embedding persuasive messages ingameplay, are scarce. As such, to advance the disciplinewe have set our focus on the development of strategies thataid the persuasive game design process. In this paper we de-scribe the Attitudinal Gameplay Model as foundation for thePersuasive Gameplay Experience. The model serves as anoverview of what game elements can be utilized for persua-sion, how they are interrelated and what mental processesof the player are important to take into account.",0,0,1,gameplay+game design+player experience,
1044,1044,"Exploring photobios. We present an approach for generating face animations from large image collections of the same person. Such collections, which we call photobios, sample the appearance of a person over changes in pose, facial expression, hairstyle, age, and other variations. By optimizing the order in which images are displayed and cross-dissolving between them, we control the motion through face space and create compelling animations (e.g., render a smooth transition from frowning to smiling). Used in this context, the cross dissolve produces a very strong motion effect; a key contribution of the paper is to explain this effect and analyze its operating range. The approach operates by creating a graph with faces as nodes, and similarities as edges, and solving for walks and shortest paths on this graph. The processing pipeline involves face detection, locating fiducials (eyes/nose/mouth), solving for pose, warping to frontal views, and image comparison based on Local Binary Patterns. We demonstrate results on a variety of datasets including time-lapse photography, personal photo collections, and images of celebrities downloaded from the Internet. Our approach is the basis for the Face Movies feature in Google's Picasa.",0,0,1,facial images+facial expression+internet,
1045,1045,"Evaluation of deep learning based pose estimation for sign language recognition. Human body pose estimation and hand detection are two important tasks for systems that perform computer vision-based sign language recognition(SLR). However, both tasks are challenging, especially when the input is color videos, with no depth information. Many algorithms have been proposed in the literature for these tasks, and some of the most successful recent algorithms are based on deep learning. In this paper, we introduce a dataset for human pose estimation for SLR domain. We evaluate the performance of two deep learning based pose estimation methods, by performing user-independent experiments on our dataset. We also perform transfer learning, and we obtain results that demonstrate that transfer learning can improve pose estimation accuracy. The dataset and results from these methods can create a useful baseline for future works.",0,0,1,estimation method+sign language recognition+hand detection,
1046,1046,"Oneday shoes a maker toolkit to understand the role of co manufacturing in personalization. Personalization of shoes is of increasing importance to designers, design researchers, and manufacturers as mass customization progresses towards ultra personalized product service systems. Many attempts have been made to design co-creation platforms that allow end users to personalize their own shoes. Those co-creation platforms primarily concentrate on color selection. This research takes a different approach and designs a toolkit for maker-oriented users to co-manufacture their own shoes. The toolkit was designed in different levels and deployed to makers via crowdsharing worldwide. Backers were surveyed before deployment and interviewed after two years to understand personalization over a larger amount of time with the research product. We find that users who have greater bespoke tools and materials in their toolkits are more likely to personalize their shoes while co-manufacturing. The research provides guidelines for researchers and designers creating toolkits, designing personalization product service systems/configurators and engaging in tangible bespoke processes.",0,0,1,personalized service+personalizations+tangible user interfaces,
1047,1047,"Making software tutorial video responsive. Tutorial videos are widely available to help people use software. These videos, however, are viewed by users as captured and offer little direct interaction between users and software. This paper presents a video navigation method that allows users to interact with software tutorial video as if they were using the software. To make the tutorial video responsive, our method records the user interaction events like mouse click and drag during capturing the video. Our method then analyzes, selects, and visualizes these user interaction events at the event locations. When a user directly interacts with an event visualization, our method automatically navigates to the proper video frame to provide the visual feedback as if the software were responding to the user input. Thus, our method provides the experience of interacting with the software through directly manipulating the tutorial video. Our study shows our method can better help users follow tutorial videos to complete tasks than the baseline timeline interface.",0,0,1,digital videos+video streams+visualization,
1048,1048,"Haptic palpation for medical simulation in virtual environments. Palpation is a physical examination technique where objects, e.g., organs or body parts, are touched with fingers to determine their size, shape, consistency and location. Many medical procedures utilize palpation as a supplementary interaction technique and it can be therefore considered as an essential basic method. However, palpation is mostly neglected in medical training simulators, with the exception of very specialized simulators that solely focus on palpation, e.g., for manual cancer detection. In this article we propose a novel approach to enable haptic palpation interaction for virtual reality-based medical simulators. The main contribution is an extensive user study conducted with a large group of medical experts. To provide a plausible simulation framework for this user study, we contribute a novel and detailed interaction algorithm for palpation with tissue dragging, which utilizes a multi-object force algorithm to support multiple layers of anatomy and a pulse force algorithm for simulation of an arterial pulse. Furthermore, we propose a modification for an off-the-shelf haptic device by adding a lightweight palpation pad to support a more realistic finger grip configuration for palpation tasks. The user study itself has been conducted on a medical training simulator prototype with a specific procedure from regional anesthesia, which strongly depends on palpation. The prototype utilizes a co-rotational finite-element approach for soft tissue simulation and provides bimanual interaction by combining the aforementioned techniques with needle insertion for the other hand. The results of the user study suggest reasonable face validity of the simulator prototype and in particular validate medical plausibility of the proposed palpation interaction algorithm.",0,0,1,haptic interactions+virtual reality+interaction techniques,
1049,1049,"Implementation of a moving omnidirectional access point robot and a position detecting system. Recently, various communication technologies have been developed in order to satisfy the requirements of many users. Especially, mobile communication technology continues to develop rapidly and Wireless Mesh Networks (WMNs) are attracting attention from many researchers in order to provide cost efficient broadband wireless connectivity. The main issue of WMNs is to improve network connectivity and stability in terms of user coverage. In this paper, we implement a moving omnidirectional access point robot (called MOAP robot) and propose a position detecting system for the robot. In order to realize moving Access Points, the MOAP robot should move to omnidirectional in 2 dimensional space. It is important that the MOAP robot moves to an accurate position in order to have a good connectivity. Thus, MOAP robot can provide good communication and stability for WMNs.",0,0,1,wireless mesh networks (wmn)+wireless mesh+broadband,
1050,1050,"Learning from history predicting reverted work at the word level in wikipedia. Wikipedia's remarkable success in aggregating millions of contributions can pose a challenge for current editors, whose hard work may be reverted unless they understand and follow established norms, policies, and decisions and avoid contentious or proscribed terms. We present a machine learning model for predicting whether a contribution will be reverted based on word level features. Unlike previous models relying on editor-level characteristics, our model can make accurate predictions based only on the words a contribution changes. A key advantage of the model is that it can provide feedback on not only whether a contribution is likely to be rejected, but also the particular words that are likely to be controversial, enabling new forms of intelligent interfaces and visualizations. We examine the performance of the model across a variety of Wikipedia articles.",0,0,1,hierarchical model+dbpedia+visualization,
1051,1051,"Sauron embedded single camera sensing of printed physical user interfaces. 3D printers enable designers and makers to rapidly produce physical models of future products. Today these physical prototypes are mostly passive. Our research goal is to enable users to turn models produced on commodity 3D printers into interactive objects with a minimum of required assembly or instrumentation. We present Sauron, an embedded machine vision-based system for sensing human input on physical controls like buttons, sliders, and joysticks. With Sauron, designers attach a single camera with integrated ring light to a printed prototype. This camera observes the interior portions of input components to determine their state. In many prototypes, input components may be occluded or outside the viewing frustum of a single camera. We introduce algorithms that generate internal geometry and calculate mirror placements to redirect input motion into the visible camera area. To investigate the space of designs that can be built with Sauron along with its limitations, we built prototype devices, evaluated the suitability of existing models for vision sensing, and performed an informal study with three CAD users. While our approach imposes some constraints on device design, results suggest that it is expressive and accessible enough to enable constructing a useful variety of devices.",0,0,1,geometry+user interfaces+3d printers,
1052,1052,"The influence of officer equipment and protection on short sprinting performance. Abstract   As advances in protective equipment are made, it has been observed that the weight law enforcement officers must carry every day is greatly increasing. Many investigations have noted the health risks of these increases, yet none have looked at its effects on officer mobility. The primary purpose of this study was to examine the influence of both the weight of officer safety equipment, as well as a lateral focal point (FP), on the stride length, stride velocity, and acceleration of the first six strides of a short sprint. Twenty male law enforcement students performed two maximal effort sprint trials, in the participating college's gymnasium, from each of four starting positions: forwards (control position), backwards, 90° left, and 90° right. Subjects placed in the FP group (n = 9) were required to maintain focus on lateral FP during the 90° left and 90° right trials, and a forwards FP during the backwards trials. On a second testing date, subjects repeated the sprint tests while wearing a 9.07 kg weight belt, simulating officer equipment and protective gear. The belt averaged 11.47 ± 1.64% of subject body mass. A significant main effect of weight belt trials was found (F = 20.494, p",0,0,1,policy enforcement+enforcement mechanisms+telecommunication equipment,
1053,1053,"Learning from playing with microworlds in policy making an experimental evaluation in fisheries management. This study determines whether stakeholders learn from playing with microworlds. This is investigated through a case study of Belgian fisheries management. Policymakers, scientists and fishermen participated in a ''before-after with control group''-experiment in which they played with a microworld that aims at gaining insight into the long-term effect of policy instruments on the Belgian fisheries system. The outcome of this experiment indicates that using the microworld did not result in learning outcomes (i.e., changes in participants' subjective knowledge, attitude and behavioural intention towards policy instruments). This however, contradicts all stakeholders' reports that they had learned from the microworld and that they had confidence in the microworld and perceived the microworld to be valid. Hence, three alternative explanations for these results are discussed: (1) methodological issues blocked the detection of learning outcomes, (2) the way in which the microworld was administered did not result in learning outcomes, or (3) participants have not ''learned'' anything new from the microworld. Finally, the paper ends with discussing guidelines and further steps in evaluating learning from microworlds.",0,0,1,information management+learning environments+subjective quality assessments,
1054,1054,"Demand analysis for an augmented reality based assembly training. The first head-mounted display (HMD) was developed in 1986 by Ivan Sutherland. Since then, augmented reality (AR) applications are largely limited to prototypes. One reason might be the lack of user comprehension regarding user requirements. In order to develop a HMD-based training system for daily use, it is important to understand user demands. This paper aims at presenting a demand analysis for an industrial engine assembly line at the car manufacturer BMW Group. We describe the background of training within industries (TWI) and present extant AR-based training studies. We use the value proposition design method to analyze our customers and classify our requirements with a common quality management method, the Kano Model. We find four ""must-have"" requirements for a HMD-based training system, which are crucial for the development of a minimal viable solution. In order to familiarize assembly employees with augmented reality applications, we design a simple demonstration for the Microsoft HoloLens. Additionally, we present future research directions focusing on comprehensive user studies in real industrial environments.",0,0,1,mobile augmented reality+augmented reality+head mounted displays,
1055,1055,"Avatar creation in virtual worlds behaviors and motivations. Abstract   Avatar creation has become common for people to participate and interact in virtual worlds. Using an online survey (N = 244), we investigated both the behavioral characteristics and major motivations for avatar creation in virtual worlds. Our results suggest that a majority of the participants had multiple avatars; these avatars’ appearance did not merely resemble the human players; and their personality did not necessarily mirror the player’s real personality. Furthermore, participants on average spent over 20 h per week and often interacting with others in the virtual worlds. Our exploratory factor analysis yielded four major motivations: virtual exploration, social navigation, contextual adaptation, and identity representation.",0,0,1,avatar+virtual worlds+virtual environments,
1056,1056,"Pipelineprofiler a visual analytics tool for the exploration of automl pipelines. In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.",0,0,1,visualization+visualization tools+visual analytics,
1057,1057,"A spatially explicit classification model for affective computing in built environments. We explore a wearables and sensors centric approach for collecting data in built environments. In addition, we propose a design methodology as a focus to assist in the design of applications and experiments for affective computing. The implications of such systems is to aid in the reproducibility of experiments and better intelligent systems.",0,0,1,sensors+affective computing+wearable computers,
1058,1058,"Agency over social media use can be enhanced through brief abstinence but only in users with high cognitive reflection tendencies. Abstract Many social media users have lost some agency over the use of these sites. Restoring this sense of agency is important as it can help users live responsibly with the technology, and can serve as a target for therapists treating people with difficulty to control their social media use. Nevertheless, knowledge about ways to increase people's sense of agency has been limited. In this study I propose that invoking reflections about agency but allowing normal use will likely produce realization about loss of agency, and result in undesirable reduced sense of agency. In contrast, I suggest that if invoking reflections on agency is followed by a brief abstinence attempt, people will process insights on their actual ability to exert control over social media use, which will result in an increase in perceived agency. I further argue that this information processing will only accrue in people high in cognitive reflection tendencies. A 2 (time: pre vs. post) by 2 (condition: abstinence vs. control) by 2 (cognitive reflection group: low vs. high) experiment with 610 Facebook users showed an increase in agency only among high cognitive reflection participants who experienced abstinence; all other groups showed decline in perceived agency. Implications are discussed.",0,0,1,social media+facebook+social networking sites,
1059,1059,"Augmented places. AimThe purpose of this paper was to examine the possibility of using Augmented Reality to reduce memory-related ethnic bias towards places and to increase understanding of multicultural place meaning and change emotional attitudes towards historical sites. BackgroundTheoretical and empirical works suggest that the direct, embodied contact with place's history facilitates understanding of its past. Places which were deprived of historical continuity are less liked and threatened by ethnic bias in collective memory. AR technology gives a possibility of recreating past of such places and in a consequence tests an influence of experience of place's history on psychological aspects of the people-place relationship. MethodA between-subjects experiment was conducted, in which participants either walked with an AR application displaying historical photos in real environment or watched photos on computers. The chosen place was the former Jewish district in Warsaw, Poland. ResultsThe results of multiple regression analyses showed that the AR application can facilitate positive attitudes towards a place, reduce ethnic bias and enhance multicultural place meaning. ConclusionsWe argue that AR could be used as a method of reviving (multi)cultural heritage, but also as a tool of reducing prejudices and increasing openness to other cultures and traditions. We tested applicability of Augmented Reality in environmental and social psychology.AR application was used to recreate embodied experience of place's past.AR experience significantly increased emotional attitude towards historical place.AR experience led to increased awareness of multicultural place meaning.AR experience significantly contributed to decrease of memory-related ethnic bias.",0,0,1,social aspect+augmented reality+ar application,
1060,1060,"The perception of visual uncertaintyrepresentation by non experts. We tested how non-experts judge point probability for seven different visual representations of uncertainty, using a case from an unfamiliar domain. Participants (n = 140) rated the probability that the boundary between two earth layers passed through a given point, for seven different visualizations of the positional uncertainty of the boundary. For all types of visualizations, most observers appear to construct an internal model of the uncertainty distribution that closely resembles a normal distribution. However, the visual form of the uncertainty range (i.e., the visualization type) affects this internal model and the internal model relates to participants' numeracy. We conclude that perceived certainty is affected by its visual representation. In a follow-up experiment we found no indications that the absence (or presence) of a prominent center line in the visualization affects the internal model. We discuss if and how our results inform which visual representation is most suitable for representing uncertainty and make suggestions for future work.",0,0,1,observer+visualization+visualization tools,
1061,1061,"A security assessment of tiles a new portfolio based graphical authentication system. In this paper we propose Tiles, a graphical authentication system in which users are assigned a target image and subsequently asked to select segments of that image. We assess the extent to which this system provides protection against two security threats: observation attacks and sharing of authentication credentials in two laboratory-based studies. We note some of the vulnerabilities of the new system but provide evidence that automated manipulation of the similarity of the decoy images can help mitigate the threat from verbal sharing and observation attacks.",0,0,1,security issues+authentication+authentication protocols,
1062,1062,"Creating the spectacle designing interactional trajectories through spectator interfaces. An ethnographic study reveals how professional artists created a spectator interface for the interactive game Day of the Figurines, designing the size, shape, height and materials of two tabletop interfaces before carefully arranging them in a local setting. We also show how participants experienced this interface. We consider how the artists worked with a multi-scale notion of interactional trajectory that combined trajectories through individual displays, trajectories through a local ecology of displays, and trajectories through an entire experience. Our findings shed light on discussions within HCI concerning interaction with tangible and tabletop displays, spectator interfaces, ecologies of displays, and trajectories through cultural experiences.",0,0,1,display devices+display system+interactive tabletop,
1063,1063,"Deep facial action unit recognition and intensity estimation from partially labelled data. Research on facial action unit (AU) analysis typically require facial images that are labelled with those action units. While unlabelled facial images abound, labelling those images with action units or intensity is costly and time-consuming. Our approach makes it possible to analyze facial AUs when only some of the images have been labelled. We use many facial images to learn a deep framework that is able to take advantage of the facial representations. A restricted Boltzmann machine uses the available AU annotations to learn the AU label or intensity distribution. We train a support vector machine for AU recognition and a support vector regression for AU intensity estimation by maximizing the log likelihood of the AU mapping functions, taking into account the learned multiple AU distribution for all training data, while simultaneously diminishing errors between the predicted action units and ground-truth action unit occurrence or intensities for all labelled data. We perform experiments on two databases. The results demonstrate the superiority of a deep neural network for learning facial features, as well as the benefit of action unit label or intensity constraints for action unit occurrence recognition or intensity estimation in fully or semi-supervised scenarios.",0,0,1,action unit+facial images+facial action,
1064,1064,"Ambiguous agents the influence of consistency of an artificial agent s social cues on emotion recognition recall and persuasiveness. ABSTRACTThis article explores the relation between consistency of social cues and persuasion by an artificial agent. Including (minimal) social cues in Persuasive Technology (PT) increases the probability that people attribute human-like characteristics to that technology, which in turn can make that technology more persuasive (see, e.g., Nass, Steuer, Tauber, & Reeder, 1993). PT in the social actor role can be equipped with a variety of social cues to create opportunities for applying social influence strategies (for an overview, see Fogg, 2003). However, multiple social cues may not always be perceived as being consistent, which could decrease their perceived human-likeness and their persuasiveness. In the current article, we investigate the relation between consistency of social cues and persuasion by an artificial agent. Findings of two studies show that consistency of social cues increases people’s recognition and recall of artificial agents’ emotional expressions, and make those agents more persuasi...",0,0,1,social influence+emotional expressions+emotion expression,
1065,1065,"Fragmentation and transition understanding perceptions of virtual possessions among young adults in spain south korea and the united states. People worldwide are increasingly acquiring collections of virtual possessions. While virtual possessions have become ubiquitous, little work exists on how people value and form attachments to these things. To investigate, we conducted a study with 48 young adults from South Korea, Spain and the United States. The study probed on participants' perceived value of their virtual possessions as compared to their material things, and the comparative similarities and differences across cultures. Findings show that young adults live in unfinished spaces and that they often experience a sense of fragmentation when trying to integrate their virtual possessions into their lives. These findings point to several design opportunities, such as tools for life story-oriented archiving, and insights on better forms of Cloud storage.",0,0,1,long-term preservation+virtual spaces+virtual environments,
1066,1066,Toward interoperability in a web of things. In this position paper we explore the challenges and issues around interoperability in the web of things. A key concern is how to increase interoperability while maintaining a high degree of innovation and exploration in the community. To that end we propose a hub- centric approach toward interoperability consisting of four levels or stages. We are working to validate this approach in the context of a large-scale IoT ecosystem project consisting of eight IoT hubs in different domains where a key requirement is hub-to-hub and hub- application interoperability.,0,0,1,internet of things+web content+world wide web,
1067,1067,"The blockchain and the commons dilemmas in the design of local platforms. This paper addresses the design dilemmas that arise when distributed ledger technologies (DLT) are to be applied in the governance of artificial material commons. DLTs, such as blockchain, are often presented as enabling technologies for self-governing communities, provided by their consensus mechanisms, transparent administration, and incentives for collaboration and cooperation. Yet, these affordances may also undermine public values such as privacy and displace human agency in governance procedures. In this paper, the conflicts regarding the governance of communities which collectively manage and produce a commons are discussed through the case of a fictional energy community. Three mechanisms are identified in this process: tracking use of and contributions to the commons; managing resources, and negotiating the underlying rule sets and user rights. Our effort is aimed at contributing to the HCI community by introducing a framework of three mechanisms and six design dilemmas that can aid in balancing conflicting values in the design of local platforms for commons-based resource management.",0,0,1,privacy+systems design+interaction design,
1068,1068,"Collaborative problem solving in local and remote vr situations. Virtual Reality supports collaboration among partners across departments and fields, independent of physical boundaries. Virtual reality applications can solve the time and cost consuming logistic problem that companies encounter when sending experts to remote locations. However, it is not yet clear how effective partners collaborate when in remote locations. In one experiment, we examined whether partners who are physically in the same room and interact with each other before they start collaborating affects performance compared to collaborators who meet and interact only within the virtual space. Participants had to solve a Rubik's cube type three-dimensional puzzle by arranging cubes that varied in color within a solution space in such a way, so that each side of the solution space showed a single color. Participants were immersed within a virtual environment and in one condition participants were collocated in the same room (local condition), while in the other one they were located in different rooms (remote condition). Results showed that collaborators in both conditions successfully completed the task but performance was better during the local compared to the remote condition.",0,0,1,virtual spaces+virtual environments+virtual reality,
1069,1069,"Gaze tracking as a game input interface. The purpose of this paper is to present current findings as well as previous work in the field of gaze tracking and propose developing methods of applying them in games with the purpose of enhancing the gameplay experience. First, eye-tracking as a technology and historical aspects of it are briefly presented. A brief summary of important findings and a review of relevant works are presented. Strengths and weaknesses of eye-tracking in games are then discussed. The article goes on to describe the origin, concept and key elements of a future application and experiment for testing gaze-tracking in a game scenario.",0,0,1,computer games+gameplay+player experience,
1070,1070,"Design and implementation of a robust acoustic recognition system for waterbird species using tms320c6713 dsk. In this paper, a new real-time approach for audio recognition of waterbird species in noisy environments, based on a Texas Instruments DSP, i.e. TMS320C6713 is proposed. For noise estimation in noisy water bird's sound, a tonal region detector TRD using a sigmoid function is introduced. This method offers flexibility since the slope and the mean of the sigmoid function can be adapted autonomously for a better trade-off between noise overvaluation and undervaluation. Then, the features Mel Frequency Cepstral Coefficients post processed by Spectral Subtraction MFCC-SS were extracted for classification using Support Vector Machine classifier. A development of the Simulink analysis models of classic MFCC and MFCC-SS is described. The audio recognition system is implemented in real time by loading the created models in DSP board, after being converted to target C code using Code Composer Studio. Experimental results demonstrate that the proposed TRD-MFCC-SS feature is highly effective and performs satisfactorily compared to conventional MFCC feature, especially in complex environment.",0,0,1,dsp+acoustics+texas instruments,
1071,1071,"Drawing large graphs by multilevel maxent stress optimization. Drawing large graphs appropriately is an important step for the visual analysis of data from real-world networks. Here we present a novel multilevel algorithm to compute a graph layout with respect to the maxent-stress metric proposed by Gansner et al. (2013) that combines layout stress and entropy. As opposed to previous work, we do not solve the resulting linear systems of the maxent-stress metric with a typical numerical solver. Instead we use a simple local iterative scheme within a multilevel approach. To accelerate local optimization, we approximate long-range forces and use shared-memory parallelism. Our experiments validate the high potential of our approach, which is particularly appealing for dynamic graphs. In comparison to the previously best maxent-stress optimizer, which is sequential, our parallel implementation is on average 30 times faster already for static graphs (and still faster if executed on a single thread) while producing a comparable solution quality.",0,0,1,parallelism+visual analysis+graph layout,
1072,1072,"Massively multiplayer online role playing games mmorpgs and socio emotional wellbeing. Expanding and building on our previous study, the current study mainly explored the degree to which older adults' social interactions in Massively Multiplayer Online Role-Playing Games (MMORPGs) are associated with four socio-emotional factors. A total of 222 older World of Warcraft players, recruited online, completed a Web questionnaire. Consistent with the findings of the previous study, older adults' socio-emotional wellbeing was associated with the quality of guild play and enjoyment of relationship. Socio-emotional wellbeing was not associated with the amount of gameplay, intensity of interaction, network level and social motivation for playing MMORPGs. In addition, the findings also indicated that a large number of older adults developed meaningful online relationships with their game friends, but that it would be difficult for them to integrate these online friends into their real lives. Quality of guild play wast mostly related to older adults' socio-emotional wellbeing.Participating in well-organized guilds seems to improve socio-emotional wellbeing.Enjoyment of relationships also contributed to older adults' wellbeing.Many older adults developed meaningful online relationships with game friends.It would be difficult for older adults to integrate online friends into real life.",0,0,1,gameplay+massively multiplayer+mmorpg,
1073,1073,"The hub facilitating walking meetings through a network of interactive devices. Walking meetings are a promising means to improve healthy behavior at work. By providing a physically active way of working, walking meetings can reduce our sitting time. Several obstacles that limit the social acceptance and wider adoption of walking meeting practice have been highlighted in previous research. Amongst these, the difficulty to take notes or present files is a recurring concern for office workers. To address these barriers, we designed the Hub, a network of stand-up meeting stations that accommodate different work-related tasks during walking meetings. We report on two pilot user tests investigating users' experiences and ideas for improvement, and present future research steps. We discuss the usefulness and relevance of the Hub concept to overcome the obstacles associated with walking meetings.",0,0,1,moving obstacles+office workers+teleconference,
1074,1074,"Personalized reminiscence therapy for patients with alzheimer s disease using a computerized system. We present the development and initial evaluation of a personalized reminiscence program, which was developed specifically for use by patients and their caregivers in the treatment of mild to moderate Alzheimer's disease (AD). The system is part of a collaborative effort assessing the effects on patients with Alzheimer's disease of two non-pharmacological computer-based interventions, namely: personalized reminiscence therapy and cognitive training.   Results from a pilot study indicated high satisfaction levels from those using the initial version of the system as well as a strong tendency towards repeated use. There was also a clear preference for using personal relevant material rather than more general subject matter.   Subsequently, we have designed and further developed an internet-based system, which will be accessible from any location (such as medical facilities, clubs for the elderly, or the residence of the patient or caregiver). The system enables independent use and administration for both patients and caregivers. We are currently conducting a large scale randomized controlled study to further evaluate the effects of this system.",0,0,1,personalized information+personalized service+personalizations,
1075,1075,"Localized haptic texture a rendering technique based on taxels for high density tactile feedback. We investigate the relevance of surface haptic rendering techniques for tactile devices. We focus on the two major existing techniques and show that they have complementary benefits. The first one, called textsc{S}urface textsc{H}aptic textsc{O}bject (textsc{SHO}), which is based on finger position, is shown to be more suitable to render sparse textures; while the second one, called textsc{S}urface textsc{H}aptic textsc{T}exture (textsc{SHT}), which is based on finger velocity, is shown to be more suitable for dense textures and fast finger movements. We hence propose a new rendering technique, called textsc{L}ocalized textsc{H}aptic textsc{T}exture (textsc{LHT}), which is based on the concept of textit{taxel} considered as an elementary tactile information that is rendered on the screen. By using a grid of taxels to encode a texture, textsc{LHT} is shown to provide a consistent tactile rendering across different velocities for high density textures, and is found to reduce user textit{error rate} by up to 77.68% compared to textsc{SHO}.",0,0,1,tactile display+haptic systems+tactile feedback,
1076,1076,"To distort or not to distort distance cartograms in the wild. Distance Cartograms (DC) distort geographical features so that the measured distance between a single location and any other location on a map indicates absolute travel time. Although studies show that users can efficiently assess travel time with DC, distortion applied in DC may confuse users, and its usefulness ""in the wild"" is unknown. To understand how real world users perceive DC's benefits and drawbacks, we devise techniques that improve DC's presentation (preserving topological relationships among map features while aiming at retaining shapes) and scalability (presenting accurate live travel time). We developed a DC-enabled system with these techniques, and deployed it to 20 participants for 4 weeks. During this period, participants spent, on average, more than 50% of their time with DC as opposed to a standard map. Participants felt DC to be intuitive and useful for assessing travel time. They indicated intent in adopting DC in their real-life scenarios.",0,0,1,web content+location based+mobile users,
1077,1077,"Smart home based longitudinal functional assessment. In this paper, we investigate methods of performing automated cognitive health assessment from smart home sensor data. Specifically, we introduce an algorithm to quantify and track changes in activities of daily living and in the mobility of a smart home resident over time using longitudinal smart home sensor data. We use an automated activity recognition algorithm to recognize a smart home resident's activities of daily living from the generated sensor data, and introduce a Compare and Count (2C) algorithm to quantify the changes in everyday behavior. We test our approach using a longitudinal sensor dataset that we collected from 18 single-resident smart homes for nearly two years and study the relationship between observed changes in the sensor-based everyday functioning parameters and changes in standard clinical health assessment scores. The results suggest that we may be able to develop sensor-based change algorithms that can predict specific components of cognitive and physical health.",0,0,1,sensors+sensor data+smart homes,
1078,1078,"The light field stereoscope immersive computer graphics via factored near eye light field displays with focus cues. Over the last few years, virtual reality (VR) has re-emerged as a technology that is now feasible at low cost via inexpensive cellphone components. In particular, advances of high-resolution micro displays, low-latency orientation trackers, and modern GPUs facilitate immersive experiences at low cost. One of the remaining challenges to further improve visual comfort in VR experiences is the vergence-accommodation conflict inherent to all stereoscopic displays. Accurate reproduction of all depth cues is crucial for visual comfort. By combining well-known stereoscopic display principles with emerging factored light field technology, we present the first wearable VR display supporting high image resolution as well as focus cues. A light field is presented to each eye, which provides more natural viewing experiences than conventional near-eye displays. Since the eye box is just slightly larger than the pupil size, rank-1 light field factorizations are sufficient to produce correct or nearly-correct focus cues; no time-multiplexed image display or gaze tracking is required. We analyze lens distortions in 4D light field space and correct them using the afforded high-dimensional image formation. We also demonstrate significant improvements in resolution and retinal blur quality over related near-eye displays. Finally, we analyze diffraction limits of these types of displays.",0,0,1,head mounted displays+auto-stereoscopic display+stereoscopic display,
1079,1079,"Security evaluation of ring oscillator puf against genetic algorithm based modeling attack. As an authentication technology to improve internet of things (IoT) security, the physical unclonable function (PUF) has attracted attention. However, it is reported that PUF is vulnerable to modeling attacks. This study evaluated the security of ring oscillator (RO) PUF against a genetic algorithm (GA) based modeling attack. Experiments using an application specific integrated circuit (ASIC) and simulation showed that about 96% of responses was predicted. Moreover, in simulation experiments, an elite model used in the GA based modeling attack was also evaluated in detail by comparing with a golden model.",0,0,1,authentication+internet of things+internet,
1080,1080,"Media multitasking and psychological wellbeing in chinese adolescents. The present study examined the relationships among media multitasking, time management, and psychological wellbeing in Chinese adolescents. A total of 320 adolescents aged 11 to 18years old were recruited and asked to complete the Media Use Questionnaire, Chinese Adolescent Mental Health Inventory, and Adolescent Time Management Disposition scale. A structural equation model was used to assess possible relationships among media multitasking, time management, and psychological wellbeing. The results showed that the media multitasking index of the sample was 2.5, indicating that adolescents also had access to other 2.5 media tasks when performing the primary media task. Media multitasking was significantly negatively correlated with psychological wellbeing. Time management disposition was negatively correlated with Media multitasking and positively correlated with psychological wellbeing. Our findings indicate that time management disposition can moderate the relationship between Media multitasking and psychological wellbeing. The theoretical and practical implications of adolescent media use are discussed.",0,0,1,digital contents+streaming media+digital media,
1081,1081,"Coping with poor advice from peers in peer based intelligent tutoring the case of avoiding bad annotations of learning objects. In this paper, we examine a challenge that arises in the application of peer-based tutoring: coping with inappropriate advice from peers. We examine an environment where students are presented with those learning objects predicted to improve their learning (on the basis of the success of previous, like-minded students) but where peers can additionally inject annotations. To avoid presenting annotations that would detract from student learning (e.g. those found confusing by other students) we integrate trust modeling, to detect over time the reputation of the annotation (as voted by previous students) and the reputability of the annotator. We empirically demonstrate, through simulation, that even when the environment is populated with a large number of poor annotations, our algorithm for directing the learning of the students is effective, confirming the value of our proposed approach for student modeling. In addition, the research introduces a valuable integration of trust modeling into educational applications.",0,0,1,learning objects+trust management+student learning,
1082,1082,"Uncertainty visualization by representative sampling from prediction ensembles. Data ensembles are often used to infer statistics to be used for a summary display of an uncertain prediction. In a spatial context, these summary displays have the drawback that when uncertainty is encoded via a spatial spread, display glyph area increases in size with prediction uncertainty. This increase can be easily confounded with an increase in the size, strength or other attribute of the phenomenon being presented. We argue that by directly displaying a carefully chosen subset of a prediction ensemble, so that uncertainty is conveyed implicitly, such misinterpretations can be avoided. Since such a display does not require uncertainty annotation, an information channel remains available for encoding additional information about the prediction. We demonstrate these points in the context of hurricane prediction visualizations, showing how we avoid occlusion of selected ensemble elements while preserving the spatial statistics of the original ensemble, and how an explicit encoding of uncertainty can also be constructed from such a selection. We conclude with the results of a cognitive experiment demonstrating that the approach can be used to construct storm prediction displays that significantly reduce the confounding of uncertainty with storm size, and thus improve viewers’ ability to estimate potential for storm damage.",0,0,1,visualization+visualization tools+display system,
1083,1083,"Using motion sensing remote controls with older adults. This paper examines how motion sensitive remote control devices can improve the usability of television sets for older adults. It investigates the use of a pointing remote control where the actions are read and selected on the TV screen by a group of users between 65-85 years old. It was seen that the test participants universally wanted a more usable and less complicated device in both appearance and employability. The preferences in relation to channel choice were relatively narrow, mainly in the use of only 4-7 channels. The argument is proposed that the use of differing design principles facilitates older adults in also becoming proficient users of new technologies, especially focusing on the use of digital television (DTV) and the many opportunities and options to access new features that arise.",0,0,1,remote control+remote controllers+digital television,
1084,1084,"Asynchronous implicit backward euler integration. In standard deformable object simulation in computer animation, all the mesh elements or vertices are timestepped synchronously, i.e., under the same timestep. Previous asynchronous methods have been largely limited to explicit integration. We demonstrate how to perform spatially-varying timesteps for the widely popular implicit backward Euler integrator. Spatially-varying timesteps are useful when the object exhibits spatially-varying material properties such as Young's modulus or mass density. In synchronous simulation, a region with a high stiffness (or low mass density) will force a small timestep for the entire mesh, at a great computational cost, or else, the motion in the stiff (or low mass density) region will be artificially damped and inaccurate. Our method can assign smaller timesteps to stiffer (or lighter) regions, which makes it possible to properly resolve (sample) the high-frequency deformable dynamics arising from the stiff (or light) materials, resulting in greater accuracy and less artificial damping. Because soft (or heavy) regions can continue using a large timestep, our method provides a significantly higher accuracy under a fixed computational budget.",0,0,1,runge-kutta+mesh topologies+riemann solver,
1085,1085,"Evaluation of language feedback methods for student videos of american sign language. This research investigates how to best present video-based feedback information to students learning American Sign Language (ASL); these results are relevant not only for the design of a software tool for providing automatic feedback to students but also in the context of how ASL instructors could convey feedback on students’ submitted work. It is known that deaf children benefit from early exposure to language, and higher levels of written language literacy have been measured in deaf adults who were raised in homes using ASL. In addition, prior work has established that new parents of deaf children benefit from technologies to support learning ASL. As part of a long-term project to design a tool to automatically analyze a video of a students’ signing and provide immediate feedback about fluent and non-fluent aspects of their movements, we conducted a study to compare multiple methods of conveying feedback to ASL students, using videos of their signing. Through two user studies, with a Wizard-of-Oz design, we compared multiple types of feedback in regard to users’ subjective judgments of system quality and the degree students’ signing improved (as judged by an ASL instructor who analyzed recordings of students’ signing before and after they viewed each type of feedback). The initial study revealed that displaying videos to students of their signing, augmented with feedback messages about their errors or correct ASL usage, yielded higher subjective scores and greater signing improvement. Students gave higher subjective scores to a version in which time-synchronized pop-up messages appeared overlaid on the student's video to indicate errors or correct ASL usage. In a subsequent study, we found that providing images of correct ASL face and hand movements when providing feedback yielded even higher subjective evaluation scores from ASL students using the system.",0,0,1,digital videos+video streams+student learning,
1086,1086,"I m in the game embodied puppet interface improves avatar control. We have developed an embodied puppet interface that translates a player's body movements to a virtual character, thus enabling the player to have a fine grained and personalized control of the avatar. To test the efficacy and short-term effects of this control interface, we developed a two-part experiment, where the performance of users controlling an avatar using the puppet interface was compared with users controlling the avatar using two other interfaces (Xbox controller, keyboard). Part 1 examined aiming movement accuracy in a virtual contact game. Part 2 examined changes of mental rotation abilities in users after playing the virtual contact game. Results from Part 1 revealed that the puppet interface group performed significantly better in aiming accuracy and response time, compared to the Xbox and keyboard groups. Data from Part 2 revealed that the puppet group tended to have greater improvement in mental rotation accuracy as well. Overall, these results suggest that the embodied mapping between a player and avatar, provided by the puppet interface, leads to important performance advantages.",0,0,1,virtual spaces+avatar+keyboard,
1087,1087,"Color sommelier interactive color recommendation system based on community generated color palettes. We present Color Sommelier, an interactive color recommendation system based on community-generated color palettes that helps users to choose harmonious colors on the fly. We used an item-based collaborative filtering technique with Adobe Color CC palettes in order to take advantage of their ratings that reflect the general public?s color harmony preferences. Every time a user chooses a color(s), Color Sommelier calculates how harmonious each of the remaining colors is with the chosen color(s). This interactive recommendation enables users to choose colors iteratively until they are satisfied. To illustrate the usefulness of the algorithm, we implemented a coloring application with a specially designed color chooser. With the chooser, users can intuitively recognize the harmony score of each color based on its bubble size and use the recommendations at their discretion. The Color Sommelier algorithm is flexible enough to be applicable to any color chooser in any software package and is easy to implement.",0,0,1,recommendation systems+collaborative filtering+recommendation,
1088,1088,"Brief lags in interrupted sequential performance. We examined effects of adding brief (1 second) lags between trials in a task designed to study errors in interrupted sequential performance. These randomly occurring lags could act as short breaks and improve performance or as short interruptions and impair performance. The lags improved placekeeping accuracy, and to interpret this effect we developed a cognitive model of placekeeping operations, which accounts for the effect in terms of the lag making memory for recent performance more distinct. Self-report data suggest that rehearsal was the dominant strategy for maintaining placekeeping information during interruptions, and we incorporate a rehearsal mechanism in the model. To evaluate the model we developed a simple new goodness-of-fit test based on analysis of variance that offers an inferential basis for rejecting models that do not accommodate effects of experimental manipulations. Brief, unfilled lags between trials improve placekeeping accuracy on the post-lag trialA model of the effect attributes it to sharpened memory for recent performance, with implications for interpreting speed-accuracy tradeoffs more generallyA model evaluation technique is developed that tests goodness of fit inferentially and helps identify incorrect theoretical assumptions",0,0,1,hierarchical model+cognitive model+cognitive process,
1089,1089,"Visualizing fuzzy overlapping communities in networks. An important feature of networks for many application domains is their community structure. This is because objects within the same community usually have at least one property in common. The investigation of community structure can therefore support the understanding of object attributes from the network topology alone. In real-world systems, objects may belong to several communities at the same time, i.e., communities can overlap. Analyzing fuzzy community memberships is essential to understand to what extent objects contribute to different communities and whether some communities are highly interconnected. We developed a visualization approach that is based on node-link diagrams and supports the investigation of fuzzy communities in weighted undirected graphs at different levels of detail. Starting with the network of communities, the user can continuously drill down to the network of individual nodes and finally analyze the membership distribution of nodes of interest. Our approach uses layout strategies and further visual mappings to graphically encode the fuzzy community memberships. The usefulness of our approach is illustrated by two case studies analyzing networks of different domains: social networking and biological interactions. The case studies showed that our layout and visualization approach helps investigate fuzzy overlapping communities. Fuzzy vertices as well as the different communities to which they belong can be easily identified based on node color and position.",0,0,1,network topology+social networks+visualization,
1090,1090,"Posting for community and culture considerations for the design of interactive digital bulletin boards. The next decade is likely to see a shift in digital public displays moving from non-interactive to interactive content. This will likely create a need for digital bulletin boards and for a better understanding of how such displays should be designed to encourage community members to interact with them. Our study addresses this by exploring community bulletin boards as a ubiquitous type of participatory non-digital display ""in the wild"". Our results highlight how they are used for content of local and contextual relevance, and how cultures of participation, personalization, location, the tangible character of architecture, access, control and flexibility might affect community members' level of engagement with them. Our analysis suggests entry points as design considerations intrinsically linked to the users' sense of agency within a delineated space. Overlaps with related work are identified throughout to provide further validation of previous findings in this area of research.",0,0,1,personalizations+user centered designs+display system,
1091,1091,"Zombies on the road a holistic design approach to balancing gamification and safe driving. This paper explores novel driving experiences that make use of gamification and augmented reality in the car. We discuss our design considerations, which are grounded in road safety psychology and video game design theory. We aim to address the tension between safe driving practices and player engagement. Specifically, we propose a holistic, iterative approach inspired by game design cognition and share our insights generated through the application of this process. We present preliminary game concepts that blend digital components with physical elements from the driving environment. We further highlight how this design process helped us to iteratively evolve these concepts towards being safer while maintaining fun. These insights and game design cognition itself will be useful to the AutomotiveUI community investigating similar novel driving experiences.",0,0,1,augmented reality+digital games+game design,
1092,1092,"The set of fear inducing pictures sfip development and validation in fearful and nonfearful individuals. Emotionally charged pictorial materials are frequently used in phobia research, but no existing standardized picture database is dedicated to the study of different phobias. The present work describes the results of two independent studies through which we sought to develop and validate this type of database—a Set of Fear Inducing Pictures (SFIP). In Study 1, 270 fear-relevant and 130 neutral stimuli were rated for fear, arousal, and valence by four groups of participants; small-animal (N = 34), blood/injection (N = 26), social-fearful (N = 35), and nonfearful participants (N = 22). The results from Study 1 were employed to develop the final version of the SFIP, which includes fear-relevant images of social exposure (N = 40), blood/injection (N = 80), spiders/bugs (N = 80), and angry faces (N = 30), as well as 726 neutral photographs. In Study 2, we aimed to validate the SFIP in a sample of spider, blood/injection, social-fearful, and control individuals (N = 66). The fear-relevant images were rated as being more unpleasant and led to greater fear and arousal in fearful than in nonfearful individuals. The fear images differentiated between the three fear groups in the expected directions. Overall, the present findings provide evidence for the high validity of the SFIP and confirm that the set may be successfully used in phobia research.",0,0,1,color images+high resolution image+database systems,
1093,1093,"Adapting performance feedback to a learner s conscientiousness. To keep a learner motivated, an intelligent tutoring system may need to adapt its feedback to the learner's characteristics. We are particularly interested in adaptation of performance feedback to the learner's personality. Following on from an earlier study that investigated the effect of generalized self-efficacy, this study examines how feedback may need to be adapted to the trait Conscientiousness from the Five Factor Model. We used a User-as-Wizard approach, with participants taking the role of the adaptive feedback generator. Participants were presented with a fictional student with a validated polarized level of Conscientiousness, along with a set of marks the student had achieved in a test. They provided feedback to the learner in the form of a short statement. We examined the level to which participants bent the truth as adaptation to the learner's conscientiousness. The study suggests that adaptation to conscientiousness may be needed: using a positive slant for highly conscientious students with failing grades.",0,0,1,intelligent tutoring system+intelligent tutors+tutoring system,
1094,1094,"Ambleds para ambientes de moradia assistidos em cidades inteligentes. Smart cities are powered by the ability to self-monitor and respond to signals and data feeds from heterogeneous physical sensors. As part of this network of physical sensors are the Ambient Assisted Living (AAL) applications to allow elderly, sick and disabled people to stay safely at home while assisted by their family and medical staff. By hiding technology into light fixtures, in this paper we propose AmbLEDs, an ambient sensing system, as an alternative to spreading sensors that are perceived as invasive, such as cameras, microphones, microcontrollers, tags or wearables, in order to create a ubiquitous collaborative system for recognizing, informing and alerting home environmental changes and human activities to support a sustainable proactive care.",0,0,1,disabled people+wearable computers+collaborative systems,
1095,1095,"Gamicsm relating education culture and gamification a link between worlds. The potential of gamification to improve users' motivation and engagement influenced many researchers and professionals to analyse its effects in educational settings. While some studies focus on adapting game elements according to demographic and behavioural information of the user profile, few of them explore (or even consider) cultural factors. These cultural factors play an essential role in our societies' development. Thus, this work proposes and evaluates a representative model to understand better the relationship between cultural factors and gamification within educational domains, namely the Gamification for Cultural Studies Model (GamiCSM). Through a qualitative approach, we map Hofstede's cultural dimensions (i.e., power distance, individualism/collectivism, uncertainty avoidance, masculinity/femininity, long/short-term orientation, and indulgence/restraint) with a Taxonomy of Gamification Elements for Education (TGEEE), a recent model for gamification elements for educational environments. Furthermore, we adapted a survey to evaluate the resultant model with eight domain experts in gamification and education. Based on this evaluation, we are able to propose a starting model, containing some additional refinements and improvements. Thus, the main contributions of this work are: (i) the first model to relate game elements and cultural dimensions within educational domains and (ii) a state-of-the-art empirical study intersecting culture, gamification and education.",0,0,1,gamification+digital games+game design,
1096,1096,"Interruptibility prediction for ubiquitous systems conventions and new directions from a growing field. When should a machine attempt to communicate with a user? This is a historical problem that has been studied since the rise of personal computing. More recently, the emergence of pervasive technologies such as the smartphone have extended the problem to be ever-present in our daily lives, opening up new opportunities for context awareness through data collection and reasoning. Complementary to this there has been increasing interest in techniques to intelligently synchronise interruptions with human behaviour and cognition. However, it is increasingly challenging to categorise new developments, which are often scenario specific or scope a problem with particular unique features. In this paper we present a meta-analysis of this area, decomposing and comparing historical and recent works that seek to understand and predict how users will perceive and respond to interruptions. In doing so we identify research gaps, questions and opportunities that characterise this important emerging field for pervasive technology.",0,0,1,ubiquitous systems+ubiquitous computing+personal computing,
1097,1097,"Applying causality principles to the axiomatization of probabilistic cellular automata. Cellular automata (CAs) consist of an bi-infinite array of identical cells, each of which may take one of a finite number of possible sstates. The entire array evolves in discrete time steps by iterating a global evolution G. Further, this global evolution G is required to be shift-invariant (it acts the same everywhere) and causal (information cannot be transmitted faster than some fixed number of cells per time step). At least in the classical [7], reversible [11] and quantum cases [1], these two top-down axiomatic conditions are sufficient to entail more bottom-up, operational descriptions of G. We investigate whether the same is true in the probabilistic case.",0,0,1,cellular automata+cellular+sub-arrays,
1098,1098,"Bounded distortion parametrization in the space of metrics. We present a framework for global parametrization that utilizes the edge lengths (squared) of the mesh as variables. Given a mesh with arbitrary topology and prescribed cone singularities, we flatten the original metric of the surface under strict bounds on the metric distortion (various types of conformal and isometric measures are supported). Our key observation is that the space of bounded distortion metrics (given any particular bounds) is convex, and a broad range of useful and well-known distortion energies are convex as well. With the addition of nonlinear Gaussian curvature constraints, the parametrization problem is formulated as a constrained optimization problem, and a solution gives a locally injective map. Our method is easy to implement. Sequential convex programming (SCP) is utilized to solve this problem effectively. We demonstrate the flexibility of the method and its uncompromised robustness and compare it to state-of-the-art methods.",0,0,1,topology+parameterization+signal distortion,
1099,1099,"Traffic flow harmonization in expressway merging. Steering a vehicle is a task increasingly challenging the driver in terms of mental resources. Reasons for this include the increasing volume of road traffic and a rising quantity of road signs, traffic lights, and other distractions at the roadside (such as billboards), to name a few. The application of Advanced Driver Assistance Systems, in particular if taking advantage of Ambient Intelligence (AmI) technology, can help to increase the perceptivity of a driver, leading as a direct consequence to more relaxed mental stress of the same. One situation where we see potential in the application of such a system are merging areas on the expressway where two or more varying traffic streams converge into a single one. In order to reduce cognitive liabilities (in this work expressed as panic or anger), drivers are exposed to while merging, we have developed two behavioral rules. The first (""increased range of perception"") enables drivers to change early upstream into a spare lane, allowing the merging traffic to join into mainline traffic at reduced conflicts, the second (""inter-car distance management"" in the broader area of merging) provide drivers with recommendations of when and how to change lanes at the best. From a technical point of view, the ""VibraSeat"" a in-house developed car seat with integrated tactile actuators, is used for delivering information about perception range and inter-car distances to the driver in a way that does not stress his/her mental capabilities. To figure out possible improvements in its application in real traffic and at a meaningful scale, cellular automaton---based simulation of a specific section of Madrid expressway M30 was performed. Results from the data-driven simulation experiments on the true to scale model indicate that AmI technology has the potential to increase road throughput or average driving speed and furthermore to decrease the panic of drivers while merging into an upper (the main) lane.",0,0,1,roadsides+cognitive process+ambient intelligence,
1100,1100,"An ergonomic evaluation of the adaptation of polish online stores to the needs of the elderly. Recently websites have been a key intermediary in the exchange of information. The share of trade conducted based on online services transactions is also dynamically growing. Among people using online services and communicating this way, are now also the elderly. These are often people whose first contact with these technologies occurred during adulthood. Many of elderly people did not use a computer in their work, and their first contact with the Internet has been during their retirement. In conclusion, the currently operating focus in Poland on young online shoppers is faulty. With the increasing proportion of elderly people in Polish society and the dissemination of computer technology among them, the need for senior-friendly online stores will grow. The choice of this form of purchase will be decided by convenience, price, range of goods and delivery terms. However, the ultimate determinants of whether elderly users will enjoy the benefits of online shopping are the ergonomic features of services, particularly criteria such as: security, ease of use, rule transparency and ascetic aesthetics.",0,0,1,internet+online shopping+online consumers,
1101,1101,"Selectively de animating video. We present a semi-automated technique for selectively deanimating video to remove the large-scale motions of one or more objects so that other motions are easier to see. The user draws strokes to indicate the regions of the video that should be immobilized, and our algorithm warps the video to remove the large-scale motion of these regions while leaving finer-scale, relative motions intact. However, such warps may introduce unnatural motions in previously motionless areas, such as background regions. We therefore use a graph-cut-based optimization to composite the warped video regions with still frames from the input video; we also optionally loop the output in a seamless manner. Our technique enables a number of applications such as clearer motion visualization, simpler creation of artistic cinemagraphs (photos that include looping motions in some regions), and new ways to edit appearance and complicated motion paths in video by manipulating a de-animated representation. We demonstrate the success of our technique with a number of motion visualizations, cinemagraphs and video editing examples created from a variety of short input videos, as well as visual and numerical comparison to previous techniques.",0,0,1,frames+video streams+visualization,
1102,1102,"Gazewheels comparing dwell time feedback and methods for gaze input. We present an evaluation and comparison of GazeWheels: techniques for dwell time gaze input and feedback. In GazeWheel, visual feedback is shown to the user in the form of a wheel that is filled. When completely filled, a selection is made where the user is gazing. We compare three methods for responding to the user when gazing away from the target: Resetting GazeWheel, Pause-and-Resume GazeWheel, and Infinite GazeWheel. We also compare the position of the GazeWheel; Co-located Feedback: shown on the target being gazed at, and Remote Feedback: shown at the top of the interface. To this end, we report on results of a user study (N=19) that investigates the benefits and drawbacks of each method at different dwell times: 500ms, 800ms, and 1000ms. Results show that Infinite GazeWheel and Pause-and-Resume GazeWheel are more error prone but significantly faster than Resetting GazeWheel when using 800-1000 ms dwell time, even when including the time for correcting errors.",0,0,1,dwell time+user information+feedback systems,
1103,1103,"The effect of an avatar s emotional expressions on players fear reactions. This research aimed to demonstrate the effects of an avatar's emotional expressions on players' fear reactions during horror gameplay. In Study 1, we found that the emotional expressions of an avatar decreased fear reactions among players. This effect was mediated by avatar embodiment. More precisely, avatar emotional expressions lower avatar embodiment, which, in turn, positively predicts players' fear reactions. In Study 2, we replicated the findings of Study 1. In addition, we found that the effects observed in Study 1 were only present in interactive gameplaynot when players watched screen-captured footage of the game. In other words, we found evidence of a moderated mediation model in which interactivity moderates the effects of an avatar's emotional expressions on players' fear reactions through avatar embodiment. Game players' fear reactions are weakened by avatars' emotional expressions.Embodiment mediates the effect of an avatar's emotional expressions on players' fear reactions.Agency moderates this mediation effect of embodiment.Enactive gameplay experiences fundamentally differ from observational experiences.",0,0,1,emotional expressions+emotion expression+avatar,
1104,1104,"Ensemble recommendations via thompson sampling an experimental study within e commerce. This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.",0,0,1,commerce+recommendation algorithms+recommendation,
1105,1105,"Predicting the sentiment in sentences based on words an exploratory study on anew and anet. Current practices for sentiment prediction from text mostly involve words-in-a-bag approach that utilize techniques such as support vector machines or naive Bayes. In this study, ANET (Affective Norms for English Text) sentence ratings of pleasure and arousal are compared with ANEW (Affective Norms for English Words) word ratings using regression and single layer neural networks. The sentences in ANET are decomposed into their words to obtain valence and arousal ratings from ANEW. A stop list is formed for non-words as well as words that are not found in ANEW. Then we studied whether the sentence sentiment reflected in terms of valence and arousal can be predicted from the sentiment of words in the sentence. Using linear regression, we found that approximately 35% of the variance in ANET valence and arousal ratings can be explained by ANEW valence and arousal ratings. Furthermore, Pearson correlation coefficient for ANEW and ANET ratings are similar for both valence and arousal, and close to 0.6. We also trained neural networks to investigate if non-linear approximations improved prediction of sentence sentiments from the constituent words. Out of several feedforward neural network configurations, a network with 200 hidden layer nodes turned out to be capable of identifying sentence sentiments accurately: the words' valence and arousal values explained 88 % of the variance in the sentences' valence ratings and 91 % of the variance in the sentences' arousal ratings. This preliminary study indicates that a proper choice of neural network might be adequate to estimate sentiments of sentences from sentiments of words.",0,0,1,support vector+pearson correlation+pearson correlation coefficients,
1106,1106,"Local area artworks collaborative art interpretation on site. In this paper we present Local Area Artworks, a system enabling collaborative art interpretation on-site deployed during an exhibition in a local art gallery. Through the system, we explore ways to re-connect people to the local place by making use of their personal mobile devices as interfaces to the shared physical space. We do this by re-emphasizing the local characteristics of wireless networks over the global connectivity to the Internet. With a collaborative writing system in a semi-public place, we encourage local art discussions and provide a platform for the public to actively participate in interpretations of individual artworks. Our preliminary findings suggest that people were (to our surprise) not questioning the inner workings of our system. Through engaging with the system, individuals felt being part of the exhibition. However, no coherent piece of text emerged during the runtime of the exhibition.",0,0,1,mobile devices+collaborative work+collaborative writing,
1107,1107,"Musculoskeletal discomfort and use of computers in the university environment. Abstract This cross-sectional study investigated musculoskeletal discomfort and computer use in university staff, through the use of online questionnaires. Results showed a high prevalence of staff reported musculoskeletal discomfort during the preceding year (80%), with neck (60%), shoulder (53%) and lower back discomfort (47%) being the most common. Most believed discomfort was caused by work, although neck discomfort was significantly less in those reporting excellent mental health (OR 0.44, p",0,0,1,university+cognitive process+semantic memory,
1108,1108,"Temporal patterns of pudendal afferent stimulation modulate reflex bladder activation. Electrical stimulation of pudendal nerve (PN) afferents to cause reflex activation or inhibition of the bladder is a promising approach to restore control of bladder function in persons with dysfunction caused by disease or injury. The objective of this work was to evaluate the effects of novel temporal patterns of stimulation on the size of isovolumetric stimulation-evoked reflex bladder contractions and determine if the temporal pattern, of stimulation, like frequency, modulates the reflex bladder response to stimulation. The temporal pattern of stimulation significantly affected the size of bladder contractions evoked by stimulation in anesthetized cats. Patterns with pauses and random patterns evoked significantly smaller bladder contractions, while patterns with subtle changes in inter-pulse-intervals (IPIs), ramp trains and alternating IPI trains, occasionally evoked larger bladder contractions than constant frequency stimulation. The use of new temporal patterns of electrical stimulation should be considered in the development of neural prosthetics for the restoration of lower urinary tract function.",0,0,1,output frequency+pulse train+brain machine interface,
1109,1109,"Sense of presence in maxwhere virtual reality. Presence is a psychological phenomenon, the sense of being in a virtual environment. Experiencing presence is not limited to the users of full immersive virtual realities, but it can be experienced also in desktop virtual realities. These VRs visualize the simulated 3D environment on two-dimensional displays. MaxWhere is a desktop virtual environment for education, learning and working. It can be described as a 3D browser because any web-page could be loaded on the predefined SmartBoards of the virtual space. Already many research has been done on the use of this software, but this is the first time when the sense of presence is considered. The study aimed to measure the sense of presence in MaxWhere virtual reality and investigate its relationship with individual navigational experience and prior knowledge of the VR. Navigational experience is how much the user feels natural and automatic the movements in the virtual space. The results showed that better navigational experience correlates with stronger sense of spatial presence. Furthermore, the more experienced users reported higher sense of presence and gave higher rating on experienced realism subscale.",0,0,1,virtual spaces+virtual environments+virtual reality,
1110,1110,"Taking our time chronic illness and time based objects in families. This study examined the use of time-based objects by patients and their families to manage chronic illnesses at home. Calendar systems and medication containers, the main types of time-based objects studied, were used as part of two family-based collaborative work practices: 1) prompting health management activities, and 2) safeguarding these activities. Additionally, these artifacts were part of two social interaction patterns that managed emotional intimacy: 1) expressing support, and 2) hiding and disguising illness. Accordingly, home-based illness management may be more collaborative than previously recognized. Moreover, through their interactive incorporation into family life, time-based objects are laden with psychosocial significance. Breakdowns in temporal support were also evident, and were accompanied by: missed medication events; rationing of medications; medication errors; and difficulties with preparation for medical appointments. We propose novel artifact designs to better support patients and their families in managing the temporal aspects of chronic illness together.",0,0,1,social relationships+collaborative activities+collaborative work,
1111,1111,"Loop detection and correction of 3d laser based slam with visual information. Three-dimensional (3D) laser-based simultaneous localization and mapping (SLAM) can provide real-time pose information and construct accurate 3D map. However, detecting loop closures is a challenge in the 3D laser-based SLAM for expensive computation of algorithms. In this paper, we propose a visual method to detect and correct loop closures. We introduce visual bags-of-words techniques for loop closure detection in the 3D laser-based SLAM. Time of computing similarities between points clouds can be saved. Our method maintains visual keyframes, each of which associates with its pose and segmentation of laser point clouds. Our experiments on KITTI dataset prove that our method can efficiently reduce motion accumulation errors and successfully ensure the real-time performance of loop closure correction.",0,0,1,point cloud+laser point+visual informations,
1112,1112,"Understanding chatbot mediated task management. Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work.",0,0,1,communication+email+collaborative work,
1113,1113,"It sounds like she is sad introducing a biosensing prototype that transforms emotions into real time music and facilitates social interaction. This paper introduces a biosensing prototype that transforms emotions into music, helping people recognize and understand their own feelings and actions and those of other people. This study presents a series of three experiments with 20 participants in four emotional states: happiness, sadness, anger, and neutral state. Their real-time emotions were captured through a wearable probe Audiolize Emotion that detects users' EEG signals, composes data into audio files which are played to users themselves and others. At last, we conducted observations and interviews with participants to explore factors linked with social interaction, users' perceptions of music, and the reflections on the use of audio form for self-expression or communication. We found that Audiolize Emotion prototype triggers communication and self-expression in two ways: building curiosity and supporting communication by extending expression form. Based on the results, we provide future directions to explore the field of emotion and communication further and plan to apply the knowledge into more fields of VR game and accessibility.",0,0,1,communication+emotional expressions+wearable computers,
1114,1114,"Investigating in car safety services on the motorway the role of screen size. Today's in-car information systems are undergoing an evolution towards device miniaturization as well as to real-time telematics services. In a road study with 26 participants, we investigated whether small smartphone-sized screens are recommendable for the communication of realtime safety services. We did not find strong overall differences between large and small screen setups in any of our investigated measures. However, when no audio was presented, safety services presentation on small screens resulted in significantly more long glances to the HMI than on large screen. Also, subjective comprehensibility of driving recommendations was best when screen size was large and audio presentation was available. Implications and further research opportunities are discussed.",0,0,1,hmi+smart phones+small screens,
1115,1115,"Make it isi interactive systems integration tool. Besides advances in usability the use of interactive systems still poses challenges for end users, particularly beginners. Users have to adapt in some way to the GUI. They have to learn the sequence of steps and how to accomplish them to perform a task. In addition, the fact that users often have to use different systems to accomplish their daily tasks increases the challenges they face.   This paper presents ISI, a tool to support developers in the engineering of interactive systems. The tool aims to facilitate the integration of independent interactive systems while offering simplified interactions to end users. Based on picture-driven computing and task automation the tool aims to reduce the challenges that users face while trying to perform a task using one or several interactive systems, as well as improving the efficiency of task performance. The research is illustrated by means of a care home application example.",0,0,1,web content+graphical user interfaces+mobile users,
1116,1116,"Bacpack exploring the role of tangibles in a museum exhibit for bio design. We present BacPack, a tangible museum exhibit for exploring bio-design. BacPack utilizes tangible tokens on a large multitouch table display to allow visitors the opportunity to participate in a playful bio-design activity-engineering bacteria for sustaining life on Mars. To understand the role of tangible tokens in facilitating engagement and learning with the exhibit, we developed and evaluated two versions of BacPack: one with tangible tokens and one that consists of only multitouch interaction. Results from an evaluation in the Tech Museum of Innovation indicate that tangible tokens provide additional opportunities for collaborative problem solving and impact learning through support for tinkering and experimentation. We discuss design considerations for exhibits that facilitate creative engagement and exploration with biology.",0,0,1,tangible interfaces+tangible user interfaces+tangible interaction,
1117,1117,"Why are you playing games you are a girl exploring gender biases in esports. Esports are rapidly growing within academia. In HCI, earlier work explored how problematic behaviors emerging from gender biases (e.g., toxicity) negatively impact female participation in esports. Here, we further explore gender biases in esports by interviewing 19 self-identified female and male professional gamers and event organizers. We inquire our interviewees about personal experiences with gender biases in esports and their perspective on how these biases impact participation, inclusivity, and career prospects. Our interviewees see gender biases in esports as a consequence of stereotypical gender roles in gaming tout-court (e.g., girls do not like violence, boys are competitive by nature). The rationale for separating male and female in esports, however, seems rooted in the need for female gamers to create role-models and grow in self-confidence. We scrutinize the considerations emerging from our interviews under a Feminist HCI lens and discuss how HCI research can help design equitable environments in esports.",0,0,1,massively multiplayer+user centered designs+interaction design,
1118,1118,"Test of spanish sentences to measure speech intelligibility in noise conditions. This article describes the development of a test for measuring the intelligibility of speech in noise for the Spanish language, similar to the test developed by Kalikow, Stevens, and Elliot (Journal of the Acoustical Society of America, 5, 1337–1360, 1977) for the English language. The test consists of six forms, each comprising 25 high-predictability (HP) sentences and 25 low-predictability (LP) sentences. The sentences were used in a perceptual task to assess their intelligibility in babble noise across three different signal-to-noise ratio (SNR) conditions in a sample of 474 normal-hearing listeners. The results showed that the listeners obtained higher scores of intelligibility for HP sentences than for LP sentences, and the scores were lower for the higher SNRs, as was expected. The final six forms were equivalent in intelligibility and phonetic content.",0,0,1,signal to noise ratio+synthetic speech+intelligibility,
1119,1119,"Reduced gui for an interactive geometry software. PurposeThe purpose of this paper is to describe an experimental study to reduce cognitive load and enhance usability for interactive geometry software. Design/methodology/approachThe Graphical User Interface is the main mechanism of communication between user and system features. Educational software interfaces should provide useful features to assist learners without generate extra cognitive load. In this context, this research aims at analyzing a reduced and a complete interface of interactive geometry software, and verifies the educational benefits they provide. We investigated whether a reduced interface makes few cognitive demands of users in comparison to a complete interface. To this end, we designed the interfaces and carried out an experiment involving 69 undergraduate students. FindingsThe experimental results indicate that an interface that hides advanced and extraneous features helps novice users to perform slightly better than novice users using a complete interface. After receiving proper training, however, a complete interface makes users more productive than a reduced interface. Originality/valueIn educational software, successful user interface designs minimize the cognitive load on users; thereby users can direct their efforts to maximizing their understanding of the educational concepts being presented. We investigated whether a reduced interface of an interactive geometry software helps to increase students' performance.We designed 60 tasks to be completed with two different interfaces, a complete and a reduced version, for the same software.We carried out an experiment involving 69 undergraduate students, randomly assigned to one of the interfaces.The experimental results indicate that the reduced interface helps novice users to perform slightly better.After receiving proper training, however, a complete interface makes users more productive using a complete interface.",0,0,1,graphical user interfaces+user interfaces+user interface designs,
1120,1120,"An integrated framework for human activity recognition. This poster presents an integrated framework to enable using standard non-sequential machine learning tools for accurate multi-modal activity recognition. Our framework contains simple pre- and post-classification strategies such as class-imbalance correction on the learning data using structure preserving oversampling, leveraging the sequential nature of sensory data using smoothing of the predicted label sequence and classifier fusion, respectively, for improved performance. Through evaluation on recent publicly-available OPPORTUNITY activity datasets comprising of a large amount of multi-dimensional, continuous-valued sensory data, we show that our proposed strategies are effective in improving the performance over common techniques such as One Nearest Neighbor (1NN) and Support Vector Machines (SVM). Our framework also shows better performance over sequential probabilistic models, such as Conditional Random Field (CRF) and Hidden Markov Models (HMM) and when these models are used as meta-learners.",0,0,1,post classification+activity recognition+sensor data,
1121,1121,"Enlightening drivers a survey on in vehicle light displays. In the last decade, several in-vehicle light displays have been proposed. These displays inform drivers via light patterns that go beyond simple status indicators. In this literature survey, we present 21 works with their light displays and patterns. In addition, we summarize the common methods and results of these works. This work will help researchers and designers find a good starting point for looking into in-vehicle light displays. It may further foster discussions about best practices in the community.",0,0,1,vehicles+display devices+display system,
1122,1122,"A deep architecture for content based recommendations exploiting recurrent neural networks. In this paper we investigate the effectiveness of Recurrent Neural Networks (RNNs) in a top-N content-based recommendation scenario. Specifically, we propose a deep architecture which adopts Long Short Term Memory (LSTM) networks to jointly learn two embeddings representing the items to be recommended as well as the preferences of the user. Next, given such a representation, a logistic regression layer calculates the relevance score of each item for a specific user and we returns the top-N items as recommendations.   In the experimental session we evaluated the effectiveness of our approach against several baselines: first, we compared it to other shallow models based on neural networks (as Word2Vec and Doc2Vec), next we evaluated it against state-of-the-art algorithms for collaborative filtering. In both cases, our methodology obtains a significant improvement over all the baselines, thus giving evidence of the effectiveness of deep learning techniques in content-based recommendation scenarios and paving the way for several future research directions.",0,0,1,logistics+collaborative filtering+recommendation,
1123,1123,"Evolving effective micro behaviors in rts game. We investigate using genetic algorithms to generate high quality micro management in combat scenarios for real-time strategy games. Macro and micro management are two key aspects of real-time strategy games. While good macro helps a player collect more resources and build more units, good micro helps a player win skirmishes against equal numbers and types of opponent units or win even when outnumbered. In this paper, we use influence maps and potential fields to generate micro management positioning and movement tactics. Micro behaviors are compactly encoded into fourteen parameters and we use genetic algorithms to search for effective micro management tactics for the given units. We tested the performance of our ECSLBot (the evolved player), obtained in this way against the default StarCraft AI, and two other state of the art bots, UAlbertaBot and Nova on several skirmish scenarios. The results show that the ECSLBot tuned by genetic algorithms outperforms the UAlbertaBot and Nova in kiting efficiency, target selection, and knowing when to flee to survive. We believe our approach is easy to extend to other types of units and can be easily adopted by other AI bots.",0,0,1,potential field+tactics+gameplay,
1124,1124,"Social media adoption. Media uses and gratifications, together with innovation characteristics merge to form a model of social media adoption.Personal, social and tension release needs drive social media adoption.Social media innovation characteristics enhance the likelihood of adoption. This research is designed to empirically investigate how social media needs and innovation influence the adoption of social media amongst Internet users. The theoretical perspective of the uses and gratifications, and Rogers' five characteristics of innovation are reviewed and extended to explain the needs and motivations of the consumer. The study is conducted by testing and quantifying the relationship between the uses and gratifications of social media, while taking into consideration the mediating effect of social media technology innovation. This research applies a two-phase, multimethod strategy in the context of Malaysia. The strategy comprises the qualitative approach via focus group discussions (FGDs) with 48 respondents and the quantitative approach via online survey questionnaires with 428 respondents. Overall, the findings suggest that social media adoption is significantly driven by three types of need category - personal (consisting of enjoyment and entertainment), social (consisting of social influence and interaction) and tension release (consisting of belongingness, companionship, playfulness). In turn, these needs are motivated by the social media innovation characteristics (relative advantage, observability, compatibility) that increase the likelihood of the adoption. The research makes a significant contribution in the area of media and technology adoption, which can be used to help marketers understand the factors that motivate social media usage, particularly the UXDs in designing human-computer interaction strategies.",0,0,1,streaming media+digital media+human computer interaction,
1125,1125,"Smart habitat a wildlife rehabilitation system. Wildlife rehabilitation centers are tasked with the difficult challenge of providing medical care to wildlife while limiting human contact to ensure a successful transition into the wild. Building off of interviews with volunteers and 6 months of participatory observation work, we present a smart habitat design for the rehabilitation of Virginian opossum joeys. Using maker technology, we crafted a prototype utilizing sensors, a microcontroller, and an android application. We then discuss the future direction for this project including improvements and a field deploy.",0,0,1,microcontrollers+smart homes+user centered designs,
1126,1126,"Older adults use of a novel communication system client goals versus participant experiences. This paper reports on the outcomes of a collaborative industry project which designed and deployed a novel communication systems for older adults. We outline a gap between the goals of our industry partner and the needs and experiences of our participants. We highlight the need to carefully consider the location of set-up, the training needs of older people, and any existing social support networks and technological systems already in use, while ultimately recognising the emotional connections of older people. These contextual factors influence individual older adults' acceptance, use and adoption of novel technologies such as the one presented in this paper.",0,0,1,communication systems+communication+social aspect,
1127,1127,"Accessibility in software engineering pursuing the mainstream from a classroom. Though equal access to all digital devices, content and applications should be ensured by default in the Digital Age, reality has yet to match this ideal, despite the numerous efforts to raise awareness of the problem.",0,0,1,engineering+software+digital devices,
1128,1128,"Attentional trade offs under resource scarcity. Resource scarcity poses challenging demands on the cognitive system. Budgeting with limited resources induces an attentional focus on the problem at hand, but it also comes with a cost. Specifically, scarcity causes a failure to notice beneficial information in the environment, or remember to execute actions in the future, that help alleviate the condition of scarcity. This neglect may arise as a result of attentional narrowing. Attentional trade-offs under scarcity can further determine memory encoding. In five experiments, we demonstrated that participants under scarcity prioritized price information but neglected a useful discount when ordering food from a menu (Experiment 1); they showed better recall for information relevant to the focal task at a subsequent surprise memory test (Experiments 2 and 3); they performed more efficiently on the focal task but neglect a useful cue in the environment that could save them resources (Experiment 4); and they failed to remember the previous instructions to execute future actions that could save them resources (Experiment 5). These results collectively demonstrate that scarcity fundamentally shapes the way people process information in the environment, by directing attention to the most urgent task, while inducing a neglect of other information that can be beneficial. The attentional neglect and memory failures may lead to suboptimal decisions and behaviors that further aggravate the condition of scarcity. The results provide new insights on the behaviors of the poor, and also important implications for public policy and the design of welfare services and programs for low-income individuals.",0,0,1,non-volatile memories+cognitive systems+user information,
1129,1129,"Examining potential mechanisms underlying the wikipedia gender gap through a collaborative editing task. Abstract   Research has identified a significant gender gap on the online encyclopedia, Wikipedia. The current research used a mixed experimental (type of feedback) and quasi-experimental (gender) design to examine the editing behaviors of college students during a public, collaborative editing task to identify potential factors underlying the Wikipedia gender gap. Overall, women edited more than men. However, in the editing condition most akin to Wikipedia, wherein female peer editors were underrepresented in the essay edits and feedback from peers was neutral, men trended towards adding more content than woman. Women added more content than men in this male-dominated essay condition when peer editors modeled constructive feedback. Although the type of edits from peer editors was counterbalanced, participants typically viewed an anonymous peer editor as male. Women viewed the anonymous editor as more critical of the participant's own work when compared with a gender-neutral peer editor. These results suggest that visible female editors on Wikipedia and broader encouragement of the use of constructive feedback may begin to alleviate the Wikipedia gender gap. Furthermore, the relatively high proportion of anonymous editors may exacerbate the Wikipedia gender gap, as anonymity may often be perceived as male and more critical.",0,0,1,anonymity+anonymous communication+collaborative work,
1130,1130,"Integration into mathematics classrooms of an adaptive and intelligent individualized e learning environment implementation and evaluation of uzwebmat. The purpose of this study is to design an adaptive and intelligent individualized e-learning environment based on learning style and expert system named UZWEBMAT and to evaluate its effects on students' learning of the unit of probability. In the study, initially, learning objects were prepared in three different ways in relation to Visual-Auditory-Kinesthetic (VAK) learning style for each subject of the probability unit. These were appropriate for secondary school mathematics curricula. Then, they were transferred into the digital environment. Each student may follow a different course, and the solution supports s/he will get may also differ highlighting the individual learning. The sample of the study consists of 81 10th grade students from two high schools in Trabzon, Turkey. Qualitative and quantitative data were collected from students to answer research questions. Quantitative data were given as frequency distribution and percentages. Qualitative data were analyzed using qualitative data analysis methods. Results of the study indicated that opinions regarding UZWEBMAT are rather positive. Aiming at individual learning, UZWEBMAT provides the most appropriate environment for students. In addition, UZWEBMAT can be used as well to reinforce traditional classroom education.",0,0,1,individual learning+learning environments+education,
1131,1131,"Prioritizing flexibility and intangibles medical crowdfunding for stigmatized individuals. HCI research on crowdfunding has primarily focused on creative or organizational endeavors. Yet a majority of crowdfunding campaigns are conducted by individuals in need, often for healthcare. To better understand and improve this common crowdfunding experience, especially for those that inhabit a vulnerable social status, we conducted 20 interviews with transmen crowdfunding for top-surgery. Design choices that optimize site flexibility (e.g. control of personal information; enable cross-site communication) and foreground intangibles, such as political values and emotional support, are priorities for individuals from a stigmatized community. Findings differed from previous crowdfunding research and contribute to limited research on transgender identities in HCI. Overall they provide unique insights into how design choices can facilitate marginalized identity management in highly public online spaces.",0,0,1,personal information+social aspect+human computer interaction,
1132,1132,"The impact of a self avatar on cognitive load in immersive virtual reality. The use of a self-avatar inside an immersive virtual reality system has been shown to have important effects on presence, interaction and perception of space. Based on studies from linguistics and cognition, in this paper we demonstrate that a self-avatar may aid the participant's cognitive processes while immersed in a virtual reality system. In our study participants were asked to memorise pairs of letters, perform a spatial rotation exercise and then recall the pairs of letters. In a between-subject factor they either had an avatar or not, and in a within-subject factor they were instructed to keep their hands still or not. We found that participants who both had an avatar and were allowed to move their hands had significantly higher letter pair recall. There was no significant difference between the other three conditions. Further analysis showed that participants who were allowed to move their hands, but could not see the self-avatar, usually didn't move their hands or stopped moving their hands after a short while. We argue that an active self-avatar may alleviate the mental load of doing the spatial rotation exercise and thus improve letter recall. The results are further evidence of the importance of an appropriate self-avatar representation in immersive virtual reality.",0,0,1,virtual reality+mixed reality+immersive virtual environments,
1133,1133,"Github developers use rockstars to overcome overflow of news. Keeping track of a constantly updating stream of news items on social networking enabled software development sites may be difficult. We analyzed the actions of 544 GitHub.com developers working across 5,657 projects to examine how the network of developers and projects influence where developers choose to contribute. Our analyses revealed the existence of a group of extremely well connected developers, or rockstars. We found that these rockstars': 1) actions have a greater influence on their followers compared to regular developers, 2) type of action affect their followers differently, 3) influence on followers may depend on a project's age, 4) increased activity on a project increases activity by followers, and 5) followers use as guides to projects to work on. We discuss the implications of these findings to the design of software development environments.",0,0,1,software developer+software+web developers,
1134,1134,"Technical models and key technologies of e health monitoring. This paper describes E-Health Monitoring (EHM) ecosystem and current EHM market segments. Based on IoT reference model, three EHM technical models are proposed. Model 1 focuses on device to device communication; model 2 focuses on network providing connection only; model 3 focuses on network combined with platform. Model 3 is equipped with a service support platform which is a new entity comparing with model 2. And then, several key technologies are proposed for service support platform of model 3.",0,0,1,communication+remote monitoring+health monitoring system,
1135,1135,"Image based skin disease detection using hybrid neural network coupled bag of features. The current work proposes a neural based detection method of two different skin diseases using skin imaging. Skin images of two diseases namely Basel Cell Carcinoma and Skin Angioma are utilized. SIFT feature extractor has been employed followed by a clustering phase on feature space in order to reduce the number of features suitable for neural based models. The extracted bag-of-features modified dataset is used to train metaheuristic supported hybrid Artificial Neural Networks to classify the skin images in order to detect the diseases under study. A well-known multi objective optimization technique called Non-dominated Sorting Genetic Algorithm — II is used to train the ANN (NN-NSGA-II). The proposed model is further compared with two other well-known metaheuristic based classifier namely NN-PSO (ANN trained with PSO) and NN-CS (ANN trained with Cuckoo Search) in terms of testing phase confusion matrix based performance measuring metrics such as accuracy, precision, recall and F-measure. Experimental results indicated towards the superiority of the proposed bag-of-features enabled NN-NSGA-II model.",0,0,1,clustering algorithms+feature space+confusion matrices,
1136,1136,"Influence of culture transparency trust and degree of automation on automation use. The reported study compares groups of 120 participants each, from the United States (U.S.), Taiwan (TW), and Turkey (TK), interacting with versions of an automated path planner that vary in transparency and degree of automation. The nationalities were selected in accordance with the theory of cultural syndromes as representatives of Dignity (U.S.), Face (TW), and Honor (TK) cultures, and were predicted to differ in readiness to trust automation, degree of transparency required to use automation, and willingness to use systems with high degrees of automation. Three experimental conditions were tested. In the first, highlight , path conflicts were highlighted leaving rerouting to the participant. In the second, replanner made requests for permission to reroute when a path conflict was detected. The third combined condition increased transparency of the replanner by combining highlighting with rerouting to make the conflict on which decision was based visible to the user. A novel framework relating transparency, stages of automation, and trust in automation is proposed in which transparency plays a primary role in decisions to use automation but is supplemented by trust where there is insufficient information otherwise. Hypothesized cultural effects and framework predictions were confirmed.",0,0,1,automation+industrial automation+trust relationship,
1137,1137,"The mars and venus effect the influence of user gender on the effectiveness of adaptive task support. Providing adaptive support to users engaged in learning tasks is the central focus of intelligent tutoring systems. There is evidence that female and male users may benefit differently from adaptive support, yet it is not understood how to most effectively adapt task support to gender. This paper reports on a study with four versions of an intelligent tutoring system for introductory computer programming offering different levels of cognitive (conceptual and problem-solving) and affective (motivational and engagement) support. The results show that female users reported significantly more engagement and less frustration with the affective support system than with other versions. In a human tutorial dialogue condition used for comparison, a consistent difference was observed between females and males. These results suggest the presence of the Mars and Venus Effect, a systematic difference in how female and male users benefit from cognitive and affective adaptive support. The findings point toward design principles to guide the development of gender-adaptive intelligent tutoring systems.",0,0,1,intelligent tutors+tutoring system+computer programming,
1138,1138,"Advanced aggregate computation for large data visualization. Large data visualization and analysis faces challenges related to performance, operability, degree of discrimination, etc. In this paper, an advanced aggregate computation is proposed to solve these issues from three aspects. By virtue of visualization-based data separation and aggregation, a large dataset is mapped to a visualization-based small dataset for efficient visualization while keeping operability of data. A minimum size of visual primitives for aggregated data is defined to ensure visibility of important but tiny information. And a D3-based rendering implementation improves the performance of consecutive visualizations.",0,0,1,visualization tools+data visualization+visualization and analysis,
1139,1139,"Operationalising and evaluating sub optimal and optimal play experiences through challenge skill manipulation. The study examines the relationship of challenge-skill balance and the player experience through evaluation of competence, autonomy, presence, interest/enjoyment, and positive and negative affect states. To manipulate challenge-skill balance, three video game modes -- boredom (low challenge), balance (medium challenge), and overload (high challenge) -- were developed and experimentally tested (n = 45). The study showed that self-reported positive affect, autonomy, presence, and interest/enjoyment differed between the levels. The balance condition generally performed well in terms of positive player experiences, confirming the key role challenge-skill balance plays in designing for optimal play experiences. Interestingly, the study found significantly lower negative affect scores when playing the boredom condition. Greater feelings of competence were also reported for the boredom condition than the balance and overload conditions. Finally, some measures point to overload as a more enjoyable experience than boredom, suggesting possible player preference for challenge > skill imbalance over skill > challenge imbalance. Implications for design and future research are presented.",0,0,1,gameplay+game design+player experience,
1140,1140,"Simulating neuroprosthetic vision for emotion recognition. We developed a phosphene vision simulator to assist in the development of image processing strategies for implementation in visual prosthetics. This simulation runs on a mobile phone, which can be placed in an AR headset to provide the experience of having prosthetic phosphene vision to individuals with normal vision. This setup allows the participants to experience the future of cortical visual neuroprostheses, while allowing us to evaluate and compare different signal processing algorithms to provide guidelines for the optimal perceptual experience. In this demo we will show how intelligent algorithms can improve the quality of perception with prosthetic vision with an image processing pipeline that allows for accurate emotion expression recognition.",0,0,1,emotion expression+cell phone+mobile phones,
1141,1141,"On collision free reinforced barriers for multi domain iot with heterogeneous uavs. Thanks to advancement of vehicle technologies, Unmanned Aerial Vehicle (UAV) now widely spread over practical services and applications affecting daily life of people positively. Especially, multiple heterogeneous UAVs with different capabilities should be considered since UAVs can play an important role in Internet of Things (IoT) environment in which the heterogeneity and the multi domain of UAVs are indispensable. Also, a concept of barrier-coverage has been proved as a promising one applicable to surveillance and security. In this paper, we present collision-free reinforced barriers by heterogeneous UAVs to support multi domain. Then, we define a problem which is to minimize maximum movement of UAVs on condition that a property of collision-free among UAVs is assured while they travel from current positions to specific locations so as to form reinforced barriers within multi domain. Because the defined problem depends on how to locate UAVs on barriers, we develop a novel approach that provides a collision-free movement as well as a creation of virtual lines in multi domain. Furthermore, we address future research topics which should be handled carefully for the barrier-coverage by heterogeneous UAVs.",0,0,1,internet+collision detection+virtual spaces,
1142,1142,"Fruitful feedback positive affective language and source anonymity improve critique reception and work outcomes. Feedback is information that can improve task performance. Online communities, educational forums, and crowd-based feedback platforms all support feedback exchange among a more diverse set of sources than ever before, with greater control over how to moderate this exchange. In this work, we study how the power relationship between the source and receiver and the tone of language influence the recep-tivity, effort, and work performance resulting from online feedback exchange. We conducted an online experiment manipulating affective language and source of feedback on a writing task. We found that critiques with positive affec-tive language increased positive emotions and reduced participants' annoyance and frustration, which led to an increase in work quality, compared to critiques without positive language. Feedback without positive affective language led to more edits, but not better work outcomes. Participants reacted more positively to feedback from an anonymous source than from a peer or an authority. Our findings provide design implications for platforms to support more fruitful feedback exchange.",0,0,1,emotional expressions+online communities+affective state,
1143,1143,"Genderfluid or attack helicopter responsible hci research practice with non binary gender variation in online communities. As non-binary genders become increasingly prevalent, researchers face decisions in how to collect, analyze and interpret research participants' genders. We present two case studies on surveys with thousands of respondents, of which hundreds reported gender as something other than simply women or men. First, Tumblr, a blogging platform, resulted in a rich set of gender identities with very few aggressive or resistive responses; the second case study, online Fantasy Football, yielded opposite proportions. By focusing on variation rather than dismissing non-binary responses as noise, we suggest that researchers can better capture gender in a way that 1) addresses gender variation without othering or erasing non-binary respondents; and 2) minimizes ""trolls'"" opportunity to use surveys as a mischief platform. The analyses of these two distinct case studies find significant gender differences in community dimensions of participation in both networked spaces as well as offering a model for inclusive mixed-methods HCI research.",0,0,1,blogging+online communities+human computer interaction,
1144,1144,"Ping pong an exergame for cognitive inhibition training. Cognitive inhibition, a key constituent of healthy cognition, has been shown to be susceptible to age-related cognitive declines. Research has shown that cognitive rehabilitation training can facil...",0,0,1,cognitive model+cognitive process,
1145,1145,"Application of support vector machine and k means clustering algorithms for robust chronic lymphocytic leukemia color cell segmentation. Chronic lymphocytic leukemia (CLL) is the most common type of blood cancer in Canadian adults. The relative 5-year survival rates for CLL in Canada is decreasing. CLL cell morphology maybe similar to normal lymphocytes and require a hematopathologist examination for diagnosis. There are a low number of related works on image analysis in CLL. This paper focuses on lymphocyte color cell segmentation using Support Vector Machine (SVM) and k-means clustering algorithms. The algorithm overcomes the occlusion problem when lymphocytes are tightly bound to the surrounding Red Blood Cells. Over and under-segmentation problems are significantly reduced. In this paper we used 440 lymphocyte images (normal and CLL), in which 140 images are used for segmentation accuracy measurement and 12 images for SVM training. The algorithm obtained 98.43% maximum accuracy for nucleus segmentation, and 98.69% for cell segmentation. The cytoplasm region can be extracted by 99.85% maximum accuracy with simple mask subtraction.",0,0,1,clustering algorithms+support vector machine+support vector,
1146,1146,"The future of natural user interfaces. This SIG is a forum to advance an integrated approach to multi-modal Natural User Interfaces. Up until now the research and design of NUI interfaces for various modalities (speech, touch, gesture) has proceeded independently. We propose having an integrated discussion with both academics and practitioners to stimulate the exchange of knowledge about the various modalities and how they might be fruitfully combined, and identifying key areas of future research and design that make the case for multi-modal NUIs. The goal is to not only create a vision of synthetic applications of NUI by connecting researchers but to also discuss ways to make the vision a reality.",0,0,1,user information+user interfaces+multi-modal interfaces,
1147,1147,"Photographically guided alignment for hdr images. This paper presents an automatic image alignment algorithm that alleviates the need to keep the camera still during the capture of a bracketed sequence to obtain an HDR image. Our algorithm assumes that the misalignment between the two consecutive exposures is translational. Using a photographically guided random search, our algorithm first finds properly exposed high contrast regions. The shift amount is then found by analyzing and matching the pixel correlations inside these regions.",0,0,1,digital image+reference image+color images,
1148,1148,"Spinning data remixing live data like a music dj. This demonstration investigates data visualization as a performance through the use of disc jockey (DJs) mixing boards. We assert that the tools DJs use in-situ can deeply inform the creation of data mixing interfaces and performances. We present a prototype system, DMix, which allows one to filter and summarize information from social streams using a audio mixing deck. It enables the Data DJ to distill multiple feeds of information in order to give an overview of a live event.",0,0,1,user information+visualization+data visualization,
1149,1149,"Shadow a system for generating artificial shadows based on object movement. This paper proposes a novel system that generates an artificial shadow by focusing on the position and angle of an object that is used to create a shadow. Our proposed system employs the metaphor of shadow-picture playing. By placing an object in front of a screen, its shadow is projected onto the screen. At the beginning, only the ""real shadow"" is displayed. After a short while, the shape of the shadow changes, and some additional effects appear on the screen. Our system utilizes the movement of the object as a trigger; it generates a kinetic artificial shadow that corresponds to the movement patterns of the object. When the user moves the object, additional effects change along with the location and position of the object.",0,0,1,target position+interreflections+image display,
1150,1150,"Instagram at the museum communicating the museum experience through social photo sharing. The everyday use of smartphones with high quality built-in cameras has lead to an increase in museum visitors' use of these devices to document and share their museum experiences. In this paper, we investigate how one particular photo sharing application, Instagram, is used to communicate visitors' experiences while visiting a museum of natural history. Based on an analysis of 222 instagrams created in the museum, as well as 14 interviews with the visitors who created them, we unpack the compositional resources and concerns contributing to the creation of instagrams in this particular context. By re-categorizing and re-configuring the museum environment, instagrammers work to construct their own narratives from their visits. These findings are then used to discuss what emerging multimedia practices imply for the visitors' engagement with and documentation of museum exhibits. Drawing upon these practices, we discuss the connection between online social media dialogue and the museum site.",0,0,1,multimedia contents+social networking sites+smart phones,
1151,1151,"An unobtrusive sleep monitoring system for the human sleep behaviour understanding. Sleep plays a vital role in good health and well-being throughout our life. Getting enough quality sleep at the right times can help protect mental and physical health, quality of life, and safety. Emerging wearable devices allow people to measure and keep track of sleep duration, patterns, and quality. Often, these approaches are intrusive and change the user's daily sleep habits. In this paper, we present an unobtrusive approach for the detection of sleep stages and positions. The proposed system is able to overcome the weakness of classic actigraphy-based systems, since it is easy to deploy and it is based on inexpensive technology. With respect to the actigraphy-based systems, the proposed system is able to detect the bed posture, that is crucial to support pressure ulcer prevention (i.e. bedsores). Results from our algorithm look promising and show that we can accurately infer sleep duration, sleep positions, and routines with a completely unobtrusive approach.",0,0,1,wearable computers+sleep state+wearable devices,
1152,1152,"Conveying spatial awareness cues in xr collaborations. Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow , and radial , and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.",0,0,1,augmented reality+virtual environments+head mounted displays,
1153,1153,"Co viewing room mobile tv content sharing in social chat. TV watching is a common leisure activity, and people often use the opportunity of TV watching to socialize with other co-watchers. However, when potential TV co-watchers like friends or family members are distributed in different locations, the social function of TV watching is disrupted. In this paper, we present a mobile TV content sharing system called Co-Viewing Room, which enables distributed users to share three types of TV content, including it whole video sharing, video clips sharing and it snapshots sharing during an online chat. We evaluated the system by comparing the influence of the three types of content sharing on users' experience and social interactions. Our results showed that people were satisfied with remote TV sharing support, and tended to be more responsive to lightweight shared content like snapshots and video clips. Also, people regarded snapshots sharing as a useful support for efficient social chat.",0,0,1,mobile users+user experience+digital television,
1154,1154,"Deep appearance models for face rendering. We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).",0,0,1,view-dependent+light transport+virtual reality,
1155,1155,"Blending history and fiction in a pervasive game prototype. Pervasive games represent an exciting development in gaming which leverages the use of sensor, visualization and networking technologies to provide immerse live-action game experiences. The field of pervasive games has been intensively researched in the recent years, as evidenced from the proliferation of available prototypes. Existing pervasive game projects commonly do not enable relocation of the game space while also overlooking several aspects which critically affect user acceptance and game experience such as scenario design, usability of employed technologies, game duration and intensity. This article introduces Barbarossa, an outdoor pervasive role-playing game. Barbarossa addresses the abovementioned issues featuring several portable game modes. It also takes into account concrete technology usage requirements for each game mode according to the game session duration and player effort required. Further, game experience is enhanced through incorporating several contextual parameters. User evaluation trials indicated warm reception of Barbarossa by players and confirmed that the main game design objectives have been largely achieved.",0,0,1,gameplay+game design+game experience,
1156,1156,"Eventpad rapid malware analysis and reverse engineering using visual analytics. Forensic analysis of malware activity in network environments is a necessary yet very costly and time consuming part of incident response. Vast amounts of data need to be screened, in a very labor-intensive process, looking for signs indicating how the malware at hand behaves inside e.g., a corporate network. We believe that data reduction and visualization techniques can assist security analysts in studying behavioral patterns in network traffic samples (e.g., PCAP). We argue that the discovery of patterns in this traffic can help us to quickly understand how intrusive behavior such as malware activity unfolds and distinguishes itself from the rest of the traffic.In this paper we present a case study of the visual analytics tool EventPad and illustrate how it is used to gain quick insights in the analysis of PCAP traffic using rules, aggregations, and selections. We show the effectiveness of the tool on real-world data sets involving office traffic and ransomware activity.",0,0,1,visualization tools+information visualization+visual analytics,
1157,1157,"A real time haptic simulator of spine surgeries. Spine surgeries are high risk operations which require the surgeons to have ample experiences. For young surgeons, effective and extensive training is critical. This paper presents a real time haptic spine surgical simulator that will be used to train residents, fellows and spine surgeons in a hospital training program. It provides a realistic environment for the trainees to practice spine surgeries and has the advantages of being interactive, low-cost, representative, and repeatable over conventional training approaches. Haptic Phantom offers the users force feedback, differentiating our system from other screen-based training systems. Computational efficiency was achieved by developing advanced graphical rendering methods. The volumetric data was classified into surface voxel cloud and inner voxel cloud by the adjacency graph which stored the relationship among voxels. To speed up the collision detection and real time rendering between the virtual surgical tools and the lumbar model, Octree-based algorithms and GPU technique were applied. To enhance the physical realism, three dimensional lumbar vertebrae models were reconstructed from CT images and associated with non-homogeneous bone density such that the rendered model best represents the spine anatomy and mechanics. We demonstrate system performance by conducting pedicle screw insertion.",0,0,1,force feedback+haptic systems+real-time rendering,
1158,1158,"The path tracing revolution in the movie industry. As path tracing allows for more realistic and faster lighting, an increasing number of movies are created the physically based way. With examples from recent movies, the architectures and novel workflows of the next generation of production renderers are introduced to a wide audience including technical directors, artists, and researchers.",0,0,1,primary path+lighting conditions+path tracing,
1159,1159,"Automatic detection of gui design smells the case of blob listener. Graphical User Interfaces (GUIs) intensively rely on event-driven programming: widgets send GUI events, which capture users' interactions, to dedicated objects called controllers. Controllers implement several GUI listeners that handle these events to produce GUI commands. In this work, we conducted an empirical study on 13 large Java Swing open-source software systems. We study to what extent the number of GUI commands that a GUI listener can produce has an impact on the change- and fault-proneness of the GUI listener code. We identify a new type of design smell, called Blob listener that characterizes GUI listeners that can produce more than two GUI commands. We show that 21% of the analyzed GUI controllers are Blob listeners. We propose a systematic static code analysis procedure that searches for Blob listener that we implement in InspectorGuidget. We conducted experiments on six software systems for which we manually identified 37 instances of Blob listener. InspectorGuidget successfully detected 36 Blob listeners out of 37. The results exhibit a precision of 97.37% and a recall of 97.59%. Finally, we propose coding practices to avoid the use of Blob listeners.",0,0,1,software+graphical user interfaces+user interfaces,
1160,1160,"Making the link providing mobile media for novice communities in the developing world. In this paper we investigate the media needs of low-income mobile users in a South African township. We develop and deploy a system that allows users to download media at no costs to themselves, in order to probe future media requirements for similar user groups. We discover that not only are the community interested in developmental information, but are also just as interested in sharing local music or videos. Furthermore, the community consume the media in ways that we did not expect, which had direct impacts on their lives. Finally, we conclude with some reflections on the value of media and the most appropriate ways to deliver it in developing world communities.",0,0,1,streaming media+mobile users+digital media,
1161,1161,"Hybrid auditory feedback a new method for mobility assistance of the visually impaired. In this paper we present a novel concept of hybrid auditory feedback in mobility assistance for people with visual disabilities in indoor environment. Hybrid auditory feedback is a gradual con-version of sound from speech-only to non-speech (i.e., spearcons) based on the sound repetitiveness and the users' frequency of the travelled route. Using a within-subject design, eight participants carried out a task using a mobility assistant application and followed a same route for few days. Preliminary results suggest that hybrid sounds in auditory feedback are more effective than non-speech and are pleasant compared to speech-only.",0,0,1,visually impaired+tactile feedback+vibrotactile feedback,
1162,1162,"Mixed reality storytelling environments based on tangible user interface take origami as an example. This paper presents a mixed reality storytelling system, which takes handicrafts as tangible interaction tools. Via the system, users can learn handicraft and then use the handicraft pieces to design, create and tell stories with HoloLens iteratively. In order to overcome the limitations of HoloLens gesture interaction, the system uses hand tracking with Kinect to implement a touch-like effect on the desktop. User study shows that our system has good usability, and it is welcomed by users. In addition, it can stimulate users interest in handicraft and storytelling, and even promote parent-child interaction effectively.",0,0,1,mixed reality+tangible user interfaces+tangible interaction,
1163,1163,"Generalized effective reducibility. We introduce two notions of effective reducibility for set-theoretical statements, based on computability with Ordinal Turing Machines (OTMs), one of which resembles Turing reducibility while the other is modelled after Weihrauch reducibility. We give sample applications by showing that certain (algebraic) constructions are not effective in the OTM-sense and considering the effective equivalence of various versions of the axiom of choice.",0,0,1,turing machines+computability+finite model theory,
1164,1164,"Evolution and analysis of hapkit an open source haptic device for educational applications. We present the design, evolution and analysis of “Hapkit,” a low-cost, open-source kinesthetic haptic device for use in educational applications. Hapkit was developed in 2013 based on the design of the Stanford Haptic Paddle, with the goal of decreasing cost and increasing accessibility for educational applications, including online teaching, K-12 school use, and college dynamic systems and control courses. In order to develop Hapkit for these purposes, we tested a variety of transmission, actuation, and structural materials. Hapkit 3.0, the latest version, uses a capstan drive, inexpensive DC motor, and 3-D printed structural materials. A frequency-domain system identification method was used to characterize Hapkit dynamics across the various designs. This method was validated using a first principles parameter measurement and a transient response analysis. This characterization shows that Hapkit 3.0 has lower damping and Coulomb friction than previous designs. We also performed a user study demonstrating that Hapkit 3.0 improves discrimination of virtual stiffness compared to previous designs. The design evolution of Hapkit resulted in a low-cost, high-performance device appropriate for open-source dissemination and educational applications.",0,0,1,haptic feedbacks+haptic interactions+tactile feedback,
1165,1165,"Annotating bi visualization dashboards needs challenges. Annotations have been identified as an important aid in analysis record-keeping and recently data discovery. In this paper we discuss the use of annotations on visualization dashboards, with a special focus on business intelligence (BI) analysis. In-depth interviews with experts lead to new annotation needs for multi-chart visualization systems, on which we based the design of a dashboard prototype that supports data and context aware annotations. We focus particularly on novel annotation aspects, such as multi-target annotations, annotation transparency across charts and data dimension levels, as well as annotation properties such as lifetime and validity. Moreover, our prototype is built on a data layer shared among different data-sources and BI applications, allowing cross application annotations. We discuss challenges in supporting context aware annotations in dashboards and other visualizations, such as dealing with changing annotated data, and provide design solutions. Finally we report reactions and recommendations from a different set of expert users.",0,0,1,recommendation+visualization+visualization tools,
1166,1166,"Democut generating concise instructional videos for physical demonstrations. Amateur instructional videos often show a single uninterrupted take of a recorded demonstration without any edits. While easy to produce, such videos are often too long as they include unnecessary or repetitive actions as well as mistakes. We introduce DemoCut, a semi-automatic video editing system that improves the quality of amateur instructional videos for physical tasks. DemoCut asks users to mark key moments in a recorded demonstration using a set of marker types derived from our formative study. Based on these markers, the system uses audio and video analysis to automatically organize the video into meaningful segments and apply appropriate video editing effects. To understand the effectiveness of DemoCut, we report a technical evaluation of seven video tutorials created with DemoCut. In a separate user evaluation, all eight participants successfully created a complete tutorial with a variety of video editing effects using our system.",0,0,1,digital videos+video streams+compressed video,
1167,1167,"Do you think what i think perceptions of delayed instant messages in computer mediated communication of romantic relations. In romantic relationships, Instant Messaging (IM) can serve as a communication channel to maintain a sense of mutual presence and relational closeness when being physically separated. However, IM is asynchronous by design. There can exist time delay for people to receive and reply to incoming messages, which may violate romantic partner's mutual expectation. Limited understanding is available around how unintended and intended delays affect the relationship of romantic partners. This work examines how romantic partners grow, perceive, and use mutual knowledge about each other in delayed IM to resolve the expectancy violation. We conducted a 7-day diary study on 16 pairs of romantic couples and used the diary entries as probes for post-study one-on-one interviews. Our findings show that couples employ different strategies of information grounding to parse and resolve delayed IM. Based on these findings, we propose several theoretical and practical implications.",0,0,1,delay jitters+communication+text messages,
1168,1168,"Understanding the specificity of web search queries. Understanding the specificity of Web search queries can help search systems better address the underlying needs of searchers and provide them relevant content. The goal of this work is to automatically determine the specificity of web search queries. Although many factors may impact the specificity of Web search queries, we investigate two factors of specificity in this research, (1) part of speech and (2) query length. We use content analysis and prior research to develop a list of nine attributes to identify query specificity. The attributes are whether a query contains a URL, a location or place name along with additional terms, compares multiple things, contains multiple distinct ideas or topics, a question that has a clear answer, request for directions, instructions or tips, a specific date and additional terms or a name and additional terms. We then apply these attributes to classify 5,115 unique queries as narrow or general. We then analyze the differences between narrow and general queries based on part of speech and query length. Our results indicate that query length and parts-of-speech usage, by themselves, can distinguish narrow and general queries. We discuss the implications of this work for search engines, marketers and users.",0,0,1,search engines+web search engines+search queries,
1169,1169,"Development and application of a classification system for voice intelligent agents. ABSTRACTThis study proposes a classification system that classifies voice intelligent agents (VIAs) into four levels (general, parental guidance, adults-only, and restricted levels) based on both n...",0,0,1,automatic classification+intelligent agents+software agents,
1170,1170,"Online pose classification and walking speed estimation using handheld devices. We describe and evaluate two methods for device pose classification and walking speed estimation that generalize well to new users, compared to previous work. These machine learning based methods are designed for the general case of a person holding a mobile device in an unknown location and require only a single low-cost, low-power sensor: a triaxial accelerometer. We evaluate our methods in straight-path indoor walking experiments as well as in natural indoor walking settings. Experiments with 14 human participants to test user generalization show that our pose classifier correctly selects among four device poses with 94% accuracy compared to 82% for previous work, and our walking speed estimates are within 12-15% (straight/indoor walk) of ground truth compared to 17-22% for previous work. Implementation on a mobile phone demonstrates that both methods can run efficiently online.",0,0,1,mobile phones+handheld+portable device,
1171,1171,"Boxelization folding 3d objects into boxes. We present a method for transforming a 3D object into a cube or a box using a continuous folding sequence. Our method produces a single, connected object that can be physically fabricated and folded from one shape to the other. We segment the object into voxels and search for a voxel-tree that can fold from the input shape to the target shape. This involves three major steps: finding a good voxelization, finding the tree structure that can form the input and target shapes' configurations, and finding a non-intersecting folding sequence. We demonstrate our results on several input 3D objects and also physically fabricate some using a 3D printer.",0,0,1,shape information+static objects+3d printers,
1172,1172,"Network properties and social sharing of emotions in social awareness streams. The relationship between social sharing of emotions, social networks and social ties is an ongoing topic of research. Such sharing of emotions occurs frequently in ""social awareness streams"" platforms like Twitter and Facebook. We use Twitter to address research questions about the association of properties of a user's network, such as size and density, with expression of emotion in the user's Twitter posts. Our analysis suggests that expression of emotion can explain some of the variance in users' Twitter networks, and that the use of emotion in interactions between users is a strong explaining factor.",0,0,1,emotional expressions+social relations+facebook,
1173,1173,"Saliency driven tactile effect authoring for real time visuotactile feedback. New-generation media such as the 4D film have appeared lately to deliver immersive physical experiences, yet the authoring has relied on content artists, impeding the popularization of such media. An automated approach for the authoring becomes increasingly crucial in lowering production costs and saving user interruption. This paper presents a fully automated framework of authoring tactile effects from existing video images to render synchronized visuotactile stimuli in real time. The spatiotemporal features of video images are analyzed in terms of visual saliency and translated into tactile cues that are rendered on tactors installed on a chair. A user study was conducted to evaluate the usability of visuotactile rendering against visual-only presentation. The result indicated that the visuotactile rendering can improve the movie to be more interesting, immersive, appealing, and understandable.",0,0,1,digital contents+tactile display+tactile feedback,
1174,1174,"Strong concepts intermediate level knowledge in interaction design research. Design-oriented research practices create opportunities for constructing knowledge that is more abstracted than particular instances, without aspiring to be at the scope of generalized theories. We propose an intermediate design knowledge form that we name strong concepts that has the following properties: is generative and carries a core design idea, cutting across particular use situations and even application domains; concerned with interactive behavior, not static appearance; is a design element and a part of an artifact and, at the same time, speaks of a use practice and behavior over time; and finally, resides on an abstraction level above particular instances. We present two strong concepts—social navigation and seamfulness—and discuss how they fulfil criteria we might have on knowledge, such as being contestable, defensible, and substantive. Our aim is to foster an academic culture of discursive knowledge construction of intermediate-level knowledge and of how it can be produced and assessed in design-oriented HCI research.",0,0,1,human computer interaction+user centered designs+interaction design,
1175,1175,"Digital neighborhood watch investigating the sharing of camera data amongst neighbors. In a neighborhood watch group, neighbors cooperate to prevent crime by sharing information and alerting police of suspicious activities. We propose a digital neighborhood watch (DNW) in which security cameras of individual homes work together to monitor the neighborhood. DNW could augment neighborhood watch by providing digital evidence of crime, increasing visibility of neighborhood activity, and automatically sending alerts when suspicious events occur. We investigate the appeal of sharing camera data with neighbors through semi-structured interviews with 11 households. Our participants validated the potential of sharing data with neighbors, particularly to provide evidence after an incident. But they also had security and privacy concerns about divulging their cameras' field of view and giving ongoing access to neighbors. For some participants, these concerns can be alleviated by enabling sharing of processed cameras views that include only the fore-ground activity or only public property (e.g., sidewalks).",0,0,1,privacy+sharing information+privacy concerns,
1176,1176,"Gait phase detection from thigh kinematics using machine learning techniques. Intelligent orthotic devices require accurate detection of gait events for real-time control. For orthoses that control the knee, an ideal system would only locate sensors at the thigh and knee, thereby facilitating sensor and electronics integration with the assistive device. To determine potential gait phase identification approaches, classification was implemented using J-48 Decision Tree, Random Forest, Multi-layer Perceptrons, and Support Vector Machine classifiers, along with 5-fold (5-FCV) and 10-fold cross validation (10-FCV). Knee angle, thigh angular velocity, and thigh acceleration were obtained from 31 able-bodied participants during walking (10 strides each). Strides were segmented into Loading Response, Push-Off, Swing, and Terminal Swing and features were extracted using a 0.1 second sliding window. Gait phase classification was performed with and without the knee angle parameter. J-48 Decision Tree with the knee angle parameter was ranked the best classifier due to its second highest classification accuracy of 97.5% and lowest mean absolute error of 0.014. Results without the knee angle parameter differed by only 0.5% and 0.003. Therefore, an inertial sensor with accelerometer and gyroscope output, located at the thigh, is a viable approach for classifying gait phases for intelligent orthosis control.",0,0,1,gyroscopes+accelerometers+assistive devices,
1177,1177,"Real world affinity diagramming practices bridging the paper digital gap. Despite the availability of computer-based alternatives both for desktop and touch screen systems, a number of cooperative work processes still commonly rely on simple paper sticky notes. In this paper, we present the first in-depth investigation of the real-world practices of people who use paper-based affinity diagrams and similar clustering processes in their work, in order to identify challenges and requirements for technology support. Findings from retrospective and artifact-based interviews with 13 participants suggest ways in which the rich interactions and material affordances offered by paper are key to the process. Instead of seeking to replicate interactions with paper on a screen, simpler transfer of information between the physical and digital worlds has the potential to address many of the most pressing problems experienced in practice. We describe different types of technology integration and augmentation, with preliminary recommendations for different situations.",0,0,1,recommendation+cooperative works+image display,
1178,1178,"Reconstructing buildings as textured low poly meshes from point clouds and images. C urrent urban building reconstruction techniques rely mainly on data gathered from either laser scans or imagebased approaches, and do usually require a large amount of manual post-processing and modeling. Difficulties arise due to erroneous and noisy data, and due to the huge amount of information to process. We propose a system that helps to overcome these time-consuming steps by automatically generating low-poly 3D building models. This is achieved by taking both information from point clouds and image information into account, exploiting the particular strengths and avoiding the relative weaknesses of these data sources: While the segmented point cloud is used to identify the dominant planar surfaces in 3D space, the images are used to extract accurate edges, fill holes and generate textured polygonal meshes of urban buildings.",0,0,1,building model+building reconstruction+triangle mesh,
1179,1179,"Vocabchecker measuring language abilities for detecting early stage dementia. Recently, dementia patients have been increasing in number worldwide, necessitating the development of techniques to detect dementia as early as possible. Considering that a typical symptom of dementia, especially Alzheimer's disease, is language impairment, speech-based dementia detection approaches have drawn much attention. This paper presents a smartphone-based dementia screening application, VocabChecker, which measures language abilities from a speech narrative via automatic speech recognition (ASR). It measures four language abilities related to dementia: number of tokens (token), number of types (type), type token ratio (TTR), and potential vocabulary size (PVS). We also reported that the use of VocabChecker has distinguished dementia patients from elderly people.",0,0,1,speech signals+vocabulary+smart phones,
1180,1180,"Implementing flipped classroom using digital media. Flipped classroom is a relatively new model in education that primarily focuses on learner-centered instructions. In other words, the model allows both management and teachers to build a more active and dynamic learning environment on the campus. The current paper tries to document the implementation of the flipped classroom model in two big data courses. Here, the course contents have been curated from a couple of websites with different contents including videos and short books as well as reports. The mixed-method approach was applied while analyzing the student perceptions in demographically two different groups. It was found that students of both groups responded positively to the flipped classroom, with each focusing on their specific goals. Consequently, the first group focused on the academic achievement whereas the second group with managerial jobs focused on solving problems in their workplaces. Students of both groups, although preferred TED talks and documentaries, they were opposed to university-like videos and O'Reilly short books and reports. Meanwhile, the use of English language contents turned out to be both a challenge and an opportunity for students. It was the first effort to implement the flipped classroom model in Iran.Younger students used flipped classroom for academic achievements.Managers used the class time for discussing their workforce problems.Students preferred resources different from lectures such as TED and documentaries.Watching videos in English was both a challenge and an opportunity for students.",0,0,1,pedagogy+digital media+academic achievement,
1181,1181,"Predictive student modeling in block based programming environments with bayesian hierarchical models. Recent years have seen a growing interest in block-based programming environments for computer science education. Although block-based programming offers a gentle introduction to coding for novice programmers, introductory computer science still presents significant challenges, so there is a great need for block-based programming environments to provide students with adaptive support. Predictive student modeling holds significant potential for adaptive support in block-based programming environments because it can identify early on when a student is struggling. However, predictive student models often make a number of simplifying assumptions, such as assuming a normal response distribution or homogeneous student characteristics, which can limit the predictive performance of models. These assumptions, when invalid, can significantly reduce the predictive accuracy of student models. To address these issues, we introduce an approach to predictive student modeling that utilizes Bayesian hierarchical linear models. This approach explicitly accounts for individual student differences and programming activity differences by analyzing block-based programs created by students in a series of introductory programming activities. Evaluation results reveal that predictive student models that account for both the distributional and hierarchical factors outperform baseline models. These findings suggest that predictive student models based on Bayesian hierarchical modeling and representing individual differences in students can substantially improve models' accuracy for predicting student performance on post-tests. By improving the predictive performance of student models, this work holds substantial potential for improving adaptive support in block-based programming environments.",0,0,1,computer science education+education+novice programmer,
1182,1182,"Innovations in autonomous systems challenges and opportunities for human agent collaboration. Autonomous agents are rapidly becoming collaborative partners in addressing diverse industry and social problem domains. With this shift brings a number of challenges and opportunities in understanding and designing for the dynamics involved in human and agents working collectively. This session will facilitate a wider discussion on the current socio-technical challenges for collaborative agent systems and human-agent interaction across a range of disciplines, and how we move forward to meet those challenges.",0,0,1,autonomous agents+intelligent agents+software agents,
1183,1183,"A study of the trade off between reducing precision and reducing resolution for data analysis and visualization. There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through quantization, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and/or resolution in a principled manner. We represent data reduction schemes as progressive streams of bits and study how various bit orderings such as by resolution, by precision, etc., impact the resulting approximation error across a variety of data sets as well as analysis tasks. Furthermore, we compute streams that are optimized for different tasks to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.",0,0,1,bandwidth+scientific data+visualization,
1184,1184,"Rapid development of civic computing services opportunities and challenges. Designing the right computing service for citizens can be extremely difficult without participatory and iterative service development processes. We discuss opportunities and challenges for quick, participatory service development by citizens, based on our experiences with two experimental context-aware services.",0,0,1,service management+service delivery+context aware services,
1185,1185,"Volvo single view of vehicle building a big data service from scratch in the automotive industry. Big data analytics is a major trend affecting business today. Many organizations collect vast amounts of data simply to investigate if its market value can be identified. In this case study, we contribute a practical example of using the big data approach to create innovative services. We describe our work to identify and integrate data sources in order establish what data is available and who may benefit from it. We then show how we worked with users to communicate the vast possibilities created when many data sources are integrated, and participated in building a new big data service. Finally, we share a set of lessons we learned which can guide future big data inquiries. Our work was conducted in the context of the inspection, service, and sale of Volvo trucks, significantly aiding risk management for Volvo Used Trucks EMEA.",0,0,1,big data+data analytics+integrated data,
1186,1186,"Promoting citizen participation through gamification. This paper presents TAB Sharing, a web-based and mobile platform for e-participation. With TAB Sharing citizens are empowered to create and share proposals with Public Administration (PA) actors and other citizens. They can submit a problem occurring in their community or an initiative that could be pursued, as well as a concrete and detailed description of a possible solution. Gamification elements have been included in TAB Sharing to foster participation. This makes the use of the application continuous over time and supports decision-making through the content exchanged. A user study with 20 citizens of two Italian municipalities has been carried out to compare a version of TAB Sharing without gamification and the one with gamification; the results of the study reported in the paper show the added value brought by gamification elements in the e-participation domain.",0,0,1,gamification+digital games+game design,
1187,1187,"Interactive paper substrates to support musical creation. We present paper substrates, interactive paper components that support the creation and manipulation of complex musical data. Substrates take different forms, from whole pages to movable strips, and contain or control typed data representations. We conducted participatory design sessions with five professional musicians with extensive experience with music creation tools. All generated innovative uses of paper substrates, manipulating their data, linking multiple representation layers and creating modular, reusable paper elements. The substrates reflect the structure of their computer-based data, but in a much more flexible and adaptable form. We use their prototypes to provide concrete examples of substrates, identify their roles, properties and functions. Finally, we explore their physical and interaction design with an interactive prototype.",0,0,1,user-centered+user centered designs+interaction design,
1188,1188,"Design of a home telepresence robot system for supporting childcare. Nowadays, for families with young children, it is not easy to have enough parental time to the care of the children. In the house, parents must perform housework and childcare at the same time, which becomes a hard labor, and one of the main reasons for parental stress. In this paper, we introduce a flexible telepresence system for childcare support, which allows parents to make simple housework, and perform childcare at the same time without the participation of another person. According to the availability and the focus required for the housework, parents can use the systems in three modes, autonomous, remote controlled and interactive. In the following content, we present the design and implementation of the system and an initial evaluation.",0,0,1,robot system+teleoperation+telepresence,
1189,1189,"Investigating visual feedforward for target expansion techniques. Target expansion techniques facilitate the pointing task by enlarging the effective sizes of targets. When the target expansion is applied to both the motor and visual spaces, the visual feedforward mechanism is key: Indeed it provides a visual aid to the user on the effective expanded targets prior to the execution or completion of the pointing task, enabling the user to take full advantage of the target expansion technique. Focusing on feedforward mechanisms, we introduce a design space that allows us to describe, classify and design target expansion techniques. To do so we first introduce and characterize the concept of atomic feedforward mechanism along three design axes. We then describe a target expansion technique as a combination of atomic feedforward mechanisms using a matrix-based notation. We provide an analytical exploration of the design space by classifying existing techniques and by designing six new techniques. We also provide a first experimental exploration of the design space in the context of distant pointing. The experimental protocol includes an innovative target layout for handling non-centroidal target expansion. The results show that feedforward dynamicity increases movement time and decreases subjective usability, while explicit expansion observability efficiently supports error prevention for distant pointing.",0,0,1,design space exploration+systems design+prototyping,
1190,1190,"Procedural voronoi foams for additive manufacturing. Microstructures at the scale of tens of microns change the physical properties of objects, making them lighter or more flexible. While traditionally difficult to produce, additive manufacturing now lets us physically realize such microstructures at low cost.   In this paper we propose to study procedural, aperiodic microstructures inspired by Voronoi open-cell foams. The absence of regularity affords for a simple approach to grade the foam geometry --- and thus its mechanical properties --- within a target object and its surface. Rather than requiring a global optimization process, the microstructures are directly generated to exhibit a specified elastic behavior. The implicit evaluation is akin to procedural textures in computer graphics, and locally adapts to follow the elasticity field. This allows very detailed structures to be generated in large objects without having to explicitly produce a full representation --- mesh or voxels --- of the complete object: the structures are added on the fly, just before each object slice is manufactured.   We study the elastic behavior of the microstructures and provide a complete description of the procedure generating them. We explain how to determine the geometric parameters of the microstructures from a target elasticity, and evaluate the result on printed samples. Finally, we apply our approach to the fabrication of objects with spatially varying elasticity, including the implicit modeling of a frame following the object surface and seamlessly connecting to the microstructures.",0,0,1,voronoi+voronoi diagrams+additive manufacturing,
1191,1191,"Interactive 4d overview and detail visualization in augmented reality. In this paper we present an approach for visualizing time-oriented data of dynamic scenes in an on-site AR view. Visualizations of time-oriented data have special challenges compared to the visualization of arbitrary virtual objects. Usually, the 4D data occludes a large part of the real scene. Additionally, the data sets from different points in time may occlude each other. Thus, it is important to design adequate visualization techniques that provide a comprehensible visualization. In this paper we introduce a visualization concept that uses overview and detail techniques to present 4D data in different detail levels. These levels provide at first an overview of the 4D scene, at second information about the 4D change of a single object and at third detailed information about object appearance and geometry for specific points in time. Combining the three levels of detail with interactive transitions such as magic lenses or distorted viewing techniques enables the user to understand the relationship between them. Finally we show how to apply this concept for construction site documentation and monitoring.",0,0,1,visualization tools+augmented reality+virtual objects,
1192,1192,"Multimodal interaction history and its use in error detection and recovery. Multimodal systems still tend to ignore the individual input behavior of users, and at the same time, suffer from erroneous sensor inputs. Although many researchers have described user behavior in specific settings and tasks, little to nothing is known about the applicability of such information, when it comes to increase the robustness of a system for multimodal inputs. We conducted a gamified experimental study to investigate individual user behavior and error types found in an actually running system. It is shown, that previous ways of describing input behavior by a simple classification scheme (like simultaneous and sequential) are not suited to build up an individual interaction history. Instead, we propose to use temporal distributions of different metrics derived from multimodal event timings. We identify the major errors that can occur in multimodal interactions and finally show how such an interaction history can practically be applied for error detection and recovery. Applying the proposed approach to the experimental data, the initial error rate is reduced from 4.9% to a minimum of 1.2%.",0,0,1,multimodal system+multi-modal interactions+multi-modal interfaces,
1193,1193,"Analysis of visual performance during the use of mobile devices while walking. Mobile computers and smartphones are often used while their users are walking. From an ergonomic viewpoint, this requires a thorough design of the user interface. Although styleguides provide multiple recommendations there is little known about basic human factors’ issues. This study provides recommendations for the visual design by analyzing the influence of walking on visual acuity with a mobile computer. N=22 volunteers participated in the experiment comparing visual acuity during standing, slow walking and fast walking. Additional conditions referred to indoor (treadmill) and outdoor (free walking) situations. The results show that walking speed has a highly significant influence on visual acuity. The results are independent of the indoor or outdoor condition. The decrease of visual acuity is similar to a row on a common eye chart. For compensating this decrease, letters and icons on a mobile device should be enlarged by about 20%.",0,0,1,user interfaces+mobile users+ergonomics,
1194,1194,"A novel algorithm for protecting from internal attacks of wireless sensor networks. Wireless sensor networks (WSNs) are composed of large number of low cost, low power and cooperatively collect the environmental information and realize the integration of the physical world and communication network. Due to open nature of the wireless medium an adversary can easily eavesdrop and replay or inject fabricated messages. Different cryptographic methods can be used to defend against some of such attacks. However, node compromise is another major problem of WSN security due to it allows an adversary to enter inside the security perimeter of the network, which raised a serious challenge for WSNs. This paper is focusing on investigating internal attacks of wireless sensor networks, by which we show our novel algorithm that under some fixed parameters designed by the network designer, we can have reasonable model for predicting the highest signal noise ratio (S/N). Therefore we may allow the sinker to be open only around that particular time period to receive the signals from the sources while the other time slots are in ""sleeping state"" to ignore any signals, including the internal attacking signal. In particularly, we found the highest S/N timing can be controlled by the sending rating for the fixed network. We can easily manipulate the sending rate to control the time of the highest S/N to protect ""internal attacks."" The simulations results have been shown to underpin our novel algorithm.",0,0,1,sensors+wireless+communication,
1195,1195,"Direct finger manipulation of 3d object image with ultrasound haptic feedback. In this study, we prototype and examine a system that allows a user to manipulate a 3D virtual object with multiple fingers without wearing any device. An autostereoscopic display produces a 3D image and a depth sensor measures the movement of the fingers. When a user touches a virtual object, haptic feedback is provided by ultrasound phased arrays. By estimating the cross section of the finger in contact with the virtual object and by creating a force pattern around it, it is possible for the user to recognize the position of the surface relative to the finger. To evaluate our system, we conducted two experiments to show that the proposed feedback method is effective in recognizing the object surface and thereby enables the user to grasp the object quickly without seeing it.",0,0,1,auto stereoscopic+virtual objects+auto-stereoscopic display,
1196,1196,"Blaze supporting two phased call graph navigation in source code. Understanding source code is crucial for successful software maintenance. A particularly important activity to understand source code is navigating the call graph [4]. Programmers have developed distinct strategies for effective call graph exploration [3, 9]. We introduce Blaze, a source code exploration tool tailored closely to these strategies. In a study, we compare Blaze to Stacksplorer [2], a tool that visualizes the immediate neighborhood of the current method in the call graph, to a tool resembling the standard Call Hierarchy view in the Eclipse IDE, and to an unmodified Xcode installation. The call graph exploration tools significantly increased success rates in typical software maintenance tasks, and using Stacksplorer or Blaze significantly reduced task completion times compared to using the Call Hierarchy or Xcode.",0,0,1,software+hierarchical structures+maintenance tasks,
1197,1197,"I read the news today oh boy. Human speakers do not create metaphors in a vacuum. Our rhetorical urges are tempered by a variety of contextual factors, such as ethos (does a metaphor reflect my values?), relevance (does a metaphor speak to my topic?), timeliness (is this a good time to use this metaphor?) and affect (does this metaphor stir the desired emotions in my audience?). The 24-h news cycle offers an ideal setting in which to explore automated metaphor generation that is both timely and topical, as not only do journalists rely on pithy metaphors to attract readers, readers often respond to the news with wittily apt, conversation-sparking metaphors of their own. Indeed, as micro-blogging platforms such as Twitter provide digital printing presses for the masses that also allow us to turn our lives and opinions into 140-character headlines, we can use computational techniques to craft personalized metaphors that suit a specific human recipient. In this paper we explore metaphor generation techniques that are shaped for a specific topical context, using approaches to topic modeling such as Latent Dirichlet Allocation, or that reflect the online personality of a specific recipient, as evidenced by their most recent emations or tweets. Each approach is instantiated in an autonomous Twitterbot, a system that creates and tweets its own content without human curation. We use Twitterbots to study the potential for humour to arise from the timely online interaction of humans and machines.",0,0,1,twitter+blogging+facebook,
1198,1198,"Emojis influence emotional communication social attributions and information processing. Abstract Many emojis symbolize nonverbal cues that are used during face-to-face communication. Despite their popularity, few studies have examined how emojis influence digital interactions. The present study addresses this gap by measuring the impact of emojis on emotion interpretation, social attributions, and information processing. Participants read messages that are typical of social exchanges in instant text messaging (IM) accompanied by emojis that mimic negative, positive and neutral facial expressions. Sentence valence and emoji valence were paired in a fully crossed design such that verbal and nonverbal messages were either congruent or incongruent. Perceived emotional state of the sender, perceived warmth, and patterns of eye movements that reflect information processing were measured. A negativity effect was observed whereby the sender's mood was perceived as negative when a negative emoji and/or a negative sentence were presented. Moreover, the presence of a negative emoji intensified the perceived negativity of negative sentences. Adding a positive emoji to a message increased the perceived warmth of the sender. Finally, processing speed and understanding of verbal messages was enhanced by the presence of congruent emojis. Our results therefore support the use of emojis, and in particular positive emojis, to improve communication, express feelings, and make a positive impression during socially-driven digital interactions.",0,0,1,text messages+emotion expression+text messaging,
1199,1199,"Partial match of 3d faces using facial curves between sift keypoints. In this work, we propose and experiment an original solution to 3D face recognition which supports partial matching of facial scans as occurs in the case of missing parts and occlusions. In the proposed approach, distinguishing traits of the face are captured by first extracting SIFT keypoints on the face scan and then measuring how the face changes along facial curves defined between pairs of keypoints. Facial curves are also associated with a measure of salience so as to distinguish curves that model characterizing traits of some subjects from curves that are frequently observed in the face of many different subjects. The recognition accuracy of the approach has been experimented on the Face Recognition Grand Challenge dataset.",0,0,1,3d face recognition+facial images+facial expression,
1200,1200,"A study of mobile banking loyalty in iran. This study investigates an exploratory study of mobile banking usage.Attitude is explained by both perceived usefulness and ease of use.Compatibility was found to be the most important driver.Perceived usefulness did not mediate the relationship between ease of use and attitude.At last, subjective norms and personal innovativeness showed a moderating effect but not absorptive capacity. The purpose of this paper is to explore barriers, the mediating role of usability and the moderating effects of personal innovativeness and subjective norms on consumers' attitudes towards use of mobile banking in Iran. Based on the consumer data collected through a survey, structural equations modeling (SEM) and path analysis were employed to test the research model. The results revealed that ""system compatibility"" was found to be the main factor affecting users' attitudes towards use of mobile banking. ""Resistance"" showed a significant negative effect on both ease of use and usefulness. ""Perceived usefulness"" mediated the relationship between ease of use and users' attitudes. At last, both subjective norms and personal innovativeness moderated the relationships between usefulness and attitude. The sample was only composed of mobile banking users and non-users were not studied. Past studies have seldom examined the role of individual drivers like personal innovativeness and social drivers like subjective norms as moderating variables in the context of developing countries.",0,0,1,mobile payment+mobile users+facilitating conditions,
1201,1201,"Mobile wellbeing. While mobile phones can be empowering, constant access to a world of people and information can also bring distraction from the present moment and from the people and things that are physically present in ways that are sometimes unwanted. Excessive use of mobile phones can also have negative consequences on our sleep and concentration. In many respects mobile phones are challenging for our general mental wellbeing. Many designers are now looking into ways to better support mental wellbeing, be it through apps for mindfulness and meditation, or the better design of notifications and sleep modes. People are also developing strategies and ways of coping with the negative aspects of mobile technology, from self-control based approaches such as uninstalling social media apps or not keeping phones by the bedside, to more practice-based approaches such as meditation. This workshop aims to bring researchers and practitioners together to discuss mobile technology, human practice and mental wellbeing.",0,0,1,cell phone+mobile phones+mobile technology,
1202,1202,"Social event driven camera control for multicharacter animations. In a virtual world, a group of virtual characters can interact with each other, and these characters may leave a group to join another. The interaction among individuals and groups often produces interesting events in a sequence of animation. The goal of this paper is to discover social events involving mutual interactions or group activities in multicharacter animations and automatically plan a smooth camera motion to view interesting events suggested by our system or relevant events specified by a user. Inspired by sociology studies, we borrow the knowledge in Proxemics, social force, and social network analysis to model the dynamic relation among social events and the relation among the participants within each event. By analyzing the variation of relation strength among participants and spatiotemporal correlation among events, we discover salient social events in a motion clip and generate an overview video of these events with smooth camera motion using a simulated annealing optimization method. We tested our approach on different motions performed by multiple characters. Our user study shows that our results are preferred in 66.19 percent of the comparisons with those by the camera control approach without event analysis and are comparable (51.79 percent) to professional results by an artist.",0,0,1,virtual spaces+virtual worlds+virtual environments,
1203,1203,"Considering visual programming environments for documenting physical computing artifacts. In online communities makers share and give feedback on DIY projects. Such feedback could also help novices who get stuck in their projects. However, documenting work in progress is little considered in current tools. We therefore developed a HowTo related web platform for documenting work in progress and studied how children (aged 1318) used it to document their physical computing projects during workshops. The evaluation outcome questions the appropriateness of our web platform and reveals the benefits of visual programming environments for documenting physical computing artifacts. Suggestions are given how to extend visual programming environments into minimalistic documentation tools that provide ways for children to successfully share their work in progress with other makers.",0,0,1,online communities+visual programming+visual programming languages,
1204,1204,"Analyzing the intelligibility of real time mobile sign language video transmitted below recommended standards. Mobile sign language video communication has the potential to be more accessible and affordable if the current recommended video transmission standard of 25 frames per second at 100 kilobits per second (kbps) as prescribed in the International Telecommunication Standardization Sector (ITU-T) Q.26/16 were relaxed. To investigate sign language video intelligibility at lower settings, we conducted a laboratory study, where fluent ASL signers in pairs held real-time free-form conversations over an experimental smartphone app transmitting real-time video at 5 fps/25 kbps, 10 fps/50 kbps, 15 fps/75 kbps, and 30 fps/150 kbps, settings well below the ITU-T standard that save both bandwidth and battery life. The aim of the laboratory study was to investigate how fluent ASL signers adapt to the lower video transmission rates, and to identify a lower threshold at which intelligible real-time conversations could be held. We gathered both subjective and objective measures from participants and calculated battery life drain. As expected, reducing the frame rate/bit rate monotonically extended the battery life. We discovered all participants were successful in holding intelligible conversations across all frame rates/bit rates. Participants did perceive the lower quality of video transmitted at 5 fps/25 kbps and felt that they were signing more slowly to compensate; however, participants' rate of fingerspelling did not actually decrease. This and other findings support our recommendation that intelligible mobile sign language conversations can occur at frame rates as low as 10 fps/50 kbps while optimizing resource consumption, video intelligibility, and user preferences.",0,0,1,smart phones+international telecommunications+video communications,
1205,1205,"A multi modal intelligent system for biofeedback interactions. Biofeedback is an emerging technology being used as a legitimate medical technique for several medical issues such as heart problems, pain, stress, depression, among others. This paper introduces the Multi-Modal Intelligent System for Biofeedback Interactions (MMISBI), an interactive and intelligent biofeedback system using an interactive mirror to facilitate and enhance the user's awareness of various physiological functions using biomedical sensors in real-time. The system comprises different biofeedback sensors that collect physiological features; the system also provides intuitive, intelligent, and adaptive user interfaces that promote a natural communication between the user and the biofeedback system. The Ambient Intelligence (AmI) technology is incorporated in the system to provide means for biofeedback responses. The proposed conceptual system is been evaluated by 15 subjects and the results are very stimulating. Ninety percent (90%) of the subjects confirmed that the system is beneficial, deployable, and affordable for personal use. On the other hand, 30% of the subjects have indicated that privacy is the resisting issue for the wide deployment of the system.",0,0,1,ambient intelligence+user interfaces+biofeedback,
1206,1206,"Interactive guidance techniques for improving creative feedback. Good feedback is critical to creativity and learning, yet rare. Many people do not know how to actually provide effective feedback. There is increasing demand for quality feedback -- and thus feedback givers -- in learning and professional settings. This paper contributes empirical evidence that two interactive techniques -- reusable suggestions and adaptive guidance -- can improve feedback on creative work. We present these techniques embodied in the CritiqueKit system to help reviewers give specific, actionable, and justified feedback. Two real-world deployment studies and two controlled experiments with CritiqueKit found that adaptively-presented suggestions improve the quality of feedback from novice reviewers. Reviewers also reported that suggestions and guidance helped them describe their thoughts and reminded them to provide effective feedback.",0,0,1,online learning+recommendation+feedback systems,
1207,1207,"Creating and analyzing stereoscopic 3d graphical user interfaces in digital games. Creating graphical user interfaces (GUI) for stereoscopic 3D (S3D) games is a difficult choice between visual comfort and effect. We present a S3D Game GUI Design Space and a list of S3D-specific attributes that emphasizes integrating visually comfortable interfaces into the game world, story and S3D view. To showcase our approach, we created two GUI concepts and evaluated them with 32 users. Our results show quality improvements for a combination of bottom position and visual attachment for a menu. In a referencing interface, placing the reference near to the target depth significantly improved perceived quality, game integration, and increased presence. These results confirm the need to create S3D GUIs with perceptual constraints in mind, demonstrating the potential to extend the user experience. Additionally, our design space offers a formal and flexible way to create new effects in S3D GUIs.",0,0,1,user interfaces+user experience+digital games,
1208,1208,"Games and big data a scalable multi dimensional churn prediction model. The emergence of mobile games has caused a paradigm shift in the video-game industry. Game developers now have at their disposal a plethora of information on their players, and thus can take advantage of reliable models that can accurately predict player behavior and scale to huge datasets. Churn prediction, a challenge common to a variety of sectors, is particularly relevant for the mobile game industry, as player retention is crucial for the successful monetization of a game. In this article, we present an approach to predicting game churn based on survival ensembles. Our method provides accurate predictions on both the level at which each player will leave the game and their accumulated playtime until that moment. Further, it is robust to different data distributions and applicable to a wide range of response variables, while also allowing for efficient parallelization of the algorithm. This makes our model well suited to perform real-time analyses of churners, even for games with millions of daily active users.",0,0,1,computer games+game industry+videogames,
1209,1209,"Using robotics and 3d printing to introduce youth to computer science and electromechanical engineering. We describe the design and implementation of a learning experience to introduce high school students to electromechanical engineering and computer science education. We used a series of hands-on collaborative group learning activities to engage participants. Our interdisciplinary team designed the experience in a way that gradually introduced the youth to topics in robotics, 3D modelling, printing and assembly, through collaborative group activities. The workshop culminated in the real-time fabrication and display of a functional 3D printed robotic hand, which was designed and customized by the participants.",0,0,1,learning environments+education+group learning,
1210,1210,"Ontopolis a semantic participatory platform for performance assessment and augmentation of urban environments. The proliferation of sensor and actuator networks in the urban environment is directly associated with an increasing volume of data, relating to various aspects of the city. Synchronously, the heterogeneity of data formats, sensory observations, measuring capabilities and accompanying contextual information is substantially growing. Aggregating immense amounts of data feeds from different networks is not necessarily sufficient for providing a better understanding about the processes taking place in the city. Instead, it is essential to extract knowledge patterns that are correlated and integrated in meaningful stories about the urban environment. In light of this new dimension of reality, the OntoPolis© project proposes an open participatory platform with the aim of allowing citizens and policy makers to assess key performance indicators (KPIs), pertinent to environmental conditions, energy consumption and mobility issues. To achieve this, the system links Semantic Web technologies and standard data encodings with diverse sensor networks that employ both technical devices and human agents. By embodying such methods in the urban design process, the project seeks to break new ground in the emerging fields of data-driven urbanism and urban informatics, towards the development of intelligent city environments. This paper introduces the conceptual and technical underpinnings of OntoPolis©, exemplifying the environmental-indicators case scenario.",0,0,1,sensor networks+sensors+user centered designs,
1211,1211,"A survey on vision based fall detection. Falls are a major cause of fatal injury for the elderly population. To improve the quality of living for seniors, a wide range of monitoring systems with fall detection functionality have been proposed over recent years. This article is a survey of systems and algorithms which aim at automatically detecting cases where a human falls and may have been injured. Existing fall detection methods can be categorized as using sensors, or being exclusively vision-based. This literature review focuses on vision-based methods.",0,0,1,activity recognition+sensors+fall detection,
1212,1212,"Multisited design an analytical lens for transnational hci. In this article, we present and articulate the analytical lens of multisited design to illuminate transnational connections between sites of design, and aid in the translation of knowledge between designers and ethnographers. This position emerges from the authors' respective engagements in ethnographic research and design engagements with a slum community center in Bangkok, Thailand, and with “makers” and entrepreneurs in Shanghai and Shenzhen, China. In both cases, we found design to be a site of engagement with and interpretation of wider connections between different locales, and between local and global networks. We identify four crucial aspects of design for the purposes of this discussion: It is normative, concerned with function and the attainment of goals; it is practical, and oriented toward constraints and opportunities; it frames and defines problems concurrently with solving them; and it takes a systems approach that accounts for the broad context of the design situation. Approaching and participating in these aspects of design evolved in concert with our ethnographic fieldwork and analysis, allowing us to take design seriously without sacrificing an ethnographic commitment to nuanced description. We conclude by touching on the epistemological similarities, rather than conflicts, between ethnography and design.",0,0,1,systems design+prototyping+human computer interaction,
1213,1213,"Faal fog computing based patient monitoring system for ambient assisted living. From the last few years, Wireless body area networks (WBANs) have attracted a lot of attention from both academia and industry due to an increase in real-time data capturing and processing for patient monitoring. This has become possible due to the technological advancements in which high computing and communication facilities are available for most of the modern handheld devices. In this environment, computing resources are available close to the proximity of the end users using the most popular technology called as Fog computing (devices used in the fog computing are called as fog devices). Most of the solutions reported in the literature for this purpose have used the traditional cloud-based infrastructure in which there may occur a long delay for getting the response even for data which is not of very huge amount which may cause a performance degradation for most of the implemented solutions (such as for treatment of neurological diseases where a real-time monitoring is required) in this environment. Hence, to cope up these issues, in this paper, we proposed a fog computing based patient monitoring system for ambient assisted living (FAAL). Data traces of the movement of the patients (for neurological diseases) are collected using sensor nodes using body area networks (BANs) and are passed using the fog gateways. To reduce the load on the communication infrastructure, an efficient clustering algorithm for data transmission is also presented in the paper. Performance of the proposed solution has been evaluated using the parameters such as-latency, and data overloading. Results obtained clearly show the superior performance of the proposed scheme as compared to the non-fog computing based environment.",0,0,1,wban+handheld+wireless body area network,
1214,1214,"Educational and digital inclusion for subjects with autism spectrum disorders in 1. Focused on inclusive public policies, the relationship between students with Autism Spectrum Disorders and mobile devices was examined for discussing the limits and possibilities of the 1:1 technological configuration for supporting educational and digital inclusion programs in the Brazilian public schools. This was a qualitative research, with exploratory and explanatory approaches. Data collection was carried out through direct observation and document analysis, interviews and focus groups. From the data collected, we could observe the flaws and the potential of the interaction of three research subjects, all attending the first years of Elementary School, with mobile devices. The laptop is not user-friendly and not easy to understand, due to the complexity of the operational system, with its multiple choices and configurations. The interaction with the tablet showed a more friendly and intuitive use, since it is used more naturally, using fingers to touch the screen. The tablet can be used anywhere and in any position, which is good for students who are hyperactive and can quality strategies for pedagogical mediations. Laptops have complex operating systems with multiple configurations.The interaction with the tablet proved to be more friendly and intuitive.The tablet handling is more natural as we use fingers to touch the screen.The tablet can be used in any position, which is good for hyperactive students.The interaction with tablets can enable strategies for pedagogical mediations.",0,0,1,mobile devices+smart phones+handheld,
1215,1215,"Technical challenges towards an aal large scale deployment. Nowadays, 13% of world’s population aged 60 years and over are dependent. AAL systems have to make the step forward and be deployed in large scale in order to respond to the needs of this population. In this paper, we present our feedback about our real deployments and the challenges we have faced during the path in order to be ready for our objective of a large scale deployment of 200 private houses. We hope this paper may help research teams to find solutions to similar problems and encourage them to externalize their solutions.",0,0,1,information systems+feedback systems+aal,
1216,1216,"Context aware ehealth information approach for the brazilian primary healthcare system. The eHealth technology will bring many benefits for patients, healthcare professionals, and institutions, especially in developing countries, such as Brazil. The eHealth solutions will enable faster, safer, and better healthcare services through the integration of Information and Communication Technologies (ICTs) with eHealth issue. This paper gives an overview of the Context-Aware Mobile Approach (CAMA) on going research activities for eHealth environments. CAMA seeks to improve the Brazilian primary healthcare system through context-aware eHealth facilities, expecting to change the way that healthcare services are applied, managed, and maintained to achieve efficiency, safety, and reliability for both patients and hospital staffs.",0,0,1,information and communication technologies+telehealth+innovative technologies,
1217,1217,"Blinking lights and other revelations experiences designing hybrid media facades. In this work we present our approach prototyping and pretesting hybrid media facades. We utilize a combination of a low-resolution light-emitting diodes (LED) and front projected high-resolution content in order to create multidimensional information layers. Our implementation in the form of a purpose built toolkit empowers designers and architects to prototype hybrid media facades, using low and high resolution simultaneously, quickly and at low cost. We further share our initial experiences through a case study setup and investigated different content settings displayed at varying viewing distances.",0,0,1,image resolution+multimedia contents+digital contents,
1218,1218,"Studying design process and example use with macaron a web based vibrotactile effect editor. Examples are a critical part of any design process, but supporting their use for a haptic medium is nontrivial. Current libraries for vibrotactile (VT) effects provide neither insight into examples' construction nor capability for deconstruction and re-composition. To investigate the special requirements of example use for VT design, we studied designers as they used a web-based effect editor, Macaron, which we created as both an evaluation platform and a practical tool. We qualitatively characterized participants' design processes and observed two basic example uses: as a starting point or template for a design task, and as a learning method. We discuss how features supporting internal visibility and composition influenced these example uses, and articulate several implications for VT editing tools and libraries of VT examples. We conclude with future work, including plans to deploy Macaron online to examine examples and other aspects of VT design in situ.",0,0,1,haptic systems+design activity+vibrotactile,
1219,1219,"How user s first impression forms on mobile user interface an erps study. In an era of mobile Internet, the using of mobile interfaces is ineluctable in our daily life. To some extent, the first impression of the mobile user interfaces determines users’ downloading and u...",0,0,1,mobile networks+user interfaces+mobile users,
1220,1220,"Geometric registration for zoomable camera using epipolar constraint and pre calibrated intrinsic camera parameter change. In general, video see-through based augmented reality (AR) cannot change the magnification of camera zooming parameter due to the difficulty of dealing with changes in intrinsic camera parameters. To realize the usage of camera zooming in AR, we propose a novel simultaneous intrinsic and extrinsic camera parameter estimation method based on an energy minimization framework. Our method is composed of the online and offline stages. An intrinsic camera parameter change depending on the zoom values is calibrated in the offline stage. Intrinsic and extrinsic camera parameters are then estimated based on the energy minimization framework in the online stage. In our method, two energy terms are added to the conventional marker-based camera parameter estimation method. One is reprojection errors based on the epipolar constraint. The other is the constraint of continuity of zoom values. By using a novel energy function, our method can estimate accurate intrinsic and extrinsic camera parameters. In an experiment, we confirmed that the proposed method can achieve accurate camera parameter estimation during camera zooming.",0,0,1,control parameters+epipolar line+augmented reality,
1221,1221,"The bhsdtr package a general purpose method of bayesian inference for signal detection theory models. We describe a novel method of Bayesian inference for hierarchical or non-hierarchical equal variance normal signal detection theory models with one or more criteria. The method is implemented as an open-source R package that uses the state-of-the-art Stan platform for sampling from posterior distributions. Our method can accommodate binary responses as well as additional ratings and an arbitrary number of nested or crossed random grouping factors. The SDT parameters can be regressed on additional predictors within the same model via intermediate unconstrained parameters, and the model can be extended by using automatically generated human-readable Stan code as a template. In the paper, we explain how our method improves on other similar available methods, give an overview of the package, demonstrate its use by providing a real-study data analysis walk-through, and show that the model successfully recovers known parameter values when fitted to simulated data. We also demonstrate that ignoring a hierarchical data structure may lead to severely biased estimates when fitting signal detection theory models.",0,0,1,general purpose+signal detection+signal detection theory,
1222,1222,"Response latencies are alive and well for identifying fakers on a self report personality inventory a reconsideration of van hooft and born 2012. Van Hooft and Born (Journal of Applied Psychology 97:301–316, 2012) presented data challenging both the correctness of a congruence model of faking on personality test items and the relative merit (i.e., effect size) of response latencies for identifying fakers. We suggest that their analysis of response times was suboptimal, and that it followed neither from a congruence model of faking nor from published protocols on appropriately filtering the noise in personality test item answering times. Using new data and following recommended analytic procedures, we confirmed the relative utility of response times for identifying personality test fakers, and our obtained results, again, reinforce a congruence model of faking.",0,0,1,integrated data+cognitive science+network latencies,
1223,1223,"An all in one toolkit for automated white box testing. Automated white-box testing is a major issue in software engineering. Over the years, several tools have been proposed for supporting distinct parts of the testing process. Yet, these tools are mostly separated and most of them support only a fixed and restricted subset of testing criteria. We describe in this paper Frama-C/LTest, a generic and integrated toolkit for automated white-box testing of C programs. LTest provides a unified support of many different testing criteria as well as an easy integration of new criteria. Moreover, it is designed around three basic services (test coverage estimation, automatic test generation, detection of uncoverable objectives) covering most major aspects of white-box testing and taking benefit from a combination of static and dynamic analyses. Services can cooperate through a shared coverage database. Preliminary experiments demonstrate the possibilities and advantages of such cooperations.",0,0,1,engineering+software+database systems,
1224,1224,"The challenges of visual kinaesthetic experience. Virtual reality experiences typically isolate the user from the real world. Notions of immersion are conventionally associated with the idea of convincing users that they are in another place, disassociated from physical reality. Given the user is however situated in that physical reality, kinesthetic bodily sensations often conflict with the virtual reality. In this paper we seek to elucidate the challenges associated with developing Visual-Kinaesthetic Experiences - experiences which provide related visual and kinaesthetic spectacle. Rather than use complex motion platforms, we submit here that physical reality is replete with interesting kinaesthetic experiences, which may be repurposed by the application of new visuals to create engaging hybrid experiences. We approach this by describing the development and deployment of Oscillate - a virtual reality experience that takes place on a swing, using it as an example to draw out what makes such experiences intrinsically interesting, and to construct three design challenges for this space.",0,0,1,virtual reality+mixed reality+embodied interaction,
1225,1225,"Haptic body powered upper extremity prosthesis simulator with tunable stiffness and sensitivity. In this paper, we present the design, implementation, and characterization of a haptic body-powered prosthesis simulator. A novel aspect of our design is enabling real-time adjustment of gripper stiffness and control cable sensitivity parameters via software based on a haptic teleoperation control algorithm. Adjustment of these parameters across a continuum of values can assist the user to decide on the optimal parameters that can best fulfill the user's functional needs, rendering the device a body-powered prosthesis prescription tool. Additionally, a switching capability between the voluntary-closing (VC) mode and the voluntary-opening (VO) mode is incorporated into the software interface. With design and operation characteristics that closely replicate the form and mechanics of an actual body-powered upper-extremity prosthesis, our haptic prosthesis enables parameter optimization for individual prosthesis users and various human-subject-based research experiments involving both amputees and non-amputees on task-based optimization of prosthesis parameters and selection of operation mode.",0,0,1,switching control+teleoperation+haptic systems,
1226,1226,"When your second life comes knocking. Conscientiousness and Emotional Stability were the strongest predictors of VW experiences.Emotional involvement was the single strongest predictor of changes to real life.Conscientiousness predicted low changes to real life.Agreeableness and Intellect predicted emotional involvement with the virtual world.Extraversion predicted similarity between users and avatars. A survey study (N=223) of participants in the social virtual world, Second Life, examined the relationship between Big Five personality factors, experiences in the virtual world and reports of changes to real life resulting from the virtual world experiences. Hypotheses about direct and indirect effects of personality on real life changes were tested with structural equation modeling. Results showed that the strength of users' relationship to the virtual environment, identification with and similarity to their avatars positively predicted reports of changes to real life, and that these three factors mediated effects of Agreeableness, Extraversion, Intellect, Conscientiousness, and Emotional Stability, on real life changes. Conscientiousness also had a direct negative relationship with real life changes. Implications are discussed for the potential of virtual social media features for activating facets of personality traits.",0,0,1,avatar+virtual worlds+virtual environments,
1227,1227,"An interface analysis method of complex information system by introducing error factors. With the rapid developments of computer technology and information technology, human-machine interfaces of aircrafts, ships, nuclear power plants, battlefield command system, and other complex information systems have evolved from the traditional control mode to digital control mode with visual information interface. This paper studies error factors of information interface in human-computer interaction based on visual cognition theory. A feasible error-cognition model is established to solve some design problems which result in serious failures in information recognition and analysis, and even in operation and execution processes. Based on Rasmussen, Norman, Reason and other error types as well as the HERA and CREAM failure identification models, we performed classification and cognitive characterization for error factors according to information search, information recognition, information identification, information selection and judgment as well as the decision-making process and obtained the comprehensive error-cognition model for complex information interface.",0,0,1,user interfaces+human computer interfaces+human computer interaction,
1228,1228,"Assisting older adults in assessing the reliability of health related websites. In this work, we address the question of how older adults may integrate information about the reliability of website information in their health-related searches. In two experiments, we compared three visual reliability indicator designs; an icon set, a simple graph and a textual list combined with website preview. Our results suggest that older adults can incorporate reliability indicators into their website judgments without significantly increasing decision time. The design of the visual indicators, however, significantly affects such ability.",0,0,1,visual informations+web content+user information,
1229,1229,"Virtual liver surgical simulator by using z buffer for object deformation. Virtual surgical simulator which is using computer graphics is much popular system than before. It is generally used in the medical areas, such as medical hospital or medical university. The simulator uses virtual organ models like liver, brain and so on. These models are usually based on the scanning data from patients and are used as volume models. Fortunately, the volume model is familiar with its cutting or deforming operation in a surgical system. For this reason, there are many kinds of surgical simulation or navigation systems using the volume model. However, visual reality of the volume model is not sufficient for human being including doctors. This means that the doctors cannot identify shape or location of a target organ from volume objects. In order to overcome this, we should use the translating method, such as marching cubes method and so on, for getting precisely polygon models which is included normal vectors of volume object. However, the method is quite time consuming and consequently the doctors cannot operate the virtual model in real-time.",0,0,1,university+virtual spaces+surgical simulation,
1230,1230,"Vis a ware integrating spatial and non spatial visualization for visibility aware urban planning. 3D visibility analysis plays a key role in urban planning for assessing the visual impact of proposed buildings on the cityscape. A call for proposals typically yields around 30 candidate buildings that need to be evaluated with respect to selected viewpoints. Current visibility analysis methods are very time-consuming and limited to a small number of viewpoints. Further, analysts neither have measures to evaluate candidates quantitatively, nor to compare them efficiently. The primary contribution of this work is the design study of Vis-A-Ware, a visualization system to qualitatively and quantitatively evaluate, rank, and compare visibility data of candidate buildings with respect to a large number of viewpoints. Vis-A-Ware features a 3D spatial view of an urban scene and non-spatial views of data derived from visibility evaluations, which are tightly integrated by linked interaction. To enable a quantitative evaluation we developed four metrics in accordance with experts from urban planning. We illustrate the applicability of Vis-A-Ware on the basis of a use case scenario and present results from informal feedback sessions with domain experts from urban planning and development. This feedback suggests that Vis-A-Ware is a valuable tool for visibility analysis allowing analysts to answer complex questions more efficiently and objectively.",0,0,1,domain knowledge+visualization+visualization tools,
1231,1231,"Player experience in a vr and non vr multiplayer game. Several studies compared the player experience of Virtual Reality (VR) games and non-VR games. However, there is a lack of research on the specific subject of VR and non-VR multiplayer games. Therefore, this work explores how implementing a multiplayer game in VR may influence the player experience compared to the non-VR alternative. We developed a short multiplayer puzzle game that can be played as a VR game or as a non-VR game. The player experience of the different game versions was evaluated by having players answer the Game Experience Questionnaire by IJsselsteijn et al. and the Simulator Sickness Questionnaire by Kennedy et al. after playing each version. The results showed that a majority of the aspects of player experience were rated significantly higher for the VR versions compared to the non-VR alternative. However, it was noted that two different VR versions fared differently compared to the non-VR alternative, and they scored vastly different when calculating a total simulator sickness score.We conclude that implementing similar multiplayer games in VR can improve the player experience, but care is required to achieve the best player experience in VR.",0,0,1,virtual reality+player experience+game experience,
1232,1232,"Immersive virtual colonoscopy. Virtual colonoscopy (VC) is a non-invasive screening tool for colorectal polyps which employs volume visualization of a colon model reconstructed from a CT scan of the patient's abdomen. We present an immersive analytics system for VC which enhances and improves the traditional desktop VC through the use of VR technologies. Our system, using a head-mounted display (HMD), includes all of the standard VC features, such as the volume rendered endoluminal fly-through, measurement tool, bookmark modes, electronic biopsy, and slice views. The use of VR immersion, stereo, and wider field of view and field of regard has a positive effect on polyp search and analysis tasks in our immersive VC system, a volumetric-based immersive analytics application. Navigation includes enhanced automatic speed and direction controls, based on the user's head orientation, in conjunction with physical navigation for exploration of local proximity. In order to accommodate the resolution and frame rate requirements for HMDs, new rendering techniques have been developed, including mesh-assisted volume raycasting and a novel lighting paradigm. Feedback and further suggestions from expert radiologists show the promise of our system for immersive analysis for VC and encourage new avenues for exploring the use of VR in visualization systems for medical diagnosis.",0,0,1,head mounted displays+volume visualization+immersive virtual environments,
1233,1233,"The role of visual complexity and prototypicality regarding first impression of websites working towards understanding aesthetic judgments. This paper experimentally investigates the role of visual complexity (VC) and prototypicality (PT) as design factors of websites, shaping users' first impressions by means of two studies. In the first study, 119 screenshots of real websites varying in VC (low vs. medium vs. high) and PT (low vs. high) were rated on perceived aesthetics. Screenshot presentation time was varied as a between-subject factor (50ms vs. 500ms vs. 1000ms). Results reveal that VC and PT affect participants' aesthetics ratings within the first 50ms of exposure. In the second study presentation times were shortened to 17, 33 and 50ms. Results suggest that VC and PT affect aesthetic perception even within 17ms, though the effect of PT is less pronounced than the one of VC. With increasing presentation time the effect of PT becomes as influential as the VC effect. This supports the reasoning of the information-processing stage model of aesthetic processing (Leder et al., 2004), where VC is processed at an earlier stage than PT. Overall, websites with low VC and high PT were perceived as highly appealing.",0,0,1,visual informations+web content+systems design,
1234,1234,"Gestar real time gesture interaction for ar with egocentric view. The existing, sophisticated AR gadgets1 in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality2 are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM) based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Realtime performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC's interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.",0,0,1,hand gesture+smart phones+interaction techniques,
1235,1235,"Point based light transport for participating media with refractive boundaries. Illumination effects in translucent materials are a combination of several physical phenomena: absorption and scattering inside the material, refraction at its surface. Because refraction can focus light deep inside the material, where it will be scattered, practical illumination simulation inside translucent materials is difficult. In this paper, we present an a Point-Based Global Illumination method for light transport on translucent materials with refractive boundaries. We start by placing volume light samples inside the translucent material and organising them into a spatial hierarchy. At rendering, we gather light from these samples for each camera ray. We compute separately the samples contributions to single, double and multiple scattering, and add them. Our approach provides high-quality results, comparable to the state of the art, with significant speed-ups (from 9 x to 60 x depending on scene complexity) and a much smaller memory footprint.",0,0,1,global illumination+indirect illumination+participating media,
1236,1236,"Vibrat o matic producing vocal vibrato using ems. Vibrato is one of the most popular vocal techniques. Based on the fact that vibrato is caused by periodic pulsation of the cricothyroid and diaphragm muscles, we explore the possibility of reproducing vibrato by actuating these muscles with electrical muscle stimulation. We present ""Vibrat-o-matic"", a system that enables users to interactively control their vocal vibrato while singing. We test the system on four participants, including two semi-professional singers, under two conditions of the electrode arrangement (stomach and neck). Although the muscles actuated by this system are not exactly the same as those used for natural vibrato, we succeeded in reproducing similar periodic modulation of volume, especially by stimulating the stomach. Moreover, although it is not suitable for training natural vibrato, it is suggested that this system could provide notion of when and where to use vibrato while singing.",0,0,1,web content+mobile users+vibrotactile feedback,
1237,1237,"4th kidrec workshop what does good look like from design research and practice to policy. Data about people are constantly collected and stored to help understand patterns which can then be leveraged by information retrieval systems (IRS), such as search engines or recommender systems, to identify and rank resources that respond to diverse users' needs. As a significant group of technology users, children's data are also collected for IRS. In the 4th edition of our proposed workshop, we seek to continue to build professional community connected to children's IRS and expand on the framework identified in the 3rd KidRec that outlines how to evaluate good IRS. This time, we are particularly interested in exploring how design, research, and practice perspectives can cohesively define policy in this area.",0,0,1,information retrieval systems+search engines+recommender systems,
1238,1238,"Kinemotion context controllable emotional motion analysis method for interactive cartoon generator. Recently, cartoon contents are applying to various media like interactive systems. In a near future, the desire of user may become to immerse their live experience into a cartoon content deeply. In automatic cartoon generation environment using NUI (Natural User Interface) like Kinect, we can comprehend the importance of linking emotion expression with user's posture. Its story and impression are uncontrollable, if the system could not choose a suitable effect for each user motion. The system should have story driven method to protect the interpretation of the world, even if there is its original piece of manga.",0,0,1,emotional expressions+emotion expression+user interfaces,
1239,1239,"Tabletops in motion the kinetics and kinematics of interactive surface physical therapy. Technology-based rehabilitation methods have shown promise for improving physical therapy programs, but much of the research is lacking quantitative analysis. We present a study conducted with healthy participants where we compared traditional ""table-based"" therapy methods with new technology-based methods. Using motion analysis and electromyography recordings, we assessed the kinetic and kinematic dimensions of participant motion during four activities. While technology-based methods are more enjoyable, our results indicate that it is the design of an activity that has a significant impact on the movements performed.",0,0,1,motion data+motion analysis+interactive surfaces,
1240,1240,"Phamilyhealth a photo sharing system for intergenerational family collaboration on health. Keeping in touch and staying aware of family members' health is an inherent desire among many families. Yet, conversations with family members about health can be challenging due to a number of reasons, such as: time, physical distance, and other concerns. Given that, it merits the development of tools that aim to facilitate sustainable health information sharing in the family context. In this paper, we present PhamilyHealth. PhamilyHealth is a web-based photo sharing system for family members to share health-related photos with one another and to encourage a family-wide, sustainable, healthy lifestyle.",0,0,1,group members+facebook+social networking sites,
1241,1241,"Removal of electrocardiogram artifacts in surface electromyography using a moving average method. This paper presents a moving average method for estimating and removing electrocardiogram (ECG) artifact in surface electromyography (sEMG) recordings. This method does not require an ECG-only recording (e.g., with muscles relaxed), which is often required by other methods. The moving average method is compared to a common template subtraction method using sEMG recordings that were contaminated by adding ECG recordings. The performance of the moving average method is comparable to the template subtraction method. It provides superior performance at low signal-to-noise ratios (SNR) and is less sensitive to SNR.",0,0,1,snr+signal to noise ratio+low signal-to-noise ratio,
1242,1242,"Exploring an ambiguous technique for eyes free mobile text entry. Mobile text entry has become an increasingly important part of many peoples' daily lives. While most input occurs through individual letters being tapped on a virtual QWERTY keyboard, this does not have to be the case. We explore how well users are able to learn an ambiguous keyboard that is modeled after a standard QWERTY layout but does not require users to tap specific keys. We show that this keyboard is a plausible text entry technique for users with little or no vision, with users achieving 19.09 Words per Minute (WPM) and 2.08% Character Error Rate after 8 hours of practice.",0,0,1,virtual spaces+keyboard+text entry,
1243,1243,"Field test of a questionnaire based mobile health reporting system. There is increasing demand to improve the quality and efficiency of healthcare delivery. Mobile health self-reporting can play a vital role here, provided that user acceptance can be achieved. We developed a mobile health reporting app for patients suffering from inflammatory bowel diseases and tested its acceptance, perceived usefulness and usability in a field trial with 35 patients over a period of eight weeks. Participants selected were already accustomed to filling in questionnaires about their health and well-being (albeit less frequently and post-hoc). Therefore we designed and implemented an interface emulating this form of interaction as closely as possible. This approach proved to be successful: our participants in the field study attested to the feasibility of this approach as indicated by the data presented here. With our example, we can show that for an effective transition to electronic/mobile systems, it should be possible to minimize the gulfs of execution and evaluation.",0,0,1,mobile terminal+location-aware+mobile users,
1244,1244,"Supporting opportunities for context aware social matching an experience sampling study. Mobile social matching systems aim to bring people together in the physical world by recommending people nearby to each other. Going beyond simple similarity and proximity matching mechanisms, we explore a proposed framework of relational, social and personal context as predictors of match opportunities to map out the design space of opportunistic social matching systems. We contribute insights gained from a study combining Experience Sampling Method (ESM) with 85 students of a U.S. university and interviews with 15 of these participants. A generalized linear mixed model analysis (n=1704) showed that personal context (mood and busyness) as well as sociability of others nearby are the strongest predictors of contextual match interest. Participant interviews suggest operationalizing relational context using social network rarity and discoverable rarity, and incorporating skill level and learning/teaching needs for activity partnering. Based on these findings we propose passive context-awareness for opportunistic social matching.",0,0,1,context-awareness+social networks+university,
1245,1245,"Active forms for responsive environments. Active Forms are interactive artefacts that are a focal point of attention for the user. Such devices are interactive and can change shape. They are embedded with both sensors and actuators and are a visualisation and embodiment of some application or service. To be more specific, Active Forms are defined as interactive products or devices that can render content thanks to perceptible changes to their physical form and appearance. We see Active Forms as ideal gateway for the interaction with and the control of Responsive Environments (RE) as defined in [1].   Tangible interaction is the precursor of Active Forms, it was essentially about coupling digital content and physical elements of an interface in an integrating combination. The content is about the internal state of the products and about some of the application(s) and service(s) they support. The tangible interaction focuses on the interface or systems that are physically embodied in the physical artifact. Tangible User Interfaces (TUI) are reactive devices that require a user input to change shape. Active Forms, on the other hand are interactive devices that change shape and appearance. The changes in the Active Forms are a result of either, user actions or internal actuators, both the physical form, such as shape or size, and the appearance, such as colour or temperature, can change.   Within Active Forms, there is a balance to be had between the cognitive load on the user, the selection of modalities, the media bandwidth and the user attention. The aim being to have the Active Forms as the user focus of interaction and attention. We have listed below some of the key features of Active Forms:   Active Forms are interactive devices that are both reactive to user actions and proactive in displaying information.   Active Forms are a gateway to applications or services within the RE and there is a change to the internal state of the device.   The user actions and the device reactions of an Active Form are merged and are spatially co-located.   Active Forms act as their embodiment, as physical objects, Active Forms also have aesthetic value per se.     § The changes in the Active Forms are a result of either, user actions or internal actuators, both the physical form, such as shape or size, and the appearance, such as colour or temperature, can change.",0,0,1,user interfaces+tangible user interfaces+tangible interaction,
1246,1246,"It s only a computer. We provide empirical evidence that virtual humans increase willingness to disclose.We frame a virtual human interviewer as computer or human during health screenings.Framing the interviewer as computer lowers evaluation fears and impression management.Framing as a computer increases expressed sadness and objectively-rated disclosure. Research has begun to explore the use of virtual humans (VHs) in clinical interviews (Bickmore, Gruber, & Picard, 2005). When designed as supportive and ""safe"" interaction partners, VHs may improve such screenings by increasing willingness to disclose information (Gratch, Wang, Gerten, & Fast, 2007). In health and mental health contexts, patients are often reluctant to respond honestly. In the context of health-screening interviews, we report a study in which participants interacted with a VH interviewer and were led to believe that the VH was controlled by either humans or automation. As predicted, compared to those who believed they were interacting with a human operator, participants who believed they were interacting with a computer reported lower fear of self-disclosure, lower impression management, displayed their sadness more intensely, and were rated by observers as more willing to disclose. These results suggest that automated VHs can help overcome a significant barrier to obtaining truthful patient information.",0,0,1,virtual agent+virtual humans+virtual spaces,
1247,1247,"Gender difference in the credibility perception of mobile websites a mixed method approach. To persuade people to buy a product or service online, they must be visually convinced and attracted to use the sales website. Thus, there is need to understand how different user groups perceive various designs of websites for better adaptation. A lot of research has shown that users' judgment of the credibility of a website is critical to its success. However, in the mobile domain, little has been done empirically to 1) investigate users' credibility perception of a website; and 2) how it changes as the user interface (UI) design is systematically altered. This paper bridges this gap by carrying out sentiment and statistical analyses of users' perceptions of four systematically modified mobile websites among 285 subjects from North America, Africa and Asia. The results show that mobile website design affects the perception of its credibility, with 1) females being more critical and sensitive to UI changes than males; and 2) the grid-layout website design preferred to the list-layout website design by both genders. The study contributes to knowledge in three ways. First, it provides a concise model for understanding users' UI perceptions, expectations and gender differences. Second, it presents important findings that will enable a gender-based mobile website adaptation. Third, it provides a set of empirically backed guidelines for mobile web design.",0,0,1,mobile web+user interfaces+mobile users,
1248,1248,"Embcomp visual interactive comparison of vector embeddings. This paper introduces embComp, a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp's central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.",0,0,1,visualization+visualization tools+visual analysis,
1249,1249,"Participants dynamic orientation to folder navigation when using a voca with a touch screen in talk in interaction. This paper reports on a conversation-analysis case study of interaction between a child with cerebral palsy and an adult using a computer-based voice output communication aid (VOCA) device with a touch-sensitive screen-input system. Data was collected from video recordings of everyday activities at school. The public nature of the VOCA-mediated turn construction process (hand movements towards the screen, on-screen folder navigation, synthetic speech) displays the projection of the turn under way and its possible continuations. The adult interlocutor orients to such projections when contributing to the topic of the ongoing turn and when initiating repair on its topical development. Contributing may activate the AAC user's further involvement in the ongoing turn, while repair may restrict the AAC user's influence on the topical progression. The findings are relevant for clinical assessment and intervention.",0,0,1,communication+communication aid+aac,
1250,1250,"Optimization of an evaluation function of the 4 sided dominoes game using a genetic algorithm. In 4-sided dominoes, the popular way of playing dominoes in Amazonas State, the strategies used for the game are more complex than those adopted in the more traditional 2-sided dominoes, the most popular dominoes game played in Brazil. This work presents the optimization of an evaluation function for the best move in 4-sided dominoes using a genetic algorithm. The evaluation function is composed of terms that incorporate the game's strategies and are defined as: punctuating, facilitating future moves and complicating opponents' moves. Coefficients were defined to determine the importance of each term of the evaluation function and a set of parameters and operators for implementation of the genetic algorithm. The players' ability was calculated by the number of wins in 5,000 matches. The results obtained during the simulations showed that the team (composed of 2 players) that used the evaluation function with its coefficients optimized by the genetic algorithm won in more than 70% of the total matches.",0,0,1,optimization problems+computer games+gameplay,
1251,1251,"Quantified ux towards a common organizational understanding of user experience. User Experience (UX) is increasingly being recognized as an important factor for the commercial success of digital products. In fact, it has become a buzzword, which is interpreted differently by different parties. This lack of common understanding inevitably leads to misunderstandings and inefficiency in industrial practice. We therefore propose a quantifiable way of describing User Experience (QUX). Based on the analysis of 84 UX evaluation methods, a sample of UX characteristics from literature, and 24 interviews with experts from academia and practice, we propose a formalism and a corresponding tool to measure, visualize, and communicate a product's UX within organizations. We showcase the benefits of our approach by integrating it into the product development processes of companies from three different industries.",0,0,1,product design+user experience+user centered designs,
1252,1252,"The chicago face database a free stimulus set of faces and norming data. Researchers studying a range of psychological phenomena (e.g., theory of mind, emotion, stereotyping and prejudice, interpersonal attraction, etc.) sometimes employ photographs of people as stimuli. In this paper, we introduce the Chicago Face Database, a free resource consisting of 158 high-resolution, standardized photographs of Black and White males and females between the ages of 18 and 40 years and extensive data about these targets. In Study 1, we report pre-testing of these faces, which includes both subjective norming data and objective physical measurements of the images included in the database. In Study 2 we surveyed psychology researchers to assess the suitability of these targets for research purposes and explored factors that were associated with researchers’ judgments of suitability. Instructions are outlined for those interested in obtaining access to the stimulus set and accompanying ratings and measures.",0,0,1,facial images+cognitive science+visual stimulus,
1253,1253,"Application of expectancy violations theory to communication with and judgments about embodied agents during a decision making task. Because users treat embodied agents (EAs) as social actors, users hold expectations about human-to-EA communication (HtEAC) similar to those in human-to-human communication. This study extends Expectancy Violations Theory (EVT) to examine how different forms of interfaces that confirm or violate user expectations affect the communication process, social judgments, ability to influence, and accuracy of recall associated with HtEAC. Positive violations of expectancy are acts or characteristics of the EA that are unexpected but evaluated favorably by the human partner. Results suggest that when the EA deviates from expectations, effects on the HtEAC process and related outcomes can be more pronounced. EAs evaluated as positive violations had more favorable effects on task attractiveness than other human or EA interaction partners. As predicted by EVT, EA interactions that were positively evaluated elicited more perceived connectedness, feelings of being understood/receptivity, and dependability than those EA interactions evaluated negatively. However, negative violations did not produce worse outcomes than negative confirmations. EVT offers a useful lens for examining the communication effects of HtEAC and points to benefits of creating EAs that evoke positive violations of expectancy.",0,0,1,embodied agent+communication+wireless communications,
1254,1254,Ensemble knn classifiers for human gait recognition based on ground reaction forces. Recognition of people based on their way of movement is one of the most interesting issues of behavioural biometrics. Among the basic characteristics of each biometric system is accuracy. It is currently considered that greater accuracy of biometric can be achieved by ensemble of two and more classifiers. The aim of this study is to present our own method for the usage of ensemble classifiers in the biometrics of the human gait based on ground reaction forces. In the presented ensemble of k-nearest neighbor classifiers the input signals were formed by dividing the ground reaction forces into sub phases characteristic of the support phase of the gait cycle. The research was carried out based on measurements from 200 people (more than 3500 gait cycles). The correct classification rate for proposed here method is more than 97.37%.,0,0,1,biometrics+classification rates+gait cycles,
1255,1255,"Data collection by the people for the people. Data Collection by the People, for the People is a CHI 2011 workshop to explore data from the crowd, bringing together mobile crowdsourcing & participatory urbanism researchers with data analysis and visualization researchers. The workshop is two-day event beginning with day of field work in the city of Vancouver, trying out mobile crowdsourcing applications and data analysis tools. Participants are encouraged to contribute applications and tools which they wish to share. Our goal is to provoke discussion and brainstorming, enabling both data collection researchers and data manipulationanalysis researchers to benefit from mutually learned lessons about crowdsourced data.",0,0,1,data aggregation+visualization+user centered designs,
1256,1256,"Ring based finger tracking using capacitive sensors and long short term memory. We present a ring-shaped interaction device, called PeriSense, utilizing capacitive sensing in order to enable finger tracking. The finger angles and its adjacent fingers are sensed by measuring capacitive proximity between electrodes and human skin. To map the capacitive measurements to the finger angles, we use long short-term memory (LSTM). By wearing the ring on the middle finger, the angles of the index, middle and ring finger can be determined from the capacitive measurements. We collected sample data from 17 users using a Leap Motion camera, which provided the reference data. In a leave-one-user-out cross-validation test, we revealed a mean absolute error of 13.02 degrees over all finger angles. The motion of the little finger and the thumb are out of the capacitive measurement range or covered by the index and ring finger, respectively. With natural finger movements, the LSTM estimates the angles of the little finger and thumb based on the movement pattern of the other fingers.",0,0,1,sensors+micromachined+piezo-resistive,
1257,1257,Thinking outsidethebox designing smart things with autistic children. This article offers a synopsis of and a critical reflection on the research project OutsideTheBox Rethinking Assistive Technology with Autistic Children. The aim of the 3-year project was to develo...,0,0,1,disabled people+assistive devices+assistive technology,
1258,1258,"Goin goblins iterative design of an entertaining archery experience. We present the iterative development of a 3D simulator for traditional archery and the design of a gaming level that should attract visitors at trade fairs and exhibitions. We want to provide users with a believable archery experience and support novel users in practicing the motion sequence of traditional archery using a virtual 3D environment. To provide a realistic haptic feedback we used a real bow interaction device and wind output in our simulation. We extended a bow damping system by electronic sensors to detect draw and release of the bow, aiming at a virtual target and user movement in front of a large projection screen. To entertain visitors at trade fairs and exhibitions we designed a two-player mode and a small 3D adventure with different tasks.",0,0,1,tactile feedback+haptic sensation+projection screens,
1259,1259,"Investigating high school students perceptions of digital badges in afterschool learning. This paper investigates high school students' perceptions of the opportunities and challenges of using digital badges to recognize and reward the skills and achievements they acquire in an afterschool science education program. Focus groups and usability tests were conducted with 10 students during the design of a badge system prototype for use in the program. We found that students recognized opportunities for personal empowerment in their use of badges, but also expressed concerns about sharing badges in various online contexts. The findings provide new insight into the values and goals that learners bring to discussions of digital badges in education. These insights hold relevance for designers of education-based badge systems as well as educators seeking to introduce badges into their practice.",0,0,1,education+usability evaluation+usability tests,
1260,1260,"Mobile crowd sensing taxonomy applications challenges and solutions. Abstract Recently, mobile crowd sensing (MCS) is captivating growing attention because of their suitability for enormous range of new types of context-aware applications and services. This is attributed to the fact that modern smartphones are equipped with unprecedented sensing, computing, and communication capabilities that allow them to perform more complex tasks besides their inherent calling features. Despite a number of merits, MCS confronts new challenges due to network dynamics, the huge volume of data, sensing task coordination, and the user privacy problems. In this paper, a comprehensive review of MCS is presented. First, we highlight the distinguishing features and potential advantages of MCS compared to conventional sensor networks. Then, a taxonomy of MCS is devised based on sensing scale, level of user involvement and responsiveness, sampling rate, and underlying network infrastructure. Afterward, we categorize and classify prominent applications of MCS in environmental, infrastructure, social, and behavioral domains. The core architecture of MCS is also described. Finally, we describe the potential advantages, determine and reiterate the open research challenges of MCS and illustrate possible solutions.",0,0,1,user privacy+communication+smart phones,
1261,1261,"Hacking masculine cultures career ambitions of female young professionals in a video game company. Women employed in video game companies are facing several barriers regarding equality and career chances. Scholars argue that career development of women is at times hindered because of hegemonic masculinity in organizations, with networks and other social factors playing a more important role than qualifications. This means that women miss out on career opportunities in a thriving and future driving industry. Yet, as gendered working environments are considered to be the result of social construction, they can also be restructured. To explore the drivers of these aspects, we conducted a qualitative field study in a video game company in a large city in Germany to understand what challenges regarding masculinity exist and how they are dealt with. Our lessons learned contribute to the realization of more gender-sensitive working environments in the video game sector and, as a result, of more diverse video games as well. Furthermore, a native English speaker dealt with typos and linguistic infelicities.",0,0,1,computer games+videogames+gameplay,
1262,1262,"A design space for privacy choices towards meaningful privacy control in the internet of things. “Notice and choice” is the predominant approach for data privacy protection today. There is considerable user-centered research on providing effective privacy notices but not enough guidance on designing privacy choices. Recent data privacy regulations worldwide established new requirements for privacy choices, but system practitioners struggle to implement legally compliant privacy choices that also provide users meaningful privacy control. We construct a design space for privacy choices based on a user-centered analysis of how people exercise privacy choices in real-world systems. This work contributes a conceptual framework that considers privacy choice as a user-centered process as well as a taxonomy for practitioners to design meaningful privacy choices in their systems. We also present a use case of how we leverage the design space to finalize the design decisions for a real-world privacy choice platform, the Internet of Things (IoT) Assistant, to provide meaningful privacy control in the IoT.",0,0,1,individual privacy+internet+user-centered,
1263,1263,"Human interaction discovery in smartphone proximity networks. Since humans are fundamentally social beings and interact frequently with others in their daily life, understanding social context is of primary importance in building context-aware applications. In this paper, using smartphone Bluetooth as a proximity sensor to create social networks, we present a probabilistic approach to mine human interaction types in real life. Our analysis is conducted on Bluetooth data continuously sensed with smartphones for over one year from 40 individuals who are professionally or personally related. The results show that the model can automatically discover a variety of social contexts. We objectively validated our model by studying its predictive and retrieval performance.",0,0,1,social relations+smart phones+mobile phones,
1264,1264,"Squad level soldier robot dynamics exploring future concepts involving intelligent autonomous robots. Future U.S. Army robots are developing capabilities to better “see” (e.g., scan and recognize objects), “think” (e.g., recognize implications and decide on a best course of action), and “act” (e.g., execute actions). This report describes systematic feedback gained from active duty Soldiers with dismounted squad level experience, to identify preferences and levels of trust with regard to squad level robotic capabilities, roles, and tactics. Soldier-based feedback will inform ongoing programs of research regarding U.S. Army Autonomous Squad Member (ASM) capabilities, through validation of mission scenarios, information requirements, and tactical maneuvers.",0,0,1,robots+robot system+autonomous robot,
1265,1265,"Egocap egocentric marker less motion capture with two fisheye cameras. Marker-based and marker-less optical skeletal motion-capture methods use an outside-in arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort with marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. Alternative suit-based systems use several inertial measurement units or an exoskeleton to capture motion with an inside-in setup, i.e. without external sensors. This makes capture independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. Therefore, we propose a new method for real-time, marker-less, and egocentric motion capture: estimating the full-body skeleton pose from a lightweight stereo pair of fisheye cameras attached to a helmet or virtual reality headset - an optical inside-in method, so to speak. This allows full-body motion capture in general indoor and outdoor scenes, including crowded scenes with many people nearby, which enables reconstruction in larger-scale activities. Our approach combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. It is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.",0,0,1,sensors+full body+virtual reality,
1266,1266,"Video diary as a means for data gathering with children encountering identities in the making. Abstract   This paper examines video diaries gathered from 10–11-year-old pupils with the aim of inquiring children׳s technology use in their everyday life. A discourse lens is utilized to provide novel insights into the nature and use of videos and diaries in Human Computer Interaction (HCI) research and design. The paper shows how the children, given the same assignment, produced their video diaries from a range of different positions such as ‘a diarist’, ‘a news anchor’ and ‘a stage performer’, and through a variety of widely known genres such as ‘an intimate, confessional diary entry’, ‘a news broadcast’, and ‘a homework assignment’. The children also smoothly moved between the positions and genres sometimes changing them several times even during one video clip. Our findings bear implications on two types of HCI research: firstly, studies using diaries for research and design purposes and, secondly, studies interested in videos produced by the research subjects. The paper emphasizes videos and diaries as a multifaceted resource not only revealing facts from the producers׳ lives but also playful experimenting with different positions and genres highlighting constant identity exploration and construction going on during the creation of the data. During the data analysis it is useful to consider within what kind of positionings and genres the pieces of data or ‘facts’ have been created as these genres and positions always frame and limit what is said and how. The paper also suggests that researchers could try to guide the research subjects to adopt certain positions and to rely on certain genres in producing their diaries or video clips to obtain better-focused data for particular research or design purposes. On the other hand, challenges involved with this kind of an attempt are also highlighted.",0,0,1,broadcast+data aggregation+human computer interaction,
1267,1267,"From interaction to performance with public displays. Interacting with public displays involves more than what happens between individuals and the system; it also concerns how people experience others around and through those displays. In this paper, we use ""performance"" as an analytical lens for understanding experiences with a public display called rhythIMs and explore how displays shift social interaction through their mediation. By performance, we refer to a situation in which people are on display and orient themselves toward an audience that may be co-located, imagined, or virtual. To understand interaction with public displays, we use two related notions of collectives--audiences and groups--to highlight the ways in which people orient to each other through public displays. Drawing examples from rhythIMs, a public display that shows patterns of instant messaging and physical presence, we demonstrate that there can be multiple, heterogeneous audiences and show how people experience these different types of collectives in various ways. By taking a performance perspective, we are able to understand how audiences that were not physically co-present with participants still influenced participants' interpretations and interactions with rhythIMs. This extension of the traditional notion of audience illuminates the roles audiences can play in a performance.",0,0,1,display devices+image display+display system,
1268,1268,"Informing aggression prevention efforts by comparing perpetrators of brief vs extended cyber aggression. As debate continues over the definition of cyberbullying, an important endeavor is identifying aggression-prevention efforts likely to impact reasons for cyberbullying and the broader phenomenon of cyber aggression. No empirical research has examined whether there are useful prevention-related distinctions between perpetrators of cyberbullying vs. perpetrators of brief cyber aggression. Using an online survey, this study explored perpetrators' beliefs, emotions, and behaviors related to 72 brief vs. 128 extended episodes of cyber aggression. The most pronounced difference was that more extended-episode perpetrators reported having been hurt by something that happened in cyberspace. One pronounced similarity was that if there had been a news story about the perpetrator doing it, 79% or more of both groups said they would not have felt proud; whereas 63% or more said they would have felt ashamed. Among both groups, 76% or more did not agree with the assertion that there should be no offline consequence for online behavior. The findings support prevention efforts intended to do the following: encourage respect and empathy, facilitate adaptive communication and decision-making skills, promote socially appropriate ways of coping with anger and conflict, and increase knowledge and application of relevant rules and laws.",0,0,1,cyber threats+cyber-crimes+communication,
1269,1269,"Patient communication in health care settings new opportunities for augmentative and alternative communication. Delivering quality health care requires effective communication between health care providers and their patients. In this article, we call on augmentative and alternative communication (AAC) practitioners to offer their knowledge and skills in support of a broader range of patients who confront communication challenges in health care settings. We also provide ideas and examples about ways to prepare people with complex communication needs for the inevitable medical encounters that they will face. We argue that AAC practitioners, educators, and researchers have a unique role to play, important expertise to share, and an extraordinary opportunity to advance the profession, while positively affecting patient outcomes across the health care continuum for a large number of people.",0,0,1,communication+wireless communications+aac,
1270,1270,"The empathic framework for task learning from implicit human feedback. Reactions such as gestures, facial expressions, and vocalizations are an abundant, naturally occurring channel of information that humans provide during interactions. A robot or other agent could leverage an understanding of such implicit human feedback to improve its task performance at no cost to the human. This approach contrasts with common agent teaching methods based on demonstrations, critiques, or other guidance that need to be attentively and intentionally provided. In this paper, we first define the general problem of learning from implicit human feedback and then propose to address this problem through a novel data-driven framework, EMPATHIC. This two-stage method consists of (1) mapping implicit human feedback to relevant task statistics such as reward, optimality, and advantage; and (2) using such a mapping to learn a task. We instantiate the first stage and three second-stage evaluations of the learned mapping. To do so, we collect a dataset of human facial reactions while participants observe an agent execute a sub-optimal policy for a prescribed training task. We train a deep neural network on this data and demonstrate its ability to (1) infer relative reward ranking of events in the training task from prerecorded human facial reactions; (2) improve the policy of an agent in the training task using live human facial reactions; and (3) transfer to a novel domain in which it evaluates robot manipulation trajectories.",0,0,1,demonstrations+facial expression+teaching method,
1271,1271,"Determination of a sagittal plane axis of rotation for a dynamic office chair. Abstract Objective This study investigated the location of the axis of rotation in sagittal plane movement of the spine in a free sitting condition to adjust the kinematics of a mobile seat for a dynamic chair. Background Dynamic office chairs are designed to avoid continuous isometric muscle activity, and to facilitate increased mobility of the back during sitting. However, these chairs incorporate increased upper body movement which could distract office workers from the performance of their tasks. A chair with an axis of rotation above the seat would facilitate a stable upper back during movements of the lower back. The selection of a natural kinematic pattern is of high importance in order to match the properties of the spine. Method Twenty-one participants performed four cycles of flexion and extension of the spine during an upper arm hang on parallel bars. The location of the axis of rotation relative to the seat was estimated using infrared cameras and reflective skin markers. Results The median axis of rotation across all participants was located 36 cm above the seat for the complete movement and 39 cm for both the flexion and extension phases, each with an interquartile range of 20 cm. Conclusion There was no significant effect of the movement direction on the location of the axis of rotation and only a weak, non-significant correlation between body height and the location of the axis of rotation. Individual movement patterns explained the majority of the variance. Application The axis of rotation for a spinal flexion/extension movement is located above the seat. The recommended radius for a guide rail of a mobile seat is between 36 cm and 39 cm.",0,0,1,infra-red cameras+location based+office workers,
1272,1272,"Designing for cohabitation naturecultures hybrids and decentering the human in design. Recent research in urban informatics has presented the city as both a complex technological center and a diverse cultural, social, and political entity. However, there has been little research into the changing role that nature plays in urban space, particularly when it comes to understanding how animals have adapted to life in technological and networked cities. In the wake of urbanization, new kinds of cohabitation, including increased interactions between humans and animals, has resulted in new challenges for those working in urban informatics. We leverage key concepts in the Anthropocene-naturecultures, hybrids, and decentering the human in design-to unpack the entanglements of animal-human-computer interaction in two design cases: The Big Cat Behavioral Tracking Initiative and The Phenology Clock. We contribute to urban informatics and HCI research by reflecting on ways in which design can promote new forms of cohabitation and support a broader conception of the city that sees animals as an essential part of the urban landscape.",0,0,1,architecture designs+systems design+human computer interaction,
1273,1273,"Virtual dives into the underwater archaeological treasures of south italy. The paper presents a virtual diving system based on a virtual reality (VR) application for the exploitation of the Underwater Cultural Heritage. The virtual diving experience has been designed to entertain users, but its added pedagogical value is explicitly emphasized too. In fact, the ludic activities, consisting in the simulation of a real diving session from the point of view of a scuba diver, are following a storyline described by a virtual diving companion who guides users during the exploration of the underwater archaeological site. The virtual diving system provides general and historical-cultural contents, but also information about the flora and fauna of the specific submerged site to the users. The results collected through user studies demonstrate that the proposed VR system is able to provide a playful learning experience, with a high emotional impact, and it has been well appreciated by a large variety of audiences, even by younger and inexperienced users.",0,0,1,virtual spaces+virtual environments+virtual reality,
1274,1274,"Kinetic tiles. We propose and demonstrate Kinetic Tiles, modular construction units for kinetic animations. Three different design methods are explored and evaluated for kinetic animation with the Kinetic Tiles using preset movements, design via animation toolkit, and design via direct input. It is expected that the Kinetic Tiles, as a new design and architecture material, will assist designers to introduce kinetic expressions to the surfaces of everyday objects and spaces.",0,0,1,architecture designs+systems design+prototyping,
1275,1275,"Exploring how persons with dementia and care partners collaboratively appropriate information and communication technologies. Persons with dementia and their care partners have been found to adapt their own technological arrangements using commercially available information and communication technologies (ICTs). Yet, little is known about these processes of technology appropriation and how care practices are impacted. Adopting a relational perspective of care, we longitudinally examined how four family care networks appropriated a new commercial ICT service into their existing technological arrangements and care practices. Cross-case analysis interpreted collaborative appropriation to encompass two interrelated processes of creating and adapting technological practices and negotiating and augmenting care relationships. Four driving forces were also proposed: motivating meanings that actors ascribe to the technology and its use; the learnability of the technology and actors’ resourcefulness; the establishment of responsive and cooperative care practices; and the qualities of empathy and shared power in care relationships. The importance of technological literacy, learning, meaning-making, and the nature and quality of care relationships are discussed. Future work is urged to employ longitudinal and naturalistic approaches, and focus design efforts on promoting synergistic care relationships and care practices.",0,0,1,ict+information and communication technologies+innovative technologies,
1276,1276,"Space walk a combination of subtle redirected walking techniques integrated with gameplay and narration. Redirected walking (RDW) denotes a collection of techniques for immersive virtual environments (IVEs), in which users are unknowingly guided on paths in the real world that vary from the paths they perceive in the IVE. For this Emerging Technologies exhibit we present a playful virtual reality (VR) experience that introduces a combination of those RDW techniques such as bending gains, rotation gains, and impossible spaces, which are all subtly integrated with the gameplay and narration to perfectly fit the given environment. Those perceptual tricks allow users to explore a virtual space station of 45 m2 in a room-scale setup by natural walking only.",0,0,1,virtual environments+virtual reality+immersive virtual environments,
1277,1277,"Augmenting service recommender systems by incorporating contextual opinions from user reviews. Context-aware recommender systems have been widely investigated in both academia and industry because they can make recommendations based on a user's current context (e.g., location, time). However, most existing context-aware techniques only use contextual information at the item level when modeling users' preferences, i.e., contextual information that correlates with users' overall evaluations of items such as ratings. Few studies have attempted to detect more fine-grained contextual preferences at the level of item aspects (e.g., a hotel's ""location"", ""food quality"", and ""service""). In this study, we use contextual weighting strategies to derive users' aspect-level context-dependent preferences from user-generated textual reviews. The inferred context-dependent preferences are then combined with users' context-independent preferences that are also inferred from reviews to reflect their stable requirements over time. To automatically incorporate both types of user preferences into the recommendation process, we propose a linear-regression-based algorithm that uses a stochastic gradient descent learning procedure. We tested the proposed recommendation algorithm with two real-life service datasets (one with hotel review data and the other with restaurant review data) and compared its contribution with three previously suggested approaches: one that does not consider contextual information; one that uses contextual information to pre-filter rating data before applying the recommendation algorithm; and one that generates recommendations according to users' aspect-level contextual preferences. The experiment results demonstrate that our approach outperforms the others in terms of recommendation accuracy.",0,0,1,recommendation+user preferences+context-aware recommender systems,
1278,1278,"Designing to support prescribed home exercises understanding the needs of physiotherapy patients. Musculoskeletal disorders are a globally significant health problem affecting millions. Physiotherapy, including prescribed exercises performed independently by patients in their homes, is a key treatment for many sufferers. However, many patients fail to complete home exercises, prolonging recovery periods or accelerating decline. Pervasive health technologies, capable of monitoring users in their homes, are ideally suited to address this problem. This paper describes user research with a group of three physiotherapists and eleven current physiotherapy patients to understand the problems and user needs underlying non-compliance with home exercise regimes. The research adopted a speed dating approach and culminated with six insights and design recommendations relating to the form and type of feedback that should be used in such systems, to how scheduling and therapist-patient communication systems should be designed and to the role of privacy.",0,0,1,communication+musculoskeletal disorders+user research,
1279,1279,"Towards practical link prediction approaches in signed social networks. The purpose of this research is to design practical link prediction models in signed social networks. Current works focus on the sign prediction, based on the assumption that it is already known whether there is a link between any two users. In other words, the no-relation status is ignored. Meanwhile, the strength of existing links are assumed to be equal, which is also not realistic. In this study, we will redefine the link prediction problem in signed networks and take a deep investigation on no-relation status. Then, we aim to propose a personalized ranking model from the individual's perspective. This research explores link prediction models in a more realistic scenario, and it will contribute to ongoing research in development of link prediction and recommendations in signed networks. Furthermore, our research will provide a better understanding on the link formation mechanism behind signed network evolution.",0,0,1,prediction modes+recommendation+social networks,
1280,1280,"Embedding clustering and coloring for dynamic maps. We describe a practical approach for visualizing multiple relationships defined on the same dataset using a geographic map metaphor, where clusters of nodes form countries and neighboring countries correspond to nearby clusters. Our aim is to provide a visualization that allows us to compare two or more such maps (showing an evolving dynamic process, or obtained using different relationships). In the case where we are considering multiple relationships, e.g., different similarity metrics, we also provide an interactive tool to visually explore the effect of combining two or more such relationships. Our method ensures good readability and mental map preservation, based on dynamic node placement with node stability, dynamic clustering with cluster stability, and dynamic coloring with color stability.",0,0,1,clustering algorithms+coloring+visualization,
1281,1281,"A system for visualizing human behavior based on car metaphors. There are many accidents such as bumping between walkers in crowded places. One of reasons for them is that it is difficult for each person to predict the behaviors of other people. On the other hand, cars implicitly communicate with other cars by presenting their contexts with equipments such as brake lights and turn signals. In this paper, we propose a system for visualizing the user context by using information presentation methods based on those found in cars, such as wearing LEDs as brake lights, which can be seen by surrounding people. The evaluation results when using our prototype system confirmed that our method visually and intuitively presented the user context. In addition, we evaluated the visibility effects of changing the mounting position of the wearable devices.",0,0,1,wearable sensors+wearable computers+wearable devices,
1282,1282,"Place brand building urban empathy as an evaluation method. The image of urban areas created by an appropriate urban branding attracts population, when cities compete for inhabitants and businesses. Experience shows that not only the positive visual and spatial features attract people, but also hard to define “atmosphere”, mood, city narrative, related to activities, events and history. Author proposes a new method of urban assessment to define features that are important, but difficult to capture - the method of urban empathy. The empathetic perception of urban space is a broad-spectrum experience: it can be lived through learning the city’s history, narrative, events in a relation to physical urban space. Emotions mapped on urban plan create the emphatic image of the city.",0,0,1,digital image+reference image+color images,
1283,1283,"Food to person interaction how to get information about what we eat. In recent years, people have become more concerned about what they eat. In order to understand if the food is fresh and nutritious, people rely on their senses and the information on food labels. But what if we could communicate with food directly? In this paper, we focus on a new way to provide information about the food items. We conducted research on how people could interact with food: by text and voice communication. Firstly, we prepared a list of ""questions-to-food"" which are of interest to the people. Secondly, we built and tested two prototypes with eight random users: students, office workers and food caterers. We then analyzed the obtained data and identified which communication approach triggered more emotions. We found out voice communication can start a new way of interaction between people and food. Also, we proposed some technical solutions for the implementation of our prototypes in the future.",0,0,1,wireless communications+emotional expressions+office workers,
1284,1284,"Visual genealogy of deep neural networks. A comprehensive and comprehensible summary of existing deep neural networks (DNNs) helps practitioners understand the behaviour and evolution of DNNs, offers insights for architecture optimization, and sheds light on the working mechanisms of DNNs. However, this summary is hard to obtain because of the complexity and diversity of DNN architectures. To address this issue, we develop DNN Genealogy, an interactive visualization tool, to offer a visual summary of representative DNNs and their evolutionary relationships. DNN Genealogy enables users to learn DNNs from multiple aspects, including architecture, performance, and evolutionary relationships. Central to this tool is a systematic analysis and visualization of 66 representative DNNs based on our analysis of 140 papers. A directed acyclic graph is used to illustrate the evolutionary relationships among these DNNs and highlight the representative DNNs. A focus + context visualization is developed to orient users during their exploration. A set of network glyphs is used in the graph to facilitate the understanding and comparing of DNNs in the context of the evolution. Case studies demonstrate that DNN Genealogy provides helpful guidance in understanding, applying, and optimizing DNNs. DNN Genealogy is extensible and will continue to be updated to reflect future advances in DNNs.",0,0,1,directed acyclic graphs+visualization+visualization tools,
1285,1285,"A computerized spatial orientation test. In three experiments, we compared performance on a paper-based perspective-taking task (the Spatial Orientation Test [SOT]; Hegarty & Waller, 2004) with performance on a computer-based version of the task. The computer-based version automates scoring angular errors, allows for different stimulus orders to be given to each participant, and allows for different testing time limits. In Experiment 1, the two media used different objects and mirror-image stimulus arrays in the two versions to mitigate the effects of memory for specific objects or responses. In Experiments 2 and 3, the two media used identical objects (also in a mirrored arrangement), to provide a more equivalent between-media comparison. We also substituted new objects for objects in the original version that had an inherent front/back (e.g., a car) and/or that were animate; directional or animate objects may add variance that is unrelated to perspective-taking ability. Experiment 3 used clarified instructions and a sample size sufficient to examine relatively small differences between the media as well as sex differences. Overall, the computer-based version produced performance that was similar to that of the paper-based version in terms of the rank-order of the participants. The new computer and paper versions of the SOT also had similar correlations with the Money Road Map test and the Santa Barbara Sense of Direction questionnaire, adding support to the claim that the computerized SOT is tapping into the same skill as the paper-based version. We provide a Java version of the new SOT, along with pdf files of instructions and practice stimuli, on the Open Science Framework website.",0,0,1,computer assisted+static objects+visual stimulus,
1286,1286,"When is best worst best a comparison of best worst scaling numeric estimation and rating scales for collection of semantic norms. Large-scale semantic norms have become both prevalent and influential in recent psycholinguistic research. However, little attention has been directed towards understanding the methodological best practices of such norm collection efforts. We compared the quality of semantic norms obtained through rating scales, numeric estimation, and a less commonly used judgment format called best-worst scaling. We found that best-worst scaling usually produces norms with higher predictive validities than other response formats, and does so requiring less data to be collected overall. We also found evidence that the various response formats may be producing qualitatively, rather than just quantitatively, different data. This raises the issue of potential response format bias, which has not been addressed by previous efforts to collect semantic norms, likely because of previous reliance on a single type of response format for a single type of semantic judgment. We have made available software for creating best-worst stimuli and scoring best-worst data. We also made available new norms for age of acquisition, valence, arousal, and concreteness collected using best-worst scaling. These norms include entries for 1,040 words, of which 1,034 are also contained in the ANEW norms (Bradley & Lang, Affective norms for English words (ANEW): Instruction manual and affective ratings (pp. 1-45). Technical report C-1, the center for research in psychophysiology, University of Florida, 1999).",0,0,1,software+psycholinguistics+affective state,
1287,1287,"Design of personalized wearable haptic interfaces to account for fingertip size and shape. The size and shape of fingertips vary significantly across humans, making it challenging to design wearable fingertip interfaces suitable for everyone. Although deemed important, this issue has often been neglected due to the difficulty of customizing devices for each different user. This article presents an innovative approach for automatically adapting the hardware design of a wearable haptic interface for a given user. We consider a three-DoF fingertip cutaneous device, composed of a static body and a mobile platform linked by three articulated legs. The mobile platform is capable of making and breaking contact with the finger pulp and re-angle to replicate contacts with arbitrarily-oriented surfaces. We analyze the performance of this device as a function of its main geometrical dimensions. Then, starting from the user's fingertip characteristics, we define a numerical procedure that best adapts the dimension of the device to: (i) maximize the range of renderable haptic stimuli; (ii) avoid unwanted contacts between the device and the skin; (iii) avoid singular configurations; and (iv) minimize the device encumbrance and weight. Together with the mechanical analysis and evaluation of the adapted design, we present a MATLAB script that calculates the device dimensions customized for a target fingertip as well as an online CAD utility for generating a ready-to-print STL file of the personalized design.",0,0,1,haptic display+haptic feedbacks+haptic interactions,
1288,1288,"An investigation of using mobile and situated crowdsourcing to collect annotated travel activity data in real word settings. Collecting annotated activity data is vital to many forms of context-aware system development. Leveraging a crowd of smartphone users to collect annotated activity data in the wild is a promising direction because the data being collected are realistic and diverse. However, current research lacks a systematic analysis comparing different approaches for collecting such data and investigating how users use these approaches to collect activity data in real world settings. In this paper, we report results from a field study investigating the use of mobile crowdsourcing to collect annotated travel activity data through three approaches: Participatory, Context-Triggered In Situ, and Context-Triggered Post Hoc. In particular, we conducted two phases of analysis. In Phase One, we analyzed and compared the resulting data collected via the three approaches and user experience. In Phase Two, we analyzed users recording and annotation behavior as well as the annotation content in using each approach in the field. Our results suggested that although Context-Triggered approaches produced a larger number of recordings, they did not necessarily lead to a larger quantity of data than the Participatory approach. It was because many of the recordings were either not labeled, incomplete, and/or fragmented due to the imperfect context detection. In addition, recordings collected by the Participatory approach tended to be more complete and contain less noise. In terms of user experience, while users appreciated automated recording and reminders because of their convenience, they highly valued having the control over what and when to record and annotate that the Participatory approach provided. Finally, we showed that activity type (Driver, Riding as Passenger, Walking) influenced users behaviors in recording and annotating their activity data. It influenced the timing of recording and annotating using the Participatory approach, users receptivity using the Context-Triggered In Situ approach, and the characteristics of the content of annotations. Based on these findings, we provide design and methodological recommendations for future work that aims to leverage mobile crowdsourcing to collect annotated activity data. The Participatory approach produced high-quality annotated activity data.User burden and control are two crucial aspects for sustaining user compliance.Activity affects recording and annotation timing and characteristics of annotations.Activity affects users receptivity when using the Context-Triggered approach.We offer suggestions on the approaches, tools, and instructions to collect activity.",0,0,1,user-centered+user centered designs+interaction design,
1289,1289,"How to present game difficulty choices exploring the impact on player experience. Matching game difficulty to player ability is a crucial step toward a rewarding player experience, yet making difficulty adjustments that are effective yet unobtrusive can be challenging. This paper examines the impact of automatic and player-initiated difficulty adjustment on player experience through two studies. In the first study, 40 participants played the casual game THYFTHYF either in motion-based or sedentary mode, using menu-based, embedded, or automatic difficulty adjustment. In the second study, we created an adapted version of the commercially available game fl0w to allow us to carry out a more focused study of sedentary casual play. Results from both studies demonstrate that the type of difficulty adjustment has an impact on perceived autonomy, but other player experience measures were not affected as expected. Our findings suggest that most players express a preference for manual difficulty choices, but that overall game experience was not notably impacted by automated difficulty adjustments.",0,0,1,gameplay+player experience+game experience,
1290,1290,"Interactive multi scale oil paint filtering on mobile devices. This work presents an interactive mobile implementation of a filter that transforms images into an oil paint look. At this, a multi-scale approach that processes image pyramids is introduced that uses flow-based joint bilateral upsampling to achieve deliberate levels of abstraction at multiple scales and interactive frame rates. The approach facilitates the implementation of interactive tools that adjust the appearance of filtering effects at run-time, which is demonstrated by an on-screen painting interface for per-pixel parameterization that fosters the casual creativity of non-artists.",0,0,1,color images+parameterization+mobile devices,
1291,1291,"Attention time perception and immersion in games. Immersion is a phenomenon experienced whilst playing digital games. Some argue that it is linked to time perception, where gamers claim that they are losing track of time while they are immersed in the game. In this work in progress, we describe an attempt to investigate the relationship between immersion and time perception. We manipulated attention because it is known to influence immersion and time perception differently. The results suggest that the experimental manipulation only affects time perception but not immersion. We therefore argue that there is a dissociation between immersion and time perception but further work is needed to investigate this in detail.",0,0,1,computer games+videogames+digital games,
1292,1292,"A research on csi based human motion detection in complex scenarios. A method for detecting human motion in complex scenarios based on Channel State Information (CSI) is presented. First, the sensitivity of CSI phase information to human motion is explored, especially to the strenuous motion. Through a large number of experiments, the influence of human motion on CSI phase is found out, and the characteristics of signal changes are extracted. The One-class Support Vector Machine (OSVM) in machine learning is used to detect the multi-target strenuous human motion. Line-Of-Sight (LOS) and Non-Line-Of-Sight (NLOS) conditions are studied in the case of obstacles appearing in the wireless link when human motion occurred. LOS and NLOS are identified by the skewness of the channel impulse response (CIR) distribution. After identifying the LOS condition and NLOS condition in the current environment, the human motion is analyzed and detected, which further improves the accuracy of human motion detection from 70% to 91%.",0,0,1,wireless+human emotion+human motions,
1293,1293,"The effects of hydration on cognitive performance during a simulated wildfire suppression shift in temperate and hot conditions. Abstract The effects on dehydration and cognitive performance from heat and/or physical activity are well established in the laboratory, although have not yet been studied for personnel working in occupations such as wildland firefighting regularly exposed to these types of conditions. This study aimed to investigate the effects of temperature and dehydration on seventy-three volunteer firefighters (35.7 ± 13.7 years, mean ± standard deviation) during a simulation of wildfire suppression under either control or hot (18–20; or 33–35 °C) temperature conditions. Results showed cognitive performance on the psychomotor vigilance task declined when participants were dehydrated in the heat and Stroop task performance was impaired when dehydrated late in the afternoon. Firefighters may be at risk of deteriorations in simple cognitive functions in the heat whilst dehydrated, although may also experience impairments in complex cognitive functions if dehydrated late in the day, irrespective of the environmental temperature.",0,0,1,switching control+cognitive process+physical impairments,
1294,1294,"Dynamic web service composition based on service integration and htn planning. Web Service Composition is an important but challenging issue because of the large number and dynamicity of Web services. This paper proposed a dynamic Web service composition framework which based on OWL-S and HTN planning. It uses OWL-S as Web service description language and HTN planning engine to calculate service execution sequence dynamically. Under this framework, we firstly develop a service integration algorithm based on an improved K-Means algorithm. Based on the proposed service integration algorithm, we design a two-stage service match algorithm, which reduce the time cost of service match making with a large number of services. In particular, our HTN planning approach is based on several service types, not all the service instances, which greatly improve the efficiency of the planning procedure.",0,0,1,web service composition+web ontology language+service execution,
1295,1295,"Bitdrones towards levitating programmable matter using interactive 3d quadcopter displays. In this paper, we present BitDrones, a platform for the construction of interactive 3D displays that utilize nano quadcopters as self-levitating tangible building blocks. Our prototype is a first step towards supporting interactive mid-air, tangible experiences with physical interaction techniques through multiple building blocks capable of physically representing interactive 3D data.",0,0,1,tangible user interfaces+display system+interaction techniques,
1296,1296,"The space bender supporting natural walking via overt manipulation of the virtual environment. Manipulating the appearance of a Virtual Environment to enable natural walking has so far focused on modifications that are intended to be unnoticed by users. In our research, we took a radically different approach by embracing the overt nature of the change. To explore this method, we designed the Space Bender, a natural walking technique for room-scale VR. It builds on the idea of overtly manipulating the Virtual Environment by ""bending"" the geometry whenever the user comes in proximity of a physical boundary. Our aim was to evaluate the feasibility of this approach in terms of performance and subjective feedback. We compared the Space Bender to two other similarly situated techniques: Stop and Reset and Teleportation, in a task requiring participants to traverse a 100 m path. Results show that the Space Bender was significantly faster than Stop and Reset, and preferred to the Teleportation technique, highlighting the potential of overt manipulation to facilitate natural walking.",0,0,1,virtual spaces+virtual environments+virtual reality,
1297,1297,"Effects of ocular artifact removal through ica decomposition on eeg phase. Neurophysiological data are widely affected by different forms of signal artifacts. In electroencephalographic recordings from the scalp, eye blinks are a main contribution as a source of signal alteration. Different approaches have been used to improve on this problem, from the rejection of part of the signal, to corrections through linear decomposition methods. A widely used technique is independent component analysis (ICA). Different studies have shown the suitability of ICA to correct a variety of artifact sources, but to our knowledge, there is no evidence of the effect of ICA in the phase of a signal, over time. This is of importance because the phase is a critical component of the physiological signals that has been implicated in several neural mechanisms. The aim of this work is to assess the level of phase distortion that ICA can potentially introduce to real and simulated data.",0,0,1,independent component analysis+signal distortion+electroencephalography (eeg),
1298,1298,"Mobile payment in fintech environment trends security challenges and services. Due to recent developments in IT technology, various Fintech technologies composing of finance and technology are being developed. Especially, because of rapidly growing online market and supply of mobile devices, the need for mobile Fintech payment service that enables easy online and off-line payment has increased. According to the 2013 report by market research company Gartner, purchase related global mobile payment market size was predicted to grow from $45.1 billion in 2012 to $224.3 billion in 2017 with average annual growth of 38%. The study will surveyed the recent trends of mobile Fintech payment services and categorized them based on the service forms to suggest requirements and security challenges so that better and securer service can be provided in the future. First, the study defined existing payment services and Fintech payment services by comparing them, and analyzed recent mobile Fintech payment services to classify mobile Fintech payment service providers into Hardware makers, Operating System makers, payment platform providers, and financial institutions to show their common features. Finally it defined requirements that mobile Fintech payment services must meet and security challenges that future and present mobile Fintech payment services will encounter in the perspective of mutual authentication, authorization, integrity, privacy, and availability. Through the suggested study, it is expected that mobile Fintech payment services will develop into more secure services in the future.",0,0,1,location-aware+mobile payment+mobile users,
1299,1299,"Gamingvideoset a dataset for gaming video streaming applications. This paper presents GamingVideoSET1, a dataset consisting of twenty-four uncompressed raw gaming videos of 30 seconds duration, 1080p resolution, and 30 fps for the research community working on gaming video quality assessment. Furthermore, the data set includes subjective quality assessment results for 90 video sequences obtained by encoding six different gaming videos using the H.264/MPEG-AVC codec standard in 15 different resolution-bitrate pairs (three resolution, five bitrates each). In addition to the reference videos, the dataset offers a total of 576 distorted videos in MP4 format, obtained by encoding the videos in 24 different resolution-bitrate pairs, and their objective quality assessment results (average and per-frame) using three video quality assessment metrics. 1The database is available at https://kingston.box.com/v/GamingVideoSET",0,0,1,subjective quality+video streaming+subjective quality assessments,
1300,1300,"Living disability theory reflections on access research and design. Accessibility research and disability studies are intertwined fields focused on, respectively, building a world more inclusive of people with disability and understanding and elevating the lived experiences of disabled people. Accessibility research tends to focus on creating technology related to impairment, while disability studies focuses on understanding disability and advocating against ableist systems. Our paper presents a reflexive analysis of the experiences of three accessibility researchers and one disability studies scholar. We focus on moments when our disability was misunderstood and causes such as expecting clearly defined impairments. We derive three themes: ableism in research, oversimplification of disability, and human relationships around disability. From these themes, we suggest paths toward more strongly integrating disability studies perspectives and disabled people into accessibility research.",0,0,1,assistive devices+disabilities+people with disabilities,
1301,1301,Wordpress of objects addressing layman participation in a post industrial society. In this paper a perspective on layman participation in the design of everyday products is presented. The development of digital fabrication technologies such as 3D printing enables an increasing involvement of the layman in appropriating the performance of objects to their own needs and desires. The question is how professional designers as well as laymen deal with openness in product design. An analogy is made with the content management system Wordpress to discuss how could be dealt with openness in a toolkit that addresses multiple skill levels of its users.,0,0,1,content management system+context management+content management,
1302,1302,"Hci in food product innovation. Food is essential to the survival of the world population. There are several processes in order to make food available to consumers: for example, production, transportation, and consumption. Since the global demand of food is always on the rise, there is a need to improve the efficiency in all the processes in food industries. For example, the food production industries are often not equipped with the right decision-making tools to allow farmers to properly deal with important factors such as environmental changes. On the other hand, tools are not difficult to create but can be very challenging to be successfully adopted by the professionals, especially when the tools require them to change their normal work practices. In this SIG, we will discuss how HCI can improve food product industries with suitable information for each food process. \",0,0,1,human computer interaction+user centered designs+interaction design,
1303,1303,"Daybyday interactive and customizable use of mobile technology in the cognitive development process of children with autistic spectrum disorder. Autistic Spectrum Disorder (ASD) was firstly described as a disturbance of affective contact, including language deficiency, social interaction limitation, and repetitive/restrictive behaviors. ASD individuals are to be motivated and encouraged to seek for independence and cognitive development, in order to overcome the restrictions imposed by the disturbance. This paper presents the development of an application aimed specifically at helping ASD children aged 8-12 years improve, by establishing a sequential and highly-customizable routine. Developed with the help of professionals that work with autistic children and their caregivers, the application proves to be a support tool for the ASD individuals’ reality.",0,0,1,information and communication technologies+affective state+mobile technology,
1304,1304,"Prosperity4all designing a multi stakeholder network for economic inclusion. People with disabilities are disproportionately affected by systemic global economic problems such as digital exclusion, income disparity, unemployment and poverty. The Prosperity4all project, an international consortium supported in part by the European Union FP7 program, is seeking to address the economic exclusion of consumers and producers at the margins, including people with disabilities. This article discusses the economic or business design models being considered in this emerging initiative. Like the work that the platform supports, the platform itself must be designed for diversity.",0,0,1,disparity+economics+disabilities,
1305,1305,"Evaluating a social media application for sustainability in the workplace. The goal of this research is to investigate the benefits of using a web-based social network to promote energy awareness, and influence energy-saving behavior of typical office workers. We propose that a social network integrated into the workplace environment - allowing people to track their own energy-related activities, to share this information, and to view and react to peers' activities - can take advantage of social influence to positively affect behavior. We are currently developing a prototype of such an application through iterative design. In the final phase of this work we will conduct experiments with a large number of subjects to test the ability of this application to influence attitudes and behaviors of office workers, and for providing a platform for commercial building operators to better communicate with occupants.",0,0,1,energy awareness+social relationships+office workers,
1306,1306,"Ropmate visually assisting the creation of rop based exploits. Exploits based on ROP (Return-Oriented Programming) are increasingly present in advanced attack scenarios. Testing systems for ROP-based attacks can be valuable for improving the security and reliability of software. In this paper, we propose ROPMATE, the first Visual Analytics system specifically designed to assist human red team ROP exploit builders. In contrast, previous ROP tools typically require users to inspect a puzzle of hundreds or thousands of lines of textual information, making it a daunting task. ROPMATE presents builders with a clear interface of well-defined and semantically meaningful gadgets, i.e., fragments of code already present in the binary application that can be chained to form fully-functional exploits. The system supports incrementally building exploits by suggesting gadget candidates filtered according to constraints on preserved registers and accessed memory. Several visual aids are offered to identify suitable gadgets and assemble them into semantically correct chains. We report on a preliminary user study that shows how ROPMATE can assist users in building ROP chains.",0,0,1,software+smart phones+visual analytics,
1307,1307,"Thermal comfort design of personalized casts. This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.",0,0,1,voronoi+infra-red cameras+voronoi diagrams,
1308,1308,"The malthusian paradox performance in an alternate reality game. The Malthusian Paradox is a transmedia alternate reality game (ARG) created by artists Dominic Shaw and Adam Sporne played by 300 participants over 3 months. We explore the design of the game, which cast players as agents of a radical organisation attempting to uncover the truth behind a kidnapping and a sinister biotech corporation and highlight how it redefined performative frames by blurring conventional performer and spectator roles in sometimes discomforting ways. Players participated in the game via a broad spectrum of interaction channels, including performative group spectacles and 1-to-1 engagements with game characters in public settings, making use of low- and high-tech physical and online artefacts including bespoke and third-party websites. Players and game characters communicated via telephony and social media in both a designed and an ad hoc manner. We reflect on the production and orchestration of the game, including the dynamic nature of the strong episodic narrative driven by professionally produced short films that attempted to respond to the actions of players and the difficulty of designing for engagement across hybrid and temporally expansive performance space. We suggest that an ARG whose boundaries are necessarily unclear affords rich and emergent, but potentially unsanctioned and uncontrolled, opportunities for interactive performance, which raises significant challenges for design.",0,0,1,telephone+computer games+gameplay,
1309,1309,"Interactive visualization of streaming data with kernel density estimation. In this paper, we discuss the extension and integration of the statistical concept of Kernel Density Estimation (KDE) in a scatterplot-like visualization for dynamic data at interactive rates. We present a line kernel for representing streaming data, we discuss how the concept of KDE can be adapted to enable a continuous representation of the distribution of a dependent variable of a 2D domain. We propose to automatically adapt the kernel bandwith of KDE to the viewport settings, in an interactive visualization environment that allows zooming and panning. We also present a GPU-based realization of KDE that leads to interactive frame rates, even for comparably large datasets. Finally, we demonstrate the usefulness of our approach in the context of three application scenarios - one studying streaming ship traffic data, another one from the oil & gas domain, where process data from the operation of an oil rig is streaming in to an on-shore operational center, and a third one studying commercial air traffic in the US spanning 1987 to 2008.",0,0,1,bandwidth+visualization+visualization tools,
1310,1310,"Exploring entities in text with descriptive non photorealistic rendering. We present a novel approach to text visualization called descriptive non-photorealistic rendering which exploits the inherent spatial and abstract dimensions in text documents to integrate 3D non-photorealistic rendering with information visualization. The visualization encodes text data onto 3D models, emphasizing the relative significance of words in the text and the physical, real-world relationships between those words. Analytic exploration is supported through a collection of interactive widgets and direct multitouch interaction with the 3D models. We applied our method to analyze a collection of vehicle complaint reports from the National Highway Traffic Safety Administration (NHTSA), and through a qualitative study, we demonstrate how our system can support tasks such as comparing the reliability of different models, finding interesting facts, and revealing possible causal relations between car parts.",0,0,1,3d modelling+visualization+information visualization,
1311,1311,"Theories behind ux research and how they are used in practice. At CHI2011 we organized a SIG session asking the question ""What theoretical roots do we build on, if any, in UX research?"" Overall, 122 single items from about 70 participants were collected, which corroborates the relevance of and interest in this topic. Whilst the theoretical foundations for UX research are not yet established, those responses can serve as candidate resources for setting the theoretical directions. A primary conclusion from the SIG discussion is that the CHI community needs theories in UX research and practice. A major contribution of the workshop will be to clarify the applicability and transferability of different theories, theoretical foundations, concepts in informing UX design and evaluation in both research and practice. In particular we will look into theories that have already been applied in practice.",0,0,1,user centered designs+user research+interaction design,
1312,1312,"Local feature descriptors for 3d object recognition in ubiquitous virtual reality. This paper represents 3D object recognition, which is an extension of the common feature point-based object recognition, based on novel descriptors utilizing local angles (for shape), gradient orientations (for texture of corners), and color information. First, the proposed algorithm extracts complementary feature points by randomly sampling the positions of the object edges. Then, it generates the proposed descriptors combining local angle patterns, gradient orientations, and color information. After making the descriptors, the method learns a codebook to enable the proposed algorithm to integrate the extracted feature points into a histogram through this codebook. Finally, the method classifies the query histogram based on a classifier. We expect that the proposed algorithm is robust to less textured and similar-shaped objects. The proposed method could be used as a core technology of the initial step of the information retrieval.",0,0,1,3d objects+3d object+virtual reality,
1313,1313,"Iot enabled web warehouse architecture a secure approach. Web warehouse (WWh) has overcome the geographical dependencies of data warehouse. With the rapid development of WWh, decision makers (humans) and intelligent devices are able to remotely retrieve the information for supporting the effective decision-making process. This paper presents a secure Web service-oriented architecture of the WWh. The proposed architecture provides the better scalability, availability of WWh, and secure analytical service for human and Internet of Things for effective decision making. In addition, the service-oriented architecture of WWh integrates intelligent devices for the process of decision making. The proposed architecture uses XML-based Web services to provide authentication, authorization, and data confidentiality and integrity. Experiments show that the proposed architecture is more reliable, scalable, and secure.",0,0,1,authentication+internet of things+internet,
1314,1314,"Semantic adaptive social web. The Social Web, or the so called Web 2.0, is growing daily by the number of users and applications. In this way, a significant part of newly generated Web content and traffic is created by the users itself. They create, connect, comment, tag, rate, remix, upload, download, new or existing resources in an architecture of participation, where user contribution and interaction adds value. Users are also involved in a broad range of social activities like creating social relationships, recommending and sharing resources with friends, creating groups and communities, commenting friends activities and profiles and so on. But not only users benefit from the user-generated content, also social applications profit from that content by using it for personalization and adaptation to user needs.",0,0,1,personalizations+social relations+social computing,
1315,1315,"A brain computer interface for extended reality interfaces. Extended reality (XR) technologies, such as augmented reality (AR) and virtual reality (VR), remain limited in their interaction modalities. Prevailing interaction methods such as hand gestures and voice recognition prove awkward in XR environments, even when performing common tasks (e.g., object selection, menu navigation, and others). In contrast, an ideal interaction method would robustly and naturally translate a user's intention into both 2D and 3D environmental controls. A direct brain-computer interface (BCI) system is ideally situated to accomplish this. Neurable's technology provides a solution to maximize XR's potential, affording users real-time mental selection via dry electroencephalography (EEG).",0,0,1,hand gesture+augmented reality+virtual reality,
1316,1316,"Back of device force feedback improves touchscreen interaction for mobile devices. Touchscreen interaction suffers from occlusion problems as fingers can cover small targets, which makes interacting with such targets challenging. To improve touchscreen interaction accuracy and consequently the selection of small or hidden objects, we introduce a back-of-device force feedback system for smartphones. We introduce a new solution that combines force feedback on the back to enhance touch input on the front screen. The interface includes three actuated pins at the back of a smartphone. All three pins are driven by microservos and can be actuated up to a frequency of 50 Hz and a maximum amplitude of 5 mm. In a first psychophysical user study, we explored the limits of the system. Thereafter, we demonstrate through a performance study that the proposed interface can enhance touchscreen interaction precision, compared to state-of-the-art methods. In particular, the selection of small targets performed remarkably well with force feedback. The study additionally shows that users subjectively felt significantly more accurate with force feedback. Based on the results, we discuss back-to-front feedback design issues and demonstrate potential applications through several prototypical concepts to illustrate where the back-of-device force feedback could be beneficial.",0,0,1,mobile phones+force feedback+haptic systems,
1317,1317,"Enhanced feed forward for a user aware multi touch device. Common multi-touch devices guide the user with feedback visualization during or after a registered interaction. Feed-forward techniques are less frequently used or not common at all. Our approach aims at a continuous process in which the system is aware of the users before, during, and after an explicit interaction takes place. This opens up the possibility for novel scenarios of user centered applications. Our setup utilizes Microsofts's depth-camera Kinect to collect the user's posture data in combination with a multi-touch device. This is a low cost and easy to install approach for collecting detailed information about the people and their position in close proximity of a multi-touch table as well as the location of their physical contact. Based on this information, we propose five phases of interaction and analyze the sequence of input during a typical workflow. Eight application concepts show the relevance of these phases using appropriate forms of visualization and we evaluated three of those concepts in a user-study.",0,0,1,microsoft kinect+visualization+multi-modal interactions,
1318,1318,"Who gets the blame for service failures attribution of responsibility toward robot versus human service providers and service firms. Abstract Service robots are on the rise. Technological advances in engineering, artificial intelligence, and machine learning enable robots to take over tasks traditionally carried out by humans. Despite the rapid increase in the employment of robots, there are still frequent failures in the performance of service robots. This research examines how and to what extent people attribute responsibility toward service robots in such cases of service failures compared to when humans provide the same service failures. Participants were randomly assigned to read vignettes describing a service failure either by a service robot or a human and were then asked who they thought was responsible for the service failure. The results of three experiments show that people attributed less responsibility toward a robot than a human for the service failure because people perceive robots to have less controllability over the task. However, people attributed more responsibility toward a service firm when a robot delivered a failed service than when a human delivered the same failed service. This research advances theory regarding the perceived responsibility of humans versus robots in the service sector, as well as the perceived responsibility of the firms involved. There are also important practical considerations raised by this research, such as how utilizing service robots may influence customer attitudes toward service firms.",0,0,1,humanoid robot+service robots+artificial intelligence,
1319,1319,"User attention oriented augmented reality on documents with document dependent dynamic overlay. When we read a document (any kind of, scientific papers, novels, etc.), we often encounter a situation that the information from the reading document is too less to comprehend what the author(s) would like to convey. In this paper, we demonstrate how the combination of a wearable eye tracker, a see-through head-mounted display (HMD) and an image based document retrieval engine enhances people's reading experiences. By using our proposed system, the reader can get supportive information in the see-through HMD when he wants. A wearable eye tracker and a document retrieval engine are used to detect which line in the document the reader is reading. We propose a method to detect the reader's attention on a word in a reading document, in order to present information at a preferable moment. Furthermore, we also propose a method to project a point of the document to a point of the HMD screen, by calculating the pose of the reading document in the camera image. This projection enables the system to overlay the information dynamically in an augmented view on the reading line. The results from the user study and the experiments show the potential of the proposed system in a practical use case.",0,0,1,wearable computers+augmented reality+head mounted displays,
1320,1320,"A safe low cost hmd for underwater vr experiences. Recently, consumer head-mounted VR displays (HMDs) like the Oculus Rift1 and Samsung Gear VR2 have driven much research into interesting applications that include full-body sensory experiences. For example, PaperDude VR [Bolton et al. 2014] uses the Oculus Rift to implement a cycling exergame in order to motivate exercise. Other than using a consumer HMD like the Rift to provide the VR visuals, PaperDude VR includes a real bicycle on a stationary trainer in order to approximate a realistic experience using other non-visual senses. In another work, Birdly [Rheiner 2014] uses the HTC Vive3 as part of a system to simulate flying. In Birdly, the nonvisual elements are slightly harder to simulate, resulting in a rather complex robotic setup which includes sensory-motor coupling and a large fan to provide wind feedback.",0,0,1,full body+head mounted displays+stereoscopic display,
1321,1321,"Social fidelity in virtual agents impacts on presence and learning. Abstract Teaching and training are increasingly moving from real world venues to computerized environments, with human instructors often being replaced or joined by virtual pedagogical agents. While system fidelity and immersive properties of virtual learning environments are frequently discussed in the literature, less often addressed is the fidelity of the social components of teaching and their inclusion in pedagogical agent design. Teaching is inherently a social process, making the social fidelity of virtual agents a potential factor affecting learning outcomes. In this paper, we explore the concept of social fidelity as it pertains to the teaching effectiveness of pedagogical agents. We define the term, distinguish two subcategories, and discuss representative examples of research in these domains. Promising avenues for improving learning outcomes with social fidelity include personalized language, politeness, personality, attention, feedback, social memory, and gestures. Key conclusions are that: Social fidelity is important to learning in non-social domains, tradeoffs exist when implementing certain forms of social fidelity, individual user differences need to be more widely considered, and more focused studies are needed to compare different levels of social fidelity to uncover how they impact learning outcomes.",0,0,1,teaching effectiveness+learning environments+virtual learning environments,
1322,1322,"Assessing topic representations for gist forming. As topic modeling has grown in popularity, tools for visualizing the process have become increasingly common. Though these tools support a variety of different tasks, they generally have a view or module that conveys the contents of an individual topic. These views support the important task of gist-forming: helping the user build a cohesive overall sense of the topic's semantic content that can be generalized outside the specific subset of words that are shown. There are a number of factors that affect these views, including the visual encoding used, the number of topic words included, and the quality of the topics themselves. To our knowledge, there has been no formal evaluation comparing the ways in which these factors might change users' interpretations. In a series of crowdsourced experiments, we sought to compare features of visual topic representations in their suitability for gist-forming. We found that gist-forming ability is remarkably resistant to changes in visual representation, though it deteriorates with topics of lower quality.",0,0,1,visual representations+visual informations+multimedia contents,
1323,1323,"Collaborative information seeking consolidating the past creating the future. The notion that information seeking is not always a solitary activity, and that people working in collaboration for information intensive tasks should be studied and supported, has become more prevalent in the recent years than ever before. The field of collaborative information seeking (CIS) is re-emerging, and bringing many researchers and practitioners from various disciplines. This workshop is an effort to gather a small and motivated set of such participants. The workshop will incorporate discussions on theoretical foundations of CIS as well as its applications. It will bring together researchers from both academia and industry, working in the fields of CSCW, CSCL, IR, HCI, and PIM to share their ideas, questions, and opinions on how theories and practices from different domains can be brought together to create a strong and rich path ahead for collaborative information seeking/retrieval/searching as well as collective information synthesis and sense-making.",0,0,1,knowledge building+sense making+cscw,
1324,1324,"Getting real a naturalistic methodology for using smartphones to collect mediated communications. This paper contributes an intentionally naturalistic methodology using smartphone logging technology to study communications in the wild. Smartphone logging can provide tremendous access to communications data from real environments. However, researchers must consider how it is employed to preserve naturalistic behaviors. Nine considerations are presented to this end. We also provide a description of a naturalistic logging approach that has been applied successfully to collecting mediated communications from iPhones. The methodology was designed to intentionally decrease reactivity and resulted in data that were more accurate than self-reports. Example analyses are also provided to show how data collected can be analyzed to establish empirical patterns and identify user differences. Smartphone logging technologies offer flexible capabilities to enhance access to real communications data, but methodologies employing these techniques must be designed appropriately to avoid provoking naturally occurring behaviors. Functionally, this methodology can be applied to establish empirical patterns and test specific hypotheses within the field of HCI research. Topically, this methodology can be applied to domains interested in understanding mediated communications such as mobile content and systems design, teamwork, and social networks.",0,0,1,wireless communications+smart phones+systems design,
1325,1325,"Blueview a perception assistant system for the visually impaired. In this paper we present a perception assistant system named BlueView. Aim of the system is to assist visually impaired people in improving their perception of points of interest (POIs) in the nearby surrounding. The system allows users to perceive POIs, and accurately locate them with an audio prompting approach. BlueView contains two components: Viewer device and Beacon point. Viewer device is a Bluetooth-enabled mobile phone. Beacon point is a Bluetooth tag with a speaker. Using a within-subject design, six participants (i.e. blind people) were involved in the experiment with the system. Preliminary results suggest that BlueView effectively assist users in perceiving and locating POIs in both single and multi user scenarios.",0,0,1,visually impaired people+visually impaired+blind people,
1326,1326,"The perverse effects of social transparency on online advice taking. Increasingly, the advice people receive on the Internet is socially transparent in the sense that it displays contextual information about the advice-givers or their actions. We hypothesize that activity transparency -seeing an advice giver's process while creating his or her recommendations - will increase advice taking. We report three experiments testing the effect of activity transparency on taking mediocre advice. We found that the presence of a web history increased the likelihood of following a financial advisor's advice and reduced participant earnings (Exp. 1), especially when the web history implied greater task focus (Exp. 2, 3). CSCW research usually emphasizes how to increase information sharing; this work suggests when shared information may be inappropriate. We suggest ways to counter activity transparency's potential downsides.",0,0,1,information sharing+world wide web+cscw,
1327,1327,"Analysis of cache behavior and performance of different bvh memory layouts for tracing incoherent rays. With CPUs moving towards many-core architectures and GPUs becoming more general purpose architectures, path tracing can now be well parallelized on commodity hardware. While parallelization is trivial in theory, properties of real hardware make efficient parallelization difficult, especially when tracing incoherent rays. We investigate how different bounding volume hierarchy (BVH) and node memory layouts as well as storing the BVH in different memory areas impacts the ray tracing performance of a GPU path tracer. We optimize the BVH layout using information gathered in a pre-processing pass applying a number of different BVH reordering techniques. Depending on the memory area and scene complexity, we achieve moderate speedups.",0,0,1,path tracing+ray tracing+bounding volume,
1328,1328,"Trigger shift participatory design of an augmented theatrical performance with young people. Trigger Shift was a project that involved collaborating with a group of young people to explore the ways commercially available technologies could be appropriated into performance art. The project led to the production of an augmented theatrical performance using the Microsoft Kinect sensor that was presented to live audiences six times over two days. In this paper we describe the bottom-up, 11-month long participatory design process conducted with our young participants. We describe the manner in which the project was introduced to our participants and the techniques used to help them actively make decisions about the design of and role of technology in the final performance. We candidly report on the problems encountered during the design process and how the project team had to be reflexive to the needs of participants and the single predefined end-goal of the project. A number of strengths and weaknesses of bottom-up participatory design with young people are highlighted, and we reflect upon these to provide guidance for future researchers undertaking work in this domain.",0,0,1,microsoft kinect+systems design+user centered designs,
1329,1329,"Super resolution capacitive touchscreens. Capacitive touchscreens are near-ubiquitous in today’s touch-driven devices, such as smartphones and tablets. By using rows and columns of electrodes, specialized touch controllers are able to capture a 2D image of capacitance at the surface of a screen. For over a decade, capacitive “pixels” have been around 4 millimeters in size – a surprisingly low resolution that precludes a wide range of interesting applications. In this paper, we show how super-resolution techniques, long used in fields such as biology and astronomy, can be applied to capacitive touchscreen data. By integrating data from many frames, our software-only process is able to resolve geometric details finer than the original sensor resolution. This opens the door to passive tangibles with higher-density fiducials and also recognition of every-day metal objects, such as keys and coins. We built several applications to illustrate the potential of our approach and report the findings of a multipart evaluation.",0,0,1,image resolution+sensors+smart phones,
1330,1330,"Effects of a standing and three dynamic workstations on computer task performance and cognitive function tests. Sedentary work entails health risks. Dynamic (or active) workstations, at which computer tasks can be combined with physical activity, may reduce the risks of sedentary behaviour. The aim of this study was to evaluate short term task performance while working on three dynamic workstations: a treadmill, an elliptical trainer, a bicycle ergometer and a conventional standing workstation. A standard sitting workstation served as control condition. Fifteen Dutch adults performed five standardised but common office tasks in an office-like laboratory setting. Both objective and perceived work performance were measured. With the exception of high precision mouse tasks, short term work performance was not affected by working on a dynamic or a standing workstation. The participant's perception of decreased performance might complicate the acceptance of dynamic workstations, although most participants indicate that they would use a dynamic workstation if available at the workplace.",0,0,1,cognitive process+perceived risk+computer workstations,
1331,1331,"Gamified performance assessment of collaborative problem solving skills. Abstract In this paper we introduce a game-based approach for Collaborative Problem Solving (CPS) Skills assessment and provide preliminary evidence from a validation pilot study. To date, educational assessments have focused more heavily on the concrete, and accessible aspects of CPS with a diminished representation of the social aspects of CPS. We addressed this issue through the integration of our CPS construct into the game-based assessment “Circuit Runner” in which participants interact with a virtual agent to solve a series of challenges in a first-person maze environment (von Davier, 2017). Circuit Runner provides an environment that allows for controlled interdependence between a user and a virtual agent that facilitates the demonstration of the broad range of cognitive and social skills required for effective CPS. Tasks are designed to incorporate telemetry-based (e.g., log file, clickstream, interaction-based) and item response data to provide a more comprehensive measure of CPS skills. Our study included 500 participants on Amazon Mechanical Turk, who completed Circuit Runner, pre- and post-game surveys, and a CPS situational judgment test (CPS-SJT). These elements, in conjunction with the game-play, allowed for an expanded exploration of CPS skills with different modalities and types of instruments. The findings support and extend efforts to provide a stronger theoretical and empirical foundation for insights regarding CPS as a skillset, as well as the design of scalable game-based CPS assessments.",0,0,1,virtual agent+social aspect+gameplay,
1332,1332,"The blue one to the left enabling expressive user interaction in a multimodal interface for object selection in virtual 3d environments. Interaction with virtual 3D environments comes with a host of challenges. For instance, because 3D objects tend to occlude one another, performing object selection by pointing gestures is problematic, and more so when there are many objects in the scene. In the real world we tend to use speech to clarify our intent, by referring to distinctive attributes of the object and/or its absolute or relative location in space. Multimodal interactive systems involving speech and gesture have generally relied on speech for commands and deictic gestures for indicating the target object. In this paper, we present a system which allows object references to be made using gestures and speech, and supports a variety of expressions inspired by real-world usage.",0,0,1,hand gesture+virtual spaces+multi-modal interfaces,
1333,1333,"Swish a shifting weight interface of simulated hydrodynamics for haptic perception of virtual fluid vessels. Current VR/AR systems are unable to reproduce the physical sensation of fluid vessels, due to the shifting nature of fluid motion. To this end, we introduce SWISH, an ungrounded mixed-reality interface, capable of affording the users a realistic haptic sensation of fluid behaviors in vessels. The chief mechanism behind SWISH is in the use of virtual reality tracking and motor actuation to actively relocate the center of gravity of a handheld vessel, emulating the moving center of gravity of a handheld vessel that contains fluid. In addition to solving challenges related to reliable and efficient motor actuation, our SWISH designs place an emphasis on reproducibility, scalability, and availability to the maker culture. Our virtual-to-physical coupling uses Nvidia Flex's Unity integration for virtual fluid dynamics with a 3D printed augmented vessel containing a motorized mechanical actuation system. To evaluate the effectiveness and perceptual efficacy of SWISH, we conduct a user study with 24 participants, 7 vessel actions, and 2 virtual fluid viscosities in a virtual reality environment. In all cases, the users on average reported that the SWISH bucket generates accurate tactile sensations for the fluid behavior. This opens the potential for multi-modal interactions with programmable fluids in virtual environments for chemistry education, worker training, and immersive entertainment.",0,0,1,virtual reality+mixed reality+ar system,
1334,1334,"Parameterizing behavior trees. This paper introduces and motivates the application of parameterization to behavior trees. As a framework, behavior trees are becoming more commonly used for agent controllers in interactive game environments. We describe a way by which behavior trees can be authored for acting upon functions with arguments, as opposed to being limited to nonparametric tasks. We expand upon this idea to provide a method by which a subtree itself can be encapsulated with an exposed parameter interface through a lookup node, which enables code reuse in a manner already exploited by object oriented programming languages. Parameterization also allows us to recast Smart Events (a mechanism for co-opting agents to perform a desired activity) as behavior trees that can act generically upon groups of typed agents. Finally, we introduce a tool called Topiary, which enables the graphically-oriented authoring of behavior trees with this functionality as part of a broader testbed for agent simulation.",0,0,1,software agents+programming languages+game environment,
1335,1335,"Effects of stereo and head tracking in 3d selection tasks. We report a 3D selection study comparing stereo and head-tracking with both mouse and pen pointing. Results indicate stereo was primarily beneficial to the pen mode, but slightly hindered mouse speed. Head tracking had fewer noticeable effects.",0,0,1,stereo video+tracking system+detection and tracking,
1336,1336,"Multi sensory media experiences. The way we experience the world is based on our five senses, which allow us unique and often surprising sensations of our environment. Interactive technologies are mainly stimulating our senses of vision and hearing, partly our sense of touch, and the sense of taste and smell are widely under-exploited. There is however a growing international interest of the film, video, and game industries in more immersive viewing and gaming experiences. In the 20th century there was a demand for a controllable way to describe colours that initiated intense research on the descriptions of colours and substantially contributed to advances in computer graphics, image processing, photography and cinematography. Similarly, the 21st century now demands an investigation of touch, taste, and smell as sensory interaction modalities to enhance media experiences.",0,0,1,gameplay+tactile sensation+gaming experiences,
1337,1337,"Bringing design to the privacy table broadening design in privacy by design through the lens of hci. In calls for privacy by design (PBD), regulators and privacy scholars have investigated the richness of the concept of ""privacy."" In contrast, ""design"" in HCI is comprised of rich and complex concepts and practices, but has received much less attention in the PBD context. Conducting a literature review of HCI publications discussing privacy and design, this paper articulates a set of dimensions along which design relates to privacy, including: the purpose of design, which actors do design work in these settings, and the envisioned beneficiaries of design work. We suggest new roles for HCI and design in PBD research and practice: utilizing values- and critically-oriented design approaches to foreground social values and help define privacy problem spaces. We argue such approaches, in addition to current ""design to solve privacy problems"" efforts, are essential to the full realization of PBD, while noting the politics involved when choosing design to address privacy.",0,0,1,individual privacy+systems design+human computer interaction,
1338,1338,"Online social support for young people does it recapitulate in person social support can it help. Abstract   As social media websites have grown in popularity, public concern about online victimization has grown as well; however, much less attention has focused on the possible beneficial effects of online social networks. If theory and research about in-person social networks pertain, then online social relationships may represent an important modern source of or vehicle for support. In a study of 231 undergraduates, three major findings emerged: (1) for people with weaker in-person social support, social media sites provide a source of social support that is less redundant of the social support they receive in person; (2) in ways that were not redundant of each other, both online and in-person social support were associated with lower levels of depression-related thoughts and feelings, and (3) the beneficial effects of online social support (like in-person social support) offset some of the adverse effects of peer victimization. The study suggests that augmenting social relations via strategic use of social media can enhance young people's social support systems in beneficial ways.",0,0,1,social relationships+social relations+social aspect,
1339,1339,"Towards automatic drawing animation using physics based evolution. We demonstrate a system to automatically animate hand-drawn characters. Starting with skeleton extraction, meshing and vertex skinning, our system simulates characters using a neural network in a physics-based environment. Using an evolutionary algorithm, it searches for networks that move characters far while keeping a good posture. We validated the system through a user study with 26 participants. For most drawings (60 %), they felt satisfied with the generated animation, and in 76% of cases, they wished to draw and animate additional characters. The participants reported mostly positive emotions after seeing the animations. Only a minority had feelings of strangeness or had negative emotions. This work demonstrates the possibility of creating an automated 2-D character animation system making little assumption on what is drawn. We believe that this work can enable more children to engage in creative play and explore their imagination.",0,0,1,network architecture+emotional expressions+drawing (graphics),
1340,1340,"An efficient fingerprint identification algorithm based on minutiae and invariant moment. While we are experiencing many advantages of digital technologies and products, the security issues are also attracting increasing concerns. The secure fingerprint identification has become one of the most important research topics because of these increasing concerns. It has promoted us to propose an efficient fingerprint identification algorithm based on minutiae and invariant moment in this paper. In the proposed algorithm, the raw fingerprint image is first enhanced by the short-time Fourier transform (STFT). After that, the fingerprint minutiae can be extracted, which thereafter are selected as the center of region of interest (ROI), according to morphological transformation. Finally, a metric called cosine similarity among invariant moments is utilized to judge the similarities between fingerprint objects in the procedure of identification. The proposed algorithm is not limited to the fingerprint image with the fingerprint core. Experimental results show that the proposed algorithm provides a better performance in terms of matching accuracy when compared with the related works. The average accuracy is up to 96.67%. Moreover, the use of invariant moment of the ROI can avoid the leakage of fingerprint information and improve the security level of fingerprint recognition. Therefore, the proposed scheme is potentially used in many applications, such as smart fingerprint lock, intelligent community management information system, and automation control of home appliances.",0,0,1,information systems+management information systems+home appliances,
1341,1341,"Automatic exam grading by a mobile camera snap a picture to grade your tests. Advances in mobile computing power have opened possibilities for processing intensive optical recognition and machine learning applications. Despite the widespread availability of mobiles phones and digital resources, the education industry has made few updates to time-consuming grading techniques: many teachers continue to grade tests by hand or use outdated technology. Our goal is to produce a seamless, efficient, and accessible mobile experience to automate grading across multiple formats. We present a solution that allows teachers to take a photo and receive a visualized result after our deep learning application recognizes handwriting patterns, grades test answers, and identifies names and IDs. As a result, we contribute an improved computational tool for teachers to quickly create and grade exams across various formats.",0,0,1,mobile terminal+mobile phones+mobile users,
1342,1342,"Evaluation in human computer interaction beyond lab studies. Many research contributions in human-computer interaction are based on user studies in the lab. However, lab studies are not always possible, and they may come with significant challenges and limitations. In this course, we take a broader look at different approaches to doing research. We present a set of evaluation methods and research contributions that do not rely on user studies in labs. The discussion focuses on research approaches, data collection methods, and tools that can be conducted without direct interaction between the researchers and the participants.",0,0,1,human computer interfaces+multi-modal interactions+human computer interaction,
1343,1343,"Local fairing with local inverse. Local fairing techniques are extensively used in the geometry processing of curves and surfaces. They also play an important role in the multiresolution shape editing and synthesis applications. However, due to the inter-dependency of the vertices after applying the current fairing techniques, their inverses are not local. Finding a local fairing operation with local inverse provides a well-defined relationship between the smooth vertices and the initial vertices. This paper introduces a new fairing operation for curves and surfaces that is smoothing and local but with a local inverse. In the curve domain, we find a class of banded smoothing matrices with banded inverses. Then, using the geometric interpretation of the corresponding local operation, this class is extended to surfaces. We discuss the advantages of using this new fairing operation in different applications. Also, the resulting operation is used to find novel subdivision schemes with well-defined reverse subdivisions.",0,0,1,geometry+b-spline surface+sharp features,
1344,1344,"Expertise level control strategies and robustness in future air traffic control decision aiding. The introduction of 4-D trajectory-based operations will require the development of new and more advanced “human-centered” decision support tools for future air traffic controllers. One approach to the design of human-centered decision aids is ecological interface design, which focuses on visualizing the boundaries of safe system performance rather than prescribing predetermined strategies or discrete solutions. Previous studies with ecological interfaces in the aviation domain revealed that humans sometimes opted for control actions close to these boundaries, giving rise to a general concern about the robustness of control actions. The goal of this study has been to empirically investigate how effectively an ecological interface for 4-D trajectory management, as developed in a previous study, supports the preservation of airspace robustness. For this purpose, a metric has been developed to evaluate both minimum and average sector-based and control-based robustness. Special attention was paid to quantifying and measuring the effect of expertise level on the robustness of human-generated control actions. Results of a human-in-the-loop experiment indicate that expert participants were most robust in their control actions, as compared with either skilled or novice participants. This result suggests that boundary-seeking control actions with ecological interfaces are mainly dependent on the level of expertise and the control strategies of the end user.",0,0,1,expert knowledge+switching control+air traffic controller,
1345,1345,"Conveying uncertainties using peripheral awareness displays in the context of automated driving. As a consequence of insufficient situation awareness and inappropriate trust, operators of highly automated driving systems may be unable to safely perform takeovers following system failures. The communication of system uncertainties has been shown to alleviate these issues by supporting trust calibration. However, the existing approaches rely on information presented in the instrument cluster and therefore require users to regularly shift their attention between road, uncertainty display, and non-driving related tasks. As a result, these displays have the potential to increase workload and the likelihood of missed signals. A driving simulator study was conducted to compare a digital uncertainty display located in the instrument cluster with a peripheral awareness display consisting of a light strip and vibro-tactile seat feedback. The results indicate that the latter display affords users flexibility to direct more attention towards the road prior to critical situations and leads to lower workload scores while improving takeover performance.",0,0,1,tactile display+vibrotactile+display system,
1346,1346,"Driving destination measures in older adult drivers with differing health statuses. Older adult drivers experience age-related health declines that can negatively impact driving performance and lead to driver license revocation. The investigation and measurement of naturalistic in-car driving behavior can be used to evaluate driving behaviors, including aspects of route planning like driving destinations, for health-related changes. This study examined GPS-based measures related to driving destinations for older adult drivers with differing health statuses. Driving destination measures related to frequency of travel to destinations and distance from home were investigated in a convenience sample of five older adult drivers with better physical and cognitive health and five older adults with poorer physical and cognitive health. All drivers had at least two years of driving data. Drivers with worse health status had higher frequency of travel to unique destinations and reduced frequency of travel to destinations greater than 20 km from home compared to drivers with better health status. Drivers with worse health status may have driven more frequently to the same destinations due to inefficiency in trip planning, intentional trip simplification, forgetfulness, or increased travel to medical-related destinations. Similarly, drivers with worse health status may have limited their driving to destinations greater than 20 km from home due to physical limitations or increased complexity of route and trip planning. Further investigation is warranted to examine longitudinal health status changes, particularly cognitive changes, trip chaining, and route characteristics.",0,0,1,routing scheme+destination nodes+multiple routes,
1347,1347,"Towards standardized processes for physical therapists to quantify patient rehabilitation. Physical rehabilitation typically requires therapists to make judgements about patient movement and functional improvement using subjective observation. This process makes it challenging to quantitatively track, compute and predict long-term patient improvement. We therefore propose a novel methodical approach to the standardized and interpretable quantification of patient movement during rehabilitation. We describe the expert-led development of a movement assessment rubric and an accompanying quantitative rating system. We present our movement capture and annotation computational tools designed to implement the rubric and assist therapists in the quantitative documentation and assessment of rehabilitation. We describe results from a movement capture study of the tool with nine stroke survivors and a movement rating study with four therapists. Findings from these studies highlight potential optimal methodical process paths for individuals engaged in capturing, understanding and predicting human movement performance.",0,0,1,recommendation+user rating+subjective quality assessments,
1348,1348,"Stress is in the eye of the beholder. Despite a long history and a large volume of affective research, measuring affective states is still a non-trivial task that is complicated by numerous conceptual and methodological decisions that the researcher has to make. We suggest that inconsistent results reported in some areas of research can be partially explained by the choice of measurements that capture different manifestations of affective phenomena, or focus on different elements of affective processes. In the present study we examine one of such topics — a relationship between stress and individual's work role. In a 2-week, multi-method in situ study we collected affective information from 40 subjects. All participants provided continuous physiological (cardiovascular) data for the entire duration of the study, submitted multiple daily self-reports of momentary affect, and filled out a onetime assessment of the global perceived stress. We found that individuals' job role (specifically, decision-making workload) was not related to the cumulative measures of momentary affect, but was negatively correlated with the overall level of perceived stress. We further found that this negative relationship was partially mediated by individuals' coping behaviors. Our results emphasize the important difference between fleeting and global (appraised) affective states, and remind about intervening variables that can significantly modify affective processes. We suggest directions for future research and discuss practical applications for stress management.",0,0,1,emotional expressions+emotion expression+affective state,
1349,1349,"Physical versus digital sticky notes in collaborative ideation. In this paper, we compare the use of physical and digital sticky notes in collaborative ideation. Inspired by a case study in a design company, we focus on a collaborative ideation task, which is often part of pair-wise brainstorming in design. For comparison and to focus on the different materiality, we developed a digital sticky notes setup designed to be as close to the physical setup as possible, not adding any advanced digital features, even though technology has reached a stage where more sophisticated use of digital sticky notes on digital boards is possible. In this paper, we present a study of ideation among pairs of experienced sticky note users. The ideation sessions were video recorded and analyzed to focus on how collaboration is supported across the two setups. Based on quantitative analyses of the participants’ interactions with the artefacts, talking patterns, position and attention during the sessions, we qualify how the differences and similarities between the 2 setups have an impact on note handling, ideation techniques, group dynamics and socio-spatial configuration, e.g. the use of the room, the boards and tables. We conclude that, while the physical setup seems more appropriate for creating notes and posting notes, the digital setup invites more note interaction. Nevertheless, we did not find significant differences in the ideation outcome (e.g., number of notes created) or how participants collaborated between the 2 setups. Hence, we argue that collaborative ideation can successfully be supported in a digital setup as well. Consequently, we believe that the next step in a technological setup is not an either or, but should bring the best of the two worlds together.",0,0,1,collaborative work+sense making+interaction design,
1350,1350,"Online social support for danqin mama a case study of parenting discussion forum for unwed single mothers in china. Abstract Voice of Chinese unwed single mothers remains unheard in public discourse in China. Since 2001, the government has enforced punishment and penalties on childbirth out of wedlock. Isolated socially offline, many turn to parenting forums as a source for social support. This study examined the role of online social support in the social and well-being of unwed single mothers in China. In particular, the largest online parenting forum that caters to unwed single mother was examined. After filtering out causal talks and other unrelated conversations, 578 initial unwed motherhood related post was narrowed to a total of 76 threads (which included 4892 messages). Findings reveal that all three types of social support: information, emotion, and tangible support were present and exchanged among the members of the discussion board. The findings highlight a positive impact of online social support on the overall wellbeing of this group. The presence and exchange of social support not only enhanced self-esteem and promoted individual empowerment, but also raised group consciousness and created a bond and a sense of belonging to an online community.",0,0,1,social aspect+tangible interfaces+tangible user interfaces,
1351,1351,"Towards multi device context aware systems for elders well being. An increasing number of devices are getting enough computing and storage capacity to adapt their behaviour to the needs and preferences of their users. However, in multi device systems, this will require new techniques allowing several devices to take the contextual information of their users into account to adapt or coordinate themselves so they can improve the well-being of their users. This paper presents the Liquid Context and the Situational Context concepts. Both concepts are focused on facilitating the migration of the virtual profiles among different devices and on the computation of the contextual information of several entities in order to coordinate the surrounding devices. The main goal of this coordination is the improvement of the elders well-being by a better adaptation of the surrounding technology to their needs and preferences.",0,0,1,web content+context-aware computing+mobile users,
1352,1352,"Green information technology influence on car owners behavior. 95% of respondents unknown regulations to define if a car is eligible to circulate.Respondents showed disposition to use social networking to reduce pollutants emission.Level of awareness with environment increased with the use of mobile application.The subjective norms-social influence had the higher impact in the adoption of GIT.GIT adoption affect user's behavior if information shown has enough clearness. The study of the influence of the Green Information Technology (GIT), using their potential to reduce the negative impact of the exhaust emission produced by light-duty vehicles, attending verification compliance to circulate in urban areas, is the main topic of the present contribution. Data collected through surveys answered by car owners in Mexico City, place the attention over the knowledge of the official exhaust emission test, their general acceptance, and the understanding of the exhaust test context, once they have received information from the proposed GIT (pGIT). The surveys consider the behavioral intention, attitude toward behavior, perceived usefulness, social influence and volitional control constructs to estimate response of car owners to the concerns about the pollutant emission and their willingness to change their position toward the pollutant emission reduction. The results show the interest of car owners to participate in a collaborative process, and develop positive attitude to cooperate in a social dilemma by considering the use of social networking.",0,0,1,information technology+mobile applications+social influence,
1353,1353,"A survey of the prevalence of fatigue its precursors and individual coping mechanisms among u s manufacturing workers. Abstract   Advanced manufacturing has resulted in significant changes on the shop-floor, influencing work demands and the working environment. The corresponding safety-related effects, including fatigue, have not been captured on an industry-wide scale. This paper presents results of a survey of U.S. manufacturing workers for the: prevalence of fatigue, its root causes and significant factors, and adopted individual fatigue coping methods. The responses from 451 manufacturing employees were analyzed using descriptive data analysis, bivariate analysis and Market Basket Analysis. 57.9% of respondents indicated that they were somewhat fatigued during the past week. They reported the ankles/feet, lower back and eyes were frequently affected body parts and a lack of sleep, work stress and shift schedule were top selected root causes for fatigue. In order to respond to fatigue when it is present, respondents reported coping by drinking caffeinated drinks, stretching/doing exercises and talking with coworkers. Frequent combinations of fatigue causes and individual coping methods were identified. These results may inform the design of fatigue monitoring and mitigation strategies and future research related to fatigue development.",0,0,1,flexible manufacturing systems+correlation analysis+office workers,
1354,1354,"Naturalistic affective expression classification by a multi stage approach based on hidden markov models. In naturalistic behaviour, the affective states of a person change at a rate much slower than the typical rate at which video or audio is recorded (e.g. 25fps for video). Hence, there is a high probability that consecutive recorded instants of expressions represent a same affective content. In this paper, a multi-stage automatic affective expression recognition system is proposed which uses Hidden Markov Models (HMMs) to take into account this temporal relationship and finalize the classification process. The hidden states of the HMMs are associated with the levels of affective dimensions to convert the classification problem into a best path finding problem in HMM. The system was tested on the audio data of the Audio/Visual Emotion Challenge (AVEC) datasets showing performance significantly above that of a one-stage classification system that does not take into account the temporal relationship, as well as above the baseline set provided by this Challenge. Due to the generality of the approach, this system could be applied to other types of affective modalities.",0,0,1,hidden markov model (hmm)+expression recognition+affective state,
1355,1355,"Probabilistic intentionality prediction for target selection based on partial cursor tracks. Pointing tasks, for example to select an object in an interface, constitute a significant part of human-computer interactions. This motivated several studies into techniques that facilitate the pointing task and improve its accuracy. In this paper, we introduce a number of intentionality prediction algorithms to determine the intended target a priori from partial cursor tracks. They yield notable reductions in the pointing time, aid effective selection assistance routines and enhance the overall pointing accuracy. A number of benchmark prediction models are also restated within a statistical framework and their probabilistic interpretation is utilised to calculate their corresponding outcomes. The relative performance of all considered predictors is assessed for point-click task data sets pertaining to both able-bodied and impaired users. Bayesian adaptive filtering is deployed to smooth highly perturbed mouse cursor tracks that are typically produced by motor impaired users undertaking a pointing task.",0,0,1,correlation analysis+adaptive filtering+human computer interaction,
1356,1356,"Lightweight binocular facial performance capture under uncontrolled lighting. Recent progress in passive facial performance capture has shown impressively detailed results on highly articulated motion. However, most methods rely on complex multi-camera set-ups, controlled lighting or fiducial markers. This prevents them from being used in general environments, outdoor scenes, during live action on a film set, or by freelance animators and everyday users who want to capture their digital selves. In this paper, we therefore propose a lightweight passive facial performance capture approach that is able to reconstruct high-quality dynamic facial geometry from only a single pair of stereo cameras. Our method succeeds under uncontrolled and time-varying lighting, and also in outdoor scenes. Our approach builds upon and extends recent image-based scene flow computation, lighting estimation and shading-based refinement algorithms. It integrates them into a pipeline that is specifically tailored towards facial performance reconstruction from challenging binocular footage under uncontrolled lighting. In an experimental evaluation, the strong capabilities of our method become explicit: We achieve detailed and spatio-temporally coherent results for expressive facial motion in both indoor and outdoor scenes -- even from low quality input images recorded with a hand-held consumer stereo camera. We believe that our approach is the first to capture facial performances of such high quality from a single stereo rig and we demonstrate that it brings facial performance capture out of the studio, into the wild, and within the reach of everybody.",0,0,1,facial expression+binoculars+handheld,
1357,1357,"Using an artificial agent as a behavior model to promote assistive technology acceptance. Despite technological advancements in assistive technologies, studies show high rates of non-use. Because of the rising numbers of people with disabilities, it is important to develop strategies to increase assistive technology acceptance. The current research investigated the use of an artificial agent (embedded into a system) as a persuasive behavior model to influence individuals’ technology acceptance beliefs. Specifically, we examined the effect of agent-delivered behavior modeling vs. two non-modeling instructional methods (agent-delivered instructional narration and no agent, text-only instruction) on individuals’ computer self-efficacy and perceived ease of use of an assistive technology. Overall, the results of the study confirmed our hypotheses, showing that the use of an artificial agent as a behavioral model leads to increased computer self-efficacy and perceived ease of use of a system. The implications for the inclusion of an artificial agent as a model in promoting technology acceptance are discussed.",0,0,1,assistive devices+assistive technology+disabilities,
1358,1358,"Exploring proprioceptive take over requests for highly automated vehicles. The uprising levels of autonomous vehicles allow the drivers to shift their attention to non-driving tasks while driving (i.e., texting, reading, or watching movies). However, these systems are prone to failure and, thus, depending on human intervention becomes crucial in critical situations. In this work, we propose using human actuation as a new mean of communicating take-over requests (TOR) through proprioception. We conducted a user study via a driving simulation in the presence of a complex working memory span task. We communicated TORs through four different modalities, namely, vibrotactile, audio, visual, and proprioception. Our results show that the vibrotactile condition yielded the fastest reaction time followed by proprioception. Additionally, proprioceptive cues resulted in the second best performance of the non-driving task following auditory cues.",0,0,1,vehicles+tactile display+vibrotactile,
1359,1359,"Cross domain developer recommendation algorithm based on feature matching. In recent years, the software crowdsourcing has become a new software development pattern. More and more developers choose to publish, search for software tasks, and solve software problems on software crowdsourcing platform. As such, the platform generates a large amount of developer and development task information every day, which makes it difficult for developers to find appropriate tasks from massive tasks. Therefore, it is significant to deploy developer recommendation system on crowdsourcing platforms. Now, most developer recommendation algorithms can only use single platform data. Since the new software crowdsourcing platforms do not have enough historical behavior information of developers, previous developer recommendation algorithms cannot recommend developers to new tasks effectively. To solve the sparsity problem, this paper proposes a cross-domain developer recommendation algorithm based on feature matching. Firstly, we seek from the auxiliary domain for the most similar tasks to the current target domain task. Then, we retrieved the corresponding developers of these tasks. Finally, we select from the target domain the most similar developer to the developers retrieved to compose the recommendation developer set of the current task. In order to verify the effectiveness of the proposed algorithm, we crawls data from two different software crowdsourcing platforms to conduct experiments and compare the proposed model with various advanced developer recommendation algorithms. The experimental results show that the proposed algorithm has advantages over the previous algorithms on different evaluation metrics.",0,0,1,recommendation systems+recommendation algorithms+recommendation,
1360,1360,"Monitoring simulated worlds in indigenous strategy games. Though all video games require the player to observe the game state, the strategy genre relies on an experience of managing rule-based simulations that model real-world material systems. Designing for this experience produces a mode of interactive vision that structures gameplay as management: gamic monitoring. This article aims to develop a theory of gamic monitoring and explore its features through settler and Indigenous strategy, simulation, and resource management games. Games scholarship has yet to fully account for recent developments in Indigenous video games or how they relate to mainstream genres. Four comparative examples demonstrate how Indigenous games speak to settler-style gameplay, particularly its dynamics of monitoring and managing populations and resources. Due to their divergent frameworks for action, Indigenous strategy games intervene in mainstream genre conventions by shifting informatic play toward relational procedures of observation and decision-making. They express a paradigm of reciprocal interaction through how they mediate and critique codified game systems. Because Indigenous strategy games reconfigure resources and political engagement according to distinct models of managing the game state, they remain useful for further research in developing alternate models of strategy gameplay.",0,0,1,computer games+videogames+gameplay,
1361,1361,"Using a team of general ai algorithms to assist game design and testing. General Video Game Playing (GVGP) has become a popular line of research in the past years, leading to the existence of a wide range of general algorithms created to tackle this challenge. This paper proposes taking advantage of this research to help in game design and testing processes. It introduces a methodology consisting of using a team of Artificial General Intelligence agents with differentiated goals (winning, exploring, collecting items, killing NPCs, etc.) and skill levels. Using several agents with distinct behaviours that play the same game simultaneously can provide substantial information to influence design and bug fixing. Two methods are proposed to aid game design: 1) the evaluation of a game based on the expected performance in the behaviour of each of the agents, and 2) the provision of visual information to analyse how the experience of the agents evolves during the play-through. Having this methodology available to designers can help them decide if the game or level under analysis fits the initial expectations. Including a Logging System can also be used to detect anomalies while the development is still at an early stage. We believe this approach allows the flexibility and portability to be easily applied to games with different characteristics.",0,0,1,videogames+gameplay+game design,
1362,1362,"Typeinbraille a braille based typing application for touchscreen devices. Smartphones provide new exciting opportunities to visually impaired users because these devices can support new assistive technologies that cannot be deployed on desktops or laptops. Some devices, like the iPhone, are rapidly gaining popularity among the visually impaired since the use of pre-installed screenreader applications renders these devices accessible. However, there are still some operations that require a longer time or higher mental workload to be completed by a visually impaired user. In this contribution we present a novel application for text entry, called TypeInBraille, that is based on the Braille code and hence is specifically designed for blind users.",0,0,1,visually-impaired users+blind users+text entry,
1363,1363,"Who s the fairest of them all device mirroring for the connected home. In the UK alone smartphone adoption has reached 61% in 2014. In home and living-room contexts, this adoption has led to ""multi-screening"", meaning the concurrent use of devices such as smartphones and tablets alongside the TV. The resultant private ""digital bubble"" [12] of this device usage has been discussed as raising a problematic barrier to socialization and interaction, with mobile phone use in particular having significant anti-social connotations [24]. However mobile devices have evolved new capabilities for sharing their activity, most notably through screen mirroring. This paper explores how we can utilize the TV to view screen-mirrored device activity, decreasing the digital isolation of device usage. We examine the extent to which users can attend to multiple devices on one TV, the effect this and prior systems have had on existing TV viewing, and propose ways in which we can aid users to manage their viewing of device activity on the TV. Moreover, we examine new approaches toward the accessibility of device activity, investigating systems which allow users to attend to whichever device activity they wish using multi-view displays, and discuss the social and privacy implications of having ""always-on"" screen-mirrored devices.",0,0,1,smart phones+cell phone+mobile phones,
1364,1364,"Construct validity of the family impact of assistive technology scale for augmentative and alternative communication. The Family Impact of Assistive Technology Scale for Augmentative and Alternative Communication (FIATS-AAC) measures parent-reported functioning and factors that affect functioning in children who use AAC. The aim of the current study was to assess its construct validity. For the study, 47 parents of children with AAC needs completed the FIATS-AAC and two other parent-reported questionnaires: a child quality-of-life measure and a community participation measure. An interview was also conducted with a sub-set of six parents. The FIATS-AAC showed a significant correlation with the child quality-of-life measure, but no significant associations with the community participation measure. Interviews suggested some consistency between parents' perceptions of their child's communicative functioning after comparing their FIATS-AAC scores and interview responses. This study provides evidence for the emerging construct validity of the FIATS-AAC as a measure linked to psychosocial aspects of quality-of-life in children with AAC needs between the ages of 6- and 12-years.",0,0,1,assistive devices+assistive technology+aac,
1365,1365,"Improving the safety of homeless young people with mobile phones values form and function. By their pervasiveness and by being worn on our bodies, mobile phones seem to have become intrinsic to safety. To examine this proposition, 43 participants, from four stakeholder groups (homeless young people, service providers, police officers, and community members), were asked to consider how homeless young people could use mobile phones to keep safe. Participants were asked to express their knowledge for place-based safety and to envision how mobile phones might be used to improve safety. Detailed analysis of the resulting data, which included value sketches, written value scenarios, and semi-structured discussion, led to specific design opportunities, related to values (e.g., supporting trust and desire to help others), function (e.g., documenting harms for future purposes), and form (e.g., leveraging social expectations for how mobile phones can be used to influence behavior). Together, these findings bound a design space for how mobile phones can be used to manage unsafe situations.",0,0,1,cellular phone+cell phone+mobile phones,
1366,1366,"Medical image and video quality assessment in e health applications and services. Healthcare services are increasingly using medical images and video in their applications. Quality assessment can be performed via image quality evaluation metrics. However, due to the specific nature of the associated data, quality evaluation of medical images poses several issues. In this paper, we provide an insight into medical image quality evaluation, by discussing the issues faced and the current trends in the literature. Based on the analysis of the medical and technical literature, three broad categories of quality evaluation metrics are presented. The different methodologies are compared in different environments and services and recommendations for future research are provided.",0,0,1,color images+image quality+video quality,
1367,1367,"The effects of pay to quit incentives on crowdworker task quality. Companies such as Zappos.com and Amazon.com provide financial incentives for newer employees to quit. The premise is that workers who will accept this offer are misaligned with their company culture, which will therefore negatively affect quality over time. Could this pay-to-quit incentive scheme align workers in online labor markets? We conduct five empirical experiments evaluating different pay-to-quit incentives with crowdworkers and evaluate their effects on mean task accuracy, retention rate, and improvement in mean task accuracy. We find that the number of times a user is prompted for the inducement, the type and frequency of performance feedback given to participants, the type of incentive, as well as the amount offered can help retain high-performing workers but encourage poor-performing workers to quit early. When we combine the best features from our experiments and examine their aggregate effectiveness, mean task accuracy is improved by 28.3%. Last, we also find that certain demographics contribute to the effectiveness of pay-to-quit incentives.",0,0,1,business models+incentive mechanism+incentive schemes,
1368,1368,"Oh i love trash personality of a robotic trash barrel. This demonstration merges two of our prior research foci: 1) exploring how robotic, expressive everyday objects-such as a trash barrel or a sofa-can and should interact with, and support, people during their daily activities, and 2) how imbue a personality into these objects, expressed through movement alone, to make their goals and intentions clear, understandable and agreeable.",0,0,1,autonomous robot+demonstrations+teleoperated,
1369,1369,"Modbot a tangible and modular making toolkit for children to create underwater robots. Underwater robot is essential equipment for exploring the marine environment. It is important that children get exposed to these technologies as earlier as possible, especially there is a high demand for developing expertise and awareness in the underwater robot. Although examples of making toolkit for children currently exist, few focus specifically on integration with the water environment. In this paper, we explore the making toolkit, ModBot, which can be applied to the water environment. The hardware was developed using electronic, counterweight, and shape modules that can be manipulated to build underwater robots. The software application allows children to learn concepts and receive construction feedback. This paper presents the system design of ModBot, the design rationale, and a user study for the usability of ModBot. Our system is expected to spark children's interests and creativity of underwater robots, and foster their understanding of the water environment.",0,0,1,computer hardware+design rationale+systems design,
1370,1370,"A multimodal emotion sensing platform for building emotion aware applications. Humans use a host of signals to infer the emotional state of others. In general, computer systems that leverage signals from multiple modalities will be more robust and accurate in the same task. We present a multimodal affect and context sensing platform. The system is composed of video, audio and application analysis pipelines that leverage ubiquitous sensors (camera and microphone) to log and broadcast emotion data in real-time. The platform is designed to enable easy prototyping of novel computer interfaces that sense, respond and adapt to human emotion. This paper describes the different audio, visual and application processing components and explains how the data is stored and/or broadcast for other applications to consume. We hope that this platform helps advance the state-of-the-art in affective computing by enabling development of novel human-computer interfaces.",0,0,1,affective computing+computer interfaces+human computer interfaces,
1371,1371,"Clockability for ordinal turing machines. We study clockability for Ordinal Turing Machines (OTMs). In particular, we show that, in contrast to the situation for ITTMs, admissible ordinals can be OTM-clockable, that \(\varSigma _{2}\)-admissible ordinals are never OTM-clockable and that gaps in the OTM-clockable ordinals are always started by admissible limits of admissible ordinals. This partially answers two questions in [3].",0,0,1,finite automata+turing machines+parallel machine,
1372,1372,"A system for rapid automatic shader level of detail. Level-of-detail (LOD) rendering is a key optimization used by modern video game engines to achieve high-quality rendering with fast performance. These LOD systems require simplified shaders, but generating simplified shaders remains largely a manual optimization task for game developers. Prior efforts to automate this process have taken hours to generate simplified shader candidates, making them impractical for use in modern shader authoring workflows for complex scenes. We present an end-to-end system for automatically generating a LOD policy for an input shader. The system operates on shaders used in both forward and deferred rendering pipelines, requires no additional semantic information beyond input shader source code, and in only seconds to minutes generates LOD policies (consisting of simplified shader, the desired LOD distance set, and transition generation) with performance and quality characteristics comparable to custom hand-authored solutions. Our design contributes new shader simplification transforms such as approximate common subexpression elimination and movement of GPU logic to parameter bind-time processing on the CPU, and it uses a greedy search algorithm that employs extensive caching and upfront collection of input shader statistics to rapidly identify simplified shaders with desirable performance-quality trade-offs.",0,0,1,caching+gpu+videogames,
1373,1373,"Failure to decrease the addictive usage of information technologies a theoretical model and empirical examination of smartphone game users. Abstract This paper aims to reveal why smartphone game addicts cannot decrease their addictive behaviors. Social cognitive theory serves as a framework for interpreting the failure of decreased behavior with a three-stage process. Following this perspective, key drivers in each process are proposed to explain the underlying reasons. A longitudinal survey with 381 valid responses was conducted. Results indicate that addicts with intention to decrease playing games may attempt to suppress their playing thought. Then, intrusive thought of playing games and impulsive playing emerge in the first stage. The ubiquity feature of smartphone strengthens the positive influence of intrusive thought on impulsive playing. In the second self-judgment stage, addicts perceive a higher level of difficulty to decrease, which further leads to deficient self-efficacy. These maladaptive judgments reduce the reaction of decreased behavior. This study is one of the first ones to reveal key factors causing decrease failure in the context of smartphone game addiction. Implications are offered.",0,0,1,cell phone+mobile phones+videogames,
1374,1374,"Intimacy in long distance relationships over video chat. Many couples live a portion of their lives in a long-distance relationship (LDR). This includes a large number of dating college students as well as couples who are geographically-separated because of situational demands such as work. We conducted interviews with individuals in LDRs to understand how they make use of video chat systems to maintain their relationships. In particular, we have investigated how couples use video to ""hang out"" together and engage in activities over extended periods of time. Our results show that regardless of the relationship situation, video chat affords a unique opportunity for couples to share presence over distance, which in turn provides intimacy. While beneficial, couples still face challenges in using video chat, including contextual (e.g., location of partners, time zones), technical (e.g., mobility, audio/video quality, networking), and personal (e.g., a lack of physicality needed by most for intimate sexual acts) challenges.",0,0,1,video streams+compressed video+video quality,
1375,1375,"Cheaper by the dozen group annotation of 3d data. This paper proposes a group annotation approach to interactive semantic labeling of data and demonstrates the idea in a system for labeling objects in 3D LiDAR scans of a city. In this approach, the system selects a group of objects, predicts a semantic label for it, and highlights it in an interactive display. In response, the user either confirms the predicted label, provides a different label, or indicates that no single label can be assigned to all objects in the group. This sequence of interactions repeats until a label has been confirmed for every object in the data set. The main advantage of this approach is that it provides faster interactive labeling rates than alternative approaches, especially in cases where all labels must be explicitly confirmed by a person. The main challenge is to provide an algorithm that selects groups with many objects all of the same label type arranged in patterns that are quick to recognize, which requires models for predicting object labels and for estimating times for people to recognize objects in groups. We address these challenges by defining an objective function that models the estimated time required to process all unlabeled objects and approximation algorithms to minimize it. Results of user studies suggest that group annotation can be used to label objects in LiDAR scans of cities significantly faster than one-by-one annotation with active learning.",0,0,1,adaptive algorithms+3d data+static objects,
1376,1376,"Enhancing supply chain outcomes through information technology and trust. Challenged by competitive pressures, and enabled by information technologies (IT), organizations are forming strategic partnerships that share, collaborate, and make collective decisions across the supply chain (SC). To study this phenomenon, researchers have focused on one or some of these five salient factors: Information Technology Integration (IT), Inter-organizational Trust (TR), Relational Governance (RG), Transaction Cost (TC), and Supply Chain Performance (PE). In this study, we develop a research model that includes all these five factors by synthesizing and integrating theoretical perspectives: transaction cost economics, and relational governance. Our theoretical model clarifies the intricate relationships between the five factors by positioning two common resources for the supply chain: Inter-organizational Trust and IT, as the independent variables that influence outcome measures: performance, and reduction in transaction costs. Relational governance, which facilitates joint decision making, is theorized as playing a central role between the resources and the outcome measures. Data collected from 167 purchasing and supply chain managers provides strong support to our research model. Our findings should help researchers as well as practitioners to develop a more complete and transparent understanding of the relevant mechanisms with which the partnership resources, exert their beneficial effects on supply chain outcomes. We examine role of IT, trust, and relational governance, in supply chains.Results suggest that IT and trust influence transaction costs and performance.Relational governance plays a mediating role.Findings provide strategic guidelines to supply chain managers.",0,0,1,economics+purchasing+information technology,
1377,1377,"Factors of collaborative working a framework for a collaboration model. Abstract   The ability of organisations to support collaborative working environments is of increasing importance as they move towards more distributed ways of working. Despite the attention collaboration has received from a number of disparate fields, there is a lack of a unified understanding of the component factors of collaboration. As part of our work on a European Integrated Project, CoSpaces, collaboration and collaborative working and the factors which define it were examined through the literature and new empirical work with a number of partner user companies in the aerospace, automotive and construction sectors. This was to support development of a descriptive human factors model of collaboration – the CoSpaces Collaborative Working Model (CCWM). We identified seven main categories of factors involved in collaboration: Context, Support, Tasks, Interaction Processes, Teams, Individuals, and Overarching Factors, and summarised these in a framework which forms a basis for the model. We discuss supporting evidence for the factors which emerged from our fieldwork with user partners, and use of the model in activities such as collaboration readiness profiling.",0,0,1,collaborative work+human factors+cooperative works,
1378,1378,"Comicolorization semi automatic manga colorization. We developed Comicolorization, a semi-automatic colorization system for manga images. Given a monochrome manga and reference images as inputs, our system generates a plausible color version of the manga. This is the first work to address the colorization of an entire manga title (a set of manga pages). Our method colorizes a whole page (not a single panel) semi-automatically, with the same color for the same character across multiple panels. To colorize the target character by the color from the reference image, we extract a color feature from the reference and feed it to the colorization network to help the colorization. Our approach employs adversarial loss to encourage the effect of the color features. Optionally, our tool allows users to revise the colorization result interactively. By feeding the color features to our deep colorization network, we accomplish colorization of the entire manga using the desired colors for each panel.",0,0,1,color features+color images+color histogram,
1379,1379,"Mining social relationship types in an organization using communication patterns. Our goal is to show that it is possible to automatically infer social relationship types among people who stay together in an organization by analyzing communication patterns. We collected indoor co-location data and instant messenger data from 22 participants for one month. Based on the data, we designed and explored several indicators which are considered to be useful for mining social relationship types. We applied machine learning techniques using the indicators and found that it is possible to develop an intelligent method to infer social relationship types.",0,0,1,social relationships+social relations+social computing,
1380,1380,"Haptable an interactive tabletop providing online haptic feedback for touch gestures. We present HapTable; a multi–modal interactive tabletop that allows users to interact with digital images and objects through natural touch gestures, and receive visual and haptic feedback accordingly. In our system, hand pose is registered by an infrared camera and hand gestures are classified using a Support Vector Machine (SVM) classifier. To display a rich set of haptic effects for both static and dynamic gestures, we integrated electromechanical and electrostatic actuation techniques effectively on tabletop surface of HapTable, which is a surface capacitive touch screen. We attached four piezo patches to the edges of tabletop to display vibrotactile feedback for static gestures. For this purpose, the vibration response of the tabletop, in the form of frequency response functions (FRFs), was obtained by a laser Doppler vibrometer for 84 grid points on its surface. Using these FRFs, it is possible to display localized vibrotactile feedback on the surface for static gestures. For dynamic gestures, we utilize the electrostatic actuation technique to modulate the frictional forces between finger skin and tabletop surface by applying voltage to its conductive layer. To our knowledge, this hybrid haptic technology is one of a kind and has not been implemented or tested on a tabletop. It opens up new avenues for gesture–based haptic interaction not only on tabletop surfaces but also on touch surfaces used in mobile devices with potential applications in data visualization, user interfaces, games, entertainment, and education. Here, we present two examples of such applications, one for static and one for dynamic gestures, along with detailed user studies. In the first one, user detects the direction of a virtual flow, such as that of wind or water, by putting their hand on the tabletop surface and feeling a vibrotactile stimulus traveling underneath it. In the second example, user rotates a virtual knob on the tabletop surface to select an item from a menu while feeling the knob's detents and resistance to rotation in the form of frictional haptic feedback.",0,0,1,vibrotactile feedback+interactive tabletop+laser doppler vibrometry,
1381,1381,"Instrumented shirt to evaluate classical human movements. Among the smart domestic devices developed to provide healthy, elderly, sick or disabled people with a better life in their homes, there are the smart-walkers or the smart-shirt. The first ones permit the patient to move in their home or to do rehabilitation exercises, whereas the second ones monitor the patient's physical status. In this article, it is presented a smartshirt able to monitor the biomedical parameters and managing some alarms for a robot-walker. In particular, it is evaluated the inertial system of the smart-shirt consisting of an accelerometer. Some typical human movements have been tested. The obtained results permit to know the movements and the positions of a patient using the antro-posterior and medio-lateral angles calculated by the acceleration signals. In the future, this instrumented shirt will be used to indicate to the robot-walker different potential problems, such as a fall or a wrong position.",0,0,1,human motions+accelerometers+disabled people,
1382,1382,"Bare handed 3d drawing in augmented reality. Head-mounted augmented reality (AR) enables embodied in situ drawing in three dimensions (3D). We explore 3D drawing interactions based on uninstrumented, unencumbered (bare) hands that preserve the user's ability to freely navigate and interact with the physical environment. We derive three alternative interaction techniques supporting bare-handed drawing in AR from the literature and by analysing several envisaged use cases. The three interaction techniques are evaluated in a controlled user study examining three distinct drawing tasks: planar drawing, path description, and 3D object reconstruction. The results indicate that continuous freehand drawing supports faster line creation than the control point based alternatives, although with reduced accuracy. User preferences for the different techniques are mixed and vary considerably between the different tasks, highlighting the value of diverse and flexible interactions. The combined effectiveness of these three drawing techniques is illustrated in an example application of 3D AR drawing.",0,0,1,user preferences+augmented reality+interaction techniques,
1383,1383,"Digital reading support for the blind by multimodal interaction. Slate-type devices allow Individuals with Blindness or Severe Visual Impairment (IBSVI) to read in place with the touch of their fingertip by audio-rendering the words they touch. Such technologies are helpful for spatial cognition while reading. However, users have to move their fingers slowly or they may lose place on screen. Also, IBSVI may wander between lines without realizing they did. In this paper, we address these two interaction problems by introducing dynamic speech-touch interaction model, and intelligent reading support system. With this model, the speed of the speech will dynamically change coping up with the user's finger speed. The proposed model is composed of: 1- Audio Dynamics Model, and 2- Off-line Speech Synthesis Technique. The intelligent reading support system predicts the direction of reading, corrects the reading word if the user drifts, and notifies the user using a sonic gutter to help her from straying off the reading line. We tested the new audio dynamics model, the sonic gutter, and the reading support model in two user studies. The participants' feedback helped us fine-tune the parameters of the two models. Finally, we ran an evaluation study where the reading support system is compared to other VoiceOver technologies. The results showed preponderance to the reading support system with its audio dynamics and intelligent reading support components.",0,0,1,synthetic speech+multi-modal interactions+visually impaired,
1384,1384,"Force gestures augmented touch screen gestures using normal and tangential force. Similar sliding gestures may have different meanings when they are performed with changing intensity. Touch screens, however, fail to properly distinguish those intensities due to their inability to sense variable pressures. Enabled by distinguishing normal and tangential forces, we explore new possibilities for gestures on a touch screen. We have implemented a pressure-sensitive prototype and have designed a set of gestures that utilize alterable forces. The gestures' feasibility has been tested through a simple experiment. Finally, we discuss the new possibility of touch interactions that are sensitive to pressure.",0,0,1,hand gesture+hand posture+virtual keyboards,
1385,1385,"Multimodal analysis and estimation of intimate self disclosure. Self-disclosure to others has a proven benefit for one’s mental health. It is shown that disclosure to computers can be similarly beneficial for emotional and psychological well-being. In this paper, we analyzed verbal and nonverbal behavior associated with self-disclosure in two datasets containing structured human-human and human-agent interviews from more than 200 participants. Correlation analysis of verbal and nonverbal behavior revealed that linguistic features such as affective and cognitive content in verbal behavior, and nonverbal behavior such as head gestures are associated with intimate self-disclosure. A multimodal deep neural network was developed to automatically estimate the level of intimate self-disclosure from verbal and nonverbal behavior. Between modalities, verbal behavior was the best modality for estimating self-disclosure within-corpora achieving r = 0.66. However, the cross-corpus evaluation demonstrated that nonverbal behavior can outperform language modality in cross-corpus evaluation. Such automatic models can be deployed in interactive virtual agents or social robots to evaluate rapport and guide their conversational strategy.",0,0,1,virtual agent+social robots+nonverbal behavior,
1386,1386,"Ensino de ihc compartilhando as experiencias docentes no contexto brasileiro. This paper summarizes the outcomes of the second workshop on education in HCI (II WEIHC) that was held in conjunction with IHC 2011. Seventeen participants including researchers, professor and professionals attended the workshop. The results point out some challenges that should be overcome to promote HCI education in Brazil, as well as a proposal of the next steps in that direction.",0,0,1,education+educational technology+education technology,
1387,1387,"Assessing the accuracy of point teleport locomotion with orientation indication for virtual reality using curved trajectories. Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point & teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point & teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.",0,0,1,virtual environments+virtual reality+immersive virtual environments,
1388,1388,"Modeling multiple distributions of student performances to improve predictive accuracy. In this paper, we propose a general approach to improve student modeling predictive accuracy. The approach was designed based on the assumption that student performance is sampled from multiple, rather than only one, distribution and thus should be modeled by multiple classification models. We applied k-means to identify student performances sampled from those multiple distributions, using no additional features beyond binary correctness of student responses. We trained a separate classification model for each distribution and applied the learned models to unseen students to evaluate our approach. The results showed that compared to the base classifier, our proposed approach is able to improve predictive accuracy: 4.3% absolute improvement in R2 and 0.03 absolute improvement in AUC, which are not trivial improvements considering the current state of the art in student modeling.",0,0,1,base classifiers+student modeling+student learning,
1389,1389,"Finding layers using hover visualizations. In 2D digital art software, it is common to organize documents in to layers that are composited to create the final image, mimicking the traditional technique of creating an image by drawing on stacked transparent celluloid sheets. While intuitive, the layer stack suffers from problems of scale and organization. Documents with many layers are unwieldy to edit, finding a specific layer is akin to finding a needle in a haystack. This paper presents a click and hover interaction which visualizes the impact of a layer in the context of the full resolution composited image, providing an easier and faster way to identify layers in complex compositions. Through a user study, we find that users prefer to use hover visualizations in all cases, and that in compositions with many overlapping, semi-transparent layers, users are able to locate layers twice as fast with the click and hover interface.",0,0,1,layered+visualization+visualization tools,
1390,1390,"High quality hair modeling from a single portrait photo. We propose a novel system to reconstruct a high-quality hair depth map from a single portrait photo with minimal user input. We achieve this by combining depth cues such as occlusions, silhouettes, and shading, with a novel 3D helical structural prior for hair reconstruction. We fit a parametric morphable face model to the input photo and construct a base shape in the face, hair and body regions using occlusion and silhouette constraints. We then estimate the normals in the hair region via a Shape-from-Shading-based optimization that uses the lighting inferred from the face model and enforces an adaptive albedo prior that models the typical color and occlusion variations of hair. We introduce a 3D helical hair prior that captures the geometric structure of hair, and show that it can be robustly recovered from the input photo in an automatic manner. Our system combines the base shape, the normals estimated by Shape from Shading, and the 3D helical hair prior to reconstruct high-quality 3D hair models. Our single-image reconstruction closely matches the results of a state-of-the-art multi-view stereo applied on a multi-view stereo dataset. Our technique can reconstruct a wide variety of hairstyles ranging from short to long and from straight to messy, and we demonstrate the use of our 3D hair models for high-quality portrait relighting, novel view synthesis and 3D-printed portrait reliefs.",0,0,1,multi-view stereo+facial images+stereo-image,
1391,1391,"Tracking mental engagement a tool for young people with add and adhd. This paper describes a reflective mobile application to help young people with ADD and ADHD better understand their engagement levels during their daily tasks. The mobile application, paired with an electroencephalographic (EEG) device, collects data about user-specific task engagement and pairs it with geographic and temporal data to provide insights into the degree to which a user is engaged in tasks based on time and location. The paper describes two mobile prototypes developed to investigate the problem space of contextual visualizations of engagement information, and it presents future directions for development and study.",0,0,1,smart phones+visualization+visualization tools,
1392,1392,"Fast edge aware processing via first order proximal approximation. We present a new framework for fast edge-aware processing of images and videos. The proposed smoothing method is based on an optimization formulation with a non-convex sparse regularization for a better smoothing behavior near strong edges. We develop mathematical tools based on first order approximation of proximal operators to accelerate the proposed method while maintaining high-quality smoothing. The first order approximation is used to estimate a solution of the proximal form in a half-quadratic solver, and also to derive a warm-start solution that can be calculated quickly when the image is loaded by the user. We extend the method to large-scale processing by estimating the smoothing operation with independent 1D convolution operations. This approach linearly scales to the size of the image and can fully take advantage of parallel processing. The method supports full color filtering and turns out to be temporally coherent for fast video processing. We demonstrate the performance of the proposed method on various applications including image smoothing, detail manipulation, HDR tone-mapping, fast edge simplification and video edge-aware processing.",0,0,1,video contents+digital videos+video streams,
1393,1393,"A hand gesture control framework on smart glasses. Nowadays, in order to overcome limitations of WIMP interaction, many novel emerging user interfaces have been discussed, such as multi-touch user interfaces [Reisman et al. 2009], tangible user interfaces (TUIs) [Jorda et al. 2007], organic user interfaces (OUIs) [Koh et al. 2011], and mid-air gesture detection [Benko and Wilson 2010]. These technologies have the potential to significantly impact on marketing in the area of smart TVs, desktops, mobile phones, tablets and wearable devices such as smart watches and smart glasses. As we know, Google Glass, a type of wearable device, which only provides a touch pad, located on the right side of the device, which can use touch gestures by simple tapping and sliding your finger on it. Hand gesture is not only one of powerful human-to-human communication modalities [Chen et al. 2007], but also can change the way with human-computer interaction. Therefore, implementing a hand gesture control framework on the glasses could provide an easy-to-use, intuitive and flexibility of interaction approach. In this paper, we proposed a hand gesture control framework on smart glasses that supported various fancy gesture controls. The user can load a virtual 3D object through his fingers just like the magician's trick; rotate the virtual 3D object by moving his hand; zoom the virtual 3D object by using a particular gesture sign.",0,0,1,wearable devices+human computer interaction+tangible user interfaces,
1394,1394,"Grand push auto a car based exertion game. Grand Push Auto is an exertion game in which players aim to push a full sized car to ever increasing speeds. The re-appropriation of a car as essentially a large weight allows us to create a highly portable and distributable exertion game in which the main game element has a weight of over 1000 kilograms. In this paper we discuss initial experiences with GPA, and present 3 questions for ongoing study which have been identified from our early testing: How might we appropriate existing objects in exertion game design, and does appropriation change how we think about these objects in different contexts, for example environmental awareness? How does this relate to more traditional sled based weight training? How can we create exertion games that allow truly brutal levels of force?",0,0,1,videogames+gameplay+game design,
1395,1395,"Ethologically inspired robot behavior implementation. For implementing ethologically inspired robot behavior in this paper a platform based on fuzzy automaton (fuzzy state-machine) is suggested. It can react the human intervention as a function of the robot state and the human action. This platform is suitable for implementing quite complicated action-reaction sequences, like the interaction of human and an animal, e.g. a behavior of an animal companion to the human. The suggested fuzzy model structure built upon the framework of low computational demand Fuzzy Rule Interpolation (FRI) methods and fuzzy automaton. For demonstrating the applicability of the proposed structure, some components of an action-reaction FRI model, will be briefly introduced in this paper.",0,0,1,robot motion+robot behavior+automation,
1396,1396,"Diybio things open source biology tools as platforms for hybrid knowledge production and scientific participation. DIYbio (Do It Yourself Biology) is a growing movement of scientists, hobbyists, artists, and tinkerers who practice biology outside of professional settings. In this paper, we present our work with several open source DIYbio tools, including OpenPCR and Pearl Blue Transilluminator, which can be used to test DNA samples for specific sequences. We frame these platforms as things that gather heterogeneous materials and concerns, and enable new forms of knowledge transfer. Working with these hybrid systems in professional and DIY settings, we conducted a workshop where non-biologists tested food products for genetic modifications. Our findings suggest new design directions at the intersection of biology, technology, and DIY: i) DIYbio platforms as rich tools for hybrid knowledge production; and ii) open source biology as a site for public engagement with science.",0,0,1,hardware platform+mobile platform+visualization tools,
1397,1397,"Literatin beyond awareness of readability in terms and conditions. Terms and Conditions (T&Cs) are frequently unread as a consequence of their complexity and length. Readability formulas are used to objectively measure this complexity, but ironically their outputs are also unreadable to many. This motivated the development of a chrome extension called Literatin that compares the complexity of popular fictional literature to T&Cs in order sensitise people to their complexity. In this paper we discuss whether this has been achieved, and outline plans to further develop the extension.",0,0,1,firefox+understandability,
1398,1398,"A gameplay loops formal language. In this paper we present an approach of procedural game content generation that focuses on a gameplay loops formal language (GLFL). In fact, during an iterative game design process, game designers suggest modifications that often require high development costs. The proposed language and its operational semantic allow reducing the gap between game designers' requirement and game developers' needs, enhancing therefore video games productivity. Using gameplay loops concept for game content generation offers a low cost solution to adjust game challenges, objectives and rewards in video games. A pilot experiment have been conducted to study the impact of this approach on game development.",0,0,1,gameplay+game designers+game design,
1399,1399,"Unsupervised recognition of interleaved activities of daily living through ontological and probabilistic reasoning. Recognition of activities of daily living (ADLs) is an enabling technology for several ubiquitous computing applications. In this field, most activity recognition systems rely on supervised learning methods to extract activity models from labeled datasets. An inherent problem of that approach consists in the acquisition of comprehensive activity datasets, which is expensive and may violate individuals' privacy. The problem is particularly challenging when focusing on complex ADLs, which are characterized by large intra- and inter-personal variability of execution. In this paper, we propose an unsupervised method to recognize complex ADLs exploiting the semantics of activities, context data, and sensing devices. Through ontological reasoning, we derive semantic correlations among activities and sensor events. By matching observed sensor events with semantic correlations, a statistical reasoner formulates initial hypotheses about the occurred activities. Those hypotheses are refined through probabilistic reasoning, exploiting semantic constraints derived from the ontology. Extensive experiments with real-world datasets show that the accuracy of our unsupervised method is comparable to the one of state of the art supervised approaches.",0,0,1,individual privacy+activity models+ubiquitous computing,
1400,1400,"Developing methodology for experimentation using a nuclear power plant simulator. Many of today’s most complicated systems are human-machine systems that involve extensive advanced technology and a team of highly trained operators. As these human-machine systems are so complex, it is important to understand the factors that influence operator performance, operator state (e.g., overloaded, underload, stress) and the types of errors that operators make. Thus, it is desirable to develop an experimental methodology for studying complex systems that involve team operations. This paper looks at Nuclear Power Plant (NPP) operations as a test case for building this methodology. The methodology will reference some aspects/details specific to NPPs, but the general principles are intended to extend to any complex system that involves team operations.",0,0,1,complex adaptive systems+nuclear plant+pressurized water reactors,
1401,1401,"Variations between perceptions of interpersonal distance in virtual environments for autism. Interpersonal distance is defined as the area which we choose to keep between ourselves and others, revealed through observation and cultural components. Although previous studies have suggested the possibility of alterations in perception of interpersonal distance in children with Autism Spectrum Disorder, it remains unknown whether these differences exist in relation to characters in a virtual environment. As many social-skills interventions for autism rely upon virtual characters to teach social behaviors, this research is key in understanding how to configure the interpersonal distance of virtual characters to an adequate level to effectively foster computerized social-skills training. We have carried out controlled trials with children with autism to identify variations in preferences from the typically developed population with both a human partner and a virtual character. The contributions of this research are twofold: first, to support existing literature in identifying differences in personal space preferences between children with autism and typically developing children; and second, to understand whether these differences carry over into the context of virtual environments.",0,0,1,virtual worlds+virtual environments+immersive virtual environments,
1402,1402,"Large scale point cloud visualization through localized textured surface reconstruction. In this paper, we introduce a novel scene representation for the visualization of large-scale point clouds accompanied by a set of high-resolution photographs. Many real-world applications deal with very densely sampled point-cloud data, which are augmented with photographs that often reveal lighting variations and inaccuracies in registration. Consequently, the high-quality representation of the captured data, i.e., both point clouds and photographs together, is a challenging and time-consuming task. We propose a two-phase approach, in which the first (preprocessing) phase generates multiple overlapping surface patches and handles the problem of seamless texture generation locally for each patch. The second phase stitches these patches at render-time to produce a high-quality visualization of the data. As a result of the proposed localization of the global texturing problem, our algorithm is more than an order of magnitude faster than equivalent mesh-based texturing techniques. Furthermore, since our preprocessing phase requires only a minor fraction of the whole data set at once, we provide maximum flexibility when dealing with growing data sets.",0,0,1,cloud services+cloud data+visualization,
1403,1403,"Quantized reality automatic fine grained spherical images acquisition for space re construction. Capturing and reconstructing real world environments in 3D has broad areas of application including virtual reality, entertainment, archiving, simulation, and training. Geometry-based methods typically estimate the geometry of the environment based on multiple 2D cameras or depth cameras. These methods are applicable when the environment consists of solid and opaque substance. However, they cannot deal with transparent or highly reflective materials, or environments with an ambiguous surface such as fog. Light field space is a method that captures space by directly recording the light field, without using geometrical information. It can solve the problem faced by geometry-based systems in which view positions are limited when a 2D camera array is used. This problem cannot be solved by introducing a 3D camera array, because in such a configuration, each camera may occlude the images of other cameras. In this paper, we propose a space acquisition method that uses an autonomous robot that moves around the environment with a camera array. By combining very dense spherical images obtained by this robot, real world scenes are captured and reconstructed without view position limitations.",0,0,1,geometry+sub-arrays+virtual reality,
1404,1404,"Gamicad a gamified tutorial system for first time autocad users. We present GamiCAD, a gamified in-product, interactive tutorial system for first time AutoCAD users. We introduce a software event driven finite state machine to model a user's progress through a tutorial, which allows the system to provide real-time feedback and recognize success and failures. GamiCAD provides extensive real-time visual and audio feedback that has not been explored before in the context of software tutorials. We perform an empirical evaluation of GamiCAD, comparing it to an equivalent in-product tutorial system without the gamified components. In an evaluation, users using the gamified system reported higher subjective engagement levels and performed a set of testing tasks faster with a higher completion ratio.",0,0,1,finite state machine (fsm)+software component+software,
1405,1405,"Real time eulerian water simulation using a restricted tall cell grid. We present a new Eulerian fluid simulation method, which allows real-time simulations of large scale three dimensional liquids. Such scenarios have hitherto been restricted to the domain of off-line computation. To reduce computation time we use a hybrid grid representation composed of regular cubic cells on top of a layer of tall cells. With this layout water above an arbitrary terrain can be represented without consuming an excessive amount of memory and compute power, while focusing effort on the area near the surface where it most matters. Additionally, we optimized the grid representation for a GPU implementation of the fluid solver. To further accelerate the simulation, we introduce a specialized multi-grid algorithm for solving the Poisson equation and propose solver modifications to keep the simulation stable for large time steps. We demonstrate the efficiency of our approach in several real-world scenarios, all running above 30 frames per second on a modern GPU. Some scenes include additional features such as two-way rigid body coupling as well as particle representations of sub-grid detail.",0,0,1,gpu+gpu implementation+fluid simulations,
1406,1406,"Designing mobile interfaces for novice and low literacy users. While mobile phones have found broad application in bringing health, financial, and other services to the developing world, usability remains a major hurdle for novice and low-literacy populations. In this article, we take two steps to evaluate and improve the usability of mobile interfaces for such users. First, we offer an ethnographic study of the usability barriers facing 90 low-literacy subjects in India, Kenya, the Philippines, and South Africa. Then, via two studies involving over 70 subjects in India, we quantitatively compare the usability of different points in the mobile design space. In addition to text interfaces such as electronic forms, SMS, and USSD, we consider three text-free interfaces: a spoken dialog system, a graphical interface, and a live operator.   Our results confirm that textual interfaces are unusable by first-time low-literacy users, and error prone for literate but novice users. In the context of healthcare, we find that a live operator is up to ten times more accurate than text-based interfaces, and can also be cost effective in countries such as India. In the context of mobile banking, we find that task completion is highest with a graphical interface, but those who understand the spoken dialog system can use it more quickly due to their comfort and familiarity with speech. We synthesize our findings into a set of design recommendations.",0,0,1,mobile phones+user interfaces+mobile users,
1407,1407,"Deploying speech interfaces to the masses. Speech systems are typically deployed either over phones, e.g. IVR agents, or on embodied agents, e.g. domestic robots. Most of these systems are limited to a particular platform i.e., only accessible by phone or in situated interactions. This limits scalability and potential domain of operation. Our goal is to make speech interfaces more widely available, and we are proposing a new approach for deploying such interfaces on the internet along with traditional platforms. In this work, we describe a lightweight speech interface architecture built on top of Freeswitch, an open source softswitch platform. A softswitch enables us to provide users with access over several types of channels (phone, VOIP, etc.) as well as support multiple users at the same time. We demonstrate two dialog applications developed using this approach: 1) Virtual Chauffeur: a voice based virtual driving experience and 2) Talkie: a speech-based chat bot.",0,0,1,internet+voip+virtual spaces,
1408,1408,"Attention and visual memory in visualization and computer graphics. A fundamental goal of visualization is to produce images of data that support visual analysis, exploration, and discovery of novel insights. An important consideration during visualization design is the role of human visual perception. How we ""see” details in an image can directly impact a viewer's efficiency and effectiveness. This paper surveys research on attention and visual perception, with a specific focus on results that have direct relevance to visualization and visual analytics. We discuss theories of low-level visual perception, then show how these findings form a foundation for more recent work on visual memory and visual attention. We conclude with a brief overview of how knowledge of visual attention and visual memory is being applied in visualization and graphics. We also discuss how challenges in visualization are motivating research in psychophysics.",0,0,1,visual analytics+interactive exploration+visualization and analysis,
1409,1409,"Mediating attention for second screen companion content. There is increasing interest in providing content to users on secondary devices while they watch TV. This material, termed companion content, can be anything from textual information, to interactive quiz games. It can be delivered throughout a broadcast and often directly relates to specific scenes in a show. This new scenario has exposed a challenging design space for creators of both the content and the enabling technology. A key question when introducing content on a secondary device is how much it detracts from, or enhances, the show the user is currently engaged with. To examine this, we investigated methods for mediating attention from the TV and onto a secondary device. By examining a typical use case we have been able to gain new insights into how best to design additional stimuli to alert users to companion content from both a broadcasting, and an HCI perspective.",0,0,1,broadcasting+multimedia contents+digital contents,
1410,1410,"That s not me surprising algorithmic inferences. Online platforms such as Google and Facebook make inferences about users based on data from their online and offline behavior that can be used for various purposes. Though some of these inferences are available for users to view, there exists a gap between what platforms are actually able to infer from collected data and what inferences users are expecting or believe to be possible. Studying users' reactions to inferences made about them, especially what surprises them, allows us to better understand this gap. We interviewed users of Google and Facebook to learn their current beliefs and expectations about how these platforms use their data to make inferences, and identified four common sources of surprise for participants: irrelevant inferences, outdated inferences, inferences with no connection to online activity, and inferences related to friends or family. We discuss the implications for designing inference-generating systems.",0,0,1,web content+facebook+mobile users,
1411,1411,"Crowdsourcing law and policy a design thinking approach to crowd civic systems. Crowdsourcing technologies, strategies and methods offer new opportunities for bridging existing gaps among law, policymaking, and the lived experience of citizens. In recent years, a number of initiatives across the world have applied crowdsourcing to contexts including constitutional reform, drafting federal bills, and generating local policies. However, crowd-civic systems also come with challenges and risks such as socio-technical barriers, marginalization of specific groups, silencing of interests, etc. Using a design-thinking approach, this workshop will address both opportunities and challenges of crowd-civic systems to develop best practices for increasing public engagement with law and policy. The workshop organizers will suggest an initial framework explicitly intended to be criticized by participants and reconfigured through a series of iterative cooperative small-group activities focusing on ``diagnosing'' the failures of past crowd-civic system efforts and the successes of online action around social issues. While the ultimate objective of the workshop is to develop a best practices guide, we see iterations on the guide as a mechanism for fostering community and collaboration among policymakers, technologists, and researchers around crowd-civic systems for law and policy.",0,0,1,systems design+prototyping+innovative technologies,
1412,1412,"Understanding lurkers in online communities a literature review. In internet culture, lurkers are a special group of website users who regularly login to online communities but seldom post. This study aims to provide an overall understanding of lurkers by explaining the definition of lurkers, discussing the reasons for lurking and providing suggestions on de-lurking. To understand the reason for lurking, this study first explains why people participate in online communities by building an integrated model of motivational factors of online behaviors. This model classifies motivational factors into four categories: the nature of the online community, individual characteristics, the degree of commitment and quality requirement. Based on this model, four types of lurking reasons are identified: environmental influence, personal preference, individual-group relationship and security consideration. Finally, several strategies for motivating participation in online communities are provided, including external stimuli, improved user-friendliness, encouragement of participation and guidance for newcomers.",0,0,1,online communities+virtual community+social networking sites,
1413,1413,"Sentiment richness authority and relevance model of information sharing during social crises the case of mh370 tweets. Abstract The study introduces a model of crisis information sharing based on Twitter discussions of the missing Malaysian Airlines Flight 370. Grounded in the Elaboration Likelihood Model, the study tests four salient factors: Sentiment, Richness, Authority, and Relevance, which can be measured by peripheral cues in tweets and in user profiles. Findings suggest that information sharing is positively associated with the presence of peripheral cues indicative of a confident, self-revealing and positive emotional language style, and is negatively related to an angry and informal style. Additionally, information sharing is related to the presence of multimedia cues and cues indicating source popularity.",0,0,1,sharing information+information sharing+user information,
1414,1414,"Pseudo haptic feedback on softness induced by grasping motion. In most of the research on pseudo-haptic feedback, subjects' hands are on the desk and the visual image is provided from a monitor placed in front of them. The setup easily induces sensory conflicts for pseudo-haptic feedback between visual and haptic perception. However, subjects rarely see simultaneously their hand in motion and in a visual display. We report here our preliminary study on pseudo-haptic feedback related to tactile perception of softness. In the study, subjects hold a hand-held display with pressure sensors. A virtual object shown on the display screen changes shape according to pressures from the subject's squeezing of the device. In this configuration, subjects are able to see their hand and the visual display at same time. We also describe the preliminary experimental results confirming the feasibility of our system and its applicability in investigating haptic pseudo-haptic.",0,0,1,haptic perception+tactile feedback+virtual objects,
1415,1415,"Biomechanical and physiological responses to electrically assisted cycling during simulated mail delivery. Abstract This study quantified the biomechanical (movements and forces) and physiological (energy expenditure) demands of postal delivery performed with electrically assisted bicycles (EABs). Ten postal workers and 10 recreational athletes performed three simulated postal tasks (simulated mail delivery circuit, delivery distance [close vs. far], and 3-min stationary cycling) while carrying 0, 16 and 32 kg. Physiological (energy expenditure) and biomechanical (internal and external forces and joint angles) responses were calculated. Energy expenditure (10–20%; p",0,0,1,email+electronic mail+office workers,
1416,1416,"Growing the blockchain information infrastructure. In this paper, we present ethnographic data that unpacks the everyday work of some of the many infrastructuring agents who contribute to creating, sustaining and growing the Blockchain information infrastructure. We argue that this infrastructuring work takes the form of entrepreneurial actions, which are self-initiated and primarily directed at sustaining or increasing the initiator's stake in the emerging information infrastructure. These entrepreneurial actions wrestle against the affordances of the installed base of the Blockchain infrastructure, and take the shape of engaging or circumventing activities. These activities purposefully aim at either influencing or working around the enablers and constraints afforded by the Blockchain information infrastructure, as its installed base is gaining inertia. This study contributes to our understanding of the purpose of infrastructuring, seen from the perspective of heterogeneous entrepreneurial agents. It supplements existing accounts of the ""when"" and ""how"" of infrastructure, with a lens for examining the ""why"" of infrastructure.",0,0,1,cryptocurrency+bitcoin+infrastructure services,
1417,1417,"Is workplace satisfaction associated with self reported quad bike loss of control events among farm workers in new zealand. Abstract   This study investigated whether rural workers who have higher workplace satisfaction are less likely to report quad bike loss of control events (LCEs). Two independent samples of farmers completed a survey regarding LCEs and workplace satisfaction. In the first sample ( n  = 130) analysis revealed no relationship ( p  = 0.74) between workplace satisfaction and LCEs but lower rates of LCEs were reported by employees (IRR 0.52, 95%CI 0.31–0.86) compared to self-employed participants. In the second sample ( n  = 112), workplace satisfaction was weakly related to LCEs (IRR 1.04, 95%CI 1.00, to 1.09) with participants who found their job more psychologically demanding more likely to have had an LCE (IRR 1.14, 95%CI 1.05–1.23). Exploring the role of psychological demands on safety behaviour with respect to quad bike use, may help to address this important safety issue.",0,0,1,perceived value+work environments+office workers,
1418,1418,"A visual navigation system for querying neural stem cell imaging data. Cellular biology deals with studying the behavior of cells. Current time-lapse imaging microscopes help us capture the progress of experiments at intervals that allow for understanding of the dynamic and kinematic behavior of the cells. On the other hand, these devices generate such massive amounts of data (250GB of data per experiment) that manual sieving of data to identify interesting patterns becomes virtually impossible. In this paper we propose an end-to-end system to analyze time-lapse images of the cultures of human neural stem cells (hNSC), that includes an image processing system to analyze the images to extract all the relevant geometric and statistical features within and between images, a database management system to manage and handle queries on the data, a visual analytic system to navigate through the data, and a visual query system to explore different relationships and correlations between the parameters. In each stage of the pipeline we make novel algorithmic and conceptual contributions, and the entire system design is motivated by many different yet unanswered exploratory questions pursued by our neurobiologist collaborators. With a few examples we show how such abstract biological queries can be analyzed and answered by our system.",0,0,1,navigation systems+systems design+visual analytics,
1419,1419,"A low cost 61 channel µecog array for use in rodents. Micro-Electrocorticography (µECoG) offers a minimally invasive, high resolution interface with large areas of cortex. A wide variety of µECoG designs have been developed and customized [1]–[4], including active, multiplexed arrays [5] and arrays on dissolving substrates for increased conformal contact [6]. However, designing and fabricating customized µECoG arrays requires access to microfabrication facilities, which many neuroscience labs do not have. Microfabrication is also typically labor intensive and expensive. Commercial µECoG arrays with 64 electrodes and coarser dimensions cost approximately $1000, limiting their suitability for chronic implantation in large numbers of animals. Here we present a high density (406 µm spacing), flexible (∼30 µm thin), 61-contact µECoG electrode array fabricated using a low-cost, commercial manufacturing process. The array costs just $26 when ordered in quantities of 100, with the cost per electrode increasing slightly when lower quantities are ordered. Fine pitch wires minimize the size of the interconnections, enabling chronic implantation in rodents. In-house post-processing of the fabricated µECoG arrays added optional electrode coatings, such as platinum black, to reduce the electrode impedance. Our electrode design and manufacturing process dramatically improves the accessibility and reduces the cost of high-volume, high-resolution neuroscience.",0,0,1,sub-arrays+interconnects+microfabrication,
1420,1420,"A multimodal database for affect recognition and implicit tagging. MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",0,0,1,affect recognition+emotional expressions+cultural background,
1421,1421,"Building multi layer social knowledge maps with google maps api. Google Maps is an intuitive online-map service which changes people’s way of navigation on Geo-maps. People can explore the maps in a multi-layer fashion in order to avoid information overloading. This paper re- ports an innovative approach to extend the “power” of Google Maps to adaptive learning. We have designed and implemented a navigator for multi-layer social knowledge maps, namely ProgressiveZoom, with Google Maps API. In our demonstration, the knowledge maps are built from the Interactive System De- sign (ISD) course at the School of Information Science, University of Pitts- burgh. Students can read the textbooks and reflect their individual and social learning progress in a context of pedagogical hierarchical structure.",0,0,1,demonstrations+navigation systems+university,
1422,1422,"Practical utility of a wearable skin vibration sensor using a pvdf film. We have developed a wearable skin vibration sensor using a polyvinylidene fluoride (PVDF) film. The sensor is attached to the finger pad between distal interphalangeal (DIP) and proximal interphalangeal (PIP) joints and users can touch an object with the bare finger. It detects the skin-propagated vibration. In previous reports, we demonstrated that the sensor output well describes tactile information. This paper investigates the practical utility of the sensor. First, an experiment on tangential vibration is conducted and results show that the sensor can respond to the tangential vibration given to the fingertip at a similar level as the normal vibration. Then, the influence of wearing tension and contact force on the sensor output is investigated. Experimental results show that the wearing tension does not largely influence the sensor output within natural tensions, and that the intensity and the peak frequency in the gain of the sensor output tend to increase with the increase of the contact force. In particular, the intensity of the sensor output is different between contact forces of 0.25 and 0.5 N.",0,0,1,tactile information+tactile display+tactile sensation,
1423,1423,"From reading math to doing math a new direction in non visual math accessibility. The ability to understand, apply, and manipulate mathematical concepts is a cornerstone of any scientific discipline; as such it is an irreplaceable component of the training and education of any students. While the advent of online and e-learning technologies has enabled the breaking of several barriers and promoted wider access to educational opportunities, it has also furthered the disenfranchisement of visually impaired students. Despite the several efforts in the field of enhancing accessibility for those with visual impairments especially in the educational discipline, it is obvious that there is still a lack of contributions in making mathematics manipulation processes accessible. This paper presents a framework that facilitates doing and manipulating mathematical algebraic content in a way that is convenient, accessible, and usable as well for students with visual impairments.",0,0,1,educational technology+education technology+visually impaired,
1424,1424,"Wigest a ubiquitous wifi based gesture recognition system. We present WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device. Compared to related work, WiGest is unique in using standard WiFi equipment, with no modi-fications, and no training for gesture recognition. The system identifies different signal change primitives, from which we construct mutually independent gesture families. These families can be mapped to distinguishable application actions. We address various challenges including cleaning the noisy signals, gesture type and attributes detection, reducing false positives due to interfering humans, and adapting to changing signal polarity. We implement a proof-of-concept prototype using off-the-shelf laptops and extensively evaluate the system in both an office environment and a typical apartment with standard WiFi access points. Our results show that WiGest detects the basic primitives with an accuracy of 87.5% using a single AP only, including through-the-wall non-line-of-sight scenarios. This accuracy in-creases to 96% using three overheard APs. In addition, when evaluating the system using a multi-media player application, we achieve a classification accuracy of 96%. This accuracy is robust to the presence of other interfering humans, highlighting WiGest's ability to enable future ubiquitous hands-free gesture-based interaction with mobile devices.",0,0,1,mobile devices+hand gesture+hands-free,
1425,1425,"Investigating three dimensional directional guidance with nonvisual feedback for target pointing task. While directional guidance is essential for spatial navigation, little has been studied about providing nonvisual cues in 3D space for individuals who are blind or have limited visual acuity. To understand the effects of different nonvisual feedback for 3D directional guidance, we conducted a user study with 12 blind-folded participants. They were asked to search for a virtual target in a 3D space with a laser pointer as quickly as possible under 6 different feedback designs varying the feedback mode (beeping vs. haptic vs. beeping+haptic) and the presence of a stereo sound. Our findings show that beeping sound feedback with and without haptic feedback outperforms the mode where only haptic feedback is provided. We also found that stereo sound feedback generated from a target significantly improves both the task completion time and travel distance. Our work can help people who are blind or have limited visual acuity to understand the directional guidance in a 3D space.",0,0,1,haptic feedbacks+tactile feedback+haptic sensation,
1426,1426,"Parole board personality and decision making using bias based reasoning. Parole decision-making directly affects the lives of hundreds of thousands of individuals each year. While some research has been done on the offender and case factors that influence the parole decision, little research has been done on the characteristics of parole board members and how these influence the parole decision. Personality is one factor that influences the way individuals make decision.",0,0,1,decision tables+binary decision+decision support systems,
1427,1427,"Predicting social emotions from readers perspective. Due to the rapid development of Web, large numbers of documents assigned by readers’ emotions have been generated through new portals. Comparing to the previous studies which focused on author's perspective, our research focuses on readers’ emotions invoked by news articles. Our research provides meaningful assistance in social media application such as sentiment retrieval, opinion summarization and election prediction. In this paper, we predict the readers’ emotion of news based on the social opinion network. More specifically, we construct the opinion network based on the semantic distance. The communities in the news network indicate specific events which are related to the emotions. Therefore, the opinion network serves as the lexicon between events and corresponding emotions. We leverage neighbor relationship in network to predict readers’ emotions. As a result, our methods obtain better result than the state-of-the-art methods. Moreover, we developed a growing strategy to prune the network for practical application. The experiment verifies the rationality of the reduction for application.",0,0,1,basic emotions+emotional expressions+emotion expression,
1428,1428,"Interactive ray casting of geodesic grids. Geodesic grids are commonly used to model the surface of a sphere and are widely applied in numerical simulations of geoscience applications. These applications range from biodiversity, to climate change and to ocean circulation. Direct volume rendering of scalar fields defined on a geodesic grid facilitates scientists in visually understanding their large scale data. Previous solutions requiring to first transform the geodesic grid into another grid structure (e.g., hexahedral or tetrahedral grid) for using graphics hardware are not acceptable for large data, because such approaches incur significant computing and storage overhead. In this paper, we present a new method for efficient ray casting of geodesic girds by leveraging the power of Graphics Processing Units (GPUs). A geodesic grid can be directly fetched from storage or streamed from simulations to the rendering stage without the need of any intermediate grid transformation. We have designed and implemented a new analytic scheme to efficiently perform value interpolation for ray integration and gradient calculations for lighting. This scheme offers a more cost-effective rendering solution over the existing direct rendering approach. We demonstrate the effectiveness of our rendering solution using real-world geoscience data.",0,0,1,storage overhead+ocean circulation+graphics hardware,
1429,1429,"Three themes for designing games that aim to promote a positive body perception in hospitalized children. Hospitalized children often experience physical changes that negatively affect their bodily perceptions, thereby adding to the stress of being sick. Existing approaches to supporting hospitalized children such as those promoted by the Clown Doctors use play to distract the child from negative bodily perceptions. In contrast, we propose reframing the bodily perception of these children through bodily virtual play facilitated by their imagination. We explore this design space through an analysis of the literature combined with design explorations around play and bodily imagination. This research results in a set of themes for games that aim to reframe bodily perception to a more positive self-image full of creative potential. We envisage that our work could help designers who aim to create digital play for sick children.",0,0,1,computer games+videogames+virtual spaces,
1430,1430,"Internet habit strength and online communication exploring gender differences. Abstract   To examine the relationship between internet habit strength and online communication, as well as the moderating effect of gender on this relationship, 1572 adolescents were recruited to complete anonymous self-report questionnaire. Current findings found that internet habit strength was positively associated with online communication, but that this association was stronger for females than it was for males. This suggests that females with stronger internet habit strength were more likely to engage in online communication than males. The findings indicate that gender does have a decision influence on online communication since it alters the habit strength in Internet.",0,0,1,anonymous communication+world wide web+intranets,
1431,1431,"Exploring trajectory driven local geographic topics in foursquare. The location based social networking services (LBSNSs) are becoming very popular today. In LBSNSs, such as Foursquare, users can explore their places of interests around their current locations, check in at these places to share their locations with their friends, etc. These check-ins contain rich information and imply human mobility patterns; thus, they can greatly facilitate mining and analysis of local geographic topics driven by users' trajectories. The local geographic topics indicate the potential and intrinsic relations among the locations in accordance with users' trajectories. These relations are useful for users in both location and friend recommendations. In this paper, we focus on exploring the local geographic topics through check-ins in Pittsburgh area in Foursquare. We use the Latent Dirichlet Allocation (LDA) model to discover the local geographic topics from the checkins. We also compare the local geographic topics on weekdays with those at weekends. Our results show that LDA works well in finding the related places of interests.",0,0,1,mobility pattern+web content+mobile users,
1432,1432,"Study of human prostate spheroids treated with zinc using x ray microfluorescence. Prostate cancer is the most frequently diagnosed type of cancer in men and the second leading cause of their cancer death in many countries. Human tumor cells cultured in three dimensions display increased pro-angiogenic capacities and resistance to interferons, chemotherapeutic agents or irradiation, as compared with cells cultured in two dimensional monolayers. In this study, we investigated Zn distribution in prostate cancer spheroids (DU145) and analyzed the differences in the response to Zinc (0-150 μM) treatment using X-ray microfluorescence with Synchrotron Radiation (SRμXRF). The measurements were performed in standard geometry of 45° incidence, exciting with a white beam and using an optical capillary with 20 μm diameter collimation in the XRF beam line at the Synchrotron Light National Laboratory (Campinas, Brazil). The results showed non-uniform distribution of Zn in all the spheroids analyzed. Zn treatment showed a decrease in Zn intensity with the increase of its concentration in DU145 spheroids. The differential response to zinc of DU145 cell spheroids suggests that zinc may have an important role in prostate cancer.",0,0,1,ray tracing+image display+display system,
1433,1433,"The effect of learning in a virtual environment on explicit and implicit memory by applying a process dissociation procedure. ABSTRACTVirtual reality-based neuropsychological assessment has unique features that have the potential to increase the level of ecological validity of test results. Based on findings from the literature on the task difficulty of cognitive tasks embedded into virtual environments, we aimed to explore the task difficulty hypothesis of virtual reality in memory assessment. Our main objective was to test for differences or equivalences between performance on explicit and on implicit memory tasks in three learning environments: a computerized measure, a 3D desktop environment, and a 3D virtual environment. Seventy-seven healthy participants, aged between 19 and 39 years old, enrolled in the study and were randomly assigned to the learning conditions and responded to typical virtual reality measures. Outcomes of explicit and implicit memory resulted after applying Process Dissociation Procedure. One-way analysis of variance did not reveal a significant main effect of learning environment on explicit memory per...",0,0,1,virtual spaces+virtual environments+virtual reality,
1434,1434,"Soft body locomotion. We present a physically-based system to simulate and control the locomotion of soft body characters without skeletons. We use the finite element method to simulate the deformation of the soft body, and we instrument a character with muscle fibers to allow it to actively control its shape. To perform locomotion, we use a variety of intuitive controls such as moving a point on the character, specifying the center of mass or the angular momentum, and maintaining balance. These controllers yield an objective function that is passed to our optimization solver, which handles convex quadratic program with linear complementarity constraints. This solver determines the new muscle fiber lengths, and moreover it determines whether each point of contact should remain static, slide, or lift away from the floor. Our system can automatically find an appropriate combination of muscle contractions that enables a soft character to fulfill various locomotion tasks, including walking, jumping, crawling, rolling and balancing.",0,0,1,biped locomotion+optical fibers+finite element method,
1435,1435,"Multivariate normal maximum likelihood with both ordinal and continuous variables and data missing at random. A novel method for the maximum likelihood estimation of structural equation models (SEM) with both ordinal and continuous indicators is introduced using a flexible multivariate probit model for the ordinal indicators. A full information approach ensures unbiased estimates for data missing at random. Exceeding the capability of prior methods, up to 13 ordinal variables can be included before integration time increases beyond 1 s per row. The method relies on the axiom of conditional probability to split apart the distribution of continuous and ordinal variables. Due to the symmetry of the axiom, two similar methods are available. A simulation study provides evidence that the two similar approaches offer equal accuracy. A further simulation is used to develop a heuristic to automatically select the most computationally efficient approach. Joint ordinal continuous SEM is implemented in OpenMx, free and open-source software.",0,0,1,conditional probabilities+open source software+software,
1436,1436,"Digital transformation in education during covid 19 a case study. Digital transformation is slow process in education which became an urgent topic in the spring of 2020 due to COVID-19. In mid March, the Hungarian Government closed the schools and universities and the classes were held in online form. This faced both students and teachers with unexpected challenges. Survey was conducted among the Computer Science and Information Technologist students of the Eszterhazy Karoly University at the end of the semester. Our survey was focused on the experience, feelings and overall expression of the students regarding to digital education and recent changes. Moreover, the survey contains questions about technical preparation and infrastructure. The responses are processed by well-known statistical data analysis tools. Based on the results, the students enjoyed the digital education and half of them are willing to continue it in the future. In addition, students would prefer to use their own devices during on tutorials which allow some changes in the labor environments. Unfortunately, some students had technical issues which may be caused by the heterogeneous software environment and can be solved with support material. Digital transformation was considered successful and the feedback will be integrated into our online classes.",0,0,1,computer science+university+education,
1437,1437,"Online social networks and police in india understanding the perceptions behavior challenges. Safety is a concern for most urban communities; police departments bear the majority of responsibility to maintain law and order and prevent crime. Police agencies across the globe are increasingly using Online Social Network (OSN) (such as Facebook and Twitter) to acquire intelligence and connect with citizens. Developing nations like India are however, still exploring OSN for policing. We interviewed 20 IPS officers and 21 citizens to understand perceptions, and explored challenges experienced while using OSN for policing. Interview analysis, highlights how citizens and police think about information shared on OSN, handling offensive comments, and acknowledgment overload, as they pursue social and safety goals. We found that success of OSN for policing demands effective communication between the stakeholders (citizens and police). Our study shows that OSN offers community-policing opportunities, enabling police to identify crime with the help of citizens. It can reduce the communication gap and improve coordination between police and citizens. We also discuss design opportunities for tools to support social interactions between stakeholders.",0,0,1,online social networks+social networks+facebook,
1438,1438,"Testing alternative models of individuals social media involvement and satisfaction. This study extends the Technology Acceptance Model 3 (TAM 3) within the context of the rapidly evolving area of social media. Since social media requires the user's active participation and processing of information as well as the creation of user-generated content, this timely study introduces the two relevant constructs of social media involvement and social media satisfaction that are associated with TAM 3. This study then develops three alternative conceptual models and empirically validates each of them using datasets obtained from the United States and Saudi Arabia. The LISREL analysis results show that the Dual Mediation Impact model is the best-fit model for both datasets. Further, the results show that a user's social media involvement and social media satisfaction are dual mediators of the TAM 3 factors on social media usage intention. While the two country data show some differences in the strength of the relationships in the Dual Mediation Impact model, the identified model provides a common framework for global use. Implications and future research directions are discussed.",0,0,1,digital media+unified theory of acceptance and use of technology+utaut,
1439,1439,"Your way your missions a location aware pervasive game exploiting the routes of players. With the development of pervasive computing, location-aware pervasive games have proliferated quickly. To counter the inefficiency of information adaptation based on a location and a radius, this article presents the design and implementation of Your Way Your Missions (YWYM), which is a location-aware pervasive game exploiting the routes of players. YWYM provides a Google Maps–based tool for players to predefine routes, and utilizes self-reporting method to obtain the planned routes of players. By such a design, missions are returned to players in terms of the location properties of missions and the planned routes of players. A field study was conducted to investigate the usage of YWYM, and the results indicate that route predefining and self-reporting methods are an effective approach to obtain the planned routes of players, and information adaptation based on the planned routes of players could help them find missions at any place and any time.",0,0,1,ambient intelligence+pervasive computing+ubiquitous computing,
1440,1440,"On the human ability to discriminate audio ambiances from similar locations of an urban environment. When developing advanced location-based systems augmented with audio ambiances, it would be cost-effective to use a few representative samples from typical environments for describing a larger number of similar locations. The aim of this experiment was to study the human ability to discriminate audio ambiances recorded in similar locations of the same urban environment. A listening experiment consisting of material from three different environments and nine different locations was carried out with nineteen subjects to study the credibility of audio representations for certain environments which would diminish the need for collecting huge audio databases. The first goal was to study to what degree humans are able to recognize whether the recording has been made in an indicated location or in another similar location, when presented with the name of the place, location on a map, and the associated audio ambiance. The second goal was to study whether the ability to discriminate audio ambiances from different locations is affected by a visual cue, by presenting additional information in form of a photograph of the suggested location. The results indicate that audio ambiances from similar urban areas of the same city differ enough so that it is not acceptable to use a single recording as ambience to represent different yet similar locations. Including an image was found to increase the perceived credibility of all the audio samples in representing a certain location. The results suggest that developers of audio-augmented location-based systems should aim at using audio samples recorded on-site for each location in order to achieve a credible impression.",0,0,1,location information+digital audio+location based,
1441,1441,"Modifiber two way morphing soft thread actuators for tangible interaction. Despite thin-line actuators becoming widely adopted in different Human-Computer Interaction (HCI) contexts, including integration into fabrics, paper art, hinges, soft robotics, and human hair, accessible line-based actuators are very limited beyond shape memory alloy (SMA) wire and motor-driven passive tendons. In this paper, we introduce a novel, yet simple and accessible, line-based actuator. ModiFiber is a twisted-then-coiled nylon thread actuator with a silicone coating. This composite thread actuator exhibits unique two-way reversible shrinking or twisting behaviors triggered by heat or electrical current (i.e., Joule heating). ModiFiber is soft, flexible, safe to operate and easily woven or sewn, hence it has a great potential as an embedded line-based actuator for HCI purposes. In this paper, we explain the material mechanisms and manufacturing approaches, followed by some performance tests and application demonstrations.",0,0,1,demonstrations+human computer interaction+tangible interaction,
1442,1442,"Subject specific knee sar prediction using a degenerate birdcage at 7t. We performed a study to predict global and local subject-specific Specific Absorption Rate (SAR) exposure in 7T Musculoskeletal (MSK) acquisition sequences. Such prediction, that is not available in current Magnetic Resonance (MR) exams, is possible combining sequences SAR exposure simulated on the generic anatomical model with subject-specific measured B+ 1 maps. The procedure has been implemented with a degenerate birdcage coil specifically designed for knee exams. Results show a good agreement between simulated B+ 1 and B+ 1 magnitude measure in-vivo, while values of global and local SAR calculated are below the recommended limit, respectively 8 W/kg and 20 W/kg.",0,0,1,polarimetric sar+airborne sar+specific absorption rate,
1443,1443,"Understanding adaptive thermal comfort new directions for ubicomp. In many parts of the world, mechanical heating and cooling is used to regulate indoor climates, with the aim of maintaining a uniform temperature. Achieving this is energy-intensive, since large indoor spaces must be constantly heated or cooled, and the difference to the outdoor temperature is large. This paper starts from the premise that comfort is not delivered to us by the indoor environment, but is instead something that is pursued as a normal part of daily life, through a variety of means. Based on a detailed study of four university students over several months, we explore how Ubicomp technologies can help create a more sustainable reality where people are more active in pursuing and maintaining their thermal comfort, and environments are less tightly controlled and less energy-intensive, and we outline areas for future research in this domain.",0,0,1,energy conservation+university+ubicomp,
1444,1444,"Instant visualization of secondary structures of molecular models. Molecular Dynamics simulations are of key importance in the drug design field. Among all possible representations commonly used to inspect these simulations, Ribbons has the advantage of giving the expert a good overview of the conformation of the molecule. Although several techniques have been previously proposed to render ribbons, all of them have limitations in terms of space or calculation time, making them not suitable for real-time interaction with simulation software. In this paper we present a novel adaptive method that generates ribbons in real-time, taking advantage of the tessellation shader. The result is a fast method that requires no precomputation, and that generates high quality shapes and shading.",0,0,1,molecular systems+visualization+visualization tools,
1445,1445,"A public transport bus as a flexible mobile smart environment sensing platform for iot. In this paper we present the requirements, design and pre-deployment testing of a transportation bus as a Mobile Enterprise Sensor Bus (M-ESB) service in China that supports two main requirements: to monitor the urban physical environment, and to monitor road conditions. Although, several such projects have been proposed previously, integrating both environment and road condition monitoring and using a data exchange interface to feed a data cloud computing system, is a novel approach. We present the architecture for M-ESB and in addition propose a new management model for the bus company to act as a Virtual Mobile Service Operator. Pre-deployment testing was undertaken to validate our system.",0,0,1,internet of things+mobile service+smart environment,
1446,1446,"Mobile posture monitoring system to prevent physical health risk of smartphone users. With the widespread use of a smartphone, users tend to use their smartphone for a long period of time in unhealthy postures; bending forward the neck and watching the relatively small screen closely with concentration. If users keep such unhealthy postures for a long time, they are susceptible to musculoskeletal disorders and eye problems such as cervical disc and myopia, respectively. To prevent users from having these diseases, we propose a new methodology to monitor the posture of smartphone users with built-in sensors. The proposed mechanism estimates various values representing user postures like the tilt angle of the neck, viewing distance, and gaze condition of the user, by analyzing sensor data from a front-faced camera, 3-axis accelerometer, orientation sensor, or any combination thereof, and warns the user if estimated values are maintained within the abnormal range over the allowed time. Via the proposed mechanism, users are able to be aware of their unhealthy postures, and then try to correct their postures.",0,0,1,smart phones+musculoskeletal disorders+small screens,
1447,1447,"The effects of a moisture wicking fabric shirt on the physiological and perceptual responses during acute exercise in the heat. This study investigated the effects that a form fitted, moisture-wicking fabric shirt, promoted to have improved evaporative and ventilation properties, has on the physiological and perceptual responses during exercise in the heat. Ten healthy male participants completed two heat stress tests consisting of 45 min of exercise (50% VO2peak) in a hot environment (33 °C, 60% RH). One heat stress test was conducted with the participant wearing a 100% cotton short sleeved t-shirt and the other heat stress test was conducted with the participant wearing a short sleeved synthetic shirt (81% polyester and 19% elastane). Rectal temperature was significantly lower (P < 0.05) in the synthetic condition during the last 15 min of exercise. Furthermore, the synthetic polyester shirt retained less sweat (P < 0.05). As exercise duration increases, the ventilation and evaporation properties of the synthetic garment may prove beneficial in the preservation of body temperature during exercise in the heat.",0,0,1,test results+human visual systems+teleconference,
1448,1448,"Using tangible smart replicas as controls for an interactive museum exhibition. This paper presents the design, creation and use of tangible smart replicas in a large-scale museum exhibition. We describe the design rationale for the replicas, the process used in their creation, as well as the implementation and deployment of these replicas in a live museum exhibition. Deployment of the exhibition resulted in over 14000 visitors interacting with the system during the 6 months that the exhibition was open. Based on log data, interviews and observations, we examine the reaction to these smart replicas from the point of view of the museum curators and also of the museum's visitors and reflect on the fulfillment of our expectations.",0,0,1,design rationale+tangible interfaces+tangible user interfaces,
1449,1449,"Strain limiting for soft finger contact simulation. The command of haptic devices for rendering direct interaction with the hand requires thorough knowledge of the forces and deformations caused by contact interactions on the fingers. In this paper, we propose an algorithm to simulate nonlinear elasticity under frictional contact, with the goal of establishing a model-based strategy to command haptic devices and to render direct hand interaction. The key novelty in our algorithm is an approach to model the extremely nonlinear elasticity of finger skin and flesh using strain-limiting constraints, which are seamlessly combined with frictional contact constraints in a standard constrained dynamics solver. We show that our approach enables haptic rendering of rich and compelling deformations of the fingertip.",0,0,1,haptic systems+haptic display+haptic interactions,
1450,1450,"Spidervision extending the human field of view for augmented awareness. We present SpiderVision, a wearable device that extends the human field of view to augment a user's awareness of things happening behind one's back. SpiderVision leverages a front and back camera to enable users to focus on the front view while employing intelligent interface techniques to cue the user about activity in the back view. The extended back view is only blended in when the scene captured by the back camera is analyzed to be dynamically changing, e.g. due to object movement. We explore factors that affect the blended extension, such as view abstraction and blending area. We contribute results of a user study that explore 1) whether users can perceive the extended field of view effectively, and 2) whether the extended field of view is considered a distraction. Quantitative analysis of the users' performance and qualitative observations of how users perceive the visual augmentation are described.",0,0,1,wearable computers+mobile users+wearable devices,
1451,1451,"Maker within constraints exploratory study of young learners using arduino at a high school in india. Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.",0,0,1,information systems+learning objects+education,
1452,1452,"Getting the healthcare we want the use of online ask the doctor platforms in practice. Online Ask the Doctor (AtD) services allow access to health professionals anytime anywhere beyond existing patient-provider relationships. Recently, many free-market AtD platforms have emerged and been adopted by a large scale of users. However, it is still unclear how people make use of these AtD platforms in practice. In this paper, we present an interview study with 12 patients/caregivers who had experience using AtD in China, highlighting patient agency in seeking more reliable and cost-effective healthcare beyond clinic settings. Specifically, we illustrate how they make strategic choices online on AtD platforms, and how they strategically integrate online and offline services together for healthcare. This paper contributes an empirical study of the use of large-scale AtD platforms in practice, demonstrates patient agency for healthcare beyond clinic settings, and recommends design implications for online healthcare services.",0,0,1,hardware platform+mobile platform+service delivery,
1453,1453,"Pen mid air an exploration of mid air gestures to complement pen input on tablets. In this paper, we report on a series of studies, exploring the potential of pen and mid-air input on tablets. We describe a field study with an early prototype of a drawing application and follow-up inquiries, such as the development and comparison of gesture sets for pen and mid-air input and pen and multi-touch input with users in a lab environment. Overall, our results suggest that pen and mid-air input should be offered to complement traditional pen and multi-touch input on tablets. We illustrate the final user-defined gestures set for pen and mid-air input and discuss how user preferences of mid-air gestures over touch gestures seem to depend on the complexity of operations and the need of additional menus on the screen.",0,0,1,hand gesture+personal digital assistants+hand posture,
1454,1454,"An integral model of e loyalty from the consumer s perspective. The purpose of this study is to develop a model which explains how the loyalty of individual users of online banking is formed. In order to do this, first, the variables that contribute to the e-loyalty have been identified and subsequently have been validated, considering their reflective or formative character. The literature review leads us to consider the customers e-satisfaction and e-trust, the switching barriers and the perceived quality of the website as main antecedents of the e-loyalty. Then, a model has been estimated using structural equation modeling (SEM). Results show that e-satisfaction, e-trust and switching barriers have a direct effect on e-loyalty. Also, e-trust creates e-loyalty with e-satisfaction as a mediator; and quality of the website creates e-loyalty mediated by e-satisfaction and/or e-trust. The developed model helps improve the understanding of the creation of loyalty from an individual in a virtual context. Development of a model that explains the loyalty of online banking individuals.The developed model considers cognitive, affective and behavioral elements.This model explains 86.6% of the e-loyalty, well above previous research.",0,0,1,trust relationship+purchase intention+virtual spaces,
1455,1455,"Mixpad augmenting interactive paper with mice keyboards for fine grained cross media interaction with documents. This demo shows an interactive paper system called MixPad, which features using mice and keyboards to enhance the conventional pen-finger-gesture based interaction with paper documents. Similar to many interactive paper systems, MixPad adopts a mobile camera-projector unit to recognize paper documents, detect pen and finger gestures and provide visual feedback. Unlike these systems, MixPad allows using mice and keyboards to help users interact with fine-grained document content on paper (e.g. individual words and user-defined arbitrary regions), and to facilitate cross-media operations. For instance, to copy a document segment from paper to a laptop, one first points a finger of her non-dominant hand to the segment roughly, and then uses a mouse in her dominant hand to refine the selection and drag it to the laptop; she can also type text as a detailed comment on a paper document. This novel interaction paradigm combines the advantages of mice, keyboards, pens and fingers, and therefore enables rich digital functions on paper.",0,0,1,hand gesture+keyboard+projector,
1456,1456,"Ubiquitous wban based electrocardiogram monitoring system. This paper presents a remote patient monitoring system within the scope of Body Area Network standardization. In this regime, wireless sensor networks are used to continuously acquire the patient's Electrocardiogram signs and transmit data to the base station via IEEE.802.15. The personal Server (PS) which is responsible to provide real-time displaying, storing, and analyzing the patient's vital signs is developed in MATLAB. It also transfers ECG streams in real-time to a remote client such as a physician or medical center through internet. The PS has the potential to be integrated with home or hospital computer systems. A prototype of this system has been developed and implemented. The developed system takes advantage of two important features for healthcare monitoring: (i) ECG data acquisition using wearable sensors and (ii) real-time data remote through internet. The fact that our system is interacting with sensor network nodes using MATLAB makes it distinct from other previous works.",0,0,1,wearable sensors+wearable computers+wban,
1457,1457,"We are the greatest showmen configuring a framework for project based mobile learning. Little research has explored how mobile-learning technologies could be used by students to produce interactive artefacts during project-based learning processes. Following a design-based approach, we report on engagements spanning classroom and outdoor learning with students (ages 6-13) and teachers from three different UK schools and a summer school of Travelling Showchildren. Working within the time constraints of each context, we deployed a variety of configurations of a project-based mobile learning (PBML) framework intended to support the production of student-designed mobile-learning activities. We contribute insights gained from these engagements, including how mobile technologies can harness students' existing desire for independence and how they can be configured to leverage the physical and social attributes of place and community as learning resources. We argue for further exploration of the potential roles for mobile technologies within project-based learning, and contribute our PBML framework with recommendations for its re-configuration in response to contextual constraints.",0,0,1,mobile technology+mobile learning+innovative technologies,
1458,1458,"The effects of teaching programming with scratch on pre service information technology teachers motivation and achievement. The aim of this study is to examine the effect of programming instruction with Scratch on student motivation and their programming achievements. The study group consisted of 52 sophomore students attending the Department of Computer Education and Instructional Technologies of Mehmet Akif Ersoy University's Faculty of Education, Turkey. Participants were randomly divided into two groups in order to have 26 students in both the test group and the control group. During the first seven weeks of the study, it is aimed that the students will understand programming logic and learn basic programming structures. For this purpose, participants in the test group were instructed using Scratch, whilst in the control group, flowcharting and problem-solving activities were conducted as per the curriculum. During the second seven weeks of the study, C# programming language instruction was conducted using the same method for both the test and control groups. Achievement Test and Motivated Strategies for Learning Questionnaire were utilized as data collection tools in the study, and a 32 (measurement time x groups) factorial design was employed. Study findings revealed that programming achievement scores for both the test and control groups increased at the end of the whole process; however, the increase was significantly different in favor of the test group at the end of the whole process. It was observed that motivation scores decreased in the control group, while the test group's scores increased. Flow chart activities decrease participants' motivation.Scratch activities increase participants' motivation.The rise of motivation continue during C# instruction process.Scratch activities are more effective in improving the programming achievements then flowcharts activities.",0,0,1,faculty+education+teaching programming,
1459,1459,"Effects of virtual reality properties on user experience of individuals with autism. In recent years, virtual reality (VR) has been become a popular training tool for individuals with Autism Spectrum Disorder (ASD). Although VR was proven to be a promising tool for individuals with ASD, effects of VR properties or attributes of user interfaces designed for VR on user experience is still an unexplored area. In this study, we explore effects of five attributes of user interfaces designed for VR on user experience of high-functioning individuals with Autism Spectrum Disorder (HFASD): instruction methods, visual fidelity, view zoom, clutter, and motion. Our motivation is to give positive contribution to the design of future VR training applications for individuals with ASD so that more benefits can be gained. Three VR experiences were designed and implemented, and a user study was performed with 15 high-functioning individuals with ASD and 15 neurotypical individuals as the control group. Results indicated that using animated instructions and avoiding verbal instructions, using low visual fidelity and normal view zoom, and using no clutter and no motion in VR warehouse training applications targeting individuals with HFASD are good design practices.",0,0,1,user experience+virtual environments+virtual reality,
1460,1460,"Driver readiness model for regulating the transfer from automation to human control. In the collaborative driving scenario of truck platooning, the first car is driven by its chauffeur and the next cars follow automatically via a so-called 'virtual tow-bar'. The chauffeurs of the following cars do not drive 'in the towbar mode', but need to be able to take back control in foreseen emph{and} unforeseen conditions. It is crucial that this transfer of control only takes place when the chauffeur is ready for it. This paper presents a Driver Readiness (DR) ontological model that specifies the core factors, with their relationships, of a chauffeur's current and near-future readiness for taking back the control of driving. A first model was derived from a literature study and an analysis of truck driving data, which was refined subsequently based on an expert review. This DR model distinguishes (a) current and required states for the physical (hand, feet, head, and seating position) and mental readiness (attention and situation awareness), (b) agents (human and machine actor), (c) policies for agent behaviors, and (d) states of the vehicle and its environment. It provides the knowledge base of a Control Transfer Support (CTS) agent that assesses the current and predicted chauffeur state and guides the transition of control in an adaptive and personalized manner. The DR model will be fed by information from the network and in-car sensors. The behaviors of the CTS agent will be generated and constrained by the instantiated policies, providing an important step towards a safe transfer of control from automation to human driver.",0,0,1,human drivers+sensors+situation awareness,
1461,1461,"Low cost gamification of online surveys improving the user experience through achievement badges. Gamification of online surveys has been shown to be effective for improving user experience and data quality. However, the precise effects of isolated game elements is unknown and survey gamification requires a lot of effort. This work proposes the use of just a single game element as a novel low-cost approach. It presents evaluation results from a case study where an existing survey was gamified using the popular game element of achievement badges. Results show that the badges improved the user experience but did not influence the respondents' behavior. These benefits are similar to related work but have been achieved with a lower effort. In summary, the case study indicates our low-cost approach to be viable and efficient for survey gamification, and achievement badges to be well-suited for gamified online surveys.",0,0,1,user experience+gamification+game design,
1462,1462,"And this kids is how i met your mother consumerist mundane and uncanny futures with sex robots. Sex Robots are no longer science fiction and may soon be-come widespread. While much discussion has developed in academia on their moral and social impact, sex robots have yet to be examined from a critical design perspective and are under-explored in HCI. We use the Story Completion Method(SCM) to explore commonplace assumptions around futures with sex robots and discuss those from a critical design perspective. Thirty five participants completed a story stem of a human encountering a sex robot or vice-versa. Through thematic analysis, we show narratives of consumerist relation-ships between humans and sex robots, stories that describe sex robots as highly-efficient sex workers that (out)perform humans in routinal sex activities, and narratives that explore sex robots as empathetic and sentient beings. Our participant-created stories both reinforce and challenge established norms of sex robots and raise questions that concern responsible design and ethics in HCI. Finally, we show opportunities and limitations of using multiple-perspective story stems in SCM",0,0,1,single robots+autonomous robot+humanoid robot,
1463,1463,"A hand gesture recognition framework and wearable gesture based interaction prototype for mobile devices. An algorithmic framework is proposed to process acceleration and surface electromyographic (SEMG) signals for gesture recognition. It includes a novel segmentation scheme, a score-based sensor fusion scheme, and two new features. A Bayes linear classifier and an improved dynamic time-warping algorithm are utilized in the framework. In addition, a prototype system, including a wearable gesture sensing device (embedded with a three-axis accelerometer and four SEMG sensors) and an application program with the proposed algorithmic framework for a mobile phone, is developed to realize gesture-based real-time interaction. With the device worn on the forearm, the user is able to manipulate a mobile phone using 19 predefined gestures or even personalized ones. Results suggest that the developed prototype responded to each gesture instruction within 300 ms on the mobile phone, with the average accuracy of 95.0% in user-dependent testing and 89.6% in user-independent testing. Such performance during the interaction testing, along with positive user experience questionnaire feedback, demonstrates the utility of the framework.",0,0,1,wearable computers+mobile phones+user experience,
1464,1464,"Grounded emotions. Emotions are grounded in contextual experience. While natural language processing tools typically look at textual content to find clues pertaining to an author's emotional state, factors occurring throughout the day, such as weather or news exposure, may prime one toward a particular emotional response. In this paper, we explore five types of external factors and through extensive analyses show their impact and correlation with a user's emotional state. Ultimately, we show that when combining all extrinsic features, we are able to predict with an accuracy of 67% the emotional state of a user.",0,0,1,emotional expressions+emotion expression+affective state,
1465,1465,"A vj centered exploration of expressive interaction. This paper identifies key themes of expressive interaction for VJs. VJs are visual artists who use digital media to express themselves to an audience during a live audio-visual performance. Those designing for the expressive use of technology can gain insight from an articulation of expressive interaction from the perspective of VJ practice. This is developed using a novel qualitative methodology designed to be sensitive to the subtle and tacit nature of expression. We detail our methodology, present the results of its application to a group of VJs and conclude with a discussion of the implications our findings may have for those wishing to design for VJs, or those in related domains that involve expressive interaction with technology.",0,0,1,digital contents+digital media+multi-modal interactions,
1466,1466,"Exposed by ais people personally witness artificial intelligence exposing personal information and exposing people to undesirable content. Do people personally witness artificial intelligence (AI) committing moral wrongs? If so, what kinds of moral wrong and what situations produce these? To address these questions, respondents select...",0,0,1,intelligent systems+user information+personal information,
1467,1467,"Using model checking to detect simultaneous masking in medical alarms. The ability of people to hear and respond to auditory medical alarms is critical to the health and safety of patients. Unfortunately, concurrently sounding alarms can perceptually interact in ways that mask one or more of them: making them impossible to hear. Because masking may only occur in extremely specific and/or rare situations, experimental evaluation techniques are insufficient for detecting masking in all of the potential alarm configurations used in medicine. Thus, a real need exists for computational methods capable of determining if masking exists in medical alarm configurations before they are deployed. In this paper, we present such a method. Using a combination of formal modeling, psychoacoustic modeling, temporal logic specification, and model checking, our method is able to prove whether a modeled of a configuration of alarms can interact in ways that produce masking. This paper provides the motivation for this method, presents its details, describes its implementation, demonstrates its power with a case study, and outlines future work.",0,0,1,model checker+formal modeling+formal specification,
1468,1468,"Tenets for social accessibility towards humanizing disabled people in design. Despite years of addressing disability in technology design and advocating user-centered design practices, popular mainstream technologies remain largely inaccessible for people with disabilities. We conducted a design course study investigating how student designers regard disability and explored how designing for multiple disabled and nondisabled users encouraged students to think about accessibility in the design process. Across two university course offerings one year apart, we examined how students focused on a design project while learning user-centered design concepts and techniques, working with people with and without disabilities throughout the project. In addition, we compared how students incorporated disability-focused design approaches within a classroom setting. We found that designing for multiple stakeholders with and without disabilities expanded student understanding of accessible design by demonstrating that people with the same disability could have diverse needs and by aligning such needs with those of nondisabled users. We also found that using approaches targeted toward designing for people with disabilities complemented interactions with users, particularly with regard to managing varying abilities across users, or incorporating social aspects. Our findings contribute to an understanding about how we might incur change in design practice by working with multiple stakeholders with and without disabilities whenever possible. We refined Design for Social Accessibility by incorporating these findings into three tenets emphasizing: (1) design for disability ought to incorporate users with and without disabilities, (2) design should address functional and social factors simultaneously, and (3) design should include tools to spur consideration of social factors in accessible design.",0,0,1,people with disabilities+user centered designs+design projects,
1469,1469,"Older web users eye movements experience counts. Eye-tracking is a valuable tool for usability research. Studies into the effect of age on eye-movement behavior tend to indicate a propensity for slower viewing and longer times spent examining information. This pattern is usually attributed to the general physiological and cognitive slow-down associated with normal aging. In this paper, however, across three different tasks based on computer and internet use (free-viewing, visual search, and browser interaction), we show that among older adults (n=18, age range: 70-93) computer experience appears to be a highly important factor in eye-movement behavior. We argue that as a consequence of the experimental environment used in modern eye-tracking studies, characteristics such as familiarity and experience with computers should be taken into account before conclusions are drawn about the raw effects of age.",0,0,1,internet+web content+world wide web,
1470,1470,"Carvr enabling in car virtual reality entertainment. Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.",0,0,1,visual informations+virtual reality+head mounted displays,
1471,1471,"Where autonomous buses might and might not bridge the gaps in the 4 a s of public transport passenger needs a review. In the domain of public transport, automation provides several advantages which can lead to better services for the customers. For tapping the full potential of future automated public transport services, passenger requirements have to be thoroughly considered in order to avoid shortcomings discouraging future users to remain or become customers and to support the role of public transit as sustainable backbone of transport.This paper assesses the potential of autonomous bus services with respect to four categories of passenger requirements described as the ""4 A's of Public Transport Passenger Needs"": availability, affordability, accessibility, and acceptability. Based on a review of currently discussed scenarios of automation, benefits and risks regarding the specific needs of different passenger groups are explored for these four categories. Finally, open issues which require special attention or further research are identified.",0,0,1,potential customers+industrial automation+service delivery,
1472,1472,"Electrotutor test driven physical computing tutorials. A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users' success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.",0,0,1,computing resource+computing systems+computer systems,
1473,1473,"The role of digital artefacts on the interactive whiteboard in supporting classroom dialogue. This paper explores how the interactive whiteboard (IWB) might be harnessed to support student learning through classroom dialogue. This powerful and increasingly prevalent technology opens up opportunities for learners to generate, modify, and evaluate new ideas, through multimodal interaction along with talk. Its use can thereby support rich new forms of dialogue that highlight differences between perspectives, and make ideas and reasoning processes more explicit. The emerging account builds upon Bahktin's conception of dialogue and Wegerif's notion of technology ‘opening up a dialogic space for reflection’, but foregrounds the role of mediating artefacts. Classroom dialogue in the context of IWB use is construed as being facilitated by teachers and learners constructing digitally represented knowledge artefacts together. These visible, dynamic, and constantly evolving resources constitute interim records of activity and act as supportive devices for learners' emerging thinking, rather than finished products of dialogue.

This primarily theoretical account is illustrated with examples from case studies of UK classroom practice. Analysing lessons in sequence has illuminated how teachers can exploit the IWB through cumulative interaction with a succession of linked digital resources, and through archiving and revisiting earlier artefacts. The tool thereby helps to support the progression of dialogue over time, across settings and even across learner groups. In sum, the article reframes the notion of dialogue for this new context in which students are actively creating and directly manipulating digital artefacts, and offers some practical examples.",0,0,1,student learning+blackboard+multi-modal interactions,
1474,1474,"An energy saving unicast routing protocol in wireless ad hoc network. Wireless ad-hoc networks are widely used in various types of applications like vehicle-to-vehicle (V2V) networks. Here, each node can deliver messages to only nodes in the wireless communication range. Messages are forwarded to destination nodes by wireless node-to-node communication in ad-hoc routing protocols. It is critical to reduce the electric energy consumption of nodes to send messages since nodes work by using the electric battery. In this paper, we newly propose a reliable unicast communication protocol named ESU (Energy-Saving Unicast routing) protocol where an energy-efficient route from a source node to a destination node is dynamically found by selecting nodes which consume smaller electric energy. In the evaluation, we show the electric energy consumption of nodes can be reduced in the ESU protocol compared with other protocols.",0,0,1,multicast routing protocol+wireless ad hoc networks+unicast routing,
1475,1475,"Computational method for corrective mechanism of cognitive decision making biases. When decisions are made by human beings these choices are often “predictably irrational”. A large number of different biases effect the human beings' information processing systems. Because of the imperfect information processing system the decisions are imperfect as well, and very often some distortion appears in the solution. In our paper we propose a simple corrective mechanism for the loss aversion bias, presenting Kano's quality model as a case study. An “interface” is placed between the input data and the optimizing algorithm and the input data are debiased instead of modifying the algorithm itself. The use of fuzzy numbers is adequate to model the loss aversion type bias and corrective tools. Computational results are presented as well to demonstrate the efficiency.",0,0,1,signal distortion+integrated data+user information,
1476,1476,"Spatialized audio improves call sign recognition during multi aircraft control. Abstract We investigated the impact of a spatialized audio display on response time, workload, and accuracy while monitoring auditory information for relevance. The human ability to differentiate sound direction implies that spatial audio may be used to encode information. Therefore, it is hypothesized that spatial audio cues can be applied to aid differentiation of critical versus noncritical verbal auditory information. We used a human performance model and a laboratory study involving 24 participants to examine the effect of applying a notional, automated parser to present audio in a particular ear depending on information relevance. Operator workload and performance were assessed while subjects listened for and responded to relevant audio cues associated with critical information among additional noncritical information. Encoding relevance through spatial location in a spatial audio display system--as opposed to monophonic, binaural presentation--significantly reduced response time and workload, particularly for noncritical information. Future auditory displays employing spatial cues to indicate relevance have the potential to reduce workload and improve operator performance in similar task domains. Furthermore, these displays have the potential to reduce the dependence of workload and performance on the number of audio cues.",0,0,1,display devices+image display+display system,
1477,1477,"No mark is an island precision and category repulsion biases in data reproductions. Data visualization is powerful in large part because it facilitates visual extraction of values. Yet, existing measures of perceptual precision for data channels (e.g., position, length, orientation, etc.) are based largely on verbal reports of ratio judgments between two values (e.g., [7] ). Verbal report conflates multiple sources of error beyond actual visual precision, introducing a ratio computation between these values and a requirement to translate that ratio to a verbal number. Here we observe raw measures of precision by eliminating both ratio computations and verbal reports; we simply ask participants to reproduce marks (a single bar or dot) to match a previously seen one. We manipulated whether the mark was initially presented (and later drawn) alone, paired with a reference (e.g. a second ‘100%’ bar also present at test, or a y-axis for the dot), or integrated with the reference (merging that reference bar into a stacked bar graph, or placing the dot directly on the axis). Reproductions of smaller values were overestimated, and larger values were underestimated, suggesting systematic memory biases. Average reproduction error was around 10% of the actual value, regardless of whether the reproduction was done on a common baseline with the original. In the reference and (especially) the integrated conditions, responses were repulsed from an implicit midpoint of the reference mark, such that values above 50% were overestimated, and values below 50% were underestimated. This reproduction paradigm may serve within a new suite of more fundamental measures of the precision of graphical perception.",0,0,1,visualization+visualization tools+data visualization,
1478,1478,"Problematic internet usage self control dilemmas the opposite effects of commitment and progress framing cues on perceived value of internet academic and social behaviors. Abstract Problem Internet Usage (PIU) is a growing public health concern and despite an upsurge in research, there is limited information regarding effective psychological interventions. A model of the dynamics of self-regulation may provide a useful framework for psychological intervention with PIU. The model describes two patterns that individuals may follow when choosing goal directed behaviors, according to whether they hold commitment or progress frameworks. The model explains and predicts how opposite behavior outcomes can be achieved by holding commitment or progress frameworks. Three online studies tested the model in the context of PIU using a student population. Incongruent goal behavior was operationalised as internet activity and congruent goal behaviors as academic and social activities. Study 1 (N = 173) tested priming of commitment or progress frameworks and examined what effects positive and negative feedback had on subsequent behavior intentions. Study 2 (N = 167) examined high versus uncertain goal engagement priming effects and whether focusing on accomplished or unaccomplished actions produced the opposite behavior intentions. Study 3 (N = 172) tested if focusing on an abstract goal versus concrete steps would prime commitment or progress frameworks. Results supported the model's predictions for the framing cues and subsequent opposite behaviors for internet and academic activities with moderate and large effects. No support was found for predictions of social activities. Results of the study provide support for the self-regulation model in a clinical domain. Results may inform clinical interventions for PIU, demonstrating how opposite behavior outcomes may be achieved for the same scenarios given different underlying mental frameworks, and indicating how those frameworks may be cued in the first place.",0,0,1,internet+perceived value+semantic priming,
1479,1479,"Augmenting supervised emotion recognition with rule based decision model. The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data.",0,0,1,basic emotions+emotional expressions+emotion expression,
1480,1480,"Transforming solitary exercises into social exergames. This paper discusses an approach for transforming solitary exercises into social exergames. We frame our discussion by highlighting the relation between the original exercises and game interactions, and by analyzing an example exergame which is successfully transformed from its original solitary exercise. We present a user study of the exergame that evaluates the need for holistic transformation strategies from solitary exercises into social exergames.",0,0,1,social relations+social computing+social aspect,
1481,1481,"Robotic learning companions for early language development. Research from the past two decades indicates that preschool is a critical time for children's oral language and vocabulary development, which in turn is a primary predictor of later academic success. However, given the inherently social nature of language learning, it is difficult to develop scalable interventions for young children. Here, we present one solution in the form of robotic learning companions, using the DragonBot platform. Designed as interactive, social characters, these robots combine the flexibility and personalization afforded by educational software with a crucial social context, as peers and conversation partners. They can supplement teachers and caregivers, allowing remote operation as well as the potential for autonomously participating with children in language learning activities. Our aim is to demonstrate the efficacy of the DragonBot platform as an engaging, social, learning companion.",0,0,1,personalizations+teaching and learning+social relations,
1482,1482,"Smartcues a multitouch query approach for details on demand through dynamically computed overlays. Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). ‘Queries’ to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.",0,0,1,clutter (information theory)+visual informations+visualization,
1483,1483,"Cnn 101 interactive visual learning for convolutional neural networks. The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users' web browsers without requiring specialized hardware, broadening the public's education access to modern deep learning techniques.",0,0,1,innovative technologies+interactive exploration+visualization and analysis,
1484,1484,"Makerspaces on social media shaping access to open design. Open Design is an emerging area of research that seeks to connect and extend the culture of making, social innovation, open-source software, and open-source hardware. A cornerstone for Open Design ...",0,0,1,software+computer hardware+social media,
1485,1485,"Texting insincerely. Text messaging is one of the most frequently used computer-mediated communication (CMC) methods. The rapid pace of texting mimics face-to-face communication, leading to the question of whether the critical non-verbal aspects of conversation, such as tone, are expressed in CMC. Much of the research in this domain has involved large corpus analyses, focusing on the contents of texts, but not how receivers comprehend texts. We ask whether punctuation - specifically, the period - may serve as a cue for pragmatic and social information. Participants read short exchanges in which the response either did or did not include a sentence-final period. When the exchanges appeared as text messages, the responses that ended with a period were rated as less sincere than those that did not end with a period. No such difference was found for handwritten notes. We conclude that punctuation is one cue used by senders, and understood by receivers, to convey pragmatic and social information. Participants read exchanges in which the response did or did not end with a period.Texts that ended with a period were rated as less sincere than those that did not.For handwritten notes, no such difference was found.",0,0,1,communication+text messages+text messaging,
1486,1486,"Synthesizing pseudo straight view from a spinning camera ball. This paper proposed a spherical camera ball and its image processing algorithm, designed to provide a ball's point of view (POV) for spectating of ball sports. The proposed spherical camera ball has six cameras embedded at fixed intervals around the surface of the ball. One of the main issues for such ball-type cameras is that the device is spinning and therefore it is hard to obtain stable video stream from such spinning cameras. This paper proposed automatic selection of cameras using matching scores between the anchor frame and current frame. The resultant movie will then always shows the point of interest within the frame. We then applied image translation to obtain a pseudo straight view in which the point of interest is always shown in the center of the frame.",0,0,1,stereo cameras+video streams+viewpoint,
1487,1487,"Investigating the effect of gamification elements on bank customers to personalize gamified systems. Abstract In recent years, several gamification studies show that using personalized solutions can increase gamified systems’ effectiveness according to users’ differences. However, there is a little knowledge on how gamification can be tailored to individuals with different demographic and personality traits especially in the banking sector. To advance research in this area and fill the mentioned gaps, in this research, conducting a survey study among 412 participants, we investigated the relationships between demographic and personality traits of individuals and their preferences for the gamification elements and the expected benefits. We developed a 38-item survey to investigate how individuals with various demographic characteristics engaged with different gamification elements, and how they expect to receive the benefits of the system in return for their activities. Furthermore, we also examined that individuals with different personality traits are more likely to engage with which type of gamification elements, and what benefits they expect from the gamified systems. Finally, we provide a set of guidelines for designers to enable them design gamified systems in a way that recognize different individuals according to their unique characteristics, and taking advantage of the provided guidance, they can treat the users in a personalized manner.",0,0,1,gamification+digital games+game design,
1488,1488,"Energy conservation for the simulation of deformable bodies. We propose a novel technique that allows one to conserve energy using the time integration scheme of one's choice. Traditionally, the time integration methods that deal with energy conservation, such as symplectic, geometric, and variational integrators, have aimed to include damping in a manner independent of the size of the time step, stating that this gives more control over the look and feel of the simulation. Generally speaking, damping adds to the overall aesthetics and appeal of a numerical simulation, especially since it damps out the high frequency oscillations that occur on the level of the discretization mesh. We propose an alternative technique that allows one to use damping as a material parameter to obtain the desired look and feel of a numerical simulation, while still exactly conserving the total energy-in stark contrast to previous methods in which adding damping effects necessarily removes energy from the mesh. This allows, for example, a deformable bouncing ball with aesthetically pleasing damping (and even undergoing collision) to collide with the ground and return to its original height exactly conserving energy, as shown in Fig. 2. Furthermore, since our method works with any time integration scheme, the user can choose their favorite time integration method with regards to aesthetics and simply apply our method as a postprocess to conserve all or as much of the energy as desired.",0,0,1,conserve energy+time integration+staggered grid,
1489,1489,"Sketch based 3d model retrieval using keyshapes for global and local representation. Since 3D models are becoming more popular, the need for effective methods capable of retrieving 3D models is becoming crucial. Current methods require an example 3D model as query. However, in many cases, such a query is not easy to get. An alternative is using a hand-drawn sketch as query. In this work, we present a new keyshape based approach named HKO-KASD for retrieving 3D models using rough sketches as queries. Our approach comprises two general steps. First, a global descriptor is used to determine the appropriate viewpoint for each model. Second, we apply a local matching process to determine the final ranking for an input sketch. To this end, we present a local descriptor capable of working with sketch representations. The global descriptors as well as the local descriptors rely on a set of keyshapes precomputed from 2D representations of 3D models and from the query sketch as well. We evaluate our method using the first-tier precision and compare it with current approaches (HELO, STELA). Our results show a significant increase in precision for many classes of 3D models.",0,0,1,descriptors+3d model retrieval+3d modelling,
1490,1490,"Effect of personality traits on ux evaluation metrics a study on usability issues valence arousal and skin conductance. Personality affects the way someone feels or acts. This paper examines the effect of personality traits, as operationalized by the Big-five questionnaire, on the number, type and severity of identified usability issues, physiological signals (skin conductance), and subjective emotional ratings (valence-arousal). Twenty-four users interacted with a web service and then participated in a retrospective thinking aloud session. Results revealed that the number of usability issues is significantly affected by the Openness trait. Emotional Stability significantly affects the type of reported usability issues. Problem severity is not affected by any trait. Valence ratings are significantly affected by Conscientiousness, whereas Agreeableness, Emotional Stability and Openness significantly affect arousal ratings. Finally, Openness has a significant effect on the number of detected peaks in user's skin conductance.",0,0,1,usability evaluation+usability studies+user interface designs,
1491,1491,"Promoting self reflection of social isolation through persuasive mobile technologies the case of mother caregivers of children with cancer. Mother caregivers of children with cancer are often unaware of how the tasks of caregiving interfere in their social relationships and lifestyle. This article explores how persuasive mobile technologies may promote the self-reflection and introspection of communication practices, emotions, and lifestyle. The article describes the design of EmotionMingle, a mobile system running in a situated display showing an ambient visualization using the metaphor of a tree to represent the status of an individual’s social network that, used in tandem with downloaded Facebook photographs, may help caregivers avoid social isolation. EmotionMingle also informs caregivers of how their emotions correlate with their communication practices and lifestyle. The results of a qualitative evaluation of the EmotionMingle prototype reveal that mother caregivers perceived it as useful and its visualizations as appropriate. The findings from this study reveal emergent practices of using mobile persuasive applications for self-reflect...",0,0,1,visualization+mobile technology+innovative technologies,
1492,1492,"Artiifact a tool for heart rate artifact processing and heart rate variability analysis. The importance of appropriate handling of artifacts in interbeat interval (IBI) data must not be underestimated. Even a single artifact may cause unreliable heart rate variability (HRV) results. Thus, a robust artifact detection algorithm and the option for manual intervention by the researcher form key components for confident HRV analysis. Here, we present ARTiiFACT, a software tool for processing electrocardiogram and IBI data. Both automated and manual artifact detection and correction are available in a graphical user interface. In addition, ARTiiFACT includes time- and frequency-based HRV analyses and descriptive statistics, thus offering the basic tools for HRV analysis. Notably, all program steps can be executed separately and allow for data export, thus offering high flexibility and interoperability with a whole range of applications.",0,0,1,correlation analysis+graphical user interfaces+user interfaces,
1493,1493,"Generating audio visual slideshows from text articles using word concreteness. We present a system that automatically transforms text articles into audio-visual slideshows by leveraging the notion of word concreteness, which measures how strongly a word or phrase is related to some perceptible concept. In a formative study we learn that people not only prefer such audio-visual slideshows but find that the content is easier to understand compared to text articles or text articles augmented with images. We use word concreteness to select search terms and find images relevant to the text. Then, based on the distribution of concrete words and the grammatical structure of an article, we time-align selected images with audio narration obtained through text-to-speech to produce audio-visual slideshows. In a user evaluation we find that our concreteness-based algorithm selects images that are highly relevant to the text. The quality of our slideshows is comparable to slideshows produced manually using standard video editing tools, and people strongly prefer our slideshows to those generated using a simple keyword-search based approach.",0,0,1,keyword search+text to speech+audio-visual,
1494,1494,"La luna. ""La Luna"" is the timeless fable of a young boy who is coming of age in the most peculiar of circumstances. A big surprise awaits the little boy as he discovers his family's most unusual line of work.   The Canadian premiere screening of ""La Luna"" will be followed by a presentation by its director, Enrico Casarosa, who will discuss the journey that led him to create this very personal short and demonstrate the singularly artistic style by which the film was crafted.",0,0,1,big data+data analytics,
1495,1495,"An automated approach to measuring child movement and location in the early childhood classroom. Children’s movement is an important issue in child development and outcome in early childhood research, intervention, and practice. Digital sensor technologies offer improvements in naturalistic movement measurement and analysis. We conducted validity and feasibility testing of a real-time, indoor mapping and location system (Ubisense, Inc.) within a preschool classroom. Real-time indoor mapping has several implications with respect to efficiently and conveniently: (a) determining the activity areas where children are spending the most and least time per day (e.g., music); and (b) mapping a focal child’s atypical real-time movements (e.g., lapping behavior). We calibrated the accuracy of Ubisense point-by-point location estimates (i.e., X and Y coordinates) against laser rangefinder measurements using several stationary points and atypical movement patterns as reference standards. Our results indicate that activity areas occupied and atypical movement patterns could be plotted with an accuracy of 30.48 cm (1 ft) using a Ubisense transponder tag attached to the participating child’s shirt. The accuracy parallels findings of other researchers employing Ubisense to study atypical movement patterns in individuals at risk for dementia in an assisted living facility. The feasibility of Ubisense was tested in an approximately 90-min assessment of two children, one typically developing and one with Down syndrome, during natural classroom activities, and the results proved positive. Implications for employing Ubisense in early childhood classrooms as a data-based decision-making tool to support children’s development and its potential integration with other wearable sensor technologies are discussed.",0,0,1,wearable sensors+teaching and learning+wearable computers,
1496,1496,"How do information security workers use host data a summary of interviews with security analysts. Modern security operations centers (SOCs) employ a variety of tools for intrusion detection, prevention, and widespread log aggregation and analysis. While research efforts are quickly proposing novel algorithms and technologies for cyber security, access to actual security personnel, their data, and their problems are necessarily limited by security concerns and time constraints. To help bridge the gap between researchers and security centers, this paper reports results of semi-structured interviews of 13 professionals from five different SOCs including at least one large academic, research, and government organization. The interviews focused on the current practices and future desires of SOC operators about host-based data collection capabilities, what is learned from the data, what tools are used, and how tools are evaluated. Questions and the responses are organized and reported by topic. Then broader themes are discussed. Forest-level takeaways from the interviews center on problems stemming from size of data, correlation of heterogeneous but related data sources, signal-to-noise ratio of data, and analysts' time.",0,0,1,system on chips+computer security+soc designs,
1497,1497,"Intelligent 3d layout design with shape grammars. Shape grammars are generative systems dedicated to specific needs of designers. In the last few years, they have received increased interest especially in building reconstruction and building model generation. We propose to combine the formalism with computational intelligence methods and apply to the 3D layout problem which require efficient search of large and discontinuous spaces. The approach is illustrated by the example of a designing 3D ICs layouts. The presented results have been generated with a use of a dedicated application PerfectShape.",0,0,1,building reconstruction+shape information+integrated circuit layout,
1498,1498,"Haptically augmented remote speech communication a study of user practices and experiences. Haptic technology provides a channel for interpersonal communication through the sense of touch. In the development of novel haptic communication devices, it is essential to explore people's use behaviors and perceptions of such a communication channel. To this end, we conducted a laboratory study on haptically augmented remote interpersonal communication. Participant pairs tested a communication system that allowed them to send squeezing and thermal feedback to each other's forearm during speech discussion. We explored the use practices and user experience of this setup and compared it to traditional speech-only communication. The findings indicate that squeezing was experienced as a more versatile and immediate type of feedback than thermal feedback. Warm and cold were on the other hand useful for communicating positive and negative meanings. Compared to speech-only communication, the added haptic modality allowed conveying emphases, emotions, and touches related to the discussion, and increased the feeling of closeness between the pairs.",0,0,1,communication+user experience+haptic systems,
1499,1499,"Using low cost computer based simulations in the spanish national transplant procedures. Spain holds a worldwide privileged position with extraordinary rates of organ deceased donation resulting from an organizational approach known as the “Spanish Model”. The Spanish National Transplant Organization (ONT) is making a continuous effort to improve its teaching methods and to share its knowledge with other countries. This study describes the use of low-cost computer-based simulations covering two key procedures of the organ deceased donation process managed by the ONT central office. The goal of these simulations is to increase the awareness of other health care professionals involved in the process and to support the ONT instructional approach. After ONT experts' validation, these simulations were used by 90 trainees for practicing after a short theoretical presentation of the procedures. Students gave their opinions in a short survey where they confirmed that the simulations helped them establish a better comprehension of the processes. These simulations are now part of the ONT instructional material.",0,0,1,central offices+teaching method+teaching and learning,
